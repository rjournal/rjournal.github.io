
@article{10.1093/cercor/bhaa084,
  title = {Cortical Excitation:Inhibition Imbalance Causes Abnormal Brain Network Dynamics as Observed in Neurodevelopmental Disorders},
  author = {Markicevic, Marija and Fulcher, Ben D and Lewis, Christopher and Helmchen, Fritjof and Rudin, Markus and Zerbi, Valerio and Wenderoth, Nicole},
  year = {2020},
  month = apr,
  journal = {Cerebral Cortex},
  volume = {30},
  number = {9},
  eprint = {https://academic.oup.com/cercor/article-pdf/30/9/4922/33547031/bhaa084.pdf},
  pages = {4922--4937},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhaa084},
  abstract = {Abnormal brain development manifests itself at different spatial scales. However, whether abnormalities at the cellular level can be diagnosed from network activity measured with functional magnetic resonance imaging (fMRI) is largely unknown, yet of high clinical relevance. Here a putative mechanism reported in neurodevelopmental disorders, that is, excitation-to-inhibition ratio (E:I), was chemogenetically increased within cortical microcircuits of the mouse brain and measured via fMRI. Increased E:I caused a significant ``reduction'' of long-range connectivity, irrespective of whether excitatory neurons were facilitated or inhibitory Parvalbumin (PV) interneurons were suppressed. Training a classifier on fMRI signals, we were able to accurately classify cortical areas exhibiting increased E:I. This classifier was validated in an independent cohort of Fmr1y/- knockout mice, a model for autism with well-documented loss of parvalbumin neurons and chronic alterations of E:I. Our findings demonstrate a promising novel approach towards inferring microcircuit abnormalities from macroscopic fMRI measurements.}
}

@article{acharyaDeepConvolutionalNeural2018,
  title = {Deep Convolutional Neural Network for the Automated Detection and Diagnosis of Seizure Using {EEG} Signals},
  author = {Acharya, U. Rajendra and Oh, Shu Lih and Hagiwara, Yuki and Tan, Jen Hong and Adeli, Hojjat},
  year = {2018},
  month = sep,
  journal = {Computers in Biology and Medicine},
  volume = {100},
  pages = {270--278},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2017.09.017},
  abstract = {An encephalogram (EEG) is a commonly used ancillary test to aide in the diagnosis of epilepsy. The EEG signal contains information about the electrical activity of the brain. Traditionally, neurologists employ direct visual inspection to identify epileptiform abnormalities. This technique can be time-consuming, limited by technical artifact, provides variable results secondary to reader expertise level, and is limited in identifying abnormalities. Therefore, it is essential to develop a computer-aided diagnosis (CAD) system to automatically distinguish the class of these EEG signals using machine learning techniques. This is the first study to employ the convolutional neural network (CNN) for analysis of EEG signals. In this work, a 13-layer deep convolutional neural network (CNN) algorithm is implemented to detect normal, preictal, and seizure classes. The proposed technique achieved an accuracy, specificity, and sensitivity of 88.67\%, 90.00\% and 95.00\%, respectively.},
  langid = {english},
  keywords = {Convolutional neural network,Deep learning,Encephalogram signals,Epilepsy,Seizure},
  file = {/Users/trenthenderson/Zotero/storage/RIZ6T7XD/S0010482517303153.html}
}

@article{adyaAutomaticIdentificationTime2001,
  title = {Automatic Identification of Time Series Features for Rule-Based Forecasting},
  author = {Adya, Monica and Collopy, Fred and Armstrong, J. Scott and Kennedy, Miles},
  year = {2001},
  journal = {International Journal of Forecasting},
  volume = {17},
  number = {2},
  pages = {143--157},
  publisher = {{Elsevier}},
  abstract = {No abstract is available for this item.},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/46XG7V3N/v17y2001i2p143-157.html}
}

@article{ahnEfficientGeneticAlgorithm2020,
  title = {Efficient Genetic Algorithm for Feature Selection for Early Time Series Classification},
  author = {Ahn, Gilseung and Hur, Sun},
  year = {2020},
  month = apr,
  journal = {Computers \& Industrial Engineering},
  volume = {142},
  pages = {106345},
  issn = {0360-8352},
  doi = {10.1016/j.cie.2020.106345},
  abstract = {This paper addresses a multi-objective feature selection problem for early time series classification. Previous research has focused on how many features to consider for a classifier, but has not considered the starting time of classification, which is also important for early classification. Motivated by this, we developed a mathematical model for which the objectives are to maximize classification performance and minimize the starting time and execution time of classification. We designed an efficient genetic algorithm to generate solutions with high probability. In experiment, we compared the proposed algorithm and general genetic algorithm under various experimental settings. From the experiment, we verified that the proposed algorithm can find a better feature set in terms of classification performance, starting time and execution time of classification than feature set found by general genetic algorithm.},
  langid = {english},
  keywords = {Earliness,Feature selection,Genetic algorithm,Time series classification},
  file = {/Users/trenthenderson/Zotero/storage/YLU5AAM6/S0360835220300796.html}
}

@article{alfaro-cidGeneticProgrammingSerial2014,
  title = {Genetic Programming and Serial Processing for Time Series Classification},
  author = {{Alfaro-Cid}, Eva and Sharman, Ken and {Esparcia-Alc{\'a}zar}, Anna I.},
  year = {2014},
  journal = {Evolutionary Computation},
  volume = {22},
  number = {2},
  pages = {265--285},
  issn = {1530-9304},
  doi = {10.1162/EVCO_a_00110},
  abstract = {This work describes an approach devised by the authors for time series classification. In our approach genetic programming is used in combination with a serial processing of data, where the last output is the result of the classification. The use of genetic programming for classification, although still a field where more research in needed, is not new. However, the application of genetic programming to classification tasks is normally done by considering the input data as a feature vector. That is, to the best of our knowledge, there are not examples in the genetic programming literature of approaches where the time series data are processed serially and the last output is considered as the classification result. The serial processing approach presented here fills a gap in the existing literature. This approach was tested in three different problems. Two of them are real world problems whose data were gathered for online or conference competitions. As there are published results of these two problems this gives us the chance to compare the performance of our approach against top performing methods. The serial processing of data in combination with genetic programming obtained competitive results in both competitions, showing its potential for solving time series classification problems. The main advantage of our serial processing approach is that it can easily handle very large datasets.},
  langid = {english},
  pmid = {24032750},
  keywords = {Algorithms,Classification,Computing Methodologies,Models; Theoretical,Programming Languages,Time Factors},
  file = {/Users/trenthenderson/Zotero/storage/KDZBMRD8/Alfaro-Cid et al. - 2014 - Genetic programming and serial processing for time.pdf}
}

@article{andrzejakIndicationsNonlinearDeterministic2001,
  title = {Indications of Nonlinear Deterministic and Finite-Dimensional Structures in Time Series of Brain Electrical Activity: Dependence on Recording Region and Brain State},
  shorttitle = {Indications of Nonlinear Deterministic and Finite-Dimensional Structures in Time Series of Brain Electrical Activity},
  author = {Andrzejak, R. G. and Lehnertz, K. and Mormann, F. and Rieke, C. and David, P. and Elger, C. E.},
  year = {2001},
  month = dec,
  journal = {Physical Review. E, Statistical, Nonlinear, and Soft Matter Physics},
  volume = {64},
  number = {6 Pt 1},
  pages = {061907},
  issn = {1539-3755},
  doi = {10.1103/PhysRevE.64.061907},
  abstract = {We compare dynamical properties of brain electrical activity from different recording regions and from different physiological and pathological brain states. Using the nonlinear prediction error and an estimate of an effective correlation dimension in combination with the method of iterative amplitude adjusted surrogate data, we analyze sets of electroencephalographic (EEG) time series: surface EEG recordings from healthy volunteers with eyes closed and eyes open, and intracranial EEG recordings from epilepsy patients during the seizure free interval from within and from outside the seizure generating area as well as intracranial EEG recordings of epileptic seizures. As a preanalysis step an inclusion criterion of weak stationarity was applied. Surface EEG recordings with eyes open were compatible with the surrogates' null hypothesis of a Gaussian linear stochastic process. Strongest indications of nonlinear deterministic dynamics were found for seizure activity. Results of the other sets were found to be inbetween these two extremes.},
  langid = {english},
  pmid = {11736210},
  keywords = {Brain,Electroencephalography,Electrophysiology,Epilepsy,Humans,Models; Statistical,Time Factors},
  file = {/Users/trenthenderson/Zotero/storage/Z53HX6I7/Andrzejak et al. - 2001 - Indications of nonlinear deterministic and finite-.pdf}
}

@article{bagnallGreatTimeSeries2017,
  title = {The Great Time Series Classification Bake off: A Review and Experimental Evaluation of Recent Algorithmic Advances},
  author = {Bagnall, Anthony and Lines, Jason and Bostrom, Aaron and Large, James and Keogh, Eamonn},
  year = {2017},
  month = may,
  journal = {Data Mining and Knowledge Discovery},
  volume = {31},
  number = {3},
  pages = {606--660},
  issn = {1573-756X},
  doi = {10.1007/s10618-016-0483-9},
  abstract = {In the last 5~years there have been a large number of new time series classification algorithms proposed in the literature. These algorithms have been evaluated on subsets of the 47 data sets in the University of California, Riverside time series classification archive. The archive has recently been expanded to 85 data sets, over half of which have been donated by researchers at the University of East Anglia. Aspects of previous evaluations have made comparisons between algorithms difficult. For example, several different programming languages have been used, experiments involved a single train/test split and some used normalised data whilst others did not. The relaunch of the archive provides a timely opportunity to thoroughly evaluate algorithms on a larger number of datasets. We have implemented 18 recently proposed algorithms in a common Java framework and compared them against two standard benchmark classifiers (and each other) by performing 100 resampling experiments on each of the 85 datasets. We use these results to test several hypotheses relating to whether the algorithms are significantly more accurate than the benchmarks and each other. Our results indicate that only nine of these algorithms are significantly more accurate than both benchmarks and that one classifier, the collective of transformation ensembles, is significantly more accurate than all of the others. All of our experiments and results are reproducible: we release all of our code, results and experimental details and we hope these experiments form the basis for more robust testing of new algorithms in the future.}
}

@incollection{bagnallTransformationBasedEnsembles2012,
  title = {Transformation {{Based Ensembles}} for {{Time Series Classification}}},
  booktitle = {Proceedings of the 2012 {{SIAM International Conference}} on {{Data Mining}} ({{SDM}})},
  author = {Bagnall, Anthony and Davis, Luke and Hills, Jon and Lines, Jason},
  year = {2012},
  month = apr,
  series = {Proceedings},
  pages = {307--318},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611972825.27},
  abstract = {Until recently, the vast majority of data mining time series classification (TSC) research has focused on alternative distance measures for 1-Nearest Neighbour (1-NN) classifiers based on either the raw data, or on compressions or smoothing of the raw data. Despite the extensive evidence in favour of 1-NN classifiers with Euclidean or Dynamic Time Warping distance, there has also been a flurry of recent research publications proposing classification algorithms for TSC. Generally, these classifiers describe different ways of incorporating summary measures in the time domain into more complex classifiers. Our hypothesis is that the easiest way to gain improvement on TSC problems is simply to transform into an alternative data space where the discriminatory features are more easily detected. To test our hypothesis, we perform a range of benchmarking experiments in the time domain, before evaluating nearest neighbour classifiers on data transformed into the power spectrum, the autocorrelation function, and the principal component space. We demonstrate that on some problems there is dramatic improvement in the accuracy of classifiers built on the transformed data over classifiers built in the time domain, but that there is also a wide variance in accuracy for a particular classifier built on different data transforms. To overcome this variability, we propose a simple transformation based ensemble, then demonstrate that it improves performance and reduces the variability of classifiers built in the time domain only. Our advice to a practitioner with a real world TSC problem is to try transforms before developing a complex classifier; it is the easiest way to get a potentially large increase in accuracy, and may provide further insights into the underlying relationships that characterise the problem.},
  isbn = {978-1-61197-232-0},
  file = {/Users/trenthenderson/Zotero/storage/HF32DJX2/Bagnall et al. - 2012 - Transformation Based Ensembles for Time Series Cla.pdf}
}

@article{baldanSCMFTSScalableDistributed2021,
  title = {{{SCMFTS}}: {{Scalable}} and {{Distributed Complexity Measures}} and {{Features}} for {{Univariate}} and {{Multivariate Time Series}} in {{Big Data Environments}}},
  shorttitle = {{{SCMFTS}}},
  author = {Bald{\'a}n, Francisco J. and Peralta, Daniel and Saeys, Yvan and Ben{\'i}tez, Jos{\'e} M.},
  year = {2021},
  month = nov,
  journal = {International Journal of Computational Intelligence Systems},
  volume = {14},
  number = {1},
  pages = {186},
  issn = {1875-6883},
  doi = {10.1007/s44196-021-00036-7},
  abstract = {Time series data are becoming increasingly important due to the interconnectedness of the world. Classical problems, which are getting bigger and bigger, require more and more resources for their processing, and Big Data technologies offer many solutions. Although the principal algorithms for traditional vector-based problems are available in Big Data environments, the lack of tools for time series processing in these environments needs to be addressed. In this work, we propose a scalable and distributed time series transformation for Big Data environments based on well-known time series features (SCMFTS), which allows practitioners to apply traditional vector-based algorithms to time series problems. The proposed transformation, along with the algorithms available in Spark, improved the best results in the state-of-the-art on the Wearable Stress and Affect Detection dataset, which is the biggest publicly available multivariate time series dataset in the University of California Irvine (UCI) Machine Learning Repository. In addition, SCMFTS showed a linear relationship between its runtime and the number of processed time series, demonstrating a linear scalable behavior, which is mandatory in Big Data environments. SCMFTS has been implemented in the Scala programming language for the Apache Spark framework, and the code is publicly available.},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/I4KSJ4I3/Bald√°n et al. - 2021 - SCMFTS Scalable and Distributed Complexity Measur.pdf}
}

@article{barandasTSFELTimeSeries2020,
  title = {{{TSFEL}}: {{Time Series Feature Extraction Library}}},
  shorttitle = {{{TSFEL}}},
  author = {Barandas, Mar{\'i}lia and Folgado, Duarte and Fernandes, Let{\'i}cia and Santos, Sara and Abreu, Mariana and Bota, Patr{\'i}cia and Liu, Hui and Schultz, Tanja and Gamboa, Hugo},
  year = {2020},
  month = jan,
  journal = {SoftwareX},
  volume = {11},
  pages = {100456},
  issn = {2352-7110},
  doi = {10.1016/j.softx.2020.100456},
  abstract = {Time series feature extraction is one of the preliminary steps of conventional machine learning pipelines. Quite often, this process ends being a time consuming and complex task as data scientists must consider a combination between a multitude of domain knowledge factors and coding implementation. We present in this paper a Python package entitled Time Series Feature Extraction Library (TSFEL), which computes over 60 different features extracted across temporal, statistical and spectral domains. User customisation is achieved using either an online interface or a conventional Python package for more flexibility and integration into real deployment scenarios. TSFEL is designed to support the process of fast exploratory data analysis and feature extraction on time series with computational cost evaluation.},
  langid = {english},
  keywords = {Feature extraction,Machine learning,Python,Time series},
  file = {/Users/trenthenderson/Zotero/storage/RKXA7Q4P/Barandas et al. - 2020 - TSFEL Time Series Feature Extraction Library.pdf;/Users/trenthenderson/Zotero/storage/RU5BB8TB/S2352711020300017.html}
}

@article{barbaraClassifyingKeplerLight2022,
  title = {Classifying {{Kepler}} Light Curves for 12,000 {{A}} and {{F}} Stars Using Supervised Feature-Based Machine Learning},
  author = {Barbara, Nicholas H and Bedding, Timothy R and Fulcher, Ben D and Murphy, Simon J and Van Reeth, Timothy},
  year = {2022},
  month = jun,
  journal = {Monthly Notices of the Royal Astronomical Society},
  pages = {stac1515},
  issn = {0035-8711},
  doi = {10.1093/mnras/stac1515},
  abstract = {With the availability of large-scale surveys like Kepler and TESS, there is a pressing need for automated methods to classify light curves according to known classes of variable stars. We introduce a new algorithm for classifying light curves that compares 7000 time-series features to find those which most effectively classify a given set of light curves. We apply our method to Kepler light curves for stars with effective temperatures in the range 6500\textendash 10,000~K. We show that the sample can be meaningfully represented in an interpretable five-dimensional feature space that separates seven major classes of light curves ({$\delta~$}Scuti stars, {$\gamma~$}Doradus stars, RR~Lyrae stars, rotational variables, contact eclipsing binaries, detached eclipsing binaries, and non-variables). We achieve a balanced classification accuracy of 82\% on an independent test set of Kepler stars using a Gaussian mixture model classifier. We use our method to classify 12,000 Kepler light curves from Quarter 9 and provide a catalogue of the results. We further outline a confidence heuristic based on probability density with which to search our catalogue, and extract candidate lists of correctly-classified variable stars.}
}

@misc{BehavioralDiscriminationTimeseries,
  title = {Behavioral Discrimination and Time-Series Phenotyping of Birdsong Performance | {{PLOS Computational Biology}}},
  howpublished = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008820}
}

@article{belkinReconcilingBiasVariance2019,
  title = {Reconciling Modern Machine-Learning Practice and the Classical Bias-Variance Trade-Off},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and {Soumik Mandal}},
  year = {2019},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {32},
  eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1903070116},
  pages = {15849--15854},
  doi = {10.1073/pnas.1903070116},
  abstract = {While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms. Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias\textendash variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias\textendash variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This ``double-descent'' curve subsumes the textbook U-shaped bias\textendash variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.}
}

@inproceedings{berndtUsingDynamicTime1994,
  title = {Using Dynamic Time Warping to Find Patterns in Time Series},
  booktitle = {Proceedings of the 3rd {{International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Berndt, Donald J. and Clifford, James},
  year = {1994},
  month = jul,
  series = {{{AAAIWS}}'94},
  pages = {359--370},
  publisher = {{AAAI Press}},
  address = {{Seattle, WA}},
  abstract = {Knowledge discovery in databases presents many interesting challenges within the context of providing computer tools for exploring large data archives. Electronic data repositories are growing quickly and contain data from commercial, scientific, and other domains. Much of this data is inherently temporal, such as stock prices or NASA telemetry data. Detecting patterns in such data streams or time series is an important knowledge discovery task. This paper describes some preliminary experiments with a dynamic programming approach to the problem. The pattern detection algorithm is based on the dynamic time warping technique used in the speech recognition field.},
  keywords = {dynamic programming,dynamic time warping,knowledge discovery,pattern analysis,time series}
}

@misc{bezansonJuliaFreshApproach2015,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  shorttitle = {Julia},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  year = {2015},
  month = jul,
  number = {arXiv:1411.1607},
  eprint = {1411.1607},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1411.1607},
  abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast. Julia questions notions generally held as "laws of nature" by practitioners of numerical computing: 1. High-level dynamic programs have to be slow. 2. One must prototype in one language and then rewrite in another language for speed or deployment, and 3. There are parts of a system for the programmer, and other parts best left untouched as they are built by the experts. We introduce the Julia programming language and its design --- a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that one can have machine performance without sacrificing human convenience.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Mathematical Software},
  file = {/Users/trenthenderson/Zotero/storage/Y7I7LAMM/Bezanson et al. - 2015 - Julia A Fresh Approach to Numerical Computing.pdf;/Users/trenthenderson/Zotero/storage/HM5N5UK2/1411.html}
}

@misc{christDistributedParallelTime2017,
  title = {Distributed and Parallel Time Series Feature Extraction for Industrial Big Data Applications},
  author = {Christ, Maximilian and {Kempa-Liehr}, Andreas W. and Feindt, Michael},
  year = {2017},
  month = may,
  number = {arXiv:1610.07717},
  eprint = {1610.07717},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1610.07717},
  abstract = {The all-relevant problem of feature selection is the identification of all strongly and weakly relevant attributes. This problem is especially hard to solve for time series classification and regression in industrial applications such as predictive maintenance or production line optimization, for which each label or regression target is associated with several time series and meta-information simultaneously. Here, we are proposing an efficient, scalable feature extraction algorithm for time series, which filters the available features in an early stage of the machine learning pipeline with respect to their significance for the classification or regression task, while controlling the expected percentage of selected but irrelevant features. The proposed algorithm combines established feature extraction methods with a feature importance filter. It has a low computational complexity, allows to start on a problem with only limited domain knowledge available, can be trivially parallelized, is highly scalable and based on well studied non-parametric hypothesis tests. We benchmark our proposed algorithm on all binary classification problems of the UCR time series classification archive as well as time series from a production line optimization project and simulated stochastic processes with underlying qualitative change of dynamics.},
  archiveprefix = {arXiv},
  keywords = {62M10,Computer Science - Machine Learning,I.2.11},
  file = {/Users/trenthenderson/Zotero/storage/A33QZ5IX/Christ et al. - 2017 - Distributed and parallel time series feature extra.pdf;/Users/trenthenderson/Zotero/storage/K5P5CRP2/1610.html}
}

@article{christTimeSeriesFeatuRe2018,
  title = {{Time Series FeatuRe Extraction} on Basis of {Scalable Hypothesis} Tests ({tsfresh} \textendash{} A {Python} Package)},
  author = {Christ, Maximilian and Braun, Nils and Neuffer, Julius and {Kempa-Liehr}, Andreas W.},
  year = {2018},
  month = sep,
  journal = {Neurocomputing},
  volume = {307},
  pages = {72--77},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2018.03.067},
  abstract = {Time series feature engineering is a time-consuming process because scientists and engineers have to consider the multifarious algorithms of signal processing and time series analysis for identifying and extracting meaningful features from time series. The Python package tsfresh (Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests) accelerates this process by combining 63 time series characterization methods, which by default compute a total of 794 time series features, with feature selection on basis automatically configured hypothesis tests. By identifying statistically significant time series characteristics in an early stage of the data science process, tsfresh closes feedback loops with domain experts and fosters the development of domain specific features early on. The package implements standard APIs of time series and machine learning libraries (e.g. pandas and scikit-learn) and is designed for both exploratory analyses as well as straightforward integration into operational data science applications.},
  langid = {english},
  keywords = {Feature engineering,Feature extraction,Feature selection,Machine learning,Time series},
  file = {/Users/trenthenderson/Zotero/storage/H69C3JDB/Christ et al. - 2018 - Time Series FeatuRe Extraction on basis of Scalabl.pdf;/Users/trenthenderson/Zotero/storage/X87XW9BQ/S0925231218304843.html}
}

@misc{cuiMultiScaleConvolutionalNeural2016,
  title = {Multi-{{Scale Convolutional Neural Networks}} for {{Time Series Classification}}},
  author = {Cui, Zhicheng and Chen, Wenlin and Chen, Yixin},
  year = {2016},
  month = may,
  number = {arXiv:1603.06995},
  eprint = {1603.06995},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1603.06995},
  abstract = {Time series classification (TSC), the problem of predicting class labels of time series, has been around for decades within the community of data mining and machine learning, and found many important applications such as biomedical engineering and clinical prediction. However, it still remains challenging and falls short of classification accuracy and efficiency. Traditional approaches typically involve extracting discriminative features from the original time series using dynamic time warping (DTW) or shapelet transformation, based on which an off-the-shelf classifier can be applied. These methods are ad-hoc and separate the feature extraction part with the classification part, which limits their accuracy performance. Plus, most existing methods fail to take into account the fact that time series often have features at different time scales. To address these problems, we propose a novel end-to-end neural network model, Multi-Scale Convolutional Neural Networks (MCNN), which incorporates feature extraction and classification in a single framework. Leveraging a novel multi-branch layer and learnable convolutional layers, MCNN automatically extracts features at different scales and frequencies, leading to superior feature representation. MCNN is also computationally efficient, as it naturally leverages GPU computing. We conduct comprehensive empirical evaluation with various existing methods on a large number of benchmark datasets, and show that MCNN advances the state-of-the-art by achieving superior accuracy performance than other leading methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/trenthenderson/Zotero/storage/TT44KUG3/Cui et al. - 2016 - Multi-Scale Convolutional Neural Networks for Time.pdf;/Users/trenthenderson/Zotero/storage/WZZTPTHX/1603.html}
}

@article{dasHeterogeneityPreictalDynamics2020,
  title = {Heterogeneity of {{Preictal Dynamics}} in {{Human Epileptic Seizures}}},
  author = {Das, Anup and Cash, Sydney S. and Sejnowski, Terrence J.},
  year = {2020},
  journal = {IEEE access: practical innovations, open solutions},
  volume = {8},
  pages = {52738--52748},
  issn = {2169-3536},
  doi = {10.1109/access.2020.2981017},
  abstract = {It is generally understood that there is a preictal phase in the development of a seizure and this precictal period is the basis for seizure prediction attempts. The focus of this study is the preictal global spatiotemporal dynamics and its intra-patient variability. We analyzed preictal broadband brain connectivity from human electrocorticography (ECoG) recordings of 185 seizures (which included 116 clinical seizures) collected from 12 patients. ECoG electrodes record from only a part of the cortex, leaving large regions of the brain unobserved. Brain connectivity was therefore estimated using the sparse-plus-latent-regularized precision matrix (SLRPM) method, which calculates connectivity from partial correlations of the conditional statistics of the observed regions given the unobserved latent regions. Brain connectivity was quantified using eigenvector centrality (EC), from which a degree of heterogeneity was calculated for the preictal periods of all seizures in each patient. Results from the SLRPM method are compared to those from the sparse-regularized precision matrix (SRPM) and correlation methods, which do not account for the unobserved inputs when estimating brain connectivity. The degree of heterogeneity estimated by the SLRPM method is higher than those estimated by the SRPM and correlation methods for the preictal periods in most patients. These results reveal substantial heterogeneity or desynchronization among brain areas in the preictal period of human epileptic seizures. Furthermore, the SLRPM method identifies more onset channels from the preictal active electrodes compared to the SRPM and correlation methods. Finally, the correlation between the degree of heterogeneity and seizure severity of patients for SLRPM and SRPM methods were lower than that obtained from the correlation method. These results support recent findings suggesting that inhibitory neurons can have anti-seizure effects by inducing variability or heterogeneity across seizures. Understanding how this variability is linked to seizure initiation may lead to better predictions and controlling therapies.},
  langid = {english},
  pmcid = {PMC7224217},
  pmid = {32411567},
  keywords = {Connectivity,eigenvector centrality (EC),electrocorticography (ECoG),latent inputs,multivariate Gaussian,partial correlation,sparse-plus-latent-regularized precision matrix (SLRPM)},
  file = {/Users/trenthenderson/Zotero/storage/ADEZXXWY/Das et al. - 2020 - Heterogeneity of Preictal Dynamics in Human Epilep.pdf}
}

@article{dayEfficientAlgorithmsAgglomerative1984,
  title = {Efficient Algorithms for Agglomerative Hierarchical Clustering Methods},
  author = {Day, William H. E. and Edelsbrunner, Herbert},
  year = {1984},
  month = dec,
  journal = {Journal of Classification},
  volume = {1},
  number = {1},
  pages = {7--24},
  issn = {1432-1343},
  doi = {10.1007/BF01890115},
  abstract = {Whenevern objects are characterized by a matrix of pairwise dissimilarities, they may be clustered by any of a number of sequential, agglomerative, hierarchical, nonoverlapping (SAHN) clustering methods. These SAHN clustering methods are defined by a paradigmatic algorithm that usually requires 0(n3) time, in the worst case, to cluster the objects. An improved algorithm (Anderberg 1973), while still requiring 0(n3) worst-case time, can reasonably be expected to exhibit 0(n2) expected behavior. By contrast, we describe a SAHN clustering algorithm that requires 0(n2 logn) time in the worst case. When SAHN clustering methods exhibit reasonable space distortion properties, further improvements are possible. We adapt a SAHN clustering algorithm, based on the efficient construction of nearest neighbor chains, to obtain a reasonably general SAHN clustering algorithm that requires in the worst case 0(n2) time and space.},
  langid = {english},
  keywords = {Algorithm complexity,Algorithm design,Centroid clustering method,Geometric model,SAHN clustering method}
}

@misc{decatTraditionalVisualSleep2021,
  title = {Beyond Traditional Visual Sleep Scoring: Massive Feature Extraction and Unsupervised Clustering of Sleep Time Series},
  shorttitle = {Beyond Traditional Visual Sleep Scoring},
  author = {Decat, Nicolas and Walter, Jasmine and Koh, Zhao H. and Sribanditmongkol, Piengkwan and Fulcher, Ben D. and Windt, Jennifer M. and Andrillon, Thomas and Tsuchiya, Naotsugu},
  year = {2021},
  month = sep,
  pages = {2021.09.08.458981},
  institution = {{bioRxiv}},
  doi = {10.1101/2021.09.08.458981},
  abstract = {Sleep is classically measured with electrophysiological recordings, which are then scored based on guidelines tailored for the visual inspection of these recordings. As such, these rules reflect a limited range of features easily captured by the human eye and do not always reflect the physiological changes associated with sleep. Here we present a novel analysis framework that characterizes sleep using over 7700 time-series features from the hctsa software. We used clustering to categorize sleep epochs based on the similarity of their features, without relying on established scoring conventions. The resulting structure overlapped substantially with that defined by visual scoring and we report novel features that are highly discriminative of sleep stages. However, we also observed discrepancies as hctsa features unraveled distinctive properties within traditional sleep stages. Our framework lays the groundwork for a data-driven exploration of sleep and the identification of new signatures of sleep disorders and conscious sleep states.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/LU9MNGVP/Decat et al. - 2021 - Beyond traditional visual sleep scoring massive f.pdf}
}

@article{decatTraditionalSleepScoring2022,
  title = {Beyond Traditional Sleep Scoring: {{Massive}} Feature Extraction and Data-Driven Clustering of Sleep Time Series},
  shorttitle = {Beyond Traditional Sleep Scoring},
  author = {Decat, Nicolas and Walter, Jasmine and Koh, Zhao H. and Sribanditmongkol, Piengkwan and Fulcher, Ben D. and Windt, Jennifer M. and Andrillon, Thomas and Tsuchiya, Naotsugu},
  year = {2022},
  month = oct,
  journal = {Sleep Medicine},
  volume = {98},
  pages = {39--52},
  issn = {1389-9457},
  doi = {10.1016/j.sleep.2022.06.013},
  abstract = {The widely used guidelines for sleep staging were developed for the visual inspection of electrophysiological recordings by the human eye. As such, these rules reflect a limited range of features in these data and are therefore restricted in accurately capturing the physiological changes associated with sleep. Here we present a novel analysis framework that extensively characterizes sleep dynamics using over 7700 time-series features from the hctsa software. We used clustering to categorize sleep epochs based on the similarity of their time-series features, without relying on established scoring conventions. The resulting sleep structure overlapped substantially with that defined by visual scoring. However, we also observed discrepancies between our approach and traditional scoring. This divergence principally stemmed from the extensive characterization by hctsa features, which captured distinctive time-series properties within the traditionally defined sleep stages that are overlooked with visual scoring. Lastly, we report time-series features that are highly discriminative of stages. Our framework lays the groundwork for a data-driven exploration of sleep sub-stages and has significant potential to identify new signatures of sleep disorders and conscious sleep states.},
  langid = {english},
  keywords = {Clustering,Electroencephalography (EEG),Massive feature extraction,Polysomnography (PSG),Sleep physiology,Sleep scoring,Time-series analysis}
}

@book{eggermontDataMiningUsing2005,
  title = {Data Mining Using Genetic Programming: Classification and Symbolic Regression},
  shorttitle = {Data Mining Using Genetic Programming},
  author = {Eggermont, Jeroen},
  year = {2005},
  publisher = {{Univ}},
  address = {{Leiden}},
  isbn = {978-90-90-19760-9},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/GVWW3GPK/Eggermont - 2005 - Data mining using genetic programming classificat.pdf}
}

@article{espejoSurveyApplicationGenetic2010,
  title = {A {{Survey}} on the {{Application}} of {{Genetic Programming}} to {{Classification}}},
  author = {Espejo, Pedro G. and Ventura, Sebasti{\'a}n and Herrera, Francisco},
  year = {2010},
  month = mar,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume = {40},
  number = {2},
  pages = {121--144},
  issn = {1558-2442},
  doi = {10.1109/TSMCC.2009.2033566},
  abstract = {Classification is one of the most researched questions in machine learning and data mining. A wide range of real problems have been stated as classification problems, for example credit scoring, bankruptcy prediction, medical diagnosis, pattern recognition, text categorization, software quality assessment, and many more. The use of evolutionary algorithms for training classifiers has been studied in the past few decades. Genetic programming (GP) is a flexible and powerful evolutionary technique with some features that can be very valuable and suitable for the evolution of classifiers. This paper surveys existing literature about the application of genetic programming to classification, to show the different ways in which this evolutionary algorithm can help in the construction of accurate and reliable classifiers.},
  keywords = {Classification,Classification tree analysis,Computer science,Data mining,decision trees,Decision trees,ensemble classifiers,Evolutionary computation,feature construction,feature selection,Genetic programming,genetic programming (GP),Machine learning,Medical diagnosis,rule-based systems,Supervised learning,Unsupervised learning},
  file = {/Users/trenthenderson/Zotero/storage/3C83CSGC/5340522.html}
}

@misc{ExploringGrangerCausality,
  title = {Exploring {{Granger}} Causality between Global Average Observed Time Series of Carbon Dioxide and Temperature - {{ProQuest}}},
  abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
  howpublished = {https://www.proquest.com/openview/158a07106a8e082f11705498abf49520/1?pq-origsite=gscholar\&cbl=48318},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/PL2GTC9Q/1.html}
}

@article{fallonTimescalesSpontaneousFMRI2020,
  title = {Timescales of Spontaneous {{fMRI}} Fluctuations Relate to Structural Connectivity in the Brain},
  author = {Fallon, John and Ward, Phillip G. D. and Parkes, Linden and Oldham, Stuart and Arnatkevi{\v c}i{\=u}t{\.e}, Aurina and Fornito, Alex and Fulcher, Ben D.},
  year = {2020},
  month = sep,
  journal = {Network Neuroscience},
  volume = {4},
  number = {3},
  pages = {788--806},
  issn = {2472-1751},
  doi = {10.1162/netn_a_00151},
  abstract = {Intrinsic timescales of activity fluctuations vary hierarchically across the brain. This variation reflects a broad gradient of functional specialization in information storage and processing, with integrative association areas displaying slower timescales that are thought to reflect longer temporal processing windows. The organization of timescales is associated with cognitive function, distinctive between individuals, and disrupted in disease, but we do not yet understand how the temporal properties of activity dynamics are shaped by the brain's underlying structural connectivity network. Using resting-state fMRI and diffusion MRI data from 100 healthy individuals from the Human Connectome Project, here we show that the timescale of resting-state fMRI dynamics increases with structural connectivity strength, matching recent results in the mouse brain. Our results hold at the level of individuals, are robust to parcellation schemes, and are conserved across a range of different timescale- related statistics. We establish a comprehensive BOLD dynamical signature of structural connectivity strength by comparing over 6,000 time series features, highlighting a range of new temporal features for characterizing BOLD dynamics, including measures of stationarity and symbolic motif frequencies. Our findings indicate a conserved property of mouse and human brain organization in which a brain region's spontaneous activity fluctuations are closely related to their surrounding structural scaffold.Reflecting structural and functional differences across brain regions, the spontaneous dynamics of neural activity vary correspondingly. Dynamical timescales are thought to be organized hierarchically, with slower timescales in integrative association areas, consistent with longer durations of information processing. In the mouse brain, this variation in BOLD dynamical properties follows the variation in structural connectivity strength, with more strongly connected regions exhibiting slower dynamics. Here we show a consistent variation in human cortex that holds at the level of individuals, and characterize a range of BOLD properties that vary strongly with structural connectivity strength. Our results indicate a conserved property of mouse and human brain organization in which a brain area's spontaneous activity fluctuations are closely related to its structural connectivity strength.},
  annotation = {\_eprint: https://direct.mit.edu/netn/article-pdf/4/3/788/1867373/netn\_a\_00151.pdf}
}

@article{frankGeneticProgrammingJulia,
  title = {Genetic {{Programming}} for {{Julia}}: Fast Performance and Parallel Island Model Implementation},
  author = {Frank, Morgan R},
  pages = {6},
  abstract = {I introduce a Julia implementation for genetic programming (GP), which is an evolutionary algorithm that evolves models as syntax trees. While some abstract high-level genetic algorithm packages, such as GeneticAlgorithms.jl, already exist for Julia, this package is not optimized for genetic programming, and I provide a relatively fast implementation here by utilizing the low-level Expr Julia type. The resulting GP implementation has a simple programmatic interface that provides ample access to the parameters controlling the evolution. Finally, I provide the option for the GP to run in parallel using the highly scalable ''island model'' for genetic algorithms, which has been shown to improve search results in a variety of genetic algorithms by maintaining solution diversity and explorative dynamics across the global population of solutions.},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/N2Z6KL24/Frank - Genetic Programming for Julia fast performance an.pdf}
}

@article{fulcherFeaturebasedTimeseriesAnalysis2017,
  title = {Feature-Based Time-Series Analysis},
  author = {Fulcher, Ben D.},
  year = {2017},
  month = oct,
  journal = {arXiv:1709.08055 [cs]},
  eprint = {1709.08055},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This work presents an introduction to feature-based time-series analysis. The time series as a data type is first described, along with an overview of the interdisciplinary time-series analysis literature. I then summarize the range of feature-based representations for time series that have been developed to aid interpretable insights into time-series structure. Particular emphasis is given to emerging research that facilitates wide comparison of feature-based representations that allow us to understand the properties of a time-series dataset that make it suited to a particular feature-based representation or analysis algorithm. The future of time-series analysis is likely to embrace approaches that exploit machine learning methods to partially automate human learning to aid understanding of the complex dynamical patterns in the time series we measure from the world.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/trenthenderson/Zotero/storage/C7ZUBMZA/Fulcher - 2017 - Feature-based time-series analysis.pdf;/Users/trenthenderson/Zotero/storage/9RVT2ZW2/1709.html}
}

@incollection{fulcherFeatureBasedTimeSeriesAnalysis2018,
  title = {Feature-Based Time-Series Analysis},
  booktitle = {Feature Engineering for Machine Learning and Data Analytics},
  author = {Fulcher, Ben D.},
  year = {2018},
  publisher = {{CRC Press}},
  abstract = {This chapter focuses on individual univariate time series sampled uniformly through time. It describes the use of time-series features for tackling time-series forecasting. The chapter provides an overview of a vast literature of representations and analysis methods for time series. It explores global distances between time-series values, subsequences that provide more localized shape-based information, global features that capture higher-order structure, and interval features that capture discriminative properties in time-series subsequences. The chapter also describes feature-based representations of time series using the problem of defining a measure of similarity between pairs of time series, which is required for many applications of time-series analysis, including many problems in time-series data mining. Time series are a fundamental data type for understanding dynamics in real-world systems. The interdisciplinary reach of the time-series analysis literature reflects the diverse range of problem classes that involve time series. Global features refer to algorithms that quantify patterns in time series across the full time interval of measurement.},
  isbn = {978-1-315-18108-0}
}

@article{fulcherHctsaComputationalFramework2017,
  title = {{hctsa}: A Computational Framework for Automated Time-Series Phenotyping Using Massive Feature Extraction},
  author = {Fulcher, Ben D. and Jones, Nick S.},
  year = {2017},
  month = nov,
  journal = {Cell Systems},
  volume = {5},
  number = {5},
  pages = {527-531.e3},
  issn = {2405-4712},
  doi = {10.1016/j.cels.2017.10.001},
  abstract = {Phenotype measurements frequently take the form of time series, but we currently lack a systematic method for relating these complex data streams to scientifically meaningful outcomes, such as relating the movement dynamics of organisms to their genotype or measurements of brain dynamics of a patient to their disease diagnosis. Previous work addressed this problem by comparing implementations of thousands of diverse scientific time-series analysis methods in an approach termed highly comparative time-series analysis. Here, we introduce hctsa, a software tool for applying this methodological approach to data. hctsa includes an architecture for computing over 7,700 time-series features and a suite of analysis and visualization algorithms to automatically select useful and interpretable time-series features for a given application. Using exemplar applications to high-throughput phenotyping experiments, we show how hctsa allows researchers to leverage decades of time-series research to quantify and understand informative structure in time-series data.},
  langid = {english},
  keywords = {high-throughput phenotyping,time-series analysis},
  file = {/Users/trenthenderson/Zotero/storage/IIQYJJNE/Fulcher and Jones - 2017 - hctsa A Computational Framework for Automated Tim.pdf;/Users/trenthenderson/Zotero/storage/LZYPEED6/S2405471217304386.html}
}

@article{fulcherHighlyComparativeFeaturebased2014,
  title = {Highly Comparative Feature-Based Time-Series Classification},
  author = {Fulcher, Ben D. and Jones, Nick S.},
  year = {2014},
  month = dec,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {26},
  number = {12},
  eprint = {1401.3531},
  eprinttype = {arxiv},
  pages = {3026--3037},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2014.2316504},
  abstract = {A highly comparative, feature-based approach to time series classification is introduced that uses an extensive database of algorithms to extract thousands of interpretable features from time series. These features are derived from across the scientific time-series analysis literature, and include summaries of time series in terms of their correlation structure, distribution, entropy, stationarity, scaling properties, and fits to a range of time-series models. After computing thousands of features for each time series in a training set, those that are most informative of the class structure are selected using greedy forward feature selection with a linear classifier. The resulting feature-based classifiers automatically learn the differences between classes using a reduced number of time-series properties, and circumvent the need to calculate distances between time series. Representing time series in this way results in orders of magnitude of dimensionality reduction, allowing the method to perform well on very large datasets containing long time series or time series of different lengths. For many of the datasets studied, classification performance exceeded that of conventional instance-based classifiers, including one nearest neighbor classifiers using Euclidean distances and dynamic time warping and, most importantly, the features selected provide an understanding of the properties of the dataset, insight that can guide further scientific investigation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Machine Learning,Physics - Data Analysis; Statistics and Probability,Quantitative Biology - Quantitative Methods},
  file = {/Users/trenthenderson/Zotero/storage/T3H4KFCM/Fulcher and Jones - 2014 - Highly comparative feature-based time-series class.pdf;/Users/trenthenderson/Zotero/storage/Y683TMQK/1401.html}
}

@inproceedings{fulcherHighlyComparativeFetal2012,
  title = {Highly Comparative Fetal Heart Rate Analysis},
  booktitle = {2012 {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}},
  author = {Fulcher, B. D. and Georgieva, A. E. and Redman, C. W. G. and Jones, N. S.},
  year = {2012},
  month = aug,
  pages = {3135--3138},
  issn = {1558-4615},
  doi = {10.1109/EMBC.2012.6346629},
  abstract = {A database of fetal heart rate (FHR) time series measured from 7 221 patients during labor is analyzed with the aim of learning the types of features of these recordings that are informative of low cord pH. Our `highly comparative' analysis involves extracting over 9 000 time-series analysis features from each FHR time series, including measures of autocorrelation, entropy, distribution, and various model fits. This diverse collection of features was developed in previous work [1]. We describe five features that most accurately classify a balanced training set of 59 `low pH' and 59 `normal pH' FHR recordings. We then describe five of the features with the strongest linear correlation to cord pH across the full dataset of FHR time series. The features identified in this work may be used as part of a system for guiding intervention during labor in future. This work successfully demonstrates the utility of comparing across a large, interdisciplinary literature on time-series analysis to automatically contribute new scientific results for specific biomedical signal processing challenges.},
  keywords = {Correlation,Entropy,Fetal heart rate,Pediatrics,Time measurement,Time series analysis,Training},
  file = {/Users/trenthenderson/Zotero/storage/NHLYPYEL/Fulcher et al. - 2012 - Highly comparative fetal heart rate analysis.pdf;/Users/trenthenderson/Zotero/storage/G8SZZQIA/6346629.html}
}

@article{fulcherHighlyComparativeTimeseries2013,
  title = {Highly Comparative Time-Series Analysis: The Empirical Structure of Time Series and Their Methods},
  shorttitle = {Highly Comparative Time-Series Analysis},
  author = {Fulcher, Ben D. and Little, Max A. and Jones, Nick S.},
  year = {2013},
  month = jun,
  journal = {Journal of The Royal Society Interface},
  volume = {10},
  number = {83},
  pages = {20130048},
  publisher = {{Royal Society}},
  doi = {10.1098/rsif.2013.0048},
  abstract = {The process of collecting and organizing sets of observations represents a common theme throughout the history of science. However, despite the ubiquity of scientists measuring, recording and analysing the dynamics of different processes, an extensive organization of scientific time-series data and analysis methods has never been performed. Addressing this, annotated collections of over 35 000 real-world and model-generated time series, and over 9000 time-series analysis algorithms are analysed in this work. We introduce reduced representations of both time series, in terms of their properties measured by diverse scientific methods, and of time-series analysis methods, in terms of their behaviour on empirical time series, and use them to organize these interdisciplinary resources. This new approach to comparing across diverse scientific data and methods allows us to organize time-series datasets automatically according to their properties, retrieve alternatives to particular analysis methods developed in other scientific disciplines and automate the selection of useful methods for time-series classification and regression tasks. The broad scientific utility of these tools is demonstrated on datasets of electroencephalograms, self-affine time series, heartbeat intervals, speech signals and others, in each case contributing novel analysis techniques to the existing literature. Highly comparative techniques that compare across an interdisciplinary literature can thus be used to guide more focused research in time-series analysis for applications across the scientific disciplines.},
  keywords = {longitudinal data analysis,signal processing,time-series analysis,time-series classification,time-series regression},
  file = {/Users/trenthenderson/Zotero/storage/ESGEKUUI/Fulcher et al. - 2013 - Highly comparative time-series analysis the empir.pdf}
}

@article{fulcherSelforganizingLivingLibrary2020,
  title = {A Self-Organizing, Living Library of Time-Series Data},
  author = {Fulcher, Ben D. and Lubba, Carl H. and Sethi, Sarab S. and Jones, Nick S.},
  year = {2020},
  month = jul,
  journal = {Scientific Data},
  volume = {7},
  number = {1},
  pages = {213},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-020-0553-0},
  abstract = {Time-series data are measured across the sciences, from astronomy to biomedicine, but meaningful cross-disciplinary interactions are limited by the challenge of identifying fruitful connections. Here we introduce the web platform, CompEngine, a self-organizing, living library of time-series data, that lowers the barrier to forming meaningful interdisciplinary connections between time series. Using a canonical feature-based representation, CompEngine places all time series in a common feature space, regardless of their origin, allowing users to upload their data and immediately explore diverse data with similar properties, and be alerted when similar data is uploaded in future. In contrast to conventional databases which are organized by assigned metadata, CompEngine incentivizes data sharing by automatically connecting experimental and theoretical scientists across disciplines based on the empirical structure of the data they measure. CompEngine's growing library of interdisciplinary time-series data also enables the comprehensive characterization of time-series analysis algorithms across diverse types of empirical data.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Data mining,Databases},
  file = {/Users/trenthenderson/Zotero/storage/TYZCFAGJ/Fulcher et al. - 2020 - A self-organizing, living library of time-series d.pdf}
}

@article{gengCODYEnablesQuantitatively2021,
  title = {{{CODY}} Enables Quantitatively Spatiotemporal Predictions on in Vivo Gut Microbial Variability Induced by Diet Intervention},
  author = {Geng, Jun and Ji, Boyang and Li, Gang and {L{\'o}pez-Isunza}, Felipe and Nielsen, Jens},
  year = {2021},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {13},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2019336118},
  abstract = {Microbial variations in the human gut are harbored in temporal and spatial heterogeneity, and quantitative prediction of spatiotemporal dynamic changes in the gut microbiota is imperative for development of tailored microbiome-directed therapeutics treatments, e.g. precision nutrition. Given the high-degree complexity of microbial variations, subject to the dynamic interactions among host, microbial, and environmental factors, identifying how microbiota colonize in the gut represents an important challenge. Here we present COmputing the DYnamics of microbiota (CODY), a multiscale framework that integrates species-level modeling of microbial dynamics and ecosystem-level interactions into a mathematical model that characterizes spatial-specific in vivo microbial residence in the colon as impacted by host physiology. The framework quantifies spatiotemporal resolution of microbial variations on species-level abundance profiles across site-specific colon regions and in feces, independent of a priori knowledge. We demonstrated the effectiveness of CODY using cross-sectional data from two longitudinal metagenomics studies\textemdash the microbiota development during early infancy and during short-term diet intervention of obese adults. For each cohort, CODY correctly predicts the microbial variations in response to diet intervention, as validated by available metagenomics and metabolomics data. Model simulations provide insight into the biogeographical heterogeneity among lumen, mucus, and feces, which provides insight into how host physical forces and spatial structure are shaping microbial structure and functionality.},
  chapter = {Biological Sciences},
  copyright = {Copyright \textcopyright{} 2021 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
  langid = {english},
  pmid = {33753486},
  keywords = {gastrointestinal,gut microbiota,systems biology},
  file = {/Users/trenthenderson/Zotero/storage/YGF2ULXT/Geng et al. - 2021 - CODY enables quantitatively spatiotemporal predict.pdf;/Users/trenthenderson/Zotero/storage/PLH7DDTF/e2019336118.html}
}

@inproceedings{hendersonEmpiricalEvaluationTimeSeries2021,
  title = {An {{Empirical Evaluation}} of {{Time-Series Feature Sets}}},
  booktitle = {2021 {{International Conference}} on {{Data Mining Workshops}} ({{ICDMW}})},
  author = {Henderson, Trent and Fulcher, Ben D.},
  year = {2021},
  month = dec,
  pages = {1032--1038},
  issn = {2375-9259},
  doi = {10.1109/ICDMW53433.2021.00134},
  abstract = {Solving time-series problems using informative features has been rising in popularity due to the availability of numerous software packages for time-series feature extraction. Feature-based time-series analysis can now be performed using any one of a range of time-series feature sets, including hctsa (7730 features: Matlab), feasts (42 features: R), tsfeatures (63 features: R), Kats (40 features: Python), tsfresh (up to 1558 features: Python), TSFEL (390 features: Python), and the C-coded catch22 (22 features, able to be run from Matlab, R, Python, and Julia). There is substantial overlap in the types of time-series analysis methods included in these feature sets (including properties of the autocorrelation function and Fourier power spectrum, and distributional shape statistics), but they are yet to be systematically compared. Here we compare these seven feature sets on their computational speed, assess the redundancy of features contained in each set, and evaluate the overlap and redundancy across different feature sets. We take an empirical approach to measuring feature similarity, based on the similarity of their outputs across a diverse set of real-world and model-simulated time series. We find that feature sets vary across approximately three orders of magnitude in their computation time per feature on a laptop for a 1000-sample time series, from the fastest feature sets catch22 and TSFEL ({$\sim$} 0.1 ms per feature) to tsfeatures ({$\sim$} 3 s per feature). Using PCA to evaluate feature redundancy within each set, we find the highest within-set redundancy for TSFEL and tsfresh. For example, in TSFEL, 90\% of the variance across 390 features can be captured with just four principal components. Finally, we introduce a metric for quantifying overlap between pairs of feature sets, which indicates substantial overlap between the feature sets. We found that the largest feature set, hctsa, is the most comprehensive, and that tsfresh is the most distinctive, due to its incorporation of large numbers of Fourier coefficients that are summarized at higher levels in the other sets. Our results provide empirical understanding of the differences between existing feature sets, information that can be used to better understand and tailor feature sets to their applications.},
  keywords = {Feature extraction,Portable computers,Redundancy,Shape,Software packages,Time measurement,Time series analysis,time-series analysis,time-series features},
  file = {/Users/trenthenderson/Zotero/storage/NBLE249M/Henderson and Fulcher - 2021 - An Empirical Evaluation of Time-Series Feature Set.pdf;/Users/trenthenderson/Zotero/storage/8IWA5TN3/9679937.html}
}

@misc{HendersontrentRcatch22V0,
  title = {Hendersontrent/{{Rcatch22}}: V0.1.12 | {{Zenodo}}},
  howpublished = {https://zenodo.org/record/4851882/export/csl\#.YQocY1MzZb8},
  file = {/Users/trenthenderson/Zotero/storage/HQYTBVGN/csl.html}
}

@article{jinBayesianSymbolicRegression2020,
  title = {Bayesian Symbolic Regression},
  author = {Jin, Ying and Fu, Weilin and Kang, Jian and Guo, Jiadong and Guo, Jian},
  year = {2020},
  month = jan,
  journal = {arXiv:1910.08892 [stat]},
  eprint = {1910.08892},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Interpretability is crucial for machine learning in many scenarios such as quantitative finance, banking, healthcare, etc. Symbolic regression (SR) is a classic interpretable machine learning method by bridging X and Y using mathematical expressions composed of some basic functions. However, the search space of all possible expressions grows exponentially with the length of the expression, making it infeasible for enumeration. Genetic programming (GP) has been traditionally and commonly used in SR to search for the optimal solution, but it suffers from several limitations, e.g. the difficulty in incorporating prior knowledge; overly-complicated output expression and reduced interpretability etc. To address these issues, we propose a new method to fit SR under a Bayesian framework. Firstly, Bayesian model can naturally incorporate prior knowledge (e.g., preference of basis functions, operators and raw features) to improve the efficiency of fitting SR. Secondly, to improve interpretability of expressions in SR, we aim to capture concise but informative signals. To this end, we assume the expected signal has an additive structure, i.e., a linear combination of several concise expressions, whose complexity is controlled by a well-designed prior distribution. In our setup, each expression is characterized by a symbolic tree, and the proposed SR model could be solved by sampling symbolic trees from the posterior distribution using an efficient Markov chain Monte Carlo (MCMC) algorithm. Finally, compared with GP, the proposed BSR(Bayesian Symbolic Regression) method saves computer memory with no need to keep an updated 'genome pool'. Numerical experiments show that, compared with GP, the solutions of BSR are closer to the ground truth and the expressions are more concise. Meanwhile we find the solution of BSR is robust to hyper-parameter specifications such as the number of trees.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/Users/trenthenderson/Zotero/storage/M8YERK9S/Jin et al. - 2020 - Bayesian Symbolic Regression.pdf;/Users/trenthenderson/Zotero/storage/53SSV9UJ/1910.html}
}

@article{johnsonHierarchicalClusteringSchemes1967,
  title = {Hierarchical Clustering Schemes},
  author = {Johnson, Stephen C.},
  year = {1967},
  month = sep,
  journal = {Psychometrika},
  volume = {32},
  number = {3},
  pages = {241--254},
  issn = {1860-0980},
  doi = {10.1007/BF02289588},
  abstract = {Techniques for partitioning objects into optimally homogeneous groups on the basis of empirical measures of similarity among those objects have received increasing attention in several different fields. This paper develops a useful correspondence between any hierarchical system of such clusters, and a particular type of distance measure. The correspondence gives rise to two methods of clustering that are computationally rapid and invariant under monotonic transformations of the data. In an explicitly defined sense, one method forms clusters that are optimally ``connected,'' while the other forms clusters that are optimally ``compact.''},
  langid = {english}
}

@article{jolliffePrincipalComponentAnalysis2016,
  title = {Principal Component Analysis: A Review and Recent Developments},
  shorttitle = {Principal Component Analysis},
  author = {Jolliffe, Ian T. and Cadima, Jorge},
  year = {2016},
  month = apr,
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {374},
  number = {2065},
  pages = {20150202},
  publisher = {{Royal Society}},
  doi = {10.1098/rsta.2015.0202},
  abstract = {Large datasets are increasingly common and are often difficult to interpret. Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance. Finding such new variables, the principal components, reduces to solving an eigenvalue/eigenvector problem, and the new variables are defined by the dataset at hand, not a priori, hence making PCA an adaptive data analysis technique. It is adaptive in another sense too, since variants of the technique have been developed that are tailored to various different data types and structures. This article will begin by introducing the basic ideas of PCA, discussing what it can and cannot do. It will then describe some variants of PCA and their application.},
  keywords = {dimension reduction,eigenvectors,multivariate analysis,palaeontology},
  file = {/Users/trenthenderson/Zotero/storage/8LFWB48G/Jolliffe and Cadima - 2016 - Principal component analysis a review and recent .pdf}
}

@article{kaoPredictionRemainingTime,
author = {Kao, Ling-Jing and Chiu, Chih-Chou and Wang, Hung-Jui and Ko, Chang Yu},
title = {Prediction of remaining time on site for e-commerce users: A {SOM} and long short-term memory study},
journal = {Journal of Forecasting},
volume = {40},
number = {7},
pages = {1274-1290},
keywords = {browsing behavior, dwell time, time on site, self-organizing map, long short-term memory},
doi = {https://doi.org/10.1002/for.2771},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.2771},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/for.2771},
abstract = {Abstract With the development of information technology, online transactions and e-commerce are gradually replacing conventional consumption patterns. To obtain a competitive advantage, industries proactively engage in digital transformations and the management of e-commerce platforms. Faced with changes in market patterns, e-commerce channels and online advertising firms hope to extend users' website browsing duration/time on site to enhance the effects of product promotion and the likelihood of advertisement clicks. The greatest challenge in predicting time on site is that clickstream data are not mutually independent, and short-, mid-, and long-term data may intervene in a time series. Such timing dependence increases difficulty of capturing or learning the characteristics of website users for ordinary prediction models and leads to confusion and deviation during model construction. Accordingly, this study proposed a prediction method integrating self-organizing map (SOM) and long short-term memory (LSTM). The SOM method was initially applied to categorize website members into groups based on similarities in browsing behavior, and the LSTM prediction model was subsequently developed using the webpage browsing data of each group. The performance of the proposed method is evaluated by comparing the prediction with the results of three competing approaches (SOM with support vector regression, SOM with multilayer perceptron, and single LSTM) on the clickstream data provided by a leading online retailer specializing in selling skin care and cosmetics products in Taiwan. The Wilcoxon signed-rank test validated the proposed SOM-LSTM model outperforms competing approaches in remaining time-on-site prediction. This study serves as a first attempt to systematically predict remaining time on site for e-commerce users in terms of empirically verifying a hybrid approach which integrates SOM and LSTM techniques.},
year = {2021}
}

@article{kodraExploringGrangerCausality2011,
  title = {Exploring {{Granger}} Causality between Global Average Observed Time Series of Carbon Dioxide and Temperature},
  author = {Kodra, Evan and Chatterjee, Snigdhansu and Ganguly, Auroop R.},
  year = {2011},
  month = jun,
  journal = {Theoretical and Applied Climatology},
  volume = {104},
  number = {3},
  pages = {325--335},
  issn = {1434-4483},
  doi = {10.1007/s00704-010-0342-3},
  abstract = {Detection and attribution methodologies have been developed over the years to delineate anthropogenic from natural drivers of climate change and impacts. A majority of prior attribution studies, which have used climate model simulations and observations or reanalysis datasets, have found evidence for human-induced climate change. This papers tests the hypothesis that Granger causality can be extracted from the bivariate series of globally averaged land surface temperature (GT) observations and observed CO2 in the atmosphere using a reverse cumulative Granger causality test. This proposed extension of the classic Granger causality test is better suited to handle the multisource nature of the data and provides further statistical rigor. The results from this modified test show evidence for Granger causality from a proxy of total radiative forcing (RC), which in this case is a transformation of atmospheric CO2, to GT. Prior literature failed to extract these results via the standard Granger causality test. A forecasting test shows that a holdout set of GT can be better predicted with the addition of lagged RC as a predictor, lending further credibility to the Granger test results. However, since second-order-differenced RC is neither normally distributed nor variance stationary, caution should be exercised in the interpretation of our results.}
}

@article{kodraExploringGrangerCausality2011a,
  title = {Exploring {{Granger}} Causality between Global Average Observed Time Series of Carbon Dioxide and Temperature},
  author = {Kodra, Evan and Chatterjee, Snigdhansu and Ganguly, Auroop R.},
  year = {2011},
  month = jun,
  journal = {Theoretical and Applied Climatology},
  volume = {104},
  number = {3},
  pages = {325--335},
  issn = {1434-4483},
  doi = {10.1007/s00704-010-0342-3},
  abstract = {Detection and attribution methodologies have been developed over the years to delineate anthropogenic from natural drivers of climate change and impacts. A majority of prior attribution studies, which have used climate model simulations and observations or reanalysis datasets, have found evidence for human-induced climate change. This papers tests the hypothesis that Granger causality can be extracted from the bivariate series of globally averaged land surface temperature (GT) observations and observed CO2 in the atmosphere using a reverse cumulative Granger causality test. This proposed extension of the classic Granger causality test is better suited to handle the multisource nature of the data and provides further statistical rigor. The results from this modified test show evidence for Granger causality from a proxy of total radiative forcing (RC), which in this case is a transformation of atmospheric CO2, to GT. Prior literature failed to extract these results via the standard Granger causality test. A forecasting test shows that a holdout set of GT can be better predicted with the addition of lagged RC as a predictor, lending further credibility to the Granger test results. However, since second-order-differenced RC is neither normally distributed nor variance stationary, caution should be exercised in the interpretation of our results.}
}

@article{lacavaContemporarySymbolicRegression2021,
  title = {Contemporary {{Symbolic Regression Methods}} and Their {{Relative Performance}}},
  author = {La Cava, William and Orzechowski, Patryk and Burlacu, Bogdan and {de Fran{\c c}a}, Fabr{\'i}cio Olivetti and Virgolin, Marco and Jin, Ying and Kommenda, Michael and Moore, Jason H.},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.14351 [cs]},
  eprint = {2107.14351},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Many promising approaches to symbolic regression have been presented in recent years, yet progress in the field continues to suffer from a lack of uniform, robust, and transparent benchmarking standards. In this paper, we address this shortcoming by introducing an open-source, reproducible benchmarking platform for symbolic regression. We assess 14 symbolic regression methods and 7 machine learning methods on a set of 252 diverse regression problems. Our assessment includes both real-world datasets with no known model form as well as ground-truth benchmark problems, including physics equations and systems of ordinary differential equations. For the real-world datasets, we benchmark the ability of each method to learn models with low error and low complexity relative to state-of-the-art machine learning methods. For the synthetic problems, we assess each method's ability to find exact solutions in the presence of varying levels of noise. Under these controlled experiments, we conclude that the best performing methods for real-world regression combine genetic algorithms with parameter estimation and/or semantic search drivers. When tasked with recovering exact equations in the presence of noise, we find that deep learning and genetic algorithm-based approaches perform similarly. We provide a detailed guide to reproducing this experiment and contributing new methods, and encourage other researchers to collaborate with us on a common and living symbolic regression benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/trenthenderson/Zotero/storage/L5FW9BIH/La Cava et al. - 2021 - Contemporary Symbolic Regression Methods and their.pdf;/Users/trenthenderson/Zotero/storage/LSJSR7F3/2107.html}
}

@article{lemosRediscoveringOrbitalMechanics2022,
  title = {Rediscovering Orbital Mechanics with Machine Learning},
  author = {Lemos, Pablo and Jeffrey, Niall and Cranmer, Miles and Ho, Shirley and Battaglia, Peter},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.02306 [astro-ph]},
  eprint = {2202.02306},
  eprinttype = {arxiv},
  primaryclass = {astro-ph},
  abstract = {We present an approach for using machine learning to automatically discover the governing equations and hidden properties of real physical systems from observations. We train a "graph neural network" to simulate the dynamics of our solar system's Sun, planets, and large moons from 30 years of trajectory data. We then use symbolic regression to discover an analytical expression for the force law implicitly learned by the neural network, which our results showed is equivalent to Newton's law of gravitation. The key assumptions that were required were translational and rotational equivariance, and Newton's second and third laws of motion. Our approach correctly discovered the form of the symbolic force law. Furthermore, our approach did not require any assumptions about the masses of planets and moons or physical constants. They, too, were accurately inferred through our methods. Though, of course, the classical law of gravitation has been known since Isaac Newton, our result serves as a validation that our method can discover unknown laws and hidden properties from observed data. More broadly this work represents a key step toward realizing the potential of machine learning for accelerating scientific discovery.},
  archiveprefix = {arXiv},
  keywords = {Astrophysics - Earth and Planetary Astrophysics,Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Machine Learning},
  file = {/Users/trenthenderson/Zotero/storage/QRABLJY2/Lemos et al. - 2022 - Rediscovering orbital mechanics with machine learn.pdf;/Users/trenthenderson/Zotero/storage/6JUF3EM5/2202.html}
}

@article{liuSensorFaultsClassification2020,
  title = {Sensor Faults Classification for {{SHM}} Systems Using Deep Learning-Based Method with {{Tsfresh}} Features},
  author = {Liu, Gang and Li, Lili and Zhang, Liangliang and Li, Qing and Law, S. S.},
  year = {2020},
  month = may,
  journal = {Smart Materials and Structures},
  volume = {29},
  number = {7},
  pages = {075005},
  publisher = {{IOP Publishing}},
  issn = {0964-1726},
  doi = {10.1088/1361-665X/ab85a6},
  abstract = {Sensors have been installed on many civil infrastructures to monitor structural conditions. False alarm will be triggered, however, by the malfunctioned sensor even under normal conditions. Therefore, pre-processing is needed to identify and classify the sensor fault before structural damage detection and assessment. This paper proposes a deep learning-based method, namely, the Tsfresh Long Short-Term Memory networks (TLSTM), to address the sensor fault classification. The python package Tsfresh is used to extract features that are sensitive to sensor fault from measured signals. These features are further selected with the Benjamini\textendash Yekutieli procedure. With the selected features, a long short-term memory (LSTM) network combining two fully-connected layers and a Softmax layer is constructed to differentiate sensor fault types. Experimental data with five types of sensor faults are obtained by mechanical and electrical simulation. The proposed method is shown able to successfully classify all these sensor fault types.},
  langid = {english}
}

@inproceedings{loveardRepresentingClassificationProblems2001,
  title = {Representing Classification Problems in Genetic Programming},
  booktitle = {In {{Proceedings}} of the 2001 Congress on Evolutionary Computation, {{Seoul}}, {{Seoul}}, {{Korea}}},
  author = {Loveard, Thomas and Ciesielski, Victor},
  year = {2001},
  pages = {1070--1077},
  abstract = {Abstract-In this paper five alternative methods are proposed to perform multi-class classification tasks using genetic programming. These methods are: Binary decomposition, in which the problem is decomposed into a set of binary problems and standard genetic programming methods are applied; Static range selection, where the set of real values returned by a genetic program is divided into class boundaries using arbitrarily chosen division points; Dynamic range selection in which a subset of training examples are used to determine where, over the set of reals, class boundaries lie; Class enumeration which constructs programs similar in syntactic structure to a decision tree; and evidence accumulation which allows separate branches of the program to add to the certainty of any given class. Results showed that the dynamic range selection method was well suited to the task of multi-class classification and was capable of producing classifiers more accurate than the other methods tried when comparable training times were allowed. Accuracy of the generated classifiers was comparable to alternative approaches over several datasets.},
  file = {/Users/trenthenderson/Zotero/storage/63ZU8QY5/Loveard and Ciesielski - 2001 - Representing classification problems in genetic pr.pdf;/Users/trenthenderson/Zotero/storage/EGHV9MD8/download.html}
}

@article{lubbaCatch22CAnonicalTimeseries2019,
  title = {Catch22: {{CAnonical Time-series CHaracteristics}}},
  author = {Lubba, Carl H. and Sethi, Sarab S. and Knaute, Philip and Schultz, Simon R. and Fulcher, Ben D. and Jones, Nick S.},
  year = {2019},
  month = nov,
  journal = {Data Mining and Knowledge Discovery},
  volume = {33},
  number = {6},
  pages = {1821--1852},
  issn = {1573-756X},
  doi = {10.1007/s10618-019-00647-x},
  abstract = {Capturing the dynamical properties of time series concisely as interpretable feature vectors can enable efficient clustering and classification for time-series applications across science and industry. Selecting an appropriate feature-based representation of time series for a given application can be achieved through systematic comparison across a comprehensive time-series feature library, such as those in the hctsa toolbox. However, this approach is computationally expensive and involves evaluating many similar features, limiting the widespread adoption of feature-based representations of time series for real-world applications. In this work, we introduce a method to infer small sets of time-series features that (i) exhibit strong classification performance across a given collection of time-series problems, and (ii) are minimally redundant. Applying our method to a set of 93 time-series classification datasets (containing over 147,000 time series) and using a filtered version of the hctsa feature library (4791 features), we introduce a set of 22 CAnonical Time-series CHaracteristics, catch22, tailored to the dynamics typically encountered in time-series data-mining tasks. This dimensionality reduction, from 4791 to 22, is associated with an approximately 1000-fold reduction in computation time and near linear scaling with time-series length, despite an average reduction in classification accuracy of just 7\%. catch22 captures a diverse and interpretable signature of time series in terms of their properties, including linear and non-linear autocorrelation, successive differences, value distributions and outliers, and fluctuation scaling properties. We provide an efficient implementation of catch22, accessible from many programming environments, that facilitates feature-based time-series analysis for scientific, industrial, financial and medical applications using a common language of interpretable time-series properties.}
}


@article{maatenVisualizingDataUsing2008,
  title = {Visualizing {{Data}} Using T-{{SNE}}},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {86},
  pages = {2579--2605},
  issn = {1533-7928},
  abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images ofobjects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
  file = {/Users/trenthenderson/Zotero/storage/9XT75SN3/Maaten and Hinton - 2008 - Visualizing Data using t-SNE.pdf}
}

@article{markicevicCorticalExcitationInhibition2020,
  title = {Cortical {{Excitation}}:{{Inhibition Imbalance Causes Abnormal Brain Network Dynamics}} as {{Observed}} in {{Neurodevelopmental Disorders}}},
  author = {Markicevic, Marija and Fulcher, Ben D and Lewis, Christopher and Helmchen, Fritjof and Rudin, Markus and Zerbi, Valerio and Wenderoth, Nicole},
  year = {2020},
  journal = {Cerebral Cortex},
  volume = {30},
  number = {9},
  pages = {4922--4937},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhaa084},
  abstract = {Abnormal brain development manifests itself at different spatial scales. However, whether abnormalities at the cellular level can be diagnosed from network activity measured with functional magnetic resonance imaging (fMRI) is largely unknown, yet of high clinical relevance. Here a putative mechanism reported in neurodevelopmental disorders, that is, excitation-to-inhibition ratio (E:I), was chemogenetically increased within cortical microcircuits of the mouse brain and measured via fMRI. Increased E:I caused a significant ``reduction'' of long-range connectivity, irrespective of whether excitatory neurons were facilitated or inhibitory Parvalbumin (PV) interneurons were suppressed. Training a classifier on fMRI signals, we were able to accurately classify cortical areas exhibiting increased E:I. This classifier was validated in an independent cohort of Fmr1y/- knockout mice, a model for autism with well-documented loss of parvalbumin neurons and chronic alterations of E:I. Our findings demonstrate a promising novel approach towards inferring microcircuit abnormalities from macroscopic fMRI measurements.},
  annotation = {\_eprint: https://academic.oup.com/cercor/article-pdf/30/9/4922/33547031/bhaa084.pdf}
}

@article{markicevicCorticalExcitationInhibition2020a,
  title = {Cortical {{Excitation}}:{{Inhibition Imbalance Causes Abnormal Brain Network Dynamics}} as {{Observed}} in {{Neurodevelopmental Disorders}}},
  shorttitle = {Cortical {{Excitation}}},
  author = {Markicevic, Marija and Fulcher, Ben D and Lewis, Christopher and Helmchen, Fritjof and Rudin, Markus and Zerbi, Valerio and Wenderoth, Nicole},
  year = {2020},
  month = jul,
  journal = {Cerebral Cortex},
  volume = {30},
  number = {9},
  pages = {4922--4937},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhaa084},
  abstract = {Abnormal brain development manifests itself at different spatial scales. However, whether abnormalities at the cellular level can be diagnosed from network activity measured with functional magnetic resonance imaging (fMRI) is largely unknown, yet of high clinical relevance. Here a putative mechanism reported in neurodevelopmental disorders, that is, excitation-to-inhibition ratio (E:I), was chemogenetically increased within cortical microcircuits of the mouse brain and measured via fMRI. Increased E:I caused a significant ``reduction'' of long-range connectivity, irrespective of whether excitatory neurons were facilitated or inhibitory Parvalbumin (PV) interneurons were suppressed. Training a classifier on fMRI signals, we were able to accurately classify cortical areas exhibiting increased E:I. This classifier was validated in an independent cohort of Fmr1y/- knockout mice, a model for autism with well-documented loss of parvalbumin neurons and chronic alterations of E:I. Our findings demonstrate a promising novel approach towards inferring microcircuit abnormalities from macroscopic fMRI measurements.},
  file = {/Users/trenthenderson/Zotero/storage/66X5KIW7/Markicevic et al. - 2020 - Cortical ExcitationInhibition Imbalance Causes Ab.pdf}
}

@article{markicevicCorticalExcitationInhibition2020b,
  title = {Cortical {{Excitation}}:{{Inhibition Imbalance Causes Abnormal Brain Network Dynamics}} as {{Observed}} in {{Neurodevelopmental Disorders}}},
  author = {Markicevic, Marija and Fulcher, Ben D and Lewis, Christopher and Helmchen, Fritjof and Rudin, Markus and Zerbi, Valerio and Wenderoth, Nicole},
  year = {2020},
  month = apr,
  journal = {Cerebral Cortex},
  volume = {30},
  number = {9},
  pages = {4922--4937},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhaa084},
  abstract = {Abnormal brain development manifests itself at different spatial scales. However, whether abnormalities at the cellular level can be diagnosed from network activity measured with functional magnetic resonance imaging (fMRI) is largely unknown, yet of high clinical relevance. Here a putative mechanism reported in neurodevelopmental disorders, that is, excitation-to-inhibition ratio (E:I), was chemogenetically increased within cortical microcircuits of the mouse brain and measured via fMRI. Increased E:I caused a significant ``reduction'' of long-range connectivity, irrespective of whether excitatory neurons were facilitated or inhibitory Parvalbumin (PV) interneurons were suppressed. Training a classifier on fMRI signals, we were able to accurately classify cortical areas exhibiting increased E:I. This classifier was validated in an independent cohort of Fmr1y/- knockout mice, a model for autism with well-documented loss of parvalbumin neurons and chronic alterations of E:I. Our findings demonstrate a promising novel approach towards inferring microcircuit abnormalities from macroscopic fMRI measurements.},
  annotation = {\_eprint: https://academic.oup.com/cercor/article-pdf/30/9/4922/33547031/bhaa084.pdf}
}

@article{markicevicCorticalExcitationInhibition2020c,
  title = {Cortical {{Excitation}}:{{Inhibition Imbalance Causes Abnormal Brain Network Dynamics}} as {{Observed}} in {{Neurodevelopmental Disorders}}},
  shorttitle = {Cortical {{Excitation}}},
  author = {Markicevic, Marija and Fulcher, Ben D and Lewis, Christopher and Helmchen, Fritjof and Rudin, Markus and Zerbi, Valerio and Wenderoth, Nicole},
  year = {2020},
  month = jul,
  journal = {Cerebral Cortex},
  volume = {30},
  number = {9},
  pages = {4922--4937},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhaa084},
  abstract = {Abnormal brain development manifests itself at different spatial scales. However, whether abnormalities at the cellular level can be diagnosed from network activity measured with functional magnetic resonance imaging (fMRI) is largely unknown, yet of high clinical relevance. Here a putative mechanism reported in neurodevelopmental disorders, that is, excitation-to-inhibition ratio (E:I), was chemogenetically increased within cortical microcircuits of the mouse brain and measured via fMRI. Increased E:I caused a significant ``reduction'' of long-range connectivity, irrespective of whether excitatory neurons were facilitated or inhibitory Parvalbumin (PV) interneurons were suppressed. Training a classifier on fMRI signals, we were able to accurately classify cortical areas exhibiting increased E:I. This classifier was validated in an independent cohort of Fmr1y/- knockout mice, a model for autism with well-documented loss of parvalbumin neurons and chronic alterations of E:I. Our findings demonstrate a promising novel approach towards inferring microcircuit abnormalities from macroscopic fMRI measurements.},
  file = {/Users/trenthenderson/Zotero/storage/6RRW569L/Markicevic et al. - 2020 - Cortical ExcitationInhibition Imbalance Causes Ab.pdf}
}

@misc{markicevicNeuromodulationStriatalD12022,
  title = {Neuromodulation of Striatal {{D1}} Cells Shapes {{BOLD}} Fluctuations in Anatomically Connected Thalamic and Cortical Regions},
  author = {Markicevic, Marija and Sturman, Oliver and Bohacek, Johannes and Rudin, Markus and Zerbi, Valerio and Fulcher, Ben D. and Wenderoth, Nicole},
  year = {2022},
  month = mar,
  pages = {2022.03.11.483972},
  institution = {{bioRxiv}},
  doi = {10.1101/2022.03.11.483972},
  abstract = {Understanding how the brain's macroscale dynamics are shaped by underlying microscale mechanisms is a key problem in neuroscience. In animal models, we can now investigate this relationship in unprecedented detail by directly manipulating cellular-level properties while measuring the whole-brain response using resting-state fMRI. Here we focused on understanding how blood-oxygen-level-dependent (BOLD) dynamics, measured within a structurally well-defined striato-thalamo-cortical circuit, are shaped by chemogenetically exciting or inhibiting D1 medium spiny neurons (MSNs) of the right dorsomedial striatum (CPdm). We characterize changes in both the BOLD dynamics of individual cortical and subcortical brain areas, and patterns of inter-regional coupling (functional connectivity) between pairs of areas. Using a classification approach based on a large and diverse set of time-series properties, we found that CPdm neuromodulation alters BOLD dynamics within thalamic subregions that project back to dorsomedial striatum. In the cortex, the strongest changes in local dynamics were observed in unimodal regions, i.e., regions that process information from a single sensory modality, while changes in the local dynamics weakened along a putative cortical hierarchical gradient towards transmodal regions. In contrast, a decrease in functional connectivity was observed only for cortico-striatal connections after D1 excitation. Our results provide a comprehensive understanding of how targeted cellular-level manipulations affect local BOLD dynamics at the macroscale, including the role of a circuit's structural characteristics and hierarchical cortical level in shaping those dynamics. These findings contribute to ongoing attempts to understand the influence of structure\textendash function relationships in shaping inter-regional communication at subcortical and cortical levels.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/HI3L98FX/Markicevic et al. - 2022 - Neuromodulation of striatal D1 cells shapes BOLD f.pdf;/Users/trenthenderson/Zotero/storage/LNAYZP9D/2022.03.11.html}
}

@article{montero-mansoFFORMAFeaturebasedForecast2020,
  title = {{{FFORMA}}: {{Feature-based}} Forecast Model Averaging},
  shorttitle = {{{FFORMA}}},
  author = {{Montero-Manso}, Pablo and Athanasopoulos, George and Hyndman, Rob J. and Talagala, Thiyanga S.},
  year = {2020},
  month = jan,
  journal = {International Journal of Forecasting},
  series = {M4 {{Competition}}},
  volume = {36},
  number = {1},
  pages = {86--92},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2019.02.011},
  abstract = {We propose an automated method for obtaining weighted forecast combinations using time series features. The proposed approach involves two phases. First, we use a collection of time series to train a meta-model for assigning weights to various possible forecasting methods with the goal of minimizing the average forecasting loss obtained from a weighted forecast combination. The inputs to the meta-model are features that are extracted from each series. Then, in the second phase, we forecast new series using a weighted forecast combination, where the weights are obtained from our previously trained meta-model. Our method outperforms a simple forecast combination, as well as all of the most popular individual methods in the time series forecasting literature. The approach achieved second position in the M4 competition.},
  langid = {english},
  keywords = {Forecast combination,M4 competition,Meta-learning,Time series features,XGBoost},
  file = {/Users/trenthenderson/Zotero/storage/NXUI49ZC/S0169207019300895.html}
}

@inproceedings{ojalaPermutationTestsStudying2009,
  title = {Permutation {{Tests}} for {{Studying Classifier Performance}}},
  booktitle = {2009 {{Ninth IEEE International Conference}} on {{Data Mining}}},
  author = {Ojala, Markus and Garriga, Gemma C.},
  year = {2009},
  month = dec,
  pages = {908--913},
  publisher = {{IEEE}},
  address = {{Miami Beach, FL, USA}},
  doi = {10.1109/ICDM.2009.108},
  abstract = {We explore the framework of permutation-based p-values for assessing the performance of classifiers. In this paper we study two simple permutation tests. The first test assess whether the classifier has found a real class structure in the data; the corresponding null distribution is estimated by permuting the labels in the data. This test has been used extensively in classification problems in computational biology. The second test studies whether the classifier is exploiting the dependency between the features in classification; the corresponding null distribution is estimated by permuting the features within classes, inspired by restricted randomization techniques traditionally used in statistics. This new test can serve to identify descriptive features which can be valuable information in improving the classifier performance. We study the properties of these tests and present an extensive empirical evaluation on real and synthetic data. Our analysis shows that studying the classifier performance via permutation tests is effective. In particular, the restricted permutation test clearly reveals whether the classifier exploits the interdependency between the features in the data.},
  isbn = {978-1-4244-5242-2},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/CR8DZANP/Ojala and Garriga - 2009 - Permutation Tests for Studying Classifier Performa.pdf}
}

@article{ohara-wildFeastsFeatureExtraction,
  title = {Feasts: {{Feature Extraction}} and {{Statistics}} for {{Time Series}}},
  author = {{O'Hara-Wild}, Mitchell and Hyndman, Rob and Wang, Earo},
  pages = {33},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/YM33F2FR/O'Hara-Wild et al. - feasts Feature Extraction and Statistics for Time.pdf}
}

@article{oizumiPhenomenologyMechanismsConsciousness2014,
  title = {From the {{Phenomenology}} to the {{Mechanisms}} of {{Consciousness}}: {{Integrated Information Theory}} 3.0},
  shorttitle = {From the {{Phenomenology}} to the {{Mechanisms}} of {{Consciousness}}},
  author = {Oizumi, Masafumi and Albantakis, Larissa and Tononi, Giulio},
  year = {2014},
  month = may,
  journal = {PLOS Computational Biology},
  volume = {10},
  number = {5},
  pages = {e1003588},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003588},
  abstract = {This paper presents Integrated Information Theory (IIT) of consciousness 3.0, which incorporates several advances over previous formulations. IIT starts from phenomenological axioms: information says that each experience is specific \textendash{} it is what it is by how it differs from alternative experiences; integration says that it is unified \textendash{} irreducible to non-interdependent components; exclusion says that it has unique borders and a particular spatio-temporal grain. These axioms are formalized into postulates that prescribe how physical mechanisms, such as neurons or logic gates, must be configured to generate experience (phenomenology). The postulates are used to define intrinsic information as ``differences that make a difference'' within a system, and integrated information as information specified by a whole that cannot be reduced to that specified by its parts. By applying the postulates both at the level of individual mechanisms and at the level of systems of mechanisms, IIT arrives at an identity: an experience is a maximally irreducible conceptual structure (MICS, a constellation of concepts in qualia space), and the set of elements that generates it constitutes a complex. According to IIT, a MICS specifies the quality of an experience and integrated information {$\Phi$}Max its quantity. From the theory follow several results, including: a system of mechanisms may condense into a major complex and non-overlapping minor complexes; the concepts that specify the quality of an experience are always about the complex itself and relate only indirectly to the external environment; anatomical connectivity influences complexes and associated MICS; a complex can generate a MICS even if its elements are inactive; simple systems can be minimally conscious; complicated systems can be unconscious; there can be true ``zombies'' \textendash{} unconscious feed-forward systems that are functionally equivalent to conscious complexes.},
  langid = {english},
  keywords = {Computer architecture,Consciousness,Logic circuits,Neurons,Photodiodes,Probability distribution,Synapses,Theories of consciousness},
  file = {/Users/trenthenderson/Zotero/storage/YRUI3FBW/Oizumi et al. - 2014 - From the Phenomenology to the Mechanisms of Consci.pdf;/Users/trenthenderson/Zotero/storage/QEB96MY9/article.html}
}

@article{osborneSampleSizeSubject2019,
  title = {Sample Size and Subject to Item Ratio in Principal Components Analysis},
  author = {Osborne, Jason and Costello, Anna},
  year = {2019},
  month = nov,
  journal = {Practical Assessment, Research, and Evaluation},
  volume = {9},
  number = {1},
  issn = {1531-7714},
  doi = {10.7275/ktzq-jq66}
}

@article{palmesTSMLTimeSeries2020,
  title = {{{TSML}} ({{Time Series Machine Learning}})},
  author = {Palmes, Paulito and Ploennigs, Joern and Brady, Niall},
  year = {2020},
  month = aug,
  journal = {Proceedings of the JuliaCon Conferences},
  volume = {1},
  number = {1},
  pages = {51},
  issn = {2642-4029},
  doi = {10.21105/jcon.00051},
  abstract = {Palmes et al., (2020). TSML (Time Series Machine Learning). JuliaCon Proceedings, 1(1), 51, https://doi.org/10.21105/jcon.00051},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/M65DGU95/Palmes et al. - 2020 - TSML (Time Series Machine Learning).pdf;/Users/trenthenderson/Zotero/storage/XG2KQ2M5/jcon.html}
}

@article{paulBehavioralDiscriminationTimeseries2021,
  title = {Behavioral Discrimination and Time-Series Phenotyping of Birdsong Performance},
  author = {Paul, Avishek and McLendon, Helen and Rally, Veronica and Sakata, Jon T. and Woolley, Sarah C.},
  year = {2021},
  month = apr,
  journal = {PLOS Computational Biology},
  volume = {17},
  number = {4},
  pages = {e1008820},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008820},
  abstract = {Variation in the acoustic structure of vocal signals is important to communicate social information. However, relatively little is known about the features that receivers extract to decipher relevant social information. Here, we took an expansive, bottom-up approach to delineate the feature space that could be important for processing social information in zebra finch song. Using operant techniques, we discovered that female zebra finches can consistently discriminate brief song phrases (``motifs'') from different social contexts. We then applied machine learning algorithms to classify motifs based on thousands of time-series features and to uncover acoustic features for motif discrimination. In addition to highlighting classic acoustic features, the resulting algorithm revealed novel features for song discrimination, for example, measures of time irreversibility (i.e., the degree to which the statistical properties of the actual and time-reversed signal differ). Moreover, the algorithm accurately predicted female performance on individual motif exemplars. These data underscore and expand the promise of broad time-series phenotyping to acoustic analyses and social decision-making.},
  langid = {english},
  keywords = {Acoustics,Birds,Decision trees,Machine learning algorithms,Sequence motif analysis,Social discrimination,Syllables,Zebra finch},
  file = {/Users/trenthenderson/Zotero/storage/X9DTMRLY/Paul et al. - 2021 - Behavioral discrimination and time-series phenotyp.pdf}
}

@article{phinyomarkFeatureExtractionFirst2014,
  title = {Feature Extraction of the First Difference of {{EMG}} Time Series for {{EMG}} Pattern Recognition},
  author = {Phinyomark, Angkoon and Quaine, Franck and Charbonnier, Sylvie and Serviere, Christine and {Tarpin-Bernard}, Franck and Laurillau, Yann},
  year = {2014},
  month = nov,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {117},
  number = {2},
  pages = {247--256},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2014.06.013},
  abstract = {This paper demonstrates the utility of a differencing technique to transform surface EMG signals measured during both static and dynamic contractions such that they become more stationary. The technique was evaluated by three stationarity tests consisting of the variation of two statistical properties, i.e., mean and standard deviation, and the reverse arrangements test. As a result of the proposed technique, the first difference of EMG time series became more stationary compared to the original measured signal. Based on this finding, the performance of time-domain features extracted from raw and transformed EMG was investigated via an EMG classification problem (i.e., eight dynamic motions and four EMG channels) on data from 18 subjects. The results show that the classification accuracies of all features extracted from the transformed signals were higher than features extracted from the original signals for six different classifiers including quadratic discriminant analysis. On average, the proposed differencing technique improved classification accuracies by 2\textendash 8\%.},
  langid = {english},
  keywords = {Differencing technique,Dynamic motions,Electromyography (EMG),Muscle‚Äìcomputer interface,Non-stationary signal},
  file = {/Users/trenthenderson/Zotero/storage/Z6FA7QF3/S0169260714002478.html}
}

@article{posadaModelSelectionModel2004,
  title = {Model Selection and Model Averaging in Phylogenetics: Advantages of {Akaike Information Criterion} and {Bayesian} Approaches Over Likelihood Ratio Tests},
  author = {Posada, David and Buckley, Thomas R.},
  year = {2004},
  month = oct,
  journal = {Systematic Biology},
  volume = {53},
  number = {5},
  pages = {793--808},
  issn = {1063-5157},
  doi = {10.1080/10635150490522304},
  abstract = {Model selection is a topic of special relevance in molecular phylogenetics that affects many, if not all, stages of phylogenetic inference. Here we discuss some fundamental concepts and techniques of model selection in the context of phylogenetics. We start by reviewing different aspects of the selection of substitution models in phylogenetics from a theoretical, philosophical and practical point of view, and summarize this comparison in table format. We argue that the most commonly implemented model selection approach, the hierarchical likelihood ratio test, is not the optimal strategy for model selection in phylogenetics, and that approaches like the Akaike Information Criterion (AIC) and Bayesian methods offer important advantages. In particular, the latter two methods are able to simultaneously compare multiple nested or nonnested models, assess model selection uncertainty, and allow for the estimation of phylogenies and model parameters using all available models (model-averaged inference or multimodel inference). We also describe how the relative importance of the different parameters included in substitution models can be depicted. To illustrate some of these points, we have applied AIC-based model averaging to 37 mitochondrial DNA sequences from the subgenus Ohomopterus (genus Carabus) ground beetles described by Sota and Vogler (2001).},
  annotation = {\_eprint: https://academic.oup.com/sysbio/article-pdf/53/5/793/24197659/53-5-793.pdf}
}

@book{PrincipalComponentAnalysis2002,
  title = {Principal {{Component Analysis}}},
  year = {2002},
  author = {Jolliffe, Ian T.},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/b98835},
  isbn = {978-0-387-95442-4},
  langid = {english},
  keywords = {Factor analysis,principal component analysis,Regression analysis,statistics,time series}
}

@article{quadePredictionDynamicalSystems2016,
  title = {Prediction of Dynamical Systems by Symbolic Regression},
  author = {Quade, Markus and Abel, Markus and Shafi, Kamran and Niven, Robert K. and Noack, Bernd R.},
  year = {2016},
  month = jul,
  journal = {Physical Review E},
  volume = {94},
  number = {1},
  pages = {012214},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.94.012214},
  abstract = {We study the modeling and prediction of dynamical systems based on conventional models derived from measurements. Such algorithms are highly desirable in situations where the underlying dynamics are hard to model from physical principles or simplified models need to be found. We focus on symbolic regression methods as a part of machine learning. These algorithms are capable of learning an analytically tractable model from data, a highly valuable property. Symbolic regression methods can be considered as generalized regression methods. We investigate two particular algorithms, the so-called fast function extraction which is a generalized linear regression algorithm, and genetic programming which is a very general method. Both are able to combine functions in a certain way such that a good model for the prediction of the temporal evolution of a dynamical system can be identified. We illustrate the algorithms by finding a prediction for the evolution of a harmonic oscillator based on measurements, by detecting an arriving front in an excitable system, and as a real-world application, the prediction of solar power production based on energy production observations at a given site together with the weather forecast.},
  file = {/Users/trenthenderson/Zotero/storage/NJKQJTYG/Quade et al. - 2016 - Prediction of dynamical systems by symbolic regres.pdf;/Users/trenthenderson/Zotero/storage/GGMLG379/PhysRevE.94.html}
}

@article{rebbapragadaFindingAnomalousPeriodic2009,
  title = {Finding Anomalous Periodic Time Series},
  author = {Rebbapragada, Umaa and Protopapas, Pavlos and Brodley, Carla E. and Alcock, Charles},
  year = {2009},
  month = mar,
  journal = {Machine Learning},
  volume = {74},
  number = {3},
  pages = {281--313},
  issn = {1573-0565},
  doi = {10.1007/s10994-008-5093-3},
  abstract = {Catalogs of periodic variable stars contain large numbers of periodic light-curves (photometric time series data from the astrophysics domain). Separating anomalous objects from well-known classes is an important step towards the discovery of new classes of astronomical objects. Most anomaly detection methods for time series data assume either a single continuous time series or a set of time series whose periods are aligned. Light-curve data precludes the use of these methods as the periods of any given pair of light-curves may be out of sync. One may use an existing anomaly detection method if, prior to similarity calculation, one performs the costly act of aligning two light-curves, an operation that scales poorly to massive data sets. This paper presents PCAD, an unsupervised anomaly detection method for large sets of unsynchronized periodic time-series data, that outputs a ranked list of both global and local anomalies. It calculates its anomaly score for each light-curve in relation to a set of centroids produced by a modified k-means clustering algorithm. Our method is able to scale to large data sets through the use of sampling. We validate our method on both light-curve data and other time series data sets. We demonstrate its effectiveness at finding known anomalies, and discuss the effect of sample size and number of centroids on our results. We compare our method to naive solutions and existing time series anomaly detection methods for unphased data, and show that PCAD's reported anomalies are comparable to or better than all other methods. Finally, astrophysicists on our team have verified that PCAD finds true anomalies that might be indicative of novel astrophysical phenomena.},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/8W8SAZS2/Rebbapragada et al. - 2009 - Finding anomalous periodic time series.pdf}
}

@article{ruizGreatMultivariateTime2021,
  title = {The Great Multivariate Time Series Classification Bake off: A Review and Experimental Evaluation of Recent Algorithmic Advances},
  author = {Ruiz, Alejandro Pasos and Flynn, Michael and Large, James and Middlehurst, Matthew and Bagnall, Anthony},
  year = {2021},
  month = mar,
  journal = {Data Mining and Knowledge Discovery},
  volume = {35},
  number = {2},
  pages = {401--449},
  issn = {1573-756X},
  doi = {10.1007/s10618-020-00727-3},
  abstract = {Time Series Classification (TSC) involves building predictive models for a discrete target variable from ordered, real valued, attributes. Over recent years, a new set of TSC algorithms have been developed which have made significant improvement over the previous state of the art. The main focus has been on univariate TSC, i.e. the problem where each case has a single series and a class label. In reality, it is more common to encounter multivariate TSC (MTSC) problems where the time series for a single case has multiple dimensions. Despite this, much less consideration has been given to MTSC than the univariate case. The UCR archive has provided a valuable resource for univariate TSC, and the lack of a standard set of test problems may explain why there has been less focus on MTSC. The UEA archive of 30 MTSC problems released in 2018 has made comparison of algorithms easier. We review recently proposed bespoke MTSC algorithms based on deep learning, shapelets and bag of words approaches. If an algorithm cannot naturally handle multivariate data, the simplest approach to adapt a univariate classifier to MTSC is to ensemble it over the multivariate dimensions. We compare the bespoke algorithms to these dimension independent approaches on the 26 of the 30 MTSC archive problems where the data are all of equal length. We demonstrate that four classifiers are significantly more accurate than the benchmark dynamic time warping algorithm and that one of these recently proposed classifiers, ROCKET, achieves significant improvement on the archive datasets in at least an order of magnitude less time than the other three.}
}

@article{ruizGreatMultivariateTime2021a,
  title = {The Great Multivariate Time Series Classification Bake off: A Review and Experimental Evaluation of Recent Algorithmic Advances},
  author = {Ruiz, Alejandro Pasos and Flynn, Michael and Large, James and Middlehurst, Matthew and Bagnall, Anthony},
  year = {2021},
  month = mar,
  journal = {Data Mining and Knowledge Discovery},
  volume = {35},
  number = {2},
  pages = {401--449},
  issn = {1573-756X},
  doi = {10.1007/s10618-020-00727-3},
  abstract = {Time Series Classification (TSC) involves building predictive models for a discrete target variable from ordered, real valued, attributes. Over recent years, a new set of TSC algorithms have been developed which have made significant improvement over the previous state of the art. The main focus has been on univariate TSC, i.e. the problem where each case has a single series and a class label. In reality, it is more common to encounter multivariate TSC (MTSC) problems where the time series for a single case has multiple dimensions. Despite this, much less consideration has been given to MTSC than the univariate case. The UCR archive has provided a valuable resource for univariate TSC, and the lack of a standard set of test problems may explain why there has been less focus on MTSC. The UEA archive of 30 MTSC problems released in 2018 has made comparison of algorithms easier. We review recently proposed bespoke MTSC algorithms based on deep learning, shapelets and bag of words approaches. If an algorithm cannot naturally handle multivariate data, the simplest approach to adapt a univariate classifier to MTSC is to ensemble it over the multivariate dimensions. We compare the bespoke algorithms to these dimension independent approaches on the 26 of the 30 MTSC archive problems where the data are all of equal length. We demonstrate that four classifiers are significantly more accurate than the benchmark dynamic time warping algorithm and that one of these recently proposed classifiers, ROCKET, achieves significant improvement on the archive datasets in at least an order of magnitude less time than the other three.}
}

@article{ruizGreatMultivariateTime2021b,
  title = {The Great Multivariate Time Series Classification Bake off: A Review and Experimental Evaluation of Recent Algorithmic Advances},
  shorttitle = {The Great Multivariate Time Series Classification Bake Off},
  author = {Ruiz, Alejandro Pasos and Flynn, Michael and Large, James and Middlehurst, Matthew and Bagnall, Anthony},
  year = {2021},
  month = mar,
  journal = {Data Mining and Knowledge Discovery},
  volume = {35},
  number = {2},
  pages = {401--449},
  issn = {1573-756X},
  doi = {10.1007/s10618-020-00727-3},
  abstract = {Time Series Classification (TSC) involves building predictive models for a discrete target variable from ordered, real valued, attributes. Over recent years, a new set of TSC algorithms have been developed which have made significant improvement over the previous state of the art. The main focus has been on univariate TSC, i.e. the problem where each case has a single series and a class label. In reality, it is more common to encounter multivariate TSC (MTSC) problems where the time series for a single case has multiple dimensions. Despite this, much less consideration has been given to MTSC than the univariate case. The UCR archive has provided a valuable resource for univariate TSC, and the lack of a standard set of test problems may explain why there has been less focus on MTSC. The UEA archive of 30 MTSC problems released in 2018 has made comparison of algorithms easier. We review recently proposed bespoke MTSC algorithms based on deep learning, shapelets and bag of words approaches. If an algorithm cannot naturally handle multivariate data, the simplest approach to adapt a univariate classifier to MTSC is to ensemble it over the multivariate dimensions. We compare the bespoke algorithms to these dimension independent approaches on the 26 of the 30 MTSC archive problems where the data are all of equal length. We demonstrate that four classifiers are significantly more accurate than the benchmark dynamic time warping algorithm and that one of these recently proposed classifiers, ROCKET, achieves significant improvement on the archive datasets in at least an order of magnitude less time than the other three.},
  langid = {english},
  keywords = {Evaluating classifiers,Multivariate time series,Time series classification,UEA archive},
  file = {/Users/trenthenderson/Zotero/storage/CY4W2N6F/Ruiz et al. - 2021 - The great multivariate time series classification .pdf}
}

@article{santosoGeneticProgrammingApproach2018,
  title = {A Genetic Programming Approach to Binary Classification Problem},
  author = {Santoso, Leo and Singh, Bhopendra and Rajest, S. and Regin, R. and Kadhim, Karrar},
  year = {2018},
  month = jul,
  journal = {EAI Endorsed Transactions on Energy Web},
  pages = {165523},
  issn = {2032-944X},
  doi = {10.4108/eai.13-7-2018.165523},
  abstract = {The Binary classification is the most challenging problem in machine learning. One of the most promising technique to solve this problem is by implementing genetic programming (GP). GP is one of Evolutionary Algorithm (EA) that used to solve problems that humans do not know how to solve it directly. The objectives of this research is to demonstrate the use of genetic programming in this type of problems; that is, other types of techniques are typically used, e.g., regression, artificial neural networks. Genetic programming presents an advantage compared to those techniques, which is that it does not need an a priori definition of its structure. The algorithm evolves automatically until finding a model that best fits a set of training data. Feature engineering was considered to improve the accuracy. In this research, feature transformation and feature creation were implemented. Thus, genetic programming can be considered as an alternative option for the development of intelligent systems mainly in the pattern recognition field.},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/PN457SXE/Santoso et al. - 2018 - A Genetic Programming Approach to Binary Classific.pdf}
}

@misc{sorzanoSurveyDimensionalityReduction2014,
  title = {A Survey of Dimensionality Reduction Techniques},
  author = {Sorzano, C. O. S. and Vargas, J. and Montano, A. Pascual},
  year = {2014},
  month = mar,
  number = {arXiv:1403.2877},
  eprint = {1403.2877},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1403.2877},
  abstract = {Experimental life sciences like biology or chemistry have seen in the recent decades an explosion of the data available from experiments. Laboratory instruments become more and more complex and report hundreds or thousands measurements for a single experiment and therefore the statistical methods face challenging tasks when dealing with such high dimensional data. However, much of the data is highly redundant and can be efficiently brought down to a much smaller number of variables without a significant loss of information. The mathematical procedures making possible this reduction are called dimensionality reduction techniques; they have widely been developed by fields like Statistics or Machine Learning, and are currently a hot research topic. In this review we categorize the plethora of dimension reduction techniques available and give the mathematical insight behind them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {/Users/trenthenderson/Zotero/storage/94HGNFZN/Sorzano et al. - 2014 - A survey of dimensionality reduction techniques.pdf;/Users/trenthenderson/Zotero/storage/VNYSP8S2/1403.html}
}

@article{subasiEEGSignalClassification2010,
  title = {{EEG} Signal Classification Using {PCA}, {ICA}, {LDA} and Support Vector Machines},
  author = {Subasi, Abdulhamit and Ismail Gursoy, M.},
  year = {2010},
  month = dec,
  journal = {Expert Systems with Applications},
  volume = {37},
  number = {12},
  pages = {8659--8666},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2010.06.065},
  abstract = {In this work, we proposed a versatile signal processing and analysis framework for Electroencephalogram (EEG). Within this framework the signals were decomposed into the frequency sub-bands using DWT and a set of statistical features was extracted from the sub-bands to represent the distribution of wavelet coefficients. Principal components analysis (PCA), independent components analysis (ICA) and linear discriminant analysis (LDA) is used to reduce the dimension of data. Then these features were used as an input to a support vector machine (SVM) with two discrete outputs: epileptic seizure or not. The performance of classification process due to different methods is presented and compared to show the excellent of classification process. These findings are presented as an example of a method for training, and testing a seizure prediction method on data from individual petit mal epileptic patients. Given the heterogeneity of epilepsy, it is likely that methods of this type will be required to configure intelligent devices for treating epilepsy to each individual's neurophysiology prior to clinical operation.},
  langid = {english},
  keywords = {Discrete wavelet transform (DWT),Electroencephalogram (EEG),Epileptic seizure,Independent component analysis (ICA),Linear discriminant analysis (LDA),Principal component analysis (PCA),Support vector machines (SVM)},
  file = {/Users/trenthenderson/Zotero/storage/93IN76R6/S0957417410005695.html}
}

@article{talknerPowerSpectrumDetrended2000,
  title = {Power Spectrum and Detrended Fluctuation Analysis: Application to Daily Temperatures},
  shorttitle = {Power Spectrum and Detrended Fluctuation Analysis},
  author = {Talkner, null and Weber, null},
  year = {2000},
  month = jul,
  journal = {Physical Review. E, Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics},
  volume = {62},
  number = {1 Pt A},
  pages = {150--160},
  issn = {1063-651X},
  doi = {10.1103/physreve.62.150},
  abstract = {The variability measures of fluctuation analysis (FA) and detrended fluctuation analysis (DFA) are expressed in terms of the power spectral density and of the autocovariance of a given process. The diagnostic potential of these methods is tested on several model power spectral densities. In particular we find that both FA and DFA reveal an algebraic singularity of the power spectral density at small frequencies corresponding to an algebraic decay of the autocovariance. A scaling behavior of the power spectral density in an intermediate frequency regime is better reflected by DFA than by FA. We apply FA and DFA to ambient temperature data from the 20th century with the primary goal to resolve the controversy in literature whether the low frequency behavior of the corresponding power spectral densities are better described by a power law or a stretched exponential. As a third possible model we suggest a Weibull distribution. However, it turns out that neither FA nor DFA can reliably distinguish between the proposed models.},
  langid = {english},
  pmid = {11088447},
  file = {/Users/trenthenderson/Zotero/storage/B5W4QMBR/Talkner and Weber - 2000 - Power spectrum and detrended fluctuation analysis.pdf}
}

@article{tanTimeSeriesExtrinsic2021,
  title = {Time Series Extrinsic Regression},
  author = {Tan, Chang Wei and Bergmeir, Christoph and Petitjean, Fran{\c c}ois and Webb, Geoffrey I.},
  year = {2021},
  month = may,
  journal = {Data Mining and Knowledge Discovery},
  volume = {35},
  number = {3},
  pages = {1032--1060},
  issn = {1573-756X},
  doi = {10.1007/s10618-021-00745-9},
  abstract = {This paper studies time series extrinsic regression (TSER): a regression task of which the aim is to learn the relationship between a time series and a continuous scalar variable; a task closely related to time series classification (TSC), which aims to learn the relationship between a time series and a categorical class label. This task generalizes time series forecasting, relaxing the requirement that the value predicted be a future value of the input series or primarily depend on more recent values. In this paper, we motivate and study this task, and benchmark existing solutions and adaptations of TSC algorithms on a novel archive of 19 TSER datasets which we have assembled. Our results show that the state-of-the-art TSC algorithm Rocket, when adapted for regression, achieves the highest overall accuracy compared to adaptations of other TSC algorithms and state-of-the-art machine learning (ML) algorithms such as XGBoost, Random Forest and Support Vector Regression. More importantly, we show that much research is needed in this field to improve the accuracy of ML models. We also find evidence that further research has excellent prospects of improving upon these straightforward baselines.}
}

@article{timmerCharacteristicsHandTremor1993,
  title = {Characteristics of Hand Tremor Time Series},
  author = {Timmer, J. and Gantert, C. and Deuschl, G. and Honerkamp, J.},
  year = {1993},
  month = nov,
  journal = {Biological Cybernetics},
  volume = {70},
  number = {1},
  pages = {75--80},
  issn = {1432-0770},
  doi = {10.1007/BF00202568},
  abstract = {Tremor is classified into physiological, essential, and parkinsonian tremor by means of clinical criteria. The aim of our work was to extract quantitative features from the measurements of the acceleration of human postural hand tremor. Different mathematical methods were adopted and modified in order to separate these three types of tremor. Best discrimination between physiological and pathological tremors has been achieved by methods distinguishing nonlinear from linear behavior. On the other hand, methods separating different forms of nonlinear behavior have been found to be superior in discriminating parkinsonian and essential tremor. By these methods physiological and pathological tremors can be separated with an error rate below 20\% and essential and parkinsonian tremor with an error rate below 10\%. This may help to classify tremor time series by objective mathematical criteria and may increase the understanding of the pathophysiological differences underlying these kinds of tremor.}
}

@article{timmerCharacteristicsHandTremor1993a,
  title = {Characteristics of Hand Tremor Time Series},
  author = {Timmer, J. and Gantert, C. and Deuschl, G. and Honerkamp, J.},
  year = {1993},
  month = nov,
  journal = {Biological Cybernetics},
  volume = {70},
  number = {1},
  pages = {75--80},
  issn = {1432-0770},
  doi = {10.1007/BF00202568},
  abstract = {Tremor is classified into physiological, essential, and parkinsonian tremor by means of clinical criteria. The aim of our work was to extract quantitative features from the measurements of the acceleration of human postural hand tremor. Different mathematical methods were adopted and modified in order to separate these three types of tremor. Best discrimination between physiological and pathological tremors has been achieved by methods distinguishing nonlinear from linear behavior. On the other hand, methods separating different forms of nonlinear behavior have been found to be superior in discriminating parkinsonian and essential tremor. By these methods physiological and pathological tremors can be separated with an error rate below 20\% and essential and parkinsonian tremor with an error rate below 10\%. This may help to classify tremor time series by objective mathematical criteria and may increase the understanding of the pathophysiological differences underlying these kinds of tremor.},
  langid = {english}
}

@article{trDimensionalityReductionComparative,
  title = {Dimensionality {{Reduction}}: {{A Comparative Review}}},
  author = {Tr, TiCC},
  pages = {36},
  abstract = {In recent years, a variety of nonlinear dimensionality reduction techniques have been proposed that aim to address the limitations of traditional techniques such as PCA and classical scaling. The paper presents a review and systematic comparison of these techniques. The performances of the nonlinear techniques are investigated on artificial and natural tasks. The results of the experiments reveal that nonlinear techniques perform well on selected artificial tasks, but that this strong performance does not necessarily extend to real-world tasks. The paper explains these results by identifying weaknesses of current nonlinear techniques, and suggests how the performance of nonlinear dimensionality reduction techniques may be improved.},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/EK83DDQV/Tr - Dimensionality Reduction A Comparative Review.pdf}
}

@article{vanderdoncktTsflexFlexibleTime2022,
  title = {{tsflex}: Flexible Time Series Processing \& Feature Extraction},
  shorttitle = {Tsflex},
  author = {Van Der Donckt, Jonas and Van Der Donckt, Jeroen and Deprost, Emiel and Van Hoecke, Sofie},
  year = {2022},
  month = jan,
  journal = {SoftwareX},
  volume = {17},
  pages = {100971},
  issn = {2352-7110},
  doi = {10.1016/j.softx.2021.100971},
  abstract = {Time series processing and feature extraction are crucial and time-intensive steps in conventional machine learning pipelines. Existing packages are limited in their applicability, as they cannot cope with irregularly-sampled or asynchronous data and make strong assumptions about the data format. Moreover, these packages do not focus on execution speed and memory efficiency, resulting in considerable overhead. We present tsflex, a Python toolkit for time series processing and feature extraction, that focuses on performance and flexibility, enabling broad applicability. This toolkit leverages window-stride arguments of the same data type as the sequence-index, and maintains the sequence-index through all operations. tsflex is flexible as it supports (1) multivariate time series, (2) multiple window-stride configurations, and (3) integrates with processing and feature functions from other packages, while (4) making no assumptions about the data sampling regularity, series alignment, and data type. Other functionalities include multiprocessing, detailed execution logging, chunking sequences, and serialization. Benchmarks show that tsflex is faster and more memory-efficient compared to similar packages, while being more permissive and flexible in its utilization.},
  langid = {english},
  keywords = {Feature extraction,Machine learning,Processing,Python,Time series},
  file = {/Users/trenthenderson/Zotero/storage/AAG5RY6P/Van Der Donckt et al. - 2022 - tsflex Flexible time series processing & feature .pdf}
}

@article{wattenbergHowUseTSNE2016,
  title = {How to {{Use}} T-{{SNE Effectively}}},
  author = {Wattenberg, Martin and Vi{\'e}gas, Fernanda and Johnson, Ian},
  year = {2016},
  month = oct,
  journal = {Distill},
  volume = {1},
  number = {10},
  pages = {e2},
  issn = {2476-0757},
  doi = {10.23915/distill.00002},
  abstract = {Although extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading.},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/FU3ZNVCY/misread-tsne.html}
}

@article{westEvaluationComparisonEEG1999,
  title = {Evaluation and {{Comparison}} of {{EEG Traces}}: {{Latent Structure}} in {{Nonstationary Time Series}}},
  shorttitle = {Evaluation and {{Comparison}} of {{EEG Traces}}},
  author = {West, Mike and Prado, Raquel and Krystal, Andrew D.},
  year = {1999},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {94},
  number = {446},
  pages = {375--387},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1999.10474128},
  abstract = {We explore and illustrate the use of time series decomposition methods for evaluating and comparing latent structure in nonstationary electroencephalographic (EEG) traces obtained from depressed patients during brain seizures induced as part of electroconvulsive therapy (ECT). Analysis of the patterns of change over time in the frequency structure of such EEG data provides insight into the neurophysiological mechanisms of action of this effective but poorly understood antidepressant treatment, and allows clinicians to modify ECT treatments to optimize therapeutic benefits while minimizing associated side effects. Our work has introduced new methods of time-frequency analysis of EEG series that identify the complete pattern of time evolution of frequency structure over the course of a seizure, and usefully assist in these scientific and clinical studies. New methods of decomposition of flexible dynamic models provide time domain decompositions of individual EEG series into collections of latent components in different frequency bands. This allows us to explore ECT seizure characteristics via inferences on the time-varying parameters that characterize these latent components, and to relate differences in such characteristics across seizures to differences in the therapeutic effectiveness and cognitive side effects of those seizures. This article discusses the scientific context and problems, development of nonstationary time series models and new methods of decomposition to explore time-frequency structure, and aspects of model fitting and analysis. We include applied studies on two datasets from recent clinical ECT studies. One is an initial illustrative analysis of a single EEG trace, the second compares the EEG data recorded during two types of ECT treatment that differ in therapeutic effectiveness and cognitive side effects. The uses of these models and time series decomposition methods in extracting and contrasting key features of the seizure underlying the EEG signals are highlighted. Through the use of these models we have quantified, for the first time, decreases in the dominant frequencies of low-frequency EEG components during ECT seizures. We have also identified preliminary evidence that such decreases are enhanced under the more effective ECTs at higher electrical dosages, a finding consistent with prior reports and the hypothesis that more effective forms of ECT are more effective in eliciting neurophysiological inhibitory processes.},
  keywords = {Bayesian inference,Dynamic latent factor,Dynamic linear model,Electroconvulsive therapy,Electroencephalographic time series,Time-frequency analysis,Time-series decomposition,Time-varying autoregression},
  annotation = {\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1999.10474128},
  file = {/Users/trenthenderson/Zotero/storage/GDK23638/01621459.1999.html}
}

@book{wickhamGgplot2ElegantGraphics2009,
  title = {{ggplot2}: Elegant Graphics for Data Analysis},
  shorttitle = {Ggplot2},
  author = {Wickham, Hadley},
  year = {2009},
  series = {Use {{R}}!},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-0-387-98141-3},
  abstract = {This book describes ggplot2, a new data visualization package for R that uses the insights from Leland Wilkison's Grammar of Graphics to create a powerful and flexible system for creating data graphics. With ggplot2, it's easy to: produce handsome, publication-quality plots, with automatic legends created from the plot specification superpose multiple layers (points, lines, maps, tiles, box plots to name a few) from different data sources, with automatically adjusted common scales add customisable smoothers that use the powerful modelling capabilities of R, such as loess, linear models, generalised additive models and robust regression save any ggplot2 plot (or part thereof) for later modification or reuse create custom themes that capture in-house or journal style requirements, and that can easily be applied to multiple plots approach your graph from a visual perspective, thinking about how each component of the data is represented on the final plot This book will be useful to everyone who has struggled with displaying their data in an informative and attractive way. You will need some basic knowledge of R (i.e. you should be able to get your data into R), but ggplot2 is a mini-language specifically tailored for producing graphics, and you'll learn everything you need in the book. After reading this book you'll be able to produce graphics customized precisely for your problems, and you'll find it easy to get graphics out of your head and on to the screen or page. Hadley Wickham is an Assistant Professor of Statistics at Rice University, and is interested in developing computational and cognitive tools for making data preparation, visualization, and analysis easier. He has developed 15 R packages and in 2006 he won the John Chambers Award for Statistical Computing for his work on the ggplot and reshape R packages.},
  isbn = {978-0-387-98141-3},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/6EUG6DBH/Wickham - 2009 - ggplot2 Elegant Graphics for Data Analysis.pdf;/Users/trenthenderson/Zotero/storage/RFR9KHP6/9780387981413.html}
}

@article{wickhamTidyData2014,
  title = {Tidy Data},
  author = {Wickham, Hadley},
  year = {2014},
  month = sep,
  journal = {Journal of Statistical Software},
  volume = {59},
  number = {1},
  pages = {1--23},
  issn = {1548-7660},
  doi = {10.18637/jss.v059.i10},
  copyright = {Copyright (c) 2013 Hadley  Wickham},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/4WZTJC5S/Wickham - 2014 - Tidy Data.pdf}
}

@article{wickhamWelcomeTidyverse2019,
  title = {Welcome to the {{Tidyverse}}},
  author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy D'Agostino and Fran{\c c}ois, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas Lin and Miller, Evan and Bache, Stephan Milton and M{\"u}ller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana Paige and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
  year = {2019},
  month = nov,
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {43},
  pages = {1686},
  issn = {2475-9066},
  doi = {10.21105/joss.01686},
  abstract = {Wickham et al., (2019). Welcome to the Tidyverse. Journal of Open Source Software, 4(43), 1686, https://doi.org/10.21105/joss.01686},
  langid = {english},
  file = {/Users/trenthenderson/Zotero/storage/XQRW3HNJ/Wickham et al. - 2019 - Welcome to the Tidyverse.pdf;/Users/trenthenderson/Zotero/storage/6L4YEIGZ/joss.html}
}

@article{yangAnomalyDetectionAlgorithm2021,
  title = {An Anomaly Detection Algorithm Selection Service for {IoT} Stream Data Based on {tsfresh} Tool and Genetic Algorithm},
  author = {Yang, Zhongguo and Abbasi, Irshad Ahmed and Mustafa, Elfatih Elmubarak and Ali, Sikandar and Zhang, Mingzhu},
  editor = {Nazir, Shah},
  year = {2021},
  month = feb,
  journal = {Security and Communication Networks},
  volume = {2021},
  pages = {6677027},
  publisher = {{Hindawi}},
  issn = {1939-0114},
  doi = {10.1155/2021/6677027},
  abstract = {Anomaly detection algorithms (ADA) have been widely used as services in many maintenance monitoring platforms. However, there are numerous algorithms that could be applied to these fast changing stream data. Furthermore, in IoT stream data due to its dynamic nature, the phenomena of conception drift happened. Therefore, it is a challenging task to choose a suitable anomaly detection service (ADS) in real time. For accurate online anomalous data detection, this paper developed a service selection method to select and configure ADS at run-time. Initially, a time-series feature extractor (Tsfresh) and a genetic algorithm-based feature selection method are applied to swiftly extract dominant features which act as representation for the stream data patterns. Additionally, stream data and various efficient algorithms are collected as our historical data. A fast classification model based on XGBoost is trained to record stream data features to detect appropriate ADS dynamically at run-time. These methods help to choose suitable service and their respective configuration based on the patterns of stream data. The features used to describe and reflect time-series data\&\#x2019;s intrinsic characteristics are the main success factor in our framework. Consequently, experiments are conducted to evaluate the effectiveness of features closed by genetic algorithm. Experimentations on both artificial and real datasets demonstrate that the accuracy of our proposed method outperforms various advanced approaches and can choose appropriate service in different scenarios efficiently.}
}

@article{zhangUsingGaussianDistribution2006,
  title = {Using {{Gaussian}} Distribution to Construct Fitness Functions in Genetic Programming for Multiclass Object Classification},
  author = {Zhang, Mengjie and Smart, Will},
  year = {2006},
  month = aug,
  journal = {Pattern Recognition Letters},
  volume = {27},
  number = {11},
  pages = {1266--1274},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2005.07.024},
  abstract = {This paper describes a new approach to the use of Gaussian distribution in genetic programming (GP) for multiclass object classification problems. Instead of using predefined multiple thresholds to form different regions in the program output space for different classes, this approach uses probabilities of different classes, derived from Gaussian distributions, to construct the fitness function for classification. Two fitness measures, overlap area and weighted distribution distance, have been developed. Rather than using the best evolved program in a population, this approach uses multiple programs and a voting strategy to perform classification. The approach is examined on three multiclass object classification problems of increasing difficulty and compared with a basic GP approach. The results suggest that the new approach is more effective and more efficient than the basic GP approach. Although developed for object classification, this approach is expected to be able to be applied to other classification problems.},
  keywords = {Fitness function,Multiclass classification,Object detection,Object recognition,Probability based genetic programming},
  file = {/Users/trenthenderson/Zotero/storage/D6TBAFJN/Zhang and Smart - 2006 - Using Gaussian distribution to construct fitness f.pdf}
}

@Manual{feasts_pkg,
title = {{feasts}: Feature Extraction and Statistics for Time Series},
author = {Mitchell O'Hara-Wild and Rob Hyndman and Earo Wang},
year = {2021},
note = {R package version 0.4.1},
url = {https://CRAN.R-project.org/package=feasts},
}

@Manual{tsfeatures_pkg,
title = {{tsfeatures}: Time Series Feature Extraction},
author = {Rob Hyndman and Yanfei Kang and Pablo Montero-Manso and Thiyanga Talagala and Earo Wang and Yangzhuoran Yang and Mitchell O'Hara-Wild},
year = {2020},
note = {R package version 1.1.1},
url = {https://CRAN.R-project.org/package=tsfeatures},
}


@Manual{Rcatch22_pkg,
title = {Rcatch22: Calculation of 22 CAnonical Time-Series CHaracteristics},
author = {Trent Henderson},
year = {2021},
note = {R package version 0.2.3},
url = {https://CRAN.R-project.org/package=Rcatch22}
}

@Manual{catch22jl_pkg,
title = {Catch22.jl},
author = {Brendan J. Harris},
year = {2021},
note = {v0.2.1},
doi = {https://doi.org/10.5281/zenodo.5030712}
}


@misc{Kats,
  title = {Kats},
  author = {{Facebook Infrastructure Data Science}},
  year = {2021},
  url = {https://facebookresearch.github.io/Kats/},
}


@Book{ggplot2,
author = {Hadley Wickham},
title = {{ggplot2}: Elegant Graphics for Data Analysis},
publisher = {Springer-Verlag New York},
year = {2016},
isbn = {978-3-319-24277-4},
url = {https://ggplot2.tidyverse.org},
}

  @Manual{caret,
    title = {{caret}: Classification and Regression Training},
    author = {Max Kuhn},
    year = {2020},
    note = {R package version 6.0-86},
    url = {https://CRAN.R-project.org/package=caret},
  }
  
  
  @Manual{R_lang,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2021},
    url = {https://www.R-project.org/}
  }


  @Manual{shiny,
    title = {{shiny}: Web Application Framework for R},
    author = {Winston Chang and Joe Cheng and JJ Allaire and Yihui Xie and Jonathan McPherson},
    year = {2020},
    note = {R package version 1.5.0},
    url = {https://CRAN.R-project.org/package=shiny},
  }
  
  
  @article{clevelandSeasonalTrend1990,
  added-at = {2009-10-28T04:42:52.000+0100},
  author = {Cleveland, Robert B. and Cleveland, William S. and McRae, Jean E. and Terpenning, Irma},
  biburl = {https://www.bibsonomy.org/bibtex/24bf4893a61f6e30b2dbf7f37884295ed/jwbowers},
  citeulike-article-id = {106881},
  date-added = {2007-09-03 22:45:16 -0500},
  date-modified = {2007-09-03 22:45:16 -0500},
  interhash = {a8931b8eac108ccff1bb30b75130aac9},
  intrahash = {4bf4893a61f6e30b2dbf7f37884295ed},
  journal = {Journal of Official Statistics},
  keywords = {graphical_methods statistics},
  pages = {3--73},
  timestamp = {2009-10-28T04:43:05.000+0100},
  title = {{STL}: A Seasonal-Trend Decomposition Procedure Based on Loess (with Discussion)},
  volume = 6,
  year = 1990
}


@misc{theft_webtool,
  doi = {10.5281/ZENODO.6656286},
  url = {https://zenodo.org/record/6656286},
  author = {Henderson,  Trent},
  title = {{hendersontrent/theft-webtool}: v0.1.1},
  publisher = {Zenodo},
  year = {2022},
  copyright = {Open Access}
}

  @Manual{normaliseR_pkg,
    title = {{normaliseR}: Re-Scale Vectors and Time-Series Features},
    author = {Trent Henderson},
    year = {2024},
    note = {R package version 0.1.2},
    url = {https://CRAN.R-project.org/package=normaliseR},
  }

@misc{henderson2023dullmomentdistributionalproperties,
      title={Never a Dull Moment: Distributional Properties as a Baseline for Time-Series Classification}, 
      author={Trent Henderson and Annie G. Bryant and Ben D. Fulcher},
      year={2023},
      eprint={2303.17809},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2303.17809}, 
}

@article{nadeauInferenceGeneralizationError2003,
  title = {Inference for the Generalization Error},
  author = {Nadeau, Claude and Bengio, Yoshua},
  year = {2003},
  journal = {Machine Learning},
  volume = {52},
  pages = {239},
  abstract = {In order to compare learning algorithms, experimental results reported in the machine learning literature often use statistical tests of significance to support the claim that a new learning algorithm generalizes better. Such tests should take into account the variability due to the choice of training set and not only that due to the test examples, as is often the case. This could lead to gross underestimation of the variance of the cross-validation estimator, and to the wrong conclusion that the new algorithm is significantly better when it is not. We perform a theoretical investigation of the variance of a variant of the cross-validation estimator of the generalization error that takes into account the variability due to the randomness of the training set as well as test examples. Our analysis shows that all the variance estimators that are based only on the results of the cross-validation experiment must be biased. This analysis allows us to propose new estimators of this variance. We show, via simulations, that tests of hypothesis about the generalization error using those new variance estimators have better properties than tests involving variance estimators currently in use and listed in Dietterich (1998). In particular, the new tests have correct size and good power. That is, the new tests do not reject the null hypothesis too often when the hypothesis is true, but they tend to frequently reject the null hypothesis when the latter is false.},
  langid = {english},
  keywords = {cross-validation,generalization error,hypothesis tests,power,size,variance estimation},
  file = {/Users/trenthenderson/Zotero/storage/JYB4P8SM/Nadeau and Bengio - 2003 - Inference for the Generalization Error.pdf}
}

@software{Jiang_KATS_2022,
author = {Jiang, Xiaodong and Srivastava, Sudeep and Chatterjee, Sourav and Yu, Yang and Handler, Jeffrey and Zhang, Peiyi and Bopardikar, Rohan and Li, Dawei and Lin, Yanjun and Thakore, Uttam and Brundage, Michael and Holt, Ginger and Komurlu, Caner and Nagalla, Rakshita and Wang, Zhichao and Sun, Hechao and Gao, Peng and Cheung, Wei and Gao, Jun and Wang, Qi and Guerard, Marius and Kazemi, Morteza and Chen, Yulin and Zhou, Chong and Lee, Sean and Laptev, Nikolay and Levendovszky, Tiham√©r and Taylor, Jake and Qian, Huijun and Zhang, Jian and Shoydokova, Aida and Singh, Trisha and Zhu, Chengjun and Baz, Zeynep and Bergmeir, Christoph and Yu, Di and Koylan, Ahmet and Jiang, Kun and Temiyasathit, Ploy and Yurtbay, Emre},
license = {MIT License},
month = {3},
title = {{Kats}},
url = {https://github.com/facebookresearch/Kats},
version = {0.2.0},
year = {2022}
}

@article{loning2019sktime,
  title={{sktime}: A unified interface for machine learning with time series},
  author={L{\"o}ning, Markus and Bagnall, Anthony and Ganesh, Sajaysurya and Kazakov, Viktor and Lines, Jason and Kir{\'a}ly, Franz J},
  journal={arXiv preprint arXiv:1909.07872},
  year={2019}
}

@Manual{theft_pkg,
    title = {{theft}: Tools for Handling Extraction of Features from Time Series},
    author = {Trent Henderson},
    year = {2025},
    note = {R package version 0.8.2},
    url = {https://CRAN.R-project.org/package=theft},
}

@Manual{theftdlc_pkg,
    title = {{theftdlc}: Analyse and Interpret Time Series Features},
    author = {Trent Henderson},
    year = {2025},
    note = {R package version 0.2.1},
    url = {https://CRAN.R-project.org/package=theftdlc},
  }
  
@Article{tsibble_pkg,
    author = {Earo Wang and Dianne Cook and Rob J Hyndman},
    title = {A new tidy data structure to support exploration and modeling of temporal data},
    journal = {Journal of Computational and Graphical Statistics},
    volume = {29},
    number = {3},
    pages = {466-478},
    year = {2020},
    publisher = {Taylor & Francis},
    doi = {10.1080/10618600.2019.1695624},
    url = {https://doi.org/10.1080/10618600.2019.1695624},
  }
  
    @Manual{correctR_pkg,
    title = {{correctR}: Corrected Test Statistics for Comparing Machine Learning Models
on Correlated Samples},
    author = {Trent Henderson},
    year = {2025},
    note = {R package version 0.3.1},
    url = {https://CRAN.R-project.org/package=correctR},
  }
  
    @Manual{dplyr_pkg,
    title = {{dplyr}: A Grammar of Data Manipulation},
    author = {Hadley Wickham and Romain Fran√ßois and Lionel Henry and Kirill M√ºller and Davis Vaughan},
    year = {2023},
    note = {R package version 1.1.4},
    url = {https://CRAN.R-project.org/package=dplyr},
  }
