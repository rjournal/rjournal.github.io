---
title: 'rvif: a Decision Rule to Detect Troubling Statistical Multicollinearity Based
  on Redefined VIF'
date: '2026-02-04'
abstract: |
  Multicollinearity is relevant in many different fields where linear regression models are applied since its presence may affect the analysis of ordinary least squares estimators not only numerically but also from a statistical point of view, which is the focus of this paper. Thus, it is known that collinearity can lead to incoherence in the statistical significance of the coefficients of the independent variables and in the global significance of the model. In this paper, the thresholds of the Redefined Variance Inflation Factor (RVIF) are reinterpreted and presented as a statistical test with a region of non-rejection (which depends on a significance level) to diagnose the existence of a degree of worrying multicollinearity that affects the linear regression model from a statistical point of view. The proposed methodology is implemented in the rvif package of R and its application is illustrated with different real data examples previously applied in the scientific literature.
draft: no
author:
- name: Román Salmerón-Gómez
  affiliation: University of Granada
  address:
  - Department of Quantitative Methods for Economics and Business
  - Campus Universitario de La Cartuja, Universidad de Granada. 18071 Granada (España)
  url: https://www.ugr.es/~romansg/web/index.html
  orcid: 0000-0003-2589-4058
  email: romansg@ugr.es
- name: Catalina B. García-García
  affiliation: University of Granada
  address:
  - Department of Quantitative Methods for Economics and Business
  - Campus Universitario de La Cartuja, Universidad de Granada. 18071 Granada (España)
  url: https://metodoscuantitativos.ugr.es/informacion/directorio-personal/catalina-garcia-garcia
  email: cbgarcia@ugr.es
  orcid: 0000-0003-1622-3877
type: package
output:
  rjtools::rjournal_article:
    self_contained: yes
    toc: no
bibliography: RJreferences.bib
editor_options:
  chunk_output_type: console
date_received: '2025-02-06'
volume: 17
issue: 4
slug: RJ-2025-040
journal:
  lastpage: 215
  firstpage: 192

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE) #, cache = TRUE)
options(tinytex.verbose = TRUE)
library(rvif)
library(knitr)
library(kableExtra)
library(memisc) # mtable
```

# Introduction

It is well known that linear relationships between the independent variables of a multiple linear regression model (multicollinearity) can affect the analysis of the model estimated by Ordinary Least Squares (OLS), either by causing unstable estimates of the coefficients of these variables or by rejecting individual significance tests of these coefficients (see, for example, @FarrarGlauber, @GunstMason1977, @Gujarati2003, @Silvey1969, @WillanWatts1978 or @Wooldrigde2013).
However, the measures traditionally applied to detect multicollinearity may conclude that multicollinearity exists even if it does not lead to the negative effects mentioned above (see Subsection [Effect of sample size..](#effect-sample-size) for more details), when, in fact, the best solution in this case may be not to treat the multicollinearity (see @OBrien).

Focusing on the possible effect of multicollinearity on the individual significance tests of the coefficients of the independent variables (tendency not to reject the null hypothesis), this paper proposes an alternative procedure that focuses on checking whether the detected multicollinearity affects the statistical analysis of the model. For this disruptive approach, a methodology is necessary that indicates whether multicollinearity affects the statistical analysis of the model. The introduction of such methodology is the main objective of this paper. The paper also shows the use of the \CRANpkg{rvif} package of R (@R) in which this procedure is implemented.

To this end, we start from the Variance Inflation Factor (VIF). The VIF is obtained from the coefficient of determination of the auxiliary regression of each independent variable of linear regression model as a function of the other independent variables. Thus, there is a VIF for each independent variable except for the intercept, for which it is not possible to calculate a coefficient of determination for the corresponding auxiliary regression. Consequently, the VIF is able to diagnose the degree of essential approximate multicollinearity (strong linear relationship between the independent variables except the intercept) existing in the model but is not able to detect the non-essential one (strong relationship between the intercept and at least one of the independent variables). 
For more information on multicollinearity of essential and non-essential type, see @MarquardtSnee1975 and @Salmeron2019.

However, the fact that the VIF detects a worrying level of multicollinearity does not always translate into a negative impact on the statistical analysis. This lack of specificity is due to the fact that other factors, such as sample size and the variance of the random disturbance, can lead to high values of the VIF but not increase the variance of the OLS estimators (see @OBrien). The explanation for this phenomenon hinges on the fact that, in the orthogonal variable reference model, which is traditionally considered as the reference, the linear relationships are assumed to be eliminated, while other factors, such as the variance of the random disturbance, maintain the same values.

Then, to avoid these inconsistencies, @Salmeron2024a propose a QR decomposition in the matrix of independent variables of the model in order to obtain an orthonormal matrix. By redefining the reference point, the variance inflation factor is also redefined, resulting in a new detection measure that analyzes the change in the VIF and the rest of relevant factors of the model, thereby overcoming the problems associated with the traditional VIF, as described by @OBrien among others. The intercept is also included in the detection (contrary to what happens with the traditional VIF), it is therefore able to detect both essential and non-essential multicollinearity. 
This new measure presented by @Salmeron2024a is called Redefined Variance Inflation Factor (RVIF).

In this paper, the RVIF is associated with a statistical test for detecting troubling multicollinearity, this test is given by a region of non-rejection that depends on a significance level. Note that most of the measures used to diagnose multicollinearity are merely indicators with rules of thumb rather than statistical tests per se. To the best of our knowledge, the only existing statistical test for diagnosing multicollinearity was presented by @FarrarGlauber and has received strong criticism (see, for example, @CriticaFarrar1, @CriticaFarrar2, @CriticaFarrar3 and  @CriticaFarrar4). 
Thus, for example, @CriticaFarrar1 indicates that the Farrar and Glauber statistic indicates that the variables are not orthogonal to each other; it tells us nothing more.
In this sense, @CriticaFarrar4 indicates that such a test simply indicates whether the null hypothesis of orthogonality is rejected by giving no information on the value of the matrix of correlations determinant above which the multicollinearity problem becomes intolerable.
Therefore, the non-rejection region presented in this paper should be a relevant contribution to the field of econometrics insofar as it would fill an existing gap in the scientific literature.

The paper is structured as follows: Sections [Preliminares](#preliminares) and [A first attempt of...](#modelo-orto) provide preliminary information to introduce the methodology used to establish the non-rejection region described in Section [A non-rejection region...](#new-VIF-orto).
Section [rvif package](#paqueteRVIF) presents the package \CRANpkg{rvif} of R (@R) and shows its main commands by replicating the results given in @Salmeron2024a and in the previous sections of this paper.
Finally, Section [Conclusions](#conclusiones-VIF) summarizes the main contributions of this paper.

# Preliminaries {#preliminares}

This section identifies some inconsistencies in the definition of the VIF and how these are reflected in the individual significance tests of the linear regression model. It also shows how these inconsistencies are overcome in the proposal presented by @Salmeron2024a and how this proposal can lead to a decision rule to determine whether the degree of multicollinearity is troubling, i.e., whether it affects the statistical analysis (individual significance tests) of the model.

## The original model

The multiple linear regression model with $n$ observations and $k$ independent variables can be expressed as:
\begin{equation}
    \mathbf{y}_{n \times 1} = \mathbf{X}_{n \times k} \cdot \boldsymbol{\beta}_{k \times 1} + \mathbf{u}_{n \times 1},
     (\#eq:model0) 
\end{equation}
where the first column of $\mathbf{X} = [\mathbf{1} \ \mathbf{X}_{2} \dots \mathbf{X}_{i} \dots \mathbf{X}_{k}]$ is composed of ones representing the intercept and $\mathbf{u}$ represents the random disturbance assumed to be centered and spherical. That is, $E[\mathbf{u}_{n \times 1}] = \mathbf{0}_{n \times 1}$ and $var(\mathbf{u}_{n \times 1}) = \sigma^{2} \cdot \mathbf{I}_{n \times n}$, where $\mathbf{0}$ is a vector of zeros, $\sigma^{2}$ is the variance of the random disturbance and $\mathbf{I}$ is the identity matrix.

Given the original model \@ref(eq:model0), the VIF is defined as the ratio between the variance of the estimator in this model, $var \left( \widehat{\beta}_{i} \right)$, and the variance of the estimator of a hypothetical reference model, that is, a hypothetical model in which orthogonality among the independent variables is assumed, $var \left( \widehat{\beta}_{i,o} \right)$. This is to say:

\begin{equation}\small{
    var \left( \widehat{\beta}_{i} \right) = \frac{\sigma^{2}}{n \cdot var(\mathbf{X}_{i})} \cdot \frac{1}{1 - R_{i}^{2}} =  var \left( \widehat{\beta}_{i,o} \right) \cdot VIF(i), \quad i=2,\dots,k, 
    (\#eq:vari-VIF)} 
\end{equation}  
\begin{equation}
    \frac{
        var \left( \widehat{\beta}_{i} \right)
    }{
        var \left( \widehat{\beta}_{i,o} \right) 
    } = VIF(i), \quad i=2,\dots,k, 
    (\#eq:vari-VIF2)
\end{equation}
where $\mathbf{X}_{i}$ is the independent variable $i$ of the model \@ref(eq:model0) and $R^{2}_{i}$ the coefficient of determination of the following auxiliary regression:
\begin{equation}
    \mathbf{X}_{i} = \mathbf{X}_{-i} \cdot \boldsymbol{\alpha} + \mathbf{v},
    \label{model_aux} \nonumber
\end{equation}
where $\mathbf{X}_{-i}$ is the result of eliminating  $\mathbf{X}_{i}$ from the matrix $\mathbf{X}$.

As observed in the expression \@ref(eq:vari-VIF), a high VIF leads to a high variance. Then, since the experimental value for the individual significance test is given by:
\begin{equation}
    t_{i} = \left| \frac{\widehat{\beta}_{i}}{\sqrt{\frac{\widehat{\sigma}^{2}}{n \cdot var(\mathbf{X}_{i})} \cdot VIF(i)}} \right|, \quad i=2,\dots,k, 
    (\#eq:texp-orig)
\end{equation}
a high VIF will lead to a low experimental statistic ($t_{i}$), provoking the tendency not to reject the null hypothesis, i.e. the experimental statistic will be lower than the theoretical statistic (given by $t_{n-k}(1-\alpha/2)$, where $\alpha$ is the significance level).

However, this statement is full of simplifications. By following  @OBrien, and as can be easily observed in the expression \@ref(eq:texp-orig), other factors, such as the estimation of the random disturbance and the size of the sample, can counterbalance the high value of the VIF to yield a low value for the experimental statistic.  That is to say, it is possible to obtain VIF values greater than 10 (the threshold traditionally established as troubling, see @Marquardt1970 for example) that do not necessarily imply high estimated variance on account of a large sample size or a low value for the estimated variance of the random disturbance. This explains, as noted in the introduction, why not all models with a high value for the VIF present effects on the statistical analysis of the model.

> Example 1.
>Thus, for example, @Garciaetal2019b considered an extension of the interest rate model presented by @Wooldrigde2013, where $k=3$, in which all the independent variables have associated coefficients significantly different from zero, presenting a VIF equal to 71.516, much higher than the threshold normally established as worrying. In other words, in this case, a high VIF does not mean that the individual significance tests are affected. This situation is probably due to the fact that in this case 131 observations are available, i.e. the expression \@ref(eq:texp-orig) can be expressed as:
    $$t_{i} = \left| \frac{\widehat{\beta}_{i}}{\sqrt{\frac{\widehat{\sigma}^{2}}{131 \cdot var(\mathbf{X}_{i})} \cdot 71.516}} \right|
    = \left| \frac{\widehat{\beta}_{i}}{\sqrt{0.546 \cdot \frac{\widehat{\sigma}^{2}}{var(\mathbf{X}_{i})}}} \right|, \quad i=2,3.$$
   Note that in this case a high value of $n$ compensates for the high value of VIF. In addition, the value of $n$ will also cause  $\widehat{\sigma}^{2}$ to decrease, since $\widehat{\sigma}^{2} = \frac{\mathbf{e}^{t}\mathbf{e}}{n-k}$, where $\mathbf{e}$ are the residuals of the original model \@ref(eq:model0).

> The Subsection [Effect of sample size..](#effect-sample-size) provides an example that illustrates in more detail the effect of sample size on the statistical analysis of the model. \hfill $\lozenge$

On the other hand, considering the hypothetical orthogonal model, the value of the experimental statistic of the individual significance test, whose null hypothesis is $\beta_{i} = 0$ in face of the alternative hypothesis $\beta_{i} \not= 0$ with $i=2,\dots,k$, is given by:
\begin{equation}
    t_{i}^{o} = \left| \frac{\widehat{\beta}_{i}}{\sqrt{\frac{\widehat{\sigma}^{2}}{n \cdot var(\mathbf{X}_{i})}}} \right|, \quad i=2,\dots,k,
    (\#eq:texp-orto-1)
\end{equation}
where the estimated variance of the estimator has been diminished due to the VIF always being greater than or equal to 1, and consequently,  $t_{i}^{o} \geq t_{i}$. However, it has been assumed that the same estimates for the independent variable coefficients and random disturbance variance are obtained in the orthogonal and original models, which does not seem to be a plausible supposition (see @Salmeron2024a Section 2.1 for more details).

## An orthonormal reference model {#sub-above}

In @Salmeron2024a the following QR decomposition of the matrix $\mathbf{X}_{n \times k}$ of the model \@ref(eq:model0) is proposed: $\mathbf{X} = \mathbf{X}_{o} \cdot \mathbf{P}$, where $\mathbf{X}_{o}$ is an orthonormal matrix of the same dimensions as $\mathbf{X}$ and $\mathbf{P}$ is a higher-order triangular matrix of dimensions $k \times k$. Then, the following hypothetical orthonormal reference  model:
\begin{equation}
    \mathbf{y} = \mathbf{X}_{o} \cdot \boldsymbol{\beta}_{o} + \mathbf{w},
    (\#eq:model-ref)
\end{equation}
verifies that:
$$\widehat{\boldsymbol{\beta}} = \mathbf{P}^{-1} \cdot \widehat{\boldsymbol{\beta}}_{o}, \
    \mathbf{e} = \mathbf{e}_{o}, \
    var \left( \widehat{\boldsymbol{\beta}}_{o} \right) =  \sigma^{2} \cdot \mathbf{I},$$
where $\mathbf{e}_{o}$ are the residuals of the  orthonormal reference model \@ref(eq:model-ref).
Note that since $\mathbf{e} = \mathbf{e}_{o}$, the estimate of $\sigma^{2}$ is the same in the original model \@ref(eq:model0) and in the orthonormal  reference model \@ref(eq:model-ref).
Moreover, since the dependent variable is the same in both models, the coefficient of determination and the experimental value of the global significance test are the same in both cases.

From these values, taking into account the expressions \@ref(eq:vari-VIF) and \@ref(eq:vari-VIF2), it is evident that the ratio between the variance of the estimator in the original model \@ref(eq:model0) and the variance of the estimator of the orthonormal reference model \@ref(eq:model-ref) is:
\begin{equation}
    \frac{
        var \left( \widehat{\beta}_{i} \right)
    }{
        var \left( \widehat{\beta}_{i,o} \right)
    } = \frac{VIF(i)}{n \cdot var(\mathbf{X}_{i})}, \quad i=2,\dots,k.
    (\#eq:redef-VIF) \nonumber
\end{equation}
Consequently, @Salmeron2024a defined the redefined VIF (RVIF) for $i=1,\dots,k$ as:
\begin{equation}\small{
    RVIF(i) = \frac{VIF(i)}{n \cdot var(\mathbf{X}_{i})} = \frac{\mathbf{X}_{i}^{t} \mathbf{X}_{i}}{\mathbf{X}_{i}^{t} \mathbf{X}_{i} - \mathbf{X}_{i}^{t} \mathbf{X}_{-i} \cdot \left( \mathbf{X}_{-i}^{t} \mathbf{X}_{-i} \right)^{-1} \cdot \mathbf{X}_{-i}^{t} \mathbf{X}_{i}}, (\#eq:RVIF)}
\end{equation}
which shows, among other questions, that it is defined for $i=1,2,\dots,k$. That is, in contrast to the VIF, the RVIF can be calculated for the intercept of the linear regression model.

Other considerations to be taken into account are the following:

- If the data are expressed in unit length, same transformation used to calculate the Condition Number (CN), then:
$$RVIF(i) = \frac{1}{1 - \mathbf{X}_{i}^{t} \mathbf{X}_{-i} \cdot \left( \mathbf{X}_{-i}^{t} \mathbf{X}_{-i} \right)^{-1} \cdot \mathbf{X}_{-i}^{t} \mathbf{X}_{i}}, \quad i=1,\dots,k.$$
- In this case (data expressed in unit length), when $\mathbf{X}_{i}$ is orthogonal to $\mathbf{X}_{-i}$, it is verified that $\mathbf{X}_{i}^{t} \mathbf{X}_{-i} = \mathbf{0}$ and, consequently $RVIF(i) = 1$ for $i=1,\dots,k$. That is, the RVIF is always greater than or equal to 1 and its minimum value is indicative of the absence of multicollinearity.

- Denoted by $a_{i}= \mathbf{X}_{i}^{t} \mathbf{X}_{i} \cdot \left( \mathbf{X}_{-i}^{t} \mathbf{X}_{-i} \right)^{-1} \cdot \mathbf{X}_{-i}^{t} \mathbf{X}_{i}$, it is verified that $RVIF(i) = \frac{1}{1-a_{i}}$ where $a_{i}$ can be interpreted as the percentage of approximate multicollinearity due to variable $\mathbf{X}_{i}$. Note the similarity of this expression to that of the VIF: $VIF(i) = \frac{1}{1-R_{i}^{2}}$ (see equation \@ref(eq:vari-VIF)).

- Finally, from a simulation for $k=3$, @Salmeron2024a show that if $a_{i} > 0.826$, then the degree of multicollinearity is worrying. In any case this value should be refined by considering higher values of $k$.

On the other hand, given the orthonormal reference model \@ref(eq:model-ref), the value for the experimental statistic of the individual significance test with the null hypothesis $\beta_{i,o} = 0$ (given the alternative hypothesis $\beta_{i,o} \not= 0$, for $i=1,\dots,k$) is:
\begin{equation}
    t_{i}^{o} = \left| \frac{\widehat{\beta}_{i,o}}{\widehat{\sigma}} \right| = \left| \frac{\mathbf{p}_{i} \cdot \widehat{\boldsymbol{\beta}}}{\widehat{\sigma}} \right|,
    (\#eq:texp-orto-2)
\end{equation}
where $\mathbf{p}_{i}$ is the $i$ row of the matrix $\mathbf{P}$.

By comparing this expression with the one given in \@ref(eq:texp-orto-1), it is observed that, as expected, not only the denominator but also the numerator has changed.
Thus, in addition to the VIF, the rest of the elements in expression \@ref(eq:texp-orig) have also changed.
Consequently, if the null hypothesis is rejected in the original model, it is not assured that the same will occur in the orthonormal reference model. For this reason, it is possible to consider that the orthonormal model proposed as the reference model in @Salmeron2024a is more plausible than the one traditionally applied.

## Possible scenarios in the individual significance tests

To determine whether the tendency not to reject the null hypothesis in the individual significance test is caused by a troubling approximate multicollinearity that inflates the variance of the estimator, or whether it is caused by variables not being statistically significantly related, the following situations are distinguished with a significance level $\alpha$:

a. If the null hypothesis is initially rejected in the original model \@ref(eq:model0), $t_{i} > t_{n-k}(1-\alpha/2)$, the following results can be obtained for the orthonormal model:

a.1. the null hypothesis is rejected, $t_{i}^{o} > t_{n-k}(1-\alpha/2)$; then, the results are consistent.

a.2. the null hypothesis is not rejected, $t_{i}^{o} < t_{n-k}(1-\alpha/2)$; this could be an inconsistency.

b. If the null hypothesis is not initially rejected in the original model \@ref(eq:model0), $t_{i} < t_{n-k}(1-\alpha/2)$, the following results may occur for the orthonormal model:

b.1 the null hypothesis is rejected, $t_{i}^{o} > t_{n-k}(1-\alpha/2)$; then, it is possible to conclude that the degree of multicollinearity affects the statistical analysis of the model, provoking not rejecting the null hypothesis in the original model.

b.2 the null hypothesis is also not rejected, $t_{i}^{o} < t_{n-k}(1-\alpha/2)$; then, the results are consistent.

In conclusion, when option b.1 is given, the null hypothesis of the individual significance test is not rejected  when the linear relationships are considered (original model) but is rejected when the linear relationships are not considered (orthonormal model). Consequently, it is possible to conclude that the linear relationships affect the statistical analysis of the model. The possible inconsistency discussed in option a.2 is analyzed in detail in Appendix [Inconsistency](#inconsistency), concluding that it will rarely occur in cases where a high degree of multicollinearity is assumed. The other two scenarios provide consistent situations.

# A first attempt to obtain a non-rejection region associated with a statistical test to detect multicollinearity {#modelo-orto}

## From the traditional orthogonal model

Considering the expressions \@ref(eq:texp-orig) and \@ref(eq:texp-orto-1), it is verified that $t_{i}^{o} = t_{i} \cdot \sqrt{VIF(i)}$. Consequently, in the orthogonal case, with a significance level $\alpha$, the null hypothesis $\beta_{i,o} = 0$ is rejected if $t_{i}^{o} > t_{n-k}(1-\alpha/2)$ for $i=2,\dots,k.$ That is, if:
\begin{equation}
    VIF(i) > \left( \frac{t_{n-k}(1-\alpha/2)}{t_{i}} \right)^{2} = c_{1}(i), \quad i=2,\dots,k.
    (\#eq:cond-false)
\end{equation}
Thus, if the VIF associated with the variable $i$ is greater than the upper bound $c_{1}(i)$, then it can be concluded that the estimator of the coefficient of that variable is significantly different from zero in the hypothetical case where the variables are orthogonal. In addition, if the null hypothesis is not rejected in the initial model, the reason for the failure to reject could be due to the degree of multicollinearity that affects the statistical analysis of the model.

Finally, note that since the interesting cases are those where the null hypothesis is not initially rejected,  $t_{i} < t_{n-k}(1-\alpha/2)$, the upper bound $c_{1}(i)$  will always be greater than one.

> Example 2.
> Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:WisseltableHTML)', '\\@ref(tab:WisseltableLATEX)'))` shows a dataset (previously presented by @Wissell) with the following variables: outstanding mortgage debt ($\mathbf{D}$, trillions of dollars), personal consumption ($\mathbf{C}$, trillions of dollars), personal income ($\mathbf{I}$, trillions of dollars) and outstanding consumer credit ($\mathbf{CP}$, trillions of dollars) for the years 1996 to 2012. 

```{r WisseltableHTML, eval = knitr::is_html_output()}
WisselTABLE = Wissel[,-3]
knitr::kable(WisselTABLE, format = "html", caption = "Data set presented previously by @Wissell", align="cccccc", digits = 3) 
```

```{r WisseltableLATEX, eval = knitr::is_latex_output()}
WisselTABLE = Wissel[,-3]
knitr::kable(WisselTABLE, format = "latex", booktabs = TRUE, caption = "Data set presented previously by Wissell", align="cccccc", digits = 3)%>%
kable_styling(latex_options = "scale_down")
```   

> Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:Wissel0tableHTML)', '\\@ref(tab:Wissel0tableLATEX)'))` shows the OLS estimation of the model explaining the outstanding mortgage debt as a function of the rest of the variables. That is:
 $$\mathbf{D} = \beta_{1} + \beta_{2} \cdot \mathbf{C} + \beta_{3} \cdot \mathbf{I} + \beta_{4} \cdot \mathbf{CP} + \mathbf{u}.$$
    Note that the estimates for the coefficients of personal consumption, personal income and outstanding consumer credit are not significantly different from zero (a significance level of 5\% is considered throughout the paper), while the model is considered to be globally valid (experimental value, F exp., higher than theoretical value).

```{r Wisselregression}
attach(Wissel)
obs = nrow(Wissel)
regWISSEL0 = lm(D~C+I+CP)
  regWISSEL0coef = as.double(regWISSEL0$coefficients)
  regWISSEL0se = as.double(summary(regWISSEL0)[[4]][,2])
  regWISSEL0texp = as.double(summary(regWISSEL0)[[4]][,3]) 
  regWISSEL0pvalue = as.double(summary(regWISSEL0)[[4]][,4]) 
  regWISSEL0sigma2 = as.double(summary(regWISSEL0)[[6]]^2)
  regWISSEL0R2 = as.double(summary(regWISSEL0)[[8]]) 
  regWISSEL0Fexp = as.double(summary(regWISSEL0)[[10]][[1]])
  regWISSEL0table = data.frame(c(regWISSEL0coef, obs), 
                               c(regWISSEL0se, regWISSEL0sigma2), 
                               c(regWISSEL0texp, regWISSEL0R2), 
                               c(regWISSEL0pvalue, regWISSEL0Fexp))
  regWISSEL0table = round(regWISSEL0table, digits=4)
  colnames(regWISSEL0table) =c("Estimator", "Standard Error", "Experimental t", "p-value")
  rownames(regWISSEL0table) =c("Intercept", "Personal consumption", "Personal income", "Outstanding consumer credit", "(Obs, Sigma Est., Coef. Det., F exp.)")
regWISSEL1 = lm(D~C)
  regWISSEL1coef = as.double(regWISSEL1$coefficients)
  regWISSEL1se = as.double(summary(regWISSEL1)[[4]][,2])
  regWISSEL1texp = as.double(summary(regWISSEL1)[[4]][,3]) 
  regWISSEL1pvalue = as.double(summary(regWISSEL1)[[4]][,4]) 
  regWISSEL1sigma2 = as.double(summary(regWISSEL1)[[6]]^2)
  regWISSEL1R2 = as.double(summary(regWISSEL1)[[8]]) 
  regWISSEL1Fexp = as.double(summary(regWISSEL1)[[10]][[1]])
  regWISSEL1table = data.frame(c(regWISSEL1coef, obs), 
                               c(regWISSEL1se, regWISSEL1sigma2), 
                               c(regWISSEL1texp, regWISSEL1R2), 
                               c(regWISSEL1pvalue, regWISSEL1Fexp))
  regWISSEL1table = round(regWISSEL1table, digits=4)
  colnames(regWISSEL1table) =c("Estimator", "Standard Error", "Experimental t", "p-value")
  rownames(regWISSEL1table) =c("Intercept", "Personal consumption", "(Obs, Sigma Est., Coef. Det., F exp.)")
regWISSEL2 = lm(D~C+I)
  regWISSEL2coef = as.double(regWISSEL2$coefficients)
  regWISSEL2se = as.double(summary(regWISSEL2)[[4]][,2])
  regWISSEL2texp = as.double(summary(regWISSEL2)[[4]][,3]) 
  regWISSEL2pvalue = as.double(summary(regWISSEL2)[[4]][,4]) 
  regWISSEL2sigma2 = as.double(summary(regWISSEL2)[[6]]^2)
  regWISSEL2R2 = as.double(summary(regWISSEL2)[[8]]) 
  regWISSEL2Fexp = as.double(summary(regWISSEL2)[[10]][[1]])
  regWISSEL2table = data.frame(c(regWISSEL2coef, obs), 
                               c(regWISSEL2se, regWISSEL2sigma2), 
                               c(regWISSEL2texp, regWISSEL2R2), 
                               c(regWISSEL2pvalue, regWISSEL2Fexp))
  regWISSEL2table = round(regWISSEL2table, digits=4)
  colnames(regWISSEL2table) =c("Estimator", "Standard Error", "Experimental t", "p-value")
  rownames(regWISSEL2table) =c("Intercept", "Personal consumption", "Personal income", "(Obs, Sigma Est., Coef. Det., F exp.)")
regWISSEL3 = lm(D~C+CP)
  regWISSEL3coef = as.double(regWISSEL3$coefficients)
  regWISSEL3se = as.double(summary(regWISSEL3)[[4]][,2])
  regWISSEL3texp = as.double(summary(regWISSEL3)[[4]][,3]) 
  regWISSEL3pvalue = as.double(summary(regWISSEL3)[[4]][,4]) 
  regWISSEL3sigma2 = as.double(summary(regWISSEL3)[[6]]^2)
  regWISSEL3R2 = as.double(summary(regWISSEL3)[[8]]) 
  regWISSEL3Fexp = as.double(summary(regWISSEL3)[[10]][[1]])
  regWISSEL3table = data.frame(c(regWISSEL3coef, obs), 
                               c(regWISSEL3se, regWISSEL3sigma2), 
                               c(regWISSEL3texp, regWISSEL3R2), 
                               c(regWISSEL3pvalue, regWISSEL3Fexp))
  regWISSEL3table = round(regWISSEL3table, digits=4)
  colnames(regWISSEL3table) =c("Estimator", "Standard Error", "Experimental t", "p-value")
  rownames(regWISSEL3table) =c("Intercept", "Personal consumption", "Outstanding consumer credit", "(Obs, Sigma Est., Coef. Det., F exp.)")
```

```{r Wissel0tableHTML, eval = knitr::is_html_output()}
knitr::kable(regWISSEL0table, format = "html", caption = "OLS estimation for the Wissel model", align="cccc", digits = 3) 
```

```{r Wissel0tableLATEX, eval = knitr::is_latex_output()}
knitr::kable(regWISSEL0table, format = "latex", booktabs = TRUE, caption = "OLS estimation for the Wissel model", align="cccc", digits = 3) %>%
kable_styling(latex_options = "scale_down")
```   

>In addition, the estimated coefficient for the variable personal consumption, which is not significantly different from zero, has the opposite sign to the simple correlation coefficient between this variable and outstanding mortgage debt, 0.953.
    Thus, in the simple linear regression between both variables (see Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:Wissel1tableHTML)', '\\@ref(tab:Wissel1tableLATEX)'))`), the estimated coefficient of the variable personal consumption is positive and significantly different from zero. However, adding a second variable (see Tables `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:Wissel2tableHTML)', '\\@ref(tab:Wissel2tableLATEX)'))` and `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:Wissel3tableHTML)', '\\@ref(tab:Wissel3tableLATEX)'))`) none of the coefficients are individually significantly different from zero although both models are globally significant.
    This is traditionally understood as a symptom of statistically troubling multicollinearity.

```{r Wissel1tableHTML, eval = knitr::is_html_output()}
knitr::kable(regWISSEL1table, format = "html", caption = "OLS estimation for part of the Wissel model", align="cccc", digits = 3) 
```

```{r Wissel1tableLATEX, eval = knitr::is_latex_output()}
knitr::kable(regWISSEL1table, format = "latex", booktabs = TRUE, caption = "OLS estimation for part of the Wissel model", align="cccc", digits = 3) %>%
kable_styling(latex_options = "scale_down")
```   

```{r Wissel2tableHTML, eval = knitr::is_html_output()}
knitr::kable(regWISSEL2table, format = "html", caption = "OLS estimation for part of the Wissel model", align="cccc", digits = 3) 
```

```{r Wissel2tableLATEX, eval = knitr::is_latex_output()}
knitr::kable(regWISSEL2table, format = "latex", booktabs = TRUE, caption = "OLS estimation for part of the Wissel model", align="cccc", digits = 3) %>%
kable_styling(latex_options = "scale_down")
```   

```{r Wissel3tableHTML, eval = knitr::is_html_output()}
knitr::kable(regWISSEL3table, format = "html", caption = "OLS estimation for part of the Wissel model", align="cccc", digits = 3) 
```

```{r Wissel3tableLATEX, eval = knitr::is_latex_output()}
knitr::kable(regWISSEL3table, format = "latex", booktabs = TRUE, caption = "OLS estimation for part of the Wissel model", align="cccc", digits = 3)%>%
kable_styling(latex_options = "scale_down")
```   

>By using expression \@ref(eq:cond-false) in order to confirm this problem, it is verified that $c_{1}(2) = 6.807$, $c_{1}(3) = 1.985$ and $c_{1}(4) = 18.743$, taking into account that $t_{13}(0.975) = 2.160$. Since the VIFs are equal to 589.754, 281.886 and 189.487, respectively, it is concluded that the individual significance tests for the three cases are affected by the degree of multicollinearity existing in the model.  \hfill $\lozenge$

## From the alternative orthonormal model \@ref(eq:model-ref)

In the Subsection [An orthonormal reference model](#sub-above) the individual significance test from the expression \@ref(eq:texp-orto-2) is redefined. Thus, the null hypothesis $\beta_{i,o}=0$ will be rejected, with a significance level $\alpha$, if the following condition is verified:
$$t_{i}^{o} > t_{n-k}(1-\alpha/2), \quad i=2,\dots,k.$$
Taking into account the expressions \@ref(eq:texp-orig) and \@ref(eq:texp-orto-2), this is equivalent to:
\begin{equation}\small{
    VIF(i) > \left( \frac{t_{n-k}(1-\alpha/2)}{\widehat{\beta}_{i,o}} \right)^{2} \cdot \widehat{var} \left( \widehat{\beta}_{i} \right) \cdot n \cdot var(\mathbf{X}_{i}) = c_{2}(i). (\#eq:cota-VIF-orto)}
\end{equation}

Thus, if the $VIF(i)$ is greater than $c_{2}(i)$, the null hypothesis is rejected in the respective individual significance tests in the orthonormal model (with $i=2,\dots,k$). Then, if the null hypothesis is not rejected in the original model and it is verified that $VIF(i) > c_{2}(i)$, it can be concluded that the multicollinearity existing in the model affects its statistical analysis. In summary, a lower bound for the VIF is established to indicate when the approximate multicollinearity is troubling in a way that can be reinterpreted and presented as a region of non-rejection of a statistical test.

> Example 3.
> Continuing with the dataset presented by @Wissell, Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:WisselORTOtableHTML)', '\\@ref(tab:WisselORTOtableLATEX)'))` shows the results of the OLS estimation of the orthonormal model obtained from the original model. 

```{r WisselORTO}
y = Wissel[,2]
X = as.matrix(Wissel[,3:6])
Xqr=qr(X)
Xo = qr.Q(Xqr)
regORTO = lm(y~Xo+0)
#summary(regORTO)
  regORTOcoef = as.double(regORTO$coefficients)
  regORTOse = as.double(summary(regORTO)[[4]][,2])
  regORTOtexp = as.double(summary(regORTO)[[4]][,3]) 
  regORTOpvalue = as.double(summary(regORTO)[[4]][,4]) 
  regORTOsigma2 = as.double(summary(regORTO)[[6]]^2)
  regORTOR2 = as.double(summary(regORTO)[[8]]) # as I have removed the intercept in the regression, this does not calculate it well
  regORTOFexp = as.double(summary(regORTO)[[10]][[1]]) # as I have removed the intercept in the regression, this does not calculate it well
  regORTOtable = data.frame(c(regORTOcoef, obs), 
                               c(regORTOse, regORTOsigma2), 
                               c(regORTOtexp, 0.9235), 
                               c(regORTOpvalue, 52.3047))
  regORTOtable = round(regORTOtable, digits=4)
  colnames(regORTOtable) =c("Estimator", "Standard Error", "Experimental t", "p-value")
  rownames(regORTOtable) =c("Intercept", "Personal consumption", "Personal income", "Outstanding consumer credit", "(Obs, Sigma Est., Coef. Det., F exp.)")
```

```{r WisselORTOtableHTML, eval = knitr::is_html_output()}
knitr::kable(regORTOtable, format = "html", caption = "OLS estimation for the orthonormal Wissel model", align="cccc", digits = 3) 
```

```{r WisselORTOtableLATEX, eval = knitr::is_latex_output()}
knitr::kable(regORTOtable, format = "latex", booktabs = TRUE, caption = "OLS estimation for the orthonormal Wissel model", align="cccc", digits = 3)%>%
kable_styling(latex_options = "scale_down") 
```  

>When these results are compared with those in Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:Wissel0tableHTML)', '\\@ref(tab:Wissel0tableLATEX)'))`, the following conclusions can be obtained:
>
- Except for the outstanding consumer credit variable, whose standard deviation has increased, the standard deviation has decreased in all cases.
>
- The absolute values of the experimental statistics of the individual significance tests associated with the intercept and the personal consumption variable have increased, while the experimental statistic of the personal income variable has decreased, and the experimental statistic of the outstanding consumer credit variable remains the same. These facts show that the change from the original model to the orthonormal model does not guarantee an increase in the absolute value of the experimental statistic.
>
- The estimation of the coefficient of the personal consumption variable is not significantly different from zero in the original model, but it is in the orthogonal model. Thus, it is concluded that multicollinearity affects the statistical analysis of the model. Note that there is also a change in the sign of the estimate, although the purpose of the orthogonal model is not to obtain estimates for the coefficients, but rather to provide a reference point against which to measure how much the variances are inflated. Note that an orthonormal model is an idealized construction that may lack a proper interpretation in practice.
>
- The values corresponding to the estimated variance for the random disturbance, the coefficient of determination and the experimental statistic (F exp.) for the global significance test remain the same.

>On the other hand, considering the VIF of the independent variables except for the intercept (589.754, 281.886 and 189.487) and their corresponding bounds (17.809, 623.127 and 3545.167) obtained from the expression \@ref(eq:cota-VIF-orto), only the variable of personal consumption verifies that the VIF is higher than the corresponding bound. These results are different from those obtained in Example 2, where the traditional orthogonal model was taken as a reference.

>Finally, Tables `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:Wissel0tableHTML)', '\\@ref(tab:Wissel0tableLATEX)'))` and `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:WisselORTOtableHTML)', '\\@ref(tab:WisselORTOtableLATEX)'))` show that the experimental values of the statistic $t$ of the variable outstanding consumer credit are the same in the original and orthonormal models. \hfill $\lozenge$

The last fact highlighted at the end of the previous example is not a coincidence, but a consequence of the QR decomposition, see Appendix [Test of...](#apendix). Therefore, in this case, the conclusion of the individual significance test will be the same in the original and in the orthonormal model, i.e. we will always be in scenarios a.1 or b.2.

Thus, this behavior establishes a situation where it is required to select the variable fixed in the last position. Some criteria to select the most appropriate variable for this placement could be:

- To fix the variable that is considered less relevant to the model. 

- To fix a variable whose associated coefficient is significantly different from zero, since this case would not be of interest for the definition of multicollinearity given in the paper. Note that the interest will be related to a coefficient considered as zero in the original model and significantly different from zero in the orthonormal one.

These options are explored in the Subsection [Choice of the variable to be fix...](#how-to-fix).

# A non-rejection region associated with a statistical test to detect multicollinearity {#new-VIF-orto}

@Salmeron2024a show that high values of RVIF are associated with a high degree of multicollinearity. The question, however, is how high RVIFs have to be to reflect troubling multicollinearity.

Taking into account the expressions \@ref(eq:RVIF) and \@ref(eq:cota-VIF-orto), it is possible to conclude that multicollinearity is affecting the statistical analysis of the model if it can be verified that:
\begin{equation}
    RVIF(i) > \left( \frac{t_{n-k}(1-\alpha/2)}{\widehat{\beta}_{i,o}} \right)^{2} \cdot \widehat{var} \left( \widehat{\beta}_{i} \right) = c_{3}(i),
    (\#eq:cota-VIFR)
\end{equation}
for any $i=1,\dots,k$. Note that the intercept is included in this proposal, in contrast to the previous section, in which it was not included.

By following @OBrien and taking into account that the estimation of the expression \@ref(eq:vari-VIF) can be expressed as:
$$\widehat{var} \left( \widehat{\beta}_{i} \right) = \widehat{\sigma}^{2} \cdot RVIF(i) = \frac{\mathbf{e}^{t}\mathbf{e}}{n-k} \cdot RVIF(i),$$
there are other factors that counterbalance a high value of RVIF, thereby avoiding high estimated variances for the estimated coefficients. These factors are the  sum of the squared residuals (SSR= $\mathbf{e}^{t}\mathbf{e}$) of the model \@ref(eq:model0) and $n$. Thus, an appropriate specification of the econometric model (i.e., one that implies a good fit and, consequently, a small SSR) and a large sample size can compensate for high RVIF values.
However, contrary to what happens for the VIF in the traditional case, these factors are taken into account in the threshold $c_{3}(i)$, as established in the expression \@ref(eq:cota-VIFR) in $\widehat{var} \left( \widehat{\beta}_{i} \right)$.

> Example 4.
> This contribution can be illustrated with the data set previously presented by @KleinGoldberger, which includes variables for consumption, $\mathbf{C}$, wage incomes, $\mathbf{I}$, non-farm incomes, $\mathbf{InA}$, and farm incomes, $\mathbf{IA}$, in United States from 1936 to 1952, as shown in Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:KGtableHTML)', '\\@ref(tab:KGtableLATEX)'))` (data from 1942 to 1944 are not available because they were war years).

```{r KGtableHTML, eval = knitr::is_html_output()}
data(KG)
KGtable = KG
colnames(KGtable) = c("Consumption", "Wage income", "Non-farm income", "Farm income")
knitr::kable(KGtable, format = "html", caption = "Data set presented previously by @KleinGoldberger", align="cccc", digits = 3) 
```

```{r KGtableLATEX, eval = knitr::is_latex_output()}
data(KG)
KGtable = KG
colnames(KGtable) = c("Consumption", "Wage income", "Non-farm income", "Farm income")
knitr::kable(KGtable, format = "latex", booktabs = TRUE, caption = "Data set presented previously by Klein and Goldberger", align="cccc", digits = 3)%>%
kable_styling(latex_options = "scale_down")
```   

> Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:regKGtableHTML)', '\\@ref(tab:regKGtableLATEX)'))` shows the OLS estimations of the model explaining consumption as a function of the rest of the variables. Note that there is some incoherence between the individual significance values of the variables and the global significance of the model.

```{r KGregression}
attach(KG)
obs = nrow(KG)
regKG = lm(consumption~wage.income+non.farm.income+farm.income)
  regKGcoef = as.double(regKG$coefficients)
  regKGse = as.double(summary(regKG)[[4]][,2])
  regKGtexp = as.double(summary(regKG)[[4]][,3]) 
  regKGpvalue = as.double(summary(regKG)[[4]][,4]) 
  regKGsigma2 = as.double(summary(regKG)[[6]]^2)
  regKGR2 = as.double(summary(regKG)[[8]]) 
  regKGFexp = as.double(summary(regKG)[[10]][[1]])
  regKGtable = data.frame(c(regKGcoef, obs), 
                               c(regKGse, regKGsigma2), 
                               c(regKGtexp, regKGR2), 
                               c(regKGpvalue, regKGFexp))
  regKGtable = round(regKGtable, digits=4)
  colnames(regKGtable) =c("Estimator", "Standard Error", "Experimental t", "p-value")
  rownames(regKGtable) =c("Intercept", "Wage income", "Non-farm income", "Farm income", "(Obs, Sigma Est., Coef. Det., F exp.)")
```

```{r regKGtableHTML, eval = knitr::is_html_output()}
knitr::kable(regKGtable, format = "html", caption = "OLS estimation for the Klein and Goldberger model", align="cccc", digits = 3) 
```

```{r regKGtableLATEX, eval = knitr::is_latex_output()}
knitr::kable(regKGtable, format = "latex", booktabs = TRUE, caption = "OLS estimation for the Klein and Goldberger model", align="cccc", digits = 3)%>%
kable_styling(latex_options = "scale_down")
```   

>The RVIFs are calculated, yielding 1.275, 0.002, 0.014 and 0.053, respectively. The associated bounds, $c_{3}(i)$, are also calculated, yielding 0.002, 0.0001, 0.018 and 1.826, respectively.

>Since the coefficient of the wage income variable is not significantly different from zero, and because it is verified that $0.002 > 0.0001$, from \@ref(eq:cota-VIFR) it is concluded that the degree of multicollinearity existing in the model is affecting its statistical analysis.
\hfill $\lozenge$

## From the RVIF {#TheTHEOREM}

Considering that in the original model \@ref(eq:model0) the null hypothesis $\beta_{i} = 0$ of the individual significance test is not rejected if:
$$RVIF(i) > \left( \frac{\widehat{\beta}_{i}}{\widehat{\sigma} \cdot t_{n-k}(1-\alpha/2)} \right)^{2} = c_{0}(i), \quad i=1,\dots,k,$$
while in the orthonormal model, the null hypothesis is rejected if $RVIF(i) > c_{3}(i)$, the following theorem can be established:

> Theorem. Given the multiple linear regression model \@ref(eq:model0), the degree of multicollinearity affects its statistical analysis (with a level of significance of $\alpha\%$) if there is a variable $i$, with $i=1,\dots,k$, that verifies $RVIF(i) > \max \{ c_{0}(i), c_{3}(i) \}$.

Note that @Salmeron2024a indicate that the RVIF must be calculated with unit length data (as any other transformation removes the intercept from the analysis), however, for the correct application of this theorem the original data must be used as no transformation has been considered in this paper. 

> Example 5. Tables `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:theoremWISSELtableHTML)', '\\@ref(tab:theoremWISSELtableLATEX)'))` and `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:theoremKGtableHTML)', '\\@ref(tab:theoremKGtableLATEX)'))` present the results of applying the theorem to the @Wissell and @KleinGoldberger models, respectively.  Note that in both cases, there is a variable $i$ that verifies that $RVIF(i) > \max \{ c_{0}(i), c_{3}(i) \}$, and consequently, we can conclude that the degree of approximate multicollinearity is affecting the statistical analysis in both models (with a level of significance of $5\%$). \hfill $\lozenge$

```{r THEOREM}
y = Wissel[,2]
X = as.matrix(Wissel[,3:6])
theoremWISSEL = multicollinearity(y, X)
rownames(theoremWISSEL) = c("Intercept", "Personal consumption", "Personal income", "Outstanding consumer credit")

y = KG[,1]
cte = rep(1, length(y))
X = as.matrix(cbind(cte, KG[,-1]))
theoremKG = multicollinearity(y, X)
rownames(theoremKG) = c("Intercept", "Wage income", "Non-farm income", "Farm income")
```

```{r theoremWISSELtableHTML, eval = knitr::is_html_output()}
knitr::kable(theoremWISSEL, format = "html", caption = "Theorem results of the Wissel model", align="ccccc", digits = 6) 
```

```{r theoremWISSELtableLATEX, eval = knitr::is_latex_output()}
knitr::kable(theoremWISSEL, format = "latex", booktabs = TRUE, caption = "Theorem results of the Wissel model", align="ccccc", digits = 6) %>%
kable_styling(latex_options = "scale_down")
``` 

```{r theoremKGtableHTML, eval = knitr::is_html_output()}
knitr::kable(theoremKG, format = "html", caption = "Theorem results of the Klein and Goldberger model", align="ccccc", digits = 6) 
```

```{r theoremKGtableLATEX, eval = knitr::is_latex_output()}
knitr::kable(theoremKG, format = "latex", booktabs = TRUE, caption = "Theorem results of the Klein and Goldberger model", align="ccccc", digits = 6) %>%
kable_styling(latex_options = "scale_down")
``` 

# The rvif package {#paqueteRVIF}

The results developed in @Salmeron2024a and in this paper have been implemented in the \CRANpkg{rvif} package of R (@R). The following shows how to replicate the results presented in both papers from the existing commands $\texttt{rvifs}$ and $\texttt{multicollinearity}$ in \CRANpkg{rvif}. For this reason, the  code executed is shown below.

In addition, the following issues will be addressed:

- Discussion on the effect of sample size in detecting the influence of multicollinearity on the statistical analysis of the model.

- Discussion on the choice of the variable to be fixed as the last one  before the orthonormalization.

The code used in these two Subsections is available at <https://github.com/rnoremlas/RVIF/tree/main/rvif%20package>. 
It is also interesting to consult the package vignette using the command `browseVignettes("rvif")`, as well as its web page with `browseURL(system.file("docs/index.html", package = "rvif"))` or <https://www.ugr.es/local/romansg/rvif/index.html>.

## Detection of multicollinearity with RVIF: does the degree of multicollinearity affect the statistical analysis of the model?

In @Salmeron2024a a series of examples are presented to illustrate the usefulness of RVIF to detect the degree of approximate multicollinearity in a multiple linear regression model. 
Results presented by @Salmeron2024a will be reproduced by using the command $\texttt{rvifs}$ of \CRANpkg{rvif} package and complemented with the contribution developed in the present work by using the command $\texttt{multicollinearity}$ of the same package.
In order to facilitate the reading of the paper, this information is available in Appendix [Examples of...](#examplesRVIF).

On the other hand, the following shows how to use the above commands to obtain the results shown in Table 9 of this paper:

```{r PAPER13, echo=TRUE}
y_W = Wissel[,2]
X_W = Wissel[,3:6]
multicollinearity(y_W, X_W)
```

It is noted that the first two arguments of the $\texttt{multicollinearity}$ command are, respectively, the dependent variable of the linear model and the design matrix containing the independent variables (intercept included as the first column).

While the results in Table 10 can be obtained using this code:

```{r PAPER14, echo=TRUE}
y_KG = KG[,1]
cte = rep(1, length(y))
X_KG = cbind(cte, KG[,2:4])
multicollinearity(y_KG, X_KG)
```

As is known, in both cases it is concluded that the degree of multicollinearity in the model affects its statistical analysis.

The $\texttt{multicollinearity}$ command is used by default with a significance level of 5% for the application of the Theorem set in Subsection [From the RVIF](#TheTHEOREM). 
Note that if the significance level is changed to 1% (third argument of the $\texttt{multicollinearity}$ command), in the Klein and Goldberger model it is obtained that the individual significance test of the intercept is also affected by the degree of existing multicollinearity:

```{r PAPER15, echo=TRUE}
multicollinearity(y_W, X_W, alpha = 0.01)
multicollinearity(y_KG, X_KG, alpha = 0.01)
```

It can be seen that the values of $c_{0}$ and $c_{3}$ change depending on the significance level used.

## Effect of the sample size on the detection of the influence of multicollinearity on the statistical analysis of the model {#effect-sample-size}

The introduction has highlighted the idea that the measures traditionally used to detect whether the degree of multicollinearity is of concern may indicate that it is troubling while the model analysis is not affected by it. Example 1 shows that this may be due, among other factors, to the size of the sample.

To explore this issue in more detail, below is given an example where traditional measures of multicollinearity detection indicate that the existing multicollinearity is troubling while the statistical analysis of the model is not affected when the sample size is high. In particular, observations are simulated for $\mathbf{X} = [ \mathbf{1} \ \mathbf{X}_{2} \ \mathbf{X}_{3} \ \mathbf{X}_{4} \ \mathbf{X}_{5} \ \mathbf{X}_{6}]$ where:
$$\mathbf{X}_{2} \sim N(5, 0.1^{2}), \quad \mathbf{X}_{3} \sim N(5, 10^{2}), \quad \mathbf{X}_{4} = \mathbf{X}_{3} + \mathbf{p}$$ 
$$\mathbf{X}_{5} \sim N(-1, 3^{2}), \quad \mathbf{X}_{6} \sim N(15, 2.5^{2}),$$
where $\mathbf{p} \sim N(5, 0.5^2)$ and considering three different sample sizes: $n = 3000$ (Simulation 1), $n = 100$ (Simulation 2) and $n = 30$ (Simulation 3).
In all cases the dependent variable is generated according to:
$$\mathbf{y} = 4 + 5 \cdot \mathbf{X}_{2} - 9 \cdot \mathbf{X}_{3} - 2 \cdot \mathbf{X}_{4} + 2 \cdot \mathbf{X}_{5} + 7 \cdot \mathbf{X}_{6} + \mathbf{u},$$
where $\mathbf{u} \sim N(0, 2^2)$.

To set the results, a seed has been established using the command *set.seed(2024)*.

```{r SAMPLE-SIZE 1}
## Simulation 1
set.seed(2024)
obs = 3000 # no individual significance test is affected 
cte = rep(1, obs)
x2 = rnorm(obs, 5, 0.1)  # related to intercept: non essential
x3 = rnorm(obs, 5, 10)
x4 = x3 + rnorm(obs, 5, 0.5) # related to x3: essential
x5 = rnorm(obs, -1, 3)
x6 = rnorm(obs, 15, 2.5)
y = 4 + 5*x2 - 9*x3 -2*x4 + 2*x5 + 7*x6 + rnorm(obs, 0, 2)
X = cbind(cte, x2, x3, x4, x5, x6)
theoremSIMULATION1 = multicollinearity(y, X)
rownames(theoremSIMULATION1) = c("Intercept", "X2", "X3", "X4", "X5", "X6")
vifsSIMULATION1 = VIF(X)
cnSIMULATION1 = CN(X)
cvsSIMULATION1 = CVs(X)

## Simulation 2
obs = 100 # decreasing the number of observations affected to intercept 
cte = rep(1, obs)
x2 = rnorm(obs, 5, 0.1)  # related to intercept: non essential
x3 = rnorm(obs, 5, 10)
x4 = x3 + rnorm(obs, 5, 0.5) # related to x3: essential
x5 = rnorm(obs, -1, 3)
x6 = rnorm(obs, 15, 2.5)
y = 4 + 5*x2 - 9*x3 -2*x4 + 2*x5 + 7*x6 + rnorm(obs, 0, 2)
X = cbind(cte, x2, x3, x4, x5, x6)
theoremSIMULATION2 = multicollinearity(y, X)
rownames(theoremSIMULATION2) = c("Intercept", "X2", "X3", "X4", "X5", "X6")
vifsSIMULATION2 = VIF(X)
cnSIMULATION2 = CN(X)
cvsSIMULATION2 = CVs(X)

## Simulation 3
obs = 30 # decreasing the number of observations affected to intercept, x2 and x4 
cte = rep(1, obs)
x2 = rnorm(obs, 5, 0.1)  # related to intercept: non essential
x3 = rnorm(obs, 5, 10)
x4 = x3 + rnorm(obs, 5, 0.5) # related to x3: essential
x5 = rnorm(obs, -1, 3)
x6 = rnorm(obs, 15, 2.5)
y = 4 + 5*x2 - 9*x3 -2*x4 + 2*x5 + 7*x6 + rnorm(obs, 0, 2)
X = cbind(cte, x2, x3, x4, x5, x6)
theoremSIMULATION3 = multicollinearity(y, X)
rownames(theoremSIMULATION3) = c("Intercept", "X2", "X3", "X4", "X5", "X6")
vifsSIMULATION3 = VIF(X)
cnSIMULATION3 = CN(X)
cvsSIMULATION3 = CVs(X)
```

With this generation it is intended that the variable $\mathbf{X}_{2}$ is linearly related to the intercept as well as $\mathbf{X}_{3}$ to $\mathbf{X}_{4}$. This is supported by the results shown in Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:traditionalSIMULATIONtableHTML)', '\\@ref(tab:traditionalSIMULATIONtableLATEX)'))`, which have been obtained using the \CRANpkg{multiColl} package of R (@R) using the commands $\texttt{CV}$, $\texttt{VIF}$ and $\texttt{CN}$.

The results imply the same conclusions in all three simulations:

- There is a worrying degree of non-essential multicollinearity in the model relating the intercept to the variable $\mathbf{X}_{2}$ since its coefficient of variation (CV) is lower than 0.1002506.

- There is a worrying degree of essential multicollinearity in the model relating the variables $\mathbf{X}_{3}$ and $\mathbf{X}_{4}$ since the associated Variance Inflation Factors (VIF) are greater than 10.

```{r traditionalSIMULATIONtableHTML, eval = knitr::is_html_output()}
traditionalSIMULATION = data.frame(c(cvsSIMULATION1, vifsSIMULATION1, cnSIMULATION1), 
                                   c(cvsSIMULATION2, vifsSIMULATION2, cnSIMULATION2), 
                                   c(cvsSIMULATION3, vifsSIMULATION3, cnSIMULATION3))
rownames(traditionalSIMULATION) = c("X2 CV", "X3 CV", "X4 CV", "X5 CV", "X6 CV", "X2 VIF", "X3 VIF", "X4 VIF", "X5 VIF", "X6 VIF", "CN")
colnames(traditionalSIMULATION) = c("Simulation 1", "Simulation 2", "Simulation 3")
knitr::kable(traditionalSIMULATION, format = "html", caption = "CVs, VIFs for data of Simulations 1, 2 and 3", align="cccccc", digits = 3) 
```

```{r traditionalSIMULATIONtableLATEX, eval = knitr::is_latex_output()}
traditionalSIMULATION = data.frame(c(cvsSIMULATION1, vifsSIMULATION1, cnSIMULATION1), 
                                   c(cvsSIMULATION2, vifsSIMULATION2, cnSIMULATION2), 
                                   c(cvsSIMULATION3, vifsSIMULATION3, cnSIMULATION3))
rownames(traditionalSIMULATION) = c("X2 CV", "X3 CV", "X4 CV", "X5 CV", "X6 CV", "X2 VIF", "X3 VIF", "X4 VIF", "X5 VIF", "X6 VIF", "CN")
colnames(traditionalSIMULATION) = c("Simulation 1", "Simulation 2", "Simulation 3")
knitr::kable(traditionalSIMULATION, format = "latex", booktabs = TRUE, caption = "CVs, VIFs and CN for data of Simulations 1, 2 and 3", align="cccccc", digits = 3) %>%
kable_styling(latex_options = "scale_down")
``` 

However, does the degree of multicollinearity detected really affect the statistical analysis of the model? According to the results shown in Tables `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:theoremSIMULATION1tableHTML)', '\\@ref(tab:theoremSIMULATION1tableLATEX)'))` to `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:theoremSIMULATION3tableHTML)', '\\@ref(tab:theoremSIMULATION3tableLATEX)'))` this is not always the case:

- In Simulation 1, when $n=3000$, the degree of multicollinearity in the model does not affect the statistical analysis of the model; scenario a.1 is always verified, i.e., both in the model proposed and in the orthonormal model, the null hypothesis is rejected in the individual significance tests.

- In Simulation 2, when $n=100$, the degree of multicollinearity in the model affects the statistical analysis of the model only in the individual significance of the intercept; in all other cases scenario a.1 is verified again.

  + As will be seen below, the fact that the individual significance of the variable $\mathbf{X}_{2}$ is not affected may be due to the number of observations in the data set. But it may also be because multicollinearity of the nonessential type affects only the intercept estimate. Thus, for example, in @Salmeron2019TAS it is shown (see Table 2 of Example 2) that solving this type of approximate multicollinearity (by centering the variables that cause it) only modifies the estimate of the intercept and its standard deviation, with the estimates of the rest of the independent variables remaining unchanged.

- In Simulation 3, when $n=30$, the degree of multicollinearity in the model affects the statistical analysis of the model in the individual significance of the intercept, in $\mathbf{X}_{2}$ and in $\mathbf{X}_{4}$.

  + In this case, as discussed, the reduction in sample size does not prevent the individual significance of $\mathbf{X}_{2}$ from being affected.

In conclusion, as @OBrien indicates, it can be seen that the increase in sample size prevents the statistical analysis of the model from being affected by the degree of existing multicollinearity, even though the values of the measures traditionally used to detect this problem indicate that it is troubling. To reach this conclusion, the use of the RVIF proposed by @Salmeron2024a and the theorem developed in this paper is decisive.

```{r theoremSIMULATION1tableHTML, eval = knitr::is_html_output()}
knitr::kable(theoremSIMULATION1, format = "html", caption = "Theorem results of the Simulation 1 model", align="cccccc", digits = 6) 
```

```{r theoremSIMULATION1tableLATEX, eval = knitr::is_latex_output()}
knitr::kable(theoremSIMULATION1, format = "latex", booktabs = TRUE, caption = "Theorem results of the Simulation 1 model", align="cccccc", digits = 6) %>%
kable_styling(latex_options = "scale_down")
``` 

```{r theoremSIMULATION2tableHTML, eval = knitr::is_html_output()}
knitr::kable(theoremSIMULATION2, format = "html", caption = "Theorem results of the Simulation 2 model", align="cccccc", digits = 6) 
```

```{r theoremSIMULATION2tableLATEX, eval = knitr::is_latex_output()}
knitr::kable(theoremSIMULATION2, format = "latex", booktabs = TRUE, caption = "Theorem results of the Simulation 2 model", align="cccccc", digits = 6) %>%
kable_styling(latex_options = "scale_down")
``` 

```{r theoremSIMULATION3tableHTML, eval = knitr::is_html_output()}
knitr::kable(theoremSIMULATION3, format = "html", caption = "Theorem results of the Simulation 3 model", align="cccccc", digits = 6) 
```

```{r theoremSIMULATION3tableLATEX, eval = knitr::is_latex_output()}
knitr::kable(theoremSIMULATION3, format = "latex", booktabs = TRUE, caption = "Theorem results of the Simulation 3 model", align="cccccc", digits = 6) %>%
kable_styling(latex_options = "scale_down")
``` 

## Selection of the variable to be set as the last before orthonormalization {#how-to-fix}

Since there are as many QR decompositions as there are possible rearrangements of the independent variables, it is convenient to test different options to determine whether the degree of multicollinearity in the regression model affects its statistical analysis.

A first possibility is to try all possible reorderings considering that the intercept must always be in first place. Thus, in the Example 2 of @Salmeron2024a (see Appendix [Examples of...](#examplesRVIF) for more details) it is considered that $\mathbf{X} = [ \mathbf{1} \ \mathbf{K} \ \mathbf{W}]$ (see Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:theoremCHOICE1tableHTML)', '\\@ref(tab:theoremCHOICE1tableLATEX)'))`), but it could also be considered that $\mathbf{X} = [ \mathbf{1} \ \mathbf{W} \ \mathbf{K}]$ (see Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:theoremCHOICE2tableHTML)', '\\@ref(tab:theoremCHOICE2tableLATEX)'))`).

Note that in these tables the values for each variable of RVIF and $c_{0}$ are always the same, but those of $c_{3}$ change depending on the position of each variable within the design matrix.

```{r Choice1}
P = CDpf[,1]
cte = CDpf[,2]
K = CDpf[,3]
W = CDpf[,4]

data2 = cbind(cte, K, W)
th2 = multicollinearity(P, data2)
rownames(th2) = c("Intercept", "Capital", "Work")
```

```{r theoremCHOICE1tableHTML, eval = knitr::is_html_output()}
knitr::kable(th2, format = "html", caption = "Theorem results of the Example 2 of @Salmeron2024a", align="cccccc", digits = 6) 
```

```{r theoremCHOICE1tableLATEX, eval = knitr::is_latex_output()}
knitr::kable(th2, format = "latex", booktabs = TRUE, caption = "Theorem results of the Example 2 of Salmerón et al. (2025)", align="cccccc", digits = 6) %>%
kable_styling(latex_options = "scale_down")
``` 

```{r Choice2}
data2 = cbind(cte, W, K)
th2 = multicollinearity(P, data2)
rownames(th2) = c("Intercept", "Work", "Capital")
```

```{r theoremCHOICE2tableHTML, eval = knitr::is_html_output()}
knitr::kable(th2, format = "html", caption = "Theorem results of the Example 2 of @Salmeron2024a (reordination 2)", align="cccccc", digits = 6) 
```

```{r theoremCHOICE2tableLATEX, eval = knitr::is_latex_output()}
knitr::kable(th2, format = "latex", booktabs = TRUE, caption = "Theorem results of the Example 2 of Salmerón et al. (2025) (reordination 2)", align="cccccc", digits = 6) %>%
kable_styling(latex_options = "scale_down")
``` 

It is observed that in one of the two possibilities considered, the individual significance of the labor variable is affected by the degree of existing multicollinearity. 

Therefore, to state that the statistical analysis of the multiple linear regression model is not affected by the multicollinearity present in the model, it is necessary to check all the possible QR decompositions and to determine in all cases that the statistical analysis is not affected. However, to determine that the statistical analysis of the model is affected by the presence of multicollinearity, it is sufficient to find one of the possible rearrangements in which the situation b.1 occurs.

```{r Choice7, eval=FALSE}
NE = employees[,1]
cte = employees[,2]
FA = employees[,3]
OI = employees[,4]
S = employees[,5]
reg = lm(NE~FA+OI+S)
summary(reg)
```

Another possibility is to set in the last position of $\mathbf{X}$ a particular variable following a specific criterion. Thus, for example, in Example 3 of @Salmeron2024a (see Appendix [Examples of...](#examplesRVIF) for more details) it is verified that the variable FA has a coefficient significantly different from zero. Fixing this variable in third place since the individual significance will not be modified yields the results shown in Table `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:theoremCHOICE8tableHTML)', '\\@ref(tab:theoremCHOICE8tableLATEX)'))`.

```{r Choice8}
NE = employees[,1]
cte = employees[,2]
FA = employees[,3]
OI = employees[,4]
S = employees[,5]
data3 = cbind(OI, S, FA)
th8 = multicollinearity(NE, data3)
rownames(th8) = c("OI", "S", "FA")
```

```{r theoremCHOICE8tableHTML, eval = knitr::is_html_output()}
knitr::kable(th8, format = "html", caption = "Theorem results of the Example 3 of @Salmeron2024a reordination", align="ccccc", digits=20) 
```

```{r theoremCHOICE8tableLATEX, eval = knitr::is_latex_output()}
knitr::kable(th8, format = "latex", booktabs = TRUE, caption = "Theorem results of the Example 3 of Salmerón et al. (2025) reordination", align="ccccc", digits=20) %>%
kable_styling(latex_options = "scale_down")
``` 

It can be seen that in this case the degree of multicollinearity in the model affects the individual significance of the OI and S variables.

# Conclusions {#conclusiones-VIF}

In this paper, following @Salmeron2024a, we propose an alternative orthogonal model that leads to a lower bound for the RVIF, indicating whether the degree of multicollinearity present in the model affects its statistical analysis. These thresholds serve as complements to the results presented by @OBrien, who stated that the estimated variances depend on other factors that can counterbalance a high value of the VIF, for example, the size of the sample or the estimated variance of the independent variables. Thus, the thresholds presented for the RVIF also depend on these factors meeting a threshold associated with each independent variable (including the intercept). Note that these thresholds will indicate whether the degree of multicollinearity affects the statistical analysis.

As these thresholds are derived from the individual significance tests of the model, it is possible to reinterpret them as a statistical test to determine whether the degree of multicollinearity in the linear regression model affects its statistical analysis. This analytic tool allows researchers to conclude whether the degree of multicollinearity is statistically troubling and whether it needs to be treated. We consider this to be a relevant contribution since, to the best of our knowledge, the only existing example of such a measure, presented by  @FarrarGlauber, has been strongly criticized (in addition to the limitations highlighted in the introduction, it should be noted that it completely ignores approximate non-essential multicollinearity since the correlation matrix does not include information on the intercept); consequently, this new statistical test with a non-rejection region will fill a gap in the scientific literature.

On the other hand, note that the position of each of the variables in the matrix $\mathbf{X}$ uniquely determines the reference orthonormal model $\mathbf{X}_{o}$. It is to say, there are as many reference models given by the proposed QR decomposition as there are possible rearrangements of the variables within the matrix $\mathbf{X}$.

In this sense, as has been shown, in order to affirm that the statistical analysis of the model is not affected by the degree of multicollinearity existing in the model (with the degree of significance used in the application of the proposed theorem), it is necessary to state that in all the possible rearrangements of $\mathbf{X}$ it is concluded that scenario b.1 does not occur. On the other hand, when there is a rearrangement in which this scenario appears, it can be stated (to the degree of significance used when applying the proposed theorem) that the degree of existing multicollinearity affects the statistical analysis of the model.

Finally, as a future line of work, it would be interesting to complete the analysis presented here by studying when the degree of multicollinearity in the model affects its numerical analysis.

# Acknowledgments

This work has been supported by project PP2019-EI-02 of the University of Granada (Spain) and by project A-SEJ-496-UGR20 of the Andalusian Government’s Counseling of Economic Transformation, Industry, Knowledge and Universities (Spain).

# Appendix

## Inconsistency in hypothesis tests: situation a.2 {#inconsistency}

From a numerical point of view it is possible to reject $H_{0}: \beta_{i} = 0$ while $H_{0}: \beta_{i,o} = 0$ is not rejected, which implies that $t_{i}^{o} < t_{n-k}(1 - \alpha/2) < t_{i}$. Or, in other words, $t_{i}/t_{i}^{o} > 1$.

However, from expression \@ref(eq:texp-orto-2) it is obtained that $\widehat{\sigma} = | \widehat{\beta}_{i,o} | / t_{i}^{o}$. By substituting $\widehat{\sigma}$ in expression \@ref(eq:texp-orig), taking into account expression \@ref(eq:RVIF), it is obtained that
    $$\frac{t_{i}}{t_{i}^{o}} = \frac{| \widehat{\beta}_{i} |}{| \widehat{\beta}_{i,o} |} \cdot \frac{1}{\sqrt{RVIF(i)}}.$$
    From this expression it can be concluded that in situations with high collinearity, $RVIF(i) \rightarrow +\infty$, the ratio $t_{i}/t_{i}^{o}$ will tend to zero, and the condition $t_{i}/t_{i}^{o} > 1$ will rarely occur. That is to say, the inconsistency in situation a.2, commented on in the preliminaries of the paper, will not appear.

On the other hand, if the variable $i$ is orthogonal to the rest of independent variables, it is verified that $\widehat{\beta}_{i,o} = \widehat{\beta}_{i}$ since $p_{i} = ( 0 \dots \underbrace{1}_{(i)} \dots 0)$. At the same time, $RVIF(i) = \frac{1}{SST_{i}}$ where $SST$ denotes the sum of total squares. If there is orthonormality, as proposed in this paper, $SST_{i} = 1$ and, as consequence, it is verified that $t_{i} = t_{i}^{o}$. Thus, the individual significance test for the original data and for the orthonormal data are the same.

## Test of individual significance of coefficient $k$ {#apendix}

Taking into account that it is verified that $\boldsymbol{\beta}_{o} = \mathbf{P} \boldsymbol{\beta}$ where:
$$\boldsymbol{\beta}_{o} = \left(
        \begin{array}{c}
            \beta_{1,o} \\
            \beta_{2,o} \\
            \vdots \\
            \beta_{k,o}
        \end{array} \right), \quad
    \mathbf{P} = \left(
        \begin{array}{cccc}
            p_{11} & p_{12} & \dots & p_{1k} \\
            0 & p_{22} & \dots & p_{2k} \\
            \vdots & \vdots &  & \vdots \\
            0 & 0 & \dots & p_{kk}
        \end{array} \right), \quad
        \boldsymbol{\beta} = \left(
        \begin{array}{c}
            \beta_{1} \\
            \beta_{2} \\
            \vdots \\
            \beta_{k}
        \end{array} \right),$$
it is obtained that $\beta_{k,o} = p_{kk} \beta_{k}$. Then, the null hypothesis $H_{0}: \beta_{k,o} = 0$ is equivalent to $H_{0}: \beta_{k} = 0$. Due to this fact, Tables `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:Wissel0tableHTML)', '\\@ref(tab:Wissel0tableLATEX)'))` and `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:WisselORTOtableHTML)', '\\@ref(tab:WisselORTOtableLATEX)'))` showed an expectable behaviour. However, this behaviour will be analyzed with more detail.

The experimental value to be considered to take a decision in the test with null hypothesis $H_{0}: \beta_{k,o} = 0$ and alternative hypothesis $H_{1}: \beta_{k,o} \not= 0$ is given by the following expression:
$$t_{k}^{o} = \left| \frac{\widehat{\beta}_{k,o}}{\sqrt{var \left( \widehat{\beta}_{k,o} \right)}} \right|.$$

Taking into account that $\widehat{\boldsymbol{\beta}}_{o} = \mathbf{P} \widehat{\boldsymbol{\beta}}$ and $var \left( \widehat{\boldsymbol{\beta}}_{o} \right) = \mathbf{P} var \left( \widehat{\boldsymbol{\beta}} \right) \mathbf{P}^{t},$ it is verified that $\widehat{\beta}_{k,o} = p_{kk} \widehat{\beta}_{k}$ and $var \left( \widehat{\beta}_{k,o} \right) = p_{kk}^{2} var \left( \widehat{\beta}_{k} \right)$. Then:
   $$t_{k}^{o} = \left| \frac{p_{kk} \widehat{\beta}_{k}}{p_{kk} \sqrt{var \left( \widehat{\beta}_{k} \right)}} \right| = \left| \frac{\widehat{\beta}_{k}}{\sqrt{var \left( \widehat{\beta}_{k} \right)}} \right| = t_{k},$$
   where $t_{k}$ is the experimental value to take a decision in the test with null hypothesis  $H_{0}: \beta_{k} = 0$ and alternative hypothesis $H_{1}: \beta_{k} \not= 0$.

## Examples of @Salmeron2024a {#examplesRVIF}   

**Example 1 of @Salmeron2024a: Detection of traditional nonessential multicollinearity**. Using data from a financial model in which the Euribor (E) is analyzed from the Harmonized Index of Consumer Prices (HICP), the balance of payments to net current account (BC) and the government deficit to net nonfinancial accounts (GD), we illustrate the detection of approximate multicollinearity of the non-essential type, i.e. where the intercept is related to one of the remaining independent variables (for details see @MarquardtSnee1975). For more information on this data set use *help(euribor)*.

Note that @Salmeron2019 establishes that an independent variable with a coefficient of variation less than 0.1002506 indicates that this variable is responsible for a non-essential multicollinearity problem.

Thus, first of all, the approximate multicollinearity detection is performed using the measures traditionally applied for this purpose: the Variance Inflation Factor (VIF) and the Condition Number (CN). Values higher than 10 for the VIF (see, for example, @Marquardt1970) and 30 for the CN (see, for example, @Belsley1991 or @BelsleyKuhWelsch), imply that the degree of existing multicollinearity is troubling. Moreover, according to @Salmeron2019, the VIF is only able to detect essential multicollinearity (relationship between independent variables excluding the intercept, see @MarquardtSnee1975), while the CN detects both essential and non-essential multicollinearity.

Therefore, the values calculated below (using the $\texttt{VIF}$, $\texttt{CN}$ and $\texttt{CVs}$ commands from the \CRANpkg{multiColl} package, see @Salmeron2021multicoll and @Salmeron2022multicoll for more details on this package) indicate that the degree of approximate multicollinearity existing in the model of the essential type is not troubling, while that of the non-essential type is troubling due to the relationship of HIPC with the intercept.

```{r PAPER1, echo=TRUE}
E = euribor[,1]
data1 = euribor[,-1]

VIF(data1) 
CN(data1)
CVs(data1)
```

This assumption is confirmed by calculating the RVIF values, which point to a strong relationship between the second variable and the intercept:

```{r PAPER2, echo = knitr::is_html_output(), eval = knitr::is_html_output()}
rvifs(data1, ul = T, intercept = T) 
```

```{r PAPER2bis, echo = knitr::is_latex_output(), eval = knitr::is_latex_output()}
rvifs(data1, ul = T, intercept = T) 
```

The output of the $\texttt{rvifs}$ command provides the values of the Redefined Variance Inflation Factor (RVIF) and the percentage of multicollinearity due to each variable (denoted as $a_{i}$ in the [An orthonormal...](#sub-above) section).

In this case, three of the four arguments available in the $\texttt{rvifs}$ command are used:

- The first of these refers to the design matrix containing the independent variables (the intercept, if any, being the first column).

- The second argument, $ul$, indicates that the data is to be transformed into unit length. This transformation makes it possible to establish that the RVIF is always greater than or equal to 1, having as a reference a minimum value that indicates the absence of worrying multicollinearity.

- The third argument, $intercept$, indicates whether there is an intercept in the design matrix.

Note that these results can also be obtained after using the $\texttt{lm}$ and $\texttt{model.matrix}$ commands as follows:

```{r PAPER_2, echo = knitr::is_html_output(), eval = knitr::is_html_output()}
reg_E = lm(euribor[,1]~as.matrix(euribor[,-c(1,2)]))
rvifs(model.matrix(reg_E)) 
```

```{r PAPER_2bis, echo = knitr::is_latex_output(), eval = knitr::is_latex_output()}
reg_E = lm(euribor[,1]~as.matrix(euribor[,-c(1,2)]))
rvifs(model.matrix(reg_E))
```

Finally, the application of the Theorem established in Subsection [From the RVIF](#TheTHEOREM) detects that the individual inference of the second variable (HIPC) is affected by the degree of multicollinearity existing in the model. These results are obtained using the $\texttt{multicollinearity}$ command from the \CRANpkg{rvif} package:

```{r PAPER3, echo=TRUE}
multicollinearity(E, data1) 
```

Therefore, it can be established that the existing multicollinearity affects the statistical analysis of the Euribor model.

**Example 2 of @Salmeron2024a: Detection of generalized nonessential multicollinearity**. Using data from a Cobb-Douglas production function in which the production (P) is analyzed from the capital (K) and the work (W), we illustrate the detection of approximate multicollinearity of the generalized non-essential type, i.e., that in which at least two independent variables with very little variability (excluding the intercept) are related to each other (for more details, see @Salmeron2020maths). For more information on this dataset use *help(CDpf)*.

Using the $\texttt{rvifs}$ command, it can be determined that both capital and labor are linearly related to each other with high RVIF values below the threshold established as a worrying value:

```{r PAPER4, echo=TRUE}
P = CDpf[,1]
data2 = CDpf[,2:4]
```

```{r PAPER4bis, echo = knitr::is_html_output(), eval = knitr::is_html_output()}
rvifs(data2, ul = T) 
```

```{r PAPER4tris, echo = knitr::is_latex_output(), eval = knitr::is_latex_output()}
rvifs(data2, ul = T) 
```

However, the application of the Theorem established in Subsection [From the RVIF](#TheTHEOREM) does not detect that the degree of multicollinearity in the model affects the statistical analysis of the model:

```{r PAPER5, echo=TRUE}
multicollinearity(P, data2)
```

Now then, if we rearrange the design matrix $\mathbf{X}$ we obtain that:

```{r PAPER5bis, echo=TRUE}
data2 = CDpf[,c(2,4,3)]
multicollinearity(P, data2)
```

Therefore, it can be established that the existing multicollinearity does affect the statistical analysis of the Cobb-Douglas production function model.

**Example 3 of @Salmeron2024a: Detection of essential multicollinearity**. Using data from a model in which the number of employees of Spanish companies (NE) is analyzed from the fixed assets (FA), operating income (OI) and sales (S), we illustrate the detection of approximate multicollinearity of the essential type, i.e., that in which at least two independent variables (excluding the intercept) are related to each other (for more details, see @MarquardtSnee1975). For more information on this dataset use *help(employees)*.

In this case, the $\texttt{rvifs}$ command shows that variables three and four (OI and S) have a high VIF value, so they are highly linearly related: 

```{r PAPER6, echo=TRUE}
NE = employees[,1]
data3 = employees[,2:5]
```

```{r PAPER6bis, echo = knitr::is_html_output(), eval = knitr::is_html_output()}
rvifs(data3, ul = T) 
```

```{r PAPER6tris, echo = knitr::is_latex_output(), eval = knitr::is_latex_output()} 
rvifs(data3, ul = T) 
```

Note that if in *rvifs(data3, ul = T)* the unit length transformation is avoided, which is done in the $\texttt{multicollinearity}$ command, the RVIF cannot be calculated since the system is computationally singular. For this reason, the intercept is eliminated below since it has been shown above that it does not play a relevant role in the linear relationships of the model.

Finally, the application of the Theorem established in Subsection [From the RVIF](#TheTHEOREM) detects that the individual inference of the third variable (OI) is affected by the degree of multicollinearity existing in the model:

```{r PAPER7, echo=TRUE}
multicollinearity(NE, data3[,-1])
```

Therefore, it can be established that the existing multicollinearity affects the statistical analysis of the model of the number of employees in Spanish companies.

**Example 4 of @Salmeron2024a: The special case of simple linear model**. The simple linear regression model is an interesting case because it has a single independent variable and the intercept. Since the intercept is not properly considered as an independent variable of the model in many cases (see introduction of @Salmeron2019 for more details), different software (including R, @R) do not consider that there can be worrisome multicollinearity in this type of model.

To illustrate this situation, @Salmeron2024a randomly generates observations for the following two simple linear regression models $\mathbf{y}_{1} = \beta_{1} + \beta_{2} \mathbf{V} + \mathbf{u}_{1}$ and $\mathbf{y}_{2} = \alpha_{1} + \alpha_{2} \mathbf{Z} + \mathbf{u}_{1}$, according to the following code:

```{r PAPER8, echo=TRUE}
set.seed(2022)
obs = 50
cte4 = rep(1, obs)
V = rnorm(obs, 10, 10)
y1 = 3 + 4*V + rnorm(obs, 0, 2)
Z = rnorm(obs, 10, 0.1)
y2 = 3 + 4*Z + rnorm(obs, 0, 2)

data4.1 = cbind(cte4, V)
data4.2 = cbind(cte4, Z)
```

For more information on these data sets use *help(SLM1)* and *help(SLM2)*.

As mentioned above, the R package (@R) denies the existence of multicollinearity in this type of model. Thus, for example, when using the $\texttt{vif}$ command of the \CRANpkg{car} package on *reg=lm(y1~V)* the following message is obtained: *Error in vif.default(reg): model contains fewer than 2 terms*.

Undoubtedly, this message is coherent with the fact that, as mentioned above, the VIF is not capable of detecting non-essential multicollinearity (which is the only multicollinearity that exists in this type of model). However, the error message provided may lead a non-specialized user to consider that the multicollinearity problem does not exist in this type of model. These issues are addressed in more depth in @Salmeron2022multicoll.

On the other hand, the calculation of the RVIF in the first model shows that the degree of multicollinearity is not troubling, since it presents very low values:

```{r PAPER10, echo=TRUE}
rvifs(data4.1, ul = T) 
```

While in the second model they are very high, indicating a problem of non-essential multicollinearity:

```{r PAPER11, echo=TRUE}
rvifs(data4.2, ul = T) 
```

By using the $\texttt{multicollinearity}$ command, it is found that the individual inference of the intercept of the second model is affected by the degree of multicollinearity in the model:

```{r PAPER12, echo=TRUE}
multicollinearity(y1, data4.1)
multicollinearity(y2, data4.2)
```

Therefore, it can be established that the multicollinearity existing in the first simple linear regression model does not affect the statistical analysis of the model, while in the second one it does.
