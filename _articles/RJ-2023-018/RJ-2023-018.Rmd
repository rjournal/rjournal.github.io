---
title: 'pCODE: Estimating Parameters of ODE Models'
abstract: ' The ordinary differential equation (ODE) models are prominent to characterize
  the mechanism of dynamical systems with various applications in biology, engineering,
  and many other areas. While the form of ODE models is often proposed based on the
  understanding or assumption of the dynamical systems, the values of ODE model parameters
  are often unknown. Hence, it is of great interest to estimate the ODE parameters
  once the observations of dynamic systems become available. The parameter cascade
  method initially proposed by [@parcascade] is shown to provide an accurate estimation
  of ODE parameters from the noisy observations at a low computational cost. This
  method is further promoted with the implementation in the R package CollocInfer
  by [@CollocInfer]. However, one bottleneck in using CollocInfer to implement the
  parameter cascade method is the tedious derivations and coding of the Jacobian and
  Hessian matrices required by the objective functions for doing estimation. We develop
  an R package pCODE to implement the parameter cascade method, which has the advantage
  that the users are not required to provide any Jacobian or Hessian matrices. Functions
  in the pCODE package accommodate users for estimating ODE parameters along with
  their variances and tuning the smoothing parameters. The package is demonstrated
  and assessed with four simulation examples with various settings. We show that pCODE
  offers a derivative-free procedure to estimate any ODE models where its functions
  are easy to understand and apply. Furthermore, the package has an online Shiny app
  at <https://pcode.shinyapps.io/pcode/>. '
author:
- name: Haixu Wang
  affiliation: Simon Fraser University
  email: |
    haixuw@sfu.ca
  address:
  - 8888 University Dr, Burnaby, BC V5A 1S6
  - Canada
- name: Jiguo Cao
  affiliation: Simon Fraser University
  email: |
    jiguoc@sfu.ca
  address:
  - 8888 University Dr, Burnaby, BC V5A 1S6
  - Canada
date: '2023-02-10'
date_received: '2022-02-16'
journal:
  firstpage: 291
  lastpage: 304
volume: 14
issue: 4
slug: RJ-2023-018
packages:
  cran: ~
  bioc: ~
draft: no
preview: preview.png
bibliography: pcode_rjournal.bib
CTV: ~
output:
  rjtools::rjournal_web_article:
    self_contained: no
    toc: no
    legacy_pdf: yes

---





# Introduction {#sec:introduction}

The evolution of a dynamic system in time can be represented by ordinary
differential equations (ODE) models consisting of a set of state
variables and their derivatives. For example, the state variables can be
the population of multiple species in an ecological system, the
velocities, and locations of particles in a physics system, the
concentration of chemicals in a reactor, or the energy of objects in a
heating process. Usually, an ODE model is defined as a set of
differential equations in the following form
$$\dot{\bm{x}}(t) = \bm{f}(\bm{x},\bm{u},t|\bm{\theta}),
\label{eq:generalode}$$ where $\bm{x}(t)$ denotes the fully or partially
observed states of the system at time $t$. The ODE model links the first
derivative, the temporal change, of the states with $\bm{x}$ itself and
other influential components $\bm{u}$ through some function $\bm{f}$
defined by the parameter vector $\bm{\theta}$.

Conventionally, parameter estimation of an ODE model combines
numerically solving the system and updating the ODE parameters given the
solution. The updating step complies with the nonlinear regression
framework as suggested in [@nonlinearestBARD] and
[@nonlinearparameterestimation]. That is, the optimal $\bm{\theta}$
minimizes the sum of squared errors between the observations and the
solutions of ODE models. It is not very typical for an ODE model to have
analytical solutions unless the dynamic system is simple and linear. In
more realistic situations where analytical solutions are not available,
the ODE solutions must be numerically obtained prior to any estimations
of the ODE parameters. The initial condition, which are values of state
variables at the initial time, dictates the unique solution obtained by
the numeric solver. However, the initial condition is often unavailable
in real practices and must be estimated before obtaining the ODE
solutions. If the initial conditions are unknown, then they are
augmented to the parameter vector and estimated simultaneously with the
ODE parameters. As a result, the complexity of optimization dramatically
increases due to the fact that the solution is sensitive to ODE
parameters and initial conditions. Also, the augmentation increases the
dimension and complexity of the optimization surface hence hindering the
convergence of optimization algorithms.

Being an improvement in the stability of estimation, the multiple
shooting methods, presented in [@BOCKmultiple], [@peifermultiple], and
[@nonlineardynamic], expands the dimension of optimization surface by
introducing additional initial conditions. The extra initial conditions
are a result of partitioning the original time interval into multiple
subintervals, and numeric solutions are obtained within each
subinterval. Ideally, the introduction of these initial conditions will
facilitate the optimization process for $\bm{\theta}$ to avoid local
minima in the optimization surface and provide a faster convergence
rate. Additionally, [@Huang_bayesian] and [@Gelman_bayesian] present
Bayesian frameworks of estimating ODE parameters with a hierarchical
model.

The aforementioned strategies depend on the numeric solutions of ODE
models and can be burdened with significant computational challenges
given complex dynamic systems. That is, the system has to be numerically
solved given each update or candidate of ODE parameters. To overcome
these challenges, [@Varah] proposes a two-step procedure to use
nonparametric techniques for interpolating the time derivatives of state
variables. Subsequently, the parameters are estimated through a
nonlinear regression framework by treating the time derivatives as the
response. This methodology is further developed in
[@wu_localestimation], [@ramsayFDA] , and [@brunel2008]. Replacing
numeric solutions with nonparametric estimations drastically reduces the
computational cost, but estimation errors are introduced in the process
of interpolating the time derivatives. The errors are due to the fact
that there are no direct observations of the derivatives. Furthermore,
[@varyingODE] shows that the estimation errors of derivatives will lead
to bias in the estimation of ODE parameters.

As contrary to the two-step procedure, [@parcascade] presents a new
approximation strategy, called the parameter cascade method, in the
family of collocation methods. The parameter cascade method integrates
data smoothing and estimation of ODE parameters into a single estimation
scheme. The parameter cascades method provides a much more efficient way
of approximating the ODE solution without numerically solving the
system. Additionally, it ensures that the ODE parameters are more
accurately estimated than the two-step approach [@varyingODE]. Various
well-known ODE models are examined in [@parcascade]. [@qi2010] has
discussed the asymptotic properties of the parameter cascades estimates
for both ODE parameters and basis coefficients. The methodology has also
been validated with many applications. [@selectionofODE] investigates
the model selection problem in ODE models based on parameters estimated
from the cascading procedure. When the system is measured with
replications, mixed effects need to be incorporated into the model. This
mixed-effect ODE model is explored in [@ODEmixedeffect].

The cascading estimation strategy is further promoted as in the R
package CollocInfer by [@CollocInfer]. CollocInfer offers a variety of
functions regarding the estimation of ODE models. It has included the
implementation of the parameter cascade method, the tuning criterion of
smoothing parameter, and the sample variance of parameter estimates
through the Newey-West method in [@neweywest]. However, one bottleneck
of using CollocInfer is the requirement of manually providing all the
partial derivatives for constructing the Jacobian and Hessian matrices
in the optimization process. For complex ODE models, the calculation and
coding of those derivatives can be tedious and easy to make mistakes. As
an improvement to the CollocInfer package, we propose a new package,
named pCODE, which offers more user-friendly functions for estimating
ODE models without specifying any derivatives. That is, an analytic
Jacobian and Hessian matrix is required for the optimization routine in
CollocInfer, whereas pCODE automatically calculates the numeric
approximations to both matrices. Furthermore, pCODE also adds a
bootstrap variance estimator besides the variance estimator obtained by
the Delta method in [@parcascade] and CollocInfer. pCODE uses the k-fold
cross-validation for finding the optimal smoothing parameter while
CollocInfer selects the smoothing parameters based on the forward
prediction errors. We have also developed an online Shiny app at
<https://pcode.shinyapps.io/pcode/>.

The remainder of this paper is organized as follows. Next section will
introduce the parameter cascade methodology for simultaneous estimation
of both ODE solutions $\bm{x}(t)$ and parameters $\bm{\theta}$. The
third section explains the core functions of the package pCODE along
with their usages and syntaxes. Subsequently, The fourth section
illustrates the application of pCODE on several datasets with
simulations for assessing the performance. At last, The fifth section
gives a conclusion of the paper.

# The parameter cascade method {#seq:method}

An ODE model is build upon a set of differential equations
$\dot{x}_{i}(t) = f_{i}(\bm{x},t|\bm{\theta})$ for $i = 1,..., I,$
corresponding to the index of state variables. Each differential
equation represents how the time derivative of one state variable
depends on $\bm{x} = (x_{1},..., x_{I})$. Furthermore,
$f_{i}(\bm{x},t|\bm{\theta})$'s are assumed to be parametrized by a
vector of parameters $\bm{\theta}$. Let
$y_{ij} = x_{i}(t_{ij}) +  e_{ij}$ be the $j$-th noisy observation of
$i$-th state variable at time $t_{ij}$, where $e_{ij}$ is the
measurement error or noise, $j = 1, ..., n_{i}$. We assume the vector of
random errors $\bm{e}_{i} = (e_{i1},...,e_{in_{i}})$ has a distribution
function $g(\bm{e}_{i})$ with the distribution parameter
$\bm{\sigma}_{i}$. Our goal is to estimate $\bm{\theta}$ from the noisy
observations from the system.

The lower stream estimation of the parameter cascade method is to
provide smooth interpolations of the observations. As suggested in
[@parcascade], the data interpolation part is done by the smoothing
splines. The goal of smoothing is to diminish the random errors in the
observations of all dimensions, hence the underlying solutions
$x_{i}(t)$'s can be recovered from the noisy observations. That is, each
$x_{i}(t)$ is expressed as a linear combination of B-spline basis
functions
$$x_{i}(t) = \sum_{i}^{K_{i}} c_{ik} \phi_{ik}(t) = \bm{c}^{\prime}_{i}\bm{\phi}_{i}(t),$$
where $K_{i}$ is the number of basis functions, and $\bm{\phi}_{i}(t)$
is the vector of basis functions evaluated at a time point $t$. Data
smoothing is then equivalent to the estimation of these basis
coefficients $\bm{c}_{i}$'s.

To comply the lower level estimation, a simple objective function
$J(\bm{c})$, following the smoothing spline routine, is considered for
obtaining basis coefficients
$\bm{c} = (\bm{c}^{\prime}_1,\ldots,\bm{c}^{\prime}_I)^{\prime}$. That
is, the negative log-likelihood function is chosen for non-normal
errors, whereas the least square criterion is naturally suitable for
i.i.d. normal errors $e_{ij} \sim \text{N}(0,\sigma_{i})$. The summation
$J(\bm{c}|\bm{\sigma}) = \sum_{i} J(\bm{c}_{i}|\bm{\sigma}_{i})$ of
individual fitting criterion over $i$ defines a composite objective
function for all $I$ dimensions. The data interpolation will use a
saturated number of basis functions which requires a penalty term in the
objective function to prevent overfitting problems. [@ramsayFDA]
suggests to use a linear differential operator for introducing a
smoothness penalty in the objective functions. Subsequently,
[@Poyton2006] utilizes the smoothness penalty in estimating parameters
and solutions of ODE models. Following the same technique, the
smoothness penalty is defined based on the discrepancy between the time
derivatives and the ODE model, i.e.,
$$\int (\dot{x}_{i}(t) - f_{i}(\bm{x},t|\bm{\theta}))^{2}dt,$$ for each
state variable $x_{i}$. Multiplying the penalty with a smoothing
parameter $\lambda_{i}$ and combining the penalties over all dimensions,
the complete objective function is defined to be
$$J(\bm{c}|\bm{\theta}) = \sum_{i=1}^{I} \big[ - \ln g(\bm{e}_{i}) + \lambda_{i}\int (\dot{x}_{i}(t) - f(\bm{x},t|\bm{\theta}))^{2}dt\big].
  \label{eq:innerobj}$$ For each given set of ODE parameters
$\bm{\theta}$, the basis coefficients $\bm{c}$ is estimated by
minimizing $J(\bm{c}|\bm{\theta})$. Hence, the estimate for the basis
coefficients $\bm{c}$ become an implicit function of the ODE parameters
$\bm{\theta}$ as $\bm{c}(\bm{\theta})$. The objective function
([\[eq:innerobj\]](#eq:innerobj){reference-type="ref"
reference="eq:innerobj"}), referred to as the inner objective function,
needs to be optimized for estimating $\bm{c}$ whenever $\bm{\theta}$ is
updated.

To clearly distinguish two sets of parameters $\bm{c}$ and
$\bm{\theta}$, $\bm{c}$ will be referred to as the nuisance parameters
since they are not essential for estimating the ODE model rather
interpolate the observations. On the other hand, $\bm{\theta}$ is
responsible for defining the structure of the ODE model and will be
denoted as structural parameters. Often, $\bm{\theta}$ will be the
primary concern given any ODE models.

The upper stream estimation of the parameter cascade method focuses on
the structural parameter $\bm{\theta}$. To comply this estimation, the
objective function $H(\bm{\theta})$, referred to as the outer objective
function, is optimized with respect only to the structural parameters
$\bm{\theta}$. It is usually defined to be the negative log-likelihood
function or the sum of squared errors given the distributions of random
errors, that is,
$$H(\bm{\theta}) = -\sum_{i=1}^{I}\text{ln }g(\widehat{\bm{e}}_{i}|\bm{\theta}),
\label{eq:outterobj}$$ where
$$\widehat{\bm{e}}_{i} = \bm{y}_{i} - \hat{x}_i(\bm{t}_{i}|\bm{\theta}).$$
Here
$\hat{x}_i(\bm{t}_{i}|\bm{\theta}) = \hat{\bm{c}}^{\prime}_{i}(\bm{\theta})\bm{\phi}_{i}(\bm{t}_{i})$,
where $\hat{\bm{c}}^{\prime}_{i}(\bm{\theta})$ is the optimizer of
$J(\bm{c}|\bm{\theta})$ given current $\bm{\theta}$.

The parameter cascade method nests the estimation of basis coefficients
in that of the structural parameters, where both estimations depend on
the choice of smoothing parameters $\bm{\lambda}$. As a result, the
interpolation of data points is not separated from estimating ODE
parameters. The stream of dependencies defined a parameter cascade which
offers great accuracy and efficiency in estimating ODE models. To take
full advantage of the parameter cascade method, the pCODE package
provides several R functions that are able to apply the aforementioned
methodology for estimating ODE models.

# Main functions in the pCODE package  {#seq:functions}

## Parameter estimation: pcode

The main function to perform the parameter cascade method is pcode in
the package. This is a generic wrapper function for handling several
scenarios given different objective functions, normal or non-normal
errors, and missing state variables.

First, we focus on the situation when we have the normal random errors
for all dimensions where $e_{ij}\sim\text{N}(0,\sigma^{2}_{i})$. In
fact, both the inner optimization function
([\[eq:innerobj\]](#eq:innerobj){reference-type="ref"
reference="eq:innerobj"}) and the outer optimization function
([\[eq:outterobj\]](#eq:outterobj){reference-type="ref"
reference="eq:outterobj"}) are calculated based on a vector of
residuals. The first part of
([\[eq:innerobj\]](#eq:innerobj){reference-type="ref"
reference="eq:innerobj"}) is easily recognized as the residuals from
fitting the observation with basic functions. If the second part, the
integral, is approximated by the composite Simpson's rule, then the
approximation can be written in a quadratic form based on a vector of
residual-like quantities. Stringing the aforementioned two vectors into
one, and the concatenation of these vectors from all dimensions will
produce a single vector of residuals. Hence, the optimization of the
inner objective function adheres to the non-linear least square (NLS)
framework. As a result, we can use the popular Levenberg-Marquart (LM)
algorithm to obtain an estimate
$\hat{\bm{c}}^{\prime}_{i}(\bm{\theta})$. Our package pcode employs the
function lsqnonlin from the package PRACMA [@pracma] for applying the LM
algorithm.

The optimization of the outer objective function appears to be exactly
an NLS problem. However, the parameter cascade strategy appends an extra
layer of optimization to each update of the outer estimation, which
characterizes a nested optimization problem. It is applicable to apply
the Levenberg-Marquart algorithm again to the outer objective, however,
the computational effort is much greater than that of inner
optimization. The reason is that a Jacobian matrix needs to be
calculated for each iteration of the update on the structural
parameters, and each entry of the Jacobian for the outer objective
involves a complete optimization of the inner one. This occurs to be the
major delay in computation time.

Most commonly, the random errors $e_{ij}$'s are i.i.d from the normal
distribution. However, some dynamic systems may produce non-normal
errors which make the NLS approach not applicable. One solution is to
perform a transformation on the observations and state variables, which
forces the errors of transformed variables to have a normal
distribution. As an alternative, pcode allows one to input a likelihood
(or log-likelihood) function as a fitting criterion for both inner and
outer objectives. In such a case, optim will be used for both levels of
objective functions.

The following demonstrates the syntax of pcode with its argument list:

::: example*
pcode( data, time, ode.model, par.names, state.names, likelihood.fun,
par.initial, basis.list, lambda, controls )
:::

where data, a matrix, contains the observations of state variables as
columns. The names of state variables are stored in state.names as a
vector. time includes the observation time points in a vector. The ODE
model to be estimated is provided to pcode as ode.model and defined in a
similar way as that in package deSolve [@deSolve]:

::: example*
ode.model \<- function(t, state, parameters) { with(as.list(c(state,
parameters)), { dV \<- c \* (V - (V\^3) / 3 + R) dR \<- -(1 / c) \* (V -
a + b \* R) return(list(c(dV, dR))) }) }
:::

For this example, the state names are passed to the function PCODE as a
vector c('V','R'), and the structural parameter names c('a','b','c') are
given to par.names argument of the function. Optimization for the
structural parameters requires a initial value input as par.initial, and
there is no need for an input of likelihood.fun when the error
distributions are Normal. basis.list contains the list of basis objects
defined by the package fda, and the default basis has 9 interior knots
with B-spline basis functions of order of 4. lambda corresponds to the
penalty parameter $\bm{\lambda}$ which controls the fitness of estimated
state variables to the differential equations. A scalar input of lambda
means that all dimensions will subject to the same penalty, or a vector
input differentiates the penalty calculated for each dimension. controls
contains addition parameters to adjust optimizations for both inner and
outer objective functions and obtain initial value for basis
coefficients $\bm{c}$. Two scenarios are discussed depending on the
different distribution of errors. All the mentioned scenarios will be
demonstrated in the fourth section with examples.

### The bootstrap variance estimator: bootsvar

All the functions of this package are derivative-free, hence the
variance of structural parameters is numerically approximated. The first
option is to use the bootstrap variance estimator. Given the estimation
of both parameters $\bm{\theta}$ and $\bm{c}$, we are able to solve the
ODE directly with estimated initial value $\hat{\bm{x}}(t_{0})$. Hence,
we can simulate bootstrap samples of the ODE model based on the
estimated distributions of errors $\bm{e}_{i}$ for all $I$ dimensions.
The detailed algorithm for obtaining a bootstrap variance estimate of
$\bm{\theta}$ is as follows

::: algorithm
::: algorithmic
\

-   $\bm{\theta}_{0}$: initial value for structural parameters

-   $B$: the number of bootstrap samples

-   $\bm{y}$: observations

$\bm{\theta}_{\star},\bm{c}_{\star}$ $\gets$ results from
pcode($\bm{\theta}_{0},\bm{y}...$) $\sigma_{i}^{2}$ $\gets$
var($\bm{y}_{i} - \bm{x}_i$) $\bm{x}_{\star}$ $\gets$ Solution to the
ODE given $\bm{\theta}_{\star}$ and $\bm{x}_{\star}(t_{0})$

Obtain $\bm{z}_{i}^{(b)}$ = $\bm{x}_{i,\star}+\bm{\epsilon}_{i}^{(b)}$.

($\bm{\theta}_{\star}^{(b)},\bm{c}_{\star}^{(b)}$) $\gets$ results from
pcode($\bm{\theta}_{\star},\bm{z}_{i}^{(b)}...$)
:::
:::

The syntax of bootsvar is illustrated as the following

::: example*
bootsvar( data, time, ode.model, par.names, state.names, par.initial,
lambda, basis.list, bootsrep, controls )
:::

Most of the arguments are the same as the function pcode with only one
addition of argument bootsrep which indicates the number of bootstrap
samples to be taken for obtaining the variance estimator.

### The Delta variance estimator: deltavar

As an alternative to the bootstrap variance estimator, the package PCODE
also offers another numeric estimator for the variance of structural
parameters. [@parcascade] has developed the approximation to
$Var(\bm{\hat{\theta}}(\bm{y}))$ via the delta-method. The resulting
approximation is of the form
$$Var(\hat{\bm{\theta}}(\bm{y})) \approx \bigg[ \frac{d\bm{\hat{\theta}}}{d\bm{y}} \bigg] \bm{\Sigma}  \bigg[ \frac{d\bm{\hat{\theta}}}{d\bm{y}} \bigg]^{\prime}$$
where $\frac{d\bm{\hat{\theta}}}{d\bm{y}}$ is obtained as
$$\label{eq:dtheta_dy}
 \frac{d\bm{\hat{\theta}}}{d\bm{y}} = - \bigg[\frac{\partial^{2}H}{\partial\bm{\theta}^2}\bigg\vert_{\bm{\hat{\theta}}(\bm{y})} \bigg]^{-1} \bigg[  \frac{\partial^{2}H}{\partial\bm{\theta}\partial\bm{y}}\bigg\vert_{\bm{\hat{\theta}}(\bm{y})} \bigg]$$
and $\bm{\Sigma}$ is a $N \times N$ variance-covariance matrix for
observations put into a vector. $N$ is the total number of observation
summing over all dimensions of $\bm{y}$ . Then the estimation of
variance relies on the computation of
([\[eq:dtheta_dy\]](#eq:dtheta_dy){reference-type="ref"
reference="eq:dtheta_dy"}). The partial derivatives are approximated by
the finite difference method, i.e.,
$$\bigg[\frac{\partial^{2}H}{\partial\bm{\theta}_{u}^2}\bigg\vert_{\bm{\hat{\theta}}(\bm{y})} \bigg] \approx \frac{H(\hat{\bm{\theta}}_{u+}(\bm{y})|\bm{\lambda}) - 2H(\hat{\bm{\theta}}(\bm{y})|\bm{\lambda}) + H(\hat{\bm{\theta}}_{u-}(\bm{y})|\bm{\lambda})}{\Delta^{2}},$$
where $\hat{\bm{\theta}}_{u+}(\bm{y})$ and
$\hat{\bm{\theta}}_{u-}(\bm{y})$ indicate the addition and subtraction
of stepsize $\Delta$ to the u-th structural parameter estimate
$\hat{\bm{\theta}}(\bm{y})$. The mixed partial derivatives are
approximated as the following
$$\bigg[\frac{\partial^{2}H}{\partial\bm{\theta}_{u}\partial\bm{\theta}_{v}}\bigg\vert_{\bm{\hat{\theta}}(\bm{y})} \bigg] \approx \frac{H(\hat{\bm{\theta}}_{u+,v+}(\bm{y})) - H(\hat{\bm{\theta}}_{u-,v+}(\bm{y})) - H(\hat{\bm{\theta}}_{u+,v-}(\bm{y})) +H(\hat{\bm{\theta}}_{u-,v-}(\bm{y}))}{4\Delta^{2}}.$$
Given any fixed argument $\bm{\theta}$ for the outer objective function
$H(\bm{\theta},\bm{\sigma}|\bm{\lambda})$, its evaluation involves the
profiled estimation of $\bm{c}(\bm{\theta},\bm{\sigma};|\bm{\lambda})$
and returns solely the likelihood or the sum of squared errors. Hence,
individual evaluations of $H(\bm{\theta},\bm{\sigma}|\bm{\lambda})$ used
in numeric approximation is obtained in the following steps:

-   Step 1: Optimize $J(\bm{c}|\bm{\theta},\bm{\lambda})$ given
    $\bm{\theta}$.

-   Step 2: Obtain $\hat{\bm{x}}(\bm{t})$ based on the estimated
    $\hat{\bm{c}}$.

-   Step 3: Return
    $\sum_{i \in I} ||\bm{y}_{i} - \hat{x}_{i}(\bm{t}_i)||^{2}$ or
    $-\sum_{i \in I}g(\bm{y}_{i} - \hat{x}_{i}(\bm{t}_i)|\bm{\theta},\bm{\lambda})$.

Approximation to the second term
$\frac{\partial^{2}H}{\partial\bm{\theta}\partial\bm{y}}\bigg\vert_{\bm{\hat{\theta}}(\bm{y})}$
utilizes the finite difference method as well. After evaluating
$H(\bm{\theta},\bm{\sigma}|\bm{\lambda})$ given $\bm{\theta}$, the mixed
partial derivative is calculated by moving the particular observation up
or down by some stepsize $\Delta_{y}$. That is,
$$\bigg[\frac{\partial^{2}H}{\partial\bm{\theta}_{u}\partial y_{v}}\bigg\vert_{\bm{\hat{\theta}}(\bm{y})} \bigg] \approx \frac{H(\hat{\bm{\theta}}_{u+},\bm{y}_{v+}) - H(\hat{\bm{\theta}}_{u+},\bm{y}_{v-}) - H(\hat{\bm{\theta}}_{u-},\bm{y}_{v+}) +H(\hat{\bm{\theta}}_{u-},\bm{y}_{v-})}{4\Delta\Delta_{y}},$$
where $\bm{y}_{v+}$ and $\bm{y}_{v-}$ represent moving up and down v-th
observation by stepsize $\Delta_{y}$ in the last step of evaluating
$H(\bm{\theta},\bm{\sigma}|\bm{\lambda})$. The syntax of numericvar is
based on that of pcode

::: example*
numericvar( data, time, ode.model, par.names, state.names, par.initial,
lambda, basis.list, stepsize, y_stepsize, controls )
:::

with addition of stepsize and y_stepsize. stepsize can be specified by a
single number for which finite difference method will use it for all
parameters or a vector where derivative of each parameter is estimated
based on its own stepsize. y_stepsize allows an input of a vector where
each element indicates the stepsize for each dimension of the ODE model.

# Illustrations {#sec:illustrations}

## A simple ODE model {#ssec:simpleode}

A simple illustration uses an one-dimensional ODE model
$$\dot{X} = \theta X (1-\frac{X}{10}).
\label{eq:simpleode}$$ The following code defines the aforementioned
model that will be provided for pcode for estimating parameters:

::: example*
model \<- function(t, state, parameters) { with(as.list(c(state,
parameters)), { dX \<- theta \* X \* (1 - X / 10) return(list(dX)) }) }
:::

Given an observation period of $[0,100]$, random noise errors are added
to the ODE solution with a Normal distribution $\text{N}(0,0.5^{2})$.
Observations of the system are generated as follows:

::: example*
times \<- seq(0, 100, length.out = 101) mod \<- ode(y = state, times =
times, func = ode.model, parms = model.par) nobs \<- length(times) scale
\<- 0.5 noise \<- scale \* rnorm(n = nobs, mean = 0, sd = 1) observ \<-
mod\[, 2\] + noise
:::

Subsequently, we can visualize the observations along the true solution
of this simple ODE model in
Figure [1](#fig:simpledata){reference-type="ref"
reference="fig:simpledata"}.

![This plot demonstrates the solution of an ODE system. The black solid
line represents the true ODE solution given a known initial value. The
estimation of the solution and parameters of the ODE system usually
depends on some noisy observations of the system. The noisy
observations, the blue points, are used for
estimation.](simple_ode_observation.png){#fig:simpledata
width="\\textwidth"}

First, a basis object needs to be defined by create.bspline.basis from
fda package

::: example*
knots \<- seq(0, 100, length.out = 21) norder \<- 4 nbasis \<-
length(knots) + norder - 2 basis \<- create.bspline.basis(c(0, 100),
nbasis, norder, breaks = knots)
:::

A B-spline basis is created given 21 konts including both interior and
boundary knots acorss observation interval $(0,100)$. The order of basis
functions is 4, so there is a total of 23 basis functions. To perform
the parameter cascade method for estimating both structural and nuisance
parameters, one can use pcode in the following way

::: example*
pcode.result \<- pcode( data = observ, time = times, ode.model = model,
par.initial = 0.3, par.names = \"theta\", state.names = \"X\",
basis.list = basis, lambda = 1e2 ) pcode.result\[\"structural.par\"\]
theta 0.09995229
:::

To validate the methodology of this function, data simulation and
parameter estimation are repeated for 100 times.

![The violin and boxplot of the estimated $\theta$ over 100
replications.](simple_ode_estpar.png){width="\\textwidth"}

 

![Solution of the ODE model with $\hat{\theta}$ and
$\hat{X}(0)$.](simple_ode_solve.png){width="\\textwidth"}

Given that $\theta = 0.1$ is used for generating data, the parameter
cascade method performs impressively well. The violin plot in
Figure [\[fig:simpleresult\]](#fig:simpleresult){reference-type="ref"
reference="fig:simpleresult"} shows that the distribution of estimates
of $\theta$ covers 0.1 (indicated by the red horizontal line) and does
not fall far away from the true value. Also, the state variable X (the
bold black curve of the plot on the right) is well predicted by solving
the ODE model with estimated initial value $\hat{x}(0)$ and structural
parameters $\hat{\theta}$.

The true variance for data generating parameter $\theta$ is
$1.003 \times 10^{-5}$. We can compare the performance of two functions
bootsvar and deltavar for estimating $\text{var}(\theta)$.

::: example*
bootsvar( data = observation, time = times, ode.model = ode.model,
par.initial = 0.3, par.names = \"theta\", state.names = \"X\",
basis.list = basis, lambda = 1e2, bootsrep = 20 ) theta 1.042763e-05

deltavar( data = observation, time = times, ode.model = ode.model,
par.initial = 0.3, par.names = \"theta\", state.names = \"X\",
basis.list = basis, lambda = 1e2, stepsize = 1e-5, y_stepsize = 1e-5 )
theta 9.923318e-06
:::

Both variance estimator give excellent estimates of the true variance of
structural parameter. Subsequently, the consistency and reliability of
those estimator can be inspected by simulation. In 100 simulation
replicates, the results of two variance estimators are summarized in
Figure [2](#fig:sipmleode_varestimate){reference-type="ref"
reference="fig:sipmleode_varestimate"}.

![This plot demonstrates the estimation of variance of parameters over
100 replications. Two violin plots are results from two methods, the
bootstrap and Delta method, introduced in the article. The red line
segment indicates the true variance. Green plot includes the estimates
from the bootstrap method, whereas the origin plot includes those from
the Delta method.](comparevar.pdf){#fig:sipmleode_varestimate
width="\\textwidth"}

The bootstrap variance estimator has a smaller variation but at the cost
of higher computational effort in comparison with the Delta variance
estimator. On the other hand, the Delta variance estimator is faster but
sensitive to the choice of the step size in estimating all the
derivatives and the subsequent $\text{var}(\theta)$.

## The FitzHugh-Nagumo ODE model

The FitzHugh-Nagumo ODE model is well known for capturing the dynamic
system of neuronal firings based on the membrane potential V and
recovery variable R. This two-dimensional ODE contains 3 positive
parameters $\bm{\theta} = (a,b,c)$, and it can be defined as
$$\begin{aligned}
\dot{V} &= c(V - \frac{V^{3}}{3} + R),  \\
\dot{R} &= -\frac{1}{c}(V - a + bR).
\end{aligned}$$ The parameters are assigned with values
$(a,b,c) = (0.2,0.2,3)$ the same as in [@parcascade]. The state
variables V and R should behave as the black solid line plotted in
Figure [3](#fig:FHN_2d){reference-type="ref" reference="fig:FHN_2d"}.
Following the routine of specifying an ODE model, the FitzHugh-Nagumo
model is defined in the following function with simulated observations

::: example*
ode.model \<- function(t, state, parameters) { with(as.list(c(state,
parameters)), { dV \<- c \* (V - (V\^3) / 3 + R) dR \<- -(1 / c) \* (V -
a + b \* R) return(list(c(dV, dR))) }) } model.par \<- c(a = 0.2, b =
0.2, c = 3) desolve.mod \<- ode(y = state, times = times, func =
ode.model, parms = c(0.2, 0.2, 3)) nobs \<- length(times) scale \<- 0.1
noise_v \<- scale \* rnorm(n = nobs, mean = 0, sd = 1) noise_r \<- scale
\* rnorm(n = nobs, mean = 0, sd = 1) observ \<- matrix(NA, nrow =
length(times), ncol = 3) observ\[, 1\] \<- times observ\[, 2\] \<-
desolve.mod\[, 2\] + noise_v observ\[, 3\] \<- desolve.mod\[, 3\] +
noise_r
:::

The data generating functions and simulated data are illustrated in
Figure [3](#fig:FHN_2d){reference-type="ref" reference="fig:FHN_2d"}.

![Given the known FitzHugh-Nagumo model, we are able to generate the
true solutions. In this case, we set the parameters to be
$(a,b,c) = (0.2,0.2,3)$ and are able to generate the true solution of
the system, $V(t)$ and $R(t)$. The top graph demonstrates the solution
$V(t)$, the black line, whereas the bottom graph shows the solution
$R(t)$. For parameter estimations, we have added random noises to the
true solution and use observations as the blue points in both
graphs.](FH_observation.png){#fig:FHN_2d width="\\textwidth"}

First step of estimating parameters is to declare basis object for each
state variable. For observation period from time 0 to time 20, the basis
is defined on 101 knots with B-spline basis function of order of 4. The
total number of nuisance parameters is 103. For this example, the same
basis will be used for both dimensions $V$ and $R$:

::: example*
knots \<- seq(0, 20, length.out = 101) norder \<- 4 nbasis \<-
length(knots) + norder - 2 basis \<- create.bspline.basis(c(0, 20),
nbasis, norder, breaks = knots) basis.list \<- list(basis, basis)
:::

Then, pcode can be executed as follows:

::: example*
pcode.result \<- pcode( data = observ\[, 2:3\], time = times, ode.model
= Dmodel, par.names = c(\"a\", \"b\", \"c\"), state.names = c(\"V\",
\"R\"), par.initial = rnorm(3), lambda = 1e2, basis.list = basis.list )

pcode.result\['structural.par'\] 0.1953324 0.2203495 2.9269980
:::

In addition, we can also compare the estimation performance of the
proposed package pCODE with the existing package CollocInfer.
Figure [4](#fig:comparison){reference-type="ref"
reference="fig:comparison"} summarizes the comparison between two
packages on the parameter estimation of the Fitz-Hugh Nagumo model.

![We have repeated the data generation and parameter estimation for 100
times given the FitzHugh-Nagumo model. The parameter estimates of
$(a,b,c)$ are summarized in the violin plots ordered from left to right.
Within each replication, we also use the `CollocInfer` package for
parameters estimation. In each plot, the blue violion plot contains
estimates from `pCODE` whereas the red one contains those from
`CollocInfer`. The black horizontal lines correspond to the true model
parameters used for simulating data
sets.](comparisonbetweentwopackages.pdf){#fig:comparison
width="\\textwidth"}

Package CollocInfer requires the Jacobian and Hessian matrices for
estimating the parameters of the ODE models, whereas the proposed pCODE
package is a derivative-free method. Both packages are estimating
parameters a and b with satisfaction, and the estimates from both
methods cover the true values. However, we can see that pCODE produced
better estimations for the third parameter c in the model. Our package
does not depend on users to provide tedious details but still performs
equally well in parameter estimations.

As a part of the estimation routine, we can use two methods for
estimating the variance of structural parameters. First, the usage of
bootstrap variance estimator is

::: example*
bootsvar.res \<- bootsvar( data = data, time = observ\[, 1\], ode.model
= ode.model, par.names = c(\"a\", \"b\", \"c\"), state.names = c(\"V\",
\"R\"), par.initial = c(0.15, 0.25, 5), lambda = 1e2, basis.list =
basis.list, controls = list(smooth.lambda = 10, verbal = 1, maxeval =
20), bootsrep = 20 )
:::

based on a bootstrap sample with size of 20. Delta variance estimator is
much faster in approximating the variance through the following
execution

::: example*
deltavar.res \<- deltavar( data = observ\[, 2:3\], time = observ\[, 1\],
ode.model = Dmodel, par.names = c(\"a\", \"b\", \"c\"), state.names =
c(\"V\", \"R\"), par.initial = c(0.1, 0.3, 4), lambda = 1e2, basis.list
= basis.list, stepsize = 0.001, y_stepsize = 0.001 )
:::

## Non-normal errors

The previous examples assume that the random errors are following Normal
distributions, hence the optimization utilizes the routines of NLS
problems. In some cases, if the state variables are bounded or the
errors have a non-normal distribution, a likelihood (or log-likelihood)
function can be passed to the function pcode in order to evaluate the
objective functions. Subsequently, the function pcode will be using
optim for optimizations of both inner and outter objective functions.

The first example would reuse the simple ODE model, i.e.,
$$\dot{X} = \theta X (1-\frac{X}{10}).$$ In this case, the observations
are simulated where log-normal errors are added to the solution of the
ODE model in the following way:
$$log(y_{i}) = log(x_{i}) + \epsilon_{i}$$ where
$\epsilon_{i} \sim N(0,\sigma^{2})$. One simulated data is illustrated
in the following Figure [5](#fig:lognormal){reference-type="ref"
reference="fig:lognormal"}:

![This plots illustrates that observations from ODE systems can follow a
non-normal distribution. Given the previous simple ODE model, we
generate random noises from a log-normal distribution and add them to
the true solution of the system. Black line represents the solution of
the simple ODE system, and simulated observations are indicated by blue
points.](log_observation.png){#fig:lognormal width="\\textwidth"}

While keeping all the original settings for pcode, a log-likelihood
function, as $g(\bm{e}_{i}|\cdot)$ in
both [\[eq:innerobj\]](#eq:innerobj){reference-type="ref"
reference="eq:innerobj"}
and [\[eq:outterobj\]](#eq:outterobj){reference-type="ref"
reference="eq:outterobj"}, is additionally specified as follows

::: example*
likfun \<- function(x) { res \<- lapply(x, function(t) dnorm(t, mean =
0, sd = 0.1, log = TRUE ) ) return(sum(unlist(res))) }
:::

Instead of defining the likelihood only as a function of residuals,
pcode allows full flexibility in constructing a likelihood as long as it
returns the evaluation. Then, the likelihood function can be passed to
pcode through the argument likelihood.fun with specification of a basis
object for interpolation

::: example*
knots \<- seq(0, 100, length.out = 21) norder \<- 4 nbasis \<-
length(knots) + norder - 2 basis \<- create.bspline.basis(c(0, 100),
nbasis, norder, breaks = knots) lkh.result \<- pcode( data = observ,
time = times, likelihood.fun = likfun, par.initial = 0.1, ode.model =
ode.model, basis.list = basis, par.names = \"theta\", state.names =
\"X\", lambda = 1e2 ) lkh.result\[\"structural.par\"\] 0.105
:::

# Summary {#sec:summary}

In this article, we have reviewed the parameter cascade method for
estimating ordinary differential equations and introduced the new
package pCODE for implementing this method. pCODE offers a
derivative-free procedure to estimate any ODE models where its functions
are easily understood and to apply. Several examples of ODE models are
considered for assessing the performance of functions from pCODE
involving estimating parameters, producing variability measures, and
tuning hyper-parameters. Subsequently, we can observe that the
implemented functions provide satisfactory results presented with
details in the fourth section. Type of examples differs in both model
complexity and error distribution. Furthermore, a special case of
predator-prey model is studied when some state variables are completely
missing from observations. The package is able to simplifies the
application procedure and reproduce the estimation results as in
[@Jiguo].

One of the future works is to expand the flexibility of this package.
Even though pCODE can provide satisfactory parameter estimates, current
functions do not allow users to input the Jacobian and Hessian matrices
to speed up the optimization process. In future package developments, we
will implement the functionality to allow the input of these matrices.
One limitation of the package is the time performance of procedures,
especially for the calculation of the bootstrap variance estimator. For
that matter, we are implementing this function with parallel computation
ability to speed up the calculation. Moreover, we plan to expand the
functionalities of the package to include data visualizations and
generations of diagnostic plots and summaries.
