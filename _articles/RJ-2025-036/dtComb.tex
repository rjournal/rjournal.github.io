% !TeX root = RJwrapper.tex
\title{dtComb: A comprehensive R library and web tool for combining diagnostic tests} 

%\author{Serra Ilayda Yerlitas, Serra Bersan Gengec, Necla Kochan, Gozde Erturk Zararsiz, Selcuk Korkmaz and Gokmen Zararsiz}

\author{Serra İlayda Yerlitaş, Serra Bersan Gengeç, Necla Koçhan, Gözde Ertürk Zararsız, Selçuk Korkmaz and Gökmen Zararsız}


\maketitle

\abstract{
Background and Objective:
The development of diagnostic tests for diagnosing and differentiating diseases is a vibrant field of research. Numerous diagnostic tests have been developed, and the diagnostic accuracy and reliability of these tests play a pivotal role in their widespread usage. Researchers have focused on integrating these diverse diagnostic tests to improve diagnostic test accuracy and reliability. On the other hand, while many methods for integrating diagnostic tests have been proposed in the literature, there is currently no comprehensive software for implementing these methods. Therefore, we developed an R package, \pkg{dtComb}, to apply these combination methods in a single platform.  
Method and Materials
We employed a total of 142 methods, categorized under four main headings: (i) linear methods (8), (ii) non-linear methods (7), (iii) mathematical operators (14), and (iv) machine learning algorithms (113), for implementation in a wide range of diagnostic test combinations. We also used various standardization methods before the analysis and resampling methods to tune the hyperparameters (i.e., parameter optimization). The \pkg{dtComb} package encompasses implementations for 142 approaches, realized through 18 distinct R functions. The R package development of \pkg{dtComb} was facilitated with \pkg{devtools} and documented with \pkg{roxygen2}. Software tests were conducted with 271 unit tests using the testthat library of the R programming language.
Results
The \pkg{dtComb} package allows combination methods to calculate the combined score to diagnose the disease and increase diagnostic accuracy within a targeted study population. We demonstrated the functionalities/capabilities of the \pkg{dtComb} package by analyzing a real dataset, the abdominal pain dataset. The results showed that the package effectively calculates the combined scores using various existing methods.
Conclusion
\pkg{dtComb} is a comprehensive R package developed to combine two markers. Additionally, a web tool has been created to facilitate ease of use for healthcare professionals and non-R users. It allows researchers to combine markers on a single platform. In this regard, dtComb can be viewed as a pioneering tool, offering this unique combination feature. The web tool is available free at \url{https://biotools.erciyes.edu.tr/dtComb/}.
}

\section{Introduction}
Correct diagnosis is a fundamental element of effective treatment in medicine and healthcare. Accurate diagnosis helps prevent unnecessary and potentially harmful treatments or procedures. Without it, patients might undergo ineffective or harmful treatments, increasing medical costs and potential complications. Reference tests are often considered the gold standard for diagnosing a disease or condition. However, these tests can be expensive and sometimes invasive (i.e., risky) or uncomfortable for the patient. On the other hand, despite their lower accuracy compared to reference tests, diagnostic markers play a crucial role in diagnosing diseases and have gained significant importance in medical research \citep{novielli2013meta}. Their role in the medical field continues to grow, providing opportunities for earlier and more effective disease management and treatment.\\
Studies have shown that a single diagnostic test or marker cannot accurately diagnose diseases, particularly complex diseases such as autoimmune diseases, certain cancers, or neurological disorders. Clinicians conduct multiple tests separately on the same person to collect as much information as possible to ensure a thorough and accurate assessment or to diagnose specific conditions \citep{kang2016linear}. Numerous studies, including this one, have shown that effectively combining available information can improve the sensitivity and specificity of diagnostics or diagnostic accuracy. Several approaches have been developed to combine diagnostic tests or markers. For instance, Su and Liu \citep{su1993linear} found that Fisher's linear discriminant function generates a linear combination of markers with either proportional or disproportional covariance matrices, aiming to maximize sensitivity consistently across the entire selectivity spectrum under a multivariate normal distribution model. In contrast, another approach introduced by Pepe and Thomson \citep{pepe2000combining} relies on ranking scores, eliminating the need for linear distributional assumptions when combining diagnostic tests. Liu et al. \citep{liu2011min} have introduced a computationally efficient semi-linear min-max combination approach. It simplifies the process by focusing on finding a single λ value, which maximizes the Mann-Whitney U statistic of the Area Under Curve (AUC). Similarly, Sameera et al. \citep{sameera2016binary} developed the best linear combination method using minimax, asserting its superior effectiveness to previously proposed methods. These methods either rely on the assumption of a fundamental distribution, such as the normal distribution, or adopt an estimation-based approach. In this context, the focus often shifts towards prediction, as the primary concern is typically the diagnostic test performance (accuracy) in detecting specific conditions in future patients \citep{wang2013predicting}.\\
Besides linear combination methods, non-linear approaches can also be employed to integrate the diagnostic tests. These approaches mainly focus on two statistical concepts: polynomial regression models and splines. Polynomial regression models address two critical issues/factors: interactions between markers and creating a polynomial feature space with degree parameters. This leads to developing models like polynomial logistic regression, polynomial ridge regression, and polynomial lasso regression, which consider these factors. Spline-based methods involve determining the number of knots and the degrees of the polynomials positioned between these knots. It also cretaed models such as β spline, Generalized Additive Models (GAMs) with smoothing splines, and natural cubic splines. Apart from these linear and non-linear combinations described in the literature, various other combinations, like the ratio of two diagnostic tests, can also be identified using mathematical operators. These diverse approaches allow flexibility in combining diagnostic tests for improved accuracy and effectiveness.\\
Apart from the aforementioned methods, a number of basic mathematical operations such as addition, multiplication, subtraction, and division have also been implemented to combine markers. These mathematical operators can potentially increase and improve diagnostic tests' efficacy by combining markers in particular ways \citep{fagan2007cerebrospinal, nyblom2004high, balta2016relation}.\\
Machine learning (ML) algorithms have recently been adopted to combine diagnostic tests. These advanced algorithms offer a robust and data-driven approach to enhance the integration and analysis of diagnostic data, potentially leading to more accurate and effective diagnostic solutions. More than 100 publications/studies focus on implementing of ML algorithms in diagnostic tests \citep{bardella1991iga, bozkurt2014comparison, chen2015diagnosis}. Zararsiz et al., for instance, aimed to improve diagnostic accuracy by combining D-dimer and leukocyte markers with various machine (statistical) learning algorithms to differentiate between surgical and non-surgical pathologies in patients with acute abdominal pain \citep{zararsiz2016statistical}. Another application was observed in the study where Abate et al. performed the combination of markers for the diagnosing Alzheimer's disease using a regression tree. The findings reported an increase in diagnosis accuracy \citep{abate2020conformation}. \\
Despite the numerous studies that have applied machine learning algorithms and various combination methods to diagnostic tests, an easy-to-use, up-to-date, and comprehensive tool for implementing existing combination approaches has not yet been developed. Therefore, in this study, we introduced \pkg{dtComb}, an R package designed to implement the existing combination approaches. First, we examined the theoretical background of related combination methods. Subsequently, we provided an illustrative example to demonstrate the viability of the package. Finally, we presented an enhanced web application of the \pkg{dtComb} package, available at \url{https://biotools.erciyes.edu.tr/dtComb/}, which will be beneficial particularly for non-R users.

\section{Material and methods}
This section will provide an overview of the combination methods implemented in the literature. Before applying these methods, we will also discuss the standardization techniques available for the markers, the resampling methods during model training, and, ultimately, the metrics used to evaluate the model’s performance.

\subsection{Combination approaches}
\subsubsection{Linear combination methods}
The "\pkg{dtComb}" package comprises eight distinct linear combination methods, which will be elaborated in this section. Before investigating these methods, we briefly introduce some notations which will be used throughout this section. \\
Notations: \\
Let $D_{i}, i = 1, 2, …, n_1$ be the marker values of $i$th individual in the diseased group, where $D_i=(D_{i1},D_{i2})$, and $H_j, j=1,2,…,n_2$  be the marker values of $j$th individual in the healthy group, where $H_j=(H_{j1},H_{j2})$. Let $x_{i1}=c(D_{{i1}},H_{j1})$ be the values of the first marker, and $x_{i2}=c(D_{i2},H_{j2})$ be values of the second marker for the $i$th individual $i=1,2,...,n$. Let $D_{i,min}=\min(D_{i1},D_{i2})$, $D_{i,max}=\max(D_{i1},D_{i2})$, $H_{j,min}=\min(H_{j1},H_{j2})$, $H_{j,max}=\max(H_{j1},H_{j2})$ and $c_i$ be the resulting combination score of the $i$th individual. 
\begin{itemize}
    \item \textit{Logistic regression:} Logistic regression is a statistical method used for binary classification. The logistic regression model estimates the probability of the binary outcome occurring based on the values of the independent variables. It is one of the most commonly applied methods in diagnostic tests, and it generates a linear combination of markers that can distinguish between control and diseased individuals. Logistic regression is generally less effective than normal-based discriminant analysis, like Su and Liu's multivariate normality-based method, when the normal assumption is met \citep{ruiz1991asymptotic,efron1975efficiency}. On the other hand, others have argued that logistic regression is more robust because it does not require any assumptions about the joint distribution of multiple markers \citep{cox1989analysis}. Therefore, it is essential to investigate the performance of linear combination methods derived from the logistic regression approach with non-normally distributed data.\\
    The objective of the logistic regression model is to maximize the logistic likelihood function. In other words, the logistic likelihood function is maximized to estimate the logistic regression model coefficients.\\
    \begin{equation}
    \label{eq:1}
c=\frac{exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}}{1+exp(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}}
\end{equation}
    The logistic regression coefficients  can provide the maximum likelihood estimation of the model, producing an easily interpretable value for distinguishing between the two groups.
    \item \textit{Scoring based on logistic regression:} The method primarily uses a binary logistic regression model, with slight modifications to enhance the combination score. The regression coefficients, as predicted in Eq \ref{eq:1}, are rounded to a user-specified number of decimal places and subsequently used to calculate the combination score \citep{leon2006bedside}.  
       \begin{equation}
c= \beta_1 x_{i1}+\beta_2 x_{i2}
\end{equation}
    \item \textit{Pepe \& Thompson's method:} Pepe \& Thompson have aimed to maximize the AUC or partial AUC to combine diagnostic tests, regardless of the distribution of markers \citep{pepe2000combining}. They developed an empirical solution of optimal linear combinations that maximize the Mann-Whitney U statistic, an empirical estimate of the ROC curve. Notably, this approach is distribution-free. Mathematically, they maximized the following objective function
    \begin{equation}
\text{maximize} \; U(a)= \frac{1}{n_1 n_2} \sum_{i=1}^{n_1} \sum_{j=1}^{n_2} I\left[D_{i1}+\alpha D_{i2}\geq H_{j1}+\alpha H_{j2}\right]
\end{equation}
    \begin{equation}
c= x_{i1}+\alpha x_{i2}
\label{eq:4}
\end{equation}
    where $a \in [-1,1]$ is interpreted as the relative weight of $x_{i2}$ to $x_{i1}$ in the combination, the weight of the second marker. This formula aims to find $\alpha$  to maximize $U(a)$. Readers are referred to see (Pepe and Thomson) \citep{pepe2000combining}.
    \item \textit{Pepe, Cai \& Langton's method:} Pepe et al. observed that when the disease status and the levels of markers conform to a generalized linear model, the regression coefficients represent the optimal linear combinations that maximize the area under the ROC curves \citep{pepe2006combining}. The following objective function is maximized to achieve a higher AUC value:
\begin{equation}
\text{maximize} \; U(a)= \frac{1}{n_1 n_2} \sum_{i=1}^{n_1} \sum_{j=1}^{n_2} I\left[D_{i1}+\alpha D_{i2}> H_{j1}+\alpha H_{j2}\right] + \frac{1}{2}I\left[D_{i1}+\alpha = H_{j1} + \alpha H_{j2}\right]
\end{equation}
    Before calculating the combination score using Eq \ref{eq:4}, the marker values are normalized or scaled to be constrained within the range of 0 to 1. In addition, it is noted that the estimate obtained by maximizing the empirical AUC can be considered as a particular case of the maximum rank correlation estimator from which the general asymptotic distribution theory was developed. Readers are referred to Pepe (2003, Chapters 4–6) for a review of the ROC curve approach and more details \citep{pepe2003statistical}.
    
    \item \textit{Min-Max method:} The Pepe \& Thomson method is straightforward if there are two markers. It is computationally challenging if we have more than two markers to be combined. To overcome the computational complexity issue of this method, Liu et al. \citep{liu2011min} proposed a non-parametric approach that linearly combines the minimum and maximum values of the observed markers of each subject. This approach, which does not rely on the normality assumption of data distributions (i.e., distribution-free), is known as the Min-Max method and may provide higher sensitivity than any single marker. The objective function of the Min-Max method is as follows:
\begin{equation}
\text{maximize} \; U(a)= \frac{1}{n_1 n_2} \sum_{i=1}^{n_1} \sum_{j=1}^{n_2} I[D_{i,max}+\alpha D_{i,min}> H_{j,max}+\alpha H_{j,min}] \end{equation}
\begin{equation}
    c= x_{i,max}+\alpha x_{i,min}
\end{equation}\\
    where $x_{i,max}=\max⁡(x_{i1},x_{i2})$ and $x_{i,min}=\min⁡(x_{i1},x_{i2})$.\\
    
The Min-Max method aims to combine repeated measurements of a single marker over time or multiple markers that are measured with the same unit. While the Min-Max method is relatively simple to implement, it has some limitations. For example, markers may have different units of measurement, so standardization can be needed to ensure uniformity during the combination process. Furthermore, it is unclear whether all available information is fully utilized when combining markers, as this method incorporates only the markers' minimum and maximum values into the model \citep{kang2016linear}.

    \item \textit{Su \& Liu's method:} 	Su and Liu examined the combination score separately under the assumption of two multivariate normal distributions when the covariance matrices were proportional or disproportionate \citep{su1993linear}. Multivariate normal distributions with different covariances were first utilized in classification problems \citep{anderson1961classification}. Then, Su and Liu also developed a linear combination method by extending the idea of using multivariate distributions to the AUC, showing that the best coefficients that maximize AUC are Fisher's discriminant coefficients. Assuming that  $D~N(\mu_D, \sum_D)$ and $H~N(\mu_H, \sum_H)$ represent the multivariate normal distributions for the diseased and non-diseased groups, respectively. The Fisher’s coefficients are as follows:
\begin{equation}
(\alpha, \beta) = (\sum_{D} + \sum_{H})^{-1} \mu \label{eq:alpha_beta}
\end{equation}
    where $\mu=\mu_D-\mu_H$. The combination score in this case is:
\begin{equation}
c= \alpha x_{i1}+ \beta x_{i2} 
\label{eq:9}
\end{equation}
    \item \textit{The Minimax method:} The Minimax method is an extension of Su & Liu's method \citep{sameera2016binary}. Suppose that D follows a multivariate normal distribution  $D\sim N(\mu_D, \sum_D)$, representing the diseased group, and H follows a multivariate normal distribution $H\sim N(\mu_H, \sum_H)$, representing the non-diseased group. Then Fisher’s coefficients are as follows:
\begin{equation}
(\alpha, \beta) = \left[t\sum_{D} + (1-t)\sum_{H}\right]^{-1} (\mu_D - \mu_H) \label{eq:alpha_beta_expression}
\end{equation}

    Given these coefficients, the combination score is calculated using Eq \ref{eq:9}. In this formula, \textit{t} is a constant with values ranging from 0 to 1. This value can be hyper-tuned by maximizing the AUC. 

    \item \textit{Todor & Saplacan’s method:} Todor and Saplacan's method uses the sine and cosine trigonometric functions to calculate the combination score \citep{todor2014tools}. The combination score is calculated using $\theta \in[-\frac{\pi}{2},\frac{\pi}{2}]$, which maximizes the AUC within this interval. The formula for the combination score is given as follows:
\begin{equation}
c= \sin{(\theta)}x_{i1}+\cos{(\theta)}x_{i2}
\end{equation}
\end{itemize}

\subsubsection{Non-linear combination methods}
In addition to linear combination methods, the \pkg{dtComb} package includes seven non-linear approaches, which will be discussed in this subsection. In this subsection, we will use the following notations:
$x_{ij}$: the value of the \textit{j}th marker for the \textit{i}th individual, $i=1,2,...,n$ and $j=1,2$ \textit{d}: degree of polynomial regressions and splines, $d = 1,2,…,p$.

\begin{itemize}
    \item \textit{Logistic Regression with Polynomial Feature Space:} This approach extends the logistic regression model by adding extra predictors created by raising the original predictor variables to a certain power. This transformation enables the model to capture and model non-linear relationships in the data by including polynomial terms in the feature space \citep{james2021introduction}. The combination score is calculated as follows:
\begin{equation}
c=\frac{exp\left(\beta_0 + \beta_1 x_{ij} + \beta_2 x_{ij}^2+...+\beta_p x_{ij}^p\right)}{1+exp\left(\beta_0 + \beta_1 x_{ij} + \beta_2 x_{ij}^2+...+\beta_p x_{ij}^p\right)}
\end{equation}
    where $c_i$ is the combination score for the \textit{i}th individual and represents the posterior probabilities. 

    \item \textit{Ridge Regression with Polynomial Feature Space:} This method combines Ridge regression with expanded feature space created by adding polynomial terms to the original predictor variables. It is a widely used shrinkage method when we have multicollinearity between the variables, which may be an issue for least squares regression. This method aims to estimate the coefficients of these correlated variables by minimizing the residual sum of squares (RSS) while adding a term (referred to as a regularization term) to prevent overfitting. The objective function is based on the L2 norm of the coefficient vector, which prevents overfitting in the model (Eq \ref{eq:beta_hat_r}). The Ridge estimate is defined as follows:
\begin{equation}
\hat{\beta}^R = \text{argmin}_{\beta} \text{RSS} + \lambda \sum_{j=1}^{2} \sum_{d=1}^{p} \beta_j^{d^2} \label{eq:beta_hat_r}
\end{equation}

where 
\begin{equation}
RSS=\sum_{i=1}^{n}\left(y_i-\beta_0-\sum_{j=1}^{2}\sum_{d=1}^{p} \beta_j^d x_{ij}^d\right) 
\end{equation}
    and $\hat{\beta}^R$ denotes the estimates of the coefficients of the Ridge regression, and the second term is called a penalty term where $\lambda \geq 0$ is a shrinkage parameter. The shrinkage parameter, $\lambda$, controls the amount of shrinkage applied to regression coefficients. A cross-validaiton is implemented to find the shrinkage parameter. To implement the Ridge regression in combining the diagnostic tests, we used the \pkg{glmnet} package \citep{friedman2010regularization}.

    \item \textit{Lasso Regression with Polynomial Feature Space:} Similar to Ridge regression, Lasso regression is also a shrinkage method that adds a penalty term to the objective function of the least square regression. The objective function, in this case, is based on the L1 norm of the coefficient vector, which leads to the sparsity in the model. Some of the regression coefficients are precisely zero when the tuning parameter λ is sufficiently large. This property of the Lasso method allows the model to automatically identify and remove less relevant variables and reduce the algorithm's complexity. The Lasso estimates are defined as follows:

    \begin{equation}
\hat{\beta}^L = \text{argmin}_{\beta} \text{RSS} + \lambda \sum_{j=1}^{2} \sum_{d=1}^{d} | \beta_j^d | \label{eq:beta_hat_l}
\end{equation}


    To implement the Lasso regression in combining the diagnostic tests, we used the \pkg{glmnet} package \citep{friedman2010regularization}.

    \item \textit{Elastic-Net Regression with Polynomial Feature Space:} Elastic-Net Regression is a method that combines Lasso (L1 regularization) and Ridge (L2 regularization) penalties to address some of the limitations of each technique. The combination of the two penalties is controlled by two hyperparameters, α∈[0,1] and λ, which enable you to adjust the trade-off between the L1 and L2 regularization terms \citep{james2021introduction}. For the implementation of the method, the \pkg{glmnet} package is used \citep{friedman2010regularization}.
    \item \textit{Splines:} Another non-linear combination technique frequently applied in diagnostic tests is the splines. Splines are a versatile mathematical and computational technique that has a wide range of applications. These splines are piecewise functions that make interpolating or approximating data points possible. There are several types of splines, such as cubic splines. Smooth curves are created by approximating a set of control points using cubic polynomial functions. When implementing splines, two critical parameters come into play: degrees of freedom and the choice of polynomial degrees (i.e., degrees of the fitted polynomials). These user-adjustable parameters, which influence the flexibility and smoothness of the resulting curve, are critical for controlling the behavior of splines. We used the \pkg{splines} package \citep{venable2016splines} in the R programming language to implement splines.

    \item \textit{Generalized Additive Models with Smoothing Splines and Generalized Additive Models with Natural Cubic Splines:} Regression models are of great interest in many fields to understand the importance of different inputs. Even though regression is widely used, the traditional linear models often fail in real life as effects may not be linear. Another method called generalized additive models was introduced to identify and characterize non-linear regression \citep{sameera2016binary}. Smoothing Splines and Natural Cubic Splines are two standard methods used within GAMs to model non-linear relationships. To implement these two methods, we used the \pkg{gam} package in R \citep{hastie2023gam}. The method of GAMs with Smoothing Splines is a more data-driven and adaptive approach where smoothing splines can automatically capture non-linear relationships without specifying the number of knots (specific points where two or more polynomial segments are joined together to create a piecewise-defined curve or surface) or the shape of the spline in advance. On the other hand, natural cubic splines are preferred when we have prior knowledge or assumptions about the shape of the non-linear relationship. Natural cubic splines are more interpretable and can be controlled by the number of knots \citep{elhakeem2022using}.
\end{itemize}

\subsubsection{Mathematical Operators}
This section will mention four arithmetic operators, eight distance measurements, and the exponential approach. Also, unlike other approaches, in this section, users can apply logarithmic, exponential, and trigonometric (sinus and cosine) transformations on the markers. Let $x_{ij}$ represent the value of the \textit{j}th variable for the \textit{i}th observation, with $i=1,2,...,n$ and $j=1,2$. Let the resulting combination score for the \textit{i}th individual be $c_i$.
\begin{itemize}
    \item \textit{Arithmetic Operators:} Arithmetic operators such as addition, multiplication, division, and subtraction can also be used in diagnostic tests to optimize the AUC, a measure of diagnostic test performance. These mathematical operations can potentially increase the AUC and improve the efficacy of diagnostic tests by combining markers in specific ways. For example, if high values in one test indicate risk, while low values in the other indicate risk, subtraction or division can effectively combine these markers.
    \item \textit{Distance Measurements:} While combining markers with mathematical operators, a distance measure is used to evaluate the relationships or similarities between marker values \citep{minaev2018distance,pandit2011comparative,cha2007comprehensive}. It's worth noting that, as far as we know, no studies have been integrating various distinct distance measures with arithmetic operators in this context. Euclidean distance is the most commonly used distance measure, which may not accurately reflect the relationship between markers. Therefore, we incorporated a variety of distances into the package we developed. These distances are given as follows:\\

\textit{Euclidean:}
\begin{equation}
c = \sqrt{(x_{i1} - 0)^2 + (x_{i2} - 0)^2} \label{eq:euclidean_distance}
\end{equation}  
\\
\textit{Manhattan:}
\begin{equation}
c = |x_{i1} - 0| + |x_{i2} - 0| \label{eq:manhattan_distance}
\end{equation}
\\
\textit{Chebyshev:} 
\begin{equation}
c = \max\{|x_{i1} - 0|, |x_{i2} - 0|\} \label{eq:max_absolute}
\end{equation}
\\
\textit{Kulczynskid:}
\begin{equation}
c = \frac{|x_{i1} - 0| + |x_{i2} - 0|}{\min\{x_{i1}, x_{i2}\}} \label{eq:custom_expression}
\end{equation}
\\
\textit{Lorentzian:} 
\begin{equation}
c = \ln(1 + |x_{i1} - 0|) + \ln(1 + |x_{i2} - 0|) \label{eq:ln_expression}
\end{equation}
\\
 \textit{Taneja:}
\begin{equation}
c = z_1 \left( \log \left( \frac{z_1}{\sqrt{x_{i1} \epsilon}} \right) \right) + z_2 \left( \log \left( \frac{z_2}{\sqrt{x_{i2} \epsilon}} \right) \right) \label{eq:log_expression}
\end{equation}
\\
where $z_1 = \frac{x_{i1} - 0}{2}, \quad z_2 = \frac{x_{i2} - 0}{2}$ \\

\textit{Kumar-Johnson:}
\begin{equation}
c = \frac{{(x_{i1}^2 - 0)^2}}{{2(x_{i1} \epsilon)^{\frac{3}{2}}}} + \frac{{(x_{i2}^2 - 0)^2}}{{2(x_{i2} \epsilon)^{\frac{3}{2}}}}, \quad  \epsilon=0.0000) \label{eq:c_expression}
\end{equation}
\\
\textit{Avg:} 
\begin{equation}
c = \frac{{|x_{i1} - 0| + |x_{i2} - 0| + \max\{(x_{i1} - 0),(x_{i2} - 0)\}}}{2} \label{eq:c_expression}
\end{equation}\\

    \item \textit{Exponential approach:} The exponential approach is another technique to explore different relationships between the diagnostic measurements. The methods in which one of the two diagnostic tests is considered as the base and the other as an exponent can be represented as  $x_{i1}^{(x_{i2})}$ and $x_{i2}^{(x_{i1})}$. The specific goals or hypothesis of the analysis, as well as the characteristics of the diagnostic tests, will determine which method to use.
\end{itemize}
\subsubsection{Machine-Learning algorithms}
Machine learning algorithms have been increasingly implemented in various fields, including the medical field, to combine diagnostic tests. Integrating diagnostic tests through ML can lead to more accurate, timely, and personalized diagnoses, particularly valuable in complex medical cases where multiple factors must be considered. In this study, we aimed to incorporate almost all ML algorithms in the package we developed. To achieve this goal, we took advantage of the \pkg{caret} package in R \citep{kuhn2008caret}. This package includes 190 classification algorithms that could be used to train models and make predictions. Our study focused on models that use numerical inputs and produce binary responses depending on the variables/features and the desired outcome. This selection process resulted in 113 models we further implemented in our study. We then classified these 113 models into five classes using the same idea given in \citep{zararsiz2016statistical}: (i) discriminant classifiers, (ii) decision tree models,  (iii) kernel-based classifiers, (iv) ensemble classifiers, and (v) others. Like in the \pkg{ caret} package, \code{mlComb()} sets up a grid of tuning parameters for a number of classification routines, fits each model, and calculates a performance measure based on resampling. After the model fitting, it uses the \code{predict()} function to calculate the probability of the "event" occurring for each observation. Finally, it performs ROC analysis based on the probabilities obtained from the prediction step.

\subsection{Standardization}
Standardization is converting/transforming data into a standard scale to facilitate meaningful comparisons and statistical inference. Many statistical techniques frequently employ standardization to improve the interpretability and comparability of data. We implemented five different standardization methods that can be applied for each marker, the formulas of which are listed below:

\begin{itemize}
    \item Z-score: \( \frac{{x - \text{mean}(x)}}{{\text{sd}(x)}} \)
    \item T-score: \( \left( \frac{{x - \text{mean}(x)}}{{\text{sd}(x)}} \times 10 \right) + 50 \)
    \item Range: \( \frac{{x - \min(x)}}{{\max(x) - \min(x)}} \)
    \item Mean: \( \frac{x}{{\text{mean}(x)}} \)
    \item Deviance: \( \frac{x}{{\text{sd}(x)}} \)
\end{itemize}


\subsection{Model building}
After specifying a combination method from the \pkg{dtComb} package, users can build and optimize model parameters using functions like \code{mlComb()}, \code{linComb()}, \code{nonlinComb()}, and \code{mathComb()}, depending on the specific model selected. Parameter optimization is done using n-fold cross-validation, repeated n-fold cross-validation, and bootstrapping methods for linear and non-linear approaches (i.e., \code{linComb()}, \code{nonlinComb()}). Additionally, for machine learning approaches (i.e., \code{mlComb()}), all of the resampling methods from the \pkg{caret} package are used to optimize the model parameters. The total number of parameters being optimized varies across models, and these parameters are fine-tuned to maximize the AUC. Returned object stores input data, preprocessed and transformed data, trained model, and resampling results.
\subsection{Evaluation of model performances}

A confusion matrix, as shown in Table \ref{tab:confusion_matrix}, is a table used to evaluate the performance of a classification model and shows the number of correct and incorrect predictions. It compares predicted and actual 

\begin{table}[h]
\centering
\caption{Confusion Matrix}
\label{tab:confusion_matrix}
\begin{tabular}{llll}
\hline
\multirow{2}{*}{Predicted labels} & \multicolumn{2}{l}{Actual class labels} & Total \\ \cline{2-4} 
                                  & Positive & Negative &       \\ \hline
Positive                          & TP       & FP       & TP+FP \\ 
Negative                          & FN       & TN       & FN+TN \\ 
Total                             & TP+FN    & FP+TN    & n     \\ \hline
\end{tabular}

   \begin{flush}
\tiny  TP: True Positive, TN: True Negative, FP: False Positive, FN: False Negative, n: Sample size
    \end{flush}
\end{table}
\noindent
class labels, with diagonal elements representing the correct predictions and off-diagonal elements representing the number of incorrect predictions. The \pkg{dtComb} package uses the \pkg{epiR} package [32], which includes different performance metrics. Various performance metrics accuracy rate (ACC), Kappa statistic (κ), sensitivity (SE), specificity (SP), apparent and true prevalence (AP, TP), positive and negative predictive values (PPV, NPV), positive and negative likelihood ratio (PLR, NLR), the proportion of true outcome negative subjects that test positive (False T+ proportion for true D-), the proportion of true outcome positive subjects that test negative (False T- proportion for true D+), the proportion of test positive subjects that are outcome negative (False T+ proportion for T+), the proportion of test negative subjects (False T- proportion for T-) that are outcome positive measures are available in the \pkg{dtComb} package. These metrics are summarized in Table \ref{tab:performance_metrics}.

\begin{table}[htbp]
    \centering \small
    \caption{Performance Metrics and Formulas}
    \label{tab:performance_metrics}
    \begin{tabular}{ll}
    \hline
    \textbf{Performance Metric} & \textbf{Formula} \\
    \hline
    Accuracy & $\text{ACC} = \frac{{\text{TP} + \text{TN}}}{2}$ \\
    Kappa & $\kappa = \frac{{\text{ACC} - P_e}}{{1 - P_e}}$ \\
    & $P_e = \frac{{(\text{TN} + \text{FN})(\text{TP} + \text{FP}) + (\text{FP} + \text{TN})(\text{FN} + \text{TN})}}{{n^2}}$ \\
    Sensitivity (Recall) & $\text{SE} = \frac{{\text{TP}}}{{\text{TP} + \text{FN}}}$ \\
    Specificity & $\text{SP} = \frac{{\text{TN}}}{{\text{TN} + \text{FP}}}$ \\
    Apparent Prevalence & $\text{AP} = \frac{{\text{TP}}}{{n}} + \frac{{\text{FP}}}{{n}}$ \\
    True Prevalence & $\text{TP} = \frac{{\text{AP} + \text{SP} - 1}}{{\text{SE} + \text{SP} - 1}}$ \\
    Positive Predictive Value (Precision) & $\text{PPV} = \frac{{\text{TP}}}{{\text{TP} + \text{FP}}}$ \\
    Negative Predictive Value & $\text{NPV} = \frac{{\text{TN}}}{{\text{TN} + \text{FN}}}$ \\
    Positive Likelihood Ratio & $\text{PLR} = \frac{{\text{SE}}}{{1 - \text{SP}}}$ \\
    Negative Likelihood Ratio & $\text{NLR} = \frac{{1 - \text{SE}}}{{\text{SP}}}$ \\
    The Proportion of True Outcome Negative Subjects That Test Positive & $\frac{{\text{FP}}}{{\text{FP} + \text{TN}}}$ \\
    The Proportion of True Outcome Positive Subjects That Test Negative & $\frac{{\text{FN}}}{{\text{TP} + \text{FN}}}$ \\
    The Proportion of Test Positive Subjects That Are Outcome Negative & $\frac{{\text{FP}}}{{\text{TP} + \text{FN}}}$ \\
    The Proportion of Test Negative Subjects That Are Outcome Positive & $\frac{{\text{FN}}}{{\text{FN} + \text{TN}}}$ \\
    \hline
    \end{tabular}
\end{table}


\subsection{Prediction of the test cases}
The class labels of the observations in the test set are predicted with the model parameters derived from the training phase. It is critical to emphasize that the same analytical procedures employed during the training phase have also been applied to the test set, such as normalization, transformation, or standardization. More specifically, if the training set underwent Z-standardization, the test set would similarly be standardized using the mean and standard deviation derived from the training set. The class labels of the test set are then estimated based on the cut-off value established during the training phase and using the parameters of the model that is trained using the training set.
\subsection{Technical details and the structure of dtComb}
The \pkg{dtComb} package is implemented using the R programming language (\url{https://www.r-project.org/}) version 4.2.0. Package development was facilitated with devtools \citep{wickham2016devtools} and documented with roxygen2 \citep{wickham2013roxygen2}. Package testing was performed using 271 unit tests \citep{wickham2011testthat}. Double programming was performed using Python (\url{https://www.python.org/}) to validate the implemented functions \citep{meszaros2007xunit}.\\

To combine diagnostic tests, the \pkg{dtComb} package allows the integration of eight linear combination methods, seven non-linear combination methods, arithmetic operators, and, in addition to these, eight distance metrics within the scope of mathematical operators and a total of 113 machine learning algorithms from the \pkg{caret} package \citep{kuhn2008caret}. These are summarized in Table \ref{tab:dtComb_features}.  
%Table 3

\begin{table}[htbp]
    \centering
    \caption{Features of dtComb}
    \label{tab:dtComb_features}
    \begin{tabular}{l p{10cm}}
    \hline
    \textbf{Modules (Tab Panels)} & \textbf{Features} \\
\hline
    \multirow{4}{*}{Combination Methods} & 
    \begin{itemize}
        \item Linear Combination Approach (8 Different methods)
        \item Non-linear Combination Approach (7 Different Methods)
        \item Mathematical Operators (14 Different methods)
        \item Machine-Learning Algorithms (113 Different Methods)
    \end{itemize} \\

  \multirow{2}{*}{Standardization Methods} &  
    \begin{itemize}
        \item Linear, non-linear, and mathematical methods
        \begin{itemize}
            \item Z-score
            \item T-score
            \item Range
            \item Mean
            \item Deviance
        \end{itemize}
        \item 16 different preprocessing methods for ML \citep{kuhn2008caret}
    \end{itemize} \\
    \multirow{2}{*}{Resampling} &
    \begin{itemize}
        \item 3 different methods for linear and non-linear combination methods
        \begin{itemize}
            \item Bootstrapping
            \item Cross-validation
            \item Repeated cross-validation
        \end{itemize}
        \item 12 different resampling methods for ML \citep{kuhn2008caret}
    \end{itemize} \\
    {Cutpoints} & 
    \begin{itemize}
        \item 34 different methods for optimum cutpoints \citep{yin2014optimal}
    \end{itemize} \\
    \hline
    \end{tabular}
\end{table}


\section{Results}

\subsection{Dataset}
To demonstrate the functionality of the \pkg{dtComb} package, we conduct a case study using four different combination methods. The data used in this study were obtained from patients who presented at Erciyes University Faculty of Medicine, Department of General Surgery, with complaints of abdominal pain \citep{zararsiz2016statistical,akyildiz2010value}. The dataset comprised D-dimer levels and leukocyte counts of 225 patients, divided into two groups: the first group consisted of 110 patients who required an immediate laparotomy. In comparison, the second group comprised 115 patients who did not. After the evaluation of conventional treatment, the patients who underwent surgery due to their postoperative pathologies are placed in the first group. In contrast, those with a negative result from their laparotomy were assigned to the second group.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\textwidth]{dtComb/Figure_1.pdf}
  \caption{Combination steps of two diagnostic tests}
  \label{figure:rlogo}
\end{figure}


\subsection{Implementation of the \pkg{dtComb} package}
To exemplify the utilization of the \pkg{dtComb} package, we implemented the Splines method from non-linear combination approaches on Abdominal pain data. The parameters that must be predefined to implement the Splines method are the degree and degrees of freedom, both set to 3. To implement the Splines method, we split the data into two parts: a training set comprising 70\% of the data and a test set comprising the remaining 30\%. The model is trained on the training set, and the model parameters used in the prediction phase are fine-tuned using a five-fold cross-validation with 10-repeats. Since higher values indicate higher risks, the Youden index is chosen from Cut-off methods with Direction = “<”. We note that markers have not been standardized, and the results are presented at the confidence level (CI 95\%).

The area under ROC curves for D-dimer levels and leukocyte counts on the logarithmic scale and combination score were 0.816, 0.802, and 0.911, respectively (Table \ref{tab:AUC_markers_combination_score} ). The ROC curves generated with the combination score from the splines model, D-dimer levels, and leukocyte count markers are also given in Fig. \ref{figure:roc}, showing that the combination score has the highest AUC. It is observed that the splines method significantly improved between 9.5\% and 10.9\% in AUC statistics compared to D-dimer level and leukocyte counts, respectively. Controlling Type I error using Bonferroni correction, comparison of combination score with markers yielded significant results ($p<0.05$) (Table \ref{tab:AUC_comparison}). Table \ref{tab:diagnostic_test_results} summarizes the diagnostic test results for each marker and the non-linear combination approach. Optimal cut-off values for both markers and the combined approach are also given in this table. Table 6 shows that the TP value of the combination score is higher than that of single markers.

\begin{table}[htbp]
    \centering
    \caption{Area Under the Curves of Markers and Combination Score}
    \label{tab:AUC_markers_combination_score}
    \begin{tabular}{@{}lllll@{}}
    \toprule
    \textbf{Variable} & \textbf{AUC (\%95 CI)} & \textbf{SE (AUC)} & \textbf{z} & $ \textbf{p}$ \\ 
    \midrule
    D-dimer level & 0.816 (0.751-0.880) & 0.033 & 9.557 & \textbf{$<$0.001} \\
    Log(leukocyte count) & 0.802 (0.728-0.877) & 0.038 & 7.971 & \textbf{$<$0.001} \\
    Combination score & 0.911 (0.868-0.954) & 0.022 & 18.803 & \textbf{$<$0.001} \\
    \bottomrule
    \end{tabular}
    \small
    
    \begin{flush}
         \small SE - Standard Error. Statistically significant p-values are shown in bold (p $<$ 0.05).%

    \end{flush}
\end{table}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\textwidth]{dtComb/Figure_2.pdf}
  \caption{\textbf{ROC curves.} ROC curves for combined diagnostic tests, with sensitivity displayed on the y-axis and 1-specificity displayed on the x-axis. As can be observed, the combination score produced the highest AUC value, indicating that the combined strategy performs the best overall.}
  \label{figure:roc}
\end{figure}



%Table 5
\begin{table}[htbp]
    \centering
    \caption{Area Under the Curve Comparison of Markers and Combination Score}
    \label{tab:AUC_comparison}
    \begin{tabular}{@{}llllll@{}}
    \toprule
    \textbf{Marker 1 (A)} & \textbf{Marker 2 (B)} & \textbf{|A-B|} & \textbf{SE(|A-B|)} & \textbf{z} & $\textbf{p}$ \\ 
    \midrule
    Combination score & D-dimer & 0.095 & 0.024 & 4.007 & \textbf{$<$0.001} \\
    Combination score & Log(leukocyte count) & 0.108 & 0.034 & 3.201 & \textbf{0.001} \\
    D-dimer & Log(leukocyte count) & 0.013 & 0.048 & 0.278 & 0.781 \\
    \bottomrule
    \end{tabular}
\end{table}

%Table 6
\begin{table}[htbp]
    \centering
    \caption{Diagnostic test results for each marker and non-linear combination model}
    \label{tab:diagnostic_test_results}
    \begin{tabular}{@{}lllll@{}}
    \toprule
    \textbf{Variable} & \textbf{TP} & \textbf{TN} & \textbf{FP} & \textbf{FN} \\ 
    \midrule
    D-dimer ($>1.6 \; \mu g$ FEU/mL) & 66 & 53 & 28 & 11 \\
    Log(leukocyte count) ($>4.16$) & 61 & 60 & 21 & 16 \\
    Combination score ($>0.448$) & 65 & 69 & 12 & 12 \\
    \bottomrule
    \end{tabular}
\end{table}

\newpage
Table \ref{tab:diagnostic_measures} summarizes the performance metrics used to assess the effectiveness of diagnostic tests, i.e., single marker, D-dimer or leukocyte, and combined approach. These measures are calculated along with their corresponding 95\% confidence intervals. The combination of markers was found to have higher specificity and positive-negative predictive value than log-transformed leukocyte counts and D-dimer level. Conversely, D-dimer has a greater sensitivity than the others.
%Table 7
\begin{table}[htbp]
    \centering
    \caption{Statistical diagnostic measures with 95\% confidence intervals for each marker and the combination Score}
    \label{tab:diagnostic_measures}
    \begin{tabular}{@{}lccc@{}}
    \toprule
    \textbf{Diagnostic Measures (95\% CI)} & \textbf{D-dimer level} & \textbf{Log(leukocyte count)} & \textbf{Combination score} \\
    \midrule
    Apparent prevalence & 0.59 (0.51-0.67) & 0.52 (0.44-0.60) & 0.49 (0.41-0.57) \\
    True prevalence & 0.49 (0.41-0.57) & 0.49 (0.41-0.57) & 0.49 (0.41-0.57) \\
    Sensitivity & 0.86 (0.76-0.93) & 0.79 (0.68-0.88) & 0.84 (0.74-0.92) \\
    Specificity & 0.65 (0.54-0.76) & 0.74 (0.63-0.83) & 0.85 (0.76-0.92) \\
    Positive predictive value & 0.70 (0.60-0.79) & 0.74 (0.64-0.83) & 0.84 (0.74-0.92) \\
    Negative predictive value & 0.83 (0.71-0.91) & 0.79 (0.68-0.87) & 0.85 (0.76-0.92) \\
    Positive likelihood ratio & 2.48 (1.81-3.39) & 3.06 (2.08-4.49) & 5.70 (3.35-9.69) \\
    Negative likelihood ratio & 0.22 (0.12-0.39) & 0.28 (0.18-0.44) & 0.18 (0.11-0.31) \\
    False T+ proportion for true D- & 0.35 (0.24-0.46) & 0.26 (0.17-0.37) & 0.15 (0.08-0.24) \\
    False T- proportion for true D+ & 0.14 (0.07-0.24) & 0.21 (0.12-0.32) & 0.16 (0.08-0.26) \\
    False T+ proportion for T+ & 0.30 (0.21-0.40) & 0.26 (0.17-0.36) & 0.16 (0.08-0.26) \\
    False T- proportion for T- & 0.17 (0.09-0.29) & 0.21 (0.13-0.32) & 0.15 (0.08-0.24) \\
    Accuracy & 0.75 (0.68-0.82) & 0.77 (0.69-0.83) & 0.85 (0.78-0.90) \\
    \bottomrule
    \end{tabular}
\end{table}

For a comprehensive analysis, distribution and scatter plots are generated to visualize each group's density and distribution of combination scores (Fig. \ref{figure:scatter}a, \ref{figure:scatter}b). Significant variations in specificity and sensitivity measurements, corresponding to different cut-off point values, are also displayed in Figure 3c.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\textwidth]{dtComb/Figure_3.pdf}
  \caption{\textbf{Distribution, scatter, and sens\&spe plots of the combination score acquired with the training model.} Distribution of the combination score for two groups: needed and not needed (a). Scatter graph with classes on the x-axis and combination score on the y-axis (b). Sensitivity and specificity graph of the combination score c. While colors show each class in Figures (a) and (b), in Figure (c), the colors represent the sensitivity and specificity of the combination score.}
    \label{figure:scatter}
\end{figure}
\subsection{Comparison of classifiers}
In this section, we discuss and compare the performance of the fitted models in detail. Various measures were considered to compare model performances, including AUC, ACC, SEN, SPE, PPV, and NPV. AUC statistics, with 95\% CI, have been calculated for each marker and method. The resulting statistics are as follows: 0.816 (0.751–0.880), 0.802 (0.728–0.877), 0.888 (0.825–0.930), 0.911 (0.868–0.954), 0.877 (0.824-0.929), and 0.875 (0.821-0.930) for D-dimer, Log(leukocyte), Pepe, Cai \& Langton, Splines, Addition, and Support Vector Machine (SVM). The results revealed that the predictive performances of markers and the combination of markers are significantly higher than random chance in determining the use of laparoscopy ($p<0.05$). According to the overall AUC and accuracies, the combined approach fitted with the Splines method performed better than the other methods (Fig. \ref{figure:radar}). The highest sensitivity and NPV were observed with the Addition method, while the highest specificity and PPV were observed with the Splines method.

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{dtComb/Figure_4.pdf}
  \caption{\textbf{Radar plots of trained models and performance measures of two markers.} Radar plots summarize the diagnostic performances of two markers and various combination methods in the training dataset. These plots illustrate the performance metrics such as AUC, ACC, SEN, SPE, PPV, and NPV measurements. In these plots, the width of the polygon formed by connecting each point indicates the model's performance in terms of AUC, ACC, SEN, SPE, PPV, and NPV metrics. It can be observed that the polygon associated with the Splines method occupies the most expensive area, which means that the Splines method performed better than the other methods.}
  \label{figure:radar}
\end{figure}
During the prediction phase, 67 observations were tested using the fitted Splines method. The output for each observation consisted of the predicted label, determined based on the combination score and the model-derived cut-off value. Table 8 displays the estimated combination scores and labels for the first ten observations in the test set.
%Table 8
\subsection{3.Web interface for the \pkg{dtComb} package}
The primary goal of developing the \pkg{dtComb} package is to combine numerous distinct combination methods and make them easily accessible to researchers. Furthermore, the package includes diagnostic statistics and visualization tools for diagnostic tests and the combination score generated by the chosen method. Nevertheless, it is worth noting that using R code may pose challenges for physicians and those unfamiliar with R programming. We have also developed a user-friendly web application for dtComb using "Shiny" \citep{chang2017shiny} to address this. This web-based tool is publicly accessible and provides an interactive interface with all the functionalities found in the \pkg{dtComb} package. \\

To initiate the analysis, users must upload their data by following the instructions outlined in the "Data upload" tab of the web tool. For convenience, we have provided three example datasets on this page to assist researchers in practicing the tool's functionality and to guide them in formatting their own data (as illustrated in Fig. \ref{figure:web}a). We also note that ROC analysis for a single marker can be performed within the ‘ROC Analysis for Single Marker(s)’ tab in the data upload section of the web interface.
\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{dtComb/Figure_5_new.pdf}
  \caption{\textbf{Web interface of the \pkg{dtComb} package.}}
  \label{figure:web}
\end{figure}
In the "Analysis" tab, one can find two crucial subpanels:
\begin{itemize}
    \item Plots (Fig. \ref{figure:web}b): This section offers various visual representations, such as ROC curves, distribution plots, scatter plots, and sensitivity nad specificity plots. These visualizations help users assess single diagnostic tests and the combination score, which is generated using user-defined combination methods.
    \item Results (Fig. \ref{figure:web}c): In this subpanel, one can access a range of statistics. It provides insights into the combination score and single diagnostic tests, AUC statistics, and comparisons to evaluate how the combination score fares against individual diagnostic tests, and various diagnostic measures. One can also predict new data based on the model parameters set previously and stored in the "Predict" tab (Fig. \ref{figure:web}d). If needed, one can download the model created during the analysis to keep the parameters of the fitted model. This lets users make new predictions by reloading the model from the "Predict" tab. Additionally, all the results can easily be downloaded using the dedicated download buttons in their respective tabs.
\end{itemize}
\section{Discussion}
The accuracy and reliability of diagnostic tests are critical factors in determining their widespread adoption and use in clinical practice. Researchers have focused on integrating various diagnostic approaches to achieve higher levels of accuracy. While numerous diagnostic tests exist, combining their results coherently and effectively has been challenging. Several methods, each with its strengths and limitations, have been developed for combining diagnostic tests or markers \citep{su1993linear,pepe2000combining,liu2011min,sameera2016binary,pepe2006combining,todor2014tools}.
Linear combination methods, including Su \& Liu, Min-Max, and Pepe \& Thomson, are frequently used in medicine and diagnostics to improve the performance of diagnostic tests for various medical problems \citep{erturkzararsiz2023linear,ma2020combination,aznar-gimeno2023comparing}. These methods involve linearly combining multiple diagnostic markers or variables to generate a new combined score, improving the diagnostic test's accuracy and reliability. Non-linear combination methods such as Lasso regression and splines have also been proposed and utilized in medical diagnostics to incorporate complexities and the interaction of variables that linear methods might not capture effectively. Application on real data showed that using a combination of markers is more effective or accurate in predicting the need for laparoscopy compared to relying on a single marker alone. Given the overall AUC and accuracies, the combined approach fitted with the Splines method performed better than the other methods (Fig. \ref{figure:radar}). Recently, others have integrated machine-learning algorithms to enhance diagnostic tests’ accuracy and the reliability \citep{chang2022artificial,alkayyali2023systematic,ghazal2022intelligent}.
Despite numerous combination methods proposed in the literature, a method commonly implemented in clinical practice involves the division of two diagnostic tests due to its ease of implementation \citep{fagan2007cerebrospinal,nyblom2004high,balta2016relation,klemt2023complete,ji2017monocyte}. Although this simple approach improves diagnostic scores compared to using a single marker, more complex yet potentially effective combination methods have been neglected or underrated. Therefore, a significant need exits to integrate these combination methods into clinical practice. Additionally, while packages like \pkg{ROCR}, \pkg{pROC}, \pkg{PRROC}, \pkg{plotROC}, \pkg{precrec}, and \pkg{ROCit} focus on a single marker, a comprehensive software tool capable of implementing any combination of methods is unavailable \citep{sing2005rocr,turck2011proc,grau2015prroc}. To remedy this, we introduced \pkg{dtComb}, a groundbreaking, user-friendly software package that includes almost all existing approaches from the diverse literature on diagnostic test combinations. The \pkg{dtComb} package, developed within the R language environment, represents a significant step forward in diagnostics. One of the critical strengths of \pkg{dtComb} lies in its ability to accommodate a wide range of diagnostic test methodologies, providing users with a comprehensive toolkit to merge and analyze diverse test results. This ensures that clinicians can tailor their diagnostic approach according to the specific requirements of individual cases, leading to more personalized and precise healthcare interventions. On the other hand, it can be challenging, especially for physicians, healthcare professionals, and those unfamiliar with R programming, to use the \pkg{dtComb} package. Therefore, we also developed a user-friendly web application for \pkg{dtComb}, which is accessible to the public and offers an interactive interface with all the functionalities in the \pkg{dtComb} package.
\section{Conclusions}
In conclusion, we developed an R package, \pkg{dtComb}, to combine markers for diagnostic tests. The package executes combination methods through distinct functions, each specifically designed to generate combined scores for various markers. These functions enable researchers to proficiently amalgamate diagnostic tests, catering to a diverse and extensive research population. Moreover, a user-friendly web tool has been developed to enhance accessibility for both researchers and clinicians who are non-R users. The tool is freely available through \url{https://biotools.erciyes.edu.tr/dtComb/}, and the source code is available on the GitHub repository at \url{https://github.com/gokmenzararsiz/dtComb}. Its simple interface enables researchers to easily combine two markers in a single platform, simplifying the analysis process and increasing efficiency. The tool is groundbreaking because it introduces a novel feature that addresses a critical need in marker discovery research while providing a more straightforward and time-saving process.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%\section{Summary}
%This file is only a basic article template. For full details of \emph{The R Journal} style and information on how to prepare your article for submission, see the \href{https://journal.r-project.org/share/author-guide.pdf}{Instructions for Authors}.

\bibliography{dtCombreferences}

\address{Serra Ilayda Yerlitas\\
  Department of Biostatistics \\
  Erciyes University\\
  Türkiye\\
  (ORCiD: 0000-0003-2830-3006)\\
  \email{ilaydayerlitas340@gmail.com}}

\address{Serra Bersan Gengec\\
  Drug Application and Research Center (ERFARMA)\\
  Erciyes University\\
  Türkiye\\
  \email{serrabersan@gmail.com}}

\address{Necla Koçhan\\
  Department of Mathematics\\
  Izmir University of Economics\\
  Türkiye\\
  (ORCiD: 0000-0003-2355-4826)\\
  \email{necla.kayaalp@gmail.com}}
  
  \address{Gözde Ertürk Zararsız\\
  Department of Biostatistics \\
  Erciyes University\\
  Türkiye\\
  (ORCiD if desired)\\
  \email{gozdeerturk9@gmail.com}}
  
  \address{Selçuk Korkmaz\\
  Department of Biostatistics \\
  Trakya University\\
  Türkiye\\
  (ORCiD if desired)\\
  \email{selcukorkmaz@gmail.com}}
  
  \address{Gökmen Zararsız\\
  Department of Biostatistics \\
  Erciyes University\\
  Türkiye\\
  (ORCiD: 0000-0001-5801-1835)\\
  \email{gokmen.zararsiz@gmail.com}}
