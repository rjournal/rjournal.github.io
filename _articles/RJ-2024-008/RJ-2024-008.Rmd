---
title: 'nortsTest: An R Package for Assessing Normality of Stationary Processes'
date: '2025-01-10'
abstract: |
  Normality is the central assumption for analyzing dependent data in several time series models, and the literature has widely studied normality tests. However, the implementations of these tests are limited. The nortsTest package is dedicated to fill this void. The package performs the asymptotic and bootstrap versions of the tests of Epps and Lobato and Velasco and the tests of Psaradakis and Vavra, random projections and El Bouch for normality of stationary processes. These tests are for univariate stationary processes but for El Bouch that also allows bivariate stationary processes. In addition, the package offers visual diagnostics for checking stationarity and normality assumptions for the most used time series models in several R packages. This work aims to show the package's functionality, presenting each test performance with simulated examples and the package utility for model diagnostic in time series analysis.
draft: no
author:
- name: Asael Alonzo Matamoros
  affiliation: Aalto University
  address:
  - Department of Computer Science
  - Eespo, Finland
  url: https://asael697.github.io
  email: izhar.alonzomatamoros@aalto.fi
- name: Alicia Nieto-Reyes
  affiliation: Universidad de Cantabria
  address:
  - Departmento de Mathemáticas, Estadística y Computación
  - Avd. de los Castros s/n. 39005 Santander, Spain
  url: https://orcid.org/0000-0002-0268-3322
  email: alicia.nieto@unican.es
- name: Claudio Agostinelli
  affiliation: University of Trento
  address:
  - Department of Mathematics
  - Via Sommarive, 14 - 38123 Povo
  url: https://orcid.org/0000-0001-6702-4312
  email: claudio.agostinelli@unitn.it
type: package
output:
  rjtools::rjournal_article:
    self_contained: yes
    toc: no
bibliography: RJreferences.bib
date_received: '2022-10-21'
volume: 16
issue: 1
slug: RJ-2024-008
journal:
  lastpage: 156
  firstpage: 135

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE, 
  warning = FALSE, 
  message = FALSE
)

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "figures/",
  dev = "png",
  dpi = 150,
  fig.asp = 0.8,
  fig.width = 8,
  fig.height = 4,
  out.width = "60%",
  fig.align = "center"
)

library(kableExtra)
library(nortsTest)
library(fGarch)
library(knitr)
library(forecast)
```

# Introduction

Normality (*a set of observations sampled from a Gaussian process*) is an essential assumption in various statistical models. Therefore, developing procedures for testing this assumption is a topic that has gained popularity over several years. Most existing literature and implementation is dedicated to independent and identically distributed random variables [@Dagostino1987]; no results show that these tests are consistent when applied to stationary processes. For this context, several tests have been proposed over the years, but as far as we know, no `R` package or consistent implementation exists.

The proposed \CRANpkg{nortsTest} package provides seven test implementations to check normality of stationary processes. This work aims to present a review of these tests and introduce the package functionality. Thus, its novelty lies in being the first package and paper dedicated to the implementation of normality tests for stationary processes. The implemented tests are: (i) the asymptotic *Epps* test, [@epps1987] and [@nietoreyes2014], based on the characteristic function and (ii) its sieve bootstrap approximation [@psaradakis2020normality], (iii) the corrected *Skewness-Kurtosis* (SK) test implemented by  @Lobato2004 as an asymptotic test and (iv) by @psaradakis2020normality with a sieve bootstrap approximation, (v) the *random projections test* proposed by @nietoreyes2014, which makes use of the tests in (i) and (iii), (vi) the *Psadarakis and Vávra test* [@vavra2017] that uses a bootstrap approximation of the @anderson1952 test statistic for stationary linear processes and (vii) a normality test by @el2022normality for multivariate dependent samples. Tests (i) to (vi) are for univariate stationary processes.
 
 Furthermore, we propose the  `check_residual()` function for checking time-series models' assumptions. This function returns a report for stationarity, seasonality, normality tests and visual diagnostics. `check_residual()` supports models from the most used packages for time-series analysis, such as the  packages \CRANpkg{forecast} [@Rob2007] and \CRANpkg{aTSA} [@aTSA] and even functions in the base `R` [@R]; for instance, it supports the  `HoltWinters` (stats `R` package) function for the Holt and Winters method [@Holt2004]. In addition, the proposed \CRANpkg{nortsTest} package has already been applied in the literature, see @Nieto-Reyes:2022-1 and @Nieto-Reyes:2022-2.

Section 2 provides the theoretical background, including preliminary concepts and results. Section 3 introduces the normality tests for stationary processes, each subsection introducing a test framework and including examples of the tests functions with simulated data. Section 4 provides numerical experiments with simulated data and a real-world application: Subsection 4.1 reports a simulation study for the implemented normality tests and Subsection 4.2  the package's functionality for model checking in a real data application. The *carbon dioxide* data measured in the Malua Loa Observatory [@astsa] is analyzed using a state space model from the \CRANpkg{forecast} package, evaluating the model's assumptions using our proposed `check_residuals()` function. Section 5 discusses the package functionality and provides our conclusions. Furthermore, we mention our future intended work on the package. 

# Preliminary concepts

This section provides some theoretical aspects of stochastic processes that are a necessary theoretical framework for the following sections. @shumway2010 and @Ts2010 give more details of the following definitions and results below.

For the purpose of this work,  $T$ is a set of real values denoted as time, $T \subseteq \mathbb{R},$ for instance $T=\mathbb{N}$ or $T=\mathbb{Z},$ the naturals or integer numbers respectively. We denote by $X:=\{X_t\}_{t\in T}$ a \textit{stochastic process} with $X_t$ a real random variable for each $t\in T.$ Following this notation, a  \textit{time-series} is just a finite collection of ordered observations of $X$ [@shumway2010]. An important measure for a stochastic process is its mean function $\mu(t) := E[X_t]$ for each $t \in T$, where $E[\cdot]$ denotes the usual expected value of a random variable. A generalization of this measure is the k-th order centered moment function $\mu_k(t) := E[(X_t -\mu(t))^k]$ for each $t \in T$ and $k > 1;$ with the process variance function being the second order centered moment, $\sigma^2(t) := \mu_2(t)$. Other important measures are the auto-covariance and auto-correlation functions, which measure the linear dependency between two different time points of a given process. For any $t,s \in T,$ they are, respectively, 
$$
\gamma(t,s) := E[(X_t -\mu(t))(X_s - \mu(s))] \mbox{ and } \rho(t,s) := \dfrac{\gamma(t,s)}{\sqrt{\mu_2(t)}\sqrt{\mu_2(s)}}.
$$
Other widely used measure functions for the analysis of  processes are the skewness and kurtosis functions, defined as $s(t) := \mu_3(t)/[\mu_2(t)]^{3/2}$ and $k(t) := \mu_4(t)/[\mu_2(t)]^2$ for each $t\in T,$ respectively. 

A generally used assumption for stochastic processes is stationarity. It has a key role in forecasting procedures of classic time-series modeling [@Ts2010] or as a principal assumption in de-noising methods for signal theory [@W2006]. 

#### Definition 1
A stochastic process $X$ is said to be \emph{strictly stationary} if, for every collection $\tau = \{t_1,t_2,\ldots, t_k\} \subset T$ and $h > 0$, the joint distribution of $\{X_t\}_{t \in \tau}$ is identical to that of $\{X_{t+h}\}_{t \in \tau}.$

The previous definition is strong for applications. A milder version of it, which makes use of the process' first two moments, is weak stationarity.

#### Definition 2
A stochastic process $X$ is said to be  \emph{weakly stationary} if its mean function is constant in time, $\mu(t) = \mu$, its auto-covariance function only depends on the difference between times, $\gamma(s,t) = \sigma|t-s|$ for a $\sigma\in \mathbb{R}$, and it has a finite variance function, $\mu_2(t) = \mu_2 < \infty$.

For the rest of this work, the term *stationary* will be used to specify a weakly stationary process. A direct consequence of the stationarity assumption is that the previous measure functions get simplified. Thus, given a stationary stochastic process $X,$ its mean function, $k$-th order centered moment, for $k>1,$ and auto-covariance function are respectively,
$$
 \mu = E[X_{t_1}]\mbox{, } \mu_k = E[(X_{t_1} -\mu)^k] \mbox{ and } \gamma(h) = E[(X_{t_1+h}-\mu)(X_{t_1}-\mu)],
$$
which are independent of $t_1\in T.$ 

Given a sample $x_1, \ldots, x_n,$  $n\in\mathbb{N},$   of equally spaced observations of $X,$ their corresponding estimators, sample mean, sample $k$-th order centered moment and sample auto-covariance, are respectively
$$
 \widehat{\mu} := n^{-1}\sum_{i=1}^nx_i\mbox{, } \widehat{\mu}_k := n^{-1}\sum_{i=1}^n(x_i - \widehat{\mu})^k \mbox{ and }\widehat{\gamma}(h) := n^{-1}\sum_{i = 1}^{n-h}(x_{i+h} - \widehat{\mu})(x_i - \widehat{\mu}).
$$ 

A particular case in which stationarity implies strictly stationarity is a  Gaussian process.

#### Definition 3
A stochastic process $X$ is said to be a *Gaussian process* if for every finite collection $\tau = \{t_1,t_2,\ldots, t_k\} \subset T,$ the joint distribution of $\{X_t\}_{t \in \tau}$ has a multivariate normal distribution.

A series of mean zero uncorrelated random variables with finite constant variance is known as *white noise*. If additionally, it is formed of independent and identically distributed (i.i.d) normal random variables,  it is known as *Gaussian white noise*; which is a particular case of stationary Gaussian process. For the rest of the work, $X_t \sim N(\mu,\sigma^2)$ denotes that the random variable $X_t$ is normally distributed with mean $\mu$ and variance $\sigma^2$ and $\chi^2(v)$ denotes the Chi square distribution with $v$ degrees of freedom. 

Other classes of stochastic processes can be defined using collections of white noise, for instance, the linear process.

#### Definition 4
Let $X$ be a stochastic process. $X$ is said to be *linear* if it can be written as
$$
X_t = \mu + \sum_{i\in\mathbb{Z}}\phi_i\epsilon_{t-i},
$$
where $\{\epsilon_i\}_{i\in\mathbb{Z}}$ is a collection of white noise random variables and $\{\phi_i\}_{i\in\mathbb{Z}}$ is a set of real values such that $\sum_{i\in\mathbb{Z}} |\phi_j| < \infty.$

An important class of processes is the *auto-regressive moving average* ($ARMA$). @Box1990 introduced it for time series analysis and forecast,  becoming very well-known in the 90s and early 21st century. 

#### Definition 5
For any non-negative integers $p,q,$ a stochastic process $X$ is an  $ARMA(p,q)$ process if it is a stationary process and 
\begin{equation}
  X_t = \sum_{i=0}^p \phi_iX_{t-i}  +\sum_{i=0}^q \theta_i\epsilon_{t-i}, (\#eq:ARMA)
\end{equation}
where $\{\phi_i\}_{i=0}^p$ and $\{\theta_i\}_{i=0}^q$  are sequences of real values with $\phi_0= 0,$ $\phi_p\neq 0,$ $\theta_0=1$ and $\theta_q\neq 0$ and $\{\epsilon_{i}\}_{i\in\mathbb{Z}}$ is a collection of white noise random variables.

Particular cases of  $ARMA$ processes are those known as auto-regressive ($AR(p) := ARMA(p,0)$) and  mean average ($MA(q) := ARMA(0,q)$) processes. Additionally, a \emph{random walk} is a non stationary AR(1) 
process satisfying \@ref(eq:ARMA) with $p=1,$ $\phi_1 = 1$ and $q=0.$ Several properties of an $ARMA$ process can be extracted from its structure. For that, the $AR$ and $MA$ polynomials are introduced
$$
 AR:\text{ } \phi(z) = 1-\sum_{i=0}^p \phi_i z^i \text{ and } MA:\text{ } \theta(z) = \sum_{i=0}^q \theta_i z^i,
$$
where $z$ is a complex number and, as before, $\phi_0 = 0,$ $\phi_p\neq 0,$ $\theta_0= 1$ and $\theta_q\neq 0.$ Conditions for stationarity, order selection and, process behavior are properties studied from these two polynomials.

For modeling volatility in financial data, @Bollerslev1986 proposed the *generalized auto-regressive conditional heteroscedastic* (GARCH) class of processes as a generalization of the *auto-regressive conditional heteroscedastic* (ARCH) processes [@engle1982].

#### Definition 6
For any $p,q \in \mathbb{N}$, a stochastic process $X$ is a $GARCH(p,q)$ process if it satisfies $X_t = \mu + \sigma_{t}\epsilon_t$ with
$$
\sigma_t^2 = \alpha_0 +\sum_{i=1}^p\alpha_i \epsilon_{t-i}^2 +\sum_{i=1}^q \beta_{i}\sigma^2_{t-i}.
$$
$\mu$ is the process mean,  $\sigma_0$ is a positive constant value, $\{\alpha_i\}_{i=1}^p$ and $\{\beta_i\}_{i=1}^q$ are non-negative sequences of real values and $\{\epsilon_{t}\}_{t \in T}$ is a collection of i.i.d. random variables. 

A more general class of processes are the *state-space models* ($SSMs$), which have gained popularity over the years because they do not impose on the process common restrictions such as linearity or stationarity and are flexible in incorporating the process different characteristics [@OBrien2010]. They are widely used for smoothing [@west2006] and forecasting [@Rob2007] in time series analysis. The main idea is to model the process dependency with two equations: the *state equation*, which models how parameters change over time, and the *innovation equation*, which models the process in terms of the parameters. Some particular SSMs that analyze the level, trend and seasonal components of the process are known as *error, trend, and seasonal* (ETS)  models. There are over 32 different variations of  ETS models [@Hyndman2008]. One of them is the *multiplicative error, additive trend-seasonality* $(ETS(M,A,A))$ model.

#### Definition 7
A SSM process $X$ follows an ETS(M,A,A) model, if the process accepts  
$$
X_t = [L_{t-1} +T_{t-1} + S_{t-1}](1 + \epsilon_t)
$$ 
as  innovation equation and
\begin{eqnarray*}L_t &= &L_{t-1} +T_{t-1} +\alpha (L_{t-1} +T_{t-1} +S_{t-m})\epsilon_t\\
	T_t &= &T_{t-1} + \beta (L_{t-1} +T_{t-1} +S_{t-m})\epsilon_t\\
	S_t &= &S_{t-m} + \gamma (L_{t-1} +T_{t-1} +S_{t-m})\epsilon_t,
\end{eqnarray*}  
as state equations.
$\alpha, \beta,\gamma \in [0,1]$, $m\in\mathbb{N}$ denotes the period of the series and $\{\epsilon_t\}$ are i.i.d normal random variables. For each $t\in\mathbb{Z},$ $L_t$, $T_t$ and $S_t$ represent respectively the level, trend and seasonal components. 

# Normality tests for stationary processes

Extensive literature exists on goodness of fit tests for normality under the assumption of independent and identically distributed random variables, including, among others, Pearson's chi-squared test [@Pearson1895], Kolmogorov-Smirnov test [@Smirnov1948], Anderson-Darling test [@anderson1952], SK test [@jarque1980] and  Shapiro-Wilk test, [@SWtest1965] and [@Royston1982]. These procedures have been widely used in many studies and applications, see @Dagostino1987 for further details. There are no results, however, showing that the above tests are consistent in the context of stationary processes, in which case the independence assumption is violated. For instance, @Gasser1975 provides a simulation study where Pearson's chi-squared test has an excessive rejection rate under the null hypothesis for dependent data. For this matter, several tests for stationary processes have been proposed over the years. A selection of which we reference here. @epps1987 provides a test based on the characteristic function, @Hinich1982 proposes a similar test based on the process' spectral density function [@Berg2010, for further insight]. @Gasser1975 gives a correction of the SK test, with several modifications made in @Lobato2004, @bai2005 and @MarianZach2017, which are popular in many financial applications. @Meddahi2005 constructs a test based on Stein's characterization of a Gaussian distribution. Using the random projection method [@Cuesta2007], @nietoreyes2014 build a test that upgrades the performance of @epps1987 and @Lobato2004 procedures. Furthermore, @vavra2017 adapts the @anderson1952 statistic for stationary linear processes approximating its sample distribution with a sieve bootstrap procedure.

Despite the existing literature, consistent implementations of goodness of fit test for normality of stationary processes in programming languages such as `R` or `Python` are limited. This is not the case for normality of independent data, the \CRANpkg{nortest} package [@nortest2015] implements tests such as Lilliefors [@Wilkinson1986], Shapiro-Francia [@Royston1993], Pearson's chi-squared, Cramer von Misses [@vonMisses1962] and Anderson-Darling. For a multivariate counterpart, the \CRANpkg{mvnTest} package [@mvntest] implements the multivariate Shapiro-Wilk, Anderson-Darling, Cramer von Misses, Royston [@Royston1992], Doornik and Hansen [@DH2008], Henze and Zirkler [@HZ1990] and the multivariate Chi square test [@S2_2016]. For the case of dependent data, we present here the \CRANpkg{nortsTest} package. Type within `R` `install.packages("nortsTest", dependencies = TRUE)` to install its latest released version from `CRAN`. \CRANpkg{nortsTest} performs the tests proposed in @epps1987, @Lobato2004, @psaradakis2020normality, @nietoreyes2014,  @vavra2017 and @el2022normality.

Additionally, the package offers visualization functions for descriptive time series analysis and several diagnostic methods for checking stationarity and normality assumptions for the most used time series models of several `R` packages. To elaborate on this, Subsection 3.1 introduces the package functionality and software and Subsection 3.2  provides an overview of tests for checking stationary and seasonality. Finally, Subsections 3.3-3.5 present a general framework of each of the implemented normality tests and their functionality by providing simulated data examples.

## Software

The package works as an extension of the \CRANpkg{nortest} package [@nortest2015], which performs normality tests in random samples but for independent data. The  building block  functions of the \CRANpkg{nortsTest} package are:

  + `epps.test()`, function that implements the  test of Epps,
  
  + `epps_bootstrap.test()`, function that implements a bootstrap approximation of the test of Epps,
  
  + `lobato.test()`, function that implements the asymptotic test of Lobato and Velasco,
  
  + `lobato_bootstrap.test()`, function that implements a bootstrap approximation of the test of Lobato and Velasco,
  
  + `rp.test()`, function that implements the random projection test of Nieto-Reyes, Cuesta-Albertos and Gamboa, 
  
  + `vavra.test()`, function that implements the test of Psaradaki and Vavra, and
  
  + `elbouch.test()`, function that implements the test of El Bouch, Michel and Comon.

Each of these functions accepts a `numeric` (*numeric*) or `ts` (*time series*) class object for storing data, and returns a `htest` (*hypothesis test*) class object with the main results for the test. To guarantee the accuracy of the results, each test performs unit root tests for checking stationarity and seasonality (see Subsection 3.2) and displays a warning message if any of them is not satisfied. 

For visual diagnostic, the package offers different plot functions based on the \CRANpkg{ggplot2} package [@ggplot2]: the `autoplot()` function plots `numeric`, `ts` and `mts` (*multivariate time series*) classes while the `gghist()` and `ggnorm()` functions are for plotting histogram and qq-plots respectively; and on the  \CRANpkg{forecast} package [@Rob2007]: `ggacf()` and `ggPacf()`  for the display of the  auto-correlation and partial auto-correlations functions respectively.

Furthermore, inspired in the function `checkresiduals()` of the \CRANpkg{forecast} package, we provide the `check_residuals()` function to test  the model assumptions using the estimated residuals. The upgrade of our proposal is that, besides providing plots for visual diagnosis (setting the `plot` option as `TRUE`), it does check stationarity, seasonality  (*Subsection 3.2*) and normality, presenting a report of the used tests and conclusions for assessing the model's assumptions. An illustration of these functions is provided in Subsection  4.2, where we show the details of the functions and their utility for assumptions commonly checked in time series modeling.

## Tests for stationarity

For checking stationarity, the \CRANpkg{nortsTest} package uses \textit{unit root} and \textit{seasonal unit root}  tests. These tests work similarly, checking whether a specific process follows a random walk model, which clearly is a non-stationary process. 

### Unit root tests

A linear stochastic process $X$ that follows a random walk model is non stationary. Its AR polynomial is $\phi(z) = 1 - z$, whose solution (root) is unique and equal to one. Thus, it is common to test the non stationarity of a  linear process by checking whether its AR polynomial has a unit root (a root equal to one).

The most commonly used tests for unit root testing are  *Augmented Dickey-Fuller* [@dickey1984], *Phillips-Perron* [@Perron1988],  *kpps* [@KppsI1992] and  \textit{Ljung-Box} [@Box]. In particular, the *Ljung-Box* test contrasts the null auto-correlation hypothesis of identically distributed Gaussian random variables, which is equivalent to test stationarity. The `uroot.test()` and `check_residual()` functions  perform these tests, making use of the  \CRANpkg{tseries} package [@tseries].

### Seasonal unit root tests

Let $X$ be a stationary process and $m$ its period. Note that for observed data, $m$ generally corresponds to the number of observations per unit of time. $X$ follows a seasonal random walk if it can be written as
$$
 X_t = X_{t-m} + \epsilon_t,
$$
where $\epsilon_t$ is a collection of i.i.d random variables. In a similar way, the process $X$ is non-stationary if it follows a seasonal random walk. Or equivalently, $X$ is non stationary if the seasonal AR(1) polynomial ($\phi_m(z) = 1 - \phi z^m$) has a unit root. The `seasonal.test()` and `check_residuals()` functions perform the *OCSB test* [@ocsb1988] from the \CRANpkg{forecast} package and the *HEGY* [@Hegy1993] and *Ch* [@ch1995] tests from the \CRANpkg{uroot} package [@uroot].

## Tests of Epps

The $\chi^2$ test for normality proposed by @epps1987  compares the empirical characteristic function of the one-dimensional marginal of the process with the one of a normally distributed random variable evaluated at certain points on the real line. Several authors, including @Lobato2004, @vavra2017 and @el2022normality, point out that the greatest challenge in the Epps' test is its implementation procedure, which we address with the \CRANpkg{nortsTest} package. Other existing tests based on the empirical characteristic function of the one-dimensional marginal of the process include @hong1999hypothesis and the references therein. This test differs, however, in that it uses spectral analysis and derivatives. 

Furthermore, @meintanis2016review reviews on testing procedures based on the empirical characteristic function. There, it is commented about the random projection test [@nietoreyes2014, and here below] as a recent development of Epps' test. In fact, in @nietoreyes2014 the consistency of Epps test is improved by taking at random the elements at which the characteristic function is evaluated. Additionally, @el2022normality proposes a sieve bootstrap modification of the Epps' test. In addition to the classical asymptotic Epps' test, we include these last two approaches here, and in the package, see the Example below and the paragraph before it. Let us provide now the foundation behind the Epps' tests.

Let $X$ be a stationary stochastic process  that satisfies
\begin{equation}
  \sum_{t=-\infty}^{\infty}|t|^k|\gamma(t)| <\infty \mbox{ for some } k >0. (\#eq:a)
\end{equation}
The null hypothesis is that the one-dimensional marginal distribution of $X$ is a Gaussian process. The procedure for constructing the test consists of defining a function $g$, estimating its inverse spectral matrix function, minimizing the generated quadratic function in terms of the unknown parameters of the random variable and, finally, obtaining the test statistic, which converges in distribution to a $\chi^2.$ 

Given $N \in\mathbb{N}$ with $N \geq 2,$ let 
$$
\Lambda :=\{\lambda:=(\lambda_1, \ldots, \lambda_N) \in \mathbb{R}^N: \lambda_i \leq \lambda_{i+1} \text{ and } \lambda_i > 0, \text{ for }  i = 1,2,\ldots, N \},
$$
and $g:\mathbb{R}\times \Lambda \rightarrow \mathbb{R}^n$ be  a measurable function, where 
$$
 g(x,\lambda):= [\cos(\lambda_1x),\sin(\lambda_1x),\ldots,\cos(\lambda_Nx),\sin(\lambda_Nx)].
$$
Additionally, let  $g_\theta:\Lambda \rightarrow \mathbb{R}^N$ be a function defined by
$$
 g_\theta(\lambda) := \left[\mbox{Re}(\Phi_\theta(\lambda_1)),\mbox{Im}(\Phi_\theta(\lambda_1)),\ldots,\mbox{Re}(\Phi_\theta(\lambda_N)),\mbox{Im}(\Phi_\theta(\lambda_N))  \right]^t,
$$
where the $\mbox{Re}(\cdot)$ and $\mbox{Im}(\cdot)$  are the real and imaginary components of a complex number and $\Phi_\theta$ is the characteristic function of a normal random variable with parameters $\theta := (\mu,\sigma^2)\in \Theta,$ an open bounded set contained in $\mathbb{R}\times \mathbb{R}^+$. For any $\lambda\in\Lambda,$ let us also denote
$$
 \widehat{g}(\lambda) := \dfrac{1}{n}\sum_{t=1}^n [\cos(\lambda_1 x_t),\sin(\lambda_1x_t),\ldots,\cos(\lambda_N x_t),\sin(\lambda_N x_t)]^t.
$$ 
Let $f(v;\theta,\lambda)$ be the spectral density matrix of $\{g(X_t,\lambda)\}_{t \in\mathbb{Z}}$ at a frequency $v.$
Then, for $v = 0$, it can be estimated by
$$
 \widehat{f}(0;\theta,\lambda) := \dfrac{1}{2\pi n}\left(\sum_{t=1}^n  \widehat{G}(x_{t,0},\lambda) +2\sum_{i=1}^{\lfloor  n^{2/5}\rfloor}(1 -i/\lfloor n^{2/5}  \rfloor)\sum_{t=1}^{n-i}\widehat{G}(x_{t,i},\lambda) \right),
$$
where $\widehat{G}(x_{t,i},\lambda) = (\widehat{g}(\lambda) -g(x_{t},\lambda))(\widehat{g}(\lambda) -g(x_{t+i},\lambda))^t$ and $\lfloor \cdot \rfloor$ denotes the floor function. The test statistic general form under $H_0$ is
$$
 Q_n(\lambda) := \min_{\theta \in \Theta} \left\{ Q_n(\theta,\lambda) \right\},
$$
with 
$$
 Q_n(\theta,\lambda):=(\widehat{g}(\lambda)-g_\theta(\lambda))^tG_n^+(\lambda)(\widehat{g}(\lambda)-g_\theta(\lambda)),
$$
where $G^{+}_n$ is the generalized inverse of the spectral density matrix $2 \pi  \widehat{f}(0;\theta,\lambda)$. Let 
$$
 \widehat{\theta} := \arg \min_{\theta \in \Theta} \left\{ Q_n(\theta,\lambda) \right\},
$$
be the argument that minimizes $Q_n(\theta,\lambda)$ such that $\widehat{\theta}$ is in a neighborhood of $\widehat{\theta}_n := (\widehat{\mu},\widehat{\gamma}(0))$. To guarantee its' existence and uniqueness, the following assumptions are required. We refer to them as assumption $(A.)$.

$(A.)$ Let $\theta_0$ be the true value of $\theta$ under $H_0$, then for every $\lambda \in \Lambda$  the following conditions are satisfied.
		 
 + $f(0;\theta,\lambda)$ is positive definite.
		 
 + $\Phi_\theta(\lambda)$ is twice differential with respect to $\theta$ in a neighborhood of $\theta_0$.
		 
 + The matrix $D(\theta_0,\lambda) = \dfrac{\partial \Phi_\theta(\lambda)}{\partial\theta |_{\theta = \theta_0}} \in \mathbb{R}^{N\times 2}$ has rank 2.

 + The set $\Theta_0(\lambda) := \{ \theta \in \Theta:  \Phi_\theta(\lambda_i) =  \Phi_{\theta_0}(\lambda_i), i=1, \ldots,N\}$ is a finite bounded set in $\Theta$. And $\theta$ is a bounded subset $\mathbb{R}\times \mathbb{R}^+$.
		 
 + $f(0;\theta,\lambda) = f(0;\theta_0,\lambda)$ and $D(\theta_0,\lambda) = D(\theta_,\lambda)$ for all $\theta \in \Theta_0(\lambda)$. 

Under these assumptions, the Epps's main result is presented as follows.

#### Theorem 1 [@epps1987, Theorem 2.1] 
Let $X$ be a stationary Gaussian process such that \@ref(eq:a) and  $(A.)$ are satisfied, then $nQ_n(\lambda)\to_d \chi^2(2N - 2)$ for every $\lambda \in \Lambda$.

The current \CRANpkg{nortsTest} version, uses $\Lambda := \{\verb|lambda|/\widehat{\gamma}(0)\}$ as the values to evaluate the empirical characteristic function, where $\widehat{\gamma}(0)$ is the sample variance. By default `lambda = c(1, 2)`. Therefore, the implemented test statistic converges to a $\chi^2$ distribution with two degrees of freedom. The user can change these $\Lambda$ values as desired by simply specifying the function's `lambda` argument, as we show in the Example below.

#### Example 1
A stationary $AR(2)$ process is drawn using a beta distribution with `shape1 = 9` and `shape2 = 1` parameters, and performed the implementation of the test of Epps, `epps.test()`. At significance level $\alpha = 0.05$, the null hypothesis of normality is correctly rejected. 

```{r, echo = TRUE}
set.seed(298)
x = arima.sim(250,model = list(ar =c(0.5,0.2)),
                 rand.gen = rbeta,shape1 = 9,shape2 = 1)

# Asymptotic Epps test
epps.test(x)
```

Asymptotic Epps test with random Lambda values as proposed in @nietoreyes2014.

```{r, echo = TRUE}
set.seed(298)
epps.test(x, lambda = abs(rnorm(mean = c(1, 2), 2)))
```

Approximated sieve bootstrap Epps test using 1000 repetitions of 250 units.

```{r, echo = TRUE}
set.seed(298)
epps_bootstrap.test(x, seed = 298)
```

## Tests of Lobato and Velasco

@Lobato2004 provides a consistent estimator for the corrected SK test statistic for stationary processes, see @Lomincki1961 and @Gasser1975 for further insight. Note that the SK test is also known as the Jarque-Bera test [@jarque1980], which is already available in several R packages [@tseries, for instance]. The improvement of this proposal over those implementations is a correction in the skewness and kurtosis estimates by the process' auto-covariance function, resulting in a consistent test statistic under the assumption of correlated data. The test in @Lobato2004 is asymptotic, which is computationally efficient, as opposed to a bootstrap based test. @psaradakis2020normality show that the bootstrap modification of the Lobato and Velasco's test is a fair competitor against the original asymptotic test, beating other tests for normality of the one-dimensional marginal distribution in terms of power. Thus, the package incorporates both the asymptotic, `lobato.test()` and its bootstrap version `lobato_bootstrap.test()`.

The general framework for the test is presented in what follows. On the contrary to the test of Epps, this proposal does not require additional parameters for the computation of the test sample statistic.

Let $X$ be a stationary stochastic process that satisfies 

\begin{equation}
 \sum_{t=0}^{\infty}|\gamma(t)| <\infty. (\#eq:aLV)
\end{equation}

The null hypothesis is that the one-dimensional marginal distribution of $X$ is normally distributed, that is 
$$
H_0: X_t \sim N(\mu,\sigma^2) \text{ for all } t \in \mathbb{R}.
$$
Let $k_q(j_1,j_2,\ldots,j_{q-1})$ be the q-th order cummulant of $X_{1},X_{1+j_1},\ldots,X_{1+j_{q-1}}$. $H_0$ is fulfilled if all the marginal cummulants above the second order are zero. In practice, it is tested just for the third and fourth order marginal cummulants. Equivalently, in terms of moments, the marginal distribution is normal by testing whether $\mu_3 = 0$ and $\mu_4 = 3 \mu_2^2$.  For non-correlated data, the SK test compares the SK statistic against upper critical values from a $\chi^2(2)$ distribution [@bai2005]. For a Gaussian process $X$ satisfying \@ref(eq:aLV), it holds the limiting result
$$
 \sqrt{n} \binom{\widehat{\mu}_3}{\widehat{\mu}_4 -3\widehat{\mu}^2_2} \to_d N[0_2,\Sigma_F)],
$$
where $0_2 := (0,0)^t \in \mathbb{R}^2$ and $\Sigma_F := \mbox{diag}(6F^{(3)}, \text{ } 24F^{(4)}) \in \mathbb{R}^{2x2}$ is a diagonal matrix with $F^{(k)} := \sum_{j = -\infty}^{\infty}\gamma(j)^k$ for $k=3,4$ [@Gasser1975]. 

The following consistent estimator  in terms of the auto-covariance function is proposed in @Lobato2004
$$
 \widehat{F}^{(k)} := \sum_{t = 1-n}^{n-1}\widehat{\gamma}(t)[\widehat{\gamma}(t) +\widehat{\gamma}(n-|t|)]^{k-1},
$$
to build a *generalized SK test* statistic
$$
 G := \dfrac{n \widehat{\mu}_3^2}{6 \widehat{F}^{(3)}} + \dfrac{n(\widehat{\mu}_4 -3\widehat{\mu}_2)^2}{24\widehat{F}^{(4)}}.
$$
Similar to the SK test for non-correlated data, the $G$ statistic is compared against upper critical values from a $\chi^2(2)$ distribution. This is seen in the below result that establishes the asymptotic properties of the test statistics, so that the general test procedure can be constructed. The result requires the following assumptions, denoted by $(B.),$ for the process $X.$ 

(B.)
 
 + $E[X_t^{16}] < \infty$  for $t \in T.$ 
	
 +	$\sum_{j_1 = -\infty}^{\infty}\cdots \sum_{j_{q-1} = -\infty}^{\infty} |k_q(j_1,\ldots,j_{q-1})| < \infty \text{ for } q=2,3,\ldots,16.$
 
 + $\sum_{j=1}^{\infty}\left(E \left[\text{ } E[(X_0-\mu)^k|B_j] -\mu_k\right]^2 \right)^{1/2} < \infty \text{  for } k = 3,4,$ where $B_j$ denotes the $\sigma$-field generated by $X_t$, $t \leq -j.$ 
 
 + $E\left[Z_k \right]^2 +2\sum_{j=1}^{\infty}E\left(\left[Z_k \right] \left[ (X_j -\mu)^k -\mu_k \right] \right) > 0$ for $k = 3,4,$ with $Z_k=(X_0 -\mu)^k -\mu_k.$

Note that these assumptions imply that the higher-order spectral densities up to order 16 are continuous and bounded.

#### Theorem 2 [@Lobato2004, Theorem 1]
Let $X$ be a stationary process. If $X$ is Gaussian and satisfies \@ref(eq:aLV) then $G \to_d \chi^2(2)$, and under assumption (B.), the test statistic G diverges whenever $\mu_3 \neq 0$ or $\mu_4 \neq 3\mu_2^2.$

#### Example 2
A stationary $MA(3)$ process is drawn using a gamma distribution with `rate = 3` and `shape = 6` parameters. The `lobato.test()` function performs the test of *Lobato and Velasco* to the simulated data. At significance level  $\alpha = 0.05$, the null hypothesis of normality is correctly rejected. 

```{r, echo = TRUE}
set.seed(298)
x = arima.sim(250,model = list(ma = c(0.2, 0.3, -0.4)),
                 rand.gen = rgamma, rate = 3, shape = 6)
# Asymptotic Lobato & Velasco
lobato.test(x)
```

Approximated sieve bootstrap Lobato and Velasco test using 1000 repetitions of 250 units.

```{r, echo = TRUE}
lobato_bootstrap.test(x, seed = 298)
```

## The Random Projections test 

The previous proposals only test for the normality of the one-dimensional marginal distribution of the process, which is inconsistent against alternatives whose one-dimensional marginal is Gaussian. @nietoreyes2014 provides a procedure to fully test normality of a stationary process using a Crammér-Wold type result [@Cuesta2007] that uses random projections to differentiate among distributions. In @nietoreyes2014 existing tests for the normality of the one dimensional marginal are applied to the random projections and the resulting p-values combined using the false discovery rate for dependent data [@Benjamin2001]. The \CRANpkg{nortsTest} package improves on this test by allowing to use the less conservative false discovery rate in @Benjamin1995.

We show the Crammér-Wold type result below. The result works for separable Hilbert spaces, however here, for its later application, we restrict it to $l^2,$ the space of square summable sequences over $\mathbb{N},$ with inner product $\langle \cdot,\cdot \rangle.$ 

#### Theorem 3 [@Cuesta2007, Theorem 3.6]
Let $\eta$ be a dissipative distribution on $l^2$ and $Z$  a $l^2$-valued random element, then $Z$ is Gaussian if and only if 
$$
 \eta\{h \in l^2: \langle Z,h \rangle \text{ has a Gaussian distribution}\} > 0.
$$ 
A dissipative distribution [@nietoreyes2014, Definition 2.1] is a generalization of the concept of absolutely continuous distribution to the infinite-dimensional space. A Dirichlet process [@gelman2013] produces random elements with a dissipative distribution in $l^2$. In practice, generate draws of $h \in l^2$ with a stick-breaking process that makes use of beta distributions.

Let $X = \{X_t\}_{t\in\mathbb{Z}}$ be a stationary process. As $X$ is normally distributed if the process  $X^{(t)} := \{X_k\}_{k \leq t}$ is Gaussian for each   $t\in\mathbb{Z},$ using the result above, @nietoreyes2014  provides a procedure for testing  that $X$ is a Gaussian process by testing whether the process $Y^h = \{Y^h_t\}_{t \in \mathbb{Z}}$ is Gaussian.
\begin{equation}
 Y^h_t := \sum_{i=0}^\infty h_i X_{t-i} = \langle X^{ (t) },h \rangle, (\#eq:proj)
\end{equation}
where $\langle X^{(t)},h \rangle$ is a real random variable for each $t \in \mathbb{Z}$ and  $h\in l^2$. Thus, $Y^h$ is a stationary process constructed by the projection of $X^{(t)}$ on the space generated by $h.$ Therefore, $X$ is a Gaussian process if and only if the one dimensional marginal distribution of $Y^{h}$ is normally distributed. Additionally, the hypothesis of the tests *Lobato and Velasco* or *Epps*, such as \@ref(eq:a), \@ref(eq:aLV), $(A)$ and $(B)$, imposed on $X$ are inherited by $Y^h$. Then, those tests can be applied to evaluate the normality of the one dimensional marginal distribution of $Y^h$. Further considerations include the specific beta parameters used to construct the distribution from which to draw $h$ and selecting a proper number of combinations to establish the number of projections required to improve the method performance. All of these details are discussed in @nietoreyes2014.

Next, we summarize the test of random projections in practice:

 1. Select $k,$ which results in $2k$  independent random projections (*by default* `k = 1`).
 
 2. Draw the $2k$ random elements to project the process from a dissipative distribution that uses a particular beta distribution. By default, use a  $\beta(2,7)$ for the first $k$ projections and a $\beta(100,1)$ for the later $k$.
 
 3. Apply the tests of *Lobato and Velasco* to the even projected processes and *Epps* to the odd projections.
 
 4. Combine the obtained $2k$ `p-values` using the false discover rate. By default, use @Benjamin2001 procedure.

The `rp.test()` function implements the above procedure. The user might provide optional parameters such as the number of projections `k`, the parameters of the first beta distribution `pars1` and those of the second `pars2`. The next example illustrates the application of the `rp.test()` to a stationary GARCH(1,1) process drawn using normal random variables.

#### Example 3
A stationary `GARCH(1,1)` process  is drawn with a standard normal distribution and parameters $\alpha_0 = 0,$ $\alpha_1 = 0.2$ and $\beta_1 = 0.3$ using the [\CRANpkg{fGarch} package, @fGarch]. Note that a `GARCH(1,1)` process is stationary if the parameters $\alpha_1$ and $\beta_1$ satisfy the inequality $\alpha_1 + \beta_1 < 1$ [@Bollerslev1986].

```{r, echo = TRUE}
set.seed(3468)
library(fGarch)
spec = garchSpec(model = list(alpha = 0.2, beta = 0.3))
x = ts(garchSim(spec, n = 300))
rp.test(x) 
```

At significance level  $\alpha = 0.05,$ the applied *random projections* test with `k = 1` as the number of projections shows no evidence to reject the null hypothesis of normality.

## The Psaradakis and Vavra's test

@vavra2017 adapted a distance test for normality for a one-dimensional marginal distribution of a stationary process. Initially, the test was based on the Anderson (1952) test statistic and used an auto-regressive sieve bootstrap approximation to the null distribution of the sample test statistic. Later, @psaradakis2020normality considered this test as the ultimate normality test based on the empirical distribution function, and adapted its methodology to a wide range of tests, including Shapiro-Wilk [@SWtest1965], Jarque-Bera [@jarque1980], Cramer von Mises [@vonMisses1962], Epps, and Lobato-Velasco. Their experiments show that the Lobato-Velasco and Jarque-Bera test's bootstrap version performs best in small samples.

Although the test is said to be applicable to a wide class of non-stationary processes by transforming them into stationary by means of a fractional difference operator, no theoretic result was apparently provided to sustain this transformation. This work restricts the presentation of the original procedure to stationary processes. 

Let $X$ be a stationary process satisfying
\begin{equation}
 X_t = \sum_{i=0}^{\infty}\theta_i \epsilon_{t-i} + \mu_0, \ t \in \mathbb{Z}, (\#eq:aPV)
\end{equation}
where $\mu_0 \in \mathbb{R}$, $\{\theta_i\}_{i=0}^\infty\in l^2$  with $\theta_0 = 1$ and $\{\epsilon_t\}_{i=0}^\infty$ is a collection of mean zero i.i.d random variables. The null hypothesis is that the one dimensional marginal distribution of $X$ is normally distributed,
$$
 H_0: F(\mu_0 +\sqrt{\gamma(0)}x)-F_N(x) = 0, \text{ for all } x\in \mathbb{R},
$$
where F is the cumulative distribution function of $X_0$, and $F_N$ denotes the standard normal cumulative distribution function. Note that  if $\epsilon_0$ is normally distributed, then the null hypothesis is satisfied. Conversely, if the null hypothesis is satisfied, then $\epsilon_0$ is normally distributed and, consequently, $X_0$.  
The considered test for $H_0$ is based on the Anderson-Darling distance statistic
\begin{equation}
 A_d = \int_{-\infty}^{\infty}\dfrac{[{F_n}(\widehat{\mu}+\sqrt{\widehat{\gamma}(0)}x)-F_N(x)]^2}{F_N(x)[1-F_N(x)]}dF_N(x), (\#eq:aPV1)
\end{equation}
where ${F_n}(\cdot)$ is the empirical distribution function associated to $F$ based on a simple random sample of size $n$. @vavra2017 proposes an auto-regressive sieve bootstrap procedure to approximate the sampling properties of $A_d$ arguing that making use of classical asymptotic inference for $A_d$ is problematic and involved. This scheme is motivated by the fact that under some assumptions for $X,$ including \@ref(eq:aPV), $\epsilon_t$ admits the representation
\begin{equation}
 \epsilon_t = \sum_{i=1}^{\infty}\phi_i(X_{t-i} - \mu_0), \ t \in \mathbb{Z}, (\#eq:ePV)
\end{equation}
for certain type of $\{\phi_i\}_{i=1}^\infty\in l^2$. The main idea behind this approach is to generate a bootstrap sample $\epsilon_t^*$ to approximate $\epsilon_t$ with a finite-order auto-regressive model. This is because the distribution of the processes $\epsilon_t$ and $\epsilon_t^*$ coincide asymptotically if the order of the auto-regressive approximation grows simultaneously with $n$ at an appropriate rate [@Buhlmann1997]. The procedure  makes use of the $\epsilon_t^{*'}s$ to obtain the $X_t^{*'}s$ through the bootstrap analog of  \@ref(eq:ePV). Then, generate a bootstrap sample of the $A_d$ statistic, $A_d^{*},$  making use of the bootstrap analog of \@ref(eq:aPV).

The `vavra.test()` function implements @psaradakis2020normality procedure. By default, it generates 1,000 sieve-bootstrap replications of the Anderson-Darling statistic. The user can provide different test procedures, such as the *Shapiro-Wilk, Jarque-Bera, Cramer von Mises, Epps* or *Lobato-Velasco* test, by specifying a text value to the `normality` argument. The presented values are Monte Carlo estimates of the $A_d$ statistic and `p.value`.

#### Example 4
A stationary $ARMA$(1,1) process is simulated using a standard normal distribution  and performs *Psaradakis and Vávra* procedure using Anderson-Darling and Cramer von Mises test statistics. At significance level $\alpha = 0.05$, there is no evidence to reject the null hypothesis of normality. 

```{r, echo = TRUE}
set.seed(298)
x = arima.sim(250,model = list(ar = 0.2, ma = 0.34))
# Default, Psaradakis and Vavra's procedure
vavra.test(x, seed = 298)
```

Approximate Cramer von Mises test for the Psaradakis and Vavra's procedure

```{r, echo = TRUE}
vavra.test(x, normality = "cvm", seed = 298)
```

## The multivariate kurtosis test

The literature contains some procedures to test the null hypothesis that a multivariate stochastic process is Gaussian. Those include @moulines1992testing, a test based on the characteristic function, and @Steinberg1992, a test based on properties of the entropy of Gaussian processes that does not make use of cumulant computations. According to @el2022normality, these tests may hardly be executable in real time. Consequently, they propose a test based on multivariate kurtosis [@mardia1970measures]. The proposed procedure is for $p=1,2,$ and we elaborate on it in what follows. In Section 6.3 of @el2022normality,  they suggest to apply random projections for higher dimensions but they do not investigate the procedure any further.

The p-value of this test is obtained as $2(1-F_N(z))$ where, as above, $F_N$ denotes the standard normal cumulative distribution function. There,
 $$
  z:=(\hat{B}_p-E[\hat{B}_p])/\sqrt{E[(\hat{B}_p-E[\hat{B}_p])^2]},
 $$
 where 
 $$
  \hat{B}_p:=n^{-1}\sum_{t=1}^n(x_t^t \hat{S}^{-1}x_t)^2,
 $$
and 
$$
 \hat{S}:=n^{-1}\sum_{t=1}^n x_t x_t^t.
$$ 
In @el2022normality, there reader can found the exact computations of $E[\hat{B}_p]$ and $E[(\hat{B}_p-E[\hat{B}_p])^2].$

This test is implemented in the `elbouch.test()` function. By default, the function computes the univariate El Bouch test. If the user provides a secondary data set, the function computes the bivariate counterpart. 

#### Example 5
Simulate a two-dimensional stationary VAR(2) process using independent AR(1) and AR(2) processes with standard normal distributions and apply the bivariate El Bouch test. At significance level $\alpha = 0.05$, there is no evidence to reject the null hypothesis of normality. 

```{r, echo=TRUE}
set.seed(23890)
x = arima.sim(250,model = list(ar = 0.2))
y = arima.sim(250,model = list(ar = c(0.4,0,.1)))
elbouch.test(y = y,x = x)
```

# Simulations and data analysis

## Numerical experiments

Inspired by the simulation studies in @vavra2017 and @nietoreyes2014, we propose here a procedure that involves drawing data from the $AR(1)$ process
\begin{equation}
 X_t = \phi X_{t-1} + \epsilon_t, \ t \in\mathbb{Z}, \text{ for } \phi \in \{ 0,\pm 0.25,\pm 0.4\}, (\#eq:eqAR)
\end{equation}
where the $\{\epsilon_t\}_{t\in\mathbb{Z}}$ are i.i.d random variables. For the distribution of the $\epsilon_t$ we consider different scenarios: standard normal ($N$), standard log-normal ($\log N$), Student t with 3 degrees of freedom ($t_3$), chi-squared with 10 degrees of freedom ($\chi^2(10)$) and gamma with $(7, 1)$ shape and scale parameters ($\Gamma(7,1)$). 

```{r tab1-static, eval = knitr::is_latex_output(),warning = FALSE}
load("data/r_sim.Rdata")
phi = c("-0.4","-0.25","0.0","0.25","0.4","max.phi")

r1 = results1[,2:14]
colnames(r1) = c("phi", phi, phi)

kable(r1, "latex", booktabs = TRUE,digits = 3, caption = "Part 1. Rejection rate estimates over $m=1,000$ trials of the seven studied goodness of fit test for the null hypothesis of normality. The data is drawn using the process defined in (8) for different values of $phi$ and $n$ displayed in the columns and different distributions for $epsilon_t$  in the rows.  $phi$ in { 0, 0.25, 0.4}, n in {100, 250}. For each test and distribution, max.phi represents the maximum rejection rate's running time in seconds among the different values of the AR parameter.") %>%
kable_styling(latex_options = c("hold_position", "scale_down"))%>%
add_header_above(c(" " = 1, "n = 100" = 6, "n = 250" = 6))%>%
pack_rows("Lobato and Velasco", 1, 5) %>%
pack_rows("Epps", 6, 10) %>%
pack_rows("Random Projections", 11, 15) %>%
pack_rows("Psaradakis and Vavra", 16, 20)%>%
pack_rows("Bootstrap Lobato", 21, 25)%>%
pack_rows("Bootstrap Epps", 26, 30)%>% 
pack_rows("El Bouch", 31, 35)
```

```{r tab1-interactive, eval = knitr::is_html_output(),warning = FALSE}
load("data/r_sim.Rdata")
phi = c("-0.4","-0.25","0.0","0.25","0.4","max.phi")

r1 = results1[,2:14]
colnames(r1) = c("phi", phi, phi)

kable(r1, "html", booktabs = TRUE, digits = 3, caption = "Part 1. Rejection rate estimates over $m=1,000$ trials of the seven studied goodness of fit test for the null hypothesis of normality. The data is drawn using the process defined in (8) for different values of $phi$ and $n$ displayed in the columns and different distributions for $epsilon_t$  in the rows.  $phi$ in { 0, 0.25, 0.4}, n in {100, 250}. For each test and distribution, max.phi represents the maximum rejection rate's running time in seconds among the different values of the AR parameter.") %>%
kable_styling(latex_options = c("hold_position", "scale_down"))%>%
add_header_above(c(" " = 1, "n = 100" = 6, "n = 250" = 6))%>%
pack_rows("Lobato and Velasco", 1, 5) %>%
pack_rows("Epps", 6, 10) %>%
pack_rows("Random Projections", 11, 15) %>%
pack_rows("Psaradakis and Vavra", 16, 20)%>%
pack_rows("Bootstrap Lobato", 21, 25)%>%
pack_rows("Bootstrap Epps", 26, 30)%>% 
pack_rows("El Bouch", 31, 35)
```

As in @vavra2017, $m=1,000$ independent draws of the above process are generated for each pair of parameter $\phi$ and distribution. Each draw is taken of length $past+n,$ with $past=500$ and $n \in \{100,250,500,1000 \}$. The first 500 data points of each realization are then discarded in order to eliminate start-up effects. The $n$ remaining data points are used to compute the value of the test statistic of interest. In each particular scenario, the rejection rate is obtained by computing the proportion of times that the test is rejected among the $m$ trials.

```{r tab2-static, eval = knitr::is_latex_output(),warning = FALSE}
r2 = results2[,2:14]
colnames(r2) = c("phi", phi, phi)

kable(r2, "latex", booktabs = TRUE, digits = 3, caption = "Part 2. Rejection rate estimates over $m=1,000$ trials of the seven studied goodness of fit test for the null hypothesis of normality. The data is drawn using the process defined in (8) for different values of $phi$ and $n$ displayed in the columns and different distributions for $epsilon_t$  in the rows.  $phi$ is in { 0, 0.25, 0.4} and n in {500, 1000}. For each test and distribution, max.phi represents the maximum rejection rate's running time in seconds among the different values of the AR parameter.") %>%
kable_styling(latex_options = c("hold_position", "scale_down"))%>%
add_header_above(c(" " = 1, "n = 500" = 6, "n = 1,000" = 6))%>%
pack_rows("Lobato and Velasco", 1, 5) %>%
pack_rows("Epps", 6, 10) %>%
pack_rows("Random Projections", 11, 15) %>%
pack_rows("Psaradakis and Vavra", 16, 20)%>%
pack_rows("Bootstrap Lobato", 21, 25)%>%
pack_rows("Bootstrap Epps", 26, 30)%>% 
pack_rows("El Bouch", 31, 35)
```

```{r tab2-interactive, eval = knitr::is_html_output(),warning = FALSE}
r2 = results2[,2:14]
colnames(r2) = c("phi", phi, phi)

kable(r2, "html", booktabs = TRUE, digits = 3, caption = "Part 2. Rejection rate estimates over $m=1,000$ trials of the seven studied goodness of fit test for the null hypothesis of normality. The data is drawn using the process defined in (8) for different values of $phi$ and $n$ displayed in the columns and different distributions for $epsilon_t$  in the rows.  $phi$ is in { 0, 0.25, 0.4} and n in {500, 1000}. For each test and distribution, max.phi represents the maximum rejection rate's running time in seconds among the different values of the AR parameter.") %>%
kable_styling(latex_options = c("hold_position", "scale_down"))%>%
add_header_above(c(" " = 1, "n = 500" = 6, "n = 1,000" = 6))%>%
pack_rows("Lobato and Velasco", 1, 5) %>%
pack_rows("Epps", 6, 10) %>%
pack_rows("Random Projections", 11, 15) %>%
pack_rows("Psaradakis and Vavra", 16, 20)%>%
pack_rows("Bootstrap Lobato", 21, 25)%>%
pack_rows("Bootstrap Epps", 26, 30)%>% 
pack_rows("El Bouch", 31, 35)
```

Tables `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:tab1-interactive)', '\\@ref(tab:tab1-static)'))` and `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:tab2-interactive)', '\\@ref(tab:tab2-static)'))` present the rejection rate estimates. For every process of length $n,$ the columns represent the used $AR(1)$ parameter and the rows the distribution used to draw the process. The obtained results are consistent with those obtained in the publications where the different tests were proposed. As expected, rejection rates are around 0.05 when the data is drawn from a standard normal distribution, as in this case the data is drawn from a Gaussian process. Conversely, high rejection rates are registered for the other distributions. Low rejection rates are observed, however, for the $\chi^2(10)$ distribution when making use of some of the tests. For instance, the *Epps* and *bootstrap Epps* tests, although they consistently tend to 1 when the length of the process, $n,$ increases.  Another case is the El Bouch test. However, this one maintains low rates for large values of $|\phi|$ when $n$ increases. Furthermore, for the random projections test, the number of projections used in this study is the default $k = 1,$  which is by far a lower number than the recommended by @nietoreyes2014. However, even in these conditions, the obtained results are satisfactory,  with the random projection test having even better performance than the tests of @epps1987 or @vavra2017.

An important aspect in selecting a procedure is its computation time. Thus, for each length of the process, $n,$ there is an additional column, max.phi, in *Tables* `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:tab1-interactive)', '\\@ref(tab:tab1-static)'))` and `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:tab2-interactive)', '\\@ref(tab:tab2-static)'))`.  Each entry in this column refers to a different distribution and contains the maximum running time in seconds to obtain the rejection rate among the different values of the AR parameter. That is, for a fix distribution, the rejection rates are computed for each of the five possibilities of $\phi$ and the time that it takes recorded. The running time in the table is the largest among the five. Furthermore, in \textit{Table} `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:tab3-interactive)', '\\@ref(tab:tab3-static)'))` we show the time in seconds that  each studied test takes to check whether a given process is Gaussian. In particular,  the table contains the average running time over 1,000 trials that takes to generate and check  a Gaussian AR(1) process with  parameter $\phi = 0.5$. This is done for different sample sizes, $n \in \{1000, 2000, 3000, 4000, 5000\}.$  According to the table, the asymptotic tests  (Lobato and Velasco, Epps, random projections and El Bouch) have similar running times. On the contrary, the bootstrap based tests (Psaradakis and Vavra, Bootstrap Epps and Lobato and Velasco) have, as expected, higher running times on average. Furthermore, Tables `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:tab1-interactive)', '\\@ref(tab:tab1-static)'))` and `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:tab2-interactive)', '\\@ref(tab:tab2-static)'))` show similar results in time performance. There, the maximum running time of the bootstrap based tests exceeds in more than ten seconds the time obtained with the asymptotic based tests. It is worth saying that the tables have been obtained with R version 4.3.1 (2023-06-16) and platform aarch64-apple-darwin20 (64-bit),running under macOS Sonoma 14.2.1.

```{r tab3-static, eval = knitr::is_latex_output(),warning = FALSE}
load("data/runtime.Rdata")

kable(runtime, "latex", booktabs = TRUE, digits = 4, caption = "Average running time in seconds, over 1000 iterations,  to compute the null hypothesis of Gaussianity for each of the studied tests (first column) and different sample sizes, $n=1000$ (second column), $n=2000$ (third column), $n=3000$ (fourth column), $n=4000$ (fifth column) and $n=5000$ (sixth column). Each iteration makes use of a Gaussian AR(1) process with parameter $phi = 0.5.$") 
```

```{r tab3-interactive, eval = knitr::is_html_output(),warning = FALSE}
load("data/runtime.Rdata")

kable(runtime,"html", booktabs = TRUE, digits = 4, caption = "Average running time in seconds, over 1000 iterations,  to compute the null hypothesis of Gaussianity for each of the studied tests (first column) and different sample sizes, $n=1000$ (second column), $n=2000$ (third column), $n=3000$ (fourth column), $n=4000$ (fifth column) and $n=5000$ (sixth column). Each iteration makes use of a Gaussian AR(1) process with parameter $phi = 0.5.$") 
```

## Real data application

As an illustrative example, we analyze the monthly mean carbon dioxide, in parts per million (*ppm*), measured at the Mauna Loa Observatory, in Hawaii, from March 1958 to November 2018. The carbon dioxide data measured as the mole fraction in dry air on Mauna Loa constitute the longest record of direct measurements of $CO2$ in the atmosphere. This dataset is available in the \CRANpkg{astsa} package [@astsa] under the name *cardox* data and it is displayed in the left panel of Figure `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(fig:fig1-interactive)', '\\@ref(fig:fig1-static)'))`. The plot's grid is created using the \CRANpkg{cowplot} package [@cowplot].

The objective of this subsection is to propose a model to analyze this time series and check the assumptions on the residuals of the model using our implemented `check_residuals()` function. The time series clearly has trend and seasonal components (see left panel of Figure `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(fig:fig1-interactive)', '\\@ref(fig:fig1-static)'))`), therefore, an adequate model that filters both components has to be selected.  We  make use of an ETS model. For its implementation, we make use the `ets()` function from the \CRANpkg{forecast} package [@Rob2007]. This function fits 32 different ETS models and selects the best model according to information criteria such as  *Akaike's information criterion* (AIC) or *Bayesian Information criteria* (BIC) [@BIC2006].
The results provided by the `ets()` function are:

```{r fig1-static, fig.cap = "Left panel: CO2 Levels at Mauna Loa, time-series plot. The cardox data show a positive tendency and strong seasonality. Right panel: forecast of the next 12 months for the CO2 levels at Mauna Loa, the model's predictions capture the time-series behaviour.", eval = knitr::is_latex_output(), fig.alt="(ref:demo-caption1)", out.width = "75%"}
library(astsa)
g1 = autoplot(cardox, main = "CO2 levels at  Mauna Loa", 
         xlab = "years", ylab = "CO2 (ppm)")
g2 = autoplot(forecast(ets(cardox), h = 12),include = 100,
         xlab = "years",ylab = "CO2 (ppm)",
         main = "Forecast: CO2  Levels at Mauna Loa")
cowplot::plot_grid(g1,g2,ncol = 2)
```

`r if (knitr::is_latex_output()) "(ref:demo-caption1) Left panel: CO2 Levels at Mauna Loa, time-series plot. The cardox data show a positive tendency and strong seasonality. Right panel: forecast of the next 12 months for the CO2 levels at Mauna Loa, the model's predictions capture the time-series behaviour."`

```{r fig1-interactive, echo = knitr::is_html_output(), eval = knitr::is_html_output(),fig.cap="CO2 Levels at Mauna Loa, time-series plot. The cardox data show a positive tendency and strong seasonality."}
library(astsa)

autoplot(cardox, main = "Carbon Dioxide levels at Mauna Loa", 
         xlab = "years", ylab = "CO2 (ppm)")
```



```{r, echo = TRUE}
library(forecast)
library(astsa)
model = ets(cardox)
summary(model)
```

The resulting model, proposed by the `ets()` function, for analyzing the *carbon dioxide* data in *Mauna Loa* is an $ETS[M,A,A]$ model. The parameters $\alpha, \beta \text{ and } \gamma$ (see Definition 1) have being estimated using the least squares method. If the assumptions on the model are satisfied, then the errors of the model behave like a Gaussian stationary process. To check it, we make use of the function `check_residuals()`. For more details on the compatibility of this function with the models obtained by other packages see the \CRANpkg{nortsTest} repository. In the following, we display the results of using the *Augmented Dickey-Fuller* test  (*Subsection 3.1*) to check the stationary assumption and the *random projection* test with `k = 1` projections to check the normality assumption. For the other test options see the function's documentation.

```{r, echo = TRUE, eval = FALSE}
check_residuals(model,unit_root = "adf",normality = "rp",
                   plot = TRUE)
```

```{r, echo = FALSE, eval = TRUE}
check_residuals(model,unit_root = "adf",normality = "rp", plot = FALSE)
```

The obtained results indicate that the null hypothesis of non stationarity is rejected at significance level $\alpha = 0.01.$ Additionally, there is no evidence to reject the null hypothesis of normality at significance level $\alpha = 0.05.$ Consequently, we conclude that the residuals follow a stationary Gaussian process, having that the resulting $ETS[M,A,A]$ model adjusts well to the *carbon dioxide* data in *Mauna Loa*.  

In the above displayed `check_residuals()` function, the `plot` argument is set to `TRUE`. The resulting plots are shown in Figure `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(fig:fig2-interactive)', '\\@ref(fig:fig2-static)'))`. The plot in the *top* panel and the auto-correlation plots in the bottom panels insinuate that the residuals have a stationary behavior. The *top* panel plot shows slight oscillations around zero and the auto-correlations functions in the *bottom* panels have values close to zero in every lag. The histogram and qq-plot in the *middle* panels suggest that the marginal distribution of the residuals is normally distributed. Therefore, Figure `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(fig:fig2-interactive)', '\\@ref(fig:fig2-static)'))` agrees with the reported results, indicating that the assumptions of the model are satisfied.

```{r fig2-interactive, eval = knitr::is_html_output(), fig.cap = "Check residuals plot for the ETS(M,A,A) model. The upper panel shows the residuals time-series plot, showing small oscillations around zero, which insinuates stationarity. The middle plots are the residuals histogram (middle-left) and quantile-quantile plot (middle-right), both plots suggest that the residuals have a normal distribution. The lower panel shows the autocorrelation functions, for both plots, the autocorrelations are close to zero giving the impression of stationarity."}
check_plot(model)
```

```{r fig2-static, fig.cap = "Check residuals plot for the ETS(M,A,A) model. The upper panel shows the residuals time-series plot, showing small oscillations around zero, which insinuates stationarity. The middle plots are the residuals histogram (middle-left) and quantile-quantile plot (middle-right), both plots suggest that the residuals have a normal distribution. The lower panel shows the autocorrelation functions, for both plots, the autocorrelations are close to zero giving the impression of stationarity.", eval = knitr::is_latex_output(), fig.alt= "(ref:demo-caption2)", out.width = "100%"}
check_plot(model)
```

`r if (knitr::is_latex_output()) "(ref:demo-caption2) Check residuals plot for the ETS(M,A,A) model. The upper panel shows the residuals time-series plot, showing small oscillations around zero, which insinuates stationarity. The middle plots are the residuals histogram (middle-left) and quantile-quantile plot (middle-right), both plots suggest that the residuals have a normal distribution. The lower panel shows the autocorrelation functions, for both plots, the autocorrelations are close to zero giving the impression of stationarity."`

As the assumptions of the model have been  checked, it can be used for instance to forecast. The result of applying the following function is displayed in Figure `r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(fig:fig3-dynamic)', '\\@ref(fig:fig1-static)'))`. It presents the carbon dioxide data for the last 8 years and a forecast of the next 12 months. It is observable from the plot that  the model captures the process trend and periodicity. 

```{r fig3-dynamic, echo = knitr::is_html_output(), eval = knitr::is_html_output(), fig.cap = "Forecast of the next 12 months for the CO2 levels at Mauna Loa, the model's predictions capture the time-series behaviour."}
autoplot(forecast(model,h = 12),include = 100,
         xlab = "years",ylab = "CO2 (ppm)",
         main = "Forecast: Carbon Dioxide Levels at Mauna Loa")
```

```{r, echo = knitr::is_latex_output(), eval = FALSE, fig.cap = "(ref:demo-caption3)"}
autoplot(forecast(model,h = 12),include = 100,
         xlab = "years",ylab = "CO2 (ppm)",
         main = "Forecast: Carbon Dioxide Levels at Mauna Loa")
```

`r if (knitr::is_latex_output()) "(ref:demo-caption3) Forecast of the next 12 months for the CO2 levels at Mauna Loa, the model's predictions capture the time-series behaviour."`

# Conclusions

For independent data, the \CRANpkg{nortest} package [@nortest2015] provides five different tests for normality, the \CRANpkg{mvnormtest} package [@mvnormtest2012] performs the Shapiro-Wilks test for multivariate data and the \CRANpkg{MissMech} package [@Mortaza2014] provides tests for normality in multivariate incomplete data. To test the normality of dependent data, some authors such as @vavra2017 and @nietoreyes2014 have available undocumented `Matlab` code, which is almost only helpful in re-doing their simulation studies. 

To our knowledge, no consistent implementation or package of tests for normality of stationary processes has been done before. Therefore, the \CRANpkg{nortsTest} is the first package to implement normality tests in stationary processes. This work gives a  general overview of a careful selection of tests for normality in the stationary process, which consists of the most available types of tests. It additionally provides examples that illustrate each of the test implementations.

For checking the model's assumptions, the \CRANpkg{forecast} and \CRANpkg{astsa} packages contain functions for visual diagnostic. Following the same idea, \CRANpkg{nortsTest} provides similar diagnostic methods; it also reports the results of testing stationarity and normality, the main assumptions for the residuals in time series analysis. 

# Future work and projects

A further version of the \CRANpkg{nortsTest} package will incorporate additional tests such as Bispectral [@Hinich1982] and Stein's characterization [@Meddahi2005]. Further future work will include a Bayesian version of a *residuals check* procedure that uses the random projection method. Any future version under development can be installed from `GitHub` using the following code.

```{r,echo = TRUE, eval = FALSE}
if (!requireNamespace("remotes")) install.packages("remotes")
remotes::install_github("asael697/nortsTest",dependencies = TRUE)
```

# Acknowledgment {-}

This work was supported by grant PID2022-139237NB-I00 funded by “ERDF A way of making Europe”  and MCIN/AEI/10.13039/501100011033.
