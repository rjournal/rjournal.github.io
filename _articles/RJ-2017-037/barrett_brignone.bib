@Manual{R-2016-145,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2012},
  note = {{ISBN} 3-900051-07-0},
  url = {http://www.R-project.org/},
}

%% Packages
@Manual{tidyverse,
    title = {tidyverse: Easily Install and Load 'Tidyverse' Packages},
    author = {Hadley Wickham},
    year = {2016},
    note = {R package version 1.0.0},
    url = {https://CRAN.R-project.org/package=tidyverse},
  }

@Manual{furniture,
    title = {furniture: Furniture for Applied Quantitative Researchers},
    author = {Tyson Barrett and Emily Brignone},
    year = {2016},
    note = {R package version 1.5.0},
    url = {https://CRAN.R-project.org/package=furniture},
  }
  
@Manual{foreign,
    title = {foreign: Read Data Stored by Minitab, S, SAS, SPSS, Stata, Systat, Weka,
dBase, ...},
    author = {{R Core Team}},
    year = {2016},
    note = {R package version 0.8-67},
    url = {https://CRAN.R-project.org/package=foreign},
  }


%% Articles from Mendeley

@article{Chang2015,
abstract = {We attempt to replicate 67 papers published in 13 well-regarded economics journals using author-provided replication files that include both data and code. Some journals in our sample require data and code replication files, and other journals do not require such files. Aside from 6 papers that use confidential data, we obtain data and code replication files for 29 of 35 papers (83{\%}) that are required to provide such files as a condition of publication, compared to 11 of 26 papers (42{\%}) that are not required to provide data and code replication files. We successfully replicate the key qualitative result of 22 of 67 papers (33{\%}) without contacting the authors. Excluding the 6 papers that use confidential data and the 2 papers that use software we do not possess, we replicate 29 of 59 papers (49{\%}) with assistance from the authors. Because we are able to replicate less than half of the papers in our sample even with help from the authors, we assert that economics research is usually not replicable. We conclude with recommendations on improving replication of economics research.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Chang, Andrew C and Li, Phillip},
doi = {10.17016/FEDS.2015.083},
eprint = {arXiv:1011.1669v3},
file = {:Users/tysonbarrett/Dropbox/1 Dissertation/Reproducibility/Chang{\_}ReproEcon{\_}2015.pdf:pdf},
isbn = {9788578110796},
issn = {19362854},
journal = {Finance and Economics Discussion Series},
keywords = {Data and Code Archives,GDP,Gross Domestic Product,Journals,Macroeconomics,National Income and Product Accounts,Publication,Replication,Research},
pages = {1--26},
pmid = {25246403},
title = {{Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say "Usually Not"}},
volume = {083},
year = {2015}
}

@article{Goodman2016,
abstract = {The language and conceptual framework of “research reproducibility” are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for “truth.”},
author = {Goodman, Steven N and Fanelli, Daniele and Ioannidis, John P A},
doi = {10.1126/scitranslmed.aaf5027},
file = {:Users/tysonbarrett/Dropbox/1 Dissertation/Reproducibility/Goodman{\_}defineRepro{\_}2016.pdf:pdf},
issn = {1946-6234},
journal = {Science Translational Medicine},
number = {341},
pages = {1--6},
pmid = {27252173},
title = {{What does research reproducibility mean?}},
volume = {8},
year = {2016}
}


@article{OpenScienceCollaboration2015,
abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47{\%} of original effect sizes were in the 95{\%} confidence interval of the replication effect size; 39{\%} of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68{\%} with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Open Science Collaboration}},
doi = {10.1126/science.aac4716},
eprint = {arXiv:1011.1669v3},
file = {:Users/tysonbarrett/Library/Application Support/Mendeley Desktop/Downloaded/Open Science Collaboration - 2015 - Estimating the reproducibility of psychological science.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {0036-8075},
journal = {Science},
number = {6251},
pages = {aac4716--aac4716},
pmid = {26315443},
title = {{Estimating the reproducibility of psychological science}},
url = {http://science.sciencemag.org/content/349/6251/aac4716},
volume = {349},
year = {2015}
}


@article{tukey1980,
author = {Tukey, John},
journal = {The American Statistician},
number = {1},
pages = {79--88},
pmid = {12784225},
title = {{We Need Both Exploratory and Confirmatory}},
volume = {34},
year = {1980}
}
@article{Begley2015,
abstract = {Medical and scientific advances are predicated on new knowledge that is robust and reliable and that serves as a solid foundation on which further advances can be built. In biomedical research, we are in the midst of a revolution with the generation of new data and scientific publications at a previously unprecedented rate. However, unfortunately, there is compelling evidence that the majority of these discoveries will not stand the test of time. To a large extent, this reproducibility crisis in basic and preclinical research may be as a result of failure to adhere to good scientific practice and the desperation to publish or perish. This is a multifaceted, multistakeholder problem. No single party is solely responsible, and no single solution will suffice. Here we review the reproducibility problems in basic and preclinical biomedical research, highlight some of the complexities, and discuss potential solutions that may help improve research quality and reproducibility.},
author = {Begley, C. Glenn and Ioannidis, John P A},
doi = {10.1161/CIRCRESAHA.114.303819},
file = {:Users/tysonbarrett/Dropbox/1 Dissertation/Reproducibility/Begley{\_}ClinicalPreclinical{\_}2014.pdf:pdf;:Users/tysonbarrett/Dropbox/1 Dissertation/Reproducibility/Ioannidis{\_}2015.pdf:pdf},
isbn = {1524-4571 (Electronic)$\backslash$r0009-7330 (Linking)},
issn = {15244571},
journal = {Circulation Research},
keywords = {funding,journals,research integrity,universities},
number = {1},
pages = {116--126},
pmid = {25552691},
title = {{Reproducibility in science: Improving the standard for basic and preclinical research}},
volume = {116},
year = {2015}
}


@techreport{nhanes,
address = {Hyattsville, MD},
author = {{National Center for Health Statistics}},
institution = {U.S. Department of Health and Human Services, Centers for Disease Control and Prevention},
title = {{National Health and Nutrition Examination Survey Data}},
url = {http://www.cdc.gov/nchs/nhanes/},
year = {2016}
}

