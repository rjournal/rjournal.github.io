---
title: 'IRTest: An R Package for Item Response Theory with Estimation of Latent Distribution'
date: '2025-06-12'
abstract: |
  Item response theory (IRT) models the relationship between respondents'  latent traits and their responses to specific items. One key aspect of IRT is  the assumption about the distribution of the latent variables,  which can influence parameter estimation accuracy.  While a normal distribution has been conventionally assumed,  this may not always be appropriate. When the assumption of normality is violated,  latent distribution estimation (LDE) can enhance parameter estimation accuracy  by accommodating non-normal characteristics. Despite there being several methods proposed  for LDE in IRT, there is a lack of software designed to handle their implementations.  This paper introduces IRTest, a software program developed for  IRT analysis that incorporates LDE procedures.  It outlines the statistical foundation of LDE,  details the functionalities of IRTest, and provides examples of IRT analyses  to demonstrate the software's applications.
draft: no
author:
- name: Seewoo Li
  affiliation: University of California, Los Angeles
  address: Social Research Methodology, Department of Education
  email: seewooli@g.ucla.edu
  orcid: 0000-0002-6290-2777
type: package
output:
  rjtools::rjournal_article:
    self_contained: yes
    toc: no
    toc_depth: 2
bibliography: refs.bib
header-includes: \usepackage{float, amsmath}
editor_options:
  markdown:
    wrap: 72
date_received: '2024-01-11'
volume: 16
issue: 4
slug: RJ-2024-033

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.pos = "H", cache=TRUE)
library(IRTest)
library(mirt)
library(ltm)
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(kableExtra)
library(plotly)
```

<!-- %%%%%%%%%%%%%%% -->

<!-- %% Section 1 %% -->

<!-- %%%%%%%%%%%%%%% -->

# Introduction

Item response theory (IRT) is a widely used statistical framework for modeling 
the probabilistic relationship between examinees' latent traits 
(i.e., ability parameters) and their responses to specific items 
[@deAyala:2009;@Hambleton+Swaminathan+Rogers:1991;@vanderLinden:2016]. 
These latent traits, an essential component of IRT, typically represent unobservable 
human characteristics in educational and psychological assessments, 
such as academic ability, depression severity, or extroversion levels. 

In the estimation process of IRT models, particularly when using marginal maximum likelihood (MML), 
the latent trait’s distribution can impact parameter estimation [@Woods:2015]. 
MML is obtained by marginalizing a joint likelihood with respect to the latent variable, 
and any misspecification in the latent distribution can lead to biased parameter estimates. 
It is also worth noting that IRT necessarily assumes that the latent variables of interest are continuous. 
In some cases, a discrete latent distribution may better reflect observed data, 
such as in located latent class (LLC) models [see @Clogg:1981;@Follmann:1988;@Haberman:2005;@McCutcheon:1987;@Xu+vonDavier:2008].

The conventional assumption of normality in latent distributions has been 
questioned, and empirical evidence and potential drawbacks
of violating this assumption have been addressed [@Dudley-Marling:2020;
@Li:2022; @Mislevy:1984; @Sass+Schmitt+Walker:2008; @Seong:1990;
@Woods+Lin:2009]. Previous studies have identified factors that can
result in a skewed and/or bimodal latent distribution
[@Harvey+Murray:1994; @Ho+Yu:2015; @Woods:2015; @Yadin:2013]:
disparities between high-achieving and low-achieving groups, the
presence of an extreme group, difficulties of test items, and an innate
human tendency to be inclined to one side of a latent ability scale.
Potential problems of this assumption being violated include
biases in parameter estimates and errors in ensuing decision-making
processes. In this case, latent distribution estimation (LDE) can
effectively reduce the biases in parameter estimates by capturing
non-normal characteristics of the latent distribution.

Several methods have been proposed for LDE: 
the empirical histogram method [EHM: @Bock+Aitkin:1981;@Mislevy:1984], 
a mixture of two normal components [2NM: @Li:2021;@Mislevy:1984], 
the Ramsay-curve method [RCM: @Woods:2006], the Davidian-curve method [DCM: @Woods+Lin:2009], 
the log-linear smoothing method [LLS:@Casabianca+Lewis:2015;@Xu+vonDavier:2008], 
and the kernel density estimation method [KDM:@Li:2022].

\CRANpkg{IRTest} is an R package for unidimensional IRT analyses which aims
to handle LDE in IRT. \pkg{IRTest} is available from the
Comprehensive R Archive Network (CRAN) at
<https://cran.r-project.org/package=IRTest>. In \pkg{IRTest},
the model-fitting functions estimate the latent distribution through 
MML estimation using the EM algorithm [MML-EM:@Bock+Aitkin:1981]. Along with the
conventional method of assuming normality, five LDE methods
(EHM, 2NM, DCM, LLS, and KDM) are currently available as model-fitting procedures. 
Extensions of these methods to multidimensional settings will be updated in accordance with 
advancements in theoretical research, as the current lack of research 
in this area necessitates ongoing development.

Currently, there are not many software programs to implement LDE, 
and their choices of LDE methods are somewhat limited since LDE may
not be one of their main concerns. Most of them offer only EHM which
may be the most straightforward way to carry out LDE [e.g.,
BILOG-MG, @BILOG-MG; flexMIRT, @flexMIRT], some software
programs such as RCLOG [@RCLOG], LLSEM [@LLSEM], and \CRANpkg{sirt} [@sirt] focus on one
particular LDE method, and \CRANpkg{mirt} [@mirt] is equipped with
EHM and DCM.

This paper details the statistical foundation of \pkg{IRTest} and its implementation. 
The remainder of this paper is organized as
follows: [Section 2](#lde-in-irt) explains the basic statistical concepts
of the IRT parameter estimation and the LDE procedure within the MML-EM
framework. [Section 3](#lde-methods) discusses the LDE methods. 
[Section 4](#package-validation) validates \pkg{IRTest} by comparing it with 
existing packages. [Section 5](#implementations-of-the-irtest) demonstrates the
\pkg{IRTest} implementations. Lastly, [Section 6](#discussion) presents
a discussion on the package.

<!-- %%%%%%%%%%%%%%% -->

<!-- %% Section 2 %% -->

<!-- %%%%%%%%%%%%%%% -->

# LDE in IRT {#lde-in-irt}

This section provides a brief overview of the statistical aspects of
LDE in IRT, and specific LDE methods will be discussed in detail in the next section. 
The objectives of this section are 1) to introduce the basic concepts of IRT, 2)
to summarize the role of the latent distribution within the
estimation process, and 3) to explain how LDE can enhance
parameter estimation accuracy.

IRT is a statistical framework widely used in educational and psychological measurement 
to analyze how examinees' responses to test items reflect their underlying traits. 
IRT models offer mathematical functions to estimate the probability of examinees' 
responses based on item parameters and their ability levels. 
Among the estimation methods, MML is often preferred over conditional maximum 
likelihood (CML) and joint maximum likelihood (JML) for its versatility in model 
selection and statistical consistency of estimation. 
MML incorporates the latent distribution when integrating out the latent variable from the joint likelihood.

Statistically, LDE extends the normality assumption models by adding distribution parameters. 
While the marginal likelihood of the normality assumption models is dependent 
only on item parameters, LDE incorporates both item and distribution parameters 
in the marginal likelihood. 
It is worth noting that some LDE methods may not follow the maximum
likelihood estimation (MLE) approach. Instead, they may utilize another index to 
estimate distribution parameters.

## IRT models

Most IRT models follow a monotonically increasing probabilistic
form and specify a functional relationship between item parameters and
ability parameters. This section presents five well-known and
widely-used IRT models, all of which are available in \pkg{IRTest}.
For brevity, the discussion is mainly focused on three-parameter
logistic model [3PLM: @Birnbaum:1968] and generalized partial credit
model [GPCM: @Muraki:1992], since 3PLM can be reduced to one-parameter
logistic model [1PLM: @Rasch:1960] or two-parameter logistic model
[2PLM: @Birnbaum:1968] and GPCM can be reduced to partial credit model
[PCM: @Masters:1982]. The former handles dichotomous item
responses and the latter handles polytomous item responses.

More than one model can be applied to item response data. For
example, when analyzing a test of dichotomous items, the 2PLM can be
applied to short-answer items, while the 3PLM can be applied to
multiple-choice items. This differentiation allows researchers to
reflect the differences in guessing behaviors observed between the two
types of items. Also, the pair of one dichotomous response model and one
polytomous response model could be used for a mixed-format test that
comprises both dichotomous and polytomous items [@Baker+Kim:2004].

#### Three-parameter logistic model (3PLM)

Let $u \in \left\{ 0, 1 \right\}$ be a dichotomous item response,
$\theta$ be the ability parameter of an examinee, and $a$, $b$, and $c \, (0<c<1)$ be 
the item discrimination, difficulty, and guessing parameters, respectively. 
The item response function of the 3PLM can be expressed as, 
\begin{equation}
\Pr( u = 1 \, | \, \theta, a, b, c) = c + (1 - c)
\frac{\exp{\left(a (\theta - b) \right)}}{1 + \exp{\left(a (\theta - b) \right)}},
(\#eq:3PLM)
\end{equation} 
where $u=1$ indicates the correct response of the examinee. The
probability ranges from $c$ to 1 because of the guessing parameter
determining the lower bound. Given the nature of dichotomous items,
$1-\Pr( u = 1 \, | \, \theta, a, b, c)$ represents the probability of
an incorrect response. The model reduces to the 2PLM when $c=0$, and to the
1PLM when $c=0$ and the same $a$ value is assumed across all items.

#### Generalized partial credit model (GPCM)

The GPCM can be regarded as a polytomous form of the 2PLM. Let
$u \in \left\{0, 1, \dots, M\right\}$ ($M \ge 2$) be a polytomous item
response, $b_v \, (v = 1, 2, \dots, M)$ be the boundary parameters, and
the rest be the same as previously defined. The item response function
of the GPCM can be expressed as, 
\begin{equation}
\Pr( u = k \, | \, \theta, a, b_1, \dots , b_M)  =
\frac{ \exp{ \sum_{v = 0}^{k}{\left(a (\theta - b_v) \right)}} }
{ \sum_{m = 0}^{M}{\exp{ \sum_{v = 0}^{m}{\left(a (\theta - b_v) \right)}} }},
(\#eq:GPCM)
\end{equation}
where $\exp{ \sum_{v = 0}^{0}{\left(a (\theta - b_v) \right)}} = 1$
for notational convenience. Equation \@ref(eq:GPCM) represents the
probability of providing a response of $u = k$. The GPCM reduces to the
PCM when the same $a$ value is assumed across all items. 
When $M = 1$, they are reduced to their
dichotomous counterparts: the 1PLM and the 2PLM, respectively.

## Role of the latent distribution {#role-of-latent-distribution}

In the implementation of MML-EM, quadrature schemes are used to
numerically approximate the integral with respect to the latent variable
[@Baker+Kim:2004; @Bock+Aitkin:1981]. A quadrature scheme transforms a
continuous latent variable $\theta$ into a discrete variable
$\theta^{*}$; the domain of $\theta$ is divided into non-overlapping $Q$
grids, each of which is assigned to a certain value of $\theta^{*}$ called a 
quadrature point. Typically, quadrature points are set to the
middle of the grids. The default option of \pkg{IRTest} is to set
quadrature points from $-6$ to $6$ with an increment of 0.1, resulting in
121 quadrature points. Within the estimation functions of \pkg{IRTest}, 
the `range` and `q` arguments determine the range and the number of quadrature 
points, respectively. The corresponding probability mass
function (PMF) of the latent variable can be expressed as, 
\begin{equation}
A \left( \theta^{*}_{q} \right) =
\frac{ g \left( \theta^{*}_{q} \right) }
{ \sum_{ q = 1 }^{Q}{g \left( \theta^{*}_{q} \right)} },
(\#eq:quadrature)
\end{equation}
where $g{ \left( \theta \right)}$ is the probability density function
(PDF) of the latent variable and $\theta^{*}_{q}$ is the $q$th
quadrature point ($q = 1, 2, \dots, Q$) [@Baker+Kim:2004]. In this
paper, the term latent distribution indicates either $g{ \left( \theta \right)}$ 
or $A{ \left( \theta^{*} \right)}$ depending on the context.

The marginal log-likelihood of the model is the quantity to be maximized
in the estimation procedure, which can be expressed as follows
[@Baker+Kim:2004]: 
\begin{equation}
\begin{split}
\log L &= \sum_{j = 1}^{N}
{ \log{ \int_{\theta}{ L_j (\theta) g (\theta) \, d \theta } } } \\
&\approx \sum_{j = 1}^{N}
{ \log{ \sum_{q = 1}^{Q}{ L_j{\left( \theta^{*}_{q} \right)}
A{\left( \theta^{*}_{q} \right)} } } }.
\end{split}
(\#eq:likeli)
\end{equation}
In the equation above, the integral is approximated by the summation
to facilitate the EM algorithm. The quantity
$L_j{\left( \theta^{*}_{q} \right)}$ is the $j$th examinee's
($j = 1, 2, \dots, N$) likelihood for their item responses given
that their ability parameter is $\theta^{*}_{q}$.

Equation \@ref(eq:likeli) shows that the latent distribution is one of the 
components of the marginal log-likelihood. Consequently, the specification of the 
latent distribution influences the value of the marginal log-likelihood of a 
model, potentially impacting the accuracy of parameter estimates.

Additionally, the latent distribution plays a role in model identification and 
affects the convergence of the MML-EM procedure.
In \pkg{IRTest}, a scale is assigned to the latent variable by setting the mean and 
standard deviation of the latent distribution to 0 and 1, respectively. 
Meanwhile, for LDE methods such as LLS and DCM, where a hyperparameter 
determines the number of distributional parameters, the MML-EM procedure may 
not converge with a small sample size and a large number of distributional 
parameters.

## LDE in the MML-EM procedure

The decomposition of the marginal log-likelihood would help explicate the
separate estimation of the item and distribution parameters, which can
be expressed as follows [@Li:2021]: 
\begin{equation}
\begin{split}
\log L &\approx \sum_{q = 1}^{Q}{ \sum_{j = 1}^{N}
{ \gamma_{jq} \log{L_j{\left( \theta^{*}_{q} \right)}} } } +
\sum_{q = 1}^{Q}{ \sum_{j = 1}^{N}
{ \gamma_{jq} \log{A{\left( \theta^{*}_{q} \right)}} } } -
\sum_{q = 1}^{Q}{ \sum_{j = 1}^{N}{ \gamma_{jq} \log{\gamma_{jq}} } } \\
&= \quad \log{L_{\text{item}}} \quad
+ \quad \log{L_{\text{distribution}}} \quad - \quad \text{(constant)} .
\end{split}
(\#eq:decomposition)
\end{equation}
The quantity
$\gamma_{jq} = E{\left(\Pr_{j}{\left( \theta_{q}^{*} \right)}\right)}$,
calculated through Bayes' theorem in the expectation-step (E-step) of
the EM algorithm, represents the expected probability of $j$th
examinee's ability parameter belonging to the $q$th grid [see
@Baker+Kim:2004]. Then, in the maximization-step (M-step), the item and
distribution parameters are estimated. Since $\gamma_{jq}$ is a function
of the latent distribution, precise specification of the latent
distribution would enhance the accuracy of $\gamma_{jq}$. Thus, the
parameter estimates are implicitly affected by the latent distribution
through $\gamma_{jq}$.

In equation \@ref(eq:decomposition), regarding $\gamma_{jq}$ as a
constant, the $L_j{\left( \theta^{*}_{q} \right)}$ in the first
term depends only on item parameters, while the
$A{\left( \theta^{*}_{q} \right)}$ in the second term depends only on
distribution parameters. This probabilistic independence allows the
separate estimation of the item and distribution parameters, from which
a selection of estimation methods of the distribution parameters may emerge.

To elaborate more on the second term of equation
\@ref(eq:decomposition), it can be rewritten and simplified as, 
\begin{equation}
\begin{split}
\log{L_{\text{distribution}}}
&= \sum_{q = 1}^{Q}{ \sum_{j = 1}^{N}
{ \gamma_{jq} \log{A{\left( \theta^{*}_{q} \right)}} } } \\
&= \sum_{q = 1}^{Q}{ \hat{f_{q}} \log{A{\left( \theta^{*}_{q} \right)}} },
\end{split}
(\#eq:llikelidist)
\end{equation}
where $f_q$ is an unknown true frequency at the $q$th grid and
$\hat{f_q} = E{\left( f_q \right)} = \sum_{j = 1}^{N}{\gamma_{jq}}$ is
the expected frequency at the $q$th grid by the definition of
$\gamma_{jq}$. In the E-step, the latent distribution is involved in
calculating $\hat{f_q}$, then, in the M-step, the distribution
parameters are estimated and updated by using the quantity $\hat{f_q}$.
This E and M cycle iterates until the algorithm converges. The estimated
parameters in the last iteration would be the final output, and the
corresponding distribution of the final output becomes the estimated
latent distribution.

Unlike the item parameter estimation being aligned with the MLE approach of the 
MML-EM procedure, the distribution parameters are not always estimated by 
maximizing equation \@ref(eq:llikelidist). With the MLE approach still 
being the dominant choice, different approaches, such as minimizing the *approximate mean integrated squared error* [@Li:2022], can be applied
to estimate distribution parameters, which is discussed in the
next section. In all case, every strategy for the estimation of
distribution parameters utilizes $\gamma_{jq}$ in its
estimation procedure.

<!-- %%%%%%%%%%%%%%% -->

<!-- %% Section 3 %% -->

<!-- %%%%%%%%%%%%%%% -->

# LDE methods {#lde-methods}

In principle, almost every density estimation method can be used for LDE in IRT.
However, existing studies have
selectively inspected and developed some methods that would enhance the
effectiveness of practical applications of IRT and/or benefit the
researchers working on IRT. This section focuses on four LDE
methods to highlight their methodological diversity. The choice
and order of the methods in this section are not intended to imply any
superiority of one method over the other. 

At the time of writing this article, there is a lack of research comparing 
LDE methods. Given that comparing models in meaningful ways is crucial for 
practical applications, further studies examining the evaluation criteria of LDE methods 
would be beneficial for practitioners to select the most suitable method for their analyses.

## Empirical histogram method (EHM)

One simple LDE strategy would be to directly employ the outputs
obtained from the E-step. EHM does this by simply calculating the
expected probabilities for each grid of the quadrature scheme, which can
be considered either as the normalized expected sample size or
nonparametric maximum likelihood estimates [@Bock+Aitkin:1981; @Laird:1978; @Mislevy:1984]. 
The entire estimation process can be easily
portrayed in the form of an equation as follows: 
\begin{equation}
\hat{ A_{q}} =
E{\left( A_{q} \right)} =
\frac{\sum_{j = 1}^{N}{ E{\left(\Pr_{j}{\left( \theta_{q}^{*} \right)}\right)} }}{N} =
\frac{\sum_{j = 1}^{N}{\gamma_jq}}{N} =
\frac{\hat{f_q}}{N},
(\#eq:ehm)
\end{equation}
where $A_{q}$ denotes $A{\left( \theta^{*}_{q} \right)}$ for notational brevity. 
It can be seen that the estimates are simply
the expected frequencies $\hat{f_q}$'s divided by the total population
$N$. EHM can be implemented in the estimation functions of 
\pkg{IRTest} by specifying the argument as `latent_dist=”EHM”`.

Alternatively, the MLE solution can be derived in the following manner 
using Lagrangian multipliers. With Lagrangian multipliers, the
quantity to be maximized becomes, 
\begin{equation}
\mathcal{L} = \sum_{q = 1}^{Q}{ \hat{f_{q}} \log{A_{q}} } -
\lambda \left( \sum_{q = 1}^{Q}{A_{q}} - 1 \right),
(\#eq:ehmmle)
\end{equation}
where the second term is introduced from the constraint for a proper
distribution (i.e., $\sum_{q}{A_{q}} = 1$). Differentiating
$\mathcal{L}$ with respect to $A_{q}$ and equating it to zero yields 
\begin{equation}
\frac{\partial \mathcal{L}}{\partial A_{q}} =
\frac{\hat{f_{q}}}{A_{q}} - \lambda = 0.
(\#eq:ehmdiffern)
\end{equation} 
Then, $A_{q} = \frac{\hat{f_{q}}}{\lambda}$ for all
$q = 1, 2, \dots, Q$, which results in $\lambda = N$ by the constraint.
This shows that $\hat{ A_{q}} = \frac{\hat{f_{q}}}{N}$ maximizes the
likelihood $\mathcal{L}$.

In addition to its simplicity and expediency, EHM has been shown to
be effective in reducing biases in parameter estimates when the
normality assumption is violated. However, the performance of EHM could be 
limited to some extent when it fails to screen out the random noise of the data, 
thereby producing less accurate parameter estimates [@Li:2021;
@Woods:2015; @Woods+Lin:2009]. To address this issue, some methods incorporate smoothing
procedures to alleviate the impacts of the random noise, which are
addressed later in this section.

## Two-component normal mixture distribution (2NM)

The 2NM is made up of two normal components added up together to form a
single distribution. As a natural extension of the normality assumption,
the 2NM could be thought of as a non-normal distribution caused by two
different latent groups where each group is assumed to follow a normal
distribution. The addition of a normal component imparts flexibility to
the 2NM to reflect bimodality and skewness of the latent
distribution. The 2NM method can be implemented in the estimation
functions of \pkg{IRTest} by specifying the argument as `latent_dist=”2NM”`.

Letting $\tau = [\pi, \mu_1, \mu_2, \sigma_1, \sigma_2]^{'}$ be the vector
of five original parameters of the 2NM, the PDF of the 2NM can be expressed as follows [@Li:2021]:
\begin{equation}
g{(\theta \, | \, \tau)} = \pi \times \phi{(\theta \, | \, \mu_1, \sigma_1)} +
(1 - \pi) \times \phi{(\theta \, | \, \mu_2, \sigma_2)},
(\#eq:2nm)
\end{equation}
where $\phi{(\theta)}$ is a normal component.

Proportionality holds when $A{\left( \theta^{*}_{q} \right)}$ in equation
\@ref(eq:llikelidist) is substituted with $g{(\theta^{*}_{q} \, | \, \tau)}$, resulting in
$\log{L_{\text{distribution}}}\propto \sum_{q = 1}^{Q}{ \hat{f_{q}} \log{g{(\theta^{*}_{q} \, | \, \tau)}} }$.
The MLE results for the 2NM parameters can be obtained by introducing
another EM algorithm. In this paper, this additional optimization
algorithm would be referred to as *the secondary EM algorithm* nested in
the M-step of the primary EM algorithm.

To estimate the 2NM parameters, another quantity
$\eta_q$ is calculated in the E-step of the secondary EM algorithm,
which represents the expected probability of $\theta_q$ belonging to the
first normal component. With the quantity $\eta_q$, the likelihood of the M-step 
of the secondary EM algorithm can be expressed as follows [@Li:2021]: 
\begin{equation}
\begin{split}
\log{L_{\text{distribution}}} &\propto
\sum_{q = 1}^{Q}{ \hat{f_q} \eta_q \log{\left[ \pi \phi{(\theta_q \, | \, \mu_1, \sigma_1)} \right]} } 
\\ &\quad + \sum_{q = 1}^{Q}{ \hat{f_q} (1 - \eta_q) \log{\left[ (1 - \pi) \phi{(\theta_q \, | \, \mu_2, \sigma_2)} \right]} }.
\end{split}
(\#eq:2nmlikeli)
\end{equation}
The closed-form solution for the 2NM parameters can be obtained by
differentiating the likelihood with respect to each parameter and
setting the first derivatives equal to zero [@Li:2021]: 
\begin{equation}
\hat{\pi} = \frac{\sum_{q = 1}^{Q}{ \hat{f_q} \eta_{q}}}{\sum_{q = 1}^{Q}{ \hat{f_q}}} =
\frac{\sum_{q = 1}^{Q}{ \hat{f_q} \eta_{q}} }{N},
(\#eq:pi)
\end{equation}
\begin{equation}
\hat{\mu_{1}} = \frac{\sum_{q = 1}^{Q}{ \hat{f_q} \eta_{q} \theta_{q}^{*}}}
{\sum_{q = 1}^{Q}{ \hat{f_q} \eta_{q}}},
(\#eq:mu1)
\end{equation}
\begin{equation}
\hat{\mu_{2}} = \frac{\sum_{q = 1}^{Q}{ \hat{f_q} (1- \eta_{q}) \theta_{q}^{*}}}
{\sum_{q = 1}^{Q}{ \hat{f_q} (1- \eta_{q})}},
(\#eq:mu2)
\end{equation}
\begin{equation}
\hat{\sigma_{1}^{2}} = \frac{\sum_{q = 1}^{Q}{ \hat{f_q} \eta_{q} \left( \theta_{q}^{*} - \hat{\mu_{1}} \right)^{2} }}
{\sum_{q = 1}^{Q}{ \hat{f_q} \eta_{q}}},
(\#eq:sigma1)
\end{equation}
and 
\begin{equation}
\hat{\sigma_{2}^{2}} = \frac{\sum_{q = 1}^{Q}{ \hat{f_q} (1- \eta_{q}) \left( \theta_{q}^{*} - \hat{\mu_{2}} \right)^{2} }}
{\sum_{q = 1}^{Q}{ \hat{f_q} (1- \eta_{q})}}.
(\#eq:sigma2)
\end{equation}

Both advantages and disadvantages of the 2NM method stem from the
parametric nature of the 2NM. The parameters of the 2NM render the
estimated latent distribution interpretable. Also, the
reparameterization of the 2NM parameters offers an inherent way to fix the
mean and variance of the latent distribution to constants [see
@Li:2021], which is a typical way to assign a scale to the latent
variable in the MML-EM procedures. On the other hand, compared with its
nonparametric counterparts, the flexibility of the 2NM is limited to some
extent. For example, the 2NM is incapable of forming a wiggly-shaped
distribution.

## Davidian-curve method (DCM)

DCM uses a semi-nonparametric distribution, where the hyperparameter
$h = 1, 2, \dots, 10$ determines the complexity of the density
[@Woods+Lin:2009]. When $h = 1$, the distribution reduces to the normal
distribution. In general, the search for an optimal hyperparameter involves 
balancing the flexibility of the latent distribution with the prevention of overfitting. 
DCM can be implemented in the estimation functions of \pkg{IRTest} 
by specifying the argument as `latent_dist=”DC”`.

In DCM, the latent distribution can be expressed as follows: 
\begin{equation}
g{\left( \theta \, | \, h, \mathrm{\mathbf{m}}\right)} =
\left\{ P_{h}{( \theta )} \right\}^{2} \varphi{(\theta)} =
\left\{ \sum_{k = 0}^{h}{m_{k} \theta^{k}} \right\}^{2} \varphi{(\theta)} ,
(\#eq:dcm)
\end{equation}
where $\mathrm{\mathbf{m}} = \left[ m_0, m_1, \dots, m_h \right]^{'}$
is a vector of coefficients, $P_h$ is a polynomial of order $h$,
$\varphi$ is the standard normal distribution, and $m_h \ne 0$
[@Woods+Lin:2009; @Zhang+Davidian:2001]. The following constraint
guarantees that the function is a proper distribution
[@Zhang+Davidian:2001]: 
\begin{equation}
\begin{split}
E{\left( P_{h}{(Z)^{2}}\right)}
&= E{\left( (\mathrm{\mathbf{mZ}} )^{2}\right)} \\
&= \mathrm{\mathbf{m}}^{'} E{\left( \mathrm{\mathbf{ZZ}}^{'} \right)} \mathrm{\mathbf{m}} \\
&= \mathrm{\mathbf{m}}^{'} \mathrm{\mathbf{Mm}} \\
&= \mathrm{\mathbf{m}}^{'} \mathrm{\mathbf{B}}^{'} \mathrm{\mathbf{Bm}} \\
&= \mathrm{\mathbf{c}}^{'} \mathrm{\mathbf{c}} \\
&= 1.
\end{split}
(\#eq:constraint)
\end{equation}
In the constraint above, $Z \sim N{(0, 1)}$,
$\mathrm{\mathbf{Z}} = \left[ 1, Z^1, Z^1, \dots, Z^h \right]^{'}$,
$\mathrm{\mathbf{M}} = E{\left( \mathrm{\mathbf{ZZ}}^{'} \right)}$,
$\mathrm{\mathbf{B}}^{'} \mathrm{\mathbf{B}} = \mathrm{\mathbf{M}}$ 
by eigenvalue decomposition, and
$\mathrm{\mathbf{c}} = \mathrm{\mathbf{Bm}}$ is a $h + 1$ dimensional
vector. By applying a polar coordinate transformation of $\mathrm{\mathbf{c}}$, the
constraint is always satisfied [see @Woods+Lin:2009;@Zhang+Davidian:2001].

As DCM follows the MLE approach, the quantity to be maximized for
LDE can be expressed as, 
\begin{equation}
\log{L_{\text{distribution}}} \propto
\sum_{q = 1}^{Q}{
\hat{f_{q}}\log\left[
\left\{
\left[ \mathrm{\mathbf{B}}^{-1} \mathrm{\mathbf{c}} \right]^{'}
\begin{bmatrix}
\left( \theta_{q}^{*} \right)^{0} \\
\left( \theta_{q}^{*} \right)^{1} \\
\vdots \\
\left( \theta_{q}^{*} \right)^{h} \\
\end{bmatrix}
\right\}^{2}
\varphi{\left( \theta_{q}^{*} \right)}
\right]
}.
(\#eq:dcmlikeli)
\end{equation} 
Since the elements of $\mathrm{\mathbf{B}}^{-1}$ are constants, the
latent distribution is estimated by finding $\mathrm{\mathbf{c}}$ that
maximizes the likelihood above.

In the implementation of DCM, ten models are typically fitted according to each
value of the hyperparameter ($h = 1, 2, \dots, 10$). Then, the best model is selected by
Hannan-Quinn (HQ) criterion [@Hannan+Quinn:1979]: 
\begin{equation}
HQ = -2 \log{L} + 2 p \left( \log{(\log{N})} \right),
(\#eq:hq)
\end{equation}
where $N$ is the total number of examinees and $p$ is the number of
parameters to be estimated. Focusing on whether HQ criterion selects
$h = 1$ or not, this model selection procedure in DCM can be used to
examine the normality of the latent distribution [@Woods+Lin:2009].

This paper does not go into details of LLS [@Casabianca+Lewis:2015;@Xu+vonDavier:2008]
because of the similarities between DCM and LLS: both of them
take the MLE approach for parameter estimation, and the hyperparameter $h$
determines the number of distribution parameters to control the degree of smoothing.

## Kernel density estimation method (KDM)

KDM is a nonparametric method for conducting the LDE procedure, and it can be
implemented in the estimation functions of \pkg{IRTest} by
specifying the argument as `latent_dist=”KDE”`. In general, a kernel
function is assigned to every observation to be stacked up all together
and form a density function. In the context of LDE in IRT, the former statement
means that $\hat{f_{q}}$ kernels are assigned to $\theta_{q}^{*}$, which
can be expressed as, 
\begin{equation}
g{\left( \theta \, | \, h \right)} =
\frac{1}{Nh} \sum_{q = 1}^{Q}{\hat{f_q} K{\left( \frac{\theta - \theta_{q}^{*}}{h} \right)}} ,
(\#eq:kdm)
\end{equation}
where $K{( \cdot )}$ is a kernel function and $h$ is a hyperparameter
often referred to as the *bandwidth*. The following discussion assumes 
the Gaussian kernel for $K{( \cdot )}$ as a general default choice [@Gramacki:2018; @Silverman:1986].

KDM takes a different approach from the previously discussed methods
in carrying out the LDE procedure. Instead of the log-likelihood ($\log{L_{\text{distribution}}}$), 
the approximate mean integrated squared error (AMISE) is used, 
which is the Tayor-series approximation of the mean integrated squared error: 
\begin{equation}
AMISE{\left( \hat{g_h} \right)} =
\frac{1}{2 N h \sqrt{\pi}} +
\frac{h^{4}}{4} R{\left( g'' \right)}.
(\#eq:amise)
\end{equation}
In the equation above, $\hat{g_h}$ is the estimated latent
distribution using the bandwidth $h$ and
$R{\left( g'' \right)} = \int{g''{( x )} \, dx}$. Note that this is 
a simplified version of AMISE by the adoption of the Gaussian kernel. 
To find $h$ that minimizes equation \@ref(eq:amise), 
another equation is obtained by differentiating AMISE with
respect to $h$ and equating it to 0 [@Gramacki:2018; @Silverman:1986;
@Wand+Jones:1995]: 
\begin{equation}
h_{AMISE} =
\left[
2 N \sqrt{\pi} R{\left( g'' \right)}
\right]^{- \frac{1}{5}} .
(\#eq:amiseh)
\end{equation}
Unfortunately, this solution cannot be immediately employed in
estimating the bandwidth, because $R{\left( g'' \right)}$ still depends
on the unknown density $g{(\theta)}$. For the details of the methods that
deal with this situation, refer to @Silverman:1986, @Gramacki:2018,
@Sheather:2004, and @Wand+Jones:1995. Utilizing the built-in R function
`stats::density()` for the KDM procedure, the default option of 
\pkg{IRTest} is `bandwidth=”SJ-ste”`, a recommended method for bandwidth
estimation [@Jones+Marron+Sheather:1996; @Sheather+Jones:1991]. Other
available options for the `bw` argument of the `stats::density()` can
also be passed to the `bandwidth` argument of \pkg{IRTest}.

On the one hand, KDM and DCM are similar in that their
hyperparameters determine the degree of smoothing. On the other hand,
once the $\gamma_{jq}$ is calculated and treated as a constant, the
hyperparameter of KDM is the only parameter to influence the LDE results,
whereas the LDE results of DCM depends on both the hyperparameter and 
the corresponding $h+1$ density parameters. The absence of distribution
parameter in KDM, except for the hyperparameter (bandwidth) itself, 
allows KDM to estimate the hyperparameter in a single
model-fitting procedure, thereby obviating the need for a model
selection process. Compared with other methods having a model selection
step, this advantage may decrease computation time and expedite the
analysis [@Li:2022].

<!-- %%%%%%%%%%%%%%% -->

<!-- %% Section 4 %% -->

<!-- %%%%%%%%%%%%%%% -->

# Package validation {#package-validation}

This section examines and validates the estimation performance of
\pkg{IRTest} by comparing its results with those from
\CRANpkg{mirt} [@mirt] and \CRANpkg{ltm} [@ltm]. \CRANpkg{mirt} and
\CRANpkg{ltm} are among the widely used R packages for IRT analyses. To this end, 
a dataset of ten dichotomous items and 1,000 examinees is
generated using the 2PLM and under a right-skewed latent distribution, 
which is used throughout this section.

```{r validation, echo=FALSE, message=FALSE, results="hide"}
Alldata <- IRTest::DataGeneration(N = 1000,
                                  nitem_D = 10,
                                  latent_dist = "2NM",
                                  d = 1.414,
                                  sd_ratio = 2,
                                  prob = .66)

simulated_data <- Alldata$data_D
colnames(simulated_data) <- paste0("Item",1:10)
true_item <- Alldata$item_D
true_theta <- Alldata$theta

model_IRTestd <- IRTest::IRTest_Dich(simulated_data, q = 61)$par_est
model_IRTestp <- IRTest::IRTest_Poly(simulated_data, q = 61)$par_est
model_mirt <- coef(mirt::mirt(simulated_data, model = 1, quadpts = 61), simplify = TRUE)$items
model_ltm <- coef(ltm::ltm(simulated_data~z1, control = list(GHk=61)))

result_a <- cbind(true_item[,1], model_IRTestd[,1], model_IRTestp[,1], model_mirt[,1], model_ltm[,2])
result_b <- cbind(true_item[,2], model_IRTestd[,2], model_IRTestp[,2], -model_mirt[,2]/model_mirt[,1], model_ltm[,1])

colnms <- c("parameter", "IRTest::IRTest_Dich()", "IRTest::IRTest_Poly()", "mirt::mirt()", "ltm::ltm()")
colnames(result_a) <- colnms
colnames(result_b) <- colnms
```

The following functions are used to fit the models: `IRTest::IRTest_Dich()` and
`IRTest::IRTest_Poly()` from \pkg{IRTest}, `mirt::mirt()` from \CRANpkg{mirt}, and
`ltm::ltm()` from \CRANpkg{ltm}. The standard normal latent distribution, the 2PLM, and
61 quadrature points are applied to all functions, and the rest of the
options are set to the default. Note that `IRTest::IRTest_Poly()` can also be
applied to binary data, but with lower computational efficiency compared to
`IRTest::IRTest_Dich()`.

```{r parameter-static, eval = knitr::is_latex_output(), echo=FALSE}
knitr::kable(rbind(result_a, result_b), format = "latex", caption = "Item parameters and their estimates", digits = 4, booktabs = T) %>% 
  kableExtra::kable_styling(font_size = 7, position = "center") %>%
  kableExtra::pack_rows("Item discrimination parameter (a)", 1, 10) %>%
  kableExtra::pack_rows("Item difficulty parameter (b)", 11, 20)
```

```{r parameter-interactive, eval = knitr::is_html_output(), layout = "l-body", echo=FALSE}
knitr::kable(rbind(result_a, result_b), format = "html", caption = "Item parameters and their estimates", digits = 4, booktabs = T)%>% 
  kableExtra::kable_styling(position = "center") %>% 
  kableExtra::kable_styling(font_size = 7, position = "center") %>%
  kableExtra::pack_rows("Item discrimination parameter (a)", 1, 10) %>%
  kableExtra::pack_rows("Item difficulty parameter (b)", 11, 20)
```


Table
`r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:parameter-interactive)', '\\@ref(tab:parameter-static)'))`
shows the item parameters and their estimates. The result shows that the
parameter estimates from \pkg{IRTest} are almost identical to those from
\CRANpkg{mirt} and \CRANpkg{ltm}. For example, the root mean square error (RMSE) 
of the $a$ (item discrimination) parameter estimates between
`IRTest::IRTest_Dich()` and `mirt::mirt()` is
`r round(sqrt(mean((result_a[,2] - result_a[,4])^2))*10000, 2)`$\times 10^{-4}$,
and that between `IRTest::IRTest_Dich()` and `ltm::ltm` is
`r round(sqrt(mean((result_a[,2] - result_a[,5])^2))*10000, 2)`$\times 10^{-4}$.
Similarly, the RMSE of the $b$ (item difficulty)
parameter estimates between `IRTest::IRTest_Dich()` and `mirt::mirt()` is
`r round(sqrt(mean((result_b[,2] - result_b[,4])^2))*10000, 2)`$\times 10^{-4}$,
and that between `IRTest::IRTest_Dich()` and `ltm::ltm()` is
`r round(sqrt(mean((result_b[,2] - result_b[,5])^2))*10000, 2)`$\times 10^{-4}$.
This shows that the estimates from the four functions are
practically identical.

```{r ise, echo=FALSE, message=FALSE, results="hide"}
n_quad <- 121
delta_theta <- seq(-6,6,length.out=n_quad)[2]-seq(-6,6,length.out=n_quad)[1]
LD_mirt_DCM <- mirt::extract.mirt(mirt::mirt(simulated_data, model = 1, quadpts = n_quad, dentype = "Davidian-3"), "Prior")[[1]]/delta_theta
LD_mirt_EHM <- as.vector(
  mirt::extract.mirt(mirt::mirt(simulated_data, model = 1, quadpts = n_quad, dentype = "EHW"), "Prior")[[1]]
  )/delta_theta
LD_IRTest_DCM <- IRTest::IRTest_Dich(simulated_data, q = n_quad, latent_dist = "DC", h=3)$Ak/delta_theta
LD_IRTest_EHM <- IRTest::IRTest_Dich(simulated_data, q = n_quad, latent_dist = "EHM")$Ak/delta_theta

# Collecting the estimates into a dataframe
LD_df <- data.frame(theta = rep(seq(-6,6,length.out=n_quad), 6),
                    density = c(LD_mirt_DCM, LD_mirt_EHM, LD_IRTest_DCM, LD_IRTest_EHM, 
                                IRTest::dist2(x = seq(-6,6,length.out=n_quad), d=1.414, sd_ratio = 2, prob = .66),
                                IRTest::dist2(x = seq(-6,6,length.out=n_quad), d=1.414, sd_ratio = 2, prob = .66)),
                    package = c(rep("mirt", n_quad*2), rep("IRTest", n_quad*2), rep("True", n_quad*2)),
                    LDE.method = rep(c("DCM","EHM","DCM","EHM","DCM","EHM"), each = n_quad)
                    )
LD_df$LDE.method <- factor(LD_df$LDE.method, levels = c("EHM", "DCM"))
```

```{r LDE-validation-html, echo=FALSE, fig=TRUE, fig.height=5, fig.width=7, fig.align='center', include=knitr::is_html_output(), eval=knitr::is_html_output(), fig.cap="Estimated latent distributions from IRTest and mirt"}
LD_df %>% 
  ggplot2::ggplot(aes(x=theta, y=density, color = package)) +
  ggplot2::geom_line(linewidth=1)+
  ggplot2::facet_wrap(~ LDE.method,
                      nrow = 2,
                      scales = "fixed") +
  ggplot2::scale_color_manual(values = c("#619CFF", "#F8766D", "#00BA38")) +
  ggplot2::lims(x = c(-4, 4),
                y = c(0, .85)) +
  ggplot2::labs(x=expression(theta),
                y='density') +
  ggplot2::theme_bw()
```



```{r LDE-validation-latex, echo=FALSE, fig=TRUE, fig.height=5, fig.width=7.5, out.width="80%", fig.align='center', include=knitr::is_latex_output(), eval=knitr::is_latex_output(), fig.cap="Estimated latent distributions from IRTest and mirt"}
LD_df %>% 
  ggplot2::ggplot(aes(x=theta, y=density, color = package)) +
  ggplot2::geom_line(linewidth=1)+
  ggplot2::facet_wrap(~ LDE.method,
                      nrow = 2,
                      scales = "fixed") +
  ggplot2::scale_color_manual(values = c("#619CFF", "#F8766D", "#00BA38")) +
  ggplot2::lims(x = c(-4, 4),
                y = c(0, .85)) +
  ggplot2::labs(x=expression(theta),
                y='density') +
  ggplot2::theme_bw()
```

To validate the LDE procedures of \pkg{IRTest}, the estimated latent 
distributions of \pkg{IRTest} are compared with those estimated 
from \CRANpkg{mirt}, using EHM and DCM. The hyperparameter $h$ of DCM is set to $h=3$ for an 
illustrative purpose by considering the complexity of the true distribution and 
the size of the data. Total 121 quadrature points are used for the MML-EM 
procedure. Integrated squared error (ISE), one of the widely used 
criterion in comparing two distributions, is used as an index to measure 
the closeness of the estimated distributions of \pkg{IRTest} and \CRANpkg{mirt} 
[for the usages of ISE, see @Jones:1991]:
\begin{equation}
\begin{split}
ISE{\left( \hat{g}^{\,\text{IRTest}}, \, \hat{g}^{\,\text{mirt}} \right)} &=
\int_{-\infty}^{\infty}{\left(
{\hat{g}(\theta)}^{\,\text{IRTest}} - {\hat{g}(\theta)}^{\,\text{mirt}}
\right)^2\, d\theta} \\
&\approx \sum_{q=1}^{Q}{\left(
{\hat{g}\left(\theta_{q}^{*}\right)}^{\,\text{IRTest}} - 
{\hat{g}\left(\theta_{q}^{*}\right)}^{\,\text{mirt}}
\right)^2 \Delta\theta^{*}},
\end{split}
(\#eq:ise)
\end{equation}
where $\Delta\theta^{*}$ is the distance between quadrature points, and 
$\hat{g}^{\,\text{IRTest}}$ and $\hat{g}^{\,\text{mirt}}$ are the estimated 
latent distributions of \pkg{IRTest} and \CRANpkg{mirt}, respectively. 
In equation \@ref(eq:ise), the integral is approximated by the summation 
using the quadrature scheme of the MML-EM procedure.

Figure 
`r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(fig:LDE-validation-html)', '\\@ref(fig:LDE-validation-latex)'))` 
illustrates the estimated latent 
distributions from both packages and the true latent distribution. Note that 
values outside of $\left|\theta\right|\le4$ are truncated for visualization, 
but they are included in the ISE calculation. The figure 
shows that the estimated distribution of \pkg{IRTest} 
(`r knitr::asis_output(ifelse(knitr::is_html_output(), '<span style="color: #619CFF;">blue line</span>', '\\textcolor[HTML]{619CFF}{blue line}'))`) 
and the one from \CRANpkg{mirt} 
(`r knitr::asis_output(ifelse(knitr::is_html_output(), '<span style="color: #F8766D;">red line</span>', '\\textcolor[HTML]{F8766D}{red line}'))`) 
closely resemble each other. The ISE values for EHM and DCM are 
`r sum((LD_mirt_EHM-LD_IRTest_EHM)^2*delta_theta) %>% round(4)` and 
`r sprintf("%1.4f", sum((LD_mirt_DCM-LD_IRTest_DCM)^2*delta_theta))`, respectively, 
which numerically validate the similarities of the estimated distributions 
of the two packages.


<!-- %%%%%%%%%%%%%%% -->

<!-- %% Section 5 %% -->

<!-- %%%%%%%%%%%%%%% -->

# Implementations of IRTest {#implementations-of-the-irtest}

The primary purpose of this section is to demonstrate the usages of
\pkg{IRTest} with examples. [Section 5.1](#the-effect-of-the-lde)
provides an example using a simulated dataset. In doing so, it
illustrates the effect of LDE when the normality assumption is violated.
[Section 5.2](#an-empirical-example) performs an IRT analysis using
the Generic Conspiracist Beliefs Scale (GCBS) data [@GCBS] available from
the Open-Source Psychometric Project at
<http://openpsychometrics.org/_rawdata/GCBS.zip>.

## The effect of LDE {#the-effect-of-the-lde}

This section illustrates the effect of LDE by showing an improvement
in estimation accuracy. To this end, an artificial item response dataset is 
generated from a non-normal latent distribution. The parameters of this 
dataset are used as the true values in evaluating errors.

#### Data generation

Using the function `IRTest::DataGeneration()`, a dataset of 40 dichotomous items
and 2,000 respondents is generated by

```{r data generation, echo=TRUE}
Alldata <- IRTest::DataGeneration(N = 2000,
                                  nitem_D = 40,
                                  latent_dist = "2NM",
                                  d = 1.414,
                                  sd_ratio = 2,
                                  prob = 2/3)
simulated_data <- Alldata$data_D
true_item <- Alldata$item_D
true_theta <- Alldata$theta
```

where `simulated_data` is the item response matrix, `true_item` is the
item parameter matrix, and `true_theta` is the vector of ability
parameters. The 2PLM is employed in generating the dataset, and
a 2NM distribution is employed to simulate a non-normal latent
distribution (`latent_dist = "2NM"`) with its parameters being `d = 1.414`,
`sd_ratio = 2`, and `prob = 2/3` [for the reparameterization of the 2NM, see
@Li:2021; @IRTest].

The highly skewed distribution is chosen to clearly demonstrate
the effect of LDE, which is likely to be rare in
practice (see Figure
`r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(fig:estimated-density-html)', '\\@ref(fig:estimated-density-latex)'))`).
Therefore, cautions are needed in interpreting and understanding the magnitude of the effect.

#### Model fitting

The function `IRTest::IRTest_Dich()` is applied to the dichotomous data 
for the model-fitting, and an LDE method is specified by
`latent_dist` argument. Two types of ability parameter estimates are illustrated: 
expected *a posteriori* (EAP) and maximum likelihood estimate (MLE). Note that *MLE* 
is also used to abbreviate maximum likelihood estimation. In \pkg{IRTest}, estimation
methods of ability parameters are determined by specifying the argument
`ability_method`, where the default option is EAP. Either a
model-fitting function (e.g., `IRTest::IRTest_Dich()`) or `IRTest::factor_score()` can be
used to estimate ability parameters.

```{r model fitting1, message=FALSE, echo=TRUE}
model_normal <- IRTest::IRTest_Dich(data = simulated_data,
                                    latent_dist = "Normal")
model_KDM <- IRTest::IRTest_Dich(data = simulated_data,
                                 latent_dist = "KDE")
theta_eap_normal <- IRTest::factor_score(model_normal)
theta_mle_normal <- IRTest::factor_score(model_normal, ability_method = "MLE")
theta_eap_KDM <- IRTest::factor_score(model_KDM)
theta_mle_KDM <- IRTest::factor_score(model_KDM, ability_method = "MLE")
```

Among these two models, `model_normal` assumes normality, 
whereas `model_KDM` estimates the latent distribution using KDM during the 
MML-EM procedure. KDM is arbitrarily selected for illustrative purposes.

#### Estimated latent distribution

\pkg{IRTest} provides two ways to draw a density curve of the
estimated latent distribution. The first is to use `plot()`, and the
second is to use `IRTest::latent_distribution()`. The `plot()` can be considered
as a shortcut for using `IRTest::latent_distribution()`. Note that
`IRTest::latent_distribution()` is a PDF, and, thus, can only be applied to LDE
methods that estimate a PDF. For example, since EHM (or LLS) estimates
a *PMF*, a message will be printed without evaluated density values if an
EHM-based (or LLS-based) object is passed to 
`IRTest::latent_distribution()`. The `plot()` can be utilized in all
cases.

The following code can be an example for utilizing `IRTest::latent_distribution()`, 
where `IRTest::dist2()` is a density function 
of the 2NM and `stats::dnorm()` is the built-in function for a normal distribution.

```{r distribution example code, message=FALSE, echo=TRUE}
density_plot <- ggplot2::ggplot() +
  ggplot2::stat_function(fun = IRTest::dist2,
                         args = list(d = 1.414, sd_ratio = 2, prob = 2/3),
                         linewidth = 1,
                         mapping = aes(color = "True")) +
  ggplot2::stat_function(fun = stats::dnorm,
                         linewidth = 1,
                         mapping = aes(color = "Normal")) +
  ggplot2::stat_function(fun = IRTest::latent_distribution,
                         args = list(model_KDM),
                         linewidth = 1,
                         mapping = aes(color = "KDM"))
```

```{r estimated-density-html, echo=FALSE, fig=TRUE, fig.height=3, fig.width=7, fig.align='center', include=knitr::is_html_output(), eval=knitr::is_html_output(), fig.cap="The true, normal, and estimated latent distributions"}
density_plot +
  ggplot2::lims(x = c(-6, 6),
                y = c(0, 0.7)) +
  ggplot2::labs(x=expression(theta),
                y='density') +
  ggplot2::scale_color_manual(values = c("#619CFF", "#F8766D", "#00BA38")) +
  ggplot2::theme_bw()
```

```{r estimated-density-latex, echo=FALSE, fig=TRUE, fig.height=3, fig.width=7, out.width="80%", fig.align='center', include=knitr::is_latex_output(), eval=knitr::is_latex_output(), fig.cap="The true, normal, and estimated latent distributions"}
density_plot +
  ggplot2::lims(x = c(-6, 6),
                y = c(0, 0.7)) +
  ggplot2::labs(x=expression(theta),
                y='density') +
  ggplot2::scale_color_manual(values = c("#619CFF", "#F8766D", "#00BA38")) +
  ggplot2::theme_bw()
```

Figure
`r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(fig:estimated-density-html)', '\\@ref(fig:estimated-density-latex)'))`
is drawn with a slight addition of aesthetic codes to the object
`density_plot`. It shows the density curves of the true latent distribution, the standard normal
distribution, and the estimated latent distribution. Notably, even with the
discrepancies between the true distribution
(`r knitr::asis_output(ifelse(knitr::is_html_output(), '<span style="color: #00BA38;">green line</span>', '\\textcolor[HTML]{00BA38}{green line}'))`)
and the standard normal distribution
(`r knitr::asis_output(ifelse(knitr::is_html_output(), '<span style="color: #F8766D;">red line</span>', '\\textcolor[HTML]{F8766D}{red line}'))`),
the estimated distribution
(`r knitr::asis_output(ifelse(knitr::is_html_output(), '<span style="color: #619CFF;">blue line</span>', '\\textcolor[HTML]{619CFF}{blue line}'))`)
almost recovered the shape of the true latent distribution.

#### Parameter estimates

Differences in parameter estimates caused by the LDE procedure are portrayed in
Figure
`r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(fig:estimated-item-plotly)', '\\@ref(fig:estimated-item-ggplot)'))`,
where the $x$-axis represents the true values (parameters) and the $y$-axis shows the corresponding errors.
It shows that the parameter estimates from KDM
(`r knitr::asis_output(ifelse(knitr::is_html_output(), '<span style="color: #619CFF;">blue dots</span>', '\\textcolor[HTML]{619CFF}{blue dots}'))`)
are generally located closer to the "$\text{Error} = 0$" line than those from the normality
assumption model
(`r knitr::asis_output(ifelse(knitr::is_html_output(), '<span style="color: #F8766D;">red dots</span>', '\\textcolor[HTML]{F8766D}{red dots}'))`),
which indicates that the estimates from KDM are more accurate on average.
RMSE is used as an evaluation criterion to assess
the accuracy of the parameter estimates, where the lower RMSE indicates
higher accuracy. Table
`r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(tab:rmse-interactive)', '\\@ref(tab:rmse-static)'))`
shows that, for all types of parameters, estimates from the KDM model
are more accurate with lower RMSE values than those from the normality
assumption model. Especially, the considerable amounts of decreases in
RMSEs for $\hat{a}$ and $\hat{b}$ substantiates the effectiveness of
LDE in enhancing the estimation accuracy of item
parameters, where the magnitudes in the RMSE reduction are originated
from the highly skewed latent distribution. As stated in 
[Section 2.2](#role-of-latent-distribution), the results show that the appropriate 
specification of the latent distribution can have a positive impact on the accuracy of parameter estimates.

```{r estimated-item-plotly, echo=FALSE, fig=TRUE, fig.height=3.4, fig.width=7, fig.align='center', include=knitr::is_html_output(), eval=knitr::is_html_output(), fig.cap="Differences in the errors of the item parameter estimates caused by LDE"}
true_values <-
  as.data.frame(true_item[,1:2]) %>%
  tidyr::pivot_longer(cols = everything(),
                      names_to = "parameter",
                      values_to = "True values")
estimated_KDM <-
  as.data.frame(model_KDM$par_est[,1:2]) %>%
  tidyr::pivot_longer(cols = everything(),
                      names_to = "parameter",
                      values_to = "Estimated values") %>%
  dplyr::mutate(true_values[,2],
                colour = "KDM")
estimated_normal <-
  as.data.frame(model_normal$par_est[,1:2]) %>%
  tidyr::pivot_longer(cols = everything(),
                      names_to = "parameter",
                      values_to = "Estimated values") %>%
  dplyr::mutate(true_values[,2],
                colour = "Normal")
p <- 
  rbind(estimated_KDM, estimated_normal) %>%
  dplyr::mutate(Errors = `Estimated values`-`True values`) %>% 
  ggplot2::ggplot(aes(x = `True values`,
                      y = `Errors`,
                      color = colour)) +
  ggplot2::geom_point(size = 2,
                      shape = 1) +
  ggplot2::facet_wrap(~ parameter,
                      ncol = 2,
                      scales = "free_x") +
  ggplot2::scale_color_manual(values = c("#619CFF", "#F8766D")) +
  ggplot2::geom_hline(yintercept = 0) +
  ggplot2::lims(y = c(-0.6, 0.6)) +
  ggplot2::theme_bw()
plotly::ggplotly(p)
```

```{r estimated-item-ggplot, echo=FALSE, fig=TRUE, fig.height=3.5, fig.width=7.5, out.width="108%", fig.align='center', include=knitr::is_latex_output(), eval=knitr::is_latex_output(), fig.cap="Differences in the errors of the item parameter estimates caused by LDE"}
true_values <-
  as.data.frame(true_item[,1:2]) %>%
  tidyr::pivot_longer(cols = everything(),
                      names_to = "parameter",
                      values_to = "True values")
estimated_KDM <-
  as.data.frame(model_KDM$par_est[,1:2]) %>%
  tidyr::pivot_longer(cols = everything(),
                      names_to = "parameter",
                      values_to = "Estimated values") %>%
  dplyr::mutate(true_values[,2],
                colour = "KDM")
estimated_normal <-
  as.data.frame(model_normal$par_est[,1:2]) %>%
  tidyr::pivot_longer(cols = everything(),
                      names_to = "parameter",
                      values_to = "Estimated values") %>%
  dplyr::mutate(true_values[,2],
                colour = "Normal")

rbind(estimated_KDM, estimated_normal) %>%
  dplyr::mutate(Errors = `Estimated values`-`True values`) %>% 
  ggplot2::ggplot(aes(x = `True values`,
                      y = `Errors`,
                      color = colour)) +
  ggplot2::geom_point(size = 2,
                      shape = 1) +
  ggplot2::facet_wrap(~ parameter,
                      ncol = 2,
                      scales = "free_x") +
  ggplot2::scale_color_manual(values = c("#619CFF", "#F8766D")) +
  ggplot2::geom_hline(yintercept = 0) +
  ggplot2::lims(y = c(-0.6, 0.6)) +
  ggplot2::theme_bw()
```

```{r rmse-table, echo=FALSE}
a_n <- sqrt(mean((model_normal$par_est[,1] - true_item[,1])^2))
a_k <- sqrt(mean((model_KDM$par_est[,1] - true_item[,1])^2))
b_n <- sqrt(mean((model_normal$par_est[,2] - true_item[,2])^2))
b_k <- sqrt(mean((model_KDM$par_est[,2] - true_item[,2])^2))
eap_n <- sqrt(mean((theta_eap_normal$theta - true_theta)^2))
eap_k <- sqrt(mean((theta_eap_KDM$theta - true_theta)^2))
mle_n <- sqrt(mean((theta_mle_normal$theta[is.finite(theta_mle_normal$theta)] - true_theta[is.finite(theta_mle_normal$theta)])^2))
mle_k <- sqrt(mean((theta_mle_KDM$theta[is.finite(theta_mle_KDM$theta)] - true_theta[is.finite(theta_mle_KDM$theta)])^2))
```

```{r rmse-static, eval = knitr::is_latex_output(), echo=FALSE}
rmse_tab <- 
  matrix(c(a_n,a_k,b_n,b_k,eap_n,eap_k,mle_n,mle_k), ncol = 2, byrow = T) %>%
  data.frame(row.names = c("a", "b", "EAP", "MLE"))
colnames(rmse_tab) <- c("Normal", "KDM")

knitr::kable(rmse_tab, format = "latex", caption = "RMSEs from the normality assumption model and KDM", digits = 3, booktabs = T) %>% 
  kableExtra::kable_styling(font_size = 7, position = "center") %>%
  column_spec(1,width = "1in") %>%
  column_spec(2,width = "1in") %>%
  column_spec(3,width = "1in")
```

```{r rmse-interactive, eval = knitr::is_html_output(), layout = "l-body", echo=FALSE}
rmse_tab <- 
  matrix(c(a_n,a_k,b_n,b_k,eap_n,eap_k,mle_n,mle_k), ncol = 2, byrow = T) %>%
  data.frame(row.names = c("$a$", "$b$", "$\\theta_{EAP}$", "$\\theta_{MLE}$"))
colnames(rmse_tab) <- c("Normal", "KDM")

knitr::kable(rmse_tab, format = "html", caption = "RMSEs from the normality assumption model and KDM", digits = 3, booktabs = T, table.attr = "style='width:40%;'") %>%
 kableExtra::kable_styling(position = "center")
```

## An empirical example {#an-empirical-example}

This section performs an IRT analysis using the GCBS data. Along with the
data analysis, one of the objectives of the section is to illustrate
the usages of the functions of \pkg{IRTest}, such as those for
calculating reliability coefficients and item/test information.

#### Data

The GCBS data contains responses from 15 polytomous items and 2,391
respondents, where each item has five categories scored from one to
five. The data can be loaded in the following manner.

```{r data, echo=TRUE}
data_GCBS <- read.csv("data/data_GCBS.csv")
```

There are `r sum(is.na(data_GCBS))` missing values in the data. 
\pkg{IRTest} uses a full-information maximum likelihood (FIML) approach
in handling missing data.

#### Model selection

DCM and KDM are used in performing LDE for illustrative purposes, 
where the DCM's $h$ is the hyperparameter indicating the complexity of the latent distribution. 
The PCM, GPCM, and graded response model [GRM: @Samejima:1969] are available IRT models 
of `IRTest::IRTest_Poly()`, where the default is the GPCM. Models are fitted as follows:

```{r model fitting, message=FALSE, echo=TRUE}
# Davidian-curve method
DCMs <- list()
for(i in 1:10){
    DCMs[[i]] <- IRTest::IRTest_Poly(data = data_GCBS, latent_dist = "DC", h = i)
}
names(DCMs) <- paste0("DC", 1:10)

# kernel density estimation method
KDM <- IRTest::IRTest_Poly(data = data_GCBS, latent_dist = "KDE")
```

A model-selection step is required for DCM: the "best-DCM" can be
selected by the HQ criterion [@Hannan+Quinn:1979] presented in 
equation \@ref(eq:hq). After ten models are fitted, the best model can
be selected with the function `IRTest::best_model()` (`stats::anova()` can also be
used). The `IRTest::best_model()` function uses a specific criterion 
to determine the best model, with options including `"logLik"`, `"deviance"`, `"AIC"`, 
`"BIC"`, and `"HQ"`. The default is `criterion = "HQ"`.

```{r best DCM, echo=TRUE, results='markup'}
do.call(what = IRTest::best_model, args = DCMs)
```

The result indicates that `DC8` is the best DCM.

The `DC8` and `KDM` can also be compared by the function
`IRTest::best_model()`.

```{r KDM DCM, echo=TRUE, results='markup'}
IRTest::best_model(DCMs$DC8, KDM, criterion = "logLik")
```

The rest of this section looks into the details of `KDM` by following
the model-comparison result.

#### Summary of the model

A brief summary of the model-fitting results can be printed with
`summary()`. It presents the convergence status, model-fit indices, and the
number of parameters and items. Also, it displays a cursory shape of the
estimated latent distribution.

```{r summary, echo=TRUE, results='markup'}
summary(KDM)
```

Alternatively, the shape of the estimated latent distribution can be
easily visualized by the `plot()` function which produces a `ggplot`-class object.
Figure
`r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(fig:realplot-html)', '\\@ref(fig:realplot-latex)'))`
shows the estimated latent distribution of the GCBS data which is
left-skewed. Figure
`r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(fig:realplot-html)', '\\@ref(fig:realplot-latex)'))`
is produced by adding aesthetic codes to `plot()` and the standard normal distribution. 
If a PDF is estimated,
additional arguments in `plot()` are passed to `ggplot2::stat_function()` of
\CRANpkg{ggplot2} [@ggplot2]. Otherwise, if a PMF is estimated (e.g.,
EHM or LLS), they are passed to `ggplot2::geom_line()` of \CRANpkg{ggplot2}.

```{r realplot-html, echo=FALSE, fig=TRUE, fig.height=3, fig.width=7, fig.align='center', include=knitr::is_html_output(), eval=knitr::is_html_output(), fig.cap="Estimated latent distribution for the GCBS data"}
plot(KDM, mapping = aes(color = "Estimated"), linewidth = 1) +
  stat_function(fun = dnorm, linewidth = .8, mapping = aes(color = "Normal"))+
  lims(x = c(-6, 6), y = c(0, 0.6)) +
  scale_color_manual(values = c("#619CFF", "#F8766D")) +
  theme_bw()
```

```{r realplot-latex, echo=FALSE, fig=TRUE, fig.height=3, fig.width=7, out.width="80%", fig.align='center', include=knitr::is_latex_output(), eval=knitr::is_latex_output(), fig.cap="Estimated latent distribution for the GCBS data"}
plot(KDM, mapping = aes(color = "Estimated"), linewidth = 1) +
  stat_function(fun = dnorm, linewidth = .8, mapping = aes(color = "Normal"))+
  lims(x = c(-6, 6), y = c(0, 0.6)) +
  scale_color_manual(values = c("#619CFF", "#F8766D")) +
  theme_bw()
```

#### Parameter estimates

Users can access to item parameter estimates and corresponding standard
errors with `stats::coef()` and `IRTest::coef_se()`, respectively.

```{r coef, echo=TRUE, results='markup'}
stats::coef(KDM)
IRTest::coef_se(KDM)
```

Likewise, `IRTest::factor_score()` returns ability parameter estimates
(`IRTest::factor_score()$theta`) and their standard errors
(`IRTest::factor_score()$theta_se`) which are not printed here for their lengths being `r nrow(data_GCBS)`.

```{r ability example code, echo=TRUE, eval=FALSE, results='markup'}
IRTest::factor_score(KDM, ability_method = "EAP")
```

#### Plotting item response functions

```{r plotitem-latex, echo=FALSE, fig.height=5, fig.width=8.5, out.width="100%", fig.align='center', include=knitr::is_latex_output(), eval=knitr::is_latex_output(), fig.cap="Item response functions of Item 4, 10, 12, and 15"}
plot1 <- IRTest::plot_item(KDM, 4)
plot2 <- IRTest::plot_item(KDM, 10)
plot3 <- IRTest::plot_item(KDM, 12)
plot4 <- IRTest::plot_item(KDM, 15)
grid.arrange(plot1, plot2, plot3, plot4, ncol=2)
```

Figure
`r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(fig:plotitem-html)', '\\@ref(fig:plotitem-latex)'))`
shows item response functions of the four items (Item 4, 10, 12, and 15) by using `IRTest::plot_item()`. 
For example, a plot of the item response function of Item 11 can be drawn with 
`IRTest::plot_item(x = KDM, item.number = 11)`.

```{r plotitem-html, echo=FALSE, fig=TRUE, fig.height=5, fig.width=8, fig.align='center', include=knitr::is_html_output(), eval=knitr::is_html_output(), fig.cap="Item response functions of Item 4, 10, 12, and 15"}
plot1 <- IRTest::plot_item(KDM, 4)
plot2 <- IRTest::plot_item(KDM, 10)
plot3 <- IRTest::plot_item(KDM, 12)
plot4 <- IRTest::plot_item(KDM, 15)
grid.arrange(plot1, plot2, plot3, plot4, ncol=2)
```

#### Reliability coefficient

Among various types of IRT reliability coefficients, \pkg{IRTest}
applies those discussed by @Green:1984 and @May+Nicewander:1994. The
coefficient from @Green:1984 is calculated on the latent variable
$\theta$ scale, and the one from @May+Nicewander:1994 is calculated on
the summed-score scale. @May+Nicewander:1994's approach has an advantage
of providing reliability coefficients for individual items. The
function `IRTest::reliability()` returns the coefficients mentioned
above: item reliability coefficients, and test reliability coefficients
on the $\theta$ scale and the summed-score scale.

```{r reliability, echo=TRUE, results='markup'}
IRTest::reliability(KDM)
```

#### Item and test information functions

In \pkg{IRTest}, `IRTest::inform_f_item()` and `IRTest::inform_f_test()` evaluate
item and test information, respectively. Figure
`r knitr::asis_output(ifelse(knitr::is_html_output(), '\\@ref(fig:inform-html)', '\\@ref(fig:inform-latex)'))`
visually illustrates how each item contributes to the test information
and how item information functions are added up all together to form the test
information function. The test information function is drawn with a black
line and the item information functions are colored.

```{r inform-html, echo=FALSE, fig=TRUE, fig.height=5, fig.width=7, fig.align='center', include=knitr::is_html_output(), eval=knitr::is_html_output(), fig.cap="Item and test information functions"}
xx <- seq(-6, 6, .1)
iteminfo <- data.frame(theta=xx,
                       Item01 = IRTest::inform_f_item(xx, KDM, 1),
                       Item02 = IRTest::inform_f_item(xx, KDM, 2),
                       Item03 = IRTest::inform_f_item(xx, KDM, 3),
                       Item04 = IRTest::inform_f_item(xx, KDM, 4),
                       Item05 = IRTest::inform_f_item(xx, KDM, 5),
                       Item06 = IRTest::inform_f_item(xx, KDM, 6),
                       Item07 = IRTest::inform_f_item(xx, KDM, 7),
                       Item08 = IRTest::inform_f_item(xx, KDM, 8),
                       Item09 = IRTest::inform_f_item(xx, KDM, 9),
                       Item10 = IRTest::inform_f_item(xx, KDM, 10),
                       Item11 = IRTest::inform_f_item(xx, KDM, 11),
                       Item12 = IRTest::inform_f_item(xx, KDM, 12),
                       Item13 = IRTest::inform_f_item(xx, KDM, 13),
                       Item14 = IRTest::inform_f_item(xx, KDM, 14),
                       Item15 = IRTest::inform_f_item(xx, KDM, 15)
                       )
iteminfo <- pivot_longer(iteminfo, 
                         cols = starts_with("Item"), 
                         names_to = "color", 
                         values_to = "information")
ppp <- 
  ggplot(iteminfo, 
         aes(x=theta, y=information, color=color))+
  geom_line(linewidth=.3) +
  geom_line(data = data.frame(theta=xx, information=inform_f_test(xx, KDM)),
            aes(x=theta, y=information),
            color="black",
            linewidth=.2) +
  lims(x = c(-6, 6))+
  theme_bw()
ggplotly(ppp)
```

```{r inform-latex, echo=FALSE, fig=TRUE, fig.height=4.4, fig.width=7.5, out.width="110%", fig.align='center', include=knitr::is_latex_output(), eval=knitr::is_latex_output(), fig.cap="Item and test information functions"}
ggplot() +
  stat_function(
    fun = IRTest::inform_f_test,
    args = list(KDM)
  ) +
  stat_function(
    fun = IRTest::inform_f_item,
    args = list(KDM, 1),
    mapping = aes(color = "Item 1")
  ) +
  stat_function(
    fun = IRTest::inform_f_item,
    args = list(KDM, 2),
    mapping = aes(color = "Item 2")
  ) +
  stat_function(
    fun = IRTest::inform_f_item,
    args = list(KDM, 3),
    mapping = aes(color = "Item 3")
  ) +
  stat_function(
    fun = IRTest::inform_f_item,
    args = list(KDM, 4),
    mapping = aes(color = "Item 4")
  ) +
  stat_function(
    fun = IRTest::inform_f_item,
    args = list(KDM, 5),
    mapping = aes(color = "Item 5")
  ) +
  stat_function(
    fun = IRTest::inform_f_item,
    args = list(KDM, 6),
    mapping = aes(color = "Item 6")
  ) +
  stat_function(
    fun = IRTest::inform_f_item,
    args = list(KDM, 7),
    mapping = aes(color = "Item 7")
  ) +
  stat_function(
    fun = IRTest::inform_f_item,
    args = list(KDM, 8),
    mapping = aes(color = "Item 8")
  ) +
  stat_function(
    fun = IRTest::inform_f_item,
    args = list(KDM, 9),
    mapping = aes(color = "Item 9")
  ) +
  stat_function(
    fun = IRTest::inform_f_item,
    args = list(KDM, 10),
    mapping = aes(color = "Item10")
  ) +
  stat_function(
    fun = IRTest::inform_f_item,
    args = list(KDM, 11),
    mapping = aes(color = "Item11")
  ) +
  stat_function(
    fun = IRTest::inform_f_item,
    args = list(KDM, 12),
    mapping = aes(color = "Item12")
  ) +
  stat_function(
    fun = IRTest::inform_f_item,
    args = list(KDM, 13),
    mapping = aes(color = "Item13")
  ) +
  stat_function(
    fun = IRTest::inform_f_item,
    args = list(KDM, 14),
    mapping = aes(color = "Item14")
  ) +
  stat_function(
    fun = IRTest::inform_f_item,
    args = list(KDM, 15),
    mapping = aes(color = "Item15")
  ) +
  lims(x = c(-6, 6))+
  labs(x = expression(theta), y = "information")+
  theme_bw()
```

<!-- %%%%%%%%%%%%%%% -->

<!-- %% Section 6 %% -->

<!-- %%%%%%%%%%%%%%% -->

# Discussion {#discussion}

While some may not find the magnitude of biases resulting from the deviations
from the normality assumption particularly significant in the context of
IRT, LDE remains an appealing option. Within IRT, increasing the number of 
items and respondents may require additional time and resources to improve parameter estimation accuracy.
Meanwhile, an appropriate usage of LDE would increase the estimation
accuracy with the cost of adding a few distribution parameters. Therefore,
unless the addition of distribution parameters compromises the model-data fit significantly, 
LDE can be a reasonable strategy for reducing estimation biases.

Although the current literature on LDE in IRT lacks 
clear guidelines on how and when to implement the LDE procedure, practitioners 
may choose to start with either KDM or DCM, even when there is limited 
or no information available about the shape of the latent distribution. 
Both approaches not only perform well in estimating non-normal latent 
distributions, but also effectively recover normal distributions when the 
normality assumption holds [@Li:2022; @Woods+Lin:2009].

The \pkg{IRTest} package offers the choice of LDE methods for
conducting IRT analyses, and its functions utilize the estimated latent
distribution for the subsequent analyses. \pkg{IRTest} is actively being
updated, and some potential enhancements can be made for a better
application of the package, which may include expanding the range of
available IRT models, decreasing computation time, and imposing
constraints on parameters. User feedback would also guide a way for the
package maintenance and enhancement. Yet, there is much more to explore 
in the field of LDE beyond what has already been studied. 
Thus, further research on LDE is expected to provide valuable guidance to
both package users and the developer.

# Acknowledgments {.unnumbered}

Special thanks to Nagap Park, Hyesung Shin, Ryan Lerch, and anonymous reviewers for 
their insightful suggestions that improved the article.

# Additional features {.unnumbered}

#### Analysis of continuous response data {.unnumbered}

Continuous item responses, ranging from 0 to 1, are often encountered 
in test datasets. Such item responses can also be handled by the 
\pkg{IRTest} package with `IRTest::IRTest_Cont()`. 
All features of \pkg{IRTest} presented in this paper are 
applicable to continuous item responses. An example code is shown below:

```{r cont-IRT, eval=FALSE, echo=TRUE}
# Generating a continuous item response data
data <- IRTest::DataGeneration(N = 1000, nitem_C = 10)$data_C

# Analysis
model_continuous <- IRTest::IRTest_Cont(data)
```


#### Analysis of mixed-format data {.unnumbered}

An IRT analysis of mixed-format data can also be conducted with 
`IRTest::IRTest_Mix()`. The difference between `IRTest::IRTest_Mix()` and 
`IRTest::IRTest_Dich()` (or `IRTest::IRTest_Poly()`) is that 
`IRTest::IRTest_Mix()` requires two separate data: one for dichotomous items 
and the other for polytomous items. An example code is shown below:

```{r mixed format, eval=FALSE, echo=TRUE}
model_mixed.format <- IRTest::IRTest_Mix(data_D = dichotomous_data,
                                         data_P = polytomous_data,
                                         model_D = rep(c("2PL", "3PL"), each = 5),
                                         model_P = "GPCM")
```

In this case, the 2PLM is applied to the first five dichotomous items,
the 3PLM is applied to the rest of the dichotomous items, and the GPCM
is applied to the polytomous items. The rest are the same with
`IRTest::IRTest_Dich()` and `IRTest::IRTest_Poly()`.

The `IRTest::IRTest_Dich()` can also take a vector for the argument `model` to
apply multiple IRT models to different types of items.



#### Item-fit statistic {.unnumbered}

An item-fit statistic can be calculated with `IRTest::item_fit()`. Currently,
@Bock:1960's $\chi^{2}$ and @Yen:1981's $Q_{1}$ are available.
