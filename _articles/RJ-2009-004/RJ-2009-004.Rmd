---
title: Easier parallel computing in R with snowfall and sfCluster
abstract: The 'Easier parallel computing in R with snowfall and sfCluster' article
  from the 2009-1 issue.
author:
- name: Jochen Knaus, Christine Porzelius, Harald Binder and Guido Schwarzer
  affiliation: Department of Medical Biometry and Statistics, University of Freiburg
  address: |
    Germany
date: '2009-06-01'
date_received: ~
journal:
  firstpage: '54'
  lastpage: '59'
volume: 1
issue: 1
slug: RJ-2009-004
packages:
  cran: []
  bioc: []
preview: preview.png
bibliography: joRNews_ResponseInserted.bib
CTV: ~
output:
  rjtools::rjournal_web_article:
    self_contained: yes
    toc: no
    legacy_pdf: yes
    web_only: yes
    mathjax: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js
    md_extension: -tex_math_single_backslash

---

::: article
Many statistical analysis tasks in areas such as bioinformatics are
computationally very intensive, while lots of them rely on
embarrassingly parallel computations [@GRAMA_ETAL]. Multiple computers
or even multiple processor cores on standard desktop computers, which
are widespread nowadays, can easily contribute to faster analyses.

R itself does not allow parallel execution. There are some existing
solutions for R to distribute calculations over many computers --- a
cluster --- for example *Rmpi*, *rpvm*, *snow*, *nws* or *papply*.
However these solutions require the user to setup and manage the cluster
on his own and therefore deeper knowledge about cluster computing itself
is needed. \>From our experience this is a barrier for lots of R users,
who basically use it as a tool for statistical computing.

Parallel computing has several pitfalls. A single program can easily
affect a complete computing infrastructure on maloperation such as
allocating too many CPUs or RAM leaving no resources for other users and
their processes, or degrading the performance of one or more individual
machines. Another problem is the difficulty of keeping track of what is
going on in the cluster, which is sometimes hard if the program fails
for some reason on a slave.

We developed the management tool sfCluster and the corresponding R
package *snowfall*, which are designed to make parallel programming
easier and more flexible. sfCluster completely hides the setup and
handling of clusters from the user and monitors the execution of all
parallel programs for problems affecting machines and the cluster.
Together with *snowfall* it allows the use of parallel computing in R
without further knowledge of cluster implementation and configuration.

# snowfall

The R package *snowfall* is built as an extended abstraction layer above
the well established *snow* package by L. Tierney, A. J. Rossini, N. Li
and H. Sevcikova [@ROSS_07]. Note this is not a technical layer, but an
enhancement in useablity. *snowfall* can use all networking types
implemented in *snow*, which are socket, MPI, PVM and NetWorkSpaces.

snowfall is also usable on its own (without sfCluster), which makes it
handy on single multicore machines, for development or for distribution
inside packages.

The design goal was to make parallel computing accessible to R
programmers without further general computing knowledge. The Application
Programming Interface (API) of *snowfall* is similar to *snow* and
indeed *snow* functions can be called directly. So porting of existing
*snow* programs is very simple.

The main *snowfall* features are as follows:

-   All cluster functions and *snow* wrappers include extended error
    handling with stopping on error, which makes it easier to find
    improper behaviour for users without deeper knowledge of clusters.

-   In addition to *snow* functions, functions for common tasks are
    provided (like loading packages and sources in the cluster,
    exchanging variables \[between functions?\] and saving intermediate
    results).

-   Cluster settings can be controlled with command line arguments. The
    user does not have to change his R script to switch between
    sequential or parallel execution, or to change the number of cores
    or type of clusters used.

-   Connector to sfCluster: Used with sfCluster, configuration does not
    have to be done on initialisation as all values are taken from the
    user-given sfCluster settings.

-   All functions work in sequential execution, too, i.e. without a
    cluster. This is useful for development and distribution of packages
    using *snowfall*. Switching between sequential and parallel
    execution does not require code changes inside the parallel program
    (and can be changed on initialisation).

Like *snow*, *snowfall* basically uses list functions for
parallelisation. Calculations are distributed on slaves for different
list elements. This is directly applicable to any data parallel problem,
for example bootstrapping or cross-validation can be represented with
this approach (other types of parallelisation can also be used, but
probably with more effort).

Generally, a cluster constitutes single machines, called nodes, which
are chosen out of a set of all machines usable as nodes, called the
universe. The calculation is started on a master node, which spawns
worker R processes (sometimes also called slaves). A CPU is a single
calculation unit of which modern computers may have more than one.

<figure id="figure:bootstrap">
<div class="center">
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(snowfall)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Initialisation of snowfall.</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># (if used with sfCluster, just call sfInit())</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">sfInit</span>(<span class="at">parallel=</span><span class="cn">TRUE</span>, <span class="at">cpus=</span><span class="dv">4</span>, <span class="at">type=</span><span class="st">&quot;SOCK&quot;</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Loading data.</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(mvna)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(sir.adm)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Wrapper, which can be parallelised.</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>wrapper <span class="ot">&lt;-</span> <span class="cf">function</span>(idx) {</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Output progress in worker logfile</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cat</span>( <span class="st">&quot;Current index: &quot;</span>, idx, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span> )</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>  index <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(sir.adm), <span class="at">replace=</span><span class="cn">TRUE</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  temp <span class="ot">&lt;-</span> sir.adm[index, ]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">crr</span>(temp<span class="sc">$</span>time, temp<span class="sc">$</span>status, temp<span class="sc">$</span>pneu)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(fit<span class="sc">$</span>coef)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Exporting needed data and loading required</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">#    packages on workers.</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="fu">sfExport</span>(<span class="st">&quot;sir.adm&quot;</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="fu">sfLibrary</span>(cmprsk)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Start network random number generator</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co">#    (as &quot;sample&quot; is using random numbers).</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="fu">sfClusterSetupRNG</span>()</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. Distribute calculation</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>result <span class="ot">&lt;-</span> <span class="fu">sfLapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>, wrapper)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Result is always in list form.</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">unlist</span>(result))</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co"># 7. Stop snowfall</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="fu">sfStop</span>()</span></code></pre></div>
</div>
<figcaption>Figure 1: <span id="figure:bootstrap"
label="figure:bootstrap"></span> Example bootstrap using
snowfall.</figcaption>
</figure>

For most basic programs, the parallelisation follows the workflow of the
example in Figure [1](#figure:bootstrap).

1.  Initialisation. Call `sfInit` with parameters if not using sfCluster
    or without parameters if used with sfCluster. Parameters are used to
    switch between parallel or sequential execution (argument
    `parallel`, default `FALSE`) and the number of CPUs wanted (argument
    `cpus` with numerical value, in sequential mode always `1`, in
    parallel mode the default is `2`). Used with sfCluster, these
    parameters are taken from the sfCluster settings.

2.  Load the data and prepare the data needed in the parallel
    calculations (for example generating data for a simulation study).

3.  Wrap parallel code into a wrapper function (function `wrapper` in
    the example), callable by an R list function.

4.  Export objects needed in the parallel calculation (e.g., `sir.adm`)
    to cluster nodes. It is also necessary to load required packages on
    all workers. Exporting objects can reduce the total amount of data
    transmitted. If there are only a few objects needed in the parallel
    function, you can export them implicitly using additional arguments
    in the wrapper function and specifying them in the parallel call
    (e.g. `sfLapply`).

5.  Optional step: start a network random number generator. Such random
    number generators ensure that nodes produce independent sequences of
    random numbers. The sequences, and hence results relying on random
    numbers, are reproducible provided that the same number of workers
    process the same sequence of tasks.

6.  Distribute the calculation to the cluster by using a parallel list
    function (`sfLapply` in the example). This function distributes
    calls of `wrapper` to workers (which usually means index $1$ to
    index $n$ is called on CPU $1$ to CPU $n$ respectively. These calls
    are then executed in parallel. If the list is longer than the amount
    of CPUs, index $n+1$ is scheduled on CPU 1 again [^1]). That also
    means all used data inside the wrapper function must be exported
    first, to have them existing on any node (see point 2).

7.  Stop cluster via `sfStop()` (If used with sfCluster this is not
    stopping the cluster itself, but allows reinitialisation with
    `sfInit`).

Probably the most irritating thing in the example is the export of the
data frame. On the provided type of cluster computing, the source
program runs only on the master first of all. Local variables and
objects remain on the master, so workers do not automatically have
access to them. All data, functions and packages needed for the parallel
calculation have to be transfered to the workers' processes first.
Export means objects are instantiated on the slaves. Unlike *snow*'s
export function, local variables can be exported, too. As a further
addition, all variables can be exported or removed from the worker
processes.[^2]

<figure id="figure:commandline">
<div class="center">
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Start a socket cluster on local machine using 3 processors</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>R CMD BATCH myParPrg.R <span class="sc">--</span>args <span class="sc">--</span>parallel <span class="sc">--</span>cpus<span class="ot">=</span><span class="dv">3</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Start a socket cluster with 5 cores (3 on localhost, 2 on machine &quot;other&quot;)</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>R <span class="sc">--</span>args <span class="sc">--</span>parallel <span class="sc">--</span>hosts<span class="ot">=</span>localhost<span class="sc">:</span><span class="dv">3</span>,other<span class="sc">:</span><span class="dv">2</span> <span class="sc">&lt;</span> myParPrg.R</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Start using MPI with 5 cores on R interactive shell.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>R <span class="sc">--</span>args <span class="sc">--</span>parallel <span class="sc">--</span>type<span class="ot">=</span>MPI <span class="sc">--</span>cpus<span class="ot">=</span><span class="dv">5</span></span></code></pre></div>
</div>
<figcaption>Figure 2: <span id="figure:commandline"
label="figure:commandline"></span> Examples for snowfall configuration
using the command line.</figcaption>
</figure>

Basic networking parameters (like execution mode and the number of CPUs
on each machine) can be set on the command line, as seen in Figure
[2](#figure:commandline). Arguments provided in `sfInit()` will
over-ride the command line arguments. For example, a script that always
uses MPI clusters might include `sfInit(type="MPI")`. This mechanism can
be used in R scripts using *snowfall*, as a connector to sfCluster, or
as a binding to other workload and management tools.

In all current parallel computing solutions intermediate results are
lost if the cluster dies, perhaps due to a shutdown or crash of one of
the used machines. *snowfall* offers a function which saves all
available parts of results to disc and reloads them on a restored run.
Indeed it does not save each finished part, but any number of CPUs parts
(for example: Working on a list with 100 segments on a 5 CPU cluster, 20
result steps are saved). This function cannot prevent a loss of results,
but can greatly save time on long running clusters. As a side effect
this can also be used to realise a kind of "dynamic" resource
allocation: just stop and restart with restoring results on a
differently sized cluster.

Note that the state of the RNG is not saved in the current version of
*snowfall*. Users wishing to use random numbers need to implement
customized save and restore functions, or use pre-calculated random
numbers.

# sfCluster

sfCluster is a Unix commandline tool which is built to handle cluster
setup, monitoring and shutdown automatically and therefore hides these
tasks from the user. This is done as safely as possible enabling cluster
computing even for inexperienced users. Using *snowfall* as the R
frontend, users can change resource settings without changing their R
program. sfCluster is written in Perl, using only Open Source tools.

![Figure 3: Workflow of sfCluster.](Workflow.png){#figure:workflow
width="100%" alt="graphic without alt text"}

On the backend, sfCluster is currently built upon MPI, using the LAM
implementation [@burns94:_lam], which is available on most common Unix
distributions.

Basically, a cluster is defined by two resources: CPU and memory. The
monitoring of memory usage is very important, as a machine is
practically unusable for high performance purposes if it is running out
of physical memory and starts to swap memory on the hard disk. sfCluster
is able to probe memory usage of a program automatically (by running it
in sequential mode for a certain time) or to set the upper bound to a
user-given value.

The resource allocation can be widely configured: Even partial usage of
a specific machine is possible (for example on a machine with 4 CPUs,
which is used for other purposes as well, it is possible to allow only
e.g. 2 cores for usage in clusters and sfCluster ensures that no more
than 2 CPUs are used by parallel computing on this machine). The
restriction of usage can leave calculation resources for other tasks,
e.g. if a computer is used for other calculations or perhaps a computing
pool for students is used for cluster programs, leaving enough CPU power
for the users of those machines.

sfCluster checks the cluster universe to find machines with free
resources to start the program (with the desired amount of resources ---
or less, if the requested resources are not available). These machines,
or better, parts of these machines, are built into a new cluster, which
belongs to the new program. This is done via LAM sessions, so each
program has its own independent LAM cluster.

Optionally sfCluster can keep track of resource usage during runtime and
can stop cluster programs if they exceed their given memory usage,
machines start to swap, or similar events. On any run, all spawned R
worker processes are detected and on shut down it is ensured that all of
them are killed, even if LAM itself is not closed/shut down correctly.

The complete workflow of sfCluster is shown in Figure
[3](#figure:workflow).

An additional mechanism for resource administration is offered through
user groups, which divide the cluster universe into "subuniverses". This
can be used to preserve specific machines for specific users. For
example, users can be divided in two groups, one able to use the whole
universe, the other only specific machines or a subset of the universe.
This feature was introduced, because we interconnect the machine pools
from two institutes to one universe, where some scientists can use all
machines, and some only the machines from their institute.

![Figure 4: Monitoring mode (9 slaves on five machines and
master).](Monitoring.png){#figure:monitoring width="100%" alt="graphic without alt text"}

sfCluster features three main parallel execution modes. All of these
setup and start a cluster for the program as described, run R and once
the program has finished, shutdown the cluster. The batch- and
monitoring mode shuts down the cluster on interruption from the user
(using keys on most Unix systems).

1.  Parallel batchmode: Parallel counterpart to `R CMD BATCH`. Called
    using `sfCluster -b` or by the optionally installed `R CMD par`.

2.  Interactive R shell (`sfCluster -i` or `R CMD parint`).

3.  Monitoring mode (`sfCluster -m` or `R CMD parmon`): Behaves
    similarly to batchmode, but features a process monitor for workers,
    access to worker logfiles, system output and debugging and runtime
    messages. The visualisation in the terminal is done using Ncurses.
    For an example output see Figure [4](#figure:monitoring): A cluster
    with 9 worker processes (marked `SL`) and one master process (marked
    `MA`) are running on five machines. Each machine has a tab with
    logfiles marked by the name of the node, e.g. `knecht4`. For each
    process, it's process identification number (PID), the node it is
    running on, its memory usage and state (like running, sleeping, dead
    etc.) is shown at the top. The `System` tab contains system messages
    from sfCluster; the R output on the master is shown in tab
    `R-Master`.

Besides the parallel execution modes, the sequential mode can be chosen,
which works in all cases, even without an installed or running cluster
environment, i.e. also on Windows systems or as part of a package build
on *snowfall*. On execution in sequential mode, *snowfall* is forced to
run in non-parallel mode.

Besides choosing the amount of required CPUs, users can specify several
options on starting sfCluster. These contain for example the R version,
the nice level of the slaves, activating the sending of emails after
finish or failure and many more.

<figure id="figure:showall">
<div class="center">
<div class="small">
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>jo<span class="sc">@</span>biom9<span class="sc">:</span><span class="er">~$</span> sfCluster <span class="sc">-</span>o</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>SESSION          <span class="sc">|</span> STATE <span class="sc">|</span>  M <span class="sc">|</span> MASTER       <span class="co">#N   RUNTIME R-FILE / R-OUT</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="sc">-----------------+-------+----+---------------------------------------------------</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>LrtpdV7T_R<span class="dv">-2</span>.<span class="fl">8.1</span> <span class="sc">|</span> run   <span class="sc">|</span> MO <span class="sc">|</span> biom9.imbi    <span class="dv">9</span>   <span class="dv">2</span><span class="sc">:</span><span class="dv">46</span><span class="sc">:</span><span class="dv">51</span> coxBst081223.R <span class="sc">/</span> coxBst081223.Rout</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>baYwQ0GB_R<span class="dv">-2</span>.<span class="fl">5.1</span> <span class="sc">|</span> run   <span class="sc">|</span> IN <span class="sc">|</span> biom9.imbi    <span class="dv">2</span>   <span class="dv">0</span><span class="sc">:</span><span class="dv">00</span><span class="sc">:</span><span class="dv">18</span> <span class="sc">-</span>undef<span class="sc">-</span> <span class="er">/</span> <span class="sc">-</span>undef<span class="sc">-</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>jo<span class="sc">@</span>biom9<span class="sc">:</span><span class="er">~$</span> sfCluster <span class="sc">-</span>o <span class="sc">--</span>all</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>SESSION          <span class="sc">|</span> STATE <span class="sc">|</span> USR <span class="sc">|</span>  M <span class="sc">|</span> MASTER       <span class="co">#N   RUNTIME R-FILE / R-OUT</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="sc">-----------------+-------+-----+----+---------------------------------------------</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>LrtpdV7T_R<span class="dv">-2</span>.<span class="fl">8.1</span> <span class="sc">|</span> run   <span class="sc">|</span> jo  <span class="sc">|</span> MO <span class="sc">|</span> biom9.imbi    <span class="dv">9</span>   <span class="dv">3</span><span class="sc">:</span><span class="dv">16</span><span class="sc">:</span><span class="dv">09</span> coxBst081223.R <span class="sc">/</span> coxBst081223.Rout</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>jlXUhxtP_R<span class="dv">-2</span>.<span class="fl">5.1</span> <span class="sc">|</span> run   <span class="sc">|</span> jo  <span class="sc">|</span> IN <span class="sc">|</span> biom9.imbi    <span class="dv">2</span>   <span class="dv">0</span><span class="sc">:</span><span class="dv">00</span><span class="sc">:</span><span class="dv">22</span> <span class="sc">-</span>undef<span class="sc">-</span> <span class="er">/</span> <span class="sc">-</span>undef<span class="sc">-</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>bSpNLNhd_R<span class="dv">-2</span>.<span class="fl">7.2</span> <span class="sc">|</span> run   <span class="sc">|</span> cp  <span class="sc">|</span> BA <span class="sc">|</span> biom9.imbi    <span class="dv">8</span>   <span class="dv">0</span><span class="sc">:</span><span class="dv">32</span><span class="sc">:</span><span class="dv">57</span> getPoints11.R <span class="sc">/</span> getPoints11.Rout</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>NPS5QHkK_R<span class="dv">-2</span>.<span class="fl">7.2</span> <span class="sc">|</span> run   <span class="sc">|</span> cp  <span class="sc">|</span> MO <span class="sc">|</span> biom9.imbi   <span class="dv">10</span>   <span class="dv">3</span><span class="sc">:</span><span class="dv">50</span><span class="sc">:</span><span class="dv">42</span> box2.R <span class="sc">/</span> box2.Rout</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>jo<span class="sc">@</span>biom9<span class="sc">:</span><span class="er">~$</span> sfCluster <span class="sc">--</span>universe <span class="sc">--</span>mem<span class="ot">=</span><span class="dv">1</span>G</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>Assumed memuse<span class="sc">:</span> <span class="dv">1024</span><span class="fu">M</span> (use <span class="st">&#39;--mem&#39;</span> to change).</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>Node                           <span class="sc">|</span> Max<span class="sc">-</span>Load <span class="sc">|</span> CPUs <span class="sc">|</span> RAM    <span class="sc">|</span> Free<span class="sc">-</span>Load <span class="sc">|</span> Free<span class="sc">-</span>RAM <span class="sc">|</span> FREE<span class="sc">-</span>TOTAL</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="sc">-------------------------------+----------+------+--------+-----------+----------+------------</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>biom8.imbi.uni<span class="sc">-</span>freiburg.de     <span class="sc">|</span>        <span class="dv">8</span> <span class="sc">|</span>    <span class="dv">8</span> <span class="sc">|</span>  <span class="fl">15.9</span>G <span class="sc">|</span>     <span class="dv">0</span>     <span class="sc">|</span>     <span class="fl">9.3</span>G <span class="sc">|</span>     <span class="dv">0</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>biom9.imbi.uni<span class="sc">-</span>freiburg.de     <span class="sc">|</span>        <span class="dv">8</span> <span class="sc">|</span>    <span class="dv">8</span> <span class="sc">|</span>  <span class="fl">15.9</span>G <span class="sc">|</span>     <span class="dv">0</span>     <span class="sc">|</span>    <span class="fl">12.6</span>G <span class="sc">|</span>     <span class="dv">0</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>biom10.imbi.uni<span class="sc">-</span>freiburg.de    <span class="sc">|</span>        <span class="dv">8</span> <span class="sc">|</span>    <span class="dv">8</span> <span class="sc">|</span>  <span class="fl">15.9</span>G <span class="sc">|</span>     <span class="dv">0</span>     <span class="sc">|</span>    <span class="fl">14.0</span>G <span class="sc">|</span>     <span class="dv">0</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>biom12.imbi.uni<span class="sc">-</span>freiburg.de    <span class="sc">|</span>        <span class="dv">2</span> <span class="sc">|</span>    <span class="dv">4</span> <span class="sc">|</span>   <span class="fl">7.9</span>G <span class="sc">|</span>     <span class="dv">0</span>     <span class="sc">|</span>     <span class="fl">5.8</span>G <span class="sc">|</span>     <span class="dv">0</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>knecht5.fdm.uni<span class="sc">-</span>freiburg.de    <span class="sc">|</span>        <span class="dv">8</span> <span class="sc">|</span>    <span class="dv">8</span> <span class="sc">|</span>  <span class="fl">15.7</span>G <span class="sc">|</span>     <span class="dv">1</span>     <span class="sc">|</span>     <span class="fl">1.2</span>G <span class="sc">|</span>     <span class="dv">1</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>knecht4.fdm.uni<span class="sc">-</span>freiburg.de    <span class="sc">|</span>        <span class="dv">8</span> <span class="sc">|</span>    <span class="dv">8</span> <span class="sc">|</span>  <span class="fl">15.7</span>G <span class="sc">|</span>     <span class="dv">1</span>     <span class="sc">|</span>     <span class="fl">4.3</span>G <span class="sc">|</span>     <span class="dv">1</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>knecht3.fdm.uni<span class="sc">-</span>freiburg.de    <span class="sc">|</span>        <span class="dv">5</span> <span class="sc">|</span>    <span class="dv">8</span> <span class="sc">|</span>  <span class="fl">15.7</span>G <span class="sc">|</span>     <span class="dv">3</span>     <span class="sc">|</span>    <span class="fl">11.1</span>G <span class="sc">|</span>     <span class="dv">3</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>biom6.imbi.uni<span class="sc">-</span>freiburg.de     <span class="sc">|</span> no<span class="sc">-</span>sched <span class="sc">|</span>    <span class="dv">4</span> <span class="sc">|</span>   <span class="fl">7.9</span>G <span class="sc">|</span>     <span class="sc">-</span>     <span class="er">|</span>        <span class="sc">-</span> <span class="er">|</span>     <span class="sc">-</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>biom7.imbi.uni<span class="sc">-</span>freiburg.de     <span class="sc">|</span>        <span class="dv">2</span> <span class="sc">|</span>    <span class="dv">4</span> <span class="sc">|</span>   <span class="fl">7.9</span>G <span class="sc">|</span>     <span class="dv">1</span>     <span class="sc">|</span>     <span class="fl">2.1</span>G <span class="sc">|</span>     <span class="dv">1</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>Potential usable CPUs<span class="sc">:</span> <span class="dv">6</span></span></code></pre></div>
</div>
</div>
<figcaption>Figure 5: Overview of running clusters from the user (first
call) and from all users (second call). The session column states a
unique identifier as well as the used R version. “state” describes
whether the cluster is currently running or dead. “USR” contains the
user who started the cluster. Column “M” includes the sfCluster running
mode: <code>MO</code>/<code>BA</code>/<code>IN</code> for monitoring,
batch and interative. Column “#N” contains the amount of CPUs used in
this cluster. The third call gives an overview of the current usage of
the whole universe, with free calculation time (“Free-Load”) and useable
memory (“Free-RAM”).</figcaption>
</figure>

# Administration

sfCluster includes features to get information about current running
clusters and free resources on the cluster universe (see Figure
[5](#figure:showall) for an example). This can help users to get an
overview of what is currently happening on the cluster machines and of
the resources available for starting their own programs. Also detailed
information about clusters, with all process PIDs, memory use and
runtime of single processes can be printed, which is useful for
administrators to determine which R process belongs to which cluster.

All these administration options are directly usable by (administrative)
root account, as well, so administrators can safely kill clusters
without wiping out all the slave processes manually.

The configuration of sfCluster itself is done via common Unix-style
configuration files. Configuration includes system variables (like
definition of triggers for stopping events on observation), the user
groups, R versions and of course the cluster universe with resource
limits.

The installation is available through tar.gz or as a Debian package;
both are instantly usable for a single (probably multicore) machine and
only need to be configured for "real" clusters. In normal cases the
default installation should work out of the box. It needs some minor
tweaking if R is not installed in the default way, e.g. if multiple R
installations are available on the machines. All needed CRAN and CPAN
packages are also installable through the installer.

Future additions will be a port to OpenMPI, integration to common batch
and resource management systems (e.g. the Sun Grid Engine or slurp) and
basic reports about cluster behaviour.

# Summary

Although many well-working solutions for parallel computing in R are
available, they have their downsides on forcing the user to manage the
underlying clusters manually. sfCluster/*snowfall* solves this problem
in a very flexible and comfortable way, enabling even inexperienced
computer users to benefit from parallel programming (without having to
learn cluster management, usage and falling into pitfalls affecting
other people's processes). *snowfall* makes sure the resulting R
programs are runable everywhere, even without a cluster.

The combination *snowfall*/sfCluster has been used daily in our
institute for several months and has evolved with user's demands and
wishes. There are packages that have been built integrating *snowfall*
with optionally usable parallelisation techniques (e.g. the package
*peperr*).\
The software and further information are available at
<http://www.imbi.uni-freiburg.de/parallel>.\

# Acknowledgments

This research was supported by the Deutsche Forschungsgemeinschaft
(German Research Foundation) with FOR 534.

Thanks to Arthur Allignol for his little bootstrapping example.

\
:::

[^1]: If calls to the wrapper function take different times, as with
    search problems, or you have computers with different speeds, most
    likely you will want to use a load balanced version, like
    sfClusterApplyLB, which dynamically re-schedules calls of `wrapper`
    to CPUs which have finished their previous job.

[^2]: There are more elaborate ways to integrate data transfer, e.g.
    NetWorkSpaces, but from our experience, *snowfall*'s exporting
    functions are enough for most common needs.
