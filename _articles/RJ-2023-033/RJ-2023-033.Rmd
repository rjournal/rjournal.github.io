---
title: Robust Functional Linear Regression Models
abstract: With advancements in technology and data storage, the availability of functional
  data whose sample observations are recorded over a continuum, such as time, wavelength,
  space grids, and depth, progressively increases in almost all scientific branches.
  The functional linear regression models, including scalar-on-function and function-on-function,
  have become popular tools for exploring the functional relationships between the
  scalar response-functional predictors and functional response-functional predictors,
  respectively. However, most existing estimation strategies are based on non-robust
  estimators that are seriously hindered by outlying observations, which are common
  in applied research. In the case of outliers, the non-robust methods lead to undesirable
  estimation and prediction results. Using a readily-available [R]{.sans-serif} package
  robflreg, this paper presents several robust methods build upon the functional principal
  component analysis for modeling and predicting scalar-on-function and function-on-function
  regression models in the presence of outliers. The methods are demonstrated via
  simulated and empirical datasets.
author:
- name: Ufuk Beyaztas
  affiliation: Marmara University
  email: |
    ufuk.beyaztas@marmara.edu.tr
  address:
  - Department of Statistics, Goztepe Campus, 34722, Kadikoy, Istanbul
  - Turkey
  - (ORCID 0000-0002-5208-4950)
- name: Han Lin Shang
  affiliation: Macquarie University
  email: |
    hanlin.shang@mq.edu.au
  address:
  - |-
    Department of Actuarial Studies and Business Analytics, Level 7, 4
    Eastern Road, Sydney, NSW 2109
  - Australia
  - (ORCID 0000-0003-1769-6430)
date: '2023-09-07'
date_received: '2022-05-20'
journal:
  firstpage: 212
  lastpage: 233
volume: 15
issue: 1
slug: RJ-2023-033
packages:
  cran: ~
  bioc: ~
draft: no
preview: preview.png
bibliography: betaztas-shang.bib
CTV: ~
output:
  rjtools::rjournal_web_article:
    self_contained: no
    toc: no
    legacy_pdf: yes

---





# Introduction

The interest and need for developing statistical methods for analyzing
functional data have increased in the last few decades. There have been
many recent theoretical and applied developments in functional data
analysis tools [see
@ramsay1991; @ramsay2002; @ramsay2006; @ferraty2006; @horvath2012; @cuevas2014; @Hsing; @MRS15; @SK16; @DM16; @KoRe].
Among many others, the scalar-on-function linear regression model
(SFLRM), where the response is scalar-valued and predictor(s) consist of
random functions [see
@cardot1991; @cardot2003; @james2002; @ResiOgden; @chen2011; @jiang2011; @GBC11; @dou2012; @TLS19; @ATW20; @BSchem],
and the function-on-function linear regression model (FFLRM), where both
the response and predictor(s) consist of random curves [see
@yao2005; @harezlak2007; @senturk2008; @matsui2009; @ivanescu2015; @SSG15; @chiou2016; @BeyaztasShang2020],
have received considerable attention among researchers as a tool for
exploring the functional relationship between a scalar
response-functional predictors and functional response-functional
predictors, respectively. In addition, please see the available
[R]{.sans-serif} packages fda [@RGH22] and refund [@GSH22] for the
implementation of many functional data analysis methods, including SFLRM
and FFLRM.

Most of the existing methods developed to estimate the SFLRM and FFLRM
are non-robust to outlying observations, i.e. observations which are
generated by a stochastic process with a distribution different from
that of the rest of the observations [see, e.g., @Rana]. In the case of
outliers, the non-robust methods produce biased estimates; thus,
predictions obtained from the fitted models become unreliable [see,
e.g.,
@zhu2011; @Maronna; @ShinLee; @Kalogridis; @Boente2020; @Harjit; @BSchem].
These methods may also lack robustness because they are coss-sectional
and the fitted mean of the response variable may not be representative
of the underlying data generating process. In this paper, we provide a
hands-on tutorial for the implementation of functional principal
component regression based on several robust approaches ( available in
the [R]{.sans-serif} package robflreg), for robustly modeling and
predicting the SFLRM and FFLRM in the presence of outliers.

The methods available in the robflreg package are centered on the robust
functional principal component analysis (RFPCA) approach of [@Bali2011].
It uses the robust projection pursuit approach of [@croux96] combined
with a robust scale estimator to produce functional principal components
and the corresponding principal component scores. With this approach,
the infinite-dimensional SFLRM and FFLRM are projected onto a
finite-dimensional space of RFPCA bases. Then, for the SFLRM, the robust
estimation methods, including the least trimmed squares (LTS) of
[@Rousseeuw1984], MM-type regression estimator (MM) of [@Yohai1987] and
[@Koller2011], S estimator, and the tau estimator of [@Salibian2008],
are used to estimate the parameter vector of regression model, where the
scalar-valued response is predicted by the robust principal component
scores of the functional predictors. For the FFLRM, on the other hand,
the robust estimation methods, including the minimum covariance
determinant estimator (MCD) of [@Rousseeuwetal1984], multivariate least
trimmed squares estimator (MLTS) of [@Agullo2008], MM estimator of
[@Kudraszow2011], S estimator of [@Bilo2000], and the tau estimator of
[@Ben2006], are used to estimate the parameter matrix of the regression
model between the robust principal component scores of the functional
response and the functional predictor variables. Besides the robust
procedures, the package robflreg allows for non-robust estimation of the
functional principal component regression model using the classical
functional principal component analysis (FPCA) of [@ramsay2006] and the
least-squares estimator.

The remainder of this paper is organized as follows. The SFLRM and
FFLRM, as well as the techniques used for modeling and predicting these
regression models, are reviewed and implemented using the robflreg
package. Some ideas on how the available [R]{.sans-serif} package
robflreg can be further improved are given at the end.

# Functional linear regression models

We present the SFLRM and FFLRM, respectively.

## The SFLRM {#the-sflrm .unnumbered}

We consider a random sample
$\left\lbrace Y_i, \bm{\mathcal{X}}_{i}(s): i = 1, \ldots, n \right\rbrace$
from the pair $( Y, \bm{\mathcal{X}} )$, where $Y \in \mathbb{R}$ is the
scalar response and
$\bm{\mathcal{X}} = [\mathcal{X}_1(s), \ldots, \mathcal{X}_P(s)]^\top$
with $\mathcal{X}_p(s) \in \mathcal{L}_2[0,\mathcal{I}]$,
$\forall~p = 1, \ldots, P$ is the vector of $P$ set of functional
predictors whose sample elements are denoted by curves belonging to
$\mathcal{L}_2$ Hilbert space, denoted by $\mathcal{H}$, with bounded
and closed interval $s \in \mathcal{I}$. We assume that the functional
predictors $\mathcal{X}_p(s)$, for $p = 1, \ldots, P$, have finite
second-order moments, i.e.,
$\text{E}[\Vert \mathcal{X}_p(s) \Vert] < \infty$. Without loss of
generality, we also assume that $Y$ and $\mathcal{X}_p(s)$, for
$p = 1, \ldots, P$, are mean-zero processes, so that
$\text{E}[Y] = \text{E}[\mathcal{X}_p(s)] = 0$ and $s \in [0,1]$. Then,
the SFRM is defined as follows: $$\label{eq:sof}
Y_i = \int_0^1 \bm{\mathcal{X}}_i^\top(s) \bm{\beta}(s) ds + \epsilon_i,$$
where $\beta_p(s) \in \mathcal{L}_2[0,1]$ is the regression coefficient
function linking $Y$ with $\mathcal{X}_p(s)$, and
$\bm{\beta}(s) = [ \beta_1(s), \ldots, \beta_P(s) ]^\top \in \mathcal{L}_2^P[0,1]$,
and $\epsilon_i$ is the error term which is assumed to follow a Gaussian
distribution with mean-zero and variance $\sigma^2$.

### Simulation of a dataset for the SFLRM {#simulation-of-a-dataset-for-the-sflrm .unnumbered}

The function `generate.sf.data()` in the package robflreg allows to
simulate a dataset for the
SFRM [\[eq:sof\]](#eq:sof){reference-type="eqref" reference="eq:sof"} as
follows:

`generate.sf.data(n,``\ `{=latex}`n.pred,``\ `{=latex}`n.gp,``\ `{=latex}`out.p``\ `{=latex}`=``\ `{=latex}`0)`

Here, the argument `n` denotes the number of observations for each
variable to be generated, `n.pred` denotes the number of functional
predictors to be generated, `n.gp` denotes the number of grid points
(i.e. the number of breaks in a fine grid on the interval $[0, 1]$), and
`out.p` is a number between 0 and 1, denoting percentage of outliers in
the generated data. In the data generation process, first,
`generate.sf.data()` simulates the functional predictors based on the
following process: $$\mathcal{X}(s) = \sum_{j=1}^5 \kappa_j \nu_j(s),$$
where $\kappa_j$ is a vector generated from a Normal distribution with
mean one and variance $\sqrt{a}j^{-3/2}$, where $a$ is is a uniformly
generated random number between 1 and 4, and
$$\nu_j(s) = \sin (j \pi s) - \cos (j \pi s).$$ The regression
coefficient functions are generated from a coefficient space that
includes ten different functions, such as $b \sin(2 \pi t)$ and
$b \cos(2 \pi t)$, where $b$ is generated from a uniform distribution
between 1 and 3. The error process is generated from the standard normal
distribution. Finally, the scalar response is obtained
using [\[eq:sof\]](#eq:sof){reference-type="eqref" reference="eq:sof"}.
Suppose outliers are allowed in the generated data, i.e., $out.p > 0$.
Then, the randomly selected $n \times out.p$ cases are generated
differently from the process mentioned above. In more detail, if
$out.p > 0$, the regression coefficient functions (possibly different
from the previously generated coefficient functions) generated from the
coefficient space with $b^*$ (instead of $b$), where $b^*$ is generated
from a uniform distribution between 3 and 5, are used to generate the
outlying observations. Further, in this case, the following process is
used to generate functional predictors:
$$\mathcal{X}^*(s) = \sum_{j=1}^5 \kappa_j^* \nu_j^*(s),$$ where
$\kappa_j^*$ is a vector generated from a Normal distribution with mean
one and variance $\sqrt{a}j^{-1/2}$ and
$$\nu_j^*(s) = 2 \sin (j \pi s) - \cos (j \pi s).$$ Moreover, the error
process is generated from a normal distribution with mean of zero and a
variance of one. A graphical display of the generated dataset with five
functional predictors and `n = 400` observations at 101 equally spaced
points in the interval $[0, 1]$ obtained by `generate.sf.data()` is
presented in Figure [1](#fig:1){reference-type="ref" reference="fig:1"}.

<figure id="fig:1">
<div class="center">
<p><img src="robflreg-001" style="width:32.0%" alt="image" /> <img
src="robflreg-002" style="width:32.0%" alt="image" /> <img
src="robflreg-003" style="width:32.0%" alt="image" /><br />
<img src="robflreg-004" style="width:32.0%" alt="image" /> <img
src="robflreg-005" style="width:32.0%" alt="image" /> <img
src="robflreg-006" style="width:32.0%" alt="image" /></p>
</div>
<figcaption>Plots of the simulated scalar response and the functional
predictor variables. a) Observations generated under the main data
generating process (black) and outlier points (grey). b-f) Curves of the
functional predictors generated under the main data generating process
(black) and outlier curves (grey).</figcaption>
</figure>

Figure [1](#fig:1){reference-type="ref" reference="fig:1"} can be
produced by the following code (refer to the supplement code file for
all the reproducible code):

`library(robflreg)`\
`library(fda.usc)`\
`set.seed(202)`

`#``\ `{=latex}`Generate``\ `{=latex}`a``\ `{=latex}`dataset``\ `{=latex}`with``\ `{=latex}`five``\ `{=latex}`functional``\ `{=latex}`predictors``\ `{=latex}`and``\ `{=latex}`400`\
`#``\ `{=latex}`observations``\ `{=latex}`at``\ `{=latex}`101``\ `{=latex}`equally``\ `{=latex}`spaced``\ `{=latex}`point``\ `{=latex}`in``\ `{=latex}`the``\ `{=latex}`interval``\ `{=latex}`[0,``\ `{=latex}`1]`\
`#``\ `{=latex}`for``\ `{=latex}`each``\ `{=latex}`variable``\ `{=latex}`for``\ `{=latex}`the``\ `{=latex}`scalar-on-function``\ `{=latex}`regression``\ `{=latex}`model`\
`sim.data``\ `{=latex}`<-``\ `{=latex}`generate.sf.data(n``\ `{=latex}`=``\ `{=latex}`400,``\ `{=latex}`n.pred``\ `{=latex}`=``\ `{=latex}`5,``\ `{=latex}`n.gp``\ `{=latex}`=``\ `{=latex}`101,``\ `{=latex}`out.p``\ `{=latex}`=``\ `{=latex}`0.1)`

`#``\ `{=latex}`Response``\ `{=latex}`variable`\
`Y``\ `{=latex}`<-``\ `{=latex}`sim.data`$Y
# Predictors
X <- sim.data$`X`\
`#``\ `{=latex}`Regression``\ `{=latex}`coefficient``\ `{=latex}`functions`\
`coeffs``\ `{=latex}`<-``\ `{=latex}`sim.data`$f.coef
# Plot the scalar response
out.indx <- sim.data$`out.indx`\
`plot(Y[-out.indx,],``\ `{=latex}`type``\ `{=latex}`=``\ `{=latex}`"p",``\ `{=latex}`pch``\ `{=latex}`=``\ `{=latex}`16,``\ `{=latex}`xlab``\ `{=latex}`=``\ `{=latex}`"Index",``\ `{=latex}`ylab``\ `{=latex}`=``\ `{=latex}`"",`\
`main``\ `{=latex}`=``\ `{=latex}`"Response",``\ `{=latex}`ylim``\ `{=latex}`=``\ `{=latex}`range(Y))`\
`points(out.indx,``\ `{=latex}`Y[out.indx,],``\ `{=latex}`type``\ `{=latex}`=``\ `{=latex}`"p",``\ `{=latex}`pch``\ `{=latex}`=``\ `{=latex}`16,``\ `{=latex}`col``\ `{=latex}`=``\ `{=latex}`"blue")`\
`#``\ `{=latex}`Plot``\ `{=latex}`the``\ `{=latex}`first``\ `{=latex}`functional``\ `{=latex}`predictor`\
`fX1``\ `{=latex}`<-``\ `{=latex}`fdata(X[[1]],``\ `{=latex}`argvals``\ `{=latex}`=``\ `{=latex}`seq(0,``\ `{=latex}`1,``\ `{=latex}`length.out``\ `{=latex}`=``\ `{=latex}`101))`\
`plot(fX1[-out.indx,],``\ `{=latex}`lty``\ `{=latex}`=``\ `{=latex}`1,``\ `{=latex}`ylab``\ `{=latex}`=``\ `{=latex}`"",``\ `{=latex}`xlab``\ `{=latex}`=``\ `{=latex}`"",``\ `{=latex}`col``\ `{=latex}`=``\ `{=latex}`"black",`\
`main``\ `{=latex}`=``\ `{=latex}`expression(X[1](s)),``\ `{=latex}`mgp``\ `{=latex}`=``\ `{=latex}`c(2,``\ `{=latex}`0.5,``\ `{=latex}`0),``\ `{=latex}`ylim``\ `{=latex}`=``\ `{=latex}`range(fX1))`\
`lines(fX1[out.indx,],``\ `{=latex}`lty``\ `{=latex}`=``\ `{=latex}`1,``\ `{=latex}`col``\ `{=latex}`=``\ `{=latex}`"grey")`

## The FFLRM {#the-fflrm .unnumbered}

Let us consider a random sample
$\lbrace \mathcal{Y}_i(t), \bm{\mathcal{X}}_i(s)$:
$i = 1, 2, \ldots n \rbrace$ from the pair
$(\mathcal{Y}, \bm{\mathcal{X}})$, where
$\mathcal{Y} \in \mathcal{L}_2 [0,1]$ is the functional response and
$\bm{\mathcal{X}} = [\mathcal{X}_1(s), \ldots, \mathcal{X}_P(s)]^\top$
with $\mathcal{X}_p(s) \in \mathcal{L}_2[0,1]$,
$\forall~p = 1, \ldots, P$ is the vector of $P$ set of functional
predictors. We assume that the functional response and functional
predictors have finite second-order moments, i.e.,
$\text{E}[\Vert \mathcal{Y}(t) \Vert] = \text{E}[\Vert \mathcal{X}_p(s) \Vert] < \infty$,
for $p = 1, \ldots, P$. Without loss of generality, we also assume that
both $\mathcal{Y}(t)$ and $\mathcal{X}_p(s)$, for $p = 1, \ldots, P$,
are mean-zero processes, so that
$\text{E}[Y(t)] = \text{E}[\mathcal{X}_p(s)] = 0$. Then, the FFRM is
defined as follows: $$\label{eq:fof}
Y_i(t) = \int_0^1 \bm{\mathcal{X}}_i^\top(s) \bm{\beta}(s,t) ds dt + \epsilon_i(t),$$
where $\beta_p(s,t) \in \mathcal{L}_2[0,1]$ is the bivariate regression
coefficient function linking $\mathcal{Y}(t)$ with $\mathcal{X}_p(s)$,
and
$\bm{\beta}(s,t) = [ \beta_1(s,t), \ldots, \beta_P(s,t) ]^\top \in \mathcal{L}_2^P[0,1]$,
and $\epsilon_i(t) \in \mathcal{L}_2[0,1]$ is the error term which is
assumed to be independent of $\mathcal{X}_p(s)$, for $p = 1, \ldots, P$
and $\text{E}[\epsilon_i(t)] = 0$.

### Simulation of a dataset for the FFLRM {#simulation-of-a-dataset-for-the-fflrm .unnumbered}

The function `generate.ff.data()` allows for the simulation of a dataset
for the FFLRM as follows:

`generate.ff.data(n.pred,``\ `{=latex}`n.curve,``\ `{=latex}`n.gp,``\ `{=latex}`out.p``\ `{=latex}`=``\ `{=latex}`0)`

Similar to the `generate.sf.data()`, `n.pred` denotes the number of
functional predictors to be generated, `n.curve` denotes the number of
observations for each functional variable to be generated, `n.gp`
denotes the number of grid points, and `out.p` is an integer between 0
and 1, denoting the outlier percentage in the generated data. When
generating a dataset, first, the function `generate.ff.data()` first
simulates the functional predictors via the following process:
$$\mathcal{X}(s) = \sum_{j=1}^5 \kappa_j \nu_j(s),$$ where $\kappa_j$ is
a vector generated from a Normal distribution with mean one and variance
$\sqrt{a}j^{-1/2}$, where $a$ is is a uniformly generated random number
between 1 and 4, and $$\nu_j(s) = \sin (j \pi s) - \cos (j \pi s).$$ The
bivariate regression coefficient functions are generated from a
coefficient space that includes ten different functions such as
$b \sin(2 \pi s) \sin(\pi t)$ and $b e^{-3(s-0.5)^2} e^{-4(t-1)^2}$,
where $b$ is generated from a uniform distribution between 1 and 3. The
error process $\epsilon(t)$, on the other hand, is generated from the
Ornstein-Uhlenbeck process:
$$\epsilon(t) = l + [\epsilon_0(t) - l] e^{-\theta t} \sigma \int_0^t e^{-\theta (t-u)} d W_u,$$
where $l, \theta > 0, \sigma > 0$ are real constants, $\epsilon_0(t)$ is
the initial value of $\epsilon(t)$ taken from $W_u$, and $W_u$ is the
Wiener process. If outliers are allowed in the generated data, i.e.,
$out.p > 0$, then, the randomly selected $n \times out.p$ cases are
generated differently from the process mentioned above. In more detail,
if $out.p > 0$, the regression coefficient functions (possibly different
from the previously generated coefficient functions) generated from the
coefficient space with $b^*$ (instead of $b$), where $b^*$ is generated
from a uniform distribution between 1 and 2, are used to generate the
outlying observations. In addition, in this case, the following process
is used to generate functional predictors:
$$\mathcal{X}^*(s) = \sum_{j=1}^5 \kappa_j^* \nu_j^*(s),$$ where
$\kappa_j^*$ is a vector generated from a Normal distribution with mean
one and variance $\sqrt{a}j^{-3/2}$ and
$$\nu_j^*(s) = 2 \sin (j \pi s) - \cos (j \pi s).$$ A graphical display
of the generated dataset with five functional predictors and `n = 200`
observations at 101 equally spaced points in the interval $[0, 1]$
obtained by `generate.ff.data()` is presented in
Figure [2](#fig:2){reference-type="ref" reference="fig:2"}.

<figure id="fig:2">
<div class="center">
<p><img src="robflreg-007" style="width:32.0%" alt="image" /> <img
src="robflreg-008" style="width:32.0%" alt="image" /> <img
src="robflreg-009" style="width:32.0%" alt="image" /><br />
<img src="robflreg-010" style="width:32.0%" alt="image" /> <img
src="robflreg-011" style="width:32.0%" alt="image" /> <img
src="robflreg-012" style="width:32.0%" alt="image" /></p>
</div>
<figcaption>Plots of the simulated functional response and functional
predictor variables: In the plots, the black curves denote the
observations of the response and functional predictors generated under
the smooth data generation process. The grey curves denote the outlying
observations in the functional variables.</figcaption>
</figure>

Figure [2](#fig:2){reference-type="ref" reference="fig:2"} can be
produced by the following code (refer to the supplement code file for
all the reproducible code):

`library(robflreg)`\
`library(fda.usc)`\
`set.seed(202)`

`#``\ `{=latex}`Generate``\ `{=latex}`a``\ `{=latex}`dataset``\ `{=latex}`with``\ `{=latex}`five``\ `{=latex}`functional``\ `{=latex}`predictors``\ `{=latex}`and``\ `{=latex}`200`\
`#``\ `{=latex}`observations``\ `{=latex}`at``\ `{=latex}`101``\ `{=latex}`equally``\ `{=latex}`spaced``\ `{=latex}`point``\ `{=latex}`in``\ `{=latex}`the``\ `{=latex}`interval``\ `{=latex}`[0,``\ `{=latex}`1]`\
`#``\ `{=latex}`for``\ `{=latex}`each``\ `{=latex}`variable``\ `{=latex}`for``\ `{=latex}`the``\ `{=latex}`function-on-function``\ `{=latex}`regression``\ `{=latex}`model`\
`sim.data``\ `{=latex}`<-``\ `{=latex}`generate.ff.data(n.pred``\ `{=latex}`=``\ `{=latex}`5,``\ `{=latex}`n.curve``\ `{=latex}`=``\ `{=latex}`200,``\ `{=latex}`n.gp``\ `{=latex}`=``\ `{=latex}`101,``\ `{=latex}`out.p``\ `{=latex}`=``\ `{=latex}`0.1)`

`#``\ `{=latex}`Response``\ `{=latex}`variable`\
`Y``\ `{=latex}`<-``\ `{=latex}`sim.data`$Y
# Predictors
X <- sim.data$`X`\
`#``\ `{=latex}`Regression``\ `{=latex}`coefficient``\ `{=latex}`functions`\
`coeffs``\ `{=latex}`<-``\ `{=latex}`sim.data`$f.coef
# Plot the scalar response
out.indx <- sim.data$`out.indx`\
`fY``\ `{=latex}`<-``\ `{=latex}`fdata(Y,``\ `{=latex}`argvals``\ `{=latex}`=``\ `{=latex}`seq(0,``\ `{=latex}`1,``\ `{=latex}`length.out``\ `{=latex}`=``\ `{=latex}`101))`\
`plot(fY[-out.indx,],``\ `{=latex}`lty``\ `{=latex}`=``\ `{=latex}`1,``\ `{=latex}`ylab``\ `{=latex}`=``\ `{=latex}`"",``\ `{=latex}`xlab``\ `{=latex}`=``\ `{=latex}`"",`\
`main``\ `{=latex}`=``\ `{=latex}`"Response",``\ `{=latex}`mgp``\ `{=latex}`=``\ `{=latex}`c(2,``\ `{=latex}`0.5,``\ `{=latex}`0),``\ `{=latex}`ylim``\ `{=latex}`=``\ `{=latex}`range(fY))`\
`lines(fY[out.indx,],``\ `{=latex}`lty``\ `{=latex}`=``\ `{=latex}`1,``\ `{=latex}`col``\ `{=latex}`=``\ `{=latex}`"grey")`\
`#``\ `{=latex}`Plot``\ `{=latex}`the``\ `{=latex}`first``\ `{=latex}`functional``\ `{=latex}`predictor`\
`fX1``\ `{=latex}`<-``\ `{=latex}`fdata(X[[1]],``\ `{=latex}`argvals``\ `{=latex}`=``\ `{=latex}`seq(0,``\ `{=latex}`1,``\ `{=latex}`length.out``\ `{=latex}`=``\ `{=latex}`101))`\
`plot(fX1[-out.indx,],``\ `{=latex}`lty``\ `{=latex}`=``\ `{=latex}`1,``\ `{=latex}`ylab``\ `{=latex}`=``\ `{=latex}`"",``\ `{=latex}`xlab``\ `{=latex}`=``\ `{=latex}`"",``\ `{=latex}`col``\ `{=latex}`=``\ `{=latex}`"black",`\
`main``\ `{=latex}`=``\ `{=latex}`expression(X[1](s)),``\ `{=latex}`mgp``\ `{=latex}`=``\ `{=latex}`c(2,``\ `{=latex}`0.5,``\ `{=latex}`0),``\ `{=latex}`ylim``\ `{=latex}`=``\ `{=latex}`range(fX1))`\
`lines(fX1[out.indx,],``\ `{=latex}`lty``\ `{=latex}`=``\ `{=latex}`1,``\ `{=latex}`col``\ `{=latex}`=``\ `{=latex}`"grey")`

# Estimation

We first review the classical and robust FPCA methods. Then, we will
focus on the robust estimation of the SFLRM and FFLRM by the functional
principal component regression.

## Functional principal component analysis (FPCA) {#functional-principal-component-analysis-fpca .unnumbered}

For a functional random variable $\mathcal{X}(s)$, let us denote its
covariance function by
$\mathcal{C}(s_1, s_2) = \text{Cov}[\mathcal{X}(s_1),~\mathcal{X}(s_2)]$
satisfying $\int_0^1 \int_0^1 \mathcal{C}(s_1, s_2) ds_1 ds_2 < \infty$.
Then, by Mercer's Theorem, the following representation holds:
$$\mathcal{C} = \sum_{k=1}^{\infty} \kappa_k \psi_k(s_1) \psi_k(s_2), \quad \forall s_1, s_2 \in [0,1],$$
where $\left \lbrace \psi_k(s): k = 1, 2, \ldots \right \rbrace$ are
orthonormal bases of eigenfunctions in $\mathcal{L}_2[0,1]$
corresponding to the non-negative eigenvalues
$\left \lbrace \kappa_k: k = 1, 2, \ldots \right \rbrace$ with
$\kappa_k \geq \kappa_{k+1}$. In practice, most of the variability in
functional variables can be captured via a finite number of the first
$K$ eigenfunctions. Thus, the covariance function of a functional
variable is estimated using a pre-determined truncation constant $K$. In
addition, the orthonormal bases of eigenfunctions are unknown in
practice. Thus, they are approximated via a suitable basis expansion
method such as B-spline, which is used in the robflreg package.

The RFPCA of [@Bali2011] follows a similar structure as the classical
FPCA, but it uses a robust scale functional instead of variance. Now let
$\Vert \alpha \Vert^2 = \langle \alpha, \alpha \rangle$ denote the norm
generated by the inner product $\langle \cdot, \cdot \rangle$. Also, let
$\mathcal{F}[\alpha]$ denote the distribution of
$\langle \alpha, \mathcal{X} \rangle$ where $\mathcal{F}$ is the
distribution of $\mathcal{X}$. Then, for a given M-scale functional
$\sigma_M(\mathcal{F})$, the orthonormal bases of eigenfunctions defined
by [@Bali2011] are as follows: $$\begin{cases}
\psi_k(\mathcal{F}) = \underset{\begin{subarray}{c}
      \Vert \alpha \Vert^2 = 1
\end{subarray}}{\arg \max}~ \sigma_M(\mathcal{F}[\alpha]), & k = 1, \\
\psi_k(\mathcal{F}) = \underset{\begin{subarray}{c}
      \Vert \alpha \Vert^2 = 1, \alpha \in \mathcal{B}_k
\end{subarray}}{\arg \max}~ \sigma_M(\mathcal{F}[\alpha]), & k \geq 2,
   \end{cases}$$ where
$\mathcal{B}_k = \left\lbrace \alpha \in \mathcal{L}_2[0,1]: \langle \alpha, \psi_k ( \mathcal{F} ) \rangle = 0, ~ 1 \leq k \leq \ K - 1 \right\rbrace$.
The $k$-th largest eigenvalue is given by:
$$\kappa_k(\mathcal{F}) = \sigma_M^2(\mathcal{F}[\psi_k]) = \underset{\begin{subarray}{c}
      \Vert \alpha \Vert^2 = 1, \alpha \in \mathcal{B}_k
\end{subarray}}{\max} \sigma_M^2(\mathcal{F}[\alpha]).$$

Denote by $\sigma_M(\mathcal{F}_n[\alpha])$ the functional for
$\sigma_M$. Let $s^2_n: \mathcal{L}_2[0,1] \rightarrow \mathbb{R}$
denote the function of empirical M-scale functional such that
$s^2(\alpha) = \sigma_M^2(\mathcal{F}[\alpha])$. Then, the RFPCA
estimates of the orthonormal bases of eigenfunctions for
$\mathcal{X}(s)$ are given by $$\begin{cases}
\widehat{\psi}_k(s) = \underset{\begin{subarray}{c}
      \Vert \alpha \Vert^2 = 1
\end{subarray}}{\arg \max}~ s_n(\alpha), & k = 1, \\
\widehat{\psi}_k(s) = \underset{\begin{subarray}{c}
      \alpha \in \widehat{\mathcal{B}}_k
\end{subarray}}{\arg \max}~ s_n(\alpha), & k \geq 2,
   \end{cases}$$ where
$\widehat{\mathcal{B}}_k = \left\lbrace \alpha \in \mathcal{L}_2 [0,1]: \Vert \alpha \Vert = 1, \langle \alpha, \widehat{\psi}_k \rangle = 0, ~ \forall~ 1 \leq k \leq \ K - 1 \right\rbrace$.
The corresponding eigenvalues, on the other hand, are given by
$$\widehat{\kappa}_k = s^2_n (\widehat{\psi}_k), \quad k \geq 1.$$

### Main RFPCA function and its arguments {#main-rfpca-function-and-its-arguments .unnumbered}

The main function to obtain the robust estimates of functional principal
components and the corresponding principal component scores is called
`getPCA()`:

`getPCA(data,``\ `{=latex}`nbasis,``\ `{=latex}`ncomp,``\ `{=latex}`gp,``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`c("classical",``\ `{=latex}`"robust"))`

In the `getPCA()` function, the data set is provided in the `data`
argument as a matrix. The grid points of the functional predictors are
provided in the `gp` argument as a vector. `nbasis` denotes the number
of B-spline basis expansion functions used to approximate the robust
functional principal components. `ncomp` specifies the number of
functional principal components to be computed. The argument `emodel`
denotes the method used for functional principal component
decomposition. If `emodel = "classical"`, then the classical functional
principal component decomposition is performed. On the other hand, if
`emodel = "robust"`, then the RFPCA method of [@Bali2011] is used to
obtain the functional principal components and the corresponding
principal component scores. Figure [3](#fig:3){reference-type="ref"
reference="fig:3"} presents the plot of five functional principal
components computed from simulated functional data using RFPCA and
`nbasis = 20` B-spline basis expansion functions.

<figure id="fig:3">
<div class="center">
<img src="robflreg-013" style="width:60.0%" />
</div>
<figcaption>Plot of the first five functional principal components of
the simulated functional data. The principal components were obtained
using the RFPCA and different colors correspond to different principal
components.</figcaption>
</figure>

Figure [3](#fig:3){reference-type="ref" reference="fig:3"} can be
produced by the following code:

`library(robflreg)`\
`#``\ `{=latex}`Generate``\ `{=latex}`a``\ `{=latex}`dataset``\ `{=latex}`with``\ `{=latex}`five``\ `{=latex}`functional``\ `{=latex}`predictors``\ `{=latex}`and``\ `{=latex}`200`\
`#``\ `{=latex}`observations``\ `{=latex}`at``\ `{=latex}`101``\ `{=latex}`equally``\ `{=latex}`spaced``\ `{=latex}`point``\ `{=latex}`in``\ `{=latex}`the``\ `{=latex}`interval``\ `{=latex}`[0,``\ `{=latex}`1]`\
`#``\ `{=latex}`for``\ `{=latex}`each``\ `{=latex}`variable``\ `{=latex}`for``\ `{=latex}`the``\ `{=latex}`function-on-function``\ `{=latex}`regression``\ `{=latex}`model`\
`set.seed(202)`\
`sim.data``\ `{=latex}`<-``\ `{=latex}`generate.ff.data(n.pred``\ `{=latex}`=``\ `{=latex}`5,``\ `{=latex}`n.curve``\ `{=latex}`=``\ `{=latex}`200,``\ `{=latex}`n.gp``\ `{=latex}`=``\ `{=latex}`101)`\
`#``\ `{=latex}`Response``\ `{=latex}`variable`\
`Y``\ `{=latex}`<-``\ `{=latex}`sim.data`$Y
gpY <- seq(0, 1, length.out = 101) # grid points

# Perform robust functional principal component analysis on the response variable Y
rob.fpca <- getPCA(data = Y, nbasis = 20, ncomp = 5, gp = gpY, emodel = "robust")

# Principal components
PCs <- rob.fpca$`PCAcoef`

`plot(PCs,``\ `{=latex}`xlab``\ `{=latex}`=``\ `{=latex}`"Grid``\ `{=latex}`point",``\ `{=latex}`ylab``\ `{=latex}`=``\ `{=latex}`"Values",``\ `{=latex}`lty``\ `{=latex}`=``\ `{=latex}`1)`

## Robust estimation of the SFLRM {#robust-estimation-of-the-sflrm .unnumbered}

In the robust estimation of the SFRM, we first consider the principal
component decomposition of the functional predictors as follows:
$$\mathcal{X}_p(s) = \sum_{k=1}^{K_p} \xi_{pk} \psi_{pk}(s),$$ where
$K_p$ is the truncation constant for the $p$-th functional predictor
$\mathcal{X}_p(s)$, $\psi_{pk}(s)$ is the $k$-th eigenfunction obtained
by the RFPCA of [@Bali2011], and $\xi_{pk}$ is the corresponding
principal component score, given by:
$$\xi_{pk} = \int_0^1 \mathcal{X}_p(s) \psi_{pk}(s) ds.$$

In practice, the eigenfunctions are approximated via a basis expansion
function such as B-spline. Let $\varphi_p(s)$ denote the B-spline basis
expansion function, and $A_p = (a_{pl})$ be an $n \times L$-dimensional
matrix of basis expansion coefficients for the $p$-th functional
predictor variable. In addition, let
$\bm{\varphi} = \int_0^1 \varphi_p(s) \varphi_p^\top (s) ds$ and
$\bm{\varphi}^{1/2}$ denote the $L \times L$ dimensional matrix of inner
products between the basis expansion functions and its square root,
respectively. Then, the infinite-dimensional RFPCA of $\mathcal{X}_p(s)$
is equivalent to the multivariate principal component analysis of
$A_p \bm{\varphi}^{1/2}$ and the $k$-th eigenfunction is given by
$\psi_{pk}(s) = \bm{\varphi}^{-1/2} v_{pk}$, where $v_{pk}$ denotes the
$p$-th eigenvector of the sample covariance matrix of
$A_p \bm{\varphi}^{1/2}$ [see, e.g., @Ocana2007 for more information].

If we assume that the $p$-th regression coefficient function
$\beta_p(s)$ admits the similar functional principal decomposition as
the functional predictors as follows:
$$\beta_p(s) = \sum_{k=1}^{K_p} b_{pk} \psi_{pk}(s),$$ where
$b_{pk} = \int_0^1 \beta_p(s) \psi_{pk}(s) ds$. Then, the
infinite-dimensional SFRM
in [\[eq:sof\]](#eq:sof){reference-type="eqref" reference="eq:sof"} is
approximated by the finite-dimensional regression model of scalar
response on all the functional principal component scores as follows:
$$Y = \sum_{p=1}^P \sum_{k=1}^{K_p} b_{pk} \xi_{pk}.$$

### Main functions for the robust estimation of a SFRM and their arguments {#main-functions-for-the-robust-estimation-of-a-sfrm-and-their-arguments .unnumbered}

The main function for robust estimation of SFRMs is called
`rob.sf.reg()`:

`rob.sf.reg(Y,``\ `{=latex}`X,``\ `{=latex}`X.scl``\ `{=latex}`=``\ `{=latex}`NULL,``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`c("classical",``\ `{=latex}`"robust"),`\
`fmodel``\ `{=latex}`=``\ `{=latex}`c("LTS",``\ `{=latex}`"MM",``\ `{=latex}`"S",``\ `{=latex}`"tau"),``\ `{=latex}`nbasis``\ `{=latex}`=``\ `{=latex}`NULL,``\ `{=latex}`gp``\ `{=latex}`=``\ `{=latex}`NULL,``\ `{=latex}`ncomp``\ `{=latex}`=``\ `{=latex}`NULL)`

In the `rob.sf.reg()` function, the scalar response is provided in the
`Y` argument as an $n \times 1$-dimensional column vector, where $n$ is
the sample size. The functional predictors, on the other hand, are
provided as a list object in the `X` argument. Each element of `X` is an
$n \times L_p$-dimensional matrix containing the observations of $p$-th
functional predictor $\mathcal{X}_p(s)$, where $L_p$ is the number of
grid points for $\mathcal{X}_p(s)$. The `rob.sf.reg()` function also
allows for scalar predictors, which can be provided in the `X.scl` as an
$n \times R$-dimensional matrix, where $R$ denotes the number of scalar
predictors. In this case, the following SFRM is considered:
$$Y_i = \int_0^1 \bm{\mathcal{X}}_i^\top(s) \bm{\beta}(s) ds + \text{X.scl}_i \bm{\gamma} + \epsilon_i,$$
where $\bm{\gamma}$ denotes the vector of coefficients for the scalar
predictors' matrix. The functional principal component decomposition
method is provided in the `emodel` argument. If `emodel = "classical"`,
then the classical functional principal component decomposition is
performed to obtain principal components and the corresponding principal
component scores. The coefficient vector of the regression problem of
scalar response on the principal component scores is estimated via the
least-squares method. If `emodel = "robust"`, then the RFPCA of
[@Bali2011] is performed to obtain the principal components and the
corresponding principal component scores. In this case, the method used
to estimate the coefficient vector of the regression problem constructed
by the scalar response and principal component scores is provided in the
`fmodel` argument. One of the methods among LTS, MM, S, and tau can be
chosen to estimate the parameter vector (by specifying, for example,
fmodel = tau\"). The number of B-spline basis expansion functions used
to approximate the functional principal components are provided in the
`nbasis` argument as a vector with length $p$. Suppose `nbasis = NULL`,
then $\min(20, L_p / 4)$ number of B-spline basis expansion functions
are used for each functional predictor. The grid points for the
functional predictors are provided in the `gp` argument as a list
object. The $p$-th element of `gp` is a vector containing the grid
points of the $p$-th functional predictor $\mathcal{X}_p(s)$. If
`gp = NULL`, then $L_p$ equally spaced time points in the interval
$[0,1]$ are used for the $p$-th functional predictor. The number of
principal components to be computed for the functional predictors are
provided in the `ncomp` argument as a vector with length $P$. If
`ncomp = NULL`, then, for each functional predictor, the number whose
usage results in at least 95% explained variation is used as the number
of principal components.

The function `get.sf.coeffs()` can be used to obtain the estimated
regression coefficient functions from a fitted functional principal
component regression:

`get.sf.coeffs(object)`

In this function, the argument `object` is the output object obtained
using the function `rob.sf.reg()`. The function `get.sf.coeffs()`
produces a list object whose $p$-th element is a vector with length
$L_p$ containing the $p$-th regression coefficient function
$\beta_p(s)$.

The plots of the estimated regression coefficient functions can be
obtained using the function `plot_sf_coeffs()`:

`plot_sf_coeffs(object,``\ `{=latex}`b)`

In Figure [4](#fig:4){reference-type="ref" reference="fig:4"}, the plots
of the regression coefficient functions obtained from simulated data
(outlier-contaminated) using `RFPCA` and MM estimator, as well as the
classical functional principal component regression, are presented. In
this function, the argument `object` is the output object obtained by
the function `get.sf.coeffs()`. On the other hand, the argument `b` is
an integer value indicating which regression parameter function to plot.
It is clear from this figure that, compared with the classical
functional principal component regression, the robust approach produces
estimated parameter functions which are more similar to the true
parameter functions.

<figure id="fig:4">
<div class="center">
<p><img src="robflreg-014" style="width:32.0%" alt="image" /> <img
src="robflreg-015" style="width:32.0%" alt="image" /> <img
src="robflreg-016" style="width:32.0%" alt="image" /></p>
</div>
<figcaption>A plot of the estimated regression coefficient functions
obtained from simulated data (outlier-contaminated) using RFPCA and MM
estimator and the classical functional principal component regression.
Blue lines denote the true parameter functions. On the other hand, black
and red lines denote the estimated parameter functions by the classical
functional principal component regression and robust MM-based functional
principal component regression, respectively.</figcaption>
</figure>

The following code can be used to produce this figure and the results
(refer to the supplement code file for all of the reproducible code):

`library(robflreg)`\
`#``\ `{=latex}`Generate``\ `{=latex}`a``\ `{=latex}`dataset``\ `{=latex}`with``\ `{=latex}`three``\ `{=latex}`functional``\ `{=latex}`predictors``\ `{=latex}`and``\ `{=latex}`400`\
`#``\ `{=latex}`observations``\ `{=latex}`at``\ `{=latex}`101``\ `{=latex}`equally``\ `{=latex}`spaced``\ `{=latex}`point``\ `{=latex}`in``\ `{=latex}`the``\ `{=latex}`interval``\ `{=latex}`[0,``\ `{=latex}`1]`\
`#``\ `{=latex}`for``\ `{=latex}`each``\ `{=latex}`variable``\ `{=latex}`for``\ `{=latex}`the``\ `{=latex}`scalar-on-function``\ `{=latex}`regression``\ `{=latex}`model`\
`set.seed(202)`\
`sim.data``\ `{=latex}`<-``\ `{=latex}`generate.sf.data(n``\ `{=latex}`=``\ `{=latex}`400,``\ `{=latex}`n.pred``\ `{=latex}`=``\ `{=latex}`3,``\ `{=latex}`n.gp``\ `{=latex}`=``\ `{=latex}`101,``\ `{=latex}`out.p``\ `{=latex}`=``\ `{=latex}`0.1)`

`#``\ `{=latex}`True``\ `{=latex}`parameter``\ `{=latex}`functions`\
`true.b1``\ `{=latex}`<-``\ `{=latex}`sim.data$f.coef[[1]]`\
`true.b2``\ `{=latex}`<-``\ `{=latex}`sim.data$f.coef[[2]]`\
`true.b3``\ `{=latex}`<-``\ `{=latex}`sim.data$f.coef[[3]]`

`#``\ `{=latex}`Response``\ `{=latex}`variable`\
`Y``\ `{=latex}`<-``\ `{=latex}`sim.data$Y`\
`#``\ `{=latex}`Predictors`\
`X``\ `{=latex}`<-``\ `{=latex}`sim.data$X`

`gp``\ `{=latex}`<-``\ `{=latex}`rep(list(seq(0,``\ `{=latex}`1,``\ `{=latex}`length.out``\ `{=latex}`=``\ `{=latex}`101)),``\ `{=latex}`3)``\ `{=latex}`#``\ `{=latex}`grid``\ `{=latex}`points``\ `{=latex}`of``\ `{=latex}`Xs`

`#``\ `{=latex}`Fit``\ `{=latex}`a``\ `{=latex}`functional``\ `{=latex}`principal``\ `{=latex}`component``\ `{=latex}`regression``\ `{=latex}`model``\ `{=latex}`for``\ `{=latex}`the``\ `{=latex}`generated``\ `{=latex}`data`\
`#``\ `{=latex}`using``\ `{=latex}`the``\ `{=latex}`classical``\ `{=latex}`functional``\ `{=latex}`principal``\ `{=latex}`component``\ `{=latex}`analysis``\ `{=latex}`method:`\
`classical.fit``\ `{=latex}`<-``\ `{=latex}`rob.sf.reg(Y,``\ `{=latex}`X,``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"classical",``\ `{=latex}`gp``\ `{=latex}`=``\ `{=latex}`gp)`

`#``\ `{=latex}`Fit``\ `{=latex}`a``\ `{=latex}`functional``\ `{=latex}`principal``\ `{=latex}`component``\ `{=latex}`regression``\ `{=latex}`model``\ `{=latex}`for``\ `{=latex}`the``\ `{=latex}`generated``\ `{=latex}`data`\
`#``\ `{=latex}`using``\ `{=latex}`the``\ `{=latex}`robust``\ `{=latex}`functional``\ `{=latex}`principal``\ `{=latex}`component``\ `{=latex}`analysis``\ `{=latex}`method``\ `{=latex}`and``\ `{=latex}`tau``\ `{=latex}`estimator:`\
`robust.fit``\ `{=latex}`<-``\ `{=latex}`rob.sf.reg(Y,``\ `{=latex}`X,``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"robust",``\ `{=latex}`fmodel``\ `{=latex}`=``\ `{=latex}`"MM",``\ `{=latex}`gp``\ `{=latex}`=``\ `{=latex}`gp)`

`#``\ `{=latex}`Estimated``\ `{=latex}`regression``\ `{=latex}`coefficient``\ `{=latex}`functions`\
`classical.coefs``\ `{=latex}`<-``\ `{=latex}`get.sf.coeffs(classical.fit)`\
`robust.coefs``\ `{=latex}`<-``\ `{=latex}`get.sf.coeffs(robust.fit)`

`#``\ `{=latex}`The``\ `{=latex}`first``\ `{=latex}`estimated``\ `{=latex}`regression``\ `{=latex}`coefficient``\ `{=latex}`function`\
`plot_sf_coeffs(object``\ `{=latex}`=``\ `{=latex}`classical.coefs,``\ `{=latex}`b``\ `{=latex}`=``\ `{=latex}`1)`\
`lines(gp[[1]],``\ `{=latex}`robust.coefs$coefficients[[1]],``\ `{=latex}`col``\ `{=latex}`=``\ `{=latex}`"red")`\
`lines(gp[[1]],``\ `{=latex}`true.b1,``\ `{=latex}`col``\ `{=latex}`=``\ `{=latex}`"blue",``\ `{=latex}`lwd``\ `{=latex}`=``\ `{=latex}`2)`

## Robust estimation of the FFLRM {#robust-estimation-of-the-fflrm .unnumbered}

Let us consider the functional principal decompositions of both the
functional response and functional predictor variables as follows:
$$\mathcal{Y}(t) = \sum_{k=1}^K \zeta_k \phi_k(t), \quad \mathcal{X}_p(s) = \sum_{j=1}^{K_p} \xi_{pj} \psi_{pj}(s),$$
where $\phi_k(t)$ and $\psi_{pj}(s)$ respectively are the $k$-th and
$j$-th eigenfunctions of $\mathcal{Y}(t)$ and $\mathcal{X}_p(s)$
obtained by the RFPCA and $\zeta_k$ and $\xi_{pj}$ are the corresponding
principal component scores given by
$$\zeta_k = \int_0^1 \mathcal{Y}(t) \phi_k(t) dt, \quad \xi_{pj} = \int_0^1 \mathcal{X}_p(s) \psi_{pj}(s) ds.$$
If we assume that the $p$-th bivariate regression coefficient function
$\beta_p(s,t)$ admits the principal component decomposition with the
eigenfunctions $\phi_k(t)$ and $\psi_{pj}(s)$ as follows:
$$\beta_p(s,t) = \sum_{k=1}^K \sum_{j=1}^{K_p} b_{pkj} \phi_k(t) \psi_{pj}(s),$$
where
$b_{pkj} = \int_0^1 \int_0^1 \beta_p(s,t) \phi_k(t) \psi_{pj}(s) dt ds$.
Then, the infinite-dimensional FFRM
in [\[eq:fof\]](#eq:fof){reference-type="eqref" reference="eq:fof"} is
approximated by the finite-dimensional regression model of principal
component scores of the functional response on all the functional
principal component scores as follows:
$$\zeta_k = \sum_{p=1}^P \sum_{j=1}^{K_p} b_{pkj} \xi_{pj}.$$ Finally,
the following regression model is obtained for the functional response
$$\mathcal{Y}(t) = \sum_{k=1}^K \left(\sum_{p=1}^P \sum_{j=1}^{K_p} b_{pkj} \xi_{pj} \right) \phi_k(t).$$

### Main functions for the robust estimation of a FFRM and their arguments {#main-functions-for-the-robust-estimation-of-a-ffrm-and-their-arguments .unnumbered}

The main function to estimate the FFRM robustly is called
`rob.ff.reg()`:

`rob.ff.reg(Y,``\ `{=latex}`X,``\ `{=latex}`model``\ `{=latex}`=``\ `{=latex}`c("full",``\ `{=latex}`"selected"),``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`c("classical",``\ `{=latex}`"robust"),`\
`fmodel``\ `{=latex}`=``\ `{=latex}`c("MCD",``\ `{=latex}`"MLTS",``\ `{=latex}`"MM",``\ `{=latex}`"S",``\ `{=latex}`"tau"),``\ `{=latex}`nbasisY``\ `{=latex}`=``\ `{=latex}`NULL,``\ `{=latex}`nbasisX``\ `{=latex}`=``\ `{=latex}`NULL,`\
`gpY``\ `{=latex}`=``\ `{=latex}`NULL,``\ `{=latex}`gpX``\ `{=latex}`=``\ `{=latex}`NULL,``\ `{=latex}`ncompY``\ `{=latex}`=``\ `{=latex}`NULL,``\ `{=latex}`ncompX``\ `{=latex}`=``\ `{=latex}`NULL)`

In the `rob.ff.reg()` function, the functional response is provided via
the `Y` argument as a matrix. On the other hand, the functional
predictors are provided in the argument `X` as a list object. Each
element of `X` is a matrix containing the observations of $p$-th
functional predictor. The model type to be fitted can be chosen with
`model` argument. If `model = "full"`, then all of the functional
predictors are used in the model. On the other hand, if
`model = "selected"`, then only the significant functional predictor
variables, determined by the forward variable selection procedure of
[@BS22], are used in the model. The functional principal component
decomposition method is provided via the `emodel` argument. If
`emodel = "classical"`, then the classical functional principal
component decomposition is performed to obtain principal components and
the corresponding principal component scores and the coefficient vector
of the regression problem of principal component scores of the
functional response on the principal component scores are estimated via
the least-squares method. If `emodel = "robust"`, then the RFPCA of
[@Bali2011] is performed to obtain the principal components and the
corresponding principal component scores. In this case, the method used
to estimate the coefficient matrix of the regression problem constructed
by the principal component scores is provided in the `fmodel` argument.
Here, one of the methods, among MCD, MLTS, MM, S, and tau estimators,
can be chosen to estimate the parameter matrix. The number of B-spline
basis expansion functions used to approximate the functional principal
components of response and predictor variables are provided in the
`nbasisY` and `nbasisX` arguments, respectively. The argument `nbasisY`
is a numeric value while the argument `nbasisX` is a vector with length
$P$. Suppose `nbasisY = NULL` and `nbasisX = NULL`, then $\min(20, L_y)$
and $\min(20, L_p)$ B-spline basis expansion functions are used to
approximate the functional principal components of functional response
and $p$-th the functional predictor, where $L_y$ and $L_p$ respectively
denote the number of grid points for $\mathcal{Y}(t)$ and
$\mathcal{X}_p(s)$. The grid points for the functional response and
functional predictors are provided in the `gpY` and `gpX` arguments,
respectively. The argument `gpY` is a vector consisting of the grid
points of the functional response $\mathcal{Y}(t)$. On the other hand,
the argument `gpX` is a list object, and its $p$-th element is a vector
containing the grid points of the $p$-th functional predictor
$\mathcal{X}_p(s)$. If `gpY = NULL` and If `gpX = NULL`, then equally
spaced time points in the interval $[0,1]$ are used for all the
functional variables. The number of functional predictors to be computed
for the functional response and functional predictors are provided in
the arguments `ncompY` and `ncompX`, respectively. The argument `ncompY`
is a numeric value while the argument `ncompX` is a vector with length
$P$. If `ncompY = NULL` and `ncompX = NULL`, then the number whose usage
results in at least 95% explained variation is used as the number of
principal components for each functional variable.

The estimated bivariate regression coefficient functions from a fitted
functional principal component regression model are obtained by the
`get.ff.coeffs()` function:

`get.ff.coeffs(object)`

In this function, the argument `object` is the output object obtained
using the function `rob.ff.reg()`. The function `get.ff.coeffs()`
produces a list object whose $p$-th element is a matrix containing the
$p$-th bivariate regression coefficient function $\beta_p(s,t)$.

The image plots of the estimated bivariate regression coefficient
functions can be obtained using the function `plot_ff_coeffs()`:

`plot_ff_coeffs(object,``\ `{=latex}`b)`

In Figure [5](#fig:5){reference-type="ref" reference="fig:5"}, the image
plots of the regression coefficient functions obtained from simulated
data using `RFPCA` and MM estimator are presented. In this function, the
argument `object` is the output object obtained by the function
`get.ff.coeffs()`. The argument `b` is an integer value indicating which
regression parameter function is to be plotted.

<figure id="fig:5">
<div class="center">
<p><img src="robflreg-017" style="width:32.0%" alt="image" /> <img
src="robflreg-018" style="width:32.0%" alt="image" /> <img
src="robflreg-019" style="width:32.0%" alt="image" /></p>
</div>
<figcaption>Image plots of the estimated bivariate regression
coefficient functions obtained from simulated data using RFPCA and MM
estimator.</figcaption>
</figure>

This figure and the results can be produced by the following code (refer
to the supplement code file for all the reproducible code):

`library(robflreg)`\
`#``\ `{=latex}`Generate``\ `{=latex}`a``\ `{=latex}`dataset``\ `{=latex}`with``\ `{=latex}`three``\ `{=latex}`functional``\ `{=latex}`predictors``\ `{=latex}`and``\ `{=latex}`200`\
`#``\ `{=latex}`observations``\ `{=latex}`at``\ `{=latex}`101``\ `{=latex}`equally``\ `{=latex}`spaced``\ `{=latex}`point``\ `{=latex}`in``\ `{=latex}`the``\ `{=latex}`interval``\ `{=latex}`[0,``\ `{=latex}`1]`\
`#``\ `{=latex}`for``\ `{=latex}`each``\ `{=latex}`variable``\ `{=latex}`for``\ `{=latex}`the``\ `{=latex}`function-on-function``\ `{=latex}`regression``\ `{=latex}`model`\
`set.seed(202)`\
`sim.data``\ `{=latex}`<-``\ `{=latex}`generate.ff.data(n.pred``\ `{=latex}`=``\ `{=latex}`3,``\ `{=latex}`n.curve``\ `{=latex}`=``\ `{=latex}`200,``\ `{=latex}`n.gp``\ `{=latex}`=``\ `{=latex}`101)`\
`#``\ `{=latex}`Response``\ `{=latex}`variable`\
`Y``\ `{=latex}`<-``\ `{=latex}`sim.data`$Y
# Predictors
X <- sim.data$`X`

`gpY``\ `{=latex}`=``\ `{=latex}`seq(0,``\ `{=latex}`1,``\ `{=latex}`length.out``\ `{=latex}`=``\ `{=latex}`101)``\ `{=latex}`#``\ `{=latex}`grid``\ `{=latex}`points``\ `{=latex}`of``\ `{=latex}`Y`\
`gpX``\ `{=latex}`<-``\ `{=latex}`rep(list(seq(0,``\ `{=latex}`1,``\ `{=latex}`length.out``\ `{=latex}`=``\ `{=latex}`101)),``\ `{=latex}`3)``\ `{=latex}`#``\ `{=latex}`grid``\ `{=latex}`points``\ `{=latex}`of``\ `{=latex}`Xs`

`#``\ `{=latex}`Fit``\ `{=latex}`a``\ `{=latex}`functional``\ `{=latex}`principal``\ `{=latex}`component``\ `{=latex}`regression``\ `{=latex}`model``\ `{=latex}`for``\ `{=latex}`the``\ `{=latex}`generated``\ `{=latex}`data`\
`#``\ `{=latex}`using``\ `{=latex}`the``\ `{=latex}`RFPCA``\ `{=latex}`and``\ `{=latex}`MM``\ `{=latex}`estimator:`\
`model.fit``\ `{=latex}`<-``\ `{=latex}`rob.ff.reg(Y,``\ `{=latex}`X,``\ `{=latex}`model``\ `{=latex}`=``\ `{=latex}`"full",``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"robust",`\
`fmodel``\ `{=latex}`=``\ `{=latex}`"MM",``\ `{=latex}`gpY``\ `{=latex}`=``\ `{=latex}`gpY,``\ `{=latex}`gpX``\ `{=latex}`=``\ `{=latex}`gpX)`

`#``\ `{=latex}`Estimated``\ `{=latex}`bivariate``\ `{=latex}`regression``\ `{=latex}`coefficient``\ `{=latex}`functions`\
`coefs``\ `{=latex}`<-``\ `{=latex}`get.ff.coeffs(model.fit)`\
`#``\ `{=latex}`Plot``\ `{=latex}`the``\ `{=latex}`first``\ `{=latex}`bivariate``\ `{=latex}`regression``\ `{=latex}`coefficient``\ `{=latex}`function`\
`plot_ff_coeffs(object``\ `{=latex}`=``\ `{=latex}`coefs,``\ `{=latex}`b``\ `{=latex}`=``\ `{=latex}`1)`

### Outlier detection in the functional response {#outlier-detection-in-the-functional-response .unnumbered}

Detection of outliers in functional data is an important problem [see,
e.g., @Sun2011; @Aribas2014; @Dai2018]. From a fitted functional
principal component regression for scalar response and scalar
predictors, the robflr package with the function `rob.out.detect()`
allows to detection of outliers in the functional response. This is
achieved by applying the function depth-based outlier detection method
of [@Febrero08] together with the h-modal depth proposed by [@cuevas07]
to the estimated residual functions obtained from `rob.ff.reg()` to
determine the outliers in the response variable. In the outlier
detection algorithm, the threshold value used to identify outliers is
determined by the smoothed bootstrap procedure proposed by [@Febrero08].
The `rob.out.detect()` is as follows:

`rob.out.detect(object,``\ `{=latex}`alpha``\ `{=latex}`=``\ `{=latex}`0.01,``\ `{=latex}`B``\ `{=latex}`=``\ `{=latex}`200,``\ `{=latex}`fplot``\ `{=latex}`=``\ `{=latex}`FALSE)`

Herein, the argument `object` is an output object obtained from
`rob.ff.reg()`. `alpha`, whose default value is 0.01, denotes the
percentile of the distribution of the functional depth. `B` denotes the
number of bootstrap samples (the default value is `B = 200`). `fplot` is
a logical argument, if `fplot = TRUE`, then the outlying points flagged
by the method are plotted along with the values of functional response
$\mathcal{Y}(t)$.

To show how the function `rob.ff.reg()` works, we simulate an
outlier-contaminated dataset for the FFRM. Then, we apply the outlier
detection algorithm with the classical FPCA - least squares estimator
and the RFPCA - MM estimator. The plots of the functional response and
detected outlying observations are presented in
Figure [6](#fig:6){reference-type="ref" reference="fig:6"}. The results
show that the classical method fails to flag 13 outlying curves, while
the robust procedure fails to flag only two outlying curves.

<figure id="fig:6">
<div class="center">
<p><img src="robflreg-020" style="width:46.0%" alt="image" /> <img
src="robflreg-021" style="width:46.0%" alt="image" /></p>
</div>
<figcaption>Plots of the functional response and detected outliers:
Classical method (left panel) vs. Robust method (right panel). The
detected outlying curves are denoted by black, while the non-outlying
curves are denoted by grey.</figcaption>
</figure>

The following code can produce the results and
Figure [6](#fig:6){reference-type="ref" reference="fig:6"}.

`library(robflreg)`\
`#``\ `{=latex}`Generate``\ `{=latex}`a``\ `{=latex}`dataset``\ `{=latex}`with``\ `{=latex}`five``\ `{=latex}`functional``\ `{=latex}`predictors``\ `{=latex}`and``\ `{=latex}`200`\
`#``\ `{=latex}`observations``\ `{=latex}`at``\ `{=latex}`101``\ `{=latex}`equally``\ `{=latex}`spaced``\ `{=latex}`point``\ `{=latex}`in``\ `{=latex}`the``\ `{=latex}`interval``\ `{=latex}`[0,``\ `{=latex}`1]`\
`#``\ `{=latex}`for``\ `{=latex}`each``\ `{=latex}`variable``\ `{=latex}`for``\ `{=latex}`the``\ `{=latex}`function-on-function``\ `{=latex}`regression``\ `{=latex}`model`\
`set.seed(202)`\
`sim.data``\ `{=latex}`<-``\ `{=latex}`generate.ff.data(n.pred``\ `{=latex}`=``\ `{=latex}`5,``\ `{=latex}`n.curve``\ `{=latex}`=``\ `{=latex}`200,``\ `{=latex}`n.gp``\ `{=latex}`=``\ `{=latex}`101,``\ `{=latex}`out.p``\ `{=latex}`=``\ `{=latex}`0.1)`\
`out.indx``\ `{=latex}`<-``\ `{=latex}`sim.data$out.indx`\
`#``\ `{=latex}`Response``\ `{=latex}`variable`\
`Y``\ `{=latex}`<-``\ `{=latex}`sim.data$Y`\
`#``\ `{=latex}`Predictors`\
`X``\ `{=latex}`<-``\ `{=latex}`sim.data$X`

`gpY``\ `{=latex}`=``\ `{=latex}`seq(0,``\ `{=latex}`1,``\ `{=latex}`length.out``\ `{=latex}`=``\ `{=latex}`101)``\ `{=latex}`#``\ `{=latex}`grid``\ `{=latex}`points``\ `{=latex}`of``\ `{=latex}`Y`\
`gpX``\ `{=latex}`<-``\ `{=latex}`rep(list(seq(0,``\ `{=latex}`1,``\ `{=latex}`length.out``\ `{=latex}`=``\ `{=latex}`101)),``\ `{=latex}`5)``\ `{=latex}`#``\ `{=latex}`grid``\ `{=latex}`points``\ `{=latex}`of``\ `{=latex}`Xs`

`#``\ `{=latex}`Perform``\ `{=latex}`classical``\ `{=latex}`functional``\ `{=latex}`principal``\ `{=latex}`component``\ `{=latex}`regression``\ `{=latex}`using``\ `{=latex}`least-squares`\
`model.classical``\ `{=latex}`<-``\ `{=latex}`rob.ff.reg(Y``\ `{=latex}`=``\ `{=latex}`Y,``\ `{=latex}`X``\ `{=latex}`=``\ `{=latex}`X,``\ `{=latex}`model``\ `{=latex}`=``\ `{=latex}`"full",``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"classical",`\
`gpY``\ `{=latex}`=``\ `{=latex}`gpY,``\ `{=latex}`gpX``\ `{=latex}`=``\ `{=latex}`gpX)`

`#``\ `{=latex}`Perform``\ `{=latex}`robust``\ `{=latex}`functional``\ `{=latex}`principal``\ `{=latex}`component``\ `{=latex}`regression``\ `{=latex}`using``\ `{=latex}`MM-estimator`\
`model.MM``\ `{=latex}`<-``\ `{=latex}`rob.ff.reg(Y``\ `{=latex}`=``\ `{=latex}`Y,``\ `{=latex}`X``\ `{=latex}`=``\ `{=latex}`X,``\ `{=latex}`model``\ `{=latex}`=``\ `{=latex}`"full",``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"robust",``\ `{=latex}`fmodel``\ `{=latex}`=``\ `{=latex}`"MM",`\
`gpY``\ `{=latex}`=``\ `{=latex}`gpY,``\ `{=latex}`gpX``\ `{=latex}`=``\ `{=latex}`gpX)`

`#``\ `{=latex}`Detect``\ `{=latex}`outliers``\ `{=latex}`using``\ `{=latex}`rob.out.detect``\ `{=latex}`function`\
`rob.out.detect(object``\ `{=latex}`=``\ `{=latex}`model.classical,``\ `{=latex}`fplot``\ `{=latex}`=``\ `{=latex}`TRUE)`\
`#``\ `{=latex}`outlying``\ `{=latex}`functions``\ `{=latex}`are:``\ `{=latex}`16``\ `{=latex}`56``\ `{=latex}`69``\ `{=latex}`70``\ `{=latex}`71``\ `{=latex}`80``\ `{=latex}`92``\ `{=latex}`96``\ `{=latex}`117``\ `{=latex}`138``\ `{=latex}`140``\ `{=latex}`173``\ `{=latex}`188`\
`rob.out.detect(object``\ `{=latex}`=``\ `{=latex}`model.MM,``\ `{=latex}`fplot``\ `{=latex}`=``\ `{=latex}`TRUE)`\
`#``\ `{=latex}`outlying``\ `{=latex}`functions``\ `{=latex}`are:``\ `{=latex}`2``\ `{=latex}`16``\ `{=latex}`56``\ `{=latex}`69``\ `{=latex}`70``\ `{=latex}`71``\ `{=latex}`80``\ `{=latex}`82``\ `{=latex}`92``\ `{=latex}`96``\ `{=latex}`117``\ `{=latex}`134``\ `{=latex}`138``\ `{=latex}`140`\
`#``\ `{=latex}`173``\ `{=latex}`188``\ `{=latex}`197``\ `{=latex}`199`

`#``\ `{=latex}`Compare``\ `{=latex}`with``\ `{=latex}`the``\ `{=latex}`original``\ `{=latex}`outliers`\
`sort(out.indx)`\
`#``\ `{=latex}`[1]``\ `{=latex}`2``\ `{=latex}`16``\ `{=latex}`47``\ `{=latex}`56``\ `{=latex}`69``\ `{=latex}`70``\ `{=latex}`71``\ `{=latex}`80``\ `{=latex}`82``\ `{=latex}`92``\ `{=latex}`96``\ `{=latex}`117``\ `{=latex}`134``\ `{=latex}`138``\ `{=latex}`140``\ `{=latex}`162``\ `{=latex}`173``\ `{=latex}`188``\ `{=latex}`197``\ `{=latex}`199`

# Prediction

We review the prediction problem for a new set of functional predictors
based on a fitted functional principal component regression model.

## Prediction for the SFRM {#prediction-for-the-sfrm .unnumbered}

When robustly predicting the unknown values of the scalar response
variable for a given new set of functional predictors
($\mathcal{X}^*(s)$), the principal component scores of the new set of
functional predictors ($\xi^*$) are obtained as follows:
$$\xi_{pk}^* = \int_0^1 \mathcal{X}^*_{pk}(s) \widehat{\psi}_{pk}(s) ds,$$
where $\widehat{\psi}_{pk}(s)$ is the $k$-th eigenfunction of the
$p$-the functional predictor obtained by the RFPCA. Then, the
predictions corresponding to the new set of functional predictors are
obtained as follows:
$$\widehat{Y}^* = \sum_{p=1}^P \sum_{k=1}^{K_p} \widehat{b}_{pk} \xi_{pk}^*,$$
where $\widehat{b}_{pk}$ is the estimated parameter vector obtained from
the fitted model `rob.sf.reg()`.

### Main function for the robust prediction of a SFRM and its arguments {#main-function-for-the-robust-prediction-of-a-sfrm-and-its-arguments .unnumbered}

The main function for the robust prediction of a SFRM is called
`predict_sf_regression()`:

`predict_sf_regression(object,``\ `{=latex}`Xnew,``\ `{=latex}`Xnew.scl``\ `{=latex}`=``\ `{=latex}`NULL)`

In the function `predict_sf_regression()`, the argument `object` is an
output object obtained from `rob.sf.reg`. The new set of functional
predictors is provided in the `Xnew` argument as a list object whose
$p$-th element is a matrix denoting the new observations of
$\mathcal{X}_p(s)$. `Xnew` must have the same length and the same
structure as the input `X` of `rob.sf.reg`. If scalar predictors are
used in the SFRM, then, in the prediction process, the new set of scalar
predictors is provided as a matrix in the `Xnew.scl` argument. The
argument `Xnew.scl` must have the same length and the same structure as
the input `X.scl` of `rob.sf.reg`.

To evaluate the prediction performance of classical and robust methods,
we simulate a dataset with size $n = 400$ for the SFRM. Then, the
simulated data are divided into a training sample with a size of 280 and
a test sample with a size of 120. Random outliers contaminate the
training sample, and both the classical and robust methods with the tau
estimator are applied to the training sample to predict the values of
the response variable in the test sample. To compare both methods, we
compute the mean squared prediction error (MSPE):
$$\text{MSPE} = \frac{1}{200} \sum_{i=1}^{200} (Y_i^* - \widehat{Y}_i^*)^2,$$
where $Y_i^*$ and $\widehat{Y}_i^*$ denote the observed and predicted
values of the scalar response in the test sample. Our results indicate
that the robust method considerably outperforms the classical method.
The MSPE computed from the classical method is 20.9388, while the MSPE
obtained from the robust method is 1.868. The reproducible code to
obtain those results is as follows:

`library(robflreg)`\
`#``\ `{=latex}`Generate``\ `{=latex}`a``\ `{=latex}`dataset``\ `{=latex}`with``\ `{=latex}`five``\ `{=latex}`functional``\ `{=latex}`predictors``\ `{=latex}`and``\ `{=latex}`400`\
`#``\ `{=latex}`observations``\ `{=latex}`at``\ `{=latex}`101``\ `{=latex}`equally``\ `{=latex}`spaced``\ `{=latex}`point``\ `{=latex}`in``\ `{=latex}`the``\ `{=latex}`interval``\ `{=latex}`[0,``\ `{=latex}`1]`\
`#``\ `{=latex}`for``\ `{=latex}`each``\ `{=latex}`variable``\ `{=latex}`for``\ `{=latex}`the``\ `{=latex}`scalar-on-function``\ `{=latex}`regression``\ `{=latex}`model`\
`set.seed(202)`\
`sim.data``\ `{=latex}`<-``\ `{=latex}`generate.sf.data(n``\ `{=latex}`=``\ `{=latex}`400,``\ `{=latex}`n.pred``\ `{=latex}`=``\ `{=latex}`5,``\ `{=latex}`n.gp``\ `{=latex}`=``\ `{=latex}`101,``\ `{=latex}`out.p``\ `{=latex}`=``\ `{=latex}`0.1)`\
`out.indx``\ `{=latex}`<-``\ `{=latex}`sim.data$out.indx`\
`#``\ `{=latex}`Response``\ `{=latex}`variable`\
`Y``\ `{=latex}`<-``\ `{=latex}`sim.data$Y`\
`#``\ `{=latex}`Predictors`\
`X``\ `{=latex}`<-``\ `{=latex}`sim.data$X`

`#``\ `{=latex}`Split``\ `{=latex}`the``\ `{=latex}`data``\ `{=latex}`into``\ `{=latex}`training``\ `{=latex}`and``\ `{=latex}`test``\ `{=latex}`samples.`\
`indx.test``\ `{=latex}`<-``\ `{=latex}`sample(c(1:400)[-out.indx],``\ `{=latex}`120)`\
`indx.train``\ `{=latex}`<-``\ `{=latex}`c(1:400)[-indx.test]`

`Y.train``\ `{=latex}`<-``\ `{=latex}`Y[indx.train,]`\
`Y.test``\ `{=latex}`<-``\ `{=latex}`Y[indx.test,]`\
`X.train``\ `{=latex}`<-``\ `{=latex}`X.test``\ `{=latex}`<-``\ `{=latex}`list()`\
`for(i``\ `{=latex}`in``\ `{=latex}`1:5)`\
`X.train[[i]]``\ `{=latex}`<-``\ `{=latex}`X[[i]][indx.train,]`\
`X.test[[i]]``\ `{=latex}`<-``\ `{=latex}`X[[i]][indx.test,]`\

`gp``\ `{=latex}`<-``\ `{=latex}`rep(list(seq(0,``\ `{=latex}`1,``\ `{=latex}`length.out``\ `{=latex}`=``\ `{=latex}`101)),``\ `{=latex}`5)``\ `{=latex}`#``\ `{=latex}`grid``\ `{=latex}`points``\ `{=latex}`of``\ `{=latex}`Xs`

`#``\ `{=latex}`Perform``\ `{=latex}`classical``\ `{=latex}`functional``\ `{=latex}`principal``\ `{=latex}`component``\ `{=latex}`regression``\ `{=latex}`model``\ `{=latex}`using``\ `{=latex}`training``\ `{=latex}`samples`\
`model.classical``\ `{=latex}`<-``\ `{=latex}`rob.sf.reg(Y.train,``\ `{=latex}`X.train,``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"classical",``\ `{=latex}`gp``\ `{=latex}`=``\ `{=latex}`gp)`\
`#``\ `{=latex}`Perform``\ `{=latex}`robust``\ `{=latex}`functional``\ `{=latex}`principal``\ `{=latex}`component``\ `{=latex}`regression``\ `{=latex}`model`\
`#``\ `{=latex}`using``\ `{=latex}`training``\ `{=latex}`samples``\ `{=latex}`and``\ `{=latex}`tau-estimator`\
`model.tau``\ `{=latex}`<-``\ `{=latex}`rob.sf.reg(Y.train,``\ `{=latex}`X.train,``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"robust",``\ `{=latex}`fmodel``\ `{=latex}`=``\ `{=latex}`"tau",``\ `{=latex}`gp``\ `{=latex}`=``\ `{=latex}`gp)`\
`#``\ `{=latex}`Predict``\ `{=latex}`the``\ `{=latex}`observations``\ `{=latex}`in``\ `{=latex}`Y.test``\ `{=latex}`using``\ `{=latex}`model.classical`\
`pred.classical``\ `{=latex}`<-``\ `{=latex}`predict_sf_regression(object``\ `{=latex}`=``\ `{=latex}`model.classical,``\ `{=latex}`Xnew``\ `{=latex}`=``\ `{=latex}`X.test)`\
`#``\ `{=latex}`Predict``\ `{=latex}`the``\ `{=latex}`observations``\ `{=latex}`in``\ `{=latex}`Y.test``\ `{=latex}`using``\ `{=latex}`model.tau`\
`pred.tau``\ `{=latex}`<-``\ `{=latex}`predict_sf_regression(object``\ `{=latex}`=``\ `{=latex}`model.tau,``\ `{=latex}`Xnew``\ `{=latex}`=``\ `{=latex}`X.test)`\
`#``\ `{=latex}`Compute``\ `{=latex}`mean``\ `{=latex}`squared``\ `{=latex}`errors``\ `{=latex}`for``\ `{=latex}`the``\ `{=latex}`test``\ `{=latex}`sample`\
`round(mean((Y.test``\ `{=latex}`-``\ `{=latex}`pred.classical)^2),``\ `{=latex}`4)``\ `{=latex}`#``\ `{=latex}`2.49``\ `{=latex}`(classical``\ `{=latex}`method)`\
`round(mean((Y.test``\ `{=latex}`-``\ `{=latex}`pred.tau)^2),``\ `{=latex}`4)``\ `{=latex}`#``\ `{=latex}`1.1457``\ `{=latex}`(tau``\ `{=latex}`method)`

## Prediction for the FFRM {#prediction-for-the-ffrm .unnumbered}

In the robust prediction of the FFRM for a given new set of functional
predictors, as in the scalar-on-function regression case, the principal
component scores of the new set of functional predictors are first
obtained:
$$\xi_{pk}^* = \int_0^1 \mathcal{X}^*_{pk}(s) \widehat{\psi}_{pk}(s) ds,$$
where $\widehat{\psi}_{pk}(s)$ is the $k$-th eigenfunction of the
$p$-the functional predictor obtained by the RFPCA. Then, the
predictions of functional response ($\widehat{\mathcal{Y}}(t)$)
corresponding to the new set of functional predictors are obtained as
follows:
$$\widehat{\mathcal{Y}}^*(t) = \sum_{k=1}^K \left(\sum_{p=1}^P \sum_{j=1}^{K_p} \widehat{b}_{pkj} \xi_{pj}^* \right) \widehat{\phi}_k(t),$$
where $\widehat{\phi}_k(t)$ is the $k$-th eigenfunction of the
functional response obtained by RFPCA and $\widehat{b}_{pkj}$ is the
estimated parameter matrix obtained from the fitted model
`rob.ff.reg()`.

### Main function for the robust prediction of a FFRM and its arguments {#main-function-for-the-robust-prediction-of-a-ffrm-and-its-arguments .unnumbered}

The main function for the robust prediction of a FFRM is called
`predict_ff_regression()`:

`predict_ff_regression(object,``\ `{=latex}`Xnew)`

Here, the argument `object` is an output object obtained from
`rob.ff.reg`. The new set of functional predictors is provided in the
`Xnew` argument as a list object whose $p$-th element is a matrix
denoting the new observations of $\mathcal{X}_p(s)$. `Xnew` must have
the same length and the same structure as the input `X` of `rob.ff.reg`.

We simulate a dataset with size $n = 200$ for the FFRM to investigate
and compare the prediction performance of the classical and robust
methods. The simulated data are divided into a training sample with a
size of 140 and a test sample with a size of 60. Random outliers
contaminate the training sample, and both the classical and robust
methods with MM estimator are applied to the training sample to predict
the values of the response variable in the test sample. To compare both
methods, we compute the following MSPE:
$$\text{MSPE} = \frac{1}{100} \sum_{i=1}^{200} \Vert \mathcal{Y}_i^*(t) - \widehat{\mathcal{Y}}_i^*(t)) \Vert^2_{\mathcal{L}_2},$$
where $\mathcal{Y}_i^*(t)$ and $\widehat{\mathcal{Y}}_i^*(t))$ denote
the observed and predicted values of the functional response in the test
sample. Our results show that the robust method produces a significantly
smaller MSPE value than the classical method. The MSPE computed from the
classical method is 3.3213, while the MSPE obtained from the robust
method is 0.5925. The reproducible code to obtain those results is as
follows:

`library(robflreg)`\
`#``\ `{=latex}`Generate``\ `{=latex}`a``\ `{=latex}`dataset``\ `{=latex}`with``\ `{=latex}`five``\ `{=latex}`functional``\ `{=latex}`predictors``\ `{=latex}`and``\ `{=latex}`200`\
`#``\ `{=latex}`observations``\ `{=latex}`at``\ `{=latex}`101``\ `{=latex}`equally``\ `{=latex}`spaced``\ `{=latex}`point``\ `{=latex}`in``\ `{=latex}`the``\ `{=latex}`interval``\ `{=latex}`[0,``\ `{=latex}`1]`\
`#``\ `{=latex}`for``\ `{=latex}`each``\ `{=latex}`variable``\ `{=latex}`for``\ `{=latex}`the``\ `{=latex}`function-on-function``\ `{=latex}`regression``\ `{=latex}`model`\
`set.seed(202)`\
`sim.data``\ `{=latex}`<-``\ `{=latex}`generate.ff.data(n.pred``\ `{=latex}`=``\ `{=latex}`5,``\ `{=latex}`n.curve``\ `{=latex}`=``\ `{=latex}`200,``\ `{=latex}`n.gp``\ `{=latex}`=``\ `{=latex}`101,``\ `{=latex}`out.p``\ `{=latex}`=``\ `{=latex}`0.1)`\
`out.indx``\ `{=latex}`<-``\ `{=latex}`sim.data$out.indx`\
`#``\ `{=latex}`Response``\ `{=latex}`variable`\
`Y``\ `{=latex}`<-``\ `{=latex}`sim.data$Y`\
`#``\ `{=latex}`Predictor``\ `{=latex}`variables`\
`X``\ `{=latex}`<-``\ `{=latex}`sim.data$X`\
`#``\ `{=latex}`Split``\ `{=latex}`the``\ `{=latex}`data``\ `{=latex}`into``\ `{=latex}`training``\ `{=latex}`and``\ `{=latex}`test``\ `{=latex}`samples.`\
`indx.test``\ `{=latex}`<-``\ `{=latex}`sample(c(1:200)[-out.indx],``\ `{=latex}`60)`\
`indx.train``\ `{=latex}`<-``\ `{=latex}`c(1:200)[-indx.test]`\
`Y.train``\ `{=latex}`<-``\ `{=latex}`Y[indx.train,]`\
`Y.test``\ `{=latex}`<-``\ `{=latex}`Y[indx.test,]`\
`X.train``\ `{=latex}`<-``\ `{=latex}`X.test``\ `{=latex}`<-``\ `{=latex}`list()`\
`for(i``\ `{=latex}`in``\ `{=latex}`1:5)`\
`X.train[[i]]``\ `{=latex}`<-``\ `{=latex}`X[[i]][indx.train,]`\
`X.test[[i]]``\ `{=latex}`<-``\ `{=latex}`X[[i]][indx.test,]`\

`gpY``\ `{=latex}`=``\ `{=latex}`seq(0,``\ `{=latex}`1,``\ `{=latex}`length.out``\ `{=latex}`=``\ `{=latex}`101)``\ `{=latex}`#``\ `{=latex}`grid``\ `{=latex}`points``\ `{=latex}`of``\ `{=latex}`Y`\
`gpX``\ `{=latex}`<-``\ `{=latex}`rep(list(seq(0,``\ `{=latex}`1,``\ `{=latex}`length.out``\ `{=latex}`=``\ `{=latex}`101)),``\ `{=latex}`5)``\ `{=latex}`#``\ `{=latex}`grid``\ `{=latex}`points``\ `{=latex}`of``\ `{=latex}`Xs`

`#``\ `{=latex}`Perform``\ `{=latex}`classical``\ `{=latex}`functional``\ `{=latex}`principal``\ `{=latex}`component``\ `{=latex}`regression``\ `{=latex}`model``\ `{=latex}`using``\ `{=latex}`training``\ `{=latex}`samples`\
`model.classical``\ `{=latex}`<-``\ `{=latex}`rob.ff.reg(Y``\ `{=latex}`=``\ `{=latex}`Y.train,``\ `{=latex}`X``\ `{=latex}`=``\ `{=latex}`X.train,``\ `{=latex}`model``\ `{=latex}`=``\ `{=latex}`"full",`\
`emodel``\ `{=latex}`=``\ `{=latex}`"classical",``\ `{=latex}`gpY``\ `{=latex}`=``\ `{=latex}`gpY,``\ `{=latex}`gpX``\ `{=latex}`=``\ `{=latex}`gpX)`\
`#``\ `{=latex}`Perform``\ `{=latex}`robust``\ `{=latex}`functional``\ `{=latex}`principal``\ `{=latex}`component``\ `{=latex}`regression`\
`#``\ `{=latex}`using``\ `{=latex}`training``\ `{=latex}`samples``\ `{=latex}`and``\ `{=latex}`MM-estimator`\
`model.MM``\ `{=latex}`<-``\ `{=latex}`rob.ff.reg(Y``\ `{=latex}`=``\ `{=latex}`Y.train,``\ `{=latex}`X``\ `{=latex}`=``\ `{=latex}`X.train,``\ `{=latex}`model``\ `{=latex}`=``\ `{=latex}`"full",``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"robust",`\
`fmodel``\ `{=latex}`=``\ `{=latex}`"MM",``\ `{=latex}`gpY``\ `{=latex}`=``\ `{=latex}`gpY,``\ `{=latex}`gpX``\ `{=latex}`=``\ `{=latex}`gpX)`\
`#``\ `{=latex}`Predict``\ `{=latex}`the``\ `{=latex}`functions``\ `{=latex}`in``\ `{=latex}`Y.test``\ `{=latex}`using``\ `{=latex}`model.classical`\
`pred.classical``\ `{=latex}`<-``\ `{=latex}`predict_ff_regression(object``\ `{=latex}`=``\ `{=latex}`model.classical,``\ `{=latex}`Xnew``\ `{=latex}`=``\ `{=latex}`X.test)`\
`#``\ `{=latex}`Predict``\ `{=latex}`the``\ `{=latex}`functions``\ `{=latex}`in``\ `{=latex}`Y.test``\ `{=latex}`using``\ `{=latex}`model.MM`\
`pred.MM``\ `{=latex}`<-``\ `{=latex}`predict_ff_regression(object``\ `{=latex}`=``\ `{=latex}`model.MM,``\ `{=latex}`Xnew``\ `{=latex}`=``\ `{=latex}`X.test)`\
`#``\ `{=latex}`Compute``\ `{=latex}`mean``\ `{=latex}`squared``\ `{=latex}`errors``\ `{=latex}`for``\ `{=latex}`the``\ `{=latex}`test``\ `{=latex}`sample`\
`round(mean((Y.test``\ `{=latex}`-``\ `{=latex}`pred.classical)^2),``\ `{=latex}`4)``\ `{=latex}`#``\ `{=latex}`1.5705``\ `{=latex}`(classical``\ `{=latex}`method)`\
`round(mean((Y.test``\ `{=latex}`-``\ `{=latex}`pred.MM)^2),``\ `{=latex}`4)``\ `{=latex}`#``\ `{=latex}`0.8166``\ `{=latex}`(MM``\ `{=latex}`method)`

# Data analysis

We consider the MaryRiverFlow dataset available in the robflreg package
to present the superiority of the robust functional principal component
regression models over the classical model when the dataset includes
outliers. The MaryRiverFlow dataset consists of hourly river-flow
measurements obtained from January 2009 to December 2014 (6 years in
total) in the Mary River, Australia. River-flow time series varies
throughout the years due to the variation of the seasons and the amount
of rainfall received at particular catchments. This problem still needs
to be solved in the hydrological domain to be addressed appropriately
using theoretical models.

Our first aim with the MaryRiverFlow dataset is to assess how the
previous river flow curve time series, $\mathcal{X}_i(s)$, affects the
current day's maximum river flow, $Y_i$. Here, the observations of
predictor are assumed to be functions of hours, i.e., there are 2188
functional observations
$\mathcal{X}_i(t) \big(1\leq t\leq 24$,$~ i=1, \ldots, 2188 \big)$ while
the response is a scalar predictor. A graphical display of the response
and predictor is presented in Figure [7](#fig:7){reference-type="ref"
reference="fig:7"}. From this figure, both the scalar response and
functional predictor include clear outliers, which motivates us to apply
the robust functional principal component regression models to better
model this dataset.

<figure id="fig:7">
<div class="center">
<p><img src="robflreg-022" style="width:49.0%" alt="image" /> <img
src="robflreg-023" style="width:49.0%" alt="image" /></p>
</div>
<figcaption>Plot of the daily maximum river flow measurement (left
panel) and the functional time series plot (right panel) of the flow
level in the Mary River.</figcaption>
</figure>

Figure [7](#fig:7){reference-type="ref" reference="fig:7"} can be
produced by the following code:

`library(robflreg)`\
`library(fda.usc)`\
`data("MaryRiverFlow")`\
`X``\ `{=latex}`<-``\ `{=latex}`MaryRiverFlow[1:2188,]`\
`Y``\ `{=latex}`<-``\ `{=latex}`apply(MaryRiverFlow[2:2189,],``\ `{=latex}`1,``\ `{=latex}`max)`\
`Day``\ `{=latex}`<-``\ `{=latex}`seq(as.Date("2009/01/01"),``\ `{=latex}`as.Date("2014/12/28"),``\ `{=latex}`by="days")`\
`plot(Day,``\ `{=latex}`Y,``\ `{=latex}`type``\ `{=latex}`=``\ `{=latex}`"p",``\ `{=latex}`pch``\ `{=latex}`=``\ `{=latex}`16,``\ `{=latex}`ylab``\ `{=latex}`=``\ `{=latex}`"Level``\ `{=latex}`(m)",``\ `{=latex}`main``\ `{=latex}`=``\ `{=latex}`"Response")`\
`X``\ `{=latex}`<-``\ `{=latex}`fdata(X,``\ `{=latex}`argvals``\ `{=latex}`=``\ `{=latex}`1:24)`\
`plot(X,``\ `{=latex}`lty``\ `{=latex}`=``\ `{=latex}`1,``\ `{=latex}`ylab``\ `{=latex}`=``\ `{=latex}`"",``\ `{=latex}`xlab``\ `{=latex}`=``\ `{=latex}`"Hour",``\ `{=latex}`col``\ `{=latex}`=``\ `{=latex}`"black",`\
`main``\ `{=latex}`=``\ `{=latex}`"Predictor",``\ `{=latex}`mgp``\ `{=latex}`=``\ `{=latex}`c(2,``\ `{=latex}`0.5,``\ `{=latex}`0))`

We assume the SFLRM,
$Y_i = \beta_0 + \int \mathcal{X}_i(s) \beta(s) ds$, where $Y_i$ is the
maximum river flow measurement for the current day and
$\mathcal{X}_i(s)$ is the true river flow measurements recorded in the
previous day. An expanding-window approach is considered to compare the
predictive performance of the robust methods with the classical one.
While doing so, The entire data are divided into two parts: a training
sample containing the days from 01/01/2009 to 20/12/2014 and a test
sample including days from 21/12/2014 to 30/12/2014. First, the models
are constructed using the entire training data to forecast the maximum
river flow measurement on 21/12/2014. Then, the maximum river flow
measurement is forecasted by increasing the training samples by one day.
This procedure is repeated until the training samples cover the entire
dataset. The MSPE is computed for each method, and the computed mean
MSPEs ($\times 10^{-3}$) along with standard errors ($\times 10^{-3}$
given in bracket) are 7.559 (5.674), 1.635 (1.303), 1.671 (1.376), 1.617
(1.274), and 1.617 (1.274) for the classical, LTS, MM, S, and
tau-estimator based SFLRM, respectively. From the results, all the
robust methods produce significantly smaller MSPE values than the
classical method. These results can be obtained by the following code:

`library(robflreg)`\
`data("MaryRiverFlow")`\
`MaryRiverFlow``\ `{=latex}`<-``\ `{=latex}`as.matrix(MaryRiverFlow)`\
`X``\ `{=latex}`<-``\ `{=latex}`list(MaryRiverFlow[1:2188,])`\
`Y``\ `{=latex}`<-``\ `{=latex}`apply(MaryRiverFlow[2:2189,],``\ `{=latex}`1,``\ `{=latex}`max)`\
`gp``\ `{=latex}`<-``\ `{=latex}`rep(list(1:24),``\ `{=latex}`1)`

`MSPE``\ `{=latex}`<-``\ `{=latex}`matrix(,``\ `{=latex}`ncol``\ `{=latex}`=``\ `{=latex}`5,``\ `{=latex}`nrow``\ `{=latex}`=``\ `{=latex}`10)`\
`colnames(MSPE)``\ `{=latex}`<-``\ `{=latex}`c("classical","LTS","MM","S","tau")`\
`starting_value``\ `{=latex}`<-``\ `{=latex}`2178`

`for(i``\ `{=latex}`in``\ `{=latex}`1:10)`

`#``\ `{=latex}`Divide``\ `{=latex}`the``\ `{=latex}`data``\ `{=latex}`into``\ `{=latex}`training``\ `{=latex}`and``\ `{=latex}`test``\ `{=latex}`samples`\
`Y.train``\ `{=latex}`<-``\ `{=latex}`Y[1:starting_value]`\
`Y.test``\ `{=latex}`<-``\ `{=latex}`Y[(starting_value+1)]`

`X.train``\ `{=latex}`<-``\ `{=latex}`list(X[[1]][1:starting_value,])`\
`X.test``\ `{=latex}`<-``\ `{=latex}`list(matrix(X[[1]][(starting_value+1),],``\ `{=latex}`nrow``\ `{=latex}`=``\ `{=latex}`1))`

`#``\ `{=latex}`Perform``\ `{=latex}`classical``\ `{=latex}`and``\ `{=latex}`robust``\ `{=latex}`functional``\ `{=latex}`principal``\ `{=latex}`component``\ `{=latex}`regression``\ `{=latex}`models`\
`model.classical``\ `{=latex}`<-``\ `{=latex}`rob.sf.reg(Y.train,``\ `{=latex}`X.train,``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"classical",``\ `{=latex}`gp``\ `{=latex}`=``\ `{=latex}`gp)`\
`model.LTS``\ `{=latex}`<-``\ `{=latex}`rob.sf.reg(Y.train,``\ `{=latex}`X.train,``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"robust",``\ `{=latex}`fmodel``\ `{=latex}`=``\ `{=latex}`"LTS",``\ `{=latex}`gp``\ `{=latex}`=``\ `{=latex}`gp)`\
`model.MM``\ `{=latex}`<-``\ `{=latex}`rob.sf.reg(Y.train,``\ `{=latex}`X.train,``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"robust",``\ `{=latex}`fmodel``\ `{=latex}`=``\ `{=latex}`"MM",``\ `{=latex}`gp``\ `{=latex}`=``\ `{=latex}`gp)`\
`model.S``\ `{=latex}`<-``\ `{=latex}`rob.sf.reg(Y.train,``\ `{=latex}`X.train,``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"robust",``\ `{=latex}`fmodel``\ `{=latex}`=``\ `{=latex}`"S",``\ `{=latex}`gp``\ `{=latex}`=``\ `{=latex}`gp)`\
`model.tau``\ `{=latex}`<-``\ `{=latex}`rob.sf.reg(Y.train,``\ `{=latex}`X.train,``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"robust",``\ `{=latex}`fmodel``\ `{=latex}`=``\ `{=latex}`"tau",``\ `{=latex}`gp``\ `{=latex}`=``\ `{=latex}`gp)`

`#``\ `{=latex}`Predict``\ `{=latex}`the``\ `{=latex}`maximum``\ `{=latex}`river``\ `{=latex}`flow``\ `{=latex}`measurement``\ `{=latex}`of``\ `{=latex}`the``\ `{=latex}`current``\ `{=latex}`day`\
`pred.classical``\ `{=latex}`<-``\ `{=latex}`predict_sf_regression(object``\ `{=latex}`=``\ `{=latex}`model.classical,``\ `{=latex}`Xnew``\ `{=latex}`=``\ `{=latex}`X.test)`\
`pred.LST``\ `{=latex}`<-``\ `{=latex}`predict_sf_regression(object``\ `{=latex}`=``\ `{=latex}`model.LTS,``\ `{=latex}`Xnew``\ `{=latex}`=``\ `{=latex}`X.test)`\
`pred.MM``\ `{=latex}`<-``\ `{=latex}`predict_sf_regression(object``\ `{=latex}`=``\ `{=latex}`model.MM,``\ `{=latex}`Xnew``\ `{=latex}`=``\ `{=latex}`X.test)`\
`pred.S``\ `{=latex}`<-``\ `{=latex}`predict_sf_regression(object``\ `{=latex}`=``\ `{=latex}`model.tau,``\ `{=latex}`Xnew``\ `{=latex}`=``\ `{=latex}`X.test)`\
`pred.tau``\ `{=latex}`<-``\ `{=latex}`predict_sf_regression(object``\ `{=latex}`=``\ `{=latex}`model.tau,``\ `{=latex}`Xnew``\ `{=latex}`=``\ `{=latex}`X.test)`

`#``\ `{=latex}`Record``\ `{=latex}`the``\ `{=latex}`MSPE``\ `{=latex}`values`\
`MSPE[i,1]``\ `{=latex}`<-``\ `{=latex}`(Y.test``\ `{=latex}`-``\ `{=latex}`pred.classical)^2`\
`MSPE[i,2]``\ `{=latex}`<-``\ `{=latex}`(Y.test``\ `{=latex}`-``\ `{=latex}`pred.LST)^2`\
`MSPE[i,3]``\ `{=latex}`<-``\ `{=latex}`(Y.test``\ `{=latex}`-``\ `{=latex}`pred.MM)^2`\
`MSPE[i,4]``\ `{=latex}`<-``\ `{=latex}`(Y.test``\ `{=latex}`-``\ `{=latex}`pred.S)^2`\
`MSPE[i,5]``\ `{=latex}`<-``\ `{=latex}`(Y.test``\ `{=latex}`-``\ `{=latex}`pred.tau)^2`\
`starting_value``\ `{=latex}`<-``\ `{=latex}`starting_value``\ `{=latex}`+``\ `{=latex}`1`

`apply(MSPE,``\ `{=latex}`2,``\ `{=latex}`mean);``\ `{=latex}`apply(MSPE,``\ `{=latex}`2,``\ `{=latex}`sd)`

Our second aim with the MaryRiverFlow dataset is to assess how the
previous river flow curve time series, $\mathcal{X}_i(s)$ affects the
current day's river flow curve time series, $\mathcal{Y}_i(t)$. In this
case, the elements of both the response and predictor are assumed to be
functions of hours. Here, we assume the FFLRM,
$\mathcal{Y}_i(t) = \beta_0(t) + \int \mathcal{X}_i(s) \beta(s,t) ds dt$.
The similar expanding-window approach used in the SFLRM example is
considered, i.e., the entire dataset is divided into two parts: a
training sample containing the days from 01/01/2009 to 20/12/2014 and a
test sample including days from 21/12/2014 to 30/12/2014. The functional
principal component regression models are constructed using the entire
training data to forecast the river flow curve time series on
21/12/2014. Then, the river flow curve time series is forecasted by
increasing the training samples by one day. This procedure is repeated
until the training samples cover the entire dataset. The MSPE is
computed for each method, and the computed mean MSPEs ($\times 10^{-3}$)
along with standard errors ($\times 10^{-3}$ given in bracket) are 5.721
(3.884), 1.565 (0.965), 1.377 (0.847), 1.546 (0.949), 1.511 (0.920), and
1.377 (0.849) for the classical, MCD, MLTS, MM, S, and tau-estimator
based FFLRMs, respectively. From the results, the robust methods produce
improved MSPEs over the classical FFLRM, i.e., the robust method models
the MaryRiverFlow data better than the classical functional principal
component regression model. These results can be obtained by the
following code:

`library(robflreg)`\
`data("MaryRiverFlow")`\
`MaryRiverFlow``\ `{=latex}`<-``\ `{=latex}`as.matrix(MaryRiverFlow)`

`#``\ `{=latex}`The``\ `{=latex}`following``\ `{=latex}`function``\ `{=latex}`is``\ `{=latex}`used``\ `{=latex}`to``\ `{=latex}`obtain``\ `{=latex}`the``\ `{=latex}`functional``\ `{=latex}`response``\ `{=latex}`and``\ `{=latex}`predictor`\
`var_fun``\ `{=latex}`=``\ `{=latex}`function(data,order)`\
`n``\ `{=latex}`=``\ `{=latex}`dim(data)[1]`\
`y``\ `{=latex}`=``\ `{=latex}`data[((order+1):n),]`\
`x=list()`\
`a=1`\
`b=order`\
`for(i``\ `{=latex}`in``\ `{=latex}`1:order)`\
`x[[i]]``\ `{=latex}`=``\ `{=latex}`data[(a:(n-b)),]`\
`a``\ `{=latex}`=``\ `{=latex}`a+1`\
`b``\ `{=latex}`=``\ `{=latex}`b-1`\
\
`return(list(x=x,y=y))`\

`#``\ `{=latex}`Grid``\ `{=latex}`points``\ `{=latex}`for``\ `{=latex}`the``\ `{=latex}`functional``\ `{=latex}`response``\ `{=latex}`and``\ `{=latex}`predictor`\
`gpY``\ `{=latex}`<-``\ `{=latex}`1:24``\ `{=latex}`#``\ `{=latex}`grid``\ `{=latex}`points``\ `{=latex}`of``\ `{=latex}`Y`\
`gpX``\ `{=latex}`<-``\ `{=latex}`rep(list(1:24),``\ `{=latex}`1)``\ `{=latex}`#``\ `{=latex}`grid``\ `{=latex}`points``\ `{=latex}`of``\ `{=latex}`Xs`

`MSPE``\ `{=latex}`<-``\ `{=latex}`matrix(,``\ `{=latex}`ncol``\ `{=latex}`=``\ `{=latex}`6,``\ `{=latex}`nrow``\ `{=latex}`=``\ `{=latex}`10)`\
`colnames(MSPE)``\ `{=latex}`<-``\ `{=latex}`c("classical","MCD","MLTS","MM","S","tau")`\
`starting_value``\ `{=latex}`<-``\ `{=latex}`2179`\
`#``\ `{=latex}`In``\ `{=latex}`two``\ `{=latex}`cases``\ `{=latex}`(when``\ `{=latex}`i``\ `{=latex}`=``\ `{=latex}`3``\ `{=latex}`and``\ `{=latex}`i``\ `{=latex}`=``\ `{=latex}`6)``\ `{=latex}`the``\ `{=latex}`covariance``\ `{=latex}`is``\ `{=latex}`not``\ `{=latex}`decomposed.`\
`#``\ `{=latex}`Thus,``\ `{=latex}`try()``\ `{=latex}`is``\ `{=latex}`used``\ `{=latex}`to``\ `{=latex}`ignore``\ `{=latex}`these``\ `{=latex}`two``\ `{=latex}`cases.`\
`for(i``\ `{=latex}`in``\ `{=latex}`1:10)`

`try(`

`data.i``\ `{=latex}`<-``\ `{=latex}`MaryRiverFlow[1:starting_value,]`\
`#``\ `{=latex}`Obtain``\ `{=latex}`the``\ `{=latex}`functional``\ `{=latex}`response``\ `{=latex}`and``\ `{=latex}`predictor`\
`XY``\ `{=latex}`=``\ `{=latex}`var_fun(data=data.i,``\ `{=latex}`order=1)`\
`X``\ `{=latex}`=``\ `{=latex}`XY`$x
  Y = XY$`y`

`#``\ `{=latex}`Perform``\ `{=latex}`classical``\ `{=latex}`and``\ `{=latex}`robust``\ `{=latex}`functional``\ `{=latex}`principal``\ `{=latex}`component``\ `{=latex}`regression``\ `{=latex}`models`\
`model.classical``\ `{=latex}`<-``\ `{=latex}`rob.ff.reg(Y``\ `{=latex}`=``\ `{=latex}`Y,``\ `{=latex}`X``\ `{=latex}`=``\ `{=latex}`X,``\ `{=latex}`model``\ `{=latex}`=``\ `{=latex}`"full",``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"classical",`\
`gpY``\ `{=latex}`=``\ `{=latex}`gpY,``\ `{=latex}`gpX``\ `{=latex}`=``\ `{=latex}`gpX)`\
`model.MCD``\ `{=latex}`<-``\ `{=latex}`rob.ff.reg(Y``\ `{=latex}`=``\ `{=latex}`Y,``\ `{=latex}`X``\ `{=latex}`=``\ `{=latex}`X,``\ `{=latex}`model``\ `{=latex}`=``\ `{=latex}`"full",``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"robust",`\
`fmodel``\ `{=latex}`=``\ `{=latex}`"MCD",``\ `{=latex}`gpY``\ `{=latex}`=``\ `{=latex}`gpY,``\ `{=latex}`gpX``\ `{=latex}`=``\ `{=latex}`gpX)`\
`model.MLTS``\ `{=latex}`<-``\ `{=latex}`rob.ff.reg(Y``\ `{=latex}`=``\ `{=latex}`Y,``\ `{=latex}`X``\ `{=latex}`=``\ `{=latex}`X,``\ `{=latex}`model``\ `{=latex}`=``\ `{=latex}`"full",``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"robust",`\
`fmodel``\ `{=latex}`=``\ `{=latex}`"MLTS",``\ `{=latex}`gpY``\ `{=latex}`=``\ `{=latex}`gpY,``\ `{=latex}`gpX``\ `{=latex}`=``\ `{=latex}`gpX)`\
`model.MM``\ `{=latex}`<-``\ `{=latex}`rob.ff.reg(Y``\ `{=latex}`=``\ `{=latex}`Y,``\ `{=latex}`X``\ `{=latex}`=``\ `{=latex}`X,``\ `{=latex}`model``\ `{=latex}`=``\ `{=latex}`"full",``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"robust",`\
`fmodel``\ `{=latex}`=``\ `{=latex}`"MM",``\ `{=latex}`gpY``\ `{=latex}`=``\ `{=latex}`gpY,``\ `{=latex}`gpX``\ `{=latex}`=``\ `{=latex}`gpX)`\
`model.S``\ `{=latex}`<-``\ `{=latex}`rob.ff.reg(Y``\ `{=latex}`=``\ `{=latex}`Y,``\ `{=latex}`X``\ `{=latex}`=``\ `{=latex}`X,``\ `{=latex}`model``\ `{=latex}`=``\ `{=latex}`"full",``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"robust",`\
`fmodel``\ `{=latex}`=``\ `{=latex}`"S",``\ `{=latex}`gpY``\ `{=latex}`=``\ `{=latex}`gpY,``\ `{=latex}`gpX``\ `{=latex}`=``\ `{=latex}`gpX)`\
`model.tau``\ `{=latex}`<-``\ `{=latex}`rob.ff.reg(Y``\ `{=latex}`=``\ `{=latex}`Y,``\ `{=latex}`X``\ `{=latex}`=``\ `{=latex}`X,``\ `{=latex}`model``\ `{=latex}`=``\ `{=latex}`"full",``\ `{=latex}`emodel``\ `{=latex}`=``\ `{=latex}`"robust",`\
`fmodel``\ `{=latex}`=``\ `{=latex}`"tau",``\ `{=latex}`gpY``\ `{=latex}`=``\ `{=latex}`gpY,``\ `{=latex}`gpX``\ `{=latex}`=``\ `{=latex}`gpX)`\
`Xnew``\ `{=latex}`=``\ `{=latex}`list(t(as.matrix(Y[dim(Y)[1],])))`

`#``\ `{=latex}`Predict``\ `{=latex}`the``\ `{=latex}`maximum``\ `{=latex}`river``\ `{=latex}`flow``\ `{=latex}`measurement``\ `{=latex}`of``\ `{=latex}`the``\ `{=latex}`current``\ `{=latex}`day`\
`predict.classical``\ `{=latex}`<-``\ `{=latex}`predict_ff_regression(object``\ `{=latex}`=``\ `{=latex}`model.classical,``\ `{=latex}`Xnew``\ `{=latex}`=``\ `{=latex}`Xnew)`\
`predict.MCD``\ `{=latex}`<-``\ `{=latex}`predict_ff_regression(object``\ `{=latex}`=``\ `{=latex}`model.MCD,``\ `{=latex}`Xnew``\ `{=latex}`=``\ `{=latex}`Xnew)`\
`predict.MLTS``\ `{=latex}`<-``\ `{=latex}`predict_ff_regression(object``\ `{=latex}`=``\ `{=latex}`model.MLTS,``\ `{=latex}`Xnew``\ `{=latex}`=``\ `{=latex}`Xnew)`\
`predict.MM``\ `{=latex}`<-``\ `{=latex}`predict_ff_regression(object``\ `{=latex}`=``\ `{=latex}`model.MM,``\ `{=latex}`Xnew``\ `{=latex}`=``\ `{=latex}`Xnew)`\
`predict.S``\ `{=latex}`<-``\ `{=latex}`predict_ff_regression(object``\ `{=latex}`=``\ `{=latex}`model.S,``\ `{=latex}`Xnew``\ `{=latex}`=``\ `{=latex}`Xnew)`\
`predict.tau``\ `{=latex}`<-``\ `{=latex}`predict_ff_regression(object``\ `{=latex}`=``\ `{=latex}`model.tau,``\ `{=latex}`Xnew``\ `{=latex}`=``\ `{=latex}`Xnew)`

`#``\ `{=latex}`Record``\ `{=latex}`the``\ `{=latex}`MSPE``\ `{=latex}`values`\
`MSPE[i,1]``\ `{=latex}`<-``\ `{=latex}`mean((predict.classical``\ `{=latex}`-``\ `{=latex}`MaryRiverFlow[starting_value+1,])^2)`\
`MSPE[i,2]``\ `{=latex}`<-``\ `{=latex}`mean((predict.MCD``\ `{=latex}`-``\ `{=latex}`MaryRiverFlow[starting_value+1,])^2)`\
`MSPE[i,3]``\ `{=latex}`<-``\ `{=latex}`mean((predict.MLTS``\ `{=latex}`-``\ `{=latex}`MaryRiverFlow[starting_value+1,])^2)`\
`MSPE[i,4]``\ `{=latex}`<-``\ `{=latex}`mean((predict.MM``\ `{=latex}`-``\ `{=latex}`MaryRiverFlow[starting_value+1,])^2)`\
`MSPE[i,5]``\ `{=latex}`<-``\ `{=latex}`mean((predict.S``\ `{=latex}`-``\ `{=latex}`MaryRiverFlow[starting_value+1,])^2)`\
`MSPE[i,6]``\ `{=latex}`<-``\ `{=latex}`mean((predict.tau``\ `{=latex}`-``\ `{=latex}`MaryRiverFlow[starting_value+1,])^2)`\
`starting_value``\ `{=latex}`<-``\ `{=latex}`starting_value``\ `{=latex}`+1`

`,silent=T)`

`apply(MSPE,``\ `{=latex}`2,``\ `{=latex}`mean,``\ `{=latex}`na.rm=TRUE);``\ `{=latex}`apply(MSPE,``\ `{=latex}`2,``\ `{=latex}`sd,``\ `{=latex}`na.rm=TRUE)`

# Conclusion

The [R]{.sans-serif} package robflreg provides an implementation of
functional principal component regression model based on several robust
procedures to fit and predict SFLRM and FFLRM. These methods are
centered on the RFPCA of [@Bali2011], a popular robust dimension
reduction technique in functional data, and several robust regression
parameter estimators. In addition, the package robflreg allows us to fit
and predict SFLRM and FFLRM via the classical FPCA and least-squares
estimator. Several simulation examples and empirical data analysis show
that the robust procedures provide better inference for functional
linear regression models when outliers are present in the response and
predictor variables. The robflreg package code can be found at:
<https://github.com/cran/robflreg>.

The aspects that the current version of the `R` package robflreg that
may be improved upon in the future are listed below.

1.  In the current version, the scalar predictors are allowed in the
    SFLRM only. In the next versions of the package, it is planned to
    improve the functions `rob.ff.reg` and `predict_ff_regression` to
    include scalar predictors (whose effects are constant and/or vary
    along the continuum of the functional predictor).

2.  The current package version does not allow for modeling the
    function-on-scalar regression model, where the response consists of
    random functions, and the predictors include scalar observations.
    The robust procedures used in the FFLRM are planned to extend the
    function-on-scalar regression model in the subsequent versions of
    the package.

3.  In the current version, the observations of the functional variables
    are assumed to be densely observed. In the next versions of the
    package, the robust methods are planned to extend the models where
    the elements of functional variables can also be observed over
    irregular and curve-specific grids.

# Acknowledgement {#acknowledgement .unnumbered}

We thank three reviewers for their careful reading of our manuscript and
valuable suggestions and comments, which have helped us produce an
improved version of our manuscript and the `R` package. The first author
was supported by The Scientific and Technological Research Council of
Turkey (TUBITAK) (grant no: 120F270). The second author was partially
supported by an Australian Research Council Discovery Project (grant no:
DP230102250). This study is dedicated to the people who lost their lives
in the earthquake that occurred in Turkey on February 6, 2023.
