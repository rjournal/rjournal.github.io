<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { color: #00769e; background-color: #f1f3f5; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #00769e; } /* Normal */
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #657422; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #00769e; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #00769e; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #00769e; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #5e5e5e; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

<style>
  div.csl-bib-body { }
  div.csl-entry {
    clear: both;
      margin-bottom: 0em;
    }
  .hanging div.csl-entry {
    margin-left:2em;
    text-indent:-2em;
  }
  div.csl-left-margin {
    min-width:2em;
    float:left;
  }
  div.csl-right-inline {
    margin-left:2em;
    padding-left:1em;
  }
  div.csl-indent {
    margin-left: 2em;
  }
</style>

  <!--radix_placeholder_meta_tags-->
  <title>elhmc: An R Package for Hamiltonian Monte Carlo Sampling in Bayesian Empirical Likelihood</title>

  <meta property="description" itemprop="description" content="In this article, we describe an R package for sampling from an&#10;empirical likelihood-based posterior using a Hamiltonian Monte Carlo&#10;method. Empirical likelihood-based methodologies have been used in the&#10;Bayesian modeling of many problems of interest in recent times. This&#10;semiparametric procedure can easily combine the flexibility of a&#10;nonparametric distribution estimator together with the&#10;interpretability of a parametric model. The model is specified by&#10;estimating equation-based constraints. Drawing inference from a&#10;Bayesian empirical likelihood (BayesEL) posterior is challenging. The&#10;likelihood is computed numerically, so no closed-form expression of&#10;the posterior exists. Moreover, for any sample of finite size, the&#10;support of the likelihood is non-convex, which hinders fast mixing of&#10;many Markov Chain Monte Carlo (MCMC) procedures. It has been recently&#10;shown that using the properties of the gradient of the log empirical&#10;likelihood, one can devise an efficient Hamiltonian Monte Carlo (HMC)&#10;algorithm to sample from a BayesEL posterior. The package requires the&#10;user to specify only the estimating equations, the prior, and their&#10;respective gradients. An MCMC sample drawn from the BayesEL posterior&#10;of the parameters, with various details required by the user, is&#10;obtained."/>

  <link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>

  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2026-01-06"/>
  <meta property="article:created" itemprop="dateCreated" content="2026-01-06"/>
  <meta name="article:author" content="Neo Han Wei"/>
  <meta name="article:author" content="Dang Trung Kien"/>
  <meta name="article:author" content="Sanjay Chaudhuri"/>

  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="elhmc: An R Package for Hamiltonian Monte Carlo Sampling in Bayesian Empirical Likelihood"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="In this article, we describe an R package for sampling from an&#10;empirical likelihood-based posterior using a Hamiltonian Monte Carlo&#10;method. Empirical likelihood-based methodologies have been used in the&#10;Bayesian modeling of many problems of interest in recent times. This&#10;semiparametric procedure can easily combine the flexibility of a&#10;nonparametric distribution estimator together with the&#10;interpretability of a parametric model. The model is specified by&#10;estimating equation-based constraints. Drawing inference from a&#10;Bayesian empirical likelihood (BayesEL) posterior is challenging. The&#10;likelihood is computed numerically, so no closed-form expression of&#10;the posterior exists. Moreover, for any sample of finite size, the&#10;support of the likelihood is non-convex, which hinders fast mixing of&#10;many Markov Chain Monte Carlo (MCMC) procedures. It has been recently&#10;shown that using the properties of the gradient of the log empirical&#10;likelihood, one can devise an efficient Hamiltonian Monte Carlo (HMC)&#10;algorithm to sample from a BayesEL posterior. The package requires the&#10;user to specify only the estimating equations, the prior, and their&#10;respective gradients. An MCMC sample drawn from the BayesEL posterior&#10;of the parameters, with various details required by the user, is&#10;obtained."/>
  <meta property="og:locale" content="en_US"/>

  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="elhmc: An R Package for Hamiltonian Monte Carlo Sampling in Bayesian Empirical Likelihood"/>
  <meta property="twitter:description" content="In this article, we describe an R package for sampling from an&#10;empirical likelihood-based posterior using a Hamiltonian Monte Carlo&#10;method. Empirical likelihood-based methodologies have been used in the&#10;Bayesian modeling of many problems of interest in recent times. This&#10;semiparametric procedure can easily combine the flexibility of a&#10;nonparametric distribution estimator together with the&#10;interpretability of a parametric model. The model is specified by&#10;estimating equation-based constraints. Drawing inference from a&#10;Bayesian empirical likelihood (BayesEL) posterior is challenging. The&#10;likelihood is computed numerically, so no closed-form expression of&#10;the posterior exists. Moreover, for any sample of finite size, the&#10;support of the likelihood is non-convex, which hinders fast mixing of&#10;many Markov Chain Monte Carlo (MCMC) procedures. It has been recently&#10;shown that using the properties of the gradient of the log empirical&#10;likelihood, one can devise an efficient Hamiltonian Monte Carlo (HMC)&#10;algorithm to sample from a BayesEL posterior. The package requires the&#10;user to specify only the estimating equations, the prior, and their&#10;respective gradients. An MCMC sample drawn from the BayesEL posterior&#10;of the parameters, with various details required by the user, is&#10;obtained."/>

  <!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
  <meta name="citation_title" content="elhmc: An R Package for Hamiltonian Monte Carlo Sampling in Bayesian Empirical Likelihood"/>
  <meta name="citation_fulltext_html_url" content="https://doi.org/10.32614/RJ-2025-041"/>
  <meta name="citation_pdf_url" content="RJ-2025-041.pdf"/>
  <meta name="citation_volume" content="17"/>
  <meta name="citation_issue" content="4"/>
  <meta name="citation_doi" content="10.32614/RJ-2025-041"/>
  <meta name="citation_journal_title" content="The R Journal"/>
  <meta name="citation_issn" content="2073-4859"/>
  <meta name="citation_firstpage" content="237"/>
  <meta name="citation_lastpage" content="254"/>
  <meta name="citation_fulltext_world_readable" content=""/>
  <meta name="citation_online_date" content="2026/01/06"/>
  <meta name="citation_publication_date" content="2026/01/06"/>
  <meta name="citation_author" content="Neo Han Wei"/>
  <meta name="citation_author_institution" content="Citibank, Singapore"/>
  <meta name="citation_author" content="Dang Trung Kien"/>
  <meta name="citation_author_institution" content="Independent Consultant"/>
  <meta name="citation_author" content="Sanjay Chaudhuri"/>
  <meta name="citation_author_institution" content="University of Nebraska-Lincoln"/>
  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=VBel: Variational Bayes for fast and accurate empirical likelihood inference;citation_author=Weichang Yu;citation_author=Jeremy Lim"/>
  <meta name="citation_reference" content="citation_title=Extended empirical likelihood for estimating equations;citation_volume=101;citation_doi=10.1093/biomet/asu014;citation_author=Min Tsao;citation_author=Fan Wu"/>
  <meta name="citation_reference" content="citation_title=Generalized linear models incorporating population level information: An empirical-likelihood-based approach;citation_volume=70.2;citation_author=Sanjay Chaudhuri;citation_author=Mark S. Handcock;citation_author=Michael S Rendall"/>
  <meta name="citation_reference" content="citation_title=Stan: A probabilistic programming language;citation_volume=76;citation_doi=10.18637/jss.v076.i01;citation_issn=1548-7660;citation_author=Bob Carpenter;citation_author=Andrew Gelman;citation_author=Matthew Hoffman;citation_author=Daniel Lee;citation_author=Ben Goodrich;citation_author=Michael Betancourt;citation_author=Marcus Brubaker;citation_author=Jiqiang Guo;citation_author=Peter Li;citation_author=Allen Riddell"/>
  <meta name="citation_reference" content="citation_title=Hamiltonian monte carlo sampling in Bayesian empirical likelihood computation;citation_volume=79;citation_doi=10.1111/rssb.12164;citation_issn=1467-9868;citation_author=Sanjay Chaudhuri;citation_author=Debashis Mondal;citation_author=Teng Yin"/>
  <meta name="citation_reference" content="citation_title=Emplik: Empirical likelihood ratio for censored/truncated data;citation_author=Mai Zhou"/>
  <meta name="citation_reference" content="citation_title=Calibration of the empirical likelihood method for a vector mean;citation_publisher=Institute of Mathematical Statistics;citation_volume=3;citation_author=Sarah C Emerson;citation_author=Art B Owen;citation_author=Calibration of the empirical likelihood method for a vector mean"/>
  <meta name="citation_reference" content="citation_title=Adaptive proposal distribution for random walk Metropolis algorithm;citation_publisher=Citeseer;citation_volume=14;citation_author=Heikki Haario;citation_author=Eero Saksman;citation_author=Johanna Tamminen"/>
  <meta name="citation_reference" content="citation_title=Adjusted empirical likelihood with high-order precision;citation_publisher=Institute of Mathematical Statistics;citation_volume=38;citation_author=Yukun Liu;citation_author=Jiahua Chen;citation_author=Adjusted empirical likelihood with high-order precision"/>
  <meta name="citation_reference" content="citation_title=Markov chain monte carlo methods for statistical inference;citation_publisher=Citeseer;citation_author=Julian Besag"/>
  <meta name="citation_reference" content="citation_title=Empirical likelihood on the full parameter space;citation_publisher=Institute of Mathematical Statistics;citation_volume=41;citation_author=Min Tsao;citation_author=Fan Wu"/>
  <meta name="citation_reference" content="citation_title=Extending the empirical likelihood by domain expansion;citation_publisher=Wiley Online Library;citation_volume=41;citation_author=Min Tsao"/>
  <meta name="citation_reference" content="citation_title=The empty set and zero likelihood problems in maximum empirical likelihood estimation;citation_publisher=Institute of Mathematical Statistics;citation_volume=6;citation_author=Wicher Bergsma;citation_author=Marcel Croon;citation_author=L Andries Ark;citation_author=The empty set and zero likelihood problems in maximum empirical likelihood estimation"/>
  <meta name="citation_reference" content="citation_title=Empty set problem of maximum empirical likelihood methods;citation_volume=3;citation_author=Marian GrendÃ¡r;citation_author=George Judge"/>
  <meta name="citation_reference" content="citation_title=Convex optimization;citation_publisher=Cambridge university press;citation_author=Stephen P Boyd;citation_author=Lieven Vandenberghe"/>
  <meta name="citation_reference" content="citation_title=Simulating hamiltonian dynamics;citation_publisher=Cambridge University Press;citation_volume=14;citation_author=Benedict Leimkuhler;citation_author=Sebastian Reich"/>
  <meta name="citation_reference" content="citation_title=Plasma physics via computer simulation;citation_publisher=CRC Press;citation_author=Charles K. Birdsall;citation_author=A. Bruce Langdon"/>
  <meta name="citation_reference" content="citation_title=MCMC for using hamiltonian dynamics;citation_author=R Neal"/>
  <meta name="citation_reference" content="citation_title=Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images;citation_publisher=IEEE;citation_author=Stuart Geman;citation_author=Donald Geman"/>
  <meta name="citation_reference" content="citation_title=Empirical likelihood and general estimating equations;citation_publisher=JSTOR;citation_author=J. Qin;citation_author=J. Lawless"/>
  <meta name="citation_reference" content="citation_title=Adjusted empirical likelihood and its properties;citation_publisher=American Statistical Association;citation_volume=17;citation_author=J. Chen;citation_author=A. M. Variyath;citation_author=B. Abraham"/>
  <meta name="citation_reference" content="citation_title=Bayesian empirical likelihood;citation_publisher=Biometrika Trust;citation_volume=90;citation_author=N. A. Lazar"/>
  <!--radix_placeholder_rmarkdown_metadata-->

  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","date_received","journal","volume","issue","slug","packages","preview","bibliography","CTV","legacy_pdf","legacy_converted","output","draft","pdf_url","citation_url","doi","creative_commons","csl"]}},"value":[{"type":"character","attributes":{},"value":["elhmc: An R Package for Hamiltonian Monte Carlo Sampling in Bayesian Empirical Likelihood"]},{"type":"character","attributes":{},"value":["In this article, we describe an R package for sampling from an\nempirical likelihood-based posterior using a Hamiltonian Monte Carlo\nmethod. Empirical likelihood-based methodologies have been used in the\nBayesian modeling of many problems of interest in recent times. This\nsemiparametric procedure can easily combine the flexibility of a\nnonparametric distribution estimator together with the\ninterpretability of a parametric model. The model is specified by\nestimating equation-based constraints. Drawing inference from a\nBayesian empirical likelihood (BayesEL) posterior is challenging. The\nlikelihood is computed numerically, so no closed-form expression of\nthe posterior exists. Moreover, for any sample of finite size, the\nsupport of the likelihood is non-convex, which hinders fast mixing of\nmany Markov Chain Monte Carlo (MCMC) procedures. It has been recently\nshown that using the properties of the gradient of the log empirical\nlikelihood, one can devise an efficient Hamiltonian Monte Carlo (HMC)\nalgorithm to sample from a BayesEL posterior. The package requires the\nuser to specify only the estimating equations, the prior, and their\nrespective gradients. An MCMC sample drawn from the BayesEL posterior\nof the parameters, with various details required by the user, is\nobtained.\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","address"]}},"value":[{"type":"character","attributes":{},"value":["Neo Han Wei"]},{"type":"character","attributes":{},"value":["Citibank, Singapore"]},{"type":"character","attributes":{},"value":["[`nhanwei@gmail.com`](mailto:nhanwei@gmail.com)\n"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","address"]}},"value":[{"type":"character","attributes":{},"value":["Dang Trung Kien"]},{"type":"character","attributes":{},"value":["Independent Consultant"]},{"type":"character","attributes":{},"value":["[`trungkiendang@hotmail.com`](mailto:trungkiendang@hotmail.com)","<http://www.stat.nus.edu.sg/>\n"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","address"]}},"value":[{"type":"character","attributes":{},"value":["Sanjay Chaudhuri"]},{"type":"character","attributes":{},"value":["University of Nebraska-Lincoln"]},{"type":"character","attributes":{},"value":["Department of Statistics","840 Hardin Hall North Wing, Lincoln, NE, USA","[`schaudhuri2@nebraska.edu`](mailto:schaudhuri2@nebraska.edu)","<http://www.stat.nus.edu.sg/>\n"]}]}]},{"type":"character","attributes":{},"value":["2026-01-06"]},{"type":"character","attributes":{},"value":["2025-03-12"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","issn","firstpage","lastpage"]}},"value":[{"type":"character","attributes":{},"value":["The R Journal"]},{"type":"character","attributes":{},"value":["2073-4859"]},{"type":"integer","attributes":{},"value":[237]},{"type":"integer","attributes":{},"value":[254]}]},{"type":"integer","attributes":{},"value":[17]},{"type":"integer","attributes":{},"value":[4]},{"type":"character","attributes":{},"value":["RJ-2025-041"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["cran","bioc"]}},"value":[{"type":"list","attributes":{},"value":[]},{"type":"list","attributes":{},"value":[]}]},{"type":"character","attributes":{},"value":["preview.png"]},{"type":"character","attributes":{},"value":["kWC.bib"]},{"type":"list","attributes":{},"value":[]},{"type":"logical","attributes":{},"value":[true]},{"type":"logical","attributes":{},"value":[true]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc","mathjax","md_extension"]}},"value":[{"type":"logical","attributes":{},"value":[true]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"]},{"type":"character","attributes":{},"value":["-tex_math_single_backslash"]}]}]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["RJ-2025-041.pdf"]},{"type":"character","attributes":{},"value":["https://doi.org/10.32614/RJ-2025-041"]},{"type":"character","attributes":{},"value":["10.32614/RJ-2025-041"]},{"type":"character","attributes":{},"value":["CC BY"]},{"type":"character","attributes":{},"value":["/home/mitchell/R/x86_64-pc-linux-gnu-library/4.5/rjtools/rjournal.csl"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["figures/bhpsAcfb0.pdf","figures/bhpsAcfb0.png","figures/bhpsAcfb0.ps","figures/bhpsAcfb1.pdf","figures/bhpsAcfb1.png","figures/bhpsAcfb1.ps","figures/bhpsContour.pdf","figures/bhpsContour.png","figures/dirScan3.jpeg","figures/elhmc-008.pdf","figures/elhmc-008.png","figures/elhmc-010.pdf","figures/elhmc-010.png","figures/elhmcHMC.pdf","figures/elhmcHMC.png","kWC.bib","kWC.tex","missfont.log","RJ-2025-041_files/anchor-4.2.2/anchor.min.js","RJ-2025-041_files/bowser-1.9.3/bowser.min.js","RJ-2025-041_files/distill-2.2.21/template.v2.js","RJ-2025-041_files/header-attrs-2.30/header-attrs.js","RJ-2025-041_files/jquery-3.6.0/jquery-3.6.0.js","RJ-2025-041_files/jquery-3.6.0/jquery-3.6.0.min.js","RJ-2025-041_files/jquery-3.6.0/jquery-3.6.0.min.map","RJ-2025-041_files/popper-2.6.0/popper.min.js","RJ-2025-041_files/tippy-6.2.7/tippy-bundle.umd.min.js","RJ-2025-041_files/tippy-6.2.7/tippy-light-border.css","RJ-2025-041_files/tippy-6.2.7/tippy.css","RJ-2025-041_files/tippy-6.2.7/tippy.umd.min.js","RJ-2025-041_files/webcomponents-2.0.0/webcomponents.js","RJ-2025-041.zip","RJournal.sty","RJwrapper.aux","RJwrapper.log","RJwrapper.Rmd.bak","RJwrapper.tex","scripts/contour.pdf","scripts/kWC.R"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

  <style type="text/css">

  body {
    background-color: white;
  }

  .pandoc-table {
    width: 100%;
  }

  .pandoc-table>caption {
    margin-bottom: 10px;
  }

  .pandoc-table th:not([align]) {
    text-align: left;
  }

  .pagedtable-footer {
    font-size: 15px;
  }

  d-byline .byline {
    grid-template-columns: 2fr 2fr;
  }

  d-byline .byline h3 {
    margin-block-start: 1.5em;
  }

  d-byline .byline .authors-affiliations h3 {
    margin-block-start: 0.5em;
  }

  .authors-affiliations .orcid-id {
    width: 16px;
    height:16px;
    margin-left: 4px;
    margin-right: 4px;
    vertical-align: middle;
    padding-bottom: 2px;
  }

  d-title .dt-tags {
    margin-top: 1em;
    grid-column: text;
  }

  .dt-tags .dt-tag {
    text-decoration: none;
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0em 0.4em;
    margin-right: 0.5em;
    margin-bottom: 0.4em;
    font-size: 70%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  d-article table.gt_table td,
  d-article table.gt_table th {
    border-bottom: none;
    font-size: 100%;
  }

  .html-widget {
    margin-bottom: 2.0em;
  }

  .l-screen-inset {
    padding-right: 16px;
  }

  .l-screen .caption {
    margin-left: 10px;
  }

  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .shaded-content {
    background: white;
  }

  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }

  .hidden {
    display: none !important;
  }

  hr.section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    margin: 0px;
  }


  d-byline {
    border-top: none;
  }

  d-article {
    padding-top: 2.5rem;
    padding-bottom: 30px;
    border-top: none;
  }

  d-appendix {
    padding-top: 30px;
  }

  d-article>p>img {
    width: 100%;
  }

  d-article h2 {
    margin: 1rem 0 1.5rem 0;
  }

  d-article h3 {
    margin-top: 1.5rem;
  }

  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }

  /* Tweak code blocks */

  d-article div.sourceCode code,
  d-article pre code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: auto;
  }

  d-article div.sourceCode {
    background-color: white;
  }

  d-article div.sourceCode pre {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }

  d-article pre {
    font-size: 12px;
    color: black;
    background: none;
    margin-top: 0;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  d-article pre a {
    border-bottom: none;
  }

  d-article pre a:hover {
    border-bottom: none;
    text-decoration: underline;
  }

  d-article details {
    grid-column: text;
    margin-bottom: 0.8em;
  }

  @media(min-width: 768px) {

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: visible !important;
  }

  d-article div.sourceCode pre {
    padding-left: 18px;
    font-size: 14px;
  }

  /* tweak for Pandoc numbered line within distill */
  d-article pre.numberSource code > span {
      left: -2em;
  }

  d-article pre {
    font-size: 14px;
  }

  }

  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  /* CSS for d-contents */

  .d-contents {
    grid-column: text;
    color: rgba(0,0,0,0.8);
    font-size: 0.9em;
    padding-bottom: 1em;
    margin-bottom: 1em;
    padding-bottom: 0.5em;
    margin-bottom: 1em;
    padding-left: 0.25em;
    justify-self: start;
  }

  @media(min-width: 1000px) {
    .d-contents.d-contents-float {
      height: 0;
      grid-column-start: 1;
      grid-column-end: 4;
      justify-self: center;
      padding-right: 3em;
      padding-left: 2em;
    }
  }

  .d-contents nav h3 {
    font-size: 18px;
    margin-top: 0;
    margin-bottom: 1em;
  }

  .d-contents li {
    list-style-type: none
  }

  .d-contents nav > ul {
    padding-left: 0;
  }

  .d-contents ul {
    padding-left: 1em
  }

  .d-contents nav ul li {
    margin-top: 0.6em;
    margin-bottom: 0.2em;
  }

  .d-contents nav a {
    font-size: 13px;
    border-bottom: none;
    text-decoration: none
    color: rgba(0, 0, 0, 0.8);
  }

  .d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6)
  }

  .d-contents nav > ul > li > a {
    font-weight: 600;
  }

  .d-contents nav > ul > li > ul {
    font-weight: inherit;
  }

  .d-contents nav > ul > li > ul > li {
    margin-top: 0.2em;
  }


  .d-contents nav ul {
    margin-top: 0;
    margin-bottom: 0.25em;
  }

  .d-article-with-toc h2:nth-child(2) {
    margin-top: 0;
  }


  /* Figure */

  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }

  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }

  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }

  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }

  /* Citations */

  d-article .citation {
    color: inherit;
    cursor: inherit;
  }

  div.hanging-indent{
    margin-left: 1em; text-indent: -1em;
  }

  /* Citation hover box */

  .tippy-box[data-theme~=light-border] {
    background-color: rgba(250, 250, 250, 0.95);
  }

  .tippy-content > p {
    margin-bottom: 0;
    padding: 2px;
  }


  /* Tweak 1000px media break to show more text */

  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }

    .grid {
      grid-column-gap: 16px;
    }

    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }

  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }

    .grid {
      grid-column-gap: 32px;
    }
  }


  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */

  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  /* Include appendix styles here so they can be overridden */

  d-appendix {
    contain: layout style;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-top: 60px;
    margin-bottom: 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    color: rgba(0,0,0,0.5);
    padding-top: 60px;
    padding-bottom: 48px;
  }

  d-appendix h3 {
    grid-column: page-start / text-start;
    font-size: 15px;
    font-weight: 500;
    margin-top: 1em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.65);
  }

  d-appendix h3 + * {
    margin-top: 1em;
  }

  d-appendix ol {
    padding: 0 0 0 15px;
  }

  @media (min-width: 768px) {
    d-appendix ol {
      padding: 0 0 0 30px;
      margin-left: -30px;
    }
  }

  d-appendix li {
    margin-bottom: 1em;
  }

  d-appendix a {
    color: rgba(0, 0, 0, 0.6);
  }

  d-appendix > * {
    grid-column: text;
  }

  d-appendix > d-footnote-list,
  d-appendix > d-citation-list,
  d-appendix > distill-appendix {
    grid-column: screen;
  }

  /* Include footnote styles here so they can be overridden */

  d-footnote-list {
    contain: layout style;
  }

  d-footnote-list > * {
    grid-column: text;
  }

  d-footnote-list a.footnote-backlink {
    color: rgba(0,0,0,0.3);
    padding-left: 0.5em;
  }



  /* Anchor.js */

  .anchorjs-link {
    /*transition: all .25s linear; */
    text-decoration: none;
    border-bottom: none;
  }
  *:hover > .anchorjs-link {
    margin-left: -1.125em !important;
    text-decoration: none;
    border-bottom: none;
  }

  /* Social footer */

  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }

  .disqus-comments {
    margin-right: 30px;
  }

  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }

  #disqus_thread {
    margin-top: 30px;
  }

  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }

  .article-sharing a:hover {
    border-bottom: none;
  }

  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }

  .subscribe p {
    margin-bottom: 0.5em;
  }


  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }


  .sidebar-section.custom {
    font-size: 12px;
    line-height: 1.6em;
  }

  .custom p {
    margin-bottom: 0.5em;
  }

  /* Styles for listing layout (hide title) */
  .layout-listing d-title, .layout-listing .d-title {
    display: none;
  }

  /* Styles for posts lists (not auto-injected) */


  .posts-with-sidebar {
    padding-left: 45px;
    padding-right: 45px;
  }

  .posts-list .description h2,
  .posts-list .description p {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  }

  .posts-list .description h2 {
    font-weight: 700;
    border-bottom: none;
    padding-bottom: 0;
  }

  .posts-list h2.post-tag {
    border-bottom: 1px solid rgba(0, 0, 0, 0.2);
    padding-bottom: 12px;
  }
  .posts-list {
    margin-top: 60px;
    margin-bottom: 24px;
  }

  .posts-list .post-preview {
    text-decoration: none;
    overflow: hidden;
    display: block;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    padding: 24px 0;
  }

  .post-preview-last {
    border-bottom: none !important;
  }

  .posts-list .posts-list-caption {
    grid-column: screen;
    font-weight: 400;
  }

  .posts-list .post-preview h2 {
    margin: 0 0 6px 0;
    line-height: 1.2em;
    font-style: normal;
    font-size: 24px;
  }

  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.4em;
    font-size: 16px;
  }

  .posts-list .post-preview .thumbnail {
    box-sizing: border-box;
    margin-bottom: 24px;
    position: relative;
    max-width: 500px;
  }
  .posts-list .post-preview img {
    width: 100%;
    display: block;
  }

  .posts-list .metadata {
    font-size: 12px;
    line-height: 1.4em;
    margin-bottom: 18px;
  }

  .posts-list .metadata > * {
    display: inline-block;
  }

  .posts-list .metadata .publishedDate {
    margin-right: 2em;
  }

  .posts-list .metadata .dt-authors {
    display: block;
    margin-top: 0.3em;
    margin-right: 2em;
  }

  .posts-list .dt-tags {
    display: block;
    line-height: 1em;
  }

  .posts-list .dt-tags .dt-tag {
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0.3em 0.4em;
    margin-right: 0.2em;
    margin-bottom: 0.4em;
    font-size: 60%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  .posts-list img {
    opacity: 1;
  }

  .posts-list img[data-src] {
    opacity: 0;
  }

  .posts-more {
    clear: both;
  }


  .posts-sidebar {
    font-size: 16px;
  }

  .posts-sidebar h3 {
    font-size: 16px;
    margin-top: 0;
    margin-bottom: 0.5em;
    font-weight: 400;
    text-transform: uppercase;
  }

  .sidebar-section {
    margin-bottom: 30px;
  }

  .categories ul {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }

  .categories li {
    color: rgba(0, 0, 0, 0.8);
    margin-bottom: 0;
  }

  .categories li>a {
    border-bottom: none;
  }

  .categories li>a:hover {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  }

  .categories .active {
    font-weight: 600;
  }

  .categories .category-count {
    color: rgba(0, 0, 0, 0.4);
  }


  @media(min-width: 768px) {
    .posts-list .post-preview h2 {
      font-size: 26px;
    }
    .posts-list .post-preview .thumbnail {
      float: right;
      width: 30%;
      margin-bottom: 0;
    }
    .posts-list .post-preview .description {
      float: left;
      width: 45%;
    }
    .posts-list .post-preview .metadata {
      float: left;
      width: 20%;
      margin-top: 8px;
    }
    .posts-list .post-preview p {
      margin: 0 0 12px 0;
      line-height: 1.5em;
      font-size: 16px;
    }
    .posts-with-sidebar .posts-list {
      float: left;
      width: 75%;
    }
    .posts-with-sidebar .posts-sidebar {
      float: right;
      width: 20%;
      margin-top: 60px;
      padding-top: 24px;
      padding-bottom: 24px;
    }
  }


  /* Improve display for browsers without grid (IE/Edge <= 15) */

  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }

  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }

  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }

  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }

  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }

  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }

  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }


  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }

  .downlevel .footnotes ol {
    padding-left: 13px;
  }

  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }

  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }

  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }

  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  .downlevel .posts-list .post-preview {
    color: inherit;
  }



  </style>

  <script type="application/javascript">

  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }

  // show body when load is complete
  function on_load_complete() {

    // add anchors
    if (window.anchors) {
      window.anchors.options.placement = 'left';
      window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
    }


    // set body to visible
    document.body.style.visibility = 'visible';

    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }

    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }

  function init_distill() {

    init_common();

    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);

    // create d-title
    $('.d-title').changeElementType('d-title');

    // separator
    var separator = '<hr class="section-separator" style="clear: both"/>';
    // prepend separator above appendix
    $('.d-byline').before(separator);
    $('.d-article').before(separator);

    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);

    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();

    // move posts container into article
    $('.posts-container').appendTo($('d-article'));

    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');

    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;

    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();

    // move refs into #references-listing
    $('#references-listing').replaceWith($('#refs'));

    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-contents a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });

    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');

    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {

      // capture layout
      var layout = $(this).attr('data-layout');

      // apply layout to markdown level block elements
      var elements = $(this).children().not('details, div.sourceCode, pre, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });


      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });

    // remove code block used to force  highlighting css
    $('.distill-force-highlighting-css').parent().remove();

    // remove empty line numbers inserted by pandoc when using a
    // custom syntax highlighting theme, except when numbering line
    // in code chunk
    $('pre:not(.numberLines) code.sourceCode a:empty').remove();

    // load distill framework
    load_distill_framework();

    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {

      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;

      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');

      // article with toc class
      $('.d-contents').parent().addClass('d-article-with-toc');

      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

      // add orcid ids
      $('.authors-affiliations').find('.author').each(function(i, el) {
        var orcid_id = front_matter.authors[i].orcidID;
        var author_name = front_matter.authors[i].author
        if (orcid_id) {
          var a = $('<a></a>');
          a.attr('href', 'https://orcid.org/' + orcid_id);
          var img = $('<img></img>');
          img.addClass('orcid-id');
          img.attr('alt', author_name ? 'ORCID ID for ' + author_name : 'ORCID ID');
          img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
          a.append(img);
          $(this).append(a);
        }
      });

      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }

      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");

      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }

       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }

      // remove d-appendix and d-footnote-list local styles
      $('d-appendix > style:first-child').remove();
      $('d-footnote-list > style:first-child').remove();

      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();

      // hoverable references
      $('span.citation[data-cites]').each(function() {
        const citeChild = $(this).children()[0]
        // Do not process if @xyz has been used without escaping and without bibliography activated
        // https://github.com/rstudio/distill/issues/466
        if (citeChild === undefined) return true

        if (citeChild.nodeName == "D-FOOTNOTE") {
          var fn = citeChild
          $(this).html(fn.shadowRoot.querySelector("sup"))
          $(this).id = fn.id
          fn.remove()
        }
        var refs = $(this).attr('data-cites').split(" ");
        var refHtml = refs.map(function(ref) {
          // Could use CSS.escape too here, we insure backward compatibility in navigator
          return "<p>" + $('div[id="ref-' + ref + '"]').html() + "</p>";
        }).join("\n");
        window.tippy(this, {
          allowHTML: true,
          content: refHtml,
          maxWidth: 500,
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        });
      });

      // fix footnotes in tables (#411)
      // replacing broken distill.pub feature
      $('table d-footnote').each(function() {
        // we replace internal showAtNode methode which is triggered when hovering a footnote
        this.hoverBox.showAtNode = function(node) {
          // ported from https://github.com/distillpub/template/pull/105/files
          calcOffset = function(elem) {
              let x = elem.offsetLeft;
              let y = elem.offsetTop;
              // Traverse upwards until an `absolute` element is found or `elem`
              // becomes null.
              while (elem = elem.offsetParent && elem.style.position != 'absolute') {
                  x += elem.offsetLeft;
                  y += elem.offsetTop;
              }

              return { left: x, top: y };
          }
          // https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetTop
          const bbox = node.getBoundingClientRect();
          const offset = calcOffset(node);
          this.show([offset.left + bbox.width, offset.top + bbox.height]);
        }
      })

      // clear polling timer
      clearInterval(tid);

      // show body now that everything is ready
      on_load_complete();
    }

    var tid = setInterval(distill_post_process, 50);
    distill_post_process();

  }

  function init_downlevel() {

    init_common();

     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));

    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;

    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();

    // remove toc
    $('.d-contents').remove();

    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });


    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);

    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();

    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });

    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));

    $('body').addClass('downlevel');

    on_load_complete();
  }


  function init_common() {

    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};

        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });

        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);

    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});

    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      // ignore leaflet img layers (#106)
      figures = figures.filter(':not(img[class*="leaflet"])')
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });

      }
    });

    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });

    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace(/^index[.]html/, "./"));
      });
    }

    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');

    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");

    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();

    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }

  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });

  </script>

  <!--/radix_placeholder_distill-->
  <script src="RJ-2025-041_files/header-attrs-2.30/header-attrs.js"></script>
  <script src="RJ-2025-041_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="RJ-2025-041_files/popper-2.6.0/popper.min.js"></script>
  <link href="RJ-2025-041_files/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="RJ-2025-041_files/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="RJ-2025-041_files/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="RJ-2025-041_files/anchor-4.2.2/anchor.min.js"></script>
  <script src="RJ-2025-041_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="RJ-2025-041_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="RJ-2025-041_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->
  <script>
    $(function() {
      console.log("Starting...")

      // Mathjax config (add automatic linebreaks when supported)
      // MathJax = {
      //    tex: {
      //        inlineMath: [['$', '$'], ['\\(', '\\)']],
      //        displayMath: [['$$', '$$'], ['\\[', '\\]']],
      //        tags: 'ams',
      //        multline: true,
      //    },
      //    options: {
      //        linebreaks: { automatic: true },
      //    },
      // };

      // Always show Published - distill hides it if not set
      function show_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'visible');
      }

      show_byline_column('Published')

      // tweak function
      var rmd_meta = JSON.parse($("#radix-rmarkdown-metadata").html());
      function get_meta(name, meta) {
        var ind = meta.attributes.names.value.findIndex((e) => e == name)
        var val = meta.value[ind]
        if (val.type != 'list') {
          return val.value.toString()
        }
        return val
      }

      // tweak description
      // Add clickable tags
      const slug = get_meta('slug', rmd_meta)
      const cite_url = get_meta('citation_url', rmd_meta)

      var title = $("d-title").text

      const buttons = $('<div class="dt-tags" style="grid-column: page;">')
      buttons.append('<a href="#citation" class="dt-tag"><i class="fas fa-quote-left"></i> Cite</a>')
      buttons.append('<a href="' + slug + '.pdf" class="dt-tag"><i class="fas fa-file-pdf"></i> PDF</a>')
      
      // Conditionally add supplementary files button
      if (document.getElementById('supplementary-materials')) {
        // create element safely
        const btn_suppl = document.createElement('a');
        btn_suppl.href = slug + '.zip';
        btn_suppl.className = 'dt-tag';
        btn_suppl.innerHTML = '<i class="fas fa-file-zipper"></i> Supplement';
        buttons.append(btn_suppl);
      }

      // adds Abstract: in front of the first <p> in the title section --
      // unless it happens to be the subtitle (FIXME: this is a bad hack - can't distill do this?)
      var tpar = $("d-title p:not(:empty)").filter(function() {
        return !$(this).hasClass("subtitle");
      }).first();
      if (tpar) {
        const abstract = $('<d-abstract>')
        abstract.append('<b>Abstract:</b><br>')
        abstract.append(tpar) // Move description to d-abstract
        $("d-title p:empty").remove() // Remove empty paragraphs after title
        abstract.append(buttons)
        abstract.insertAfter($('d-title')) // Add abstract section after title */
      }

      // tweak by-line
      var byline = $("d-byline div.byline")
      ind = rmd_meta.attributes.names.value.findIndex((e) => e == "journal")
      const journal = get_meta('journal', rmd_meta)
      const volume = get_meta('volume', rmd_meta)
      const issue = get_meta('issue', rmd_meta)
      const jrtitle = get_meta('title', journal)
      const year = ((jrtitle == "R News") ? 2000 : 2008) + parseInt(volume)
      const firstpage = get_meta('firstpage', journal)
      const lastpage = get_meta('lastpage', journal)
      byline.append('<div class="rjournal grid">')
      $('div.rjournal').append('<h3>Volume</h3>')
      $('div.rjournal').append('<h3>Pages</h3>')
      $('div.rjournal').append('<a class="volume" href="../../issues/'+year+'-'+issue+'">'+volume+'/'+issue+'</a>')
      $('div.rjournal').append('<p class="pages">'+firstpage+' - '+lastpage+'</p>')

      const received_date = new Date(get_meta('date_received', rmd_meta))
      byline.find('h3:contains("Published")').parent().append('<h3>Received</h3><p>'+received_date.toLocaleDateString('en-US', {month: 'short'})+' '+received_date.getDate()+', '+received_date.getFullYear()+'</p>')

    })
  </script>

  <style>
      /*
    .nav-dropdown-content .nav-dropdown-header {
      text-transform: lowercase;
    }
    */

    d-byline .byline {
      grid-template-columns: 2fr 2fr 2fr 2fr;
    }

    d-byline .rjournal {
      grid-column-end: span 2;
      grid-template-columns: 1fr 1fr;
      margin-bottom: 0;
    }

    d-title h1, d-title p, d-title figure,
    d-abstract p, d-abstract b {
      grid-column: page;
    }

    d-title .dt-tags {
      grid-column: page;
    }

    .dt-tags .dt-tag {
      text-transform: lowercase;
    }

    d-article h1 {
      line-height: 1.1em;
    }

    d-abstract p, d-article p {
      text-align: justify;
    }

    @media(min-width: 1000px) {
      .d-contents.d-contents-float {
        justify-self: end;
      }

      nav.toc {
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }

    .posts-list .dt-tags .dt-tag {
      text-transform: lowercase;
    }

    @keyframes highlight-target {
      0% {
        background-color: #ffa;
      }
      66% {
        background-color: #ffa;
      }
      100% {
        background-color: none;
      }
    }

    d-article :target, d-appendix :target {
       animation: highlight-target 3s;
    }

    .header-section-number {
      margin-right: 0.5em;
    }
    
    d-appendix .citation-appendix,
    .d-appendix .citation-appendix {
      color: rgb(60, 60, 60);
    }

    d-article h2 {
      border-bottom: 0px solid rgba(0, 0, 0, 0.1);
      padding-bottom: 0rem;
    }
    d-article h3 {
      font-size: 20px;
    }
    d-article h4 {
      font-size: 18px;
      text-transform: none;
    }

    @media (min-width: 1024px) {
      d-article h2 {
        font-size: 32px;
      }
      d-article h3 {
        font-size: 24px;
      }
      d-article h4 {
        font-size: 20px;
      }
    }
  </style>


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"elhmc: An R Package for Hamiltonian Monte Carlo Sampling in Bayesian Empirical Likelihood","description":"In this article, we describe an R package for sampling from an\nempirical likelihood-based posterior using a Hamiltonian Monte Carlo\nmethod. Empirical likelihood-based methodologies have been used in the\nBayesian modeling of many problems of interest in recent times. This\nsemiparametric procedure can easily combine the flexibility of a\nnonparametric distribution estimator together with the\ninterpretability of a parametric model. The model is specified by\nestimating equation-based constraints. Drawing inference from a\nBayesian empirical likelihood (BayesEL) posterior is challenging. The\nlikelihood is computed numerically, so no closed-form expression of\nthe posterior exists. Moreover, for any sample of finite size, the\nsupport of the likelihood is non-convex, which hinders fast mixing of\nmany Markov Chain Monte Carlo (MCMC) procedures. It has been recently\nshown that using the properties of the gradient of the log empirical\nlikelihood, one can devise an efficient Hamiltonian Monte Carlo (HMC)\nalgorithm to sample from a BayesEL posterior. The package requires the\nuser to specify only the estimating equations, the prior, and their\nrespective gradients. An MCMC sample drawn from the BayesEL posterior\nof the parameters, with various details required by the user, is\nobtained.","doi":"10.32614/RJ-2025-041","authors":[{"author":"Neo Han Wei","authorURL":"#","affiliation":"Citibank, Singapore","affiliationURL":"#","orcidID":""},{"author":"Dang Trung Kien","authorURL":"#","affiliation":"Independent Consultant","affiliationURL":"#","orcidID":""},{"author":"Sanjay Chaudhuri","authorURL":"#","affiliation":"University of Nebraska-Lincoln","affiliationURL":"#","orcidID":""}],"publishedDate":"2026-01-06T00:00:00.000+11:00","citationText":"Wei, et al., 2026"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>elhmc: An R Package for Hamiltonian Monte Carlo Sampling in Bayesian Empirical Likelihood</h1>

<!--radix_placeholder_categories-->
<!--/radix_placeholder_categories-->
<p><p>In this article, we describe an R package for sampling from an
empirical likelihood-based posterior using a Hamiltonian Monte Carlo
method. Empirical likelihood-based methodologies have been used in the
Bayesian modeling of many problems of interest in recent times. This
semiparametric procedure can easily combine the flexibility of a
nonparametric distribution estimator together with the
interpretability of a parametric model. The model is specified by
estimating equation-based constraints. Drawing inference from a
Bayesian empirical likelihood (BayesEL) posterior is challenging. The
likelihood is computed numerically, so no closed-form expression of
the posterior exists. Moreover, for any sample of finite size, the
support of the likelihood is non-convex, which hinders fast mixing of
many Markov Chain Monte Carlo (MCMC) procedures. It has been recently
shown that using the properties of the gradient of the log empirical
likelihood, one can devise an efficient Hamiltonian Monte Carlo (HMC)
algorithm to sample from a BayesEL posterior. The package requires the
user to specify only the estimating equations, the prior, and their
respective gradients. An MCMC sample drawn from the BayesEL posterior
of the parameters, with various details required by the user, is
obtained.</p></p>
</div>

<div class="d-byline">
  Neo Han Wei  (Citibank, Singapore)
  
,   Dang Trung Kien  (Independent Consultant)
  
,   Sanjay Chaudhuri  (University of Nebraska-Lincoln)
  
<br/>2026-01-06
</div>

<div class="d-article">
<div class="article">
<h3 data-number="1" id="introduction"><span class="header-section-number">1</span> Introduction</h3>
<p>Empirical likelihood has several advantages over a traditional
parametric likelihood. Even though a correctly specified parametric
likelihood is usually the most efficient for parameter estimation,
semiparametric methods like empirical likelihood, which use a
nonparametric estimate of the underlying distribution, are often more
efficient when the model is misspecified. Empirical likelihood
incorporates parametric model-based information as constraints in
estimating the underlying distribution, which makes the parametric
estimates interpretable. Furthermore, it allows easy incorporation of
known additional information not involving the parameters in the
analysis.</p>
<p>Bayesian empirical likelihood (BayesEL) <span class="citation" data-cites="lazar2003bayesian">(<a href="#ref-lazar2003bayesian" role="doc-biblioref">Lazar 2003</a>)</span> methods
employ empirical likelihood in the Bayesian paradigm. Given some
information about the model parameters in the form of a prior
distribution and estimating equations obtained from the model, a
likelihood is constructed from a constrained empirical estimate of the
underlying distribution. The prior is then used to define a posterior
based on this estimated likelihood. Inference on the parameter is drawn
based on samples generated from the posterior distribution.</p>
<p>BayesEL methods are quite flexible and have been found useful in many
areas of statistics. The examples include small area estimation,
quantile regression, analysis of complex survey data, etc.</p>
<p>BayesEL procedures, however, require an efficient Markov Chain Monte
Carlo (MCMC) procedure to sample from the resulting posterior. It turns
out that such a procedure is not easily specified. For many parameter
values, it may not be feasible to compute the constrained empirical
distribution function, and the likelihood is estimated to be zero. That
is, the estimated likelihood is not supported over the whole space.
Moreover, this support is non-convex and impossible to determine in most
cases. Thus, a naive random walk MCMC would quite often propose
parameters outside the support and get stuck.</p>
<p>Many authors have encountered this problem in frequentist applications.
Such "empty set" problems are quite common <span class="citation" data-cites="grendar2009empty">(<a href="#ref-grendar2009empty" role="doc-biblioref">GrendÃ¡r and Judge 2009</a>)</span> and
become more frequent in problems with a large number of parameters
<span class="citation" data-cites="bergsma2012empty">(<a href="#ref-bergsma2012empty" role="doc-biblioref">Bergsma et al. 2012</a>)</span>. Several authors
<span class="citation" data-cites="chen2008adjusted emerson2009calibration liu2010adjusted">(<a href="#ref-chen2008adjusted" role="doc-biblioref">Chen et al. 2008</a>; <a href="#ref-emerson2009calibration" role="doc-biblioref">Emerson et al. 2009</a>; <a href="#ref-liu2010adjusted" role="doc-biblioref">Liu et al. 2010</a>)</span> have
suggested the addition of extra observations generated from the
available data designed specifically to avoid empty sets. They show that
such observations can be proposed without changing the asymptotic
distribution of the corresponding Wilksâ statistics. Some authors
(<span class="citation" data-cites="tsao2013extending tsao2013empirical tsaoFu2014">(<a href="#ref-tsao2013extending" role="doc-biblioref">Tsao 2013</a>; <a href="#ref-tsao2013empirical" role="doc-biblioref">Tsao and Wu 2013</a>, <a href="#ref-tsaoFu2014" role="doc-biblioref">2014</a>)</span>) have used a
transformation so that the contours of the resultant empirical
likelihood could be extended beyond the feasible region. However, in
most Bayesian applications, the data are finite in size and not large,
for which the asymptotic arguments have little use.</p>
<p>With the availability of user-friendly software packages like <code>STAN</code>
<span class="citation" data-cites="stan2017">(<a href="#ref-stan2017" role="doc-biblioref">Carpenter et al. 2017</a>)</span>, gradient-assisted MCMC methods like Hamiltonian Monte Carlo
(HMC) are becoming increasingly popular in Bayesian computation. When
the estimating equations are smooth with respect to the parameters,
gradient-based methods would have a huge advantage in sampling from a
BayesEL posterior. This is because <span class="citation" data-cites="chaudhuriMondalTeng2017">Chaudhuri et al. (<a href="#ref-chaudhuriMondalTeng2017" role="doc-biblioref">2017</a>)</span> have shown
that under mild conditions, the gradient of the log-posterior would
diverge to infinity at the boundary of its support. Due to this
phenomenon, if an HMC chain approaches the boundary of the posterior
support, it would be reflected towards its center.</p>
<p>There is no software to implement HMC sampling from a BayesEL posterior
with smooth estimating equations and priors. We describe such a library
called <code>elhmc</code> written for the <code>R</code> platform. The main function in the
library only requires the user to specify the estimating equations,
prior, and respectively their Hessian and gradient with respect to the
parameters as functions. Outputs with user-specified degree of detail
can be obtained.</p>
<p>The <code>elhmc</code> package has been used by practitioners since it was made available
on <code>CRAN</code>. In recent times, various other libraries for sampling from a
BayesEL posterior have been made available. Among them, the library
<code>VBel</code> <span class="citation" data-cites="VBel">(<a href="#ref-VBel" role="doc-biblioref">Yu and Lim 2024</a>)</span> deserves special mention. The authors compute a
variational approximation of the BayesEL posterior from which samples
can be easily drawn. However, most of the time <code>elhmc</code> is considered to
be the benchmark.</p>
<p>The rest of the article is structured as follows. We start with the
theoretical background behind the software package. In section
<!-- QUARTO TODO: Fix section reference --> <a href="#sec:theory">2</a> we first
define the empirical likelihood and construct a Bayesian empirical
likelihood from it. The next part of this section is devoted to a review
of the properties of the log empirical likelihood gradient. A review of
the HMC method with special emphasis on BayesEL sampling is provided
next (Section (sec:hmc)). Section (sec:package) mainly contains the description of the <code>elhmc</code>
library. Some illustrative examples with artificial and real data sets
are presented in Section (sec:examples).</p>
<h3 data-number="2" id="sec:theory"><span class="header-section-number">2</span> Theoretical background</h3>
<h4 class="unnumbered" data-number="2.1" id="basics-of-bayesian-empirical-likelihood">Basics of Bayesian Empirical Likelihood</h4>
<p>Suppose <span class="math inline">\(x=(x_1,\ldots,x_n)\in \mathbb{R}^p\)</span> are <span class="math inline">\(n\)</span> observations from a
distribution <span class="math inline">\(F^0\)</span> depending on a parameter vector
<span class="math inline">\(\theta=(\theta^{(1)}, \ldots,\theta^{(d)})\in\Theta\subseteq \mathbb{R}^d\)</span>.
We assume that both <span class="math inline">\(F^0\)</span> and the true parameter value <span class="math inline">\(\theta^0\)</span> are
unknown. However, certain smooth functions
<span class="math inline">\(g(\theta,x)=\left(g_1(\theta,x),\ldots,g_q(\theta,x)\right)^T\)</span> are
known to satisfy
<span class="math display" id="eq:smoothfun">\[\begin{equation}
\label{smoothfun}
E_{F^0}[g(\theta^0,x)]=0.
\end{equation}   \tag{1}\]</span></p>
<p>Additionally, information about the parameter is available in the form
of a prior density <span class="math inline">\(\pi(\theta)\)</span> supported on <span class="math inline">\(\Theta\)</span>. We assume that
it is neither possible nor desirable to specify <span class="math inline">\(F^0\)</span> in a parametric
form. On the other hand, it is not beneficial to estimate <span class="math inline">\(F^0\)</span>
completely nonparametrically without taking into account the information
from <a href="#eq:smoothfun">(1)</a> in the estimation procedure.</p>
<p>Empirical likelihood provides a semiparametric procedure to estimate
<span class="math inline">\(F^0\)</span>, by incorporating information contained in <a href="#eq:smoothfun">(1)</a>. A
likelihood can be computed from the estimate. Moreover, if some
information about the parameter is available in the form of a prior
distribution, the same likelihood can be employed to derive a posterior
of the parameter given the observations.</p>
<p>Let <span class="math inline">\(F\in\mathcal{F}_{\theta}\)</span> be a distribution function depending on
the parameter <span class="math inline">\(\theta\)</span>. The empirical likelihood is the maximum of the
ânonparametric likelihood"
<span class="math display" id="eq:eqn2">\[\begin{equation}
\label{eqn2}
L(F)=\prod_{i=1}^n \{F(x_i)-F(x_i-)\}
\end{equation}   \tag{2}\]</span>
over <span class="math inline">\(\mathcal{F}_\theta\)</span>, <span class="math inline">\(\theta\in\Theta\)</span>, under constraints
depending on <span class="math inline">\(g(\theta,x)\)</span>.</p>
<p>More specifically, by defining <span class="math inline">\(\omega_i=F(x_i)-F(x_i-)\)</span>, the empirical
likelihood for <span class="math inline">\(\theta\)</span> is defined by,
<span class="math display" id="eq:eqn3">\[\begin{equation}
\label{eqn3}
L(\theta)\mathrel{\mathop:}=\max_{\omega\in\mathcal{W}_{\theta}}\prod_{i=1}^n \omega_i
\end{equation}   \tag{3}\]</span>
where
<span class="math display">\[\mathcal{W}_{\theta}=\Big\{\omega: \sum_{i=1}^n\omega_i g(\theta,x_i)=0\Big\}\cap\Delta_{n-1}\]</span>
and <span class="math inline">\(\Delta_{n-1}\)</span> is the <span class="math inline">\(n-1\)</span> dimensional simplex, i.e.
<span class="math inline">\(\omega_i\geq 0\)</span>, <span class="math inline">\(\forall i\)</span> and <span class="math inline">\(\sum_{i=1}^n\omega_i=1\)</span>. For any
<span class="math inline">\(\theta\)</span>, if the problem in <a href="#eq:eqn3">(3)</a> is infeasible, i.e.
<span class="math inline">\(\mathcal{W}_{\theta}=\emptyset\)</span>, we define <span class="math inline">\(L(\theta)\mathrel{\mathop:}= 0\)</span>.</p>
<p>Using the empirical likelihood <span class="math inline">\(L(\theta)\)</span> and the prior <span class="math inline">\(\pi(\theta)\)</span>
we can define a posterior as:
<span class="math display" id="eq:eqn4">\[\begin{equation}
\label{eqn4}
\Pi(\theta|x)=\frac{L(\theta)\pi(\theta)}{\int L(\theta)\pi(\theta) d\theta}\propto L(\theta)\pi(\theta).
\end{equation}   \tag{4}\]</span></p>
<p>In Bayesian empirical likelihood (BayesEL), <span class="math inline">\(\Pi(\theta|x)\)</span> is used as
the posterior to draw inferences on the parameter.</p>
<p>Returning back to <a href="#eq:eqn3">(3)</a> above, suppose we denote:
<span class="math display" id="eq:eqn5">\[\begin{equation}
\label{eqn5}
\hat{\omega}(\theta)=\mathop{\mathrm{\arg\!\max}}_{\omega\in\mathcal{W}_{\theta}}\prod_{i=1}^n \omega_i.
\qquad\qquad
\Big(\text{ i.e. } L(\theta)=\prod^n_{i=1}\hat{\omega}_i(\theta)\Big)
\end{equation}   \tag{5}\]</span>
Each <span class="math inline">\(\hat\omega_i\geq 0\)</span> if and only if the origin in <span class="math inline">\(\mathbb{R}^q\)</span>
can be expressed as a convex combination of
<span class="math inline">\(g(\theta,x_1),\ldots,g(\theta,x_n)\)</span>. Otherwise, the optimisation
problem is infeasible, and <span class="math inline">\(\mathcal{W}_{\theta}=\emptyset\)</span>.
Furthermore, when <span class="math inline">\(\hat{\omega}_i&gt;0\)</span>, <span class="math inline">\(\forall i\)</span> is feasible, the
solution <span class="math inline">\(\hat{\omega}\)</span> of <a href="#eq:eqn5">(5)</a> is unique.</p>
<p>The estimate of <span class="math inline">\(F^0\)</span> is given by:<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>
<span class="math display">\[\hat{F}^0(x)=\sum_{i=1}^n\hat{\omega}_i(\theta)1_{\{x_i\leq x\}}.\]</span>
The distribution <span class="math inline">\(\hat{F}^0\)</span> is a step function with a jump of
<span class="math inline">\(\hat{\omega}_i(\theta)\)</span> on <span class="math inline">\(x_i\)</span>. If
<span class="math inline">\(\mathcal{W}_{\theta}=\Delta_{n-1}\)</span>, i.e.Â no information about
<span class="math inline">\(g(\theta,x)\)</span> is present, it easily follows that
<span class="math inline">\(\hat{\omega}_i(\theta)=n^{-1}\)</span>, for each <span class="math inline">\(i=1\)</span>, <span class="math inline">\(2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(n\)</span> and
<span class="math inline">\(\hat{F}^0\)</span> is the well-known empirical distribution function.</p>
<p>By construction, <span class="math inline">\(\Pi(\theta|x)\)</span> can only be computed numerically. No
analytic form is available. Inferences are drawn through the
observations from <span class="math inline">\(\Pi(\theta|x)\)</span> sampled using Markov chain Monte Carlo
techniques.</p>
<p>Adaptation of Markov chain Monte Carlo methods to BayesEL applications
poses several challenges. First of all, it is not possible to determine
the full conditional densities in a closed form. So techniques like
Gibbs sampling <span class="citation" data-cites="geman1984stochastic">(<a href="#ref-geman1984stochastic" role="doc-biblioref">Geman and Geman 1984</a>)</span> cannot be used. In most cases,
random walk Metropolis procedures, with carefully chosen step sizes, are
attempted. However, the nature of the support of <span class="math inline">\(\Pi(\theta|x)\)</span>, which
we discuss in detail below, makes the choice of an appropriate step size
extremely difficult.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:scheme"></span>
<img src="figures/dirScan3.jpeg" alt="Schematic illustration of the Empirical likelihood problem. The support of the empirical likelihood is $\Theta_1$, a subset of $\mathbb{R}^d$. We take $n=8$ observations. The estimating equations $g(x,\theta)$ are $q=2$ dimensional. Note that $\Theta_1$ is non-convex and may not be bounded. The convex hull of the $q$-dimensional vectors, i.e., $\mathcal{C}(\theta,x)$, is a pentagon in $\mathbb{R}^2$. The largest faces of $\mathcal{C}(\theta,x)$ are the one-dimensional sides of the pentagon. It follows that, $\theta^{(k)}\in\Theta_1$ iff the origin of $\mathbb{R}^2$, denoted $0_2$ is in the interior $\mathcal{C}^0(\theta,x)$ of $\mathcal{C}(\theta,x)$. This also implies that the optimal empirical likelihood weights $\hat{\omega}(\theta^{(k)})$ are strictly positive and lie in the interior of the $n-1$, i.e. $7$-dimensional simplex. There is no easy way to determine $\Theta_1$. We check if $0_2\in \mathcal{C}^0(\theta,x)$ or equivalently if $\hat{\omega}(\theta^{(k)})$ are in the interior of $\Delta_7$ in order to determine if $\theta^{(k)}\in \Theta_1$. As the sequence $\theta^{(k)}$ approaches the boundary of $\theta_1$, the convex polytope $\mathcal{C}(\theta^{(k)},x)$ changes in such a way, so that $0_2$ converges to its boundary. The sequence of optimal weights $\hat{\omega}(\theta^{(k)})$, will converge to the boundary of $\Delta_7$. The current software is based on @chaudhuriMondalTeng2017, who show that, under simple conditions, along almost every sequence $\theta^{(k)}$ converging to the boundary of $\Theta_1$, at least one component of the gradient of log-empirical likelihood based posterior diverges to positive or negative infinity." width="100%" />
<p class="caption">
Figure 1: Schematic illustration of the Empirical likelihood problem. The support of the empirical likelihood is <span class="math inline">\(\Theta_1\)</span>, a subset of <span class="math inline">\(\mathbb{R}^d\)</span>. We take <span class="math inline">\(n=8\)</span> observations. The estimating equations <span class="math inline">\(g(x,\theta)\)</span> are <span class="math inline">\(q=2\)</span> dimensional. Note that <span class="math inline">\(\Theta_1\)</span> is non-convex and may not be bounded. The convex hull of the <span class="math inline">\(q\)</span>-dimensional vectors, i.e., <span class="math inline">\(\mathcal{C}(\theta,x)\)</span>, is a pentagon in <span class="math inline">\(\mathbb{R}^2\)</span>. The largest faces of <span class="math inline">\(\mathcal{C}(\theta,x)\)</span> are the one-dimensional sides of the pentagon. It follows that, <span class="math inline">\(\theta^{(k)}\in\Theta_1\)</span> iff the origin of <span class="math inline">\(\mathbb{R}^2\)</span>, denoted <span class="math inline">\(0_2\)</span> is in the interior <span class="math inline">\(\mathcal{C}^0(\theta,x)\)</span> of <span class="math inline">\(\mathcal{C}(\theta,x)\)</span>. This also implies that the optimal empirical likelihood weights <span class="math inline">\(\hat{\omega}(\theta^{(k)})\)</span> are strictly positive and lie in the interior of the <span class="math inline">\(n-1\)</span>, i.e.Â <span class="math inline">\(7\)</span>-dimensional simplex. There is no easy way to determine <span class="math inline">\(\Theta_1\)</span>. We check if <span class="math inline">\(0_2\in \mathcal{C}^0(\theta,x)\)</span> or equivalently if <span class="math inline">\(\hat{\omega}(\theta^{(k)})\)</span> are in the interior of <span class="math inline">\(\Delta_7\)</span> in order to determine if <span class="math inline">\(\theta^{(k)}\in \Theta_1\)</span>. As the sequence <span class="math inline">\(\theta^{(k)}\)</span> approaches the boundary of <span class="math inline">\(\theta_1\)</span>, the convex polytope <span class="math inline">\(\mathcal{C}(\theta^{(k)},x)\)</span> changes in such a way, so that <span class="math inline">\(0_2\)</span> converges to its boundary. The sequence of optimal weights <span class="math inline">\(\hat{\omega}(\theta^{(k)})\)</span>, will converge to the boundary of <span class="math inline">\(\Delta_7\)</span>. The current software is based on <span class="citation" data-cites="chaudhuriMondalTeng2017">Chaudhuri et al. (<a href="#ref-chaudhuriMondalTeng2017" role="doc-biblioref">2017</a>)</span>, who show that, under simple conditions, along almost every sequence <span class="math inline">\(\theta^{(k)}\)</span> converging to the boundary of <span class="math inline">\(\Theta_1\)</span>, at least one component of the gradient of log-empirical likelihood based posterior diverges to positive or negative infinity.
</p>
</div>
</div>
<p>Provided that the prior is positive over the whole <span class="math inline">\(\Theta\)</span>, which is
true in most applications, the support of <span class="math inline">\(\Pi(\theta|x)\)</span> is a subset of
the support of the likelihood <span class="math inline">\(L(\theta)\)</span> which can be defined as (see
Figure <a href="#fig:scheme">1</a>):
<span class="math display" id="eq:support">\[\begin{equation}
\label{support}
\Theta_1=\left\{\theta: L(\theta)&gt;0\right\}.
\end{equation}   \tag{6}\]</span>
Thus, the efficiency of the MCMC algorithm would depend on <span class="math inline">\(\Theta_1\)</span>
and the behaviour of <span class="math inline">\(\Pi(\theta|x)\)</span> on it.</p>
<p>By definition, <span class="math inline">\(\Theta_1\)</span> is closely connected to the set
<span class="math display" id="eq:convexhull">\[\begin{equation}
\label{convexhull}
\mathcal{C}(\theta,x)=\left\{\sum_{i=1}^n\omega_ig(\theta,x_i) \Big|\omega\in \Delta_{n-1}\right\},
\end{equation}   \tag{7}\]</span>
which is the closed convex hull of the <span class="math inline">\(q\)</span> dimensional vectors
<span class="math inline">\(G(x,\theta)=\{g(\theta,x_i),\ldots,g(\theta,x_n)\}\)</span> in <span class="math inline">\(\mathbb{R}^q\)</span>
(the pentagon in Figure (fig:scheme)). Suppose <span class="math inline">\(\mathcal{C}^0(\theta,x)\)</span> and
<span class="math inline">\(\partial \mathcal{C}(\theta,x)\)</span> are respectively the interior and
boundary of <span class="math inline">\(\mathcal{C}(\theta,x)\)</span>. By construction,
<span class="math inline">\(\mathcal{C}(\theta,x)\)</span> is a convex polytope. Since the data <span class="math inline">\(x\)</span> is
fixed, the set <span class="math inline">\(\mathcal{C}(\theta,x)\)</span> is a set-valued function of
<span class="math inline">\(\theta\)</span>. For any <span class="math inline">\(\theta\in\Theta\)</span>, the problem in <a href="#eq:eqn3">(3)</a> is
feasible (i.e.Â <span class="math inline">\(\mathcal{W}_{\theta}\ne\emptyset\)</span>) if and only if the
origin of <span class="math inline">\(\mathbb{R}^q\)</span>, denoted by <span class="math inline">\(0_q\)</span>, is in
<span class="math inline">\(\mathcal{C}(\theta,x)\)</span>. That is, <span class="math inline">\(\theta\in\Theta_1\)</span> if and only if the
same <span class="math inline">\(0_q\in\mathcal{C}^0(\theta,x)\)</span>. It is not possible to determine
<span class="math inline">\(\Theta_1\)</span> in general. The only way is to check if, for any potential
<span class="math inline">\(\theta\)</span>, the origin <span class="math inline">\(0_q\)</span> is in <span class="math inline">\(\mathcal{C}^0(\theta,x)\)</span>. There is no
quick numerical way to check the latter either. Generally, an attempt is
made to solve <a href="#eq:eqn3">(3)</a>. The existence of such a solution indicates
that <span class="math inline">\(\theta\in L(\theta)\)</span>.</p>
<p>Examples show <span class="citation" data-cites="chaudhuriMondalTeng2017">(<a href="#ref-chaudhuriMondalTeng2017" role="doc-biblioref">Chaudhuri et al. 2017</a>)</span> that even for simple problems,
<span class="math inline">\(\Theta_1\)</span> may not be a convex set. Designing an efficient random walk
Markov chain Monte Carlo algorithm on a potentially non-convex support
is an extremely challenging task. Unless the step sizes and the proposal
distributions are adapted well to the proximity of the current position
to the boundary of <span class="math inline">\(\Theta_1\)</span>, the chain may repeatedly propose values
outside the likelihood support and, as a result, converge very slowly.
Adaptive algorithms like the one proposed by <span class="citation" data-cites="haario1999adaptive">Haario et al. (<a href="#ref-haario1999adaptive" role="doc-biblioref">1999</a>)</span> do not
tackle the non-convexity problem well.</p>
<p>Hamiltonian Monte Carlo methods solve well-known equations of motion
from classical mechanics to propose new values of <span class="math inline">\(\theta\in\Theta\)</span>.
Numerical solutions of these equations of motion are dependent on the
gradient of the log posterior. The norm of the gradient of the log
empirical likelihood used in BayesEL procedures diverges near the
boundary of <span class="math inline">\(\Theta_1\)</span>. This property makes the Hamiltonian Monte Carlo
procedures very efficient for sampling a BayesEL posterior. It ensures
that once in <span class="math inline">\(\Theta_1\)</span>, the chain would rarely step outside the support
and repeatedly sample from the posterior.</p>
<h4 class="unnumbered" data-number="2.2" id="sec:elprop">A Review of Some Properties of the Gradient of Log Empirical Likelihood</h4>
<p>Various properties of log-empirical likelihood have been discussed in
the literature. However, the properties of its gradients with respect to
the model parameters are relatively unknown. Our main goal in this
section is to review the behaviour of gradients of log-empirical
likelihood on the support of the empirical likelihood. We only state the
relevant results here. The proofs of these results can be found in
<span class="citation" data-cites="chaudhuriMondalTeng2017">Chaudhuri et al. (<a href="#ref-chaudhuriMondalTeng2017" role="doc-biblioref">2017</a>)</span>.</p>
<p>Recall that, (see Figure (fig:scheme)) the support <span class="math inline">\(\Theta_1\)</span> can only be specified by
checking if <span class="math inline">\(0_q\in\mathcal{C}^0(x,\theta_0)\)</span> for each individual
<span class="math inline">\(\theta_0\in\Theta\)</span>. If for some <span class="math inline">\(\theta_0\in\Theta\)</span>, the origin lies on
the boundary of <span class="math inline">\(\mathcal{C}(x,\theta_0)\)</span>, i.e.
<span class="math inline">\(0_q\in\partial \mathcal{C}(x,\theta_0)\)</span>, the problem in <a href="#eq:eqn3">(3)</a>
is still feasible, however, <span class="math inline">\(L\left(\theta_0\right)=0\)</span> and the solution
of <a href="#eq:eqn5">(5)</a> is not unique. Below we discuss how, under mild
conditions, for any <span class="math inline">\(\theta_0\in\Theta\)</span>, for a large subset
<span class="math inline">\(S\subseteq\partial \mathcal{C}(x,\theta_0)\)</span>, if <span class="math inline">\(0_q\in S\)</span>, the
absolute value of at least one component of the gradient of
<span class="math inline">\(\log\left(L\left(\theta_0\right)\right)\)</span> would be large.</p>
<p>Before we proceed, we make the following assumptions:</p>
<ol type="1">
<li><p><span class="math inline">\(\Theta\)</span> is an open set. <span id="A0" data-label="A0"></span></p></li>
<li><p><span class="math inline">\(g\)</span> is a continuously differentiable function of <span class="math inline">\(\theta\)</span> in
<span class="math inline">\(\Theta\)</span>, <span class="math inline">\(q \le d\)</span> and <span class="math inline">\(\Theta_1\)</span> is non-empty. <span id="A1" data-label="A1"></span></p></li>
<li><p>The sample size <span class="math inline">\(n &gt; q\)</span>. The matrix <span class="math inline">\(G(x, \theta)\)</span> has full row rank
for any <span class="math inline">\(\theta \in \Theta\)</span>.</p></li>
<li><p>For any fixed <span class="math inline">\(x\)</span>, let <span class="math inline">\(\nabla g(x_i,\theta)\)</span> be the <span class="math inline">\(q \times d\)</span>
Jacobian matrix for any <span class="math inline">\(\theta \in \Theta\)</span>. Suppose
<span class="math inline">\(w=(w_1,\ldots, w_n)\in\Delta_{n-1}\)</span> and there are at least <span class="math inline">\(q\)</span>
elements of <span class="math inline">\(w\)</span> that are greater than <span class="math inline">\(0\)</span>. Then, for any
<span class="math inline">\(\theta \in \Theta\)</span>, the matrix
<span class="math inline">\(\sum_{i=1}^n w_i \nabla g(x_i,\theta)\)</span> has full row rank.</p></li>
</ol>
<p>Under the above assumptions, several results about the log empirical
likelihood and its gradient can be deduced.</p>
<p>First of all, since the properties of the gradient of the log empirical
likelihood at the boundary of the support are of interest, some
topological properties of the support need to be investigated. Under the
standard topology of <span class="math inline">\(\mathbb{R}^q\)</span>, since <span class="math inline">\(\mathcal{C}(x,\theta)\)</span> is a
convex polytope with a finite number of faces and extreme points, using
the smoothness of <span class="math inline">\(g\)</span>, it is easy to see that, for any
<span class="math inline">\(\theta_0\in\Theta_1\)</span> one can find a real number <span class="math inline">\(\delta&gt;0\)</span>, such that
the open ball centred at <span class="math inline">\(\theta_0\)</span> with radius <span class="math inline">\(\delta\)</span> is contained in
<span class="math inline">\(\Theta_1\)</span>. That is, <span class="math inline">\(\Theta_1\)</span> is an open subset of <span class="math inline">\(\Theta\)</span>.</p>
<p>Now, since <span class="math inline">\(\Theta_1\)</span> is an open set, the boundary <span class="math inline">\(\partial\Theta_1\)</span> of
<span class="math inline">\(\Theta_1\)</span> is not contained in <span class="math inline">\(\Theta_1\)</span>. Let <span class="math inline">\(\theta^{(0)}\)</span> lie within
<span class="math inline">\(\Theta\)</span> and on the boundary of <span class="math inline">\(\Theta_1\)</span> (i.e.Â <span class="math inline">\(\partial\Theta_1\)</span>).
Then it follows that the primal problem <a href="#eq:eqn3">(3)</a> is feasible at
<span class="math inline">\(\theta^{(0)}\)</span> and <span class="math inline">\(0_q\)</span> lies on the boundary of
<span class="math inline">\(\mathcal{C}(x,\theta^{(0)})\)</span> (i.e.
<span class="math inline">\(\partial \mathcal{C}(x,\theta^{(0)})\)</span>).</p>
<p>Our main objective is to study the utility of Hamiltonian Monte Carlo
methods for drawing samples from a BayesEL posterior. The sampling
scheme will produce a sequence of sample points in
<span class="math inline">\(\theta^{(k)}\in\Theta_1\)</span> (see Figure
<a href="#fig:scheme">1</a>). It would
be efficient as long as <span class="math inline">\(\log L\left(\theta^{(k)}\right)\)</span> is large. The
sampling scheme could potentially become inefficient if some
<span class="math inline">\(\theta^{(k)}\)</span> is close to the boundary <span class="math inline">\(\partial\Theta_1\)</span>. Thus, it is
sufficient to consider the properties of the log empirical likelihood
and its gradient along such a sequence converging to a point
<span class="math inline">\(\theta^{(0)}\in\partial\Theta_1\)</span>.</p>
<p>From the discussion above it is evident that when
<span class="math inline">\(\theta^{(0)} \in \partial \Theta_1\)</span> the problem in <a href="#eq:eqn3">(3)</a> is
feasible but the likelihood <span class="math inline">\(L\left(\theta^{(0)}\right)\)</span> will always be
zero and <a href="#eq:eqn5">(5)</a> will not have a unique solution. Since
<span class="math inline">\(\mathcal{C}(x,\theta^{(0)})\)</span> is a polytope, and <span class="math inline">\(0_q\)</span> lies on one of
its faces, there exists a subset <span class="math inline">\(\mathcal{I}_0\)</span> of the observations and
<span class="math inline">\(0\)</span> belongs to the interior of the convex hull generated by all
<span class="math inline">\(g(x_i,\theta^{(0)})\)</span> for <span class="math inline">\(i \in \mathcal{I}_0\)</span> (in Figure
<a href="#fig:scheme">1</a>,
<span class="math inline">\(\mathcal{I}_0=\{x_4,x_5\}\)</span>). It follows from the supporting hyperplane
theorem <span class="citation" data-cites="boyd2004convex">(<a href="#ref-boyd2004convex" role="doc-biblioref">Boyd and Vandenberghe 2004</a>)</span> that there exists a unit vector
<span class="math inline">\(a\in \mathbb{R}^q\)</span> such that
<span class="math display">\[a^{\text{\tiny T}} g(x_i, \theta^{(0)}) =0 \quad \mbox{for} \quad i \in \mathcal{I}_0, \qquad\text{and}\qquad a^{\text{\tiny T}} g(x_{i}, \theta^{(0)}) &gt;0 \quad \mbox{for} \quad i \in \mathcal{I}_0^c.\]</span>
From some algebraic manipulation it easily follows that any
<span class="math inline">\(\omega\in\mathcal{W}_{\theta^{(0)}}\)</span> (<span class="math inline">\(\mathcal{W}_{\theta}\)</span> as defined
in <a href="#eq:eqn3">(3)</a> with <span class="math inline">\(\theta=\theta^{(0)}\)</span>) must satisfy,<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>
<span class="math display">\[\omega_i=0 \quad \mbox{for} \quad i \in \mathcal{I}_0^c \qquad\text{and}\qquad \omega_i&gt;0 \quad \mbox{for} \quad i \in \mathcal{I}_0.\]</span></p>
<p>It is well known that the solution of <a href="#eq:eqn5">(5)</a> i.e.
<span class="math inline">\(\hat{w}(\theta)\)</span> is smooth for all <span class="math inline">\(\theta\in\Theta_1\)</span>
<span class="citation" data-cites="qin1994empirical">(<a href="#ref-qin1994empirical" role="doc-biblioref">Qin and Lawless 1994</a>)</span>. As <span class="math inline">\(\theta^{(k)}\)</span> converges to <span class="math inline">\(\theta^{(0)}\)</span>, the
properties of <span class="math inline">\(\hat{w}(\theta^{(k)})\)</span> need to be considered. To that
goal, we first make a specific choice of <span class="math inline">\(\hat{w}(\theta^{(0)})\)</span>.</p>
<p>First, we consider a restriction of problem <a href="#eq:eqn5">(5)</a> to
<span class="math inline">\(\mathcal{I}_0\)</span>.</p>
<p><span class="math display" id="eq:submax">\[\begin{equation}
\label{submax}
\hat\nu(\theta)  =\mathop{\mathrm{\arg\!\max}}_{\nu\in\mathcal{V}_\theta} \prod_{i\in\mathcal{I}_0} \nu_i
\end{equation}   \tag{8}\]</span>
where
<span class="math display">\[\mathcal{V}_\theta=\left\{\nu: \sum_{i\in \mathcal{I}_0}\nu_i g(x_i,\theta)=0\right\}\cap\Delta_{|\mathcal{I}_0|-1}.\]</span>
We now define
<span class="math display">\[\hat  \omega_i(\theta^{(0)}) = \hat\nu(\theta^{(0)}), \quad i \in \mathcal{I}_0 \quad \mbox{and} \quad \hat  \omega_i(\theta^{(0)}) = 0, \quad i \in \mathcal{I}_0^c,\]</span>
and
<span class="math display">\[L(\theta^{(0)})= \prod_{i=1}^n \hat  \omega_i(\theta^{(0)}).\]</span></p>
<p>Since <span class="math inline">\(\theta^{(0)}\)</span> is in the interior of <span class="math inline">\(\mathcal{I}_0\)</span>, the problem
<a href="#eq:submax">(8)</a> has a unique solution. For each
<span class="math inline">\(\theta^{(k)}\in\Theta_1\)</span>, <span class="math inline">\(\hat{\omega}(\theta^{(k)})\)</span> is continuous
taking values in a compact set. Thus as <span class="math inline">\(\theta^{(k)}\)</span> converges to
<span class="math inline">\(\theta^{(0)}\)</span>, <span class="math inline">\(\hat{\omega}(\theta^{(k)})\)</span> converges to a limit.
Furthermore, this limit is a solution of <a href="#eq:eqn5">(5)</a> at
<span class="math inline">\(\theta^{(0)}\)</span>. However, counterexamples show <span class="citation" data-cites="chaudhuriMondalTeng2017">(<a href="#ref-chaudhuriMondalTeng2017" role="doc-biblioref">Chaudhuri et al. 2017</a>)</span>
that the limit may not be <span class="math inline">\(\hat{\omega}_i(\theta^{(0)})\)</span> as defined
above. That is, the vectors <span class="math inline">\(\hat{\omega}(\theta^{(k)})\)</span> do not extend
continuously to the boundary <span class="math inline">\(\partial\Theta_1\)</span> as a whole. However, we
can show that:
<span class="math display">\[\lim_{k\to\infty}\hat\omega_i(\theta^{(k)}) = \hat \omega_i(\theta^{(0)})   = 0, \text{for all\ } i \in \mathcal{I}_0^c.\]</span>
That is, the components of <span class="math inline">\(\hat\omega(\theta^{(k)})\)</span> which are zero in
<span class="math inline">\(\hat\omega(\theta^{(0)})\)</span> are continuously extendable. Furthermore,
<span class="math display">\[\lim_{k\to\infty}L(\theta^{(k)})=L(\theta^{(0)})=0.\]</span>
That is, the likelihood is continuous at <span class="math inline">\(\theta^{(0)}\)</span>.</p>
<p>However, this is not true for the components
<span class="math inline">\(\hat{\omega}_i\left(\theta^{(k)}\right)\)</span>, <span class="math inline">\(i\in\mathcal{I}_0\)</span> for which
<span class="math inline">\(\hat{\omega}_i\left(\theta^{(k)}\right)&gt; 0\)</span>.</p>
<p>Since the set <span class="math inline">\(\mathcal{C}(x,\theta)\)</span> is a convex polytope in
<span class="math inline">\(\mathbb{R}^q\)</span>, the maximum dimension of any of its faces is <span class="math inline">\(q-1\)</span>,
which would have exactly <span class="math inline">\(q\)</span> extreme points.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> Furthermore, any face
with a smaller dimension can be expressed as an intersection of such
<span class="math inline">\(q-1\)</span> dimensional faces.</p>
<p>In certain cases, however, the whole vector
<span class="math inline">\(\hat{\omega}\left(\theta^{(k)}\right)\)</span> extends continuously to
<span class="math inline">\(\hat{\omega}\left(\theta^{(0)}\right)\)</span>. In order to argue that, we
define
<span class="math display" id="eq:Theta-2">\[\begin{equation}
\label{Theta_2}
  \mathcal{C}(x_{\mathcal{I}},\theta) = \left\{\sum_{i \in \mathcal{I}} \omega_i g(x_i,\theta)\, \Big|\, \omega\in \Delta_{|\mathcal{I}|-1}\right\}
\end{equation}   \tag{9}\]</span>
and
<span class="math display">\[\begin{equation}
  \partial\Theta_1^{(q-1)} = \Big\{  \theta:  0 \in  \mathcal{C}^0(x_{\mathcal{I}},\theta)  \mbox{ for some } \mathcal{I}~ s.t. \mathcal{C}(x_{\mathcal{I}},\theta)  \text{has exactly q extreme points} \Big\}  \cap\partial\Theta_1.
\end{equation}\]</span></p>
<p>Thus <span class="math inline">\(\partial\Theta_1^{(q-1)}\)</span> is the set of all boundary points
<span class="math inline">\(\theta^{(0)}\)</span> of <span class="math inline">\(\Theta_1\)</span> such that <span class="math inline">\(0\)</span> belongs to a
<span class="math inline">\((q-1)\)</span>-dimensional face of the convex hull
<span class="math inline">\(\mathcal{C}(x,\theta^{(0)})\)</span>. Now for any
<span class="math inline">\(\theta^{(0)}\in \partial\Theta_1^{(q-1)}\)</span>, there is a unique set of
weight <span class="math inline">\(\nu\in\Delta_{|\mathcal{I}|-1}\)</span> such that,
<span class="math inline">\(\sum_{i\in\mathcal{I}}\nu_ig\left(x_i,\theta^{(0)}\right)=0\)</span>. That is
the set of feasible solutions of <a href="#eq:submax">(8)</a> is a singleton set.
This, after taking note that <span class="math inline">\(\hat{\omega}\)</span> takes values in a compact
set, an argument using convergent subsequences, implies that for any
sequence <span class="math inline">\(\theta^{(k)}\in\Theta_1\)</span> converging to <span class="math inline">\(\theta^{(0)}\)</span>, the
whole vector <span class="math inline">\(\hat{\omega}\left(\theta^{(k)}\right)\)</span> converges to
<span class="math inline">\(\hat{\omega}\left(\theta^{(0)}\right)\)</span>. That is, the whole vector
<span class="math inline">\(\hat{\omega}\left(\theta^{(k)}\right)\)</span> extends continuously to
<span class="math inline">\(\hat{\omega}\left(\theta^{(0)}\right)\)</span>.</p>
<p>We now consider the behaviour of the gradient of the log empirical
likelihood near the boundary of <span class="math inline">\(\Theta_1\)</span>. First, note that, for any
<span class="math inline">\(\theta \in \Theta_1\)</span>, the gradient of the log empirical likelihood is
given by
<span class="math display">\[\nabla \log L(\theta)  =  -n\sum_{i=1}^n \hat \omega_i(\theta)    \hat{\lambda}(\theta)^{\text{\tiny T}}  \nabla g(x_i,\theta).\]</span>
where <span class="math inline">\(\hat{\lambda}(\theta)\)</span> is the estimated Lagrange multiplier
satisfying the equation:</p>
<p><span class="math display" id="eq:lagmult">\[\begin{equation}
\label{eq:lagmult}
\sum_{i=1}^n \frac{g(x_i,\theta)}{\left\{1+ \hat\lambda(\theta)^{\text{\tiny T}} g(x_i,\theta) \right\}}=0.
\end{equation}   \tag{10}\]</span></p>
<p>Note that, the gradient depends on the value of the Lagrange multiplier
but not on the value of its gradient.</p>
<p>Now, Under assumption A3, it follows that the gradient of the log
empirical likelihood diverges on the set of all boundary points
<span class="math inline">\(\partial\Theta_1^{(q-1)}\)</span>. More specifically one can show:</p>
<ol type="1">
<li><p>As <span class="math inline">\(\theta^{(k)}\rightarrow \theta^{(0)}\)</span>,
<span class="math inline">\(\parallel\hat \lambda(\theta^{(k)})\parallel\to\infty\)</span>.</p></li>
<li><p>If <span class="math inline">\(\theta^{(0)}\in \partial\Theta_1^{(q-1)}\)</span>, under as
<span class="math inline">\(\theta^{(k)}\rightarrow \theta^{(0)}\)</span>,
<span class="math inline">\({\parallel \nabla \log L(\theta^{(k)}) \parallel}\to \infty\)</span>.</p></li>
</ol>
<p>Therefore, it follows that at every boundary point <span class="math inline">\(\theta^{(0)}\)</span> of
<span class="math inline">\(\Theta_1\)</span> such that <span class="math inline">\(0\)</span> belongs to one of the <span class="math inline">\((q-1)\)</span>-dimensional faces
of <span class="math inline">\(\mathcal{C}(x,\theta^{(0)})\)</span>, at least one component of the
estimated Lagrange multiplier and the gradient of the log empirical
likelihood diverges to positive or negative infinity. The gradient of
the negative log empirical likelihood represents the direction of the
steepest increase of the negative log empirical likelihood. Since the
value of the log empirical likelihood should typically be highest around
the center of the support <span class="math inline">\(\Theta_1\)</span>, the gradient near the boundary of
<span class="math inline">\(\Theta_1\)</span> should point towards its center. This property can be
exploited in forcing candidates of <span class="math inline">\(\theta\)</span> generated by HMC proposals
to bounce back towards the interior of <span class="math inline">\(\Theta_1\)</span> from its boundaries
and in consequence reducing the chance of them getting out of the
support.</p>
<h4 class="unnumbered" data-number="2.3" id="sec:hmc">Hamiltonian Monte Carlo Sampling for Bayesian Empirical Likelihood</h4>
<p>Hamiltonian Monte Carlo algorithm is a Metropolis algorithm where the
successive steps are proposed by using Hamiltonian dynamics. One can
visualise these dynamics as a cube sliding without friction under
gravity in a bowl with a smooth surface. The total energy of the cube is
the sum of the potential energy <span class="math inline">\(U(\theta)\)</span>, defined by its position
<span class="math inline">\(\theta\)</span> (in this case its height) and kinetic energy <span class="math inline">\(K(p)\)</span>, which is
determined by its momentum <span class="math inline">\(p\)</span>. The total energy of the cube will be
conserved and it will continue to slide up and down on the smooth
surface of the bowl forever. The potential and the kinetic energy would,
however, vary with the position of the cube.</p>
<p>In order to use the Hamiltonian dynamics to sample from the posterior
<span class="math inline">\(\Pi\left(\theta\mid x\right)\)</span> we set our potential and kinetic energy
as follows:
<span class="math display">\[U(\theta)=-\log\Pi(\theta|x)\quad\text{and}\quad K(p)=\frac{1}{2}p^TM^{-1}p.\]</span>
Here, the momentum vector <span class="math inline">\(p=\left(p_1,p_2,\ldots,p_d\right)\)</span> is a
totally artificial construct usually generated from a <span class="math inline">\(N(0, M)\)</span>
distribution. Most often the covariance matrix <span class="math inline">\(M\)</span> is chosen to be a
diagonal matrix with diagonal <span class="math inline">\((m_1,m_2,\ldots,m_d)\)</span>, in which case each
<span class="math inline">\(m_i\)</span> is interpreted as the mass of the <span class="math inline">\(i\)</span>th parameter. The Hamiltonian
of the system is the total energy
<span class="math display" id="eq:hamiltonian-dynamics">\[\begin{equation}
\label{hamiltonian dynamics}
\mathcal{H}(\theta,p)=U(\theta)+K(p).
\end{equation}   \tag{11}\]</span></p>
<p>In Hamiltonian mechanics, the variation in the position <span class="math inline">\(\theta\)</span> and
momentum <span class="math inline">\(p\)</span> with time <span class="math inline">\(t\)</span> is determined by the partial derivatives of
<span class="math inline">\(\mathcal{H}\)</span> with <span class="math inline">\(p\)</span> and <span class="math inline">\(\theta\)</span> respectively. In particular, the
motion is governed by the pair of so-called Hamiltonian equations:
<span class="math display" id="eq:PDE1">\[\begin{eqnarray}
\frac{d\theta}{dt}&amp;=&amp;\frac{\partial \mathcal{H}}{\partial p}=M^{-1}p, \label{PDE1}\\
\frac{dp}{dt}&amp;=&amp;-\frac{\partial\mathcal{ H}}{\partial \theta}=-\frac{\partial U(\theta)}{\partial \theta}.\label{PDE2}
\end{eqnarray}   \tag{12}\]</span>
It is easy to show that <span class="citation" data-cites="neal2011mcmc">(<a href="#ref-neal2011mcmc" role="doc-biblioref">Neal 2011</a>)</span> Hamiltonian dynamics is
reversible, invariant, and volume preserving, which makes it suitable
for MCMC sampling schemes.</p>
<p>In HMC we propose successive states by solving the Hamiltonian equations
<a href="#eq:PDE1">(12)</a> and <a href="#PDE2" data-reference-type="eqref" data-reference="PDE2"><span class="math display">\[PDE2\]</span></a>. Unfortunately, they cannot be solved analytically
(except of course for a few simple cases), and they must be approximated
numerically at discrete time points. There are several ways to
numerically approximate these two equations in the literature
<span class="citation" data-cites="leimkuhler2004simulating">(<a href="#ref-leimkuhler2004simulating" role="doc-biblioref">Leimkuhler and Reich 2004</a>)</span>. For the purpose of MCMC sampling, we need a
method that is reversible and volume-preserving.</p>
<p>Leapfrog integration <span class="citation" data-cites="birdsall2004plasma">(<a href="#ref-birdsall2004plasma" role="doc-biblioref">Birdsall and Langdon 2004</a>)</span> is one such method to
numerically integrate the pair of Hamiltonian equations. In this method,
a step-size <span class="math inline">\(\epsilon\)</span> for the time variable <span class="math inline">\(t\)</span> is first chosen. Given
the value of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(p\)</span> at the current time point <span class="math inline">\(t\)</span> (denoted
here by <span class="math inline">\(\theta(t)\)</span> and <span class="math inline">\(p(t)\)</span> respectively), the leapfrog updates the
position and the momentum at time <span class="math inline">\(t+\epsilon\)</span> as follows
<span class="math display" id="eq:leapfrog1">\[\begin{eqnarray}
p\left(t+\frac{\epsilon}{2}\right)&amp;=&amp;p(t)-\frac{\epsilon}{2}\frac{\partial U(\theta(t))}{\partial\theta},\label{leapfrog1}\\
\theta(t+\epsilon)&amp;=&amp;\theta(t)+\epsilon M^{-1}p\left(t+\frac{\epsilon}{2}\right),\label{leapfrog2}\\
p(t+\epsilon)&amp;=&amp;p\left(t+\frac{\epsilon}{2}\right)-\frac{\epsilon}{2}\frac{\partial U(\theta(t+\epsilon))}{\partial\theta}.\label{leapfrog3}
\end{eqnarray}   \tag{13}\]</span></p>
<p>Theoretically, due to its symmetry, the leapfrog integration satisfies
the reversibility and preserves the volume. However, because of the
numerical inaccuracies, the volume is not preserved. This is similar to
the Langevin-Hastings algorithm <span class="citation" data-cites="besag2004markov">(<a href="#ref-besag2004markov" role="doc-biblioref">Besag 2004</a>)</span>, which is a special
case of HMC. Fortunately, the lack of invariance in volume is easily
corrected. The accept-reject step in the MCMC procedure ensures that the
chain converges to the correct posterior.</p>
<p>At the beginning of each iteration of the HMC algorithm, the momentum
vector <span class="math inline">\(p\)</span> is randomly sampled from the <span class="math inline">\(N(0,M)\)</span> distribution. Starting
with the current state <span class="math inline">\((\theta,p)\)</span>, the leapfrog integrator described
above is used to simulate Hamiltonian dynamics for <span class="math inline">\(T\)</span> steps with a step
size of <span class="math inline">\(\epsilon\)</span>. At the end of this <span class="math inline">\(T\)</span>-step trajectory, the momentum
<span class="math inline">\(p\)</span> is negated so that the Metropolis proposal is symmetric. At the end
of this T-step iteration, the proposed state <span class="math inline">\((\theta^*,p^*)\)</span> is
accepted with probability
<span class="math display">\[\min\{1,\exp(-\mathcal{H}(\theta^*,p^*)+\mathcal{H}(\theta,p))\}.\]</span></p>
<p>The gradient of the log-posterior used in the leapfrog is a sum of the
gradient of the log empirical likelihood and the gradient of the log
prior. The prior is user-specified and it is hypothetically possible
that even though at least one component of the gradient of the log
empirical likelihood diverges at the boundary <span class="math inline">\(\partial\Theta_1\)</span>, the
log prior gradient may behave in a way so that the effect is nullified
and the log posterior gradient remains finite over the closure of
<span class="math inline">\(\Theta_1\)</span>. We make the following assumption on the prior mainly to
avoid this possibility (see <span class="citation" data-cites="chaudhuriMondalTeng2017">Chaudhuri et al. (<a href="#ref-chaudhuriMondalTeng2017" role="doc-biblioref">2017</a>)</span> for more details).</p>
<p><span class="math display" id="eq:liminf2">\[\begin{equation}
   \liminf_{k\to\infty}   \frac{ \log\pi(\theta^{(k-1)})  -  \log\pi(\theta^{(k)}) }{ \log L(\theta^{(k-1)} ) - \log L(\theta^{(k)} ) } \ge b(n, \theta^{(0)}).
   \tag{14}
  \end{equation}\]</span></p>
<ul>
<li>Consider a sequence <span class="math inline">\(\{\theta^{(k)} \}\)</span>, <span class="math inline">\(k=1, 2,\ldots\)</span>, of points
in <span class="math inline">\(\Theta_1\)</span> such that <span class="math inline">\(\theta^{(k)}\)</span> converges to a boundary point
<span class="math inline">\(\theta^{(0)}\)</span> of <span class="math inline">\(\Theta_1\)</span>. Assume that <span class="math inline">\(\theta^{(0)}\)</span> lies within
<span class="math inline">\(\Theta\)</span> and <span class="math inline">\(L(\theta^{(k)})\)</span> strictly decreases to
<span class="math inline">\(L(\theta^{(0)})\)</span>, Then, for some constant
<span class="math inline">\(b(n, \theta^{(0)}) &gt; -1\)</span>, we have</li>
</ul>
<p>The assumption implies that near the boundary of the support, the main
contribution in the gradient of the log-posterior with respect to any
parameter appearing in the argument of the estimating equations comes
from the corresponding gradient of the log empirical likelihood. This is
in most cases expected, especially if the sample size is large. For a
large sample size, the log-likelihood should be the dominant term in the
log-posterior. We are just assuming here that the gradients behave the
same way. It would also ensure that at the boundary, the gradient of the
log-likelihood and the log-posterior do not cancel each other, which is
crucial for the proposed Hamiltonian Monte Carlo to work.</p>
<p>Under these assumptions, <span class="citation" data-cites="chaudhuriMondalTeng2017">Chaudhuri et al. (<a href="#ref-chaudhuriMondalTeng2017" role="doc-biblioref">2017</a>)</span> show that the gradient
of the log-posterior diverges along almost every sequence as the
parameter values approach the boundary <span class="math inline">\(\partial \Theta_1\)</span> from the
interior of the support. More specifically, they prove that:</p>
<p><span class="math display" id="eq:postdiv">\[\begin{equation}
\label{eq:postdiv}
\Bigl\| \nabla \log \pi(\theta^{(k)} \mid x) \Bigr\| \rightarrow \infty, \hspace{.1in} \mbox{ as } \hspace{.1in} k \rightarrow \infty.
\end{equation}   \tag{15}\]</span></p>
<p>Since the <span class="math inline">\(q-1\)</span> dimensional faces of <span class="math inline">\(\mathcal{C}(x,\theta^{(0)})\)</span> have
larger volume than its faces with lower dimension (see Figure
<a href="#fig:scheme">1</a>), a random
sequence of points from the interior to the boundary would converge to a
point on <span class="math inline">\(\partial \Theta_1^{(q-1)}\)</span> with probability <span class="math inline">\(1\)</span>. Thus under
our assumptions, the gradient of the log-posterior would diverge to
infinity for these sequences with a high probability. The lower
dimensional faces of the convex hull (a polytope) are an intersection of
<span class="math inline">\(q-1\)</span> dimensional faces. Although, it is not clear if the norm of the
gradient of the posterior will diverge on those faces. It is conjectured
that this would happen. However, even if the conjecture is not true,
from the setup, it is clear that the sampler would rarely move to the
region where the origin belongs to the lower dimensional faces of the
convex hull.</p>
<p>As has been pointed out above, the gradient vector would always point
towards the mode of the posterior. From our results, since the gradient
is large near the support boundary, whenever the HMC sampler approaches
the boundary due to the high value of the gradient it would reflect
towards the interior of the support and not get out of it. The leapfrog
parameters can be controlled to increase efficiency of sampling.</p>
<h3 data-number="3" id="sec:package"><span class="header-section-number">3</span> Package description</h3>
<p>The main function of the package is <code>ELHMC</code>. It draws samples from an
empirical likelihood Bayesian posterior of the parameter of interest
using Hamiltonian Monte Carlo once the estimating equations involving
the parameters, the prior distribution of the parameters, the gradients
of the estimating equations, and the log priors are specified. Some
other parameters which control the HMC process can also be specified.</p>
<p>Suppose that the data set consists of observations
<span class="math inline">\(x = \left( x_1, ..., x_n \right)\)</span> where each <span class="math inline">\(x_i\)</span> is a vector of
length <span class="math inline">\(p\)</span> and follows a probability distribution <span class="math inline">\(F\)</span> of family
<span class="math inline">\(\mathcal{F}_{\theta}\)</span>. Here
<span class="math inline">\(\theta = \left(\theta_1,...,\theta_d\right)\)</span> is the <span class="math inline">\(d-\)</span>dimensional
parameter of interest associated with <span class="math inline">\(F\)</span>. Suppose there exist smooth
functions
<span class="math inline">\(g\left(\theta, x_i\right) = \left(g_1\left(\theta, x_i\right)\right.\)</span>,
<span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\left. g_q\left(\theta, x_i\right)\right)^T\)</span> which satisfy
<span class="math inline">\(E_F\left[g\left(\theta,x_i\right)\right] = 0\)</span>. As we have explained
above, <code>ELHMC</code> is used to draw samples of <span class="math inline">\(\theta\)</span> from its posterior
defined by an empirical likelihood.</p>
<div class="layout-chunk" data-layout="l-body">
<table>
<caption><span id="tab:T1">Table 1: </span>Arguments for function <code>ELHMC</code></caption>
<colgroup>
<col style="width: 6%" />
<col style="width: 93%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>initial</code></td>
<td style="text-align: left;">A vector containing the initial values of the parameter</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>data</code></td>
<td style="text-align: left;">A matrix containing the data</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>fun</code></td>
<td style="text-align: left;">The estimating function <span class="math inline">\(g\)</span>. It takes in a parameter vector <code>params</code> as the first argument and a data point vector <code>x</code> as the second parameter. This function returns a vector.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>dfun</code></td>
<td style="text-align: left;">A function that calculates the gradient of the estimating function <span class="math inline">\(g\)</span>. It takes in a parameter vector <code>params</code> as the first argument and a data point vector <code>x</code> as the second argument. This function returns a matrix.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>prior</code></td>
<td style="text-align: left;">A function with one argument <code>x</code> that returns the log joint prior density of the parameters of interest.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>dprior</code></td>
<td style="text-align: left;">A function with one argument <code>x</code> that returns the gradients of the log densities of the parameters of interest</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>n.samples</code></td>
<td style="text-align: left;">Number of samples to draw</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>lf.steps</code></td>
<td style="text-align: left;">Number of leap frog steps in each Hamiltonian Monte Carlo update (defaults to <span class="math inline">\(10\)</span>).</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>epsilon</code></td>
<td style="text-align: left;">The leap frog step size (defaults to <span class="math inline">\(0.05\)</span>).</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>p.variance</code></td>
<td style="text-align: left;">The covariance matrix of a multivariate normal distribution used to generate the initial values of momentum <code>p</code> in Hamiltonian Monte Carlo. This can also be a single numeric value or a vector (defaults to <span class="math inline">\(0.1\)</span>).</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>tol</code></td>
<td style="text-align: left;">EL tolerance</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>detailed</code></td>
<td style="text-align: left;">If this is set to <code>TRUE</code>, the function will return a list with extra information.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>print.interval</code></td>
<td style="text-align: left;">The frequency at which the results would be printed on the terminal. Defaults to 1000.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>plot.interval</code></td>
<td style="text-align: left;">The frequency at which the drawn samples would be plotted. The last half of the samples drawn are plotted after each plot.interval steps. The acceptance rate is also plotted. Defaults to 0, which means no plot.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>which.plot</code></td>
<td style="text-align: left;">The vector of parameters to be plotted after each plot.interval. Defaults to NULL, which means no plot.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>FUN</code></td>
<td style="text-align: left;">the same as <code>fun</code> but takes in a matrix <code>X</code> instead of a vector <code>x</code> and returns a matrix so that <code>FUN(params, X)[i, ]</code> is the same as <code>fun(params, X[i, ])</code>. Only one of <code>FUN</code> and <code>fun</code> should be provided. If both are then <code>fun</code> is ignored.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>DFUN</code></td>
<td style="text-align: left;">the same as <code>dfun</code> but takes in a matrix <code>X</code> instead of a vector <code>x</code> and returns an array so that <code>DFUN(params, X)[, , i]</code> is the same as <code>dfun(params, X[i, ])</code>. Only one of <code>DFUN</code> and <code>dfun</code> should be provided. If both are then <code>dfun</code> is ignored.</td>
</tr>
</tbody>
</table>
</div>
<p>Table <a href="#tab:T1">1</a> enlists the full list of arguments for <code>ELHMC</code>. Arguments <code>data</code> and <code>fun</code> define the problem. They
are the data set <span class="math inline">\(x\)</span> and the collection of smooth functions in <span class="math inline">\(g\)</span>. The
user-specified starting point for <span class="math inline">\(\theta\)</span> is given in <code>initial</code>, whereas <code>n.samples</code> is the
number of samples of <span class="math inline">\(\theta\)</span> to be drawn. The gradient matrix of <span class="math inline">\(g\)</span>
with respect to the parameter <span class="math inline">\(\theta\)</span> (i.e.Â <span class="math inline">\(\nabla_{\theta}g\)</span>) has to
be specified in <code>dfun</code>. At the moment the function does not compute the
gradient numerically by itself. The prior <code>prior</code> represents the joint density
functions of <span class="math inline">\(\theta_1,..,\theta_q\)</span>, which for the purpose of this
description we denote by <span class="math inline">\(\pi\)</span>. The gradient of the log prior function
is specified in <code>dprior</code>. The function returns a vector containing the values of
<span class="math inline">\(\frac{\partial}{\partial \theta_1}\pi\left(\theta\right),..,\frac{\partial}{\partial\theta_d}\pi\left(\theta\right)\)</span>.
Finally, the arguments <code>epsilon</code>, <code>lf.steps</code>, <code>p.variance</code> and <code>tol</code> are hyper-parameters which control the
Hamiltonian Monte Carlo algorithm.</p>
<p>The arguments <code>print.interval</code>, <code>plot.interval</code>, and <code>which.plot</code> can be used to tune the HMC samplers. They can be
used for printing and plotting the sampled values at specified intervals
while the code is running. The argument <code>which.plot</code> allows the user to only plot the
variables whose convergence needs to be checked.</p>
<p>Given the data and a value of <span class="math inline">\(\theta\)</span>, <code>ELHMC</code> computes the optimal weights
using the <code>el.test</code> function from <code>emplik</code> library <span class="citation" data-cites="Zhou.:2014nr">(<a href="#ref-Zhou.:2014nr" role="doc-biblioref">Zhou 2014</a>)</span>. The <code>el.test</code> provides
<span class="math inline">\(\hat{\lambda}\left(\theta^{(k)}\right)\)</span> from which the gradient of the
log-empirical likelihood can be computed.</p>
<p>If <span class="math inline">\(\theta\not\in\Theta_1\)</span>, i.e.Â problem <a href="#eq:eqn5">(5)</a> is not feasible,
then <code>el.test</code> converges to weights all close to zero which do not sum to one.
Furthermore, the norm of <span class="math inline">\(\hat{\lambda}\left(\theta^{(k)}\right)\)</span> will
be large. In such cases, the empirical likelihood will be zero. This
means that, whenever the optimal weights are computed, we need to check
if they sum to one (within numerical errors) or not.</p>
<p>The function <code>ELHMC</code> returns a list. If argument <code>detailed</code> is set to <code>FALSE</code>, the list contains
samples of the parameters of interest <span class="math inline">\(\theta\)</span>, the Monte Carlo
acceptance rate as listed in table (tab:T2). If <code>detailed</code> is set to <code>TRUE</code>, additional information such as the
trajectories of <span class="math inline">\(\theta\)</span> and the momentum is included in the returned
list (see Table (tab:T3)).</p>
<p>At the moment <code>ELHMC</code> only allows a diagonal covariance matrix for the momentum
<span class="math inline">\(p\)</span>. The default value for the stepsize <code>epsilon</code> and step number <code>lf.steps</code> are <span class="math inline">\(0.05\)</span> and
<span class="math inline">\(10\)</span> respectively. For a specific problem they need to be determined by
trial and error, using the outputs from <code>plot.interval</code>, and <code>print.interval</code> commands.</p>
<div class="layout-chunk" data-layout="l-body">
<table>
<caption><span id="tab:returned">Table 2: </span>Elements of the list returned by <code>ELHMC</code> if <code>detailed=FALSE</code></caption>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>samples</code></td>
<td style="text-align: left;">A matrix containing the parameter samples</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>acceptance.rate</code></td>
<td style="text-align: left;">The acceptance rate</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>call</code></td>
<td style="text-align: left;">The matched call</td>
</tr>
</tbody>
</table>
</div>
<div class="layout-chunk" data-layout="l-body">
<table>
<caption><span id="tab:detailedreturned">Table 3: </span>Elements of the list returned by <code>ELHMC</code> if <code>detailed=TRUE</code></caption>
<colgroup>
<col style="width: 8%" />
<col style="width: 91%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>samples</code></td>
<td style="text-align: left;">A matrix containing the parameter samples</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>acceptance.rate</code></td>
<td style="text-align: left;">The acceptance rate</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>proposed</code></td>
<td style="text-align: left;">A matrix containing the proposed values at <code>n.samples - 1</code> Hamiltonian Monte Carlo updates</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>acceptance</code></td>
<td style="text-align: left;">A vector of <code>TRUE/FALSE</code> values indicates whether each proposed value is accepted</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>trajectory</code></td>
<td style="text-align: left;">A list with 2 elements <code>trajectory.q</code> and <code>trajectory.p</code>. These are lists of matrices containing position and momentum values along trajectory in each Hamiltonian Monte Carlo update.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>call</code></td>
<td style="text-align: left;">The matched call</td>
</tr>
</tbody>
</table>
</div>
<h3 data-number="4" id="sec:examples"><span class="header-section-number">4</span> Examples</h3>
<p>In this section, we present two examples of usage of the package. Both
examples in some sense supplement the conditions considered by
<span class="citation" data-cites="chaudhuriMondalTeng2017">Chaudhuri et al. (<a href="#ref-chaudhuriMondalTeng2017" role="doc-biblioref">2017</a>)</span>. In each case, it is seen that the function can
sample from the resulting empirical likelihood-based posterior quite
efficiently.</p>
<h4 class="unnumbered" data-number="4.1" id="sample-the-mean-of-a-simple-data-set">Sample the mean of a simple data set</h4>
<p>In the first example, suppose the data set consists of eight data points
<span class="math inline">\(v = \left(v_1,...,v_8\right)\)</span>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> v <span class="ot">&lt;-</span> <span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>), <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>), <span class="fu">c</span>(<span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>), <span class="fu">c</span>(<span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>),</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>            <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>), <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">0</span>), <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> <span class="fu">print</span>(v)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>     [,<span class="dv">1</span>] [,<span class="dv">2</span>]</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>,]    <span class="dv">1</span>    <span class="dv">1</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>[<span class="dv">2</span>,]    <span class="dv">1</span>    <span class="dv">0</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>[<span class="dv">3</span>,]    <span class="dv">1</span>   <span class="sc">-</span><span class="dv">1</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>[<span class="dv">4</span>,]    <span class="dv">0</span>   <span class="sc">-</span><span class="dv">1</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>[<span class="dv">5</span>,]   <span class="sc">-</span><span class="dv">1</span>   <span class="sc">-</span><span class="dv">1</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>[<span class="dv">6</span>,]   <span class="sc">-</span><span class="dv">1</span>    <span class="dv">0</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>[<span class="dv">7</span>,]   <span class="sc">-</span><span class="dv">1</span>    <span class="dv">1</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>[<span class="dv">8</span>,]    <span class="dv">0</span>    <span class="dv">1</span></span></code></pre></div>
<p>The parameters of interest are the mean
<span class="math inline">\(\theta = \left(\theta_1, \theta_2\right)\)</span>. Since
<span class="math inline">\(E\left[\theta - v_i\right] = 0\)</span>, the smooth function is
<span class="math inline">\(g = \theta - v_i\)</span> with
<span class="math inline">\(\nabla_\theta g = \left(\left(1, 0\right), \left(0, 1\right)\right)\)</span>:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>Function<span class="sc">:</span> fun</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> g <span class="ot">&lt;-</span> <span class="cf">function</span>(params, x) {</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   params <span class="sc">-</span> x</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span> }</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>Function<span class="sc">:</span> dfun</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> dlg <span class="ot">&lt;-</span> <span class="cf">function</span>(params, x) {</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">rbind</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>), <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span> }</span></code></pre></div>
<p>Functions <code>g</code> and <code>dlg</code> are supplied to arguments <code>fun</code> and <code>dfun</code> in <code>ELHMC</code>. These two functions
must have <code>params</code> as the first argument and <code>x</code> as the second. <code>params</code> represents a sample
of <span class="math inline">\(\theta\)</span> whereas <code>x</code> represents a data point <span class="math inline">\(v_i\)</span> or a row in the matrix
<code>v</code>. <code>fun</code> should return a vector and <code>dfun</code> a matrix whose <span class="math inline">\(\left(i, j\right)\)</span> entry is
<span class="math inline">\(\partial g_i/\partial\theta_j\)</span>.</p>
<p>We assume that both <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> have independent standard
normal distributions as priors. Next, we define the functions that
calculate the prior densities and gradients of log prior densities as
<code>pr</code> and <code>dpr</code> in the following ways:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>Function<span class="sc">:</span> prior</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> pr <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="sc">-</span>.<span class="dv">5</span><span class="sc">*</span>(x[<span class="dv">1</span>]<span class="sc">^</span><span class="dv">2</span><span class="sc">+</span>x[<span class="dv">2</span>]<span class="sc">^</span><span class="dv">2</span>)<span class="sc">-</span><span class="fu">log</span>(<span class="dv">2</span><span class="sc">*</span>pi)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span> }</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>Function<span class="sc">:</span> dprior</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> dpr <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="sc">-</span>x</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span> }</span></code></pre></div>
<p>Functions <code>pr</code> and <code>dpr</code> are assigned to <code>prior</code> and <code>dprior</code> in <code>ELHMC</code>. <code>prior</code> and <code>dprior</code> must take in only one
argument <code>x</code> and return a vector of the same length as <span class="math inline">\(\theta\)</span>.</p>
<p>We can now use <code>ELHMC</code> to draw samples of <span class="math inline">\(\theta\)</span>. Let us draw 1000 samples,
with starting point <span class="math inline">\(\left(0.9, 0.95\right)\)</span> using 12 leapfrog steps
with step size 0.06 for both <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> for each
Hamiltonian Monte Carlo update:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> <span class="fu">library</span>(elhmc)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> <span class="fu">set.seed</span>(<span class="dv">476</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> thetas <span class="ot">&lt;-</span> <span class="fu">ELHMC</span>(<span class="at">initial =</span> <span class="fu">c</span>(<span class="fl">0.9</span>, <span class="fl">0.95</span>), <span class="at">data =</span> v, <span class="at">fun =</span> g, <span class="at">dfun =</span> dlg,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>                 <span class="at">prior =</span> pr, <span class="at">dprior =</span> dpr, <span class="at">n.samples =</span> <span class="dv">1000</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>                 <span class="at">lf.steps =</span> <span class="dv">12</span>, <span class="at">epsilon =</span> <span class="fl">0.06</span>, <span class="at">detailed =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>We extract and visualise the distribution of the samples using a boxplot
(Figure <a href="#fig:theta">2</a>):</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> <span class="fu">boxplot</span>(thetas<span class="sc">$</span>samples, <span class="at">names =</span> <span class="fu">c</span>(<span class="fu">expression</span>(theta[<span class="dv">1</span>]), <span class="fu">expression</span>(theta[<span class="dv">2</span>])))</span></code></pre></div>
<p>Since we set <code>detailed = TRUE</code>, we have data on the trajectory of <span class="math inline">\(\theta\)</span> as well as
momentum <span class="math inline">\(p\)</span>. They are stored in element <code>trajectory</code> of <code>thetas</code> and can be accessed by
<code>thetas$trajectory</code>. <code>thetas$trajectory</code> is a list with two elements
named <code>trajectory.q</code> and <code>trajectory.p</code> denoting trajectories for <span class="math inline">\(\theta\)</span> and momentum <span class="math inline">\(p\)</span>. <code>trajectory.q</code> and <code>trajectory.p</code> are
both lists with elements <code>1</code>, ..., <code>n.samples - 1</code>. Each of these elements is a matrix
containing trajectories of <span class="math inline">\(\theta\)</span> (<code>trajectory.q</code>) and <span class="math inline">\(p\)</span> (<code>trajectory.p</code>) at each Hamiltonian
Monte Carlo update.</p>
<p>We illustrate by extracting the trajectories of <span class="math inline">\(\theta\)</span> at the first
update and plotting them (Figure
<a href="#fig:trajectoryeg1">3</a>):</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> q <span class="ot">&lt;-</span> thetas<span class="sc">$</span>trajectory<span class="sc">$</span>trajectory.q[[<span class="dv">1</span>]]</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> <span class="fu">plot</span>(q, <span class="at">xlab =</span> <span class="fu">expression</span>(theta[<span class="dv">1</span>]), <span class="at">ylab =</span> <span class="fu">expression</span>(theta[<span class="dv">2</span>]),</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>      <span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="at">cex =</span> <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">16</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> <span class="fu">points</span>(v[,<span class="dv">1</span>],v[,<span class="dv">2</span>],<span class="at">type=</span><span class="st">&quot;p&quot;</span>,<span class="at">cex=</span><span class="fl">1.5</span>,<span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> <span class="fu">abline</span>(<span class="at">h=</span><span class="sc">-</span><span class="dv">1</span>); <span class="fu">abline</span>(<span class="at">h=</span><span class="dv">1</span>); <span class="fu">abline</span>(<span class="at">v=</span><span class="sc">-</span><span class="dv">1</span>); <span class="fu">abline</span>(<span class="at">v=</span><span class="dv">1</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> <span class="fu">arrows</span>(q[<span class="sc">-</span><span class="fu">nrow</span>(q), <span class="dv">1</span>], q[<span class="sc">-</span><span class="fu">nrow</span>(q), <span class="dv">2</span>], q[<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>], q[<span class="sc">-</span><span class="dv">1</span>, <span class="dv">2</span>],</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>        <span class="at">length =</span> <span class="fl">0.1</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>)</span></code></pre></div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:theta"></span>
<img src="figures/elhmc-008.png" alt="Posterior distribution of $\theta_1$ and $\theta_2$ samples." width="48%" />
<p class="caption">
Figure 2: Posterior distribution of <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> samples.
</p>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:trajectoryeg1"></span>
<img src="figures/elhmc-010.png" alt="Trajectory of $\theta$ during the first Monte Carlo update." width="48%" />
<p class="caption">
Figure 3: Trajectory of <span class="math inline">\(\theta\)</span> during the first Monte Carlo update.
</p>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:trajectory"></span>
<img src="figures/elhmc-008.png" alt="Samples of $\theta$ drawn from the posterior." width="48%" /><img src="figures/elhmc-010.png" alt="Samples of $\theta$ drawn from the posterior." width="48%" />
<p class="caption">
Figure 4: Samples of <span class="math inline">\(\theta\)</span> drawn from the posterior.
</p>
</div>
</div>
<p>The specialty in this example is in the choice of the data points in
<span class="math inline">\(v\)</span>. <span class="citation" data-cites="chaudhuriMondalTeng2017">Chaudhuri et al. (<a href="#ref-chaudhuriMondalTeng2017" role="doc-biblioref">2017</a>)</span> show that the chain will reflect if the
one-dimensional boundaries of the convex hull (in this case the unit
square) have two observations, which happens with probability one for
continuous distributions. In this example, however, there is more than
one point in two one-dimensional boundaries. However, we can see that
the HMC method works very well here.</p>
<h4 class="unnumbered" data-number="4.2" id="ex:2">Logistic regression with an additional constraint</h4>
<p>In this example, we consider a constrained logistic regression of one
binary variable on another, where the expectation of the response is
known. The frequentist estimation problem using empirical likelihood was
considered by <span class="citation" data-cites="chaudhuri_handcock_rendall_2008">Chaudhuri et al. (<a href="#ref-chaudhuri_handcock_rendall_2008" role="doc-biblioref">2008</a>)</span>. It has been shown that
empirical likelihood-based formulation has a major applicational
advantage over the fully parametric formulation. Below we consider a
Bayesian extension of the proposed empirical likelihood-based
formulation and use <code>ELHMC</code> to sample from the resulting posterior.</p>
<div class="layout-chunk" data-layout="l-body">
<table>
<caption><span id="tab:bhps">Table 4: </span>The dataset used in this example.</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\(x=0\)</span></th>
<th style="text-align: right;"><span class="math inline">\(x=1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(y=0\)</span></td>
<td style="text-align: right;">5903</td>
<td style="text-align: right;">5157</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(y=1\)</span></td>
<td style="text-align: right;">230</td>
<td style="text-align: right;">350</td>
</tr>
</tbody>
</table>
</div>
<p>The data set <span class="math inline">\(v\)</span> consists of <span class="math inline">\(n\)</span> observations of two variables and two
columns <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. In the ith row <span class="math inline">\(y_i\)</span> represents the indicator of
whether a woman gave birth between time <span class="math inline">\(t - 1\)</span> and <span class="math inline">\(t\)</span> while <span class="math inline">\(x_i\)</span> is
the indicator of whether she had at least one child at time <span class="math inline">\(t - 1\)</span>. The
data can be found in Table
<a href="#tab:bhps">4</a>
above. In addition, it was known that the prevalent general fertility
rate in the population was <span class="math inline">\(0.06179\)</span>. <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>We are interested in fitting a logistic regression model to the data
with <span class="math inline">\(X\)</span> as the independent variable and <span class="math inline">\(Y\)</span> as the dependent variable.
However, we also would like to constrain the sample general fertility
rate to its value in the population. The logistic regression model takes
the form of:</p>
<p><span class="math display">\[P \left(Y = 1 | X = x\right) = \frac{\exp\left(\beta_0 + \beta_1 x\right)}{1 + \exp\left(\beta_0 + \beta_1 x\right)}.\]</span></p>
<p>From the model, using conditions similar to zero-mean residual and
exogeneity, it is clear that:</p>
<p><span class="math display">\[\begin{equation*}
E\left[y_i - \frac{\exp\left(\beta_0 + \beta_1 x_i\right)}{1 + \exp\left(\beta_0 + \beta_1 x_i\right)}\right] = 0,\quad
E\left[x_i\left\{y_i - \frac{\exp\left(\beta_0 + \beta_1 x_i\right)}{1 + \exp\left(\beta_0 + \beta_1 x_i\right)}\right\}\right] = 0.
\end{equation*}\]</span></p>
<p>Furthermore from the definition of general fertility rate, we get:</p>
<p><span class="math display">\[E\left[y_i - 0.06179\right] = 0.\]</span></p>
<p>Following <span class="citation" data-cites="chaudhuri_handcock_rendall_2008">Chaudhuri et al. (<a href="#ref-chaudhuri_handcock_rendall_2008" role="doc-biblioref">2008</a>)</span>, we define the estimating
equations <span class="math inline">\(g\)</span> as follows:</p>
<p><span class="math display">\[g \left(\beta, v_i\right) = \begin{bmatrix}
y_i - \frac{\exp\left(\beta_0 + \beta_1 x_i\right)}{1 + \exp\left(\beta_0 + \beta_1 x_i\right)} \\
x_i\left[y_i - \frac{\exp\left(\beta_0 + \beta_1 x_i\right)}{1 + \exp\left(\beta_0 + \beta_1 x_i\right)}\right] \\
y_i - 0.06179 \\
\end{bmatrix}\]</span></p>
<p>The gradient of <span class="math inline">\(g\)</span> with respect to <span class="math inline">\(\beta\)</span> is given by:</p>
<p><span class="math display">\[\nabla_{\beta}g = \begin{bmatrix}
\frac{-\exp\left(\beta_0 + \beta_1 x_i\right)}{\left(\exp\left(\beta_0 + \beta_1 x_i\right) + 1\right)^2} &amp; \frac{-\exp\left(\beta_0 + \beta_1 x_i\right) x_i}{\left(\exp\left(\beta_0 + \beta_1 x_i\right) + 1\right)^2} \\
\frac{-\exp\left(\beta_0 + \beta_1 x_i\right) x_i}{\left(\exp\left(\beta_0 + \beta_1 x_i\right) + 1\right)^2} &amp; \frac{-\exp\left(\beta_0 + \beta_1 x_i\right) x_i^2}{\left(\exp\left(\beta_0 + \beta_1 x_i\right) + 1\right)^2} \\
0 &amp; 0 \\
\end{bmatrix}\]</span></p>
<p>In R, we create functions <code>g</code> and <code>dg</code> to represent <span class="math inline">\(g\)</span> and <span class="math inline">\(\nabla_{\beta}g\)</span>:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>Function<span class="sc">:</span> fun</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> g <span class="ot">&lt;-</span> <span class="cf">function</span>(params, X) {</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>  result <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="fu">nrow</span>(X), <span class="at">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>  a <span class="ot">&lt;-</span> <span class="fu">exp</span>(params[<span class="dv">1</span>] <span class="sc">+</span> params[<span class="dv">2</span>] <span class="sc">*</span> X[, <span class="dv">1</span>])</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>  a <span class="ot">&lt;-</span> a <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> a)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>  result[, <span class="dv">1</span>] <span class="ot">&lt;-</span> X[, <span class="dv">2</span>] <span class="sc">-</span> a</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>  result[, <span class="dv">2</span>] <span class="ot">&lt;-</span> (X[, <span class="dv">2</span>] <span class="sc">-</span> a) <span class="sc">*</span> X[, <span class="dv">1</span>]</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>  result[, <span class="dv">3</span>] <span class="ot">&lt;-</span> X[, <span class="dv">2</span>] <span class="sc">-</span> <span class="fl">0.06179</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>  result</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>Function<span class="sc">:</span> dfun</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> dg <span class="ot">&lt;-</span> <span class="cf">function</span>(params, X) {</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>  result <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">2</span>, <span class="fu">nrow</span>(X)))</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>  a <span class="ot">&lt;-</span> <span class="fu">exp</span>(params[<span class="dv">1</span>] <span class="sc">+</span> params[<span class="dv">2</span>] <span class="sc">*</span> X[, <span class="dv">1</span>])</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>  a <span class="ot">&lt;-</span> <span class="sc">-</span>a <span class="sc">/</span> (a <span class="sc">+</span> <span class="dv">1</span>) <span class="sc">^</span> <span class="dv">2</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>  result[<span class="dv">1</span>, <span class="dv">1</span>, ] <span class="ot">&lt;-</span> a</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>  result[<span class="dv">1</span>, <span class="dv">2</span>, ] <span class="ot">&lt;-</span> result[<span class="dv">1</span>, <span class="dv">1</span>, ] <span class="sc">*</span> X[, <span class="dv">1</span>]</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>  result[<span class="dv">2</span>, <span class="dv">1</span>, ] <span class="ot">&lt;-</span> result[<span class="dv">1</span>, <span class="dv">2</span>, ]</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>  result[<span class="dv">2</span>, <span class="dv">2</span>, ] <span class="ot">&lt;-</span> result[<span class="dv">1</span>, <span class="dv">2</span>, ] <span class="sc">*</span> X[, <span class="dv">1</span>]</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>  result[<span class="dv">3</span>, , ] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>  result</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>We choose independent <span class="math inline">\(N\left(0, 100\right)\)</span> priors for both <span class="math inline">\(\beta_0\)</span>
and <span class="math inline">\(\beta_1\)</span>:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>Function<span class="sc">:</span> prior</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> pr <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>    <span class="sc">-</span> <span class="fl">0.5</span><span class="sc">*</span><span class="fu">t</span>(x)<span class="sc">%*%</span>x<span class="sc">/</span><span class="dv">10</span><span class="sc">^</span><span class="dv">4</span> <span class="sc">-</span> <span class="fu">log</span>(<span class="dv">2</span><span class="sc">*</span>pi<span class="sc">*</span><span class="dv">10</span><span class="sc">^</span><span class="dv">4</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span> },</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>Function<span class="sc">:</span> dprior</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> dpr <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>   <span class="sc">-</span>x <span class="sc">*</span> <span class="dv">10</span> <span class="sc">^</span> (<span class="sc">-</span><span class="dv">4</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span> },</span></code></pre></div>
<p>where <code>pr</code> is the prior and <code>dpr</code> is the gradient of the log prior for <span class="math inline">\(\beta\)</span>.</p>
<p>Our goal is to use <code>ELHMC</code> to draw samples of
<span class="math inline">\(\beta = \left( \beta_0, \beta_1 \right)\)</span> from their resulting posterior
based on empirical likelihood.</p>
<p>We start our sampling from <span class="math inline">\((-3.2,0.55)\)</span> and use two stages of sampling.
In the first stage, <span class="math inline">\(50\)</span> points are sampled with <span class="math inline">\(\epsilon=0.001\)</span>,
<span class="math inline">\(T=15\)</span>, and the momentum generated from a <span class="math inline">\(N(0,0.02\cdot I_2)\)</span>
distribution. The acceptance rate at this stage is very high, but it is
designed to find a good starting point for the second stage, where the
acceptance rate can be easily controlled.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> bstart.init<span class="ot">=</span><span class="fu">c</span>(<span class="sc">-</span><span class="fl">3.2</span>,.<span class="dv">55</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> betas.init <span class="ot">&lt;-</span> <span class="fu">ELHMC</span>(<span class="at">initial =</span> bstart.init, <span class="at">data =</span> data, <span class="at">FUN =</span> g, <span class="at">DFUN =</span> dg,</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>               <span class="at">n.samples =</span> <span class="dv">50</span>, <span class="at">prior =</span> pr, <span class="at">dprior =</span> dpr, <span class="at">epsilon =</span> <span class="fl">0.001</span>,</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>               <span class="at">lf.steps =</span> <span class="dv">15</span>, <span class="at">detailed =</span> T, <span class="at">p.variance =</span> <span class="fl">0.2</span>)</span></code></pre></div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:density"></span>
<img src="figures/bhpsContour.png" alt="Contour plot of the non-normalised log posterior with the HMC sampling path (left) and density plot (right) of the samples for the constrained logistic regression problem." width="48%" /><img src="figures/elhmcHMC.png" alt="Contour plot of the non-normalised log posterior with the HMC sampling path (left) and density plot (right) of the samples for the constrained logistic regression problem." width="48%" />
<p class="caption">
Figure 5: Contour plot of the non-normalised log posterior with the HMC sampling path (left) and density plot (right) of the samples for the constrained logistic regression problem.
</p>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:acf"></span>
<img src="figures/bhpsAcfb0.png" alt="The autocorrelation function of the samples drawn from the posterior of $\beta$." width="48%" /><img src="figures/bhpsAcfb1.png" alt="The autocorrelation function of the samples drawn from the posterior of $\beta$." width="48%" />
<p class="caption">
Figure 6: The autocorrelation function of the samples drawn from the posterior of <span class="math inline">\(\beta\)</span>.
</p>
</div>
</div>
<p>In this second stage, we draw 500 samples of <span class="math inline">\(\beta\)</span> with starting
values as the last value from the first stage. The number of leapfrog
steps per Monte Carlo update is set to 30, with a step size of 0.004 for
both <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. We use
<span class="math inline">\(N \left(0, 0.02\textbf(I_2)\right)\)</span> as prior for the momentum.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> bstart<span class="ot">=</span>betas.init<span class="sc">$</span>samples[<span class="dv">50</span>,]</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> betas <span class="ot">&lt;-</span> <span class="fu">ELHMC</span>(<span class="at">initial =</span> bstart, <span class="at">data =</span> data, <span class="at">fun =</span> g, <span class="at">dfun =</span> dg,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>                <span class="at">n.samples =</span> <span class="dv">500</span>, <span class="at">prior =</span> pr, <span class="at">dprior =</span> dpr, <span class="at">epsilon =</span> <span class="fl">0.004</span>,</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>                <span class="at">lf.steps =</span> <span class="dv">30</span>, <span class="at">detailed =</span> <span class="cn">FALSE</span>, <span class="at">p.variance =</span> <span class="fl">0.2</span>,<span class="at">print.interval=</span><span class="dv">10</span>,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>        <span class="at">plot.interval=</span><span class="dv">1</span>,<span class="at">which.plot=</span><span class="fu">c</span>(<span class="dv">1</span>))</span></code></pre></div>
<p>Based on our output, we can make inferences about <span class="math inline">\(\beta\)</span>. As an
example, the autocorrelation plots and the density plot of the last
<span class="math inline">\(1000\)</span> samples of <span class="math inline">\(\beta\)</span> are shown in Figure
<a href="#fig:density">5</a>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> <span class="fu">library</span>(MASS)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> beta.density <span class="ot">&lt;-</span> <span class="fu">kde2d</span>(betas<span class="sc">$</span>sample[, <span class="dv">1</span>], betas<span class="sc">$</span>samples[, <span class="dv">2</span>])</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> <span class="fu">persp</span>(beta.density, <span class="at">phi =</span> <span class="dv">50</span>, <span class="at">theta =</span> <span class="dv">20</span>,</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>       <span class="at">xlab =</span> <span class="st">&#39;Intercept&#39;</span>, <span class="at">ylab =</span> <span class="st">&#39;&#39;</span>, <span class="at">zlab =</span> <span class="st">&#39;Density&#39;</span>,</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>       <span class="at">ticktype =</span> <span class="st">&#39;detailed&#39;</span>, <span class="at">cex.axis =</span> <span class="fl">0.35</span>, <span class="at">cex.lab =</span> <span class="fl">0.35</span>, <span class="at">d =</span> <span class="fl">0.7</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> <span class="fu">acf</span>(betas<span class="sc">$</span>sample[<span class="fu">round</span>(n.samp<span class="sc">/</span><span class="dv">2</span>)<span class="sc">:</span>n.samp, <span class="dv">1</span>],</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>       <span class="at">main=</span><span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Series &quot;</span>,beta[<span class="dv">0</span>])))</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>R<span class="sc">&gt;</span> <span class="fu">acf</span>(betas<span class="sc">$</span>sample[<span class="fu">round</span>(n.samp<span class="sc">/</span><span class="dv">2</span>)<span class="sc">:</span>n.samp, <span class="dv">2</span>],</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="sc">+</span>       <span class="at">main=</span><span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">&quot;Series &quot;</span>,beta[<span class="dv">1</span>])))</span></code></pre></div>
<p>It is well known <span class="citation" data-cites="chaudhuri_handcock_rendall_2008">(<a href="#ref-chaudhuri_handcock_rendall_2008" role="doc-biblioref">Chaudhuri et al. 2008</a>)</span> that the constrained
estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> have very low standard error. The
acceptance rate is close to <span class="math inline">\(78\%\)</span>. It is evident that our software can
sample from such a narrow ridge with ease. Furthermore, the
autocorrelation of the samples seems to decrease very quickly with the
lag, which would not be the case for most other MCMC procedures.</p>
<h3 class="unnumbered" id="acknowledgement">Acknowledgement</h3>
<p>Dang Trung Kien would like to acknowledge the support of MOE AcRF
R-155-000-140-112 from the National University of Singapore. Sanjay
Chaudhuri acknowledges the partial support from NSF-DMS grant 2413491
from the National Science Foundation USA. The authors are grateful to
Professor Michael Rendall, Department of Sociology, University of
Maryland, College Park for kindly sharing the data set on which the
second example is based.</p>
</div>
<h3 class="appendix" data-number="5" id="supplementary-materials"><span class="header-section-number">5</span> Supplementary materials</h3>
<p>Supplementary materials are available in addition to this article. It can be downloaded at
<a href="RJ-2025-041.zip">RJ-2025-041.zip</a></p>
<h3 class="appendix" data-number="6" id="note"><span class="header-section-number">6</span> Note</h3>
<p>This article is converted from a Legacy LaTeX article using the
<a href="https://cran.r-project.org/package=texor">texor</a> package.
The pdf version is the official version. To report a problem with the html,
refer to CONTRIBUTE on the R Journal homepage.</p>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bergsma2012empty" class="csl-entry" role="listitem">
W. Bergsma, M. Croon, L. A. van der Ark, et al. The empty set and zero likelihood problems in maximum empirical likelihood estimation. <em>Electronic Journal of Statistics</em>, 6: 2356â2361, 2012.
</div>
<div id="ref-besag2004markov" class="csl-entry" role="listitem">
J. Besag. Markov chain monte carlo methods for statistical inference. 2004.
</div>
<div id="ref-birdsall2004plasma" class="csl-entry" role="listitem">
C. K. Birdsall and A. B. Langdon. <em>Plasma physics via computer simulation.</em> CRC Press, 2004.
</div>
<div id="ref-boyd2004convex" class="csl-entry" role="listitem">
S. P. Boyd and L. Vandenberghe. <em>Convex optimization.</em> Cambridge university press, 2004.
</div>
<div id="ref-stan2017" class="csl-entry" role="listitem">
B. Carpenter, A. Gelman, M. Hoffman, D. Lee, B. Goodrich, M. Betancourt, M. Brubaker, J. Guo, P. Li and A. Riddell. Stan: A probabilistic programming language. <em>Journal of Statistical Software, Articles</em>, 76(1): 1â32, 2017. URL <a href="https://www.jstatsoft.org/v076/i01">https://www.jstatsoft.org/v076/i01</a>.
</div>
<div id="ref-chaudhuri_handcock_rendall_2008" class="csl-entry" role="listitem">
S. Chaudhuri, M. S. Handcock and M. S. Rendall. Generalized linear models incorporating population level information: An empirical-likelihood-based approach. <em>Journal of the Royal Statistical Society series B</em>, 70.2: 311â328, 2008.
</div>
<div id="ref-chaudhuriMondalTeng2017" class="csl-entry" role="listitem">
S. Chaudhuri, D. Mondal and T. Yin. Hamiltonian monte carlo sampling in <span>Bayesian</span> empirical likelihood computation. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 79(1): 293â320, 2017. URL <a href="http://dx.doi.org/10.1111/rssb.12164">http://dx.doi.org/10.1111/rssb.12164</a>.
</div>
<div id="ref-chen2008adjusted" class="csl-entry" role="listitem">
J. Chen, A. M. Variyath and B. Abraham. Adjusted empirical likelihood and its properties. <em>Journal of Computational and Graphical Statistics</em>, 17(2): 426â443, 2008.
</div>
<div id="ref-emerson2009calibration" class="csl-entry" role="listitem">
S. C. Emerson, A. B. Owen, et al. Calibration of the empirical likelihood method for a vector mean. <em>Electronic Journal of Statistics</em>, 3: 1161â1192, 2009.
</div>
<div id="ref-geman1984stochastic" class="csl-entry" role="listitem">
S. Geman and D. Geman. Stochastic relaxation, <span>Gibbs</span> distributions, and the <span>Bayesian</span> restoration of images. <em>Pattern Analysis and Machine Intelligence, <span>IEEE</span> Transactions on</em>, (6): 721â741, 1984.
</div>
<div id="ref-grendar2009empty" class="csl-entry" role="listitem">
M. GrendÃ¡r and G. Judge. Empty set problem of maximum empirical likelihood methods. <em>Electronic Journal of Statistics</em>, 3: 1542â1555, 2009.
</div>
<div id="ref-haario1999adaptive" class="csl-entry" role="listitem">
H. Haario, E. Saksman and J. Tamminen. Adaptive proposal distribution for random walk <span>Metropolis</span> algorithm. <em>Computational Statistics</em>, 14(3): 375â396, 1999.
</div>
<div id="ref-lazar2003bayesian" class="csl-entry" role="listitem">
N. A. Lazar. <span>Bayesian</span> empirical likelihood. <em>Biometrika</em>, 90(2): 319â326, 2003.
</div>
<div id="ref-leimkuhler2004simulating" class="csl-entry" role="listitem">
B. Leimkuhler and S. Reich. <em>Simulating hamiltonian dynamics.</em> Cambridge University Press, 2004.
</div>
<div id="ref-liu2010adjusted" class="csl-entry" role="listitem">
Y. Liu, J. Chen, et al. Adjusted empirical likelihood with high-order precision. <em>The Annals of Statistics</em>, 38(3): 1341â1362, 2010.
</div>
<div id="ref-neal2011mcmc" class="csl-entry" role="listitem">
R. Neal. <span>MCMC</span> for using hamiltonian dynamics. <em>Handbook of Markov Chain Monte Carlo</em>, 113â162, 2011.
</div>
<div id="ref-qin1994empirical" class="csl-entry" role="listitem">
J. Qin and J. Lawless. Empirical likelihood and general estimating equations. <em>The Annals of Statistics</em>, 300â325, 1994.
</div>
<div id="ref-tsao2013extending" class="csl-entry" role="listitem">
M. Tsao. Extending the empirical likelihood by domain expansion. <em>Canadian Journal of Statistics</em>, 41(2): 257â274, 2013.
</div>
<div id="ref-tsao2013empirical" class="csl-entry" role="listitem">
M. Tsao and F. Wu. Empirical likelihood on the full parameter space. <em>The Annals of Statistics</em>, 41(4): 2176â2196, 2013.
</div>
<div id="ref-tsaoFu2014" class="csl-entry" role="listitem">
M. Tsao and F. Wu. Extended empirical likelihood for estimating equations. <em>Biometrika</em>, 101(3): 703â710, 2014. URL <a href="http://dx.doi.org/10.1093/biomet/asu014">http://dx.doi.org/10.1093/biomet/asu014</a>.
</div>
<div id="ref-VBel" class="csl-entry" role="listitem">
W. Yu and J. Lim. <em><span>VBel</span>: Variational <span>Bayes</span> for fast and accurate empirical likelihood inference.</em> 2024. URL <a href="https://CRAN.R-project.org/package=VBel">https://CRAN.R-project.org/package=VBel</a>. <span>R</span> package version 1.1.0.
</div>
<div id="ref-Zhou.:2014nr" class="csl-entry" role="listitem">
M. Zhou. <em>Emplik: Empirical likelihood ratio for censored/truncated data.</em> 2014. URL <a href="http://CRAN.R-project.org/package=emplik">http://CRAN.R-project.org/package=emplik</a>. <span>R</span> package version 0.9-9-2.
</div>
</div>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>By convention,
<span class="math inline">\(x_i=(x_{i1},x_{i2},\ldots,x_{ip})^T\le x=(x_1,x_2,\ldots,x_p)^T\)</span>
iff <span class="math inline">\(x_{ij}\le x_{j}\)</span> <span class="math inline">\(\forall j\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
<li id="fn2"><p>In Figure (fig:scheme), <span class="math inline">\(\omega_1=\omega_2=\omega_3=0\)</span>,
<span class="math inline">\(\omega_4&gt;0\)</span>, and <span class="math inline">\(\omega_5&gt;0\)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
<li id="fn3"><p>In Figure (fig:scheme), <span class="math inline">\(q=2\)</span>, and the faces of maximum dimension
are the sides of the pentagon. They have <span class="math inline">\(q=2\)</span> end i.e.Â extreme
points.<a href="#fnref3" class="footnote-back" role="doc-backlink">â©ï¸</a></p></li>
<li id="fn4"><p>The authors are grateful to Prof.Â Michael Rendall, Department of
Sociology, University of Maryland, College Park, for kindly sharing
the data on which this example is based.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<a href="#fnref4" class="footnote-back" role="doc-backlink">â©ï¸</a></li>
</ol>
</section>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
<h3 id="references">References</h3>
<div id="references-listing"></div>
<h3 id="reuse">Reuse</h3>
<p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
<h3 id="citation">Citation</h3>
<p>For attribution, please cite this work as</p>
<pre class="citation-appendix short">Wei, et al., "elhmc: An R Package for Hamiltonian Monte Carlo Sampling in Bayesian Empirical Likelihood", The R Journal, 2026</pre>
<p>BibTeX citation</p>
<pre class="citation-appendix long">@article{RJ-2025-041,
  author = {Wei, Neo Han and Kien, Dang Trung and Chaudhuri, Sanjay},
  title = {elhmc: An R Package for Hamiltonian Monte Carlo Sampling in Bayesian Empirical Likelihood},
  journal = {The R Journal},
  year = {2026},
  note = {https://doi.org/10.32614/RJ-2025-041},
  doi = {10.32614/RJ-2025-041},
  volume = {17},
  issue = {4},
  issn = {2073-4859},
  pages = {237-254}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
