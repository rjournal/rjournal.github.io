---
title: 'SIMEXBoost: An R package for Analysis of High-Dimensional Error-Prone Data
  Based on Boosting Method'
abstract: ' Boosting is a powerful statistical learning method. Its key feature is
  the ability to derive a strong learner from simple yet weak learners by iteratively
  updating the learning results. Moreover, boosting algorithms have been employed
  to do variable selection and estimation for regression models. However, measurement
  error usually appears in covariates. Ignoring measurement error can lead to biased
  estimates and wrong inferences. To the best of our knowledge, few packages have
  been developed to address measurement error and variable selection simultaneously
  by using boosting algorithms. In this paper, we introduce an R package [SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost),
  which covers some widely used regression models and applies the simulation and extrapolation
  method to deal with measurement error effects. Moreover, the package [SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost)
  enables us to do variable selection and estimation for high-dimensional data under
  various regression models. To assess the performance and illustrate the features
  of the package, we conduct numerical studies. '
author:
- name: Li-Pang Chen
  affiliation: Department of Statistics, National Chengchi University
  orcid: 0000-0001-5440-5036
  email: |
    lchen723@nccu.edu.tw
  address:
  - No. 64, Section 2, Zhinan Rd, Wenshan District, Taipei City, 116
  - Taiwan (R.O.C.)
- name: Bangxu Qiu
  affiliation: Department of Statistics, National Chengchi University
  email: |
    1135427976@qq.com
  address:
  - No. 64, Section 2, Zhinan Rd, Wenshan District, Taipei City, 116
  - Taiwan (R.O.C.)
date: '2024-04-11'
date_received: '2022-04-13'
journal:
  firstpage: ~
  lastpage: ~
volume: 15
issue: 4
slug: RJ-2023-080
packages:
  cran: ~
  bioc: ~
draft: no
preview: preview.png
bibliography: SIMEXBoost-Chen.bib
CTV: ~
output:
  rjtools::rjournal_web_article:
    self_contained: no
    toc: no
    legacy_pdf: yes

---

# Introduction {#Introduction}

In statistical analysis, regression models are important methods for
characterizing the relationship between response and the covariates.
When the response follows exponential family distributions, generalized
linear models (GLM) are commonly used to link the response and the
covariates. If the response is taken as failure time and is incomplete
due to censoring (e.g., [@Lawless:2003]), the accelerated failure time
model (AFT) might be one of useful strategies to characterize the
survival outcome in survival analysis. In recent years, complex modeling
structures have been explored when building GLM or survival models,
including semi-parametric or mixed-effects structures. To address these
challenges, several statistical learning methods, including the boosting
approaches (e.g., [@Hastie:2008]), have been developed.

In the contemporary statistical analysis, researchers may frequently
encounter high-dimensionality in variables. In particular,
high-dimensional data may contain many irrelevant covariates that may
affect analysis results. Therefore, it is crucial to do variable
selection. In the development of statistical methods, some useful
strategies have been proposed, such as regularization approaches (e.g.,
[@Tibshirani:1996]; [@Zou:2006]; [@Zou:2005]) or feature screening
methods (e.g., [@Chen:2021]; [@Chen:2020]). In addition, [@Wolfson:2011]
and [@Brown:2017] proposed the boosting method to do variable selection,
which avoids having to deal with non-differentiable penalty functions.
The other important feature is measurement error in variables, which is
ubiquitous in applications. Moreover, ignoring measurement error effects
may affect the estimation results (e.g., [@ChenYi:2021]). Therefore, it
is necessary to correct for measurement error effects. A large body of
methods has been well established to address variable selection,
correction of measurement error, or both. Recently, [@Chen:2023]
developed the SIMEX method and the regression calibration method with
the boosting algorithm accommodated to handle variable selection and
measurement error correction for GLMs. [@ChenQiu:2023] considered the
AFT model to fit time-to-event responses and proposed the SIMEX method
to address measurement error. [@Chen:2023b] derived the corrected
estimating function based on the logistic regression or probit models
and applied the boosting method to do variable selection for binary
outcomes.

In applications, several commonly used packages associated with existing
methods have been developed for public use. A detailed list is
summarized in TableÂ [1](#tab:compare){reference-type="ref"
reference="tab:compare"}. Specifically, with variable selection and
measurement error ignored, most packages related to boosting methods,
including [bst](https://CRAN.R-project.org/package=bst) and
[adabag](https://CRAN.R-project.org/package=adabag), can handle
classification with binary or multi-class responses. Some packages can
deal with different response families. For example,
[xgboost](https://CRAN.R-project.org/package=xgboost) and
[lightgbm](https://CRAN.R-project.org/package=lightgbm) can deal with
continuous responses; [gbm](https://CRAN.R-project.org/package=gbm) and
[gamboostLSS](https://CRAN.R-project.org/package=gamboostLSS) can be
used to model survival data under the Cox model and the AFT model,
respectively; [GMMBoost](https://CRAN.R-project.org/package=GMMBoost) is
useful for handling mixed-effects models. In the presence of
high-dimensional but precisely measured variables,
[glmnet](https://CRAN.R-project.org/package=glmnet) and
[SIS](https://CRAN.R-project.org/package=SIS) are two popular packages
for variable selection or feature screening, respectively. On the
contrary, if variables are subject to measurement error, the two
packages [GLSME](https://CRAN.R-project.org/package=GLSME) and
[mecor](https://CRAN.R-project.org/package=mecor) focus on linear models
and aim to adjust for measurement error effects in the response and/or
covariates. Moreover, the simulation and extrapolation (SIMEX) method
(e.g., [@ChenYi:2021]; [@Carroll:2006]) has been a powerful strategy to
correct for measurement error effects, and has been widely used under
several types of regression models in existing R packages, including
[simex](https://CRAN.R-project.org/package=simex) for GLM,
[augSIMEX](https://CRAN.R-project.org/package=augSIMEX) for GLM with
error-prone continuous and discrete variables, and
[simexaft](https://CRAN.R-project.org/package=simexaft) for the AFT
model. In addition to the R software, [@BOOME] developed a Python
package **BOOME** to handle variable selection and measurement error for
binary outcomes.

::: {#tab:compare}
                                                                      Usage                                                                                                                                                                              
  --------------------------------------------------------------- -------------- -------------- -------------- -------------- -------------- -------------- -------------- -------------- -------------- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --
  2-10 Packages                                                         LM         Class$^1$         Pois           Cox            AFT           SL$^2$           ME             VS            Col                                                       
  [SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost)      $\checkmark$   $\checkmark$   $\checkmark$     $\times$     $\checkmark$   $\checkmark$   $\checkmark$   $\checkmark$   $\checkmark$                                                  
  [@SIMEXBoostR]                                                                                                                                                                                                                                         
  [glmnet](https://CRAN.R-project.org/package=glmnet)              $\checkmark$   $\checkmark$   $\checkmark$   $\checkmark$     $\times$       $\times$       $\times$     $\checkmark$   $\checkmark$                                                  
  [@glmnetR]                                                                                                                                                                                                                                             
  [SIS](https://CRAN.R-project.org/package=SIS)                    $\checkmark$   $\checkmark$   $\checkmark$   $\checkmark$     $\times$       $\times$       $\times$     $\checkmark$     $\times$                                                    
  [@SISR]                                                                                                                                                                                                                                                
  [GLSME](https://CRAN.R-project.org/package=GLSME)                $\checkmark$     $\times$       $\times$       $\times$       $\times$       $\times$     $\checkmark$     $\times$       $\times$                                                    
  [@GLSMER]                                                                                                                                                                                                                                              
  [mecor](https://CRAN.R-project.org/package=mecor)                $\checkmark$     $\times$       $\times$       $\times$       $\times$       $\times$     $\checkmark$     $\times$       $\times$                                                    
  [@mecorR]                                                                                                                                                                                                                                              
  [augSIMEX](https://CRAN.R-project.org/package=augSIMEX)          $\checkmark$   $\checkmark$   $\checkmark$     $\times$       $\times$       $\times$     $\checkmark$     $\times$       $\times$                                                    
  [@augSIMEXR]                                                                                                                                                                                                                                           
  [simex](https://CRAN.R-project.org/package=simex)                $\checkmark$   $\checkmark$   $\checkmark$   $\checkmark$     $\times$       $\times$     $\checkmark$     $\times$       $\times$                                                    
  [@simexR]                                                                                                                                                                                                                                              
  [simexaft](https://CRAN.R-project.org/package=simexaft)            $\times$       $\times$       $\times$       $\times$     $\checkmark$     $\times$     $\checkmark$     $\times$       $\times$                                                    
  [@simexaftR]                                                                                                                                                                                                                                           
  [bst](https://CRAN.R-project.org/package=bst)                      $\times$     $\checkmark$     $\times$       $\times$       $\times$     $\checkmark$     $\times$       $\times$       $\times$                                                    
  [@bstR]                                                                                                                                                                                                                                                
  [xgboost](https://CRAN.R-project.org/package=xgboost)            $\checkmark$   $\checkmark$     $\times$       $\times$       $\times$     $\checkmark$     $\times$       $\times$       $\times$                                                    
  [@xgboostR]                                                                                                                                                                                                                                            
  [gbm](https://CRAN.R-project.org/package=gbm)                    $\checkmark$   $\checkmark$   $\checkmark$   $\checkmark$     $\times$     $\checkmark$     $\times$       $\times$       $\times$                                                    
  [@gbmR]                                                                                                                                                                                                                                                
  [adabag](https://CRAN.R-project.org/package=adabag)                $\times$     $\checkmark$     $\times$       $\times$       $\times$     $\checkmark$     $\times$       $\times$       $\times$                                                    
  [@adabagR]                                                                                                                                                                                                                                             
  [lightgbm](https://CRAN.R-project.org/package=lightgbm)          $\checkmark$   $\checkmark$     $\times$       $\times$       $\times$     $\checkmark$     $\times$       $\times$       $\times$                                                    
  [@lightgbmR]                                                                                                                                                                                                                                           
  [GMMBoost](https://CRAN.R-project.org/package=GMMBoost)          $\checkmark$   $\checkmark$     $\times$       $\times$       $\times$     $\checkmark$     $\times$       $\times$       $\times$                                                    
  [@GMMBoostR]                                                                                                                                                                                                                                           
  [gamboostLSS](https://CRAN.R-project.org/package=gamboostLSS)    $\checkmark$     $\times$     $\checkmark$     $\times$     $\checkmark$   $\checkmark$     $\times$       $\times$       $\times$                                                    
  [@gamboostLSSR]                                                                                                                                                                                                                                        
  **BOOME** (in Python)                                              $\times$     $\checkmark$     $\times$       $\times$       $\times$     $\checkmark$   $\checkmark$   $\checkmark$   $\checkmark$                                                  
  [@BOOME]                                                                                                                                                                                                                                               

  : Comparisons among existing and proposed packages. This table
  summarizes three categories of packages: (i) variable selection
  without measurement error correction
  ([glmnet](https://CRAN.R-project.org/package=glmnet),
  [SIS](https://CRAN.R-project.org/package=SIS)), (ii) measurement error
  correction without variable selection
  ([GLSME](https://CRAN.R-project.org/package=GLSME),
  [mecor](https://CRAN.R-project.org/package=mecor),
  [augSIMEX](https://CRAN.R-project.org/package=augSIMEX),
  [simex](https://CRAN.R-project.org/package=simex),
  [simexaft](https://CRAN.R-project.org/package=simexaft)), (iii)
  statistical learning approaches that handle estimation without
  consideration of measurement error and variable selection
  ([bst](https://CRAN.R-project.org/package=bst),
  [xgboost](https://CRAN.R-project.org/package=xgboost),
  [gbm](https://CRAN.R-project.org/package=gbm),
  [adabag](https://CRAN.R-project.org/package=adabag),
  [lightgbm](https://CRAN.R-project.org/package=lightgbm),
  [GMMBoost](https://CRAN.R-project.org/package=GMMBoost),
  [gamboostLSS](https://CRAN.R-project.org/package=gamboostLSS)). The
  proposed package
  [SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost) is
  included in these three categories. In the Usage heading, 'LM' denotes
  the linear model; 'Class' is the classification; 'Pois' is the Poisson
  regression, 'Cox' is the Cox model; 'AFT' is the AFT model; 'SL' is
  statistical learning; 'ME' represents measurement error correction;
  'VS' is variable selection; and 'Col' represents collinearity.
:::

\
$^1$ "Class" includes binary or multiclass classification, and the
construction of logistic regression models.\
$^2$ "SL" contains several estimation methods based on machine learning
approaches, such as tree and random forest. In addition, the
corresponding packages may handle complex structures, including
semi-paramatric models, mixed-effects models, and generalized additive
models.

While many packages have been available to handle either variable
selection or measurement error correction, few packages deal with these
two features simultaneously. To address those concerns, we develop the R
package [SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost)
([@SIMEXBoostR]) by extending the method in [@Chen:2023] and
[@ChenQiu:2023], which covers commonly used GLM and AFT models.
Motivated by the idea of the boosting algorithm (see e.g.,
[@Hastie:2008], Section 16.2),
[SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost) aims to use
estimating functions to iteratively retain informative covariates and
exclude unimportant ones, yielding variable selection result. In
addition, to deal with measurement error effects, the package
[SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost) primarily
employs the SIMEX method to efficiently correct for measurement error
effects for different types of regression models. There are several
advantages of
[SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost) over the
existing packages. Specifically, as summarized in
TableÂ [1](#tab:compare){reference-type="ref" reference="tab:compare"},
while [glmnet](https://CRAN.R-project.org/package=glmnet) and
[SIS](https://CRAN.R-project.org/package=SIS) are able to handle
variable selection, they fail to deal with measurement error effects. In
addition, the two packages
[GLSME](https://CRAN.R-project.org/package=GLSME) and
[mecor](https://CRAN.R-project.org/package=mecor) focus on linear models
and aim to adjust for measurement error effects in the response and/or
covariates, but they cannot deal with variable selection. On the
contrary, the contribution of the package
[SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost) is able
handle measurement error in variables, and do variable selection and
estimation simultaneously. Moreover, boosting iteration may reduce the
possibility of falsely excluding important covariates and enhance the
accuracy of the estimator. Most importantly,
[SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost) is able to
deal with collinearity of variables by using the $L_2$-norm penalty
function.

The remainder is organized as follows. In the second section, we
introduce the data structure and the corresponding regression models. In
addition, the boosting algorithm is outlined. In the third section, we
introduce the measurement error model and extend the correction of
measurement error effects to the boosting algorithm. In the fourth
section, we introduce functions and their arguments in the R package
[SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost). In the
fifth section, we demonstrate the application of the R package
[SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost) and conduct
simulation studies to assess the performance of the boosting estimators.
Moreover, we also implement
[SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost) in a real
dataset. A general discussion is presented in the last section. The
supporting information, including a real dataset, programming code, and
numerical results in csv files, are placed in the corresponding author's
GitHub, whose link is given by
<https://github.com/lchen723/SIMEXBoost.git>.

# Notation, Models, and Boosting Procedure {#notation}

## Model {#Def: Data}

Let $Y$ denote the response, and let $\mathbf{X}$ be the $p$-dimensional
vector of covariates. Suppose that we have a sample of $n$ subjects and
for $i=1, \cdots, n$, $\{Y_i, \mathbf{X}_i\}$ has the same distribution
as $\left\{ Y, \mathbf{X} \right\}$.

Let $\boldsymbol{\beta}$ be a $p$-dimensional vector of (unknown)
parameters associated with the covariates $\mathbf{X}$, and write
$\mathbf{X}^\top \boldsymbol{\beta}$ as the linear predictor. In the
framework of statistical learning, to characterize the relationship
between $Y$ and $\mathbf{X}$, a commonly used approach is to link $Y$
and $\mathbf{X}^\top \boldsymbol{\beta}$ through the convex loss
function $L: \mathbb{S} \times \mathbb{R} \rightarrow \mathbb{R}$, where
$\mathbb{S}$ is the support of $Y$. Let the risk function be defined as
the expectation of the loss function, i.e.,
$R(\boldsymbol{\beta}) \triangleq E\left\{ L(Y, \mathbf{X}^\top \boldsymbol{\beta}) \right\}$.
Under the finite sample size $n$, the empirical version of
$R(\boldsymbol{\beta})$ is given by $$\begin{aligned}
\frac{1}{n} \sum \limits_{i=1}^n L(Y_i, \mathbf{X}_i^\top \boldsymbol{\beta} ).
\end{aligned}$$ Our goal is to estimate $\boldsymbol{\beta}$ by
minimizing the risk function, and the resulting estimator is given by
$$\begin{aligned}
\widehat{\boldsymbol{\beta}} = \mathop{\mathrm{argmin}}\limits_{\boldsymbol{\beta}} \bigg\{ \frac{1}{n} \sum \limits_{i=1}^n L(Y_i, \mathbf{X}_i^\top \boldsymbol{\beta} ) \bigg\}.
\end{aligned}$$ Equivalently, $\widehat{\boldsymbol{\beta}}$ satisfies
the estimating equation $\mathbf{g}(\mathbf{X},\boldsymbol{\beta}) = 0$,
where $\mathbf{g}(\mathbf{X},\boldsymbol{\beta})$ is the estimating
function of $\boldsymbol{\beta}$, defined as the first order derivative
of
$\frac{1}{n} \sum \limits_{i=1}^n L(Y_i, \mathbf{X}_i^\top \boldsymbol{\beta} )$
with respect to $\boldsymbol{\beta}$.

## Boosting Procedure {#Boost_VSE}

High-dimensionality and sparsity of $\boldsymbol{\beta}$ are crucial
concerns, which reflect the idea that some covariates are not
informative with respect to $Y$. To address these issues and provide a
reliable estimator of $\boldsymbol{\beta}$, we employ the boosting
procedure to perform variable selection and estimation (e.g.,
[@Hastie:2008]). This version of the boosting algorithm is motivated by
[@Wolfson:2011] and [@Brown:2017], and is applied to handle GLM
([@Chen:2023]) as well as AFT models ([@ChenQiu:2023]). An overall
procedure is presented in
AlgorithmÂ [\[algo-ThrEEBoost\]](#algo-ThrEEBoost){reference-type="ref"
reference="algo-ThrEEBoost"} with three key steps. Specifically, Steps 1
and 2 in
AlgorithmÂ [\[algo-ThrEEBoost\]](#algo-ThrEEBoost){reference-type="ref"
reference="algo-ThrEEBoost"} treat the estimating function evaluated at
an iterated value as the signal, and use it to determine informative
indexes of covariates and parameters. Noting that there is a parameter
$\tau$ in Step 2 that is used to control the number of selected
covariates in each iteration. In our numerical studies, $\tau=0.9$ seems
to be a suitable choice and has a satisfactory performance.

After that, Step 3 in
AlgorithmÂ [\[algo-ThrEEBoost\]](#algo-ThrEEBoost){reference-type="ref"
reference="algo-ThrEEBoost"} updates $\boldsymbol{\beta}$ using the
informative indexes determined in Step 2 by the sign of signals with
increment $\kappa$. This approach follows the steepest descent method
(see e.g., [@Boyd:2004], Section 9.4.2) and can be used to deal with the
$L_1$-norm for variable selection. In addition, as discussed in Section
16.2.1 of [@Hastie:2008], the value $\kappa$ has the opposite
relationship with the number of iteration $M$ that smaller $\kappa$
requires larger $M$. In our consideration, we specify $\kappa = 0.05$.
Finally, repeating the iteration $M$ times gives the desired estimator.
With $M$ time iterations, we can obtain the final set $\mathcal{J}_M$
containing informative covariates and the corresponding
$\boldsymbol{\beta}^{(M)} = \big( \beta_1^{(M)}, \cdots, \beta_p^{(M)}  \big)^\top$,
accomplishing variable selection.

::: algorithm
Let $\boldsymbol{\beta}^{(0)}=0$ denote an initial value
:::

Finally, for the application of
AlgorithmÂ [\[algo-ThrEEBoost\]](#algo-ThrEEBoost){reference-type="ref"
reference="algo-ThrEEBoost"}, we consider some specific models and the
corresponding regression models listed below. With models specified, we
can further determine the estimating function
$\mathbf{g}(\mathbf{X},\boldsymbol{\beta})$.

Linear regression models:

:   Â \
    Given the dataset
    $\big\{ \{Y_i, \mathbf{X}_i\} : i=1,\cdots,n\big\}$ with $Y_i$ being
    a continuous and univariate random variable, linear models are
    characterized as $$\begin{aligned}
     \label{LM}
    Y_i = \mathbf{X}_i^\top \boldsymbol{\beta} + \epsilon_i
    \end{aligned}$$ where $\epsilon_i$ is the noise term with
    $E(\epsilon_i)=0$ and $\text{var}(\epsilon_i)=\sigma_\epsilon^2$.
    The estimating function is defined as the first order derivative of
    the least squares function: $$\begin{aligned}
     \label{LM-g}
    \mathbf{g}(\mathbf{X},\boldsymbol{\beta}) = \sum \limits_{i=1}^n -\mathbf{X}_i ( Y_i - \mathbf{X}_i^\top \boldsymbol{\beta}).
    \end{aligned}$$

Logistic regression models:

:   Â \
    If $Y_i$ is a binary and univariate random variable, then $Y_i$ and
    $\mathbf{X}_i$ are usually characterized by a logistic regression
    model: $$\begin{aligned}
     \label{Logit}
    \pi_i = \frac{\exp\big(\mathbf{X}_i^\top \boldsymbol{\beta}\big)}{1+\exp\big(\mathbf{X}_i^\top \boldsymbol{\beta}\big)},
    \end{aligned}$$ where $\pi_i \triangleq P(Y_i=1 | \mathbf{X}_i)$.
    Following the idea in [@Agresti:2012], we can construct the
    likelihood function based on
    ([\[Logit\]](#Logit){reference-type="ref" reference="Logit"}).
    Therefore, the resulting estimating function is given by the first
    order derivative of the likelihood function: $$\begin{aligned}
     \label{Logit-g}
    \mathbf{g}(\mathbf{X},\boldsymbol{\beta}) = - \sum \limits_{i=1}^n \mathbf{X}_i \left\{ Y_i - \frac{\exp\big(\mathbf{X}_i^\top \boldsymbol{\beta}\big)}{1+\exp\big(\mathbf{X}_i^\top \boldsymbol{\beta}\big)} \right\}. 
    \end{aligned}$$

Poisson regression models:

:   Â \
    When $Y_i$ is a count and univariate random variable, one can adopt
    the Poisson regression model to fit $Y_i$ and $\mathbf{X}_i$:
    $$\begin{aligned}
     \label{Pois}
    \log \lambda_i = \mathbf{X}_i^\top \boldsymbol{\beta} 
    \end{aligned}$$ where $\lambda_i$ is the parameter of the Poisson
    distribution. Following the framework of generalized linear models,
    the likelihood function based on
    ([\[Pois\]](#Pois){reference-type="ref" reference="Pois"}) can be
    determined. Therefore, the first order derivative of the likelihood
    function under ([\[Pois\]](#Pois){reference-type="ref"
    reference="Pois"}) yields the corresponding estimating function,
    which is given by $$\begin{aligned}
     \label{Pois-g}
    \mathbf{g}(\mathbf{X},\boldsymbol{\beta}) = - \sum \limits_{i=1}^n \mathbf{X}_i \left\{ Y_i - \exp\big(\mathbf{X}_i^\top \boldsymbol{\beta}\big) \right\}.
    \end{aligned}$$

Accelerated failure time models:

:   Â \
    In survival analysis, the response is known as the failure time,
    denoted $\widetilde{T}_i>0$, and the accelerated failure time (AFT)
    model is a commonly used model for characterizing the relationship
    between the survival time and the covariates (e.g.,
    [@Lawless:2003]). Specifically, the AFT model is formulated as
    $$\begin{aligned}
     \label{AFT}
    \log \widetilde{T}_i =\mathbf{X}_i^\top \boldsymbol{\beta}+\eta_i,
    \end{aligned}$$ where $\eta_i$ is the noise term of
    ([\[AFT\]](#AFT){reference-type="ref" reference="AFT"}). In the
    framework of survival analysis, the main challenge is that
    $\widetilde{T}_i$ is usually incomplete due to how the observations
    are collected. In particular, in this study, the failure time may
    suffer from *length-biased* and *interval-censoring*, which cause
    the data to be biased and incomplete.

    Â Â Â Â  Specifically, for the length-biased sampling, it is common to
    assume that the incidence rate of the initial event is constant over
    calendar time, and the truncation time, denoted $\widetilde{A}_i$,
    is uniformly distributed in $[0,\xi]$, where $\xi$ is the maximum
    support of $\widetilde{T}_i$ (e.g., [@ChenQiu:2023]). For the
    length-biased data, we can observe
    $(\widetilde{A}_i$,$\widetilde{T}_i)$ only if
    $\widetilde{T}_i \geq \widetilde{A}_i$, and thus, we denote
    $(T_i,A_i) \equiv (\widetilde{T}_i,\widetilde{A}_i) \big| \widetilde{T}_i \geq \widetilde{A}_i$
    as the observed version of $(\widetilde{T}_i$,$\widetilde{A}_i)$.

    Â Â Â Â  On the other hand, for the observed $T_i$, we may encounter the
    interval-censoring. Suppose that $T_i$ is not exactly observed but
    only determined at a sequence of examination times, denoted as
    $A_i = U_0 <U_{1} < \cdots < U_{N} \leq  \xi$ for some constant
    $N>0$. The failure time is then known to lie in the interval
    $(L,R)$, where $L_i = \max\{U_k : U_k < T_i, k = 0, \cdots ,N\}$ and
    $R_i = \min\{U_k : U_k \geq T_i, k=1, \cdots ,N+1\}$ with
    $U_{N+1}\triangleq \infty$. Moreover, if $T_i$ occurs before the
    first examination time, then $(L_i,R_i) \triangleq (A_i, U_1)$; if
    the failure has not occurred at the last examination time, then
    $(L_i,R_i) \triangleq (U_N, \infty)$. Finally, let $\Delta_i$ denote
    the indicator, where a value 1 indicates that $T_i$ is observed and
    zero otherwise. As a consequence, for a sample with size $n$, the
    length-biased and interval-censored survival data is given by
    $\big\{ \{ A_i, \Delta_i, Y_i, X_i\} : i=1,\cdots,n \big\}$ with
    $Y_i \triangleq \{\Delta_iT_i, (1-\Delta_i)L_i, (1-\Delta_i)R_i\}$.

    Â Â Â Â  Based on the length-biased and interval-censored data, we can
    construct the estimating function (e.g., [@ChenQiu:2023])
    $$\begin{aligned}
     \label{eq7}
    \mathbf{g}(\mathbf{X},\boldsymbol{\beta}) = \sum_{i=1}^n \mathbf{X}_i \Bigg\{\Delta_i\frac{Y_{\beta,i}}{{\rm exp}(Y_{\beta,i})}+(1-\Delta_i)\frac{\int_{L_{i,0}}^{R_{i,0}} u^{-1}{\rm log}udF_{0}(u)}{F_{0}(R_{i,0})-F_{0}(L_{i,0})} \Bigg\},
    \end{aligned}$$ where
    $Y_{\beta,i}=\log T_i-\mathbf{X}_i^\top \boldsymbol{\beta}$,
    $R_{i,0}=R_i  \exp(-\mathbf{X}_i^\top\beta)$,
    $L_{i,0}=L_i \exp(-\mathbf{X}_i^\top\beta)$, and $F_{0}$ is the
    cumulative distribution function of $\eta_i$.

# A Modified Boosting Method with the Presence of Covariate Measurement Error {#Main-Result}

## Measurement Error Models

For $i=1,\cdots,n$, let $\mathbf{X}_i^\ast$ denote the surrogate, or
observed covariate, of $\mathbf{X}_i$. Let
$\boldsymbol{\Sigma}_{X^\ast}$ and $\boldsymbol{\Sigma}_X$ be the
$p \times p$ covariance matrices of $\mathbf{X}_i^\ast$ and
$\mathbf{X}_i$, respectively. In our development, we focus on the
classical measurement error model (e.g., [@Carroll:2006], Chapter 1):
$$\begin{aligned}
 \label{mea_classic}
\mathbf{X}_i^\ast =  \mathbf{X}_i + \mathbf{e}_i,
\end{aligned}$$ where $\mathbf{e}_i$ is independent of
$\left\{\mathbf{X}_i, Y_i \right\}$ and $\epsilon_i$ in
([\[LM\]](#LM){reference-type="ref" reference="LM"}), $\mathbf{e}_i$
follows a normal distribution with mean zero and the covariance matrix
$\boldsymbol{\Sigma}_e$, say $N(0,\boldsymbol{\Sigma}_e)$. Noting that
the covariance matrix $\boldsymbol{\Sigma}_e$ is usually unknown. To
determine it, we can either employ sensitivity analyses to reasonably
specify values, or directly estimate it if additional information, such
as repeated measurements or validation sample is available (see e.g.,
[@ChenYi:2021]). As a result, in the presence of measurement error, the
*observed* dataset is now given by
$\big\{ \{Y_i, \mathbf{X}_i^\ast\} : i=1,\cdots,n\big\}$.

## Boosting with Measurement Error Correction

In the presence of measurement error, it is known that directly using
$\mathbf{X}_i^\ast$ in the estimating procedure without the correction
of measurement error effects may incur biased estimate and wrong
conclusion (see e.g., [@Carroll:2006]). Therefore, even though
AlgorithmÂ [\[algo-ThrEEBoost\]](#algo-ThrEEBoost){reference-type="ref"
reference="algo-ThrEEBoost"} is valid to estimate $\boldsymbol{\beta}$
for regression models in the 'Boosting Procedure' subsection, it is
insufficient in the presence of measurement error effects.

To deal with measurement error in covariates, we extend
AlgorithmÂ [\[algo-ThrEEBoost\]](#algo-ThrEEBoost){reference-type="ref"
reference="algo-ThrEEBoost"} by adopting the SIMEX method to eliminate
the impact of measurement error (e.g., [@ChenYi:2021]). The modified
algorithm, called *SIMEXBoost*, is summarized in
AlgorithmÂ [\[algo-SIMEX\]](#algo-SIMEX){reference-type="ref"
reference="algo-SIMEX"}.

The idea of the SIMEX method is to first establish the trend of
measurement error-induced biases as a function of the variance of
measurement error by artificially creating a sequence of surrogate
measurements, and then extrapolate this trend back to the case without
measurement error. Specifically, in Step 1 of
AlgorithmÂ [\[algo-SIMEX\]](#algo-SIMEX){reference-type="ref"
reference="algo-SIMEX"}, we artificially create a sequence of
error-contaminated surrogate measurements by introducing different
degrees of measurement error. After that, as shown in Step 2 of
AlgorithmÂ [\[algo-SIMEX\]](#algo-SIMEX){reference-type="ref"
reference="algo-SIMEX"}, we apply those surrogate measurements to the
boosting procedure, and use a new function
$\mathbf{g}_\text{\tiny SIM}(\boldsymbol{\beta};b,\zeta)$, which is
defined as ([\[LM-g\]](#LM-g){reference-type="ref" reference="LM-g"}),
([\[Logit-g\]](#Logit-g){reference-type="ref" reference="Logit-g"}),
([\[Pois-g\]](#Pois-g){reference-type="ref" reference="Pois-g"}), or
([\[eq7\]](#eq7){reference-type="ref" reference="eq7"}) with
$\mathbf{X}_i$ replaced by $\mathbf{W}_i \left(b,\zeta\right)$ defined
in ([\[Sim_W\]](#Sim_W){reference-type="ref" reference="Sim_W"}), to
obtain biased estimates by running an estimation method developed for
error-free settings. Finally, Step 3 in
AlgorithmÂ [\[algo-SIMEX\]](#algo-SIMEX){reference-type="ref"
reference="algo-SIMEX"} traces the pattern of biased estimates against
varying magnitudes of measurement error and then does extrapolation
based on linear or quadratic regression models.

Noting that there are several parameters in
AlgorithmÂ [\[algo-SIMEX\]](#algo-SIMEX){reference-type="ref"
reference="algo-SIMEX"}. The parameters $M$, $\kappa$, and $\tau$ in
Step 2 are the same as those in
AlgorithmÂ [\[algo-ThrEEBoost\]](#algo-ThrEEBoost){reference-type="ref"
reference="algo-ThrEEBoost"}. On the other hand, Step 1 contains a value
$B$ and a sequence of $\mathcal{Z}$ that are usually user-specified.
Typically, $\mathcal{Z}$ is usually defined as $K$ equal-width cutpoints
in an interval $[0,1]$ for some positive constant $K$. A value $B$ is
used to perform the Monte Carlo computation in
([\[SIMEX-ave\]](#SIMEX-ave){reference-type="ref"
reference="SIMEX-ave"}) and make the estimator more stable. A larger
value of $B$ may implicitly incur longer computational time. Our
numerical experiments show that $B=50$ gives satisfactory performance.

::: algorithm

Step 1:

:   Generate the working data $\mathbf{W}_i(b,\zeta)$ by
    $$\begin{aligned}
     \label{Sim_W}
    \mathbf{W}_i \left(b,\zeta\right) = \mathbf{X}_i^\ast + \sqrt{\zeta} \mathbf{e}_{i,b}
    \end{aligned}$$ for $b=1,\cdots,B$ and $\zeta \in \mathcal{Z}$,
    where $\mathbf{e}_{i,b} \sim N(0,\boldsymbol{\Sigma}_e)$
    independently.

Step 2:

:   Boosting estimation.\
    Perform the following boosting procedure with $M$ iterations.

When $\boldsymbol{\beta}^{(M)}(b,\zeta)$ is obtained for $b=1,\cdots,B$
and $\zeta \in \mathcal{Z}$, compute an average $$\begin{aligned}
 \label{SIMEX-ave}
\boldsymbol{\beta}^{(M)}(\zeta) = \frac{1}{B} \sum \limits_{b=1}^B \boldsymbol{\beta}^{(M)}(b,\zeta) \ \ \text{for}\ \ \zeta \in \mathcal{Z}.
\end{aligned}$$

Step 3:

:   Extrapolation.\
    Fit a sequence
    $\left\{ \left( \zeta, \boldsymbol{\beta}^{(M)} (\zeta) \right) : \zeta \in \mathcal{Z} \right\}$
    by a regression model, and the final value is given by the
    extrapolated value at $\zeta=-1$.
:::

# Description and Implementation of [SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost) {#Sec-package}

We develop an R package, called
[SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost), to
implement the variable selection and estimation with measurement error
correction described in the preceding section. This package depends on
the [MASS](https://CRAN.R-project.org/package=MASS) package only. The
package [SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost)
contains three functions: ME_Data, Boost_VSE, and SIMEXBoost. The
function ME_Data aims to generate artificial data under specific models
listed in 'Boosting Procedure' subsection and error-prone covariates.
The function Boost_VSE implements the boosting procedure in
AlgorithmÂ [\[algo-ThrEEBoost\]](#algo-ThrEEBoost){reference-type="ref"
reference="algo-ThrEEBoost"}, and the function SIMEXBoost implements the
error-eliminated boosting procedure as displayed in
AlgorithmÂ [\[algo-SIMEX\]](#algo-SIMEX){reference-type="ref"
reference="algo-SIMEX"}. We now describe the details of these three
functions.

##  ME_Data {#me_data .unnumbered}

We use the following command to obtain the artificial data:

::: example
ME_Data(X,beta,type=\"normal\",sigmae,pr0=0.5)
:::

where the meaning of each argument is described as follows:

-   X: An $n\times p$ matrix with components generated by random
    variables. It is provided by the user.

-   beta: A $p$-dimensional vector of parameters, which is specified by
    the user.

-   type: A regression model that is specified to generate the response.
    Some choices listed in 'Boosting Procedure' subsection are provided
    in this argument. normal means the linear regression model
    ([\[LM\]](#LM){reference-type="ref" reference="LM"}) with the error
    term generated by the standard normal distribution; binary means the
    logistic regression model ([\[Logit\]](#Logit){reference-type="ref"
    reference="Logit"}); poisson means the Poisson regression model
    ([\[Pois\]](#Pois){reference-type="ref" reference="Pois"}). In
    addition, the accelerated failure time (AFT) model is considered to
    fit length-biased and interval-censored survival data. Specifically,
    AFT-normal generates the length-biased and interval-censored
    survival data under the AFT model
    ([\[AFT\]](#AFT){reference-type="ref" reference="AFT"}) with the
    error term being normal distributions; AFT-loggamma generates the
    length-biased and interval-censored survival data under the AFT
    model with the error term being log-gamma distributions.

-   sigmae: A $p \times p$ covariance matrix $\boldsymbol{\Sigma}_e$ in
    the measurement error model
    ([\[mea_classic\]](#mea_classic){reference-type="ref"
    reference="mea_classic"}). Given $\boldsymbol{\Sigma}_e$ with
    non-zero entries, by
    ([\[mea_classic\]](#mea_classic){reference-type="ref"
    reference="mea_classic"}), one can generate the error-prone
    covariates $\mathbf{X}_i^\ast$. Moreover, if $\boldsymbol{\Sigma}_e$
    is given by the zero matrix, then $\mathbf{e}_i$ is generated as
    zero values, yielding that $\mathbf{X}_i^\ast$ is equal to
    $\mathbf{X}_i$, and thus, the resulting covariate is the original
    input given by users.

-   pr0: A numerical value in an interval $(0,1)$. It is used to
    determine the censoring rate for the length-biased and
    interval-censored data.

The function ME_Data returns a list of components:

-   response: It gives the response generated by a specific regression
    model. type=\"normal\" gives a n-dimensional continuous vector;
    type=\"binary\" gives a n-dimensional vector with binary entries;
    type=\"poisson\" gives a n-dimensional vector with entries being
    counting numbers. In addition, type=\"AFT-normal\" and
    type=\"AFT-loggamma\" generates a $n\times 2$ matrix of
    length-biased and interval-censored responses, where the first
    column is the lower bound of an interval-censored response and the
    second column is the upper bound of an interval-censored response.

-   ME_covariate: This output gives a $n \times p$ matrix of
    "error-prone" or "precisely measured" covariates. Specifically, as
    discussed in an argument sigmae, if $\boldsymbol{\Sigma}_e$ is a
    non-zero covariance matrix, then the result of ME_covariate is given
    by $\mathbf{X}_i^\ast$; if $\boldsymbol{\Sigma}_e$ is a zero matrix,
    then the result of ME_covariate is the original input, say
    $\mathbf{X}_i$.

##  Boost_VSE {#boost_vse .unnumbered}

We use the following function to perform
AlgorithmÂ [\[algo-ThrEEBoost\]](#algo-ThrEEBoost){reference-type="ref"
reference="algo-ThrEEBoost"}:

::: example
Boost_VSE(Y,Xstar,type=\"normal\",Iter=200,Lambda=0)
:::

where the meaning of each argument is described as follows:

-   Y: The response variable. If type is specified as normal, binary, or
    poisson, then Y should be a n-dimensional vector; if type is given
    by AFT-normal or AFT-loggamma, then Y should be a $n \times 2$
    matrix of interval-censored responses, where the first column is the
    lower bound of an interval-censored response and the second column
    is the upper bound of an interval-censored response.

-   Xstar: This argument needs a $n \times p$ matrix of covariates. It
    can be error-prone or precisely measured.

-   type: This argument specifies the regression models as previously
    described in the function `ME_Data` as well as their corresponding
    estimating functions given by
    ([\[LM-g\]](#LM-g){reference-type="ref" reference="LM-g"}),
    ([\[Logit-g\]](#Logit-g){reference-type="ref" reference="Logit-g"}),
    ([\[Pois-g\]](#Pois-g){reference-type="ref" reference="Pois-g"}),
    and ([\[eq7\]](#eq7){reference-type="ref" reference="eq7"}).

-   Iter: The number of iterations $M$ for the boosting procedure in
    AlgorithmÂ [\[algo-ThrEEBoost\]](#algo-ThrEEBoost){reference-type="ref"
    reference="algo-ThrEEBoost"}.

-   Lambda: A tuning parameter that aims to deal with the collinearity
    of covariates. Lambda=0 means that no $L_2$-norm is involved, and it
    is the default value.

The function Boost_VSE returns a list of components:

-   BetaHat: The vector of estimated coefficient obtained by
    AlgorithmÂ [\[algo-ThrEEBoost\]](#algo-ThrEEBoost){reference-type="ref"
    reference="algo-ThrEEBoost"}. In particular, if the covariate Xstar
    is generated by ME_Data with the argument sigmae being the zero
    matrix, then the resulting vector is the "ordinary" estimator based
    on the boosting procedure; if the covariate Xstar is generated by
    ME_Data with the argument sigmae being a non-zero covariance matrix,
    then we call the resulting vector as the *naive* estimator due to
    involvement of measurement error effects without correction.

##  SIMEXBoost {#simexboost .unnumbered}

Basically, some arguments in this function are the same as Boost_VSE,
except for some slightly different requirements and additional arguments
that are related to the SIMEX method.

We use the following function to perform
AlgorithmÂ [\[algo-SIMEX\]](#algo-SIMEX){reference-type="ref"
reference="algo-SIMEX"}:

::: example
SIMEXBoost(Y,Xstar,zeta=c(0,0.25,0.5,0.75,1),B=500,type=\"normal\",sigmae,
Iter=100, Lambda=0, Extrapolation=\"linear\")
:::

where the meanings of arguments Y, Xstar, type, Iter, and Lambda are the
same as those in the function Boost_VSE; additional arguments zeta, B,
sigmae, and Extrapolation, which are used to implement the SIMEX method
to correct for measurement error effects, are described as follows:

-   zeta: The user-specific sequence of values described as
    $\mathcal{Z}$ in Step 1 of
    AlgorithmÂ [\[algo-SIMEX\]](#algo-SIMEX){reference-type="ref"
    reference="algo-SIMEX"}.

-   B: The user-specific positive number of repetition described as $B$
    in Step 1 of
    AlgorithmÂ [\[algo-SIMEX\]](#algo-SIMEX){reference-type="ref"
    reference="algo-SIMEX"}.

-   sigmae: An $p \times p$ covariance matrix $\boldsymbol{\Sigma}_e$ in
    the measurement error model
    ([\[mea_classic\]](#mea_classic){reference-type="ref"
    reference="mea_classic"}). In practical applications, if auxiliary
    information is unavailable, sensitivity analyses can be adopted to
    reasonably specify values of $\boldsymbol{\Sigma}_e$. If additional
    information, such as repeated measurements or validation samples, is
    available, one can directly estimate $\boldsymbol{\Sigma}_e$.

-   Extrapolation: A extrapolation function for the SIMEX method
    implemented to Step 3 of
    AlgorithmÂ [\[algo-SIMEX\]](#algo-SIMEX){reference-type="ref"
    reference="algo-SIMEX"}. In the framework of the SIMEX method,
    quadratic and linear functions are common. Therefore, in this
    argument, we provide two choices of the extrapolation functions,
    linear and quadratic.

The function SIMEXBoost returns a list of components:

-   BetaHatCorrect: The resulting vector of corrected estimates obtained
    by AlgorithmÂ [\[algo-SIMEX\]](#algo-SIMEX){reference-type="ref"
    reference="algo-SIMEX"}.

# Numerical Studies and Demonstration of Programming Code  {#Numerical}

In this section, we demonstrate the usage of the functions in the
package [SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost).
There are two parts in this section: we first illustrate simulation
studies for linear regression, Poisson regression, and AFT models. After
that, we apply the package to analyze a real-world dataset with binary
responses based on a logistic regression model.

## Simulation Studies

In this section, we use simulation studies to demonstrate applications
of functions in the package
[SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost) and assess
the performance of the estimators derived by two functions `Boost_VSE`
and `SIMEXBoost`.

We consider the dimension of covariates $p=200$ or $500$, and let the
sample size $n=400$. Let the true value
$\boldsymbol{\beta}_0 = (1,1,1,\mathbf{0}_{p-3}^\top)^\top$, where
$\mathbf{0}_q$ is a $q$-dimensional zero vector. The unobserved
covariate $\mathbf{X}_i$ is generated by the standard normal
distribution, and it can be used to generate the response $Y_i$ based on
([\[LM\]](#LM){reference-type="ref" reference="LM"}),
([\[Pois\]](#Pois){reference-type="ref" reference="Pois"}), and
([\[AFT\]](#AFT){reference-type="ref" reference="AFT"}). For the
error-prone covariate $\mathbf{X}_i^\ast$, it can be generated by
([\[mea_classic\]](#mea_classic){reference-type="ref"
reference="mea_classic"}), where $\boldsymbol{\Sigma}_e$ is a diagonal
matrix with common entries $\sigma_e$ being 0.1, 0.3, and 0.5.

Based on the artificial data
$\Big\{ \{Y_i,\mathbf{X}_i^\ast\} : i=1,\cdots,n \Big\}$, we first use
the function Boost_VSE to obtain the estimator *without* measurement
error correction, which is called the *naive* estimator. Next, we apply
the function SIMEXBoost to derive the *corrected* estimator. To assess
the performance of variable selection, we examine the specificity (Spe)
and the sensitivity (Sen), where the specificity is defined as the
proportion of zero coefficients that are correctly estimated to be zero,
and the sensitivity is defined as the proportion of non-zero
coefficients that are correctly estimated to be non-zero. In addition,
to evaluate the performance of estimation, we use the $L_1$ and
$L_2$-norms to measure bias, which are respectively defined as
$$\begin{aligned}
 \label{eva-bias}
\big\| \widehat{\boldsymbol{\beta}} - \boldsymbol{\beta}_0 \big\|_1 = \sum \limits_{j=1}^p \big| \widehat{\beta}_j - \beta_{0,j} \big| \ \ \text{and} \ \ \big\| \widehat{\boldsymbol{\beta}} - \boldsymbol{\beta}_0 \big\|_2 = \bigg\{\sum \limits_{j=1}^p \big( \widehat{\beta}_j - \beta_{0,j} \big)^2 \bigg\}^{1/2},
\end{aligned}$$ where $\widehat{\boldsymbol{\beta}}$ is the estimator,
$\widehat{\beta}_j$ and $\beta_{0,j}$ are the $j$th component in
$\widehat{\boldsymbol{\beta}}$ and $\boldsymbol{\beta}_0$, respectively.

We use two ' for' loops cover all combinations of $p$ and $\sigma_e$ (
sigmae). For each p and sigmae, we first adapt the function ME_Data to
generate the simulated data, where Y and Xstar represent the response
and error-prone covariates, respectively. After that, to examine the
naive estimator derived by
AlgorithmÂ [\[algo-ThrEEBoost\]](#algo-ThrEEBoost){reference-type="ref"
reference="algo-ThrEEBoost"}, we use the function Boost_VSE to derive
the naive estimator and denote it by naive. To implement
AlgorithmÂ [\[algo-SIMEX\]](#algo-SIMEX){reference-type="ref"
reference="algo-SIMEX"}, we adopt the function SIMEXBoost, where two
components $\mathcal{Z}$ and $B$ in Step 1 of
AlgorithmÂ [\[algo-SIMEX\]](#algo-SIMEX){reference-type="ref"
reference="algo-SIMEX"} are specified as zeta=c(0,0.25,0.5,0.75,1) and
B=50, respectively. For the SIMEXBoost method, we also examine linear
and quadratic extrapolation functions with the argument
Extrapolation=\"linear\" or Extrapolation=\"quadratic\", and denote two
resulting estimators as correctL and correctQ, respectively. For the two
functions Boost_VSE and SIMEXBoost, we set the number of iterations
Iter=50. To save the space in the limited text, we simply illustrate the
model ([\[LM\]](#LM){reference-type="ref" reference="LM"}) with the
argument type=\"normal\"; numerical results under other models can be
reprocued by the following code with type=\"normal\" replaced by
type=\"poisson\" or type=\"AFT-normal\".

Next, we assess the performance of naive, correctL, and correctQ. Given
a true vector of parameter beta0, we compute $L_1$ and $L_2$-norms in
([\[eva-bias\]](#eva-bias){reference-type="ref" reference="eva-bias"})
to examine the bias, and compute Spe and Sen to examine variable
selection. Under a given p and sigma, biases and variable selection
results are recorded by EST1, EST2, and EST3 for the estimators naive,
correctL, and correctQ, respectively. Finally, numerical results of
three estimators naive, correctL, and correctQ under all settings are
summarized by NAIVE, SIMEXL, and SIMEXQ, respectively.

Numerical results under ([\[LM\]](#LM){reference-type="ref"
reference="LM"}), ([\[Pois\]](#Pois){reference-type="ref"
reference="Pois"}), and ([\[AFT\]](#AFT){reference-type="ref"
reference="AFT"}) are placed in
TablesÂ [2](#tab:Sim-1){reference-type="ref"
reference="tab:Sim-1"}-[4](#tab:Sim-4){reference-type="ref"
reference="tab:Sim-4"}, respectively. We observe that the naive and
corrected estimates are affected by the magnitudes of measurement error
effects and the dimension $p$. When values of p and sigma become large,
the biases given by $L_1$ and $L_2$-norms are increasing. For the
comparison between the naive and corrected estimators, we can see that
biases produced by the naive estimator are significantly larger than
those obtained by the corrected estimator. In addition, for the variable
selection result, the corrected estimator is able to correctly retain
the informative covariates and exclude unimportant ones, except for some
cases that one or two covariates may be falsely included. On the
contrary, we can observe from the naive method that values of Spe_naive
are always small while values of Sen_naive are equal to one. It
indicates that the naive estimator retains the truly important
covariates, and meanwhile, includes a lot of unimportant ones, which
shows an evidence that the naive method fails to do variable selection.
Finally, for the comparison between two extrapolation functions,
Extrapolation=\"linear\" and Extrapolation=\"quadratic\", we observe
that the specification of a linear extrapolation function has slightly
better performance than the case under a quadratic extrapolation
function, especially when sigma is large.

While the proposed SIMEXBoost method can handle measurement error well,
the main concern is the computational time. According to the record of
the system CPU time (in seconds) from the R function `proc.time()` under
the device ASUS DESKTOP-HJSD47S with the processor Intel(R) Core(TM)
i7-10700 CPU @ 2.90GHz, we find that, for each setting with fixed p and
sigmae, the proposed SIMEXBoost method requires 14.09 and 15.04 seconds
under `Extrapolation="linear"` and `Extrapolation="quadratic"` to derive
the estimator for $p=200$, respectively. Unsurprising, when the
dimension increases to $p=500$, the computational times under
`Extrapolation="linear"` and `Extrapolation="quadratic"` are increasing
to 17.70 and 19.68 seconds, respectively. On the other hand, without
measurement error correction under each setting, the naive method needs
12.98 and 15.71 seconds to derive the estimator for $p=200$ and $500$,
yielding the slightly faster computational time than the SIMEXBoost
method. It might be due to the measurement error correction with the
involvement of $\mathcal{Z}$ and the number of repetitions $B$ in Step 2
of AlgorithmÂ [\[algo-SIMEX\]](#algo-SIMEX){reference-type="ref"
reference="algo-SIMEX"}. As a result, we comment that the SIMEXBoost
method is able to address measurement error correction, but a slightly
longer computational time is the price that the users should pay for.

::: {#tab:Sim-1}
   $p$   $\sigma_e$         Methods          $L_1$-norm   $L_2$-norm    Spe     Sen                              
  ----- ------------ ---------------------- ------------ ------------ ------- ------- -- -- -- -- -- -- -- -- -- --
   200      0.1              Naive             6.900        0.738      0.487   1.000                             
                       SIMEXBoost-Linear       0.116        0.099      1.000   1.000                             
                      SIMEXBoost-Quadratic     0.109        0.090      1.000   1.000                             
            0.3              Naive             6.350        0.684      0.503   1.000                             
                       SIMEXBoost-Linear       0.101        0.072      1.000   1.000                             
                      SIMEXBoost-Quadratic     0.106        0.075      1.000   1.000                             
            0.5              Naive             7.600        0.758      0.426   1.000                             
                       SIMEXBoost-Linear       0.257        0.182      1.000   1.000                             
                      SIMEXBoost-Quadratic     0.281        0.188      1.000   1.000                             
   500      0.1              Naive             8.050        0.733      0.732   1.000                             
                       SIMEXBoost-Linear       0.054        0.051      1.000   1.000                             
                      SIMEXBoost-Quadratic     0.069        0.057      1.000   1.000                             
            0.3              Naive             8.900        0.778      0.710   1.000                             
                       SIMEXBoost-Linear       0.300        0.187      1.000   1.000                             
                      SIMEXBoost-Quadratic     0.300        0.187      1.000   1.000                             
            0.5              Naive             10.000       0.889      0.692   1.000                             
                       SIMEXBoost-Linear       0.600        0.354      1.000   1.000                             
                      SIMEXBoost-Quadratic     0.600        0.354      1.000   1.000                             

  : Simulation results for the model ([\[LM\]](#LM){reference-type="ref"
  reference="LM"}) with type=\"normal\". "Naive" is the naive method
  obtained by the function Boost_VSE. "SIMEXBoost-Linear" and
  "SIMEXBoost-Quadratic" refer to the proposed method obtained by the
  function SIMEXBoost with the argument Extrapolation = \"linear\" and
  Extrapolation = \"quadratic\", respectively. $L_1$-norm and $L_2$-norm
  are given by ([\[eva-bias\]](#eva-bias){reference-type="ref"
  reference="eva-bias"}). Spe and Sen are specificity and sensitivity,
  respectively.
:::

::: {#tab:Sim-3}
   $p$   $\sigma_e$         Methods          $L_1$-norm   $L_2$-norm    Spe     Sen                              
  ----- ------------ ---------------------- ------------ ------------ ------- ------- -- -- -- -- -- -- -- -- -- --
   200      0.1              Naive             1.600        0.283      0.848   1.000                             
                       SIMEXBoost-Linear       0.208        0.126      1.000   1.000                             
                      SIMEXBoost-Quadratic     0.538        0.407      1.000   1.000                             
            0.3              Naive             1.750        0.304      0.838   1.000                             
                       SIMEXBoost-Linear       0.178        0.129      1.000   1.000                             
                      SIMEXBoost-Quadratic     0.520        0.301      1.000   1.000                             
            0.5              Naive             2.500        0.387      0.782   1.000                             
                       SIMEXBoost-Linear       0.366        0.230      1.000   1.000                             
                      SIMEXBoost-Quadratic     1.371        0.877      0.995   1.000                             
   500      0.1              Naive             1.400        0.265      0.944   1.000                             
                       SIMEXBoost-Linear       0.096        0.087      1.000   1.000                             
                      SIMEXBoost-Quadratic     0.428        0.385      1.000   1.000                             
            0.3              Naive             1.850        0.304      0.930   1.000                             
                       SIMEXBoost-Linear       0.354        0.236      1.000   1.000                             
                      SIMEXBoost-Quadratic     0.279        0.193      1.000   1.000                             
            0.5              Naive             3.000        0.458      0.903   1.000                             
                       SIMEXBoost-Linear       0.554        0.393      1.000   1.000                             
                      SIMEXBoost-Quadratic     1.238        0.771      1.000   1.000                             

  : Simulation results for the model
  ([\[Pois\]](#Pois){reference-type="ref" reference="Pois"}) with
  `type="poisson"`. "Naive" is the naive method obtained by the function
  `Boost_VSE`. "SIMEXBoost-Linear" and "SIMEXBoost-Quadratic" refer to
  the proposed method obtained by the function SIMEXBoost with the
  argument Extrapolation = \"linear\" and Extrapolation = \"quadratic\",
  respectively. $L_1$-norm and $L_2$-norm are given by
  ([\[eva-bias\]](#eva-bias){reference-type="ref"
  reference="eva-bias"}). Spe and Sen are specificity and sensitivity,
  respectively.
:::

::: {#tab:Sim-4}
   $p$   $\sigma_e$         Methods          $L_1$-norm   $L_2$-norm    Spe     Sen                              
  ----- ------------ ---------------------- ------------ ------------ ------- ------- -- -- -- -- -- -- -- -- -- --
   200      0.1              Naive             0.875        0.603      1.000   1.000                             
                       SIMEXBoost-Linear       0.378        0.232      1.000   1.000                             
                      SIMEXBoost-Quadratic     0.411        0.258      1.000   1.000                             
            0.3              Naive             1.450        1.078      1.000   0.667                             
                       SIMEXBoost-Linear       0.400        0.247      1.000   1.000                             
                      SIMEXBoost-Quadratic     0.877        0.605      1.000   1.000                             
            0.5              Naive             4.700        2.801      1.000   0.667                             
                       SIMEXBoost-Linear       1.614        1.179      0.995   1.000                             
                      SIMEXBoost-Quadratic     2.958        2.204      0.995   1.000                             
   500      0.1              Naive             2.750        1.521      0.998   0.333                             
                       SIMEXBoost-Linear       0.959        0.560      1.000   1.000                             
                      SIMEXBoost-Quadratic     2.710        2.288      0.998   1.000                             
            0.3              Naive             8.425        5.398      1.000   0.667                             
                       SIMEXBoost-Linear       0.949        0.554      1.000   1.000                             
                      SIMEXBoost-Quadratic     0.820        0.518      1.000   1.000                             
            0.5              Naive             4.350        2.743      1.000   0.333                             
                       SIMEXBoost-Linear       2.541        1.364      0.998   0.667                             
                      SIMEXBoost-Quadratic     4.003        2.604      0.998   1.000                             

  : Simulation results for the model
  ([\[AFT\]](#AFT){reference-type="ref" reference="AFT"}) with
  `type="AFT-normal"`. "Naive" is the naive method obtained by the
  function `Boost_VSE`. "SIMEXBoost-Linear" and "SIMEXBoost-Quadratic"
  refer to the proposed method obtained by the function SIMEXBoost with
  the argument Extrapolation = \"linear\" and Extrapolation =
  \"quadratic\", respectively. $L_1$-norm and $L_2$-norm are given by
  ([\[eva-bias\]](#eva-bias){reference-type="ref"
  reference="eva-bias"}). Spe and Sen are specificity and sensitivity,
  respectively.
:::

## Real Data Example

In this section, we apply the package
[SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost) to analyze
the Company Bankruptcy Prediction data, which was from the Taiwan
Economic Journal during a period 1999--2009, and it is now available on
the Kaggle website
(<https://www.kaggle.com/datasets/fedesoriano/company-bankruptcy-prediction>).
In this dataset, there are 6819 observations and 95 continuous
covariates related to customers' banking records, such as assets,
liability, income, and so on. In addition, the response is a binary
random variable, where the value 1 reflects that the company is
bankrupt, and 0 otherwise. All variables' names and descriptions can be
found on the Kaggle website. The main interest in this study is to
identify covariates that are informative to the bankruptcy status, and
our goal is to apply ([\[Logit\]](#Logit){reference-type="ref"
reference="Logit"}) to characterize the bankruptcy status and
covariates.

In addition to detecting important covariates from the multivariate
variables, as mentioned in [@Chen:2023b], measurement error is
ubiquitous in variables related to customers' banking records. Hence,
one should take measurement error effects into account when doing
variable selection. Following the example of the simulation studies, we
primarily consider two scenarios: first, without consideration of
measurement error, one can directly apply the function Boost_VSE to do
variable selection. Second, if we wish to implement the function
SIMEXBoost and take measurement error correction into account, the
covariance matrix sigmae is needed. Since the dataset has no additional
information to estimate $\boldsymbol{\Sigma}_e$, we may conduct
*sensitivity analyses*, which enable us to characterize various degrees
of measurement error and examine the different magnitudes of measurement
error effects (e.g., [@ChenYi:2021]; [@Chen:2023b]). Specifically, we
specify $\boldsymbol{\Sigma}_e$ as a diagonal matrix with common entries
being $R=0.1, 0.3$ and $0.5$, and the extrapolation function is taken as
linear or quadratic functions in
AlgorithmÂ [\[algo-SIMEX\]](#algo-SIMEX){reference-type="ref"
reference="algo-SIMEX"}.

To show the implementation of data analysis, we demonstrate the
programming code below. For the convenience of data analysis, we export
the full dataset as a csv file, which is available in
<https://github.com/lchen723/SIMEXBoost.git>. One can download the
dataset 'bankruptcy_data.csv'. Based on the dataset, we denote Y and
Xstar as the response and the observed covariates, respectively. For the
naive method without measurement error correction, we adopt the function
Boost_VSE with 50 iterations. For the implementation of sensitivity
analyses, we use a ' for' loop for different values of R. For each R, we
run SIMEXBoost with type=\"binary\" and two extrapolation functions
Extrapolation = \"linear\" and Extrapolation = \"quadratic\".

Numerical results are summarized in
TableÂ [\[rda\]](#rda){reference-type="ref" reference="rda"}, where the
column "ID" is the indexes of selected covariates, and the heading "EST"
is the estimates of the coefficients. According to our analysis results,
we find that the covariate "Net Income to Stockholder's Equity" (ID #90)
is only one that is commonly selected from Boost_VSE and SIMEXBoost
under different values of $R$, which indicates that the covariate #90 is
an informative variable regardless of measurement error effects have
been corrected or not. For the corrected estimator with different $R$
and extrapolation functions, we observe that variables "Operating Profit
Rate: Operating Income/Net Sales" (ID #6), "Pre-tax net Interest Rate:
Pre-Tax Income/Net Sales" (ID #7), "Continuous interest rate (after
tax): Net Income-Exclude Disposal Gain or Loss/Net Sales" (ID #10), and
"Liability-Assets Flag" (ID #85) are selected, except for the scenarios
"Correct-L-0.3" and "Correct-L-0.5". Moreover, different variables are
selected under different values of $R$. It might be due to the impact of
different magnitudes of measurement error. On the other hand, without
measurement error correction, we observe from the naive estimator that
most selected variables are different from the proposed estimator, such
as "Research and development expense rate: (Research and Development
Expenses)/Net Sales" (ID #12), "Tax rate (A): Effective Tax Rate" (ID
#15), "Per Share Net profit before tax (Yuan Â¥): Pretax Income Per
Share" (ID #23), "Total Asset Growth Rate: Total Asset Growth" (ID #29),
and "Cash Reinvestment %: Cash Reinvestment Ratio" (ID #32), and those
estimates are close to zero. It implies that noninformative variables
are possibly selected by the naive method if measurement error is not
taken into account in analysis, and it shows an impact of measurement
error in data analysis.

Finally, we use the function proc.time() to record the system CPU time,
and we find that the function SIMEXBoost requires 3.84 and 3.50 seconds
to run, under Extrapolation = \"linear\" and Extrapolation =
\"quadratic\" with a fixed $R$, respectively, while the function
Boost_VSE needs 0.45 seconds to derive the estimates. Consistent with
the finding in simulation studies, the SIMEXBoost method requires
slightly longer computational time than the naive method, which is
caused by the repetition of boosting procedure and SIMEX correction.

::: tabular
ccccccccccccccccccccccc

ID & EST \
& Naive & Correct-L-0.1 & Correct-L-0.3 & Correct-L-0.5 & Correct-Q-0.1
& Correct-Q-0.3 & Correct-Q-0.5\
6 & $-$ & 0.400 & $-$ & -0.383 & 3.025 & 1.781 & 2.296\
7 & $-$ & 0.474 & $-$ & $-$ & 2.895 & 2.126 & 2.943\
8 & $-$ & 0.322 & $-$ & -0.267 & 3.143 & 2.628 & 2.122\
10 & $-$ & 0.396 & 0.216 & $-$ & 1.861 & 1.760 & 1.749\
12 & -0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\
15 & -0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\
23 & 0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\
25 & $-$ & $-$ & $-$ & $-$ & -0.257 & $-$ & $-$\
29 & -0.100 & $-$ & $-$ & $-$ & & $-$ & $-$\
32 & -0.050 & $-$ & $-$ & $-$ & & $-$ & $-$\
37 & $-$ & $-$ & -0.235 & $-$ & 0.766 & 1.452 & 0.365\
38 & -0.150 & $-$ & -0.307 & $-$ & 1.604 & 1.679 & 0.404\
43 & 0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\
46 & $-$ & -0.300 & $-$ & $-$ & 0.257 & $-$ & $-$\
48 & -0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\
55 & -0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\
59 & 0.050 & -0.300 & $-$ & $-$ & -1.282 & -1.285 & $-$\
64 & -0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\
65 & $-$ & 0.306 & 0.300 & $-$ & 0.916 & 1.261 & $-$\
68 & -0.150 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\
74 & -0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\
77 & -0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\
84 & 0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\
85 & $-$ & -0.472 & $-$ & 0.237 & -6.365 & -2.636 & -2.956\
86 & -0.150 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\
87 & $-$ & -0.300 & $-$ & $-$ & -0.912 & $-$ & $-$\
90 & -1.350 & -1.518 & -2.397 & -2.433 & 1.613 & -1.602 & -3.668\
94 & 0.200 & 0.240 & $-$ & $-$ & -1.553 & -1.532 & -0.362\
:::

# Discussion

The package [SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost)
provides a novel method for handling high-dimensional data subject to
measurement error in covariates. It covers widely used GLM and AFT
models in survival analysis, and provides a strategy to deal with
variable selection and measurement error correction simultaneously.
Moreover, our package is able to handle the collinearity in the
covariates. As evidence by longer computational times in numerical
studies, the function SIMEXBoost seems to be more computationally
demanding compared to the naive implementation, which is basically
caused by settings of $B$ and $\mathcal{Z}$ for correction of
measurement error. Typically, following the similar idea of the Monte
Carlo simulation, larger values of $B$ and $\mathcal{Z}$ usually give
the stable estimator but also incur longer computational time. This is a
common phenomenon in measurement error analysis and the SIMEX method
(e.g., [@Yi:2017]). In summary, users need to consider whether to take
measurement error effects into account based on their data and set
arguments, such as $B$ and $\mathcal{Z}$, for the implementation based
on their computational resources.

The current state of the package allows for measurement error in
continuous covariates and parametric models for exponential family
distributed responses or time-to-event outcomes. There are still many
possible extensions to the methods, such as consideration of measurement
error in binary covariates or measurement error in the response,
variable selection for semi-parametric regression models, including the
Cox model in survival analysis or the partially linear single index
model.

# R Software {#r-software .unnumbered}

The R package
[SIMEXBoost](https://CRAN.R-project.org/package=SIMEXBoost) is now
available on the CRAN website
(<https://cran.r-project.org/web/packages/SIMEXBoost/index.html>).

# Acknowledgments {#acknowledgments .unnumbered}

The authors would like to thank the editorial team for useful comments
to improve the initial manuscript. Chen's research was supported by
National Science and Technology Council with grant ID
110-2118-M-004-006-MY2.
