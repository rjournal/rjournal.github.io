@ARTICLE{Adadi,
  author={Adadi, Amina and Berrada, Mohammed},
  journal={IEEE Access}, 
  title={Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence ({XAI})}, 
  year={2018},
  volume={6},
  number={},
  pages={52138-52160},
  keywords={Conferences;Machine learning;Market research;Prediction algorithms;Machine learning algorithms;Biological system modeling;Explainable artificial intelligence;interpretable machine learning;black-box models},
  doi_removed={10.1109/ACCESS.2018.2870052},
  url={https://doi.org/10.1109/access.2018.2870052 }}



@article{wachter_counterfactuals,
  year = {2018},
  edition = {},
  number = {2},
  journal = {Harvard Journal of Law and Technology},
  pages = {841-887},
  publisher = {Harvard Journal of Law and Technology},
  school = {},
  title = {Counterfactual explanations without opening the black box: Automated decisions and the {GDPR}},
  volume = {31},
  author = {Wachter, S and Mittelstadt, BDM and Russell, C},
  editor = {},
  series = {}
}


@Manual{marginaleffects,
    title = {marginaleffects: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis
Tests},
    author = {Vincent Arel-Bundock},
    year = {2023},
    note = {R package version 0.11.1},
    url = {https://CRAN.R-project.org/package=marginaleffects},
}

@Article{bartus_marginal_effects,
    author  = {Tamás Bartus},
    title   = {Estimation of marginal effects using margeff},
    journal = {The Stata Journal},
    year    = {2005},
    volume  = {5},
    number  = {3},
    pages   = {309 - 329},
}

@Manual{ggparty,
    title = {ggparty: 'ggplot' Visualizations for the 'partykit' Package},
    author = {Martin Borkovec and Niyaz Madin},
    year = {2019},
    note = {R package version 1.0.0},
    url = {https://CRAN.R-project.org/package=ggparty},
}

@Manual{r6,
    title = {R6: Encapsulated Classes with Reference Semantics},
    author = {Winston Chang},
    year = {2021},
    note = {R package version 2.5.1},
    url = {https://CRAN.R-project.org/package=R6}
}


@misc{misc_bike_sharing_dataset_275,
  author       = {Fanaee-T,Hadi},
  title        = {{Bike Sharing Dataset}},
  year         = {2013},
  howpublished = {UCI Machine Learning Repository},
  url         = {https://doi.org/10.24432/C5W894}
}

@Article{molnar_imlpackage,
    author = {Christoph Molnar and Bernd Bischl and Giuseppe
      Casalicchio},
    title = {iml: An {R} package for Interpretable Machine Learning},
    url = {https://doi.org/10.21105/joss.00786},
    year = {2018},
    publisher = {Journal of Open Source Software},
    volume = {3},
    number = {26},
    pages = {786},
    journal = {JOSS},
}


@Article{friedman_pdp,
  Title                    = {Greedy function approximation: A gradient boosting machine.},
  Author                   = {Friedman, Jerome H.},
  Journal                  = {Ann. Statist.},
  Year                     = {2001},

  month_removed                    = {10},
  Number                   = {5},
  Pages                    = {1189--1232},
  Volume                   = {29},
  Fjournal                 = {The Annals of Statistics},
  Publisher                = {The Institute of Mathematical Statistics},
  doi_removed              = {10.1214/aos/1013203451},
  url                      = {https://doi.org/10.1214/aos/1013203451}
}


@book{gof_design,
  added-at = {2010-06-05T16:40:25.000+0200},
  asin = {0201633612},
  author = {Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John M.},
  biburl = {https://www.bibsonomy.org/bibtex/27e3f1154ab1fbce54752a46dba7f2217/pnk},
  description_removed = {Amazon.com: Design Patterns: Elements of Reusable Object-Oriented Software (9780201633610): Erich Gamma, Richard Helm, Ralph Johnson, John M. Vlissides: Books},
  dewey = {005.12},
  ean = {9780201633610},
  edition = {1st},
  interhash = {7fe32957be97afaf4ecb38b5490d23b4},
  intrahash = {7e3f1154ab1fbce54752a46dba7f2217},
  isbn_removed = {0201633612},
  keywords = {DBIS Design Object-Oriented Patterns SS2010 Seminar Software},
  publisher = {Addison-Wesley Professional},
  timestamp = {2010-06-05T16:40:25.000+0200},
  title = {Design Patterns: Elements of Reusable Object-Oriented Software},
  url_removed = {http://www.amazon.com/Design-Patterns-Elements-Reusable-Object-Oriented/dp/0201633612/ref=ntt_at_ep_dpi_1},
  year = 1994
}


@article{goldstein_ice,
    author = {Alex Goldstein and Adam Kapelner and Justin Bleich and Emil Pitkin},
    title = {Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation},
    journal = {Journal of Computational and Graphical Statistics},
    volume = {24},
    number = {1},
    pages = {44-65},
    year  = {2015},
    publisher = {Taylor & Francis},
    doi_removed = {10.1080/10618600.2014.907095},
    url = {https://doi.org/10.1080/10618600.2014.907095}
}



@article{partykit,
  author  = {Torsten Hothorn and Achim Zeileis},
  title   = {partykit: A Modular Toolkit for Recursive Partytioning in {R}},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {118},
  pages   = {3905--3909},
  url_removed     = {http://jmlr.org/papers/v16/hothorn15a.html}
}

@Manual{tidymodels,
    title = {Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles},
    author = {Max Kuhn and Hadley Wickham},
    url = {https://www.tidymodels.org},
    year = {2020}
}

@Manual{parsnip,
    title = {parsnip: A Common API to Modeling and Analysis Functions},
    author = {Max Kuhn and Davis Vaughan},
    year = {2023},
    note = {R package version 1.1.1},
    url = {https://CRAN.R-project.org/package=parsnip}
}

@article{caret,
    title = {Building predictive models in R using the caret package},
    volume = {28},
    url = {https://doi.org/10.18637/jss.v028.i05},
    doi_removed = {10.18637/jss.v028.i05},
    number = {5},
    journal = {Journal of Statistical Software},
    author = {Max Kuhn},
    year = {2008},
    pages = {1–26}
}


 @Article{mlr3,
    title = {{mlr3}: A modern object-oriented machine learning
      framework in {R}},
    author = {Michel Lang and Martin Binder and Jakob Richter and
      Patrick Schratz and Florian Pfisterer and Stefan Coors and Quay
      Au and Giuseppe Casalicchio and Lars Kotthoff and Bernd Bischl},
    journal = {Journal of Open Source Software},
    year = {2019},
    month_removed = {dec},
    doi_removed = {10.21105/joss.01903},
    url = {https://joss.theoj.org/papers/10.21105/joss.01903},
}


@Manual{leeper_margins,
  Title                    = {margins: Marginal effects for model objects},
  url = {https://CRAN.R-project.org/package=margins},
  Author                   = {Thomas J. Leeper},
  Note                     = {{R} package version 0.3.23},
  Year                     = {2018}
}

@Article{ggeffects,
    title = {ggeffects: Tidy Data Frames of Marginal Effects from
      Regression Models.},
    volume = {3},
    doi_removed = {10.21105/joss.00772},
    url = {https://doi.org/10.21105/joss.00772},
    number = {26},
    journal = {Journal of Open Source Software},
    author = {Daniel Lüdecke},
    year = {2018},
    pages = {772},
}


@article{mize_discrete_change,
    author = {Trenton D. Mize and Long Doan and J. Scott Long},
    title ={A General Framework for Comparing Predictions and Marginal Effects across Models},
    journal = {Sociological Methodology},
    volume = {49},
    number = {1},
    pages = {152-189},
    year = {2019},
    doi_removed = {10.1177/0081175019852763},
    url = {https://doi.org/10.1177/0081175019852763} 
}


@book{molnar_iml,
  title      = {Interpretable Machine Learning},
  author     = {Christoph Molnar},
  year       = {2022},
  subtitle   = {A Guide for Making Black Box Models Explainable},
  edition    = {2nd},
  url        = {https://christophm.github.io/interpretable-ml-book}
}


@incollection{scholbeck_framework,
    author="Scholbeck, Christian A.
    and Molnar, Christoph
    and Heumann, Christian
    and Bischl, Bernd
    and Casalicchio, Giuseppe",
    editor="Cellier, Peggy
    and Driessens, Kurt",
    title="Sampling, Intervention, Prediction, Aggregation: A Generalized Framework for Model-Agnostic Interpretations",
    booktitle="Machine Learning and Knowledge Discovery in Databases",
    booksubtitle="International Workshops of ECML PKDD 2019, Würzburg, Germany, September 16–20, 2019, Proceedings, Part I\@",
    year="2020",
    series="Communications in Computer and Information Science",
    volume="1167",
    publisher="Springer International Publishing",
    address="Cham",
    pages="205--216",
    isbn_removed="978-3-030-43823-4",
    url="https://doi.org/10.1007/978-3-030-43823-4_18"
}

@Article{scholbeck_fme,
author={Scholbeck, Christian A.
and Casalicchio, Giuseppe
and Molnar, Christoph
and Bischl, Bernd
and Heumann, Christian},
title={Marginal effects for non-linear prediction functions},
journal={Data Mining and Knowledge Discovery},
year={2024},
month_removed={Sep},
day={01},
volume={38},
number={5},
pages={2997-3042},
abstract={Beta coefficients for linear regression models represent the ideal form of an interpretable feature effect. However, for non-linear models such as generalized linear models, the estimated coefficients cannot be interpreted as a direct feature effect on the predicted outcome. Hence, marginal effects are typically used as approximations for feature effects, either as derivatives of the prediction function or forward differences in prediction due to changes in feature values. While marginal effects are commonly used in many scientific fields, they have not yet been adopted as a general model-agnostic interpretation method for machine learning models. This may stem from the ambiguity surrounding marginal effects and their inability to deal with the non-linearities found in black box models. We introduce a unified definition of forward marginal effects (FMEs) that includes univariate and multivariate, as well as continuous, categorical, and mixed-type features. To account for the non-linearity of prediction functions, we introduce a non-linearity measure for FMEs. Furthermore, we argue against summarizing feature effects of a non-linear prediction function in a single metric such as the average marginal effect. Instead, we propose to average homogeneous FMEs within population subgroups, which serve as conditional feature effect estimates.},
issn_removed={1573-756X},
doi_removed={10.1007/s10618-023-00993-x},
url={https://doi.org/10.1007/s10618-023-00993-x}
}




@manual{stata_manual,
  title        = {Stata: Release 18},
  author       = {StataCorp},
  address      = {College  Station,  TX:  StataCorp  LLC.},
  year         = 2023
}


@Manual{rpart,
    title = {rpart: Recursive Partitioning and Regression Trees},
    author = {Terry Therneau and Beth Atkinson},
    year = {2019},
    note = {R package version 4.1-15},
    url = {https://CRAN.R-project.org/package=rpart},
}

  
@article{wright_ranger,
   title={ranger: A Fast Implementation of Random Forests for High Dimensional Data in {C++} and {R}},
   volume={77},
   issn_removed={1548-7660},
   url={http://dx.doi.org/10.18637/jss.v077.i01},
   doi_removed ={10.18637/jss.v077.i01},
   number={1},
   journal={Journal of Statistical Software},
   publisher={Foundation for Open Access Statistic},
   author={Wright, Marvin N. and Ziegler, Andreas},
   year={2017}
}

@article{williams_margins,
	author = "Williams, R.",
	title = "Using the margins command to estimate and interpret adjusted predictions and marginal effects",
	journal = "Stata Journal",
	publisher = "Stata Press",
	address = "College Station, TX",
	volume = "12",
	number = "2",
	year = "2012",
	pages = "308-331(24)",
	url_removed = "http://www.stata-journal.com/article.html?article=st0260"
}

@misc{scholbeck_bridgingthegap,
      title={Position Paper: Bridging the Gap Between Machine Learning and Sensitivity Analysis}, 
      author={Christian A. Scholbeck and Julia Moosbauer and Giuseppe Casalicchio and Hoshin Gupta and Bernd Bischl and Christian Heumann},
      year={2023},
      howpublished={arXiv},
      url={https://doi.org/10.48550/arXiv.2312.13234},
      eprint={2312.13234},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@incollection{kamath_xai_book,
    author="Kamath, Uday
    and Liu, John",
    title="Introduction to Interpretability and Explainability",
    bookTitle="Explainable Artificial Intelligence: An Introduction to Interpretable Machine Learning",
    year="2021",
    publisher="Springer International Publishing",
    address="Cham",
    pages="1--26",
    abstract="In recent years, we have seen gains in adoption of machine learning and artificial intelligence applications. However, continued adoption is being constrained by several limitations. The field of Explainable AI addresses one of the largest shortcomings of machine learning and deep learning algorithms today: the interpretability and explainability of models. As algorithms become more powerful and are better able to predict with better accuracy, it becomes increasingly important to understand how and why a prediction is made. Without interpretability and explainability, it would be difficult for us to trust the predictions of real-life applications of AI. Human-understandable explanations will encourage trust and continued adoption of machine learning systems as well as increasing system safety. As an emerging field, explainable AI will be vital for researchers and practitioners in the coming years.",
    isbn_removed="978-3-030-83356-5",
    doi_removed="10.1007/978-3-030-83356-5_1",
    url="https://doi.org/10.1007/978-3-030-83356-5_1"
}


@Article{boulesteix_ml_medicine,
    author={Boulesteix, Anne-Laure
    and Wright, Marvin N.
    and Hoffmann, Sabine
    and K{\"o}nig, Inke R.},
    title={Statistical Learning Approaches in the Genetic Epidemiology of Complex Diseases},
    journal={Human Genetics},
    year={2020},
    month_removed={01},
    day={01},
    volume={139},
    number={1},
    pages={73-84},
    abstract={In this paper, we give an overview of methodological issues related to the use of statistical learning approaches when analyzing high-dimensional genetic data. The focus is set on regression models and machine learning algorithms taking genetic variables as input and returning a classification or a prediction for the target variable of interest; for example, the present or future disease status, or the future course of a disease. After briefly explaining the basic motivation and principle of these methods, we review different procedures that can be used to evaluate the accuracy of the obtained models and discuss common flaws that may lead to over-optimistic conclusions with respect to their prediction performance and usefulness.},
    issn_removed={1432-1203},
    doi_removed={10.1007/s00439-019-01996-9},
    url={https://doi.org/10.1007/s00439-019-01996-9}
}

@article{rajkomar_ml_medicine,
    author = {Rajkomar, Alvin and Dean, Jeffrey and Kohane, Isaac},
    title = {Machine Learning in Medicine},
    journal = {New England Journal of Medicine},
    volume = {380},
    number = {14},
    pages = {1347-1358},
    year = {2019},
    doi_removed = {10.1056/NEJMra1814259},
    note_removed = {PMID: 30943338},
    url = {https://doi.org/10.1056/NEJMra1814259},
    eprint_removed = {https://www.nejm.org/doi/pdf/10.1056/NEJMra1814259}  
}

@Article{dueben_climate_ml,
    AUTHOR = {Dueben, P. D. and Bauer, P.},
    TITLE = {Challenges and Design Choices for Global Weather and Climate Models Based on Machine Learning},
    JOURNAL = {Geoscientific Model Development},
    VOLUME = {11},
    YEAR = {2018},
    NUMBER = {10},
    PAGES = {3999--4009},
    url = {https://doi.org/10.5194/gmd-11-3999-2018},
    doi_removed = {10.5194/gmd-11-3999-2018}

}




@article{dwyer_psychology_ml,
    author = {Dwyer, Dominic B. and Falkai, Peter and Koutsouleris, Nikolaos},
    title = {Machine Learning Approaches for Clinical Psychology and Psychiatry},
    journal = {Annual Review of Clinical Psychology},
    volume = {14},
    number = {1},
    pages = {91-118},
    year = {2018},
    doi_removed = {10.1146/annurev-clinpsy-032816-045037},
    note_removed ={PMID: 29401044},
    url = { 
              https://doi.org/10.1146/annurev-clinpsy-032816-045037    },
    eprint_removed = { 
            https://doi.org/10.1146/annurev-clinpsy-032816-045037
    }
    ,
        abstract = { Machine learning approaches for clinical psychology and psychiatry explicitly focus on learning statistical functions from multidimensional data sets to make generalizable predictions about individuals. The goal of this review is to provide an accessible understanding of why this approach is important for future practice given its potential to augment decisions associated with the diagnosis, prognosis, and treatment of people suffering from mental illness using clinical and biological data. To this end, the limitations of current statistical paradigms in mental health research are critiqued, and an introduction is provided to critical machine learning methods used in clinical studies. A selective literature review is then presented aiming to reinforce the usefulness of machine learning methods and provide evidence of their potential. In the context of promising initial results, the current limitations of machine learning approaches are addressed, and considerations for future clinical translation are outlined. }
}

@article{mullainathan_econometrics_ml,
    Author = {Mullainathan, Sendhil and Spiess, Jann},
    Title = {Machine Learning: An Applied Econometric Approach},
    Journal = {Journal of Economic Perspectives},
    Volume = {31},
    Number = {2},
    Year = {2017},
    month_removed = {05},
    Pages = {87-106},
    DOI_removed = {10.1257/jep.31.2.87},
    url = {https://doi.org/10.1257/jep.31.2.87}}




@article{athey_economics_ml,
    author = {Athey, Susan and Imbens, Guido W.},
    title = {Machine Learning Methods That Economists Should Know About},
    journal = {Annual Review of Economics},
    volume = {11},
    number = {1},
    pages = {685-725},
    year = {2019},
    doi_removed = {10.1146/annurev-economics-080217-053433},
    
    url = { 
        
            https://doi.org/10.1146/annurev-economics-080217-053433
        
        
    
    },
    eprint_removed = { 
        
            https://doi.org/10.1146/annurev-economics-080217-053433
        
        
    
    }
    ,
        abstract = { We discuss the relevance of the recent machine learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods, and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the ML literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, and matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, including causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models. }
}


@article{breiman_two_cultures,
    author = {Leo Breiman},
    title = {Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)},
    volume = {16},
    journal = {Statistical Science},
    number = {3},
    publisher = {Institute of Mathematical Statistics},
    pages = {199 -- 231},
    year = {2001},
    doi_removed = {10.1214/ss/1009213726},
    url = {https://doi.org/10.1214/ss/1009213726}
}


@book{greene_econometric_analysis,
    author   = {Greene, William},
    title    = {Econometric Analysis},
    abstract = {{For first-year graduate courses in Econometrics for Social Scientists.
    Bridging the gap between social science studies and econometric analysis   Designed to bridge the gap between social science studies and field-econometrics, Econometric Analysis, 8th Edition, Global Edition, presents this ever-growing area at an accessible graduate level. The book first introduces students to basic techniques, a rich variety of models, and underlying theory that is easy to put into practice. It then presents students with a sufficient theoretical background to understand advanced techniques and to recognise new variants of established models. This focus, along with hundreds of worked numerical examples, ensures that students can apply the theory to real-world application and are prepared to be successful economists in the field.
    }},
    edition = {8th},
    publisher = {Pearson International},
    year     = {2019},
    isbn_removed     = {9781292231136},
    doi      = {},
    url_removed      = {https://elibrary.pearson.de/book/99.150005/9781292231150}
}


@article{mccabe_me_psychology,
    author = {Connor J. McCabe and Max A. Halvorson and Kevin M. King and Xiaolin Cao and Dale S. Kim},
    title = {Interpreting Interaction Effects in Generalized Linear Models of Nonlinear Probabilities and Counts},
    journal = {Multivariate Behavioral Research},
    volume = {57},
    number = {2-3},
    pages = {243-263},
    year = {2022},
    publisher = {Routledge},
    doi_removed = {10.1080/00273171.2020.1868966},
    note_removed ={PMID: 33523708},
    url = {https://doi.org/10.1080/00273171.2020.1868966},
    eprint_removed = {https://doi.org/10.1080/00273171.2020.1868966}
}

@Article{onukwugha_me_primer,
    author={Onukwugha, Eberechukwu
    and Bergtold, Jason
    and Jain, Rahul},
    title={A Primer on Marginal Effects---Part {I}: Theory and Formulae},
    journal={PharmacoEconomics},
    year={2015},
    month_removed={01},
    day={01},
    volume={33},
    number={1},
    pages={25-30},
    abstract={Marginal analysis evaluates changes in an objective function associated with a unit change in a relevant variable. The primary statistic of marginal analysis is the marginal effect (ME). The ME facilitates the examination of outcomes for defined patient profiles while measuring the change in original units (e.g., costs, probabilities). The ME has a long history in economics; however, it is not widely used in health services research despite its flexibility and ability to provide unique insights. This paper, the first in a two-part series, introduces and illustrates the calculation of the ME for a variety of regression models often used in health services research. Part One includes a review of prior studies discussing MEs, followed by derivation of ME formulas for various regression models including linear, logistic, multinomial logit model (MLM), generalized linear model (GLM) for continuous data, GLM for count data, two-part model, sample selection (two-stage) model, and parametric survival model. Prior theoretical papers in health services research reported the derivation and interpretation of ME primarily for the linear and logistic models, with less emphasis on count models, survival models, MLM, two-part models, and sample selection models. These additional models are relevant for health services research studies examining costs and utilization. Part Two of the series will focus on the methods for estimating and interpreting the ME in applied research. The illustration, discussion, and application of ME in this two-part series support the conduct of future studies applying the marginal concept.},
    issn_removed={1179-2027},
    doi_removed={10.1007/s40273-014-0210-6},
    url={https://doi.org/10.1007/s40273-014-0210-6}
}


@article{apley_ale,
    author = {Apley, Daniel W. and Zhu, Jingyu},
    title = "Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models",
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {82},
    number = {4},
    pages = {1059-1086},
    year = {2020},
    month_removed = {06},
    abstract = "{In many supervised learning applications, understanding and visualizing the effects of the predictor variables on the predicted response is of paramount importance. A shortcoming of black box supervised learning models (e.g. complex trees, neural networks, boosted trees, random forests, nearest neighbours, local kernel-weighted methods and support vector regression) in this regard is their lack of interpretability or transparency. Partial dependence plots, which are the most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to partial dependence plots, we present a new visualization approach that we term accumulated local effects plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, accumulated local effects plots are far less computationally expensive than partial dependence plots. We also provide an R package ALEPlot as supplementary material to implement our proposed method.}",
    issn_removed = {1369-7412},
    doi_removed = {10.1111/rssb.12377},
    url = {https://doi.org/10.1111/rssb.12377},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/82/4/1059/49323845/jrsssb\_82\_4\_1059.pdf},
}

@incollection{casalicchio_featureimportance,
    author="Casalicchio, Giuseppe
    and Molnar, Christoph
    and Bischl, Bernd",
    editor="Berlingerio, Michele
    and Bonchi, Francesco
    and G{\"a}rtner, Thomas
    and Hurley, Neil
    and Ifrim, Georgiana",
    title="Visualizing the Feature Importance for Black Box Models",
    booktitle="Machine Learning and Knowledge Discovery in Databases",
    year="2019",
    publisher="Springer International Publishing",
    address="Cham",
    pages="655--670",
    abstract="In recent years, a large amount of model-agnostic methods to improve the transparency, trustability, and interpretability of machine learning models have been developed. Based on a recent method for model-agnostic global feature importance, we introduce a local feature importance measure for individual observations and propose two visual tools: partial importance (PI) and individual conditional importance (ICI) plots which visualize how changes in a feature affect the model performance on average, as well as for individual observations. Our proposed methods are related to partial dependence (PD) and individual conditional expectation (ICE) plots, but visualize the expected (conditional) feature importance instead of the expected (conditional) prediction. Furthermore, we show that averaging ICI curves across observations yields a PI curve, and integrating the PI curve with respect to the distribution of the considered feature results in the global feature importance. Another contribution of our paper is the Shapley feature importance, which fairly distributes the overall performance of a model among the features according to the marginal contributions and which can be used to compare the feature importance across different models. Code related to this paper is available at: https://github.com/giuseppec/featureImportance.",
    isbn_removed="978-3-030-10925-7",
    url="https://doi.org/10.1007/978-3-030-10925-7_40"
}

@article{strumbelj_shapley,
    author  = {Erik {\v{S}}trumbelj and Igor Kononenko},
    title   = {An Efficient Explanation of Individual Classifications Using Game Theory},
    journal = {Journal of Machine Learning Research},
    year    = {2010},
    volume  = {11},
    number  = {1},
    pages   = {1--18},
    url_removed     = {http://jmlr.org/papers/v11/strumbelj10a.html}
}


@inproceedings{ribeiro_lime,
    author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
    title = {"{W}hy Should {I} Trust You?": Explaining the Predictions of Any Classifier},
    year = {2016},
    isbn_removed = {9781450342322},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2939672.2939778},
    doi_removed = {10.1145/2939672.2939778},
    abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
    booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {1135–1144},
    numpages = {10},
    keywords = {explaining machine learning, interpretability, interpretable machine learning, black box classifier},
    location = {San Francisco, California, USA},
    series = {KDD '16}
}

@inproceedings{lundberg_shap,
    author = {Lundberg, Scott M. and Lee, Su-In},
    title = {A Unified Approach to Interpreting Model Predictions},
    year = {2017},
    isbn_removed = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {4768–4777},
    numpages = {10},
    location = {Long Beach, California, USA},
    series = {NIPS'17}
}

@inproceedings{covert_sage,
    author = {Covert, Ian C. and Lundberg, Scott and Lee, Su-In},
    title = {Understanding Global Feature Contributions with Additive Importance Measures},
    year = {2020},
    isbn_removed = {9781713829546},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {Understanding the inner workings of complex machine learning models is a longstanding problem and most recent research has focused on local interpretability. To assess the role of individual input features in a global sense, we explore the perspective of defining feature importance through the predictive power associated with each feature. We introduce two notions of predictive power (model-based and universal) and formalize this approach with a framework of additive importance measures, which unifies numerous methods in the literature. We then propose SAGE, a model-agnostic method that quantifies predictive power while accounting for feature interactions. Our experiments show that SAGE can be calculated efficiently and that it assigns more accurate importance values than other methods.},
    booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
    articleno = {1444},
    numpages = {12},
    location = {Vancouver, BC, Canada},
    series = {NIPS'20}
}


@InProceedings{herbinger_repid,
  title = 	 { REPID: Regional Effect Plots with implicit Interaction Detection },
  author =       {Herbinger, Julia and Bischl, Bernd and Casalicchio, Giuseppe},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {10209--10233},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month_removed = 	 {28--30 Mar},
  publisher =    {PMLR},
  pdf_removed = 	 {https://proceedings.mlr.press/v151/herbinger22a/herbinger22a.pdf},
  url_removed = 	 {https://proceedings.mlr.press/v151/herbinger22a.html},
  abstract = 	 { Machine learning models can automatically learn complex relationships, such as non-linear and interaction effects. Interpretable machine learning methods such as partial dependence plots visualize marginal feature effects but may lead to misleading interpretations when feature interactions are present. Hence, employing additional methods that can detect and measure the strength of interactions is paramount to better understand the inner workings of machine learning models. We demonstrate several drawbacks of existing global interaction detection approaches, characterize them theoretically, and evaluate them empirically. Furthermore, we introduce regional effect plots with implicit interaction detection, a novel framework to detect interactions between a feature of interest and other features. The framework also quantifies the strength of interactions and provides interpretable and distinct regions in which feature effects can be interpreted more reliably, as they are less confounded by interactions. We prove the theoretical eligibility of our method and show its applicability on various simulation and real-world examples. }
}

@misc{britton_vine,
      title={VINE: Visualizing Statistical Interactions in Black Box Models}, 
      author={Matthew Britton},
      year={2019},
      howpublished={arXiv},
      url={https://doi.org/10.48550/arXiv.1904.00561},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{mehrabi_survey_bias,
    author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
    title = {A Survey on Bias and Fairness in Machine Learning},
    year = {2021},
    issue_date = {July 2022},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {54},
    number = {6},
    issn_removed = {0360-0300},
    url = {https://doi.org/10.1145/3457607},
    doi_removed = {10.1145/3457607},
    abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
    journal = {ACM Comput. Surv.},
    month_removed = {jul},
    articleno = {115},
    numpages = {35},
    keywords = {representation learning, natural language processing, machine learning, deep learning, Fairness and bias in artificial intelligence}
}


@inproceedings{molnar_pitfalls,
    author="Molnar, Christoph
    and K{\"o}nig, Gunnar
    and Herbinger, Julia
    and Freiesleben, Timo
    and Dandl, Susanne
    and Scholbeck, Christian A.
    and Casalicchio, Giuseppe
    and Grosse-Wentrup, Moritz
    and Bischl, Bernd",
    editor="Holzinger, Andreas
    and Goebel, Randy
    and Fong, Ruth
    and Moon, Taesup
    and M{\"u}ller, Klaus-Robert
    and Samek, Wojciech",
    title="General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning Models",
    booktitle = "xxAI - Beyond Explainable AI. xxAI 2020. Lecture Notes in Computer Science, vol 13200",
    booktitle_changed = "xxAI - Beyond Explainable AI: International Workshop, Held in Conjunction with ICML 2020, July 18, 2020, Vienna, Austria, Revised and Extended Papers",
    year="2022",
    publisher="Springer",
    address="Cham",
    pages_removed="39--68",
    abstract="An increasing number of model-agnostic interpretation techniques for machine learning (ML) models such as partial dependence plots (PDP), permutation feature importance (PFI) and Shapley values provide insightful model interpretations, but can lead to wrong conclusions if applied incorrectly. We highlight many general pitfalls of ML model interpretation, such as using interpretation techniques in the wrong context, interpreting models that do not generalize well, ignoring feature dependencies, interactions, uncertainty estimates and issues in high-dimensional settings, or making unjustified causal interpretations, and illustrate them with examples. We focus on pitfalls for global methods that describe the average model behavior, but many pitfalls also apply to local methods that explain individual predictions. Our paper addresses ML practitioners by raising awareness of pitfalls and identifying solutions for correct model interpretation, but also addresses ML researchers by discussing open issues for further research.",
    isbn_removed="978-3-031-04083-2",
    doi_removed="10.1007/978-3-031-04083-2_4",
    url="https://doi.org/10.1007/978-3-031-04083-2_4"
}

@inproceedings{hooker_cert,
    author = {Hooker, Giles},
    title = {Diagnosing Extrapolation: Tree-Based Density Estimation},
    year = {2004},
    isbn_removed = {1581138881},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    booktitle = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    pages = {569–574},
    numpages = {6},
    keywords = {modeling methodologies, C4.5, visualization, trees-based models, clustering, diagnostics, CART, density estimation, interpretation, extrapolation},
    location = {Seattle, WA, USA},
    series = {KDD '04}
}

@Article{hooker_generalizedfanova,
    Title                    = {Generalized Functional {ANOVA} Diagnostics for High-Dimensional Functions of Dependent Variables},
    Author                   = {Giles Hooker},
    Journal                  = {Journal of Computational and Graphical Statistics},
    Year                     = {2007},
    Number                   = {3},
    Pages                    = {709-732},
    Volume                   = {16},
    Publisher                = {Taylor \& Francis},
    url                      = {
    https://doi.org/10.1198/106186007X237892}
}

@inproceedings{hooker_fanova,
    author = {Hooker, Giles},
    title = {Discovering Additive Structure in Black Box Functions},
    booktitle = {Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
    series = {KDD '04},
    year = {2004},
    isbn_removed = {1-58113-888-1},
    location = {Seattle, WA, USA},
    pages = {575--580},
    numpages = {6},
    url = {http://doi.acm.org/10.1145/1014052.1014122},
    acmid = {1014122},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {additive models, diagnostics, draphical models, feature selection, functional ANOVA, interpretation, visualization}
}


@Article{hooker_importance,
    author={Hooker, Giles
    and Mentch, Lucas
    and Zhou, Siyu},
    title={Unrestricted permutation forces extrapolation: Variable importance requires at least one more model, or there is no free variable importance},
    journal={Statistics and Computing},
    year={2021},
    month_removed={Oct},
    day={29},
    volume={31},
    number={6},
    pages={82},
    abstract={This paper reviews and advocates against the use of permute-and-predict (PaP) methods for interpreting black box functions. Methods such as the variable importance measures proposed for random forests, partial dependence plots, and individual conditional expectation plots remain popular because they are both model-agnostic and depend only on the pre-trained model output, making them computationally efficient and widely available in software. However, numerous studies have found that these tools can produce diagnostics that are highly misleading, particularly when there is strong dependence among features. The purpose of our work here is to (i) review this growing body of literature, (ii) provide further demonstrations of these drawbacks along with a detailed explanation as to why they occur, and (iii) advocate for alternative measures that involve additional modeling. In particular, we describe how breaking dependencies between features in hold-out data places undue emphasis on sparse regions of the feature space by forcing the original model to extrapolate to regions where there is little to no data. We explore these effects across various model setups and find support for previous claims in the literature that PaP metrics can vastly over-emphasize correlated features in both variable importance measures and partial dependence plots. As an alternative, we discuss and recommend more direct approaches that involve measuring the change in model performance after muting the effects of the features under investigation.},
    issn_removed={1573-1375},
    doiremoved={10.1007/s11222-021-10057-z},
    url={https://doi.org/10.1007/s11222-021-10057-z}
}

@book{tan_data_mining,
    author   = {Tan, Pang-Ning and Karpatne, Anuj and Steinbach, Michael and Kumar, Vipin},
    title    = {{Introduction to Data Mining: Global Edition}},
    abstract = {{Introduction to Data Mining presents fundamental concepts and algorithms for those learning data mining for the first time.
    Each concept is explored thoroughly and supported with numerous examples. The text requires only a modest background in mathematics. Each major topic is organised into two chapters, beginning with basic concepts that provide necessary background for understanding each data mining technique, followed by more advanced concepts and algorithms.
    }},
    pages    = {864},
    publisher = {Pearson},
    year     = {2019},
    isbn_removed_removed     = {9780273769224},
    doi      = {},
    url_removed      = {https://elibrary.pearson.de/book/99.150005/9780273775324}
}

@Article{molnar_cpfi,
author={Molnar, Christoph
and K{\"o}nig, Gunnar
and Bischl, Bernd
and Casalicchio, Giuseppe},
title={Model-agnostic feature importance and effects with dependent features: A conditional subgroup approach},
journal={Data Mining and Knowledge Discovery},
year={2024},
month_removed={Sep},
day={01},
volume={38},
number={5},
pages={2903-2941},
abstract={The interpretation of feature importance in machine learning models is challenging when features are dependent. Permutation feature importance (PFI) ignores such dependencies, which can cause misleading interpretations due to extrapolation. A possible remedy is more advanced conditional PFI approaches that enable the assessment of feature importance conditional on all other features. Due to this shift in perspective and in order to enable correct interpretations, it is beneficial if the conditioning is transparent and comprehensible. In this paper, we propose a new sampling mechanism for the conditional distribution based on permutations in conditional subgroups. As these subgroups are constructed using tree-based methods such as transformation trees, the conditioning becomes inherently interpretable. This not only provides a simple and effective estimator of conditional PFI, but also local PFI estimates within the subgroups. In addition, we apply the conditional subgroups approach to partial dependence plots, a popular method for describing feature effects that can also suffer from extrapolation when features are dependent and interactions are present in the model. In simulations and a real-world application, we demonstrate the advantages of the conditional subgroup approach over existing methods: It allows to compute conditional PFI that is more true to the data than existing proposals and enables a fine-grained interpretation of feature effects and importance within the conditional subgroups.},
issn_removed={1573-756X},
doi_removed={10.1007/s10618-022-00901-9},
url={https://doi.org/10.1007/s10618-022-00901-9}
}

@Article{randomForest_package,
title = {Classification and Regression by randomForest},
author = {Andy Liaw and Matthew Wiener},
journal = {R News},
year = {2002},
volume = {2},
number = {3},
pages = {18-22},
url = {https://CRAN.R-project.org/doc/Rnews/},
}
