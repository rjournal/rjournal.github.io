        ---------- Reviewer ----------
        
        ## Overview ##
        
        I cannot comment a great deal on the overall suitability to the R
        community, as TDA methods and persistent homology are not areas I have
        a great deal of expertise in. The paper discussion and benchmarking
        analysis both seem sensible and well done. In general the methods they
        used are up to date, however the author's approach has enough
        limitations (detailed below) I think it should be revised before
        acceptance.
        
We thank the reviewer for a thorough and clear review. We address each of their
concerns below.
        
        ### Benchmarking measures
        
        For synthetic benchmarking generally the minimum and median are more
        informative measures then the mean or standard deviation. Both the
        mean and standard deviation (used in this paper) are highly positively
        skewed by outliers. For benchmarks generally any noise _must_ be
        positive, e.g. there are no random processes that will cause the code
        to run _faster_ than the minimum. See <URL> for further discussion on
        these points. The mean and standard deviation are poor enough
        indicators I think the figures should be redone, possibly as box plot
        with an explicit minimum or as point clouds with all measurements. I 
        believe it was mentioned there were only 10 iterations per run, so
        showing all points might be feasible.
        
The reviewer is correct, the data would be better visualized with more robust
measures. To correct our previous figures, we have visualized the runtime
data using minimum, median, and maximum. Unfortunately, visualizing each
iteration (even with jitter) resulted in cluttered plots that were challening
to interpret. We thank the reviewer for including a URL that discussed
appropriate measures for our data; our correction conforms with the reviewer's
suggestion and the linked document.
        
        In addition I would suggest the runtime figures (and possibly the memory
        as well) be shown with logarithmic scale. This is because generally you
        want to compare the relative runtime between two points, not the
        absolute runtime. On a logorithmic scale a 2x increase in runtime is the
        same distance if a benchmark changes from 10ms to 20ms and from 100ms to
        200ms, but in an absolute scale the second change looks much larger.
        Using a logorithmic scale would also make the plots more descriptive for
        the smaller datasets.
        
We agree fully with the reviewer's point and corresponding suggestion. As such,
the runtime and memory data have been visualized on a logarithmic scale.
        
        ### Benchmarking Memory
        
        The authors mention issues measuring the memory of the processes,
        because most of the computation occurs outside of R's managed heap.
        They work around this by using a proxy measurement of memory usage,
        however this has the severe limitation that it only applies to one of
        the 3 implementations benchmarked. This limitation is extremely severe,
        and I think must be changed before the paper can be accepted.
        Fortunately there are tools the authors can use to accomplish this.
        `bench::bench_process_memory()` can be used to sample the memory for
        the entire R process (including child processes and memory allocated
        in C++). If the authors run this after their benchmark is run and look
        at the `max` column it will give them the maximum memory usage of the
        process (and its children). Another alternative is running their
        processes with the GNU time command (https://www.gnu.org/software/time/)
        and looking at the maximum resident set size in KB. e.g.
        `time -f '%M' R -f my-benchmark-script.R`. This does basically the same
        thing as `bench::bench_process_memory()`.
        Using the suggested approach would work for all of the benchmarks in
        this paper, and would make it much more informative. Memory usage is a
        very important consideration, particularly for R which is generally
        very memory hungry.
        
We agree with the reviewer's point and have used `bench::bench_process_memory`
to collect memory benchmark data for all point cloud size, dimension, and
shape combinations. The resulting data has been visualized in Figure 7. Text
in the results and discussion sections has been adjusted to reflect the newly
added data.
        Examples of text revisions/additions
        - Results: "In addition to runtime differences...than the other engines"
        - Discussion: "While Vietoris-Rips complexes can...dimension increases."
We thank the reviewer for the improvement and for providing a clear
method to correct our previous limitation.

        I think more discussion should be give to the big O `O()` runtime of the
        implementations. It is mentioned in the Fig. 3 legend that they all
        follow a power law, but at least from the data shown Ripser seems to
        have much lower coefficients, if it is indeed the same big O growth
        rate.

We agree with the reviewer's point of discussing the big O runtime of the
various persistent homology engines. However, instead of focusing on the
coefficients, we added results and discussion pertaining to the exponent of the
power law (which matters more as dimension increases). To address this, we
have added a faceted bar plot figure that illustrates the growth rate of each
engine's calculation function with various point cloud dimensions and shapes.
Additionally, text in the results and discussion sections has been edited/added
to discuss the big O complexity (memory and runtime, last paragraph in dicussion).

        There are a number of benchmarks that could not be calculated due to
        insufficient ram, however the authors are only benchmarking with 16 GB
        of memory. In 2020 16 GB of memory is not a great deal for computational
        work, commodity laptops often have more memory. Amazon Web Services has
        remote machines that have memory limits up to 24 Terabytes!
        (https://aws.amazon.com/ec2/instance-types/high-memory/) and you can
        reserve machines with more modest memory (e.g. 64 GB) machines very cost
        effectively. The paper would be much stronger if machine capacity was
        not a limiting factor. I would strongly urge the authors to look into
        using more powerful infrastructure to run these benchmarks so their
        results were not so limited.

We agree that 16 GB RAM is a benchmarking limitation. We gained access to 32 GB
RAM machines at our academic institution and have collected memory benchmark
data on these. Although increased memory availability would increase the point
cloud sizes we would potentially explore, we stick to 32 GB for 2 reasons.
First, the memory data already illustrates a clear power law trend whose
coefficients are calculated and provided. There is very little "noise" in the
visualized data, which leads us to believe that increased memory would only
confirm the same trend with larger point clouds. Since the memory requirement
grows superlinearly, even doubling the available memory to 64 GB would still
restrict point cloud size to within the same order of magnitude. Second,
although machines with more memory are available cost-effectively, this is
currently a no-cost study. Acquiring and then spending funding would take time
and consume monetary resources. We believe the additions made to the memory
benchmark results provide sufficient evidence for our high-level conclusions
(e.g. relative package efficiency, growth rates for each package) with the
increased RAM used.
        
        ### Usability discussion
        
        I think a (possibly short) section comparing the usability of the two
        packages (TDA and TDAstats) would be useful. Computational efficiency
        should not be the only thing users weigh when deciding what package to
        use, and if there are major differences in the interfaces, ease of use,
        feature set, open bugs or maintainer responsiveness that should be
        discussed. If the authors feel the two packages are the same in terms
        of these aspects that should be mentioned. It might also be worth
        having a short code example of using each package.
        
We agree that computational efficiency should not be the only deciding factor
between packages. Although there are no major differences in the ease of use,
interface, or maintainer responsiveness between the TDA and TDAstats packages,
there is a significant different in the feature set between the two. We have
added the following paragraph in the Discussion section that compares the
functionality of the two packages:
        "On average, Ripser computed...packages are hosted on CRAN."

We refrained from adding code examples as it risks unnecessary redundancy - both
packages are well documented and have associated vignettes and publications with
extensive sample code. We have referenced the packages appropriately and any
interested users can easily find the code via the References section (in
addition to CRAN and GitHub).

        ## Conclusion ##

        Overall I think the authors did an admirable job analyzing the
        benchmarks and in writing the paper. However the benchmarking
        methodology was flawed enough in terms of metrics, display, measurement
        of memory and memory limits that I think it should be revised and
        redone before the paper is accepted.

We thank the reviewer for their kind words and agree that the submitted
manuscript had room for improvement. We have made revisions to address the
reviewer's concerns and hope that the revised manuscript is now worthy of
acceptance.

