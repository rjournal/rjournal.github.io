% !TeX root = RJwrapper.tex
\title{Supplementary materials}
\author{by Halaleh Kamari, Sylvie Huet and Marie-Luce Taupin}

\maketitle


\section{Appendix} \label{app:technical}
\begin{mydef}\label{rem:subdiff}
For $F(x)=\Vert Ax\Vert$, where $A$ is a symmetric matrix that does not depend on $x$, we have: $\partial F(x)=\{{A^2x/\Vert Ax\Vert}\}$ if $x\neq 0$, and $\partial F(x)=\{w\in\mathbb{R}^n,\quad\Vert A^{-1}w\Vert\leq 1\}$ if $x=0$.
\end{mydef}
\begin{mydef}\label{fooc}
Let $F:\mathbb{R}^n\rightarrow \mathbb{R}$ be a convex function. We have the following first order optimality condition:
$$\widehat{x}\in \text{argmin}_{x\in\mathbb{R}^n}F(x)\Leftrightarrow 0\in\partial F(\widehat{x}).$$
This results from the fact that $F(y)\geq F(\widehat{x})+ <0,y-\widehat{x}> $ for all $y\in\mathbb{R}^n$ in both cases \citep{giraud2014introduction}.
\end{mydef}
\subsection{RKHS construction}\label{subsec:Data}
We begin this Section with a brief introduction to the RKHS.
Let $\mathcal{H}$ be a Hilbert space of real valued functions on a set $\mathcal{X}$. The space $\mathcal{H}$ is a RKHS if for all $X\in\mathcal{X}$ the evaluation functionals $L_X:f\in\mathcal{H}\rightarrow f(X)\in\mathbb{R}$ are continuous. The Riesz representation Theorem ensures the existence of an unique element $k_X(.)$ in $\mathcal{H}$ verifying the property that $
\forall X\in\mathcal{X},\:\forall f\in\mathcal{H},\: f(X)=L_X(f)=\langle f,k_X\rangle_{\mathcal{H}},$
where $\langle.,.\rangle_{\mathcal{H}}$ denotes the inner product in $\mathcal{H}$.
It follows that for all $X$ ,$X'$ in $\mathcal{X}$, and $k_X(.)$, $k_{X'}(.)$ in $\mathcal{H}$, we have
$k_X(X')=L_{X'}(k_X)=\langle k_X,k_{X'}\rangle_{\mathcal{H}}.$
This allows to define the reproducing kernel of $\mathcal{H}$ as $
k:\mathcal{X}\times\mathcal{X}((X,X'))\rightarrow \mathbb{R}(k_X(X')).$
The reproducing kernel $k(X,X')$ is positive definite since it is symmetric, and for any $n\in\mathbb{N}$, $\{X_i\}_{i=1}^n\in\mathcal{X}$ and $\{c_i\}_{i=1}^n\in\mathbb{R}$, we have:
\begin{align*}
\sum_{i=1}^n\sum_{j=1}^nc_ic_jk(X_i,X_j)=\sum_{i=1}^n\sum_{j=1}^n\langle c_i k(X_i,.),c_jk(X_j,.)\rangle_{\mathcal{H}}=\Vert \sum_{i=1}^nc_ik(X_i,.)\Vert_{\mathcal{H}}^2\geq 0.
\end{align*}
For more background on RKHS, we refer to various standard references such as \citet{aronszajn50reproducing}, \citet{saitoh1988theory}, and \citet{Berlinet2004ReproducingKH}.

In this work, the idea is to construct a RKHS $\mathcal{H}$ such that any function $f$ in $\mathcal{H}$ is decomposed as its Hoeffding decomposition, and therefore, any function $f$ in $\mathcal{H}$ is a candidate to approximate the Hoeffding decomposition of $m$.  
To do so, the method of \citet{DURRANDE201357} as described below is used:\\ 
Let $\mathcal{X} = \mathcal{X}_{1} \times \ldots \times \mathcal{X}_{d}$ be a subset
of $\mathbb{R}^{d}$. For each $a\in \{1,\cdots,d\}$, we choose a RKHS  $\mathcal{H}_{a}$ and its associated kernel $k_{a}$ defined on the set $\mathcal{X}_{a} \subset \mathbb{R}$ such that the two following properties are satisfied:
\begin{itemize}
\item[(i)]\label{intro:idurr} $k_{a}:\mathcal{X}_{a} \times \mathcal{X}_{a} \rightarrow \mathbb{R}$ is $P_{a}\otimes P_{a}$ measurable,
\item[(ii)]\label{intro:iidurr} $E_{X_{a}}\sqrt{k_{a}(X_{a}, X_{a})} < \infty$.
\end{itemize}
The property \hyperref[intro:iidurr]{(ii)} depends on the kernel $k_a$, $a=1,...,d$ and the distribution of $X_a$, $a=1,...,d$. It is not very restrictive since it is satisfied, for example, for any bounded kernel.

The RKHS $\mathcal{H}_{a}$ can be decomposed as a sum of two orthogonal
sub-RKHSs, $\mathcal{H}_{a} = \mathcal{H}_{0 a}\stackrel{\perp}{\oplus} \mathcal{H}_{1 a},$
where $\mathcal{H}_{0 a}$ is the RKHS of zero mean functions, $\mathcal{H}_{0 a} = \{ f_{a} \in \mathcal{H}_{a}:\: E_{X_{a}}(f_{a}(X_{a})) =
   0\},$
and $\mathcal{H}_{1 a}$ is the RKHS of constant functions, $\mathcal{H}_{1 a} = \{  f_{a} \in \mathcal{H}_{a}:\: f_{a}(X_{a}) =C \}.$
The kernel $k_{0a}$ associated with the RKHS $\mathcal{H}_{0 a}$ is defined
by:
\begin{equation}
\label{kernelmodify}
k_{0a} (X_{a},X'_{a}) = k_{a}(X_{a},X'_{a}) - 
\frac{E_{U \sim P_{a}}(k_{a}(X_{a},U))E_{U \sim P_{a}}(k_{a}(X'_{a},U))}
{E_{(U,V)\sim P_{a}\otimes P_{a}}k_{a}(U,V)}.
\end{equation} 
Let $k_{v}(X_{v}, X'_{v}) = \prod_{a \in v} k_{0a} (X_{a},X'_{a}),$ then the  ANOVA kernel $k(.,.)$ is defined as follows:
\begin{equation*}
 k(X, X') = \prod_{a=1}^{d} 
\Big(1+k_{0a}(X_{a}, X'_{a})\Big) = 
1 + \sum_{v \in \mathcal{P}} k_{v}(X_{v}, X'_{v}).
\end{equation*}
For $\mathcal{H}_{v}$ being the RKHS associated with the kernel $k_{v}$, the 
RKHS associated with the ANOVA kernel is then defined by $\mathcal{H} = \prod_{a=1}^{d}( \mathbbm{1} \stackrel{\perp}{\oplus}
   \mathcal{H}_{0a}) = \mathbbm{1} + \sum_{v \in \mathcal{P}} \mathcal{H}_{v},$
where $\perp$ denotes the $L^2$ inner product.
According to this construction, any function $f \in \mathcal{H}$ satisfies $f(X)=\langle f,k(X,.)\rangle_\mathcal{H}=f_0+\sum_{v\in\mathcal{P}}f_v(X_v),$
which is the Hoeffding decomposition of $f$. 
 
The regularity properties of the RKHS $\mathcal{H}$ constructed as described above, depend on the set of the kernels ($k_{a}$, $a=1,...,d$). This method allows us to choose different approximation spaces independently of the distribution of the input variables $X_1,...,X_d$, by choosing different sets of kernels. While, in the meta-modelling approach based on the polynomial Chaos expansion, according to the distribution of the input variables $X_1,...,X_d$, a unique family of orthonormal polynomials $\{\phi_j\}_{j=0}^\infty$ is determined.
Here, the distribution of the components of $X$ occurs only for the orthogonalization of the spaces $\mathcal{H}_v$, $v\in\mathcal{P}$, and not in the choice of the RKHS, under the condition that properties \hyperref[intro:idurr]{(i)} and \hyperref[intro:iidurr]{(ii)} are satisfied. This is one of the main advantages of this method compared to the method based on the truncated polynomial Chaos expansion where the smoothness of the approximation is handled only by the choice of the truncation \citep{BlatmanSudret}. 
\subsection{RKHS group lasso algorithm}\label{appendix:gls}
We consider the minimization of the RKHS group lasso criterion given by,
$$C_g(f_0,\theta)=\Vert Y-f_0I_n-\sum_{v\in\mathcal{P}}K_v\theta_v\Vert^2+\sqrt{n}\mu_g\sum_{v\in\mathcal{P}}\Vert K_v^{1/2}\theta_v\Vert .$$ 
We begin with the constant term $f_0$. The ordinary first derivative of the function $C_g(f_0,\theta)$ at $f_0$ is equal to:
\begin{equation*}
\frac{\partial C_g}{\partial f_0}=-2\sum_{i=1}^n(Y-f_0I_n-\sum_{v\in\mathcal{P}}K_v\theta_v),
\end{equation*}
and therefore, 
\begin{align}\label{fzchapo}
\widehat{f_0}=\frac{1}{n}\sum_{i=1}^n{Y_i}-\frac{1}{n}\sum_i\sum_v(K_v\theta_v)_i,
\end{align}
where $(K_v\theta_v)_i$ denotes the i-th component of $K_v\theta_v$. 

The next step is to calculate $\widehat{\theta}=\text{argmin}_{\theta\in\mathbb{R}^{n\times\vert \mathcal{P}\vert}}C_g(f_0,\theta).$ 
Since $C_g(f_0,\theta)$ is convex and separable, we use a block coordinate descent algorithm, group $v$ by group $v$. In the following, we fix a group $v$, and we find the minimizer of $C_g(f_0,\theta)$ with respect to $\theta_v$ for the given values of $f_0$ and $\theta_w$, $w\neq v$. Set
\begin{align*}
C_{g,v}(f_0,\theta_v)=\Vert R_v-K_v\theta_v\Vert^2+\sqrt{n}\mu_g\Vert K_v^{1/2}\theta_v\Vert,
\end{align*}
where 
\begin{equation}
\label{residul}
R_v = Y-f_0-\sum_{w\neq v}K_w\theta_w.
\end{equation}
We aim to minimize $C_{g,v}(f_0,\theta_v)$ with respect to $\theta_v$. Let $\partial C_{g,v}$ be the sub-differential of $C_{g,v}(f_0,\theta_v)$ with respect to $\theta_v$:
\begin{equation*}
\partial C_{g,v}(f_0,\theta)=\{-2K_v(R_v-K_v\theta_v)+\sqrt{n}\mu_gt_v\: :\: t_v\in\partial\Vert K_v^{1/2}\theta_v\Vert\}.
\end{equation*}
The first order optimality condition (see Preliminary \eref{fooc}) ensures the existence of $\widehat{t}_v\in\partial\Vert K_v^{1/2}\theta_v\Vert$ fulfilling, 
\begin{equation}
\label{fop}
-2K_v(R_v-K_v\theta_v)+\sqrt{n}\mu_g\widehat{t}_v=0.
\end{equation}
Using the sub-differential definition (see Preliminary \ref{rem:subdiff}), we obtain: 
\begin{align*}
\partial\Vert K_v^{1/2}\theta_v\Vert= \{\frac{K_v\theta_v}{\Vert K_v^{1/2}\theta_v\Vert}\}\quad \text{if}\quad \theta_v\neq 0,
\end{align*}
and,
\begin{align*}
 \partial\Vert K_v^{1/2}\theta_v\Vert=\{\widehat{t}_v\in\mathbb{R}^n,\:\Vert K_v^{-1/2}\widehat{t}_v\Vert\leq 1\}\quad\text{if}\quad \theta_v=0.
\end{align*}
Let $\widehat{\theta}_v$ be the minimizer of $C_{g,v}$. The sub-differential equations above give the two following cases:\\ 
Case 1. If $\widehat{\theta}_v=0$, then there exists $\widehat{t}_v\in\mathbb{R}^n$ such that $\Vert K_v^{-1/2}\widehat{t}_v\Vert\leq 1$ and it fulfils Equation~\eref{fop}, $2K_vR_v=\sqrt{n}\mu_g\widehat{t}_v$. 
Therefore, the necessary and sufficient condition for which the solution $\widehat{\theta}_v = 0$ is the optimal one is $2\Vert K_v^{1/2}R_v\Vert/\sqrt{n}\leq\mu_g.$\\
Case 2. If $\widehat{\theta}_v\neq0$, then $\widehat{t}_v=K_v\widehat{\theta}_v/\Vert K_v^{1/2}\widehat{\theta}_v\Vert$ and it fulfils Equation~\eref{fop}, 
\begin{align*}
2K_v(R_v-K_v\widehat{\theta}_v)=\sqrt{n}\mu_g\frac{K_v\widehat{\theta}_v}{\Vert K_v^{1/2}\widehat{\theta}_v\Vert}.
\end{align*}
We obtain then,
\begin{align}
\label{ttnzero}
\widehat{\theta}_v=(K_v+\frac{\sqrt{n}\mu_g}{2\Vert K_v^{1/2}\widehat{\theta}_v\Vert}I_n)^{-1}R_v.
\end{align}
Since $\widehat{\theta}_v$ appears in both sides of the Equation \eref{ttnzero}, a numerical procedure is needed:
\begin{prop}\label{theo:ttahat}
For $\rho>0$ let $\theta(\rho)=(K_v+\rho I_n)^{-1}R_v$. There exists a non-zero solution to Equation \eref{ttnzero} if and only if there exists $\rho>0$ such that: 
\begin{equation}
\label{condttahat}
\mu_g=\frac{2\rho}{\sqrt{n}}\Vert K_v^{1/2}\theta(\rho)\Vert. 
\end{equation}
Then $\widehat{\theta}_v=\theta(\rho)$. 
\end{prop}
\subparagraph*{Proof}
If there exists a non-zero solution to Equation \eref{ttnzero}, then $\Vert K_v^{1/2}\widehat{\theta}_v\Vert\neq 0$ since $K_v$ is positive definite. Take $\rho={\sqrt{n}\mu_g/2\Vert K_v^{1/2}\widehat{\theta}_v\Vert},$ then 
$\theta(\rho)=(K_v+\frac{\sqrt{n}\mu_g}{2\Vert K_v^{1/2}\widehat{\theta}_v\Vert}I_n)^{-1}R_v=\widehat{\theta}_v,$
and, for such $\rho$ Equation \eref{condttahat} is satisfied. Conversely, if there exists $\rho>0$ such that Equation \eref{condttahat} is satisfied, then $\Vert K_v^{1/2}\theta(\rho)\Vert\neq 0$ and $\rho=\frac{\sqrt{n}\mu_g}{2\Vert K_v^{1/2}\theta(\rho)\Vert}.$ Therefore, 
$\theta(\rho)=(K_v+\frac{\sqrt{n}\mu_g}{2\Vert K_v^{1/2}\theta(\rho)\Vert}I_n)^{-1}R_v,$
which is Equation \eref{ttnzero} calculated in $\widehat{\theta}_v=\theta(\rho)$.
\hfill $\Box$  

\begin{rem}
Define $y(\rho)=2\rho\Vert K_v^{1/2}\theta(\rho)\Vert -\sqrt{n}\mu_g$ with $\theta(\rho)=(K_v+\rho I_n)^{-1}R_v$, then $y(\rho)=0$ has a unique solution, denoted $\widehat{\rho}$, which leads to calculate $\widehat{\theta}(\widehat{\rho})$.
\end{rem}
\subparagraph*{Proof}
For $\rho=0$ we have $y(0)=-\sqrt{n}\mu_g<0$, since $\mu_g>0$; and for $\rho\rightarrow +\infty$ we have $y(\rho)>0$, since $\Vert K_v^{1/2}(\frac{K_v}{\rho}+I_n)^{-1}R_v\Vert\rightarrow\Vert K_v^{1/2}R_v\Vert$ and $\Vert 2K_v^{1/2}R_v\Vert >\sqrt{n}\mu_g$. 
Moreover, we have: 
\begin{align*}
y(\rho)=2\Vert (\frac{I_n}{\rho}+k_v^{-1})^{-1}k_v^{-1/2}R_v\Vert-\sqrt{n}\mu_g
=2(X^TA^{-2}X)^{1/2}-\sqrt{n}\mu_g,
\end{align*}
where $A=(I_n/\rho+k_v^{-1})$ and $X=k_v^{-1/2}R_v$. 
The first derivative of $y(\rho)$ in $\rho$ is obtained by $\frac{\partial y(\rho)}{\partial\rho}=(X^TA^{-2}X)^{-1/2}\frac{\partial (X^TA^{-2}X)}{\partial\rho}$. Finally, by simple calculations we get,  
\begin{align*}
\frac{\partial y(\rho)}{\partial\rho}=\frac{2\Vert (\frac{I_n}{\rho}+k_v^{-1})^{-3/2}k_v^{-1/2}R_v\Vert}{\rho^2\Vert (\frac{I_n}{\rho}+k_v^{-1})^{-1}k_v^{-1/2}R_v\Vert}>0.
\end{align*}
Therefore $y(\rho)$ is an increasing function of $\rho$, and the proof is complete.
\hfill $\Box$ 

In order to calculate $\rho$ and so $\widehat{\theta}_v=\theta(\rho)$, we use Algorithm \ref{algo:rho} which is a part of the RKHS group lasso algorithm when $\widehat{\theta}_v\neq 0$.
\begin{algorithm}[h!]
\caption{Algorithm to find $\rho$ and $\widehat{\theta}_v$:}\label{algo:rho}
\small{
{\setlength{\tabcolsep}{4pt}
\begin{algorithmic}[1]
\If{$\widehat{\theta}_{\text{old}}=0$} \Comment{$\widehat{\theta}_{\text{old}}$ is $\widehat{\theta}_v$ computed in the previous step of the RKHS group lasso algorithm.}
\State{Set $\rho\gets 1$ and calculate $y(\rho)$}
 \If{$y(\rho)>0$}
 \State{Find $\widehat{\rho}$ that minimizes $y(\rho)$ on the interval $[0,1]$}
 \Else
 \Repeat
  \State{Set $\rho\gets\rho\times 10$ and calculate $y(\rho)$}
 \Until{$y(\rho)>0$} 
 \State{Find $\widehat{\rho}$ that minimizes $y(\rho)$ on the interval $[\rho/10,\rho]$}
 \EndIf 
\Else
\State{Set $\rho\gets \sqrt{n}\mu_g/2\Vert K_v^{1/2}\widehat{\theta}_{\text{old}}\Vert$ and calculate $y(\rho)$}
 \If{$y(\rho)>0$}
 \Repeat
  \State{Set $\rho\gets\rho/10$ and calculate $y(\rho)$}
 \Until{$y(\rho)<0$} 
 \State{Find $\widehat{\rho}$ that minimizes $y(\rho)$ on the interval $[\rho,\rho\times 10]$}
 \Else
 \Repeat
  \State{Set $\rho\gets\rho\times 10$ and calculate $y(\rho)$}
 \Until{$y(\rho)>0$} 
 \State{Find $\widehat{\rho}$ that minimizes $y(\rho)$ on the interval $[\rho/10,\rho]$}
 \EndIf 
\EndIf 
\State{calculate $\widehat{\theta}_v=\theta(\widehat{\rho})$}
\end{algorithmic}}}
\end{algorithm}
\subsection{Computational cost}
The complexity for the matrices $K_v$, $v\in\mathcal{P}$ is equal to $n^3$, which is given by the singular value decomposition to get eigenvalues and eigenvectors of each $K_v$.
Supposing that the matrices $K_v$, $v\in\mathcal{P}$ was first created and are already stored, the complexity for the constant term $f_0$ is given by the second term in equation \eref{fzchapo}, which is equal to $n^2$. Given $\widehat{\rho}$, the complexity for $\widehat{\theta}_v$, $v\in\mathcal{P}$, is given by the backsolving of $(K_v+\widehat{\rho} I_n)\widehat{\theta}_v=R_v$ to get $\widehat{\theta}_v$, which is equal to $n^2$.  The computation of $\widehat{\rho}$ is done using Brent-Dekker method implemented as function \code{gsl\_root\_fsolver\_brent} from \citet{galassi2018scientific}. This method combines an interpolation strategy with the bisection algorithm and takes $O(m)$ iterations to converge, where $m$ is the number of steps that the bisection algorithm would take.   
\subsection{RKHS ridge group sparse algorithm}
We consider the minimization of the RKHS ridge group sparse criterion:
$$C(f_0,\theta)=\Vert Y-f_0I_n-\sum_{v\in\mathcal{P}}K_v\theta_v\Vert^2+\sqrt{n}\gamma\sum_{v\in\mathcal{P}}\Vert K_v\theta_v\Vert+n\mu\sum_{v\in\mathcal{P}}\Vert K_v^{1/2}\theta_v\Vert.$$
The constant term $f_0$ is estimated as in the RKHS group lasso algorithm. In order to calculate $\widehat{\theta}=\text{argmin}_{\theta\in\mathbb{R}^{n\times\vert\mathcal{P}\vert}}C(f_0,\theta)$, we use once again the block coordinate descent algorithm group $v$ by group $v$. In the following, we fix a group $v$, and we find the minimizer of $C(f_0,\theta)$ with respect to $\theta_v$ for given values of $f_0$ and $\theta_w$, $w\neq v$. We aim at minimizing with respect to $\theta_v$,
\begin{align*}
C_v(f_0,\theta_v)=\Vert R_v-K_v\theta_v\Vert^2+\sqrt{n}\gamma\Vert K_v\theta_v\Vert+n\mu\Vert K_v^{1/2}\theta_v\Vert,
\end{align*}
where $R_v$ is defined by \eref{residul}.

Let $\partial C_{v}$ be the sub-differential of $C_{v}(f_0,\theta_v)$ with respect to $\theta_v$,
\begin{equation*}
\partial C_v=\{-2K_v(R_v-K_v\theta_v)+\sqrt{n}\gamma s_v+n\mu t_v\: :\: s_v\in\partial\Vert K_v\theta_v\Vert,\quad t_v\in\partial\Vert K_v^{1/2}\theta_v\Vert\},
\end{equation*} 
According to the first order optimality condition (see Preliminary \ref{fooc}), we know that there exists $\widehat{s}_v\in\partial\Vert K_v\theta_v\Vert$ and $\widehat{t}_v\in\partial\Vert K_v^{1/2}\theta_v\Vert$ such that, 
\begin{equation}
\label{foprgs}
-2K_v(R_v-K_v\theta_v)+\sqrt{n}\gamma\widehat{s}_v+n\mu\widehat{t}_v=0.
\end{equation}
The sub-differential definition (see Preliminary \ref{rem:subdiff}) gives: 
\begin{align*}
\{\partial\Vert K_v^{1/2}\theta_v\Vert= \{\frac{K_v\theta_v}{\Vert K_v^{1/2}\theta_v\Vert}\},\:\partial\Vert K_v\theta_v\Vert= \{\frac{K_v^2\theta_v}{\Vert K_v\theta_v\Vert}\}\}\quad \text{if}\quad \theta_v\neq 0,
\end{align*}
and,
\begin{align*}
 \{\partial\Vert K_v^{1/2}\theta_v\Vert=\{\widehat{t}_v\in\mathbb{R}^n,\Vert K_v^{-1/2}\widehat{t}_v\Vert\leq 1\},\:\partial\Vert K_v\theta_v\Vert=\{\widehat{s}_v\in\mathbb{R}^n,\Vert K_v^{-1} \widehat{s}_v\Vert\leq 1\}\}\quad \text{if}\quad \theta_v=0.
\end{align*}
Let $\widehat{\theta}_v$ be the minimizer of the $C_v(f_0,\theta_v)$.
Using the sub-differential equations above, the estimator $\widehat{\theta}_v$, $v\in\mathcal{P}$ is obtained following the two cases below:\\ 
Case 1. If $\widehat{\theta}_v=0$, then there exists $\widehat{s}_v\in\mathbb{R}^n$ such that $\Vert K_v^{-1}\widehat{s}_v\Vert\leq 1$ and it fulfils Equation \eref{foprgs}, $2K_vR_v-n\mu \widehat{t}_v=\sqrt{n}\gamma \widehat{s}_v,$ 
with $\widehat{t}_v\in\mathbb{R}^n$, $\Vert K_v^{-1/2}\widehat{t}_v\Vert\leq 1$. Set $J(\widehat{t}_v)=\Vert 2R_v-n\mu K_v^{-1}\widehat{t}_v\Vert,$ and, 
$J^*=\text{argmin}_{\widehat{t}_v\in\mathbb{R}^n}\{J(\widehat{t}_v),\text{ such that }\Vert K_v^{-1/2}\widehat{t}_v\Vert\leq 1\}.$ 
Then the solution to Equation \eref{foprgs} is zero if and only if $J^*\leq\gamma$.\\
Case 2. If $\widehat{\theta}_v\neq0$, then we have $\widehat{s}_v=K_v^2\widehat{\theta}_v/\Vert K_v\widehat{\theta}_v\Vert$, and $\widehat{t}_v=K_v\widehat{\theta}_v/\Vert K_v^{1/2}\widehat{\theta}_v\Vert$ fulfilling Equation~\eref{foprgs}, $2K_v(R_v-K_v\widehat{\theta}_v)=\sqrt{n}\gamma\frac{K_v^2\widehat{\theta}_v}{\Vert K_v\widehat{\theta}_v\Vert_2}+n\mu\frac{K_v\widehat{\theta}_v}{\Vert K_v^{1/2}\widehat{\theta}_v\Vert},$ 
that is, 
\begin{align}\label{tthatgrp}
\widehat{\theta}_v=(K_v+\frac{\sqrt{n}\gamma}{2\Vert K_v\widehat{\theta}_v\Vert}K_v+\frac{n\mu}{2\Vert K_v^{1/2}\widehat{\theta}_v\Vert}I_n)^{-1}R_v\quad \text{if}\quad \widehat{\theta}_v\neq0.
\end{align}
In this case the calculation of $\widehat{\theta}_v$ needs a numerical algorithm.
\begin{prop} (Proposition 8.4 in \citet{huet:hal-01434895})\label{theo:ttahatgrp}
For $\rho_1,\rho_2>0$, let $\theta(\rho_1,\rho_2)=(K_v+\rho_1K_v+\rho_2I_n)^{-1}R_v$. If $\mu>0$, there exists a non zero solution to Equation \eref{tthatgrp} if and only if there exists $\rho_1,\rho_2>0$ such that $\gamma=\frac{2\rho_1}{\sqrt{n}}\Vert K_v\theta(\rho_1,\rho_2)\Vert,\:\mbox{and}\:\mu=\frac{2\rho_2}{n}\Vert K^{1/2}_v\theta(\rho_1,\rho_2)\Vert.$ 
Then $\widehat{\theta}_v=\theta(\rho_1,\rho_2)$. 
\end{prop}
\subparagraph*{Proof} The proof is given in \citet{huet:hal-01434895}.
\hfill $\Box$ 

\subsection{Computational cost}
The complexity for the matrices $K_v$, $v\in\mathcal{P}$ and the constant term $f_0$ is the same as for RKHS group lasso algorithm. Given $\widehat{\rho}_1,\widehat{\rho}_2$, the complexity for $\widehat{\theta}_v$, $v\in\mathcal{P}$, is given by the backsolving of $(K_v+\rho_1K_v+\rho_2I_n)\widehat{\theta}_v=R_v$ to get $\widehat{\theta}_v$, which is equal to $n^2$.  The computation of $\widehat{\rho}_1,\widehat{\rho}_2$, is insured using a combination of three methods: first we implement a modified version of Newton's method, if it does not achieve the convergence second we implement a version of the Hybrid algorithm and if it does not achieve the convergence, third we implement a version of the discrete Newton algorithm called Broyden algorithm. These methods are implemented as functions \code{gsl\_multiroot\_fdfsolver\_gnewton}, \code{gsl\_multiroot\_fsolver\_hybrids}, and \code{gsl\_multiroot\_fsolver\_broyden} from \citet{galassi2018scientific}, respectively. These methods converge fast in general, and for $n$ big enough their complexity is dominated by $n^2$.
\section{Overview of the RKHSMetaMod functions}\label{sec:Overview}
In the R environment, one can install and load the \pkg{RKHSMetaMod} package by using the following commands:
\begin{example}
install.packages("RKHSMetaMod")
library("RKHSMetaMod")
\end{example}
The optimization problems in this package are solved using block coordinate descent algorithm which requires various computational algorithms including generalized Newton, Broyden and Hybrid methods. 
In order to gain the efficiency in terms of the calculation time and be able to deal with high dimensional problems, the computationally efficient tools of C++ packages \pkg{Eigen} \citep{eigenweb} and \pkg{GSL} \citep{galassi2018scientific} via \pkg{RcppEigen} \citep{RcppEigen} and \pkg{RcppGSL} \citep{RcppGSL} packages are used in the \pkg{RKHSMetaMod} package. For different examples of usage of \pkg{RcppEigen} and \pkg{RcppGSl} functions see the work by \citet{Eddelbuettel:2013:SRC:2517725}. 

The complete documentation of \pkg{RKHSMetaMod} package is available from CRAN \citep{rkhsmeamodpackage}. Here, a brief documentation of some of its main and companion functions is presented in the next two Sections.   
\subsection{Main RKHSMetaMod functions}\label{subsec:Main}
Let us begin by introducing some notations. For a given Dmax$\in\mathbb{N}$, let $\mathcal{P}_{\text{Dmax}}$ be the set of parts of $\{1,...,d\}$ with dimension $1$ to Dmax. The cardinal of $\mathcal{P}_{\text{Dmax}}$ is denoted by $\text{vMax}=\sum_{j=1}^{\text{Dmax}}\binom{d}{j}.$
\subparagraph{\code{RKHSMetMod} function:} For a given value of Dmax and a chosen kernel, this function calculates the Gram matrices $K_v$, $v\in\mathcal{P}_{\text{Dmax}}$, and produces a sequence of estimators $\widehat{f}$ associated with a given grid of values of tuning parameters $\mu,\gamma$, i.e. the solutions to the RKHS ridge group sparse (if $\gamma\neq0$) or the RKHS group lasso problem (if $\gamma=0$).
Table \ref{metmod} gives a summary of all the input arguments of the \code{RKHSMetMod} function as well as the default values for non-mandatory arguments. 
\begin{table}[h!]
\centering
\small{
{\setlength{\tabcolsep}{4pt}
\begin{tabular}{l|p{10cm}} 
 Input parameter &  Description \\ \hline
\code{Y} &  Vector of the response observations of size $n$.\\ 
\code{X} &  \multicolumn{1}{m{10cm}}{Matrix of the input observations with $n$ rows and $d$ columns. Rows correspond to the observations and columns correspond to the variables.}\\ 
\code{kernel} & \multicolumn{1}{m{10cm}}{Character, indicates the type of the kernel chosen to construct the RKHS $\mathcal{H}$.}\\
\code{Dmax} & \multicolumn{1}{m{10cm}}{Integer, between $1$ and $d$, indicates the maximum order of interactions considered in the RKHS meta-model: Dmax$=1$ is used to consider only the main effects, Dmax$=2$ to include the main effects and the second-order interactions, and so on.}\\
\code{gamma} & \multicolumn{1}{m{10cm}}{Vector of non-negative scalars, values of the tuning parameter $\gamma$ in decreasing order. If $\gamma=0$ the function solves the RKHS group lasso optimization problem and for $\gamma>0$ it solves the RKHS ridge group sparse optimization problem.}\\
\code{frc} & \multicolumn{1}{m{10cm}}{Vector of positive scalars. Each element of the vector sets a value to the tuning parameter $\mu$: $\mu=\mu_{\text{max}}/(\sqrt{n}\times\text{frc})$. The value $\mu_{\text{max}}$ is calculated inside the program.}\\
\code{verbose} & \multicolumn{1}{m{10cm}}{Logical. Set as TRUE to print: the group $v$ for which the correction of the Gram matrix $K_v$ is done, and for each pair of the tuning parameters $(\mu,\gamma)$: the number of current iteration, active groups and convergence criteria. It is set as FALSE by default.}\\
\end{tabular}}}
\caption{List of the input arguments of the \code{RKHSMetMod} function. \label{metmod}}
\end{table} 

The \code{RKHSMetMod} function returns a list of $l$ components, with $l$ being equal to the number of pairs of the tuning parameters $(\mu,\gamma)$, i.e. $l=\vert \text{gamma}\vert\times\vert\text{frc}\vert$. Each component of the list is a list of three components \code{mu}, \code{gamma} and \code{Meta-Model}:
\begin{itemize}
\item \code{mu}: value of the tuning parameter $\mu$ if $\gamma>0$, or $\mu_g=\sqrt{n}\times\mu$ if $\gamma=0$.
\item \code{gamma}: value of the tuning parameter $\gamma$.
\item \code{Meta-Model}: a RKHS ridge group sparse or RKHS group lasso object associated with the tuning parameters \code{mu} and \code{gamma}. Table \ref{metmodoutput} gives a summary of all arguments of the output \code{Meta-Model} of \code{RKHSMetMod} function. 
\end{itemize}
\begin{table}[h!]
\centering
\small{
{\setlength{\tabcolsep}{4pt}
\begin{tabular}{l|p{10cm}} 
 Output parameter &  Description \\ \hline
\code{intercept} &  \multicolumn{1}{m{10cm}}{Scalar, estimated value of intercept.}\\ 
\code{teta} &  \multicolumn{1}{m{10cm}}{Matrix with vMax rows and $n$ columns. Each row of the matrix is the estimated vector $\theta_{v}$ for $v=1,...,$vMax.}\\ 
\code{fit.v} & \multicolumn{1}{m{10cm}}{Matrix with $n$ rows and vMax columns. Each row of the matrix is the estimated value of $f_{v}=K_{v}\theta_{v}$.}\\
\code{fitted} & \multicolumn{1}{m{10cm}}{Vector of size $n$, indicates the estimator of $m$.}\\
\code{Norm.n} & \multicolumn{1}{m{10cm}}{Vector of size vMax, estimated values for the empirical $L^2$-norm.}\\
\code{Norm.H} & \multicolumn{1}{m{10cm}}{Vector of size vMax, estimated values for the Hilbert norm.}\\
\code{supp} &  \multicolumn{1}{m{10cm}}{Vector of active groups.} \\
\code{Nsupp} &  \multicolumn{1}{m{10cm}}{Vector of the names of the active groups.} \\
\code{SCR} &  \multicolumn{1}{m{10cm}}{Scalar equals to $\Vert Y-f_{0}-\sum_{v}K_{v}\theta_{v}\Vert ^{2}$.} \\
\code{crit} &  \multicolumn{1}{m{10cm}}{Scalar indicates the value of the penalized criterion.} \\
\code{gamma.v} &  \multicolumn{1}{m{10cm}}{Vector of size vMax, coefficients of the empirical $L^2$-norm.} \\
\code{mu.v} &  \multicolumn{1}{m{10cm}}{Vector of size vMax, coefficients of the Hilbert norm.} \\
\code{iter} &  \multicolumn{1}{m{10cm}}{List of two components: \code{maxIter}, and the number of iterations until the convergence is achieved.} \\
\code{convergence} &  \multicolumn{1}{m{10cm}}{TRUE or FALSE. Indicates whether the algorithm has converged or not.} \\
\code{RelDiffCrit} &  \multicolumn{1}{m{10cm}}{Scalar, value of the first convergence criterion at the last iteration, i.e. $\Vert{\theta_{lastIter}-\theta_{lastIter-1}/\theta_{lastIter-1}}\Vert ^{2}$.} \\
\code{RelDiffPar} &  \multicolumn{1}{m{10cm}}{Scalar, value of the second convergence criterion at the last iteration, i.e. ${crit_{lastIter}-crit_{lastIter-1}/crit_{lastIter-1}}$.} \\
\end{tabular}}}
\caption{List of the arguments of the output \code{Meta-Model} of \code{RKHSMetMod} function. \label{metmodoutput}}
\end{table} 
\subparagraph{\code{RKHSMetMod$\_$qmax} function:} For a given value of Dmax and a chosen kernel, this function calculates the Gram matrices $K_v$, $v\in\mathcal{P}_{\text{Dmax}}$; determines $\mu$, referred to as $\mu_{qmax}$, for which the number of groups in the support of the RKHS group lasso solution is equal to $qmax$; and produces a sequence of estimators $\widehat{f}$ associated with the tuning parameter $\mu_{qmax}$ and a grid of values of the tuning parameter $\gamma$. All the estimators $\widehat{f}$ produced by this function have at most $qmax$ groups in their support.
This function has the following input arguments: 
\begin{itemize}
\item[$-$] \code{Y}, \code{X}, \code{kernel}, \code{Dmax}, \code{gamma}, \code{verbose} (see Table \ref{metmod}).
\item[$-$] \code{qmax}: integer, the maximum number of groups in the support of the obtained estimator.
\item[$-$] \code{rat}: positive scalar, to restrict the minimum value of $\mu$ considered in "Algorithm 3", $$\mu_{\text{min}}=\frac{\mu_{\text{max}}}{(\sqrt{n}\times\text{rat})},$$ 
where $\mu_{\text{max}}$ is calculated inside the program.
\item[$-$] \code{Num}: integer, to restrict the number of different values of the tuning parameter $\mu$ to be evaluated in the RKHS group lasso algorithm until it achieves $\mu_{qmax}$. For example, if \code{Num} equals $1$, the program is implemented for three different values of $\mu\in[\mu_{\text{min}},\mu_{\text{max}})$: 
\begin{align*}
&\mu_{1}=\frac{(\mu_{\text{min}}+\mu_{\text{max}})}{2},\\
&\mu_{2}=\left\{ \begin{array}{rcl}
         \frac{(\mu_{\text{min}}+\mu_{1})}{2} & \mbox{if}& \vert \widehat{S}_{\widehat{f}(\mu_1)_{\text{Group Lasso}}}\vert <qmax,\\ 
         \frac{(\mu_{1}+\mu_{\text{max}})}{2}   & \mbox{if} & \vert \widehat{S}_{\widehat{f}(\mu_1)_{\text{Group Lasso}}}\vert >qmax, 
                \end{array}\right.\\
&  \mu_{3}=\mu_{\text{min}},\\              
\end{align*}
where $\vert \widehat{S}_{\widehat{f}(\mu_1)_{\text{Group Lasso}}}\vert$ is the number of groups in the support of the solution of the RKHS group lasso problem associated with $\mu_{1}$.

If \code{Num} $>1$, the path to cover the interval $[\mu_{\text{min}},\mu_{\text{max}})$ is detailed in "Algorithm 3".  
\end{itemize}
The \code{RKHSMetMod$\_$qmax} function returns a list of three components \code{mus}, \code{qs}, and \code{MetaModel}:
\begin{itemize}
\item \code{mus}: vector of all values of $\mu_i$ in "Algorithm 3".
\item \code{qs}: vector with the same length as \code{mus}. Each element of the vector shows the number of groups in the support of the RKHS meta-model obtained by solving RKHS group lasso problem for an element in \code{mus}.
\item \code{MetaModel}: list with the same length as the vector \code{gamma}. Each component of the list is a list of three components \code{mu}, \code{gamma} and \code{Meta-Model}:
\begin{itemize}
\item \code{mu}: value of $\mu_{qmax}$.
\item \code{gamma}: element of the input vector \code{gamma} associated with the estimated \code{Meta-Model}.
\item \code{Meta-Model}: a RKHS ridge group sparse or RKHS group lasso object associated with the tuning parameters \code{mu} and \code{gamma} (see Table \ref{metmodoutput}). 
\end{itemize}
\end{itemize}  
\subsection{Companion functions}\label{subsec:Comp}
\subparagraph{\code{calc$\_$Kv} function:} For a given value of Dmax and a chosen kernel, this function calculates the Gram matrices $K_v$, $v\in\mathcal{P}_{\text{Dmax}}$, and returns their associated eigenvalues and eigenvectors. 
This function has,
\begin{itemize}
\item four mandatory input arguments: 
\begin{itemize}
\item \code{Y}, \code{X}, \code{kernel}, \code{Dmax} (see Table \ref{metmod}).
\end{itemize}
\item three facultative input arguments: 
\begin{itemize}
\item \code{correction}: logical, set as TRUE to make correction to the matrices $K_v$. It is set as TRUE by default.
\item \code{verbose}: logical, set as TRUE to print the group for which the correction is done. It is set as TRUE by default.
\item \code{tol}: scalar to be chosen small, set as $1e^{-8}$ by default.
\end{itemize}
\end{itemize}
The \code{calc$\_$Kv} function returns a list of two components \code{kv} and \code{names.Grp}:
\begin{itemize}
\item \code{kv}: list of vMax components, each component is a list of, 
\begin{itemize}
\item \code{Evalues}: vector of eigenvalues.
\item \code{Q}: matrix of eigenvectors.
\end{itemize}
\item \code{names.Grp}: vector of group names of size vMax.
\end{itemize}
\subparagraph*{\code{RKHSgrplasso} function:} 
For a given value of the tuning parameter $\mu_g$, this function fits the solution to the RKHS group lasso optimization problem.  
This function has, 
\begin{itemize}
\item three mandatory input arguments: 
\begin{itemize}
\item \code{Y} (see Table \ref{metmod}). 
\item \code{Kv}: list of the eigenvalues and the eigenvectors of the positive definite Gram matrices $K_v$ for $v=1,...,$vMax and their associated group names (output of the function \code{calc$\_$Kv}).
\item \code{mu}: positive scalar indicates the value of the tuning parameter $\mu_g=\sqrt{n}\mu$. 
\end{itemize}
\item two facultative input arguments: 
\begin{itemize}
\item \code{maxIter}: integer, to set the maximum number of loops through all groups. It is set as $1000$ by default. 
\item \code{verbose}: logical, set as TRUE to print the number of current iteration, active groups and convergence criterion. It is set as FALSE by default.
\end{itemize}
\end{itemize}
This function returns a RKHS group lasso object associated with the tuning parameter $\mu_g$. Its output is a list of $13$ components:
\begin{itemize}
\item \code{intercept}, \code{teta}, \code{fit.v}, \code{fitted}, \code{Norm.H}, \code{supp}, \code{Nsupp}, \code{SCR}, \code{crit}, \code{MaxIter}, \code{convergence}, \code{RelDiffCrit}, and \code{RelDiffPar} (see Table \ref{metmodoutput}). 
\end{itemize}
\subparagraph*{\code{mu$\_$max} function:} 
This function calculates the value $\mu_{\text{max}}$. 
It has two mandatory input arguments: the response vector \code{Y}, and the list \code{matZ} of the eigenvalues and eigenvectors of the positive definite Gram matrices $K_v$ for $v=1,...,$vMax. This function returns the $\mu_{max}$ value.
\subparagraph*{\code{pen$\_$MetMod} function:} 
This function produces a sequence of the RKHS meta-models associated with a given grid of values of the tuning parameters $\mu,\gamma$. Each RKHS meta-model in the sequence is the solution to the RKHS ridge group sparse optimization problem associated with a pair of values of $(\mu,\gamma)$ in the grid of values of $\mu,\gamma$.
This function has,
\begin{itemize}
\item seven mandatory input arguments: 
\begin{itemize}
\item \code{Y} (see Table \ref{metmod}).
\item \code{gamma}: vector of positive scalars. Values of the penalty parameter $\gamma$ in decreasing order.
\item \code{Kv}: list of the eigenvalues and the eigenvectors of the positive definite Gram matrices $K_v$ for $v=1,...,$vMax and their associated group names (output of the function \code{calc$\_$Kv}).
\item \code{mu}: vector of positive scalars. Values of the tuning parameter $\mu$ in decreasing order.
\item \code{resg}: list of the RKHS group lasso objects associated with the components of \code{mu}, used as initial parameters at "Step $1$".
\item \code{gama$\_$v} and \code{mu$\_$v}: vector of vMax positive scalars. These two inputs are optional. They are provided to associate the weights to the two penalty terms in the RKHS ridge group sparse criterion. In order to consider no weights, i.e. all the weights are equal to one, we set these two inputs to scalar zero.
\end{itemize}
\item three facultative input arguments: 
\begin{itemize}
\item \code{maxIter}: integer, to set the maximum number of loops through initial active groups at "Step $1$" and maximum number of loops through all groups at "Step $2$".
It is set as $1000$ by default.
\item \code{verbose}: logical, set as TRUE to print for each pair of the tuning parameters $(\mu,\gamma)$, the number of current iteration, active groups and convergence criterion. It is set as FALSE by default.
\item \code{calcStwo}: logical, set as TRUE to execute "Step $2$".
It is set as FALSE by default.
\end{itemize}
\end{itemize}
The function \code{pen$\_$MetMod} returns a list of $l$ components, with $l$ being equal to the number of pairs of the tuning parameters $(\mu,\gamma)$. Each component of the list is a list of three components \code{mu}, \code{gamma} and \code{Meta-Model}:
\begin{itemize}
\item \code{mu}: positive scalar, an element of the input vector \code{mu} associated with the estimated \code{Meta-Model}.
\item \code{gamma}: positive scalar, an element of the input vector \code{gamma} associated with the estimated \code{Meta-Model}.
\item \code{Meta-Model}: a RKHS ridge group sparse object associated with the tuning parameters \code{mu} and \code{gamma} (see Table \ref{metmodoutput}). 
\end{itemize}
\subparagraph*{\code{PredErr} function:} 
By considering a testing dataset, this function calculates the prediction errors for the obtained RKHS meta-models. This function has eight mandatory input arguments:
\begin{itemize}
\item[$-$] \code{X}, \code{gamma}, \code{kernel}, \code{Dmax} (see Table \ref{metmod}).
\item[$-$] \code{XT}: matrix of observations of the testing dataset with $n^{\text{test}}$ rows and $d$ columns.
\item[$-$] \code{YT}: vector of response observations of the testing dataset of size $n^{\text{test}}$.
\item[$-$] \code{mu}: vector of positive scalars. Values of the tuning parameter $\mu$ in decreasing order.
\item[$-$] \code{res}: list of the estimated RKHS meta-models for the learning dataset associated with the tuning parameters $(\mu,\gamma)$ (it could be the output of one of the functions \code{RKHSMetMod} or \code{pen$\_$MetMod}).
\end{itemize}
Note that, the same \code{kernel} and \code{Dmax} have to be chosen as the ones used for the learning dataset.

The function \code{PredErr} returns a matrix of the prediction errors. Each element of the matrix corresponds to the prediction error of one RKHS meta-model in \code{res}.
\subparagraph*{\code{prediction} function:} 
This function calculates the predicted values for a new dataset based on the \textit{best} RKHS meta-model estimator. It has six input arguments:
\begin{itemize}
\item[$-$] \code{X}, \code{kernel}, \code{Dmax} (see Table \ref{metmod}).
\item[$-$] \code{Xnew}: matrix of new observations with $n^{\text{new}}$ rows and $d$ columns.
\item[$-$] \code{res}: list of the estimated RKHS meta-models for a learning dataset associated with the tuning parameters $(\mu,\gamma)$ (it could be the output of one of the functions \code{RKHSMetMod}, \code{RKHSMetMod$\_$qmax} or \code{pen$\_$MetMod}).
\item[$-$] \code{Err}: matrix of the prediction errors associated with the RKHS meta-models in \code{res} (output of the function \code{PredErr}).
\end{itemize}
The function \code{prediction} returns a vector of the predicted values based on the \textit{best} RKHS meta-model estimator in \code{res}. This function is available at \href{https://github.com/halalehkamari/RKHSMetaMod}{GitHub}.
\subparagraph*{\code{SI$\_$emp} function:} For each RKHS meta-model $\widehat{f}$, this function calculates the empirical Sobol indices for all the groups that are active in the support of $\widehat{f}$. This function has two input arguments:
\begin{itemize}
\item[$-$] \code{res}: list of the estimated meta-models using RKHS ridge group sparse or RKHS group lasso algorithms (it could be the output of one of the functions \code{RKHSMetMod}, \code{RKHSMetMod$\_$qmax} or \code{pen$\_$MetMod}).
\item[$-$] \code{ErrPred}: matrix or NULL. If matrix, each element of the matrix corresponds to the prediction error of a RKHS meta-model in \code{res} (output of the function \code{PredErr}). Set as NULL by default.
\end{itemize}
The empirical Sobol indices are then calculated for each RKHS meta-model in \code{res}, and a list of vectors of the Sobol indices is returned.
If the argument \code{ErrPred} is the matrix of the prediction errors, the vector of empirical Sobol indices is returned for the \textit{best} RKHS meta-model in the \code{res}.
\vspace{0.3cm} 
\bibliography{supplementary-materials-kamari-halaleh}

\address{Halaleh Kamari\\
  UMR 8071 Laboratoire de Mathématiques et Modélisation d'Évry (LaMME)\\
  Bâtiment IBGBI de l'Université d'Evry Val d'Essonne, 23 Bd de France, 91037 Evry CEDEX \\
  France\\
  %(ORCiD if desired)\\
  \email{halala.kamari@gmail.com}}

\address{Sylvie Huet\\
  UR 1404 Mathématiques et Informatique Appliquées du Génome à l'Environnement (MaIAGE)\\
  Bâtiment 210 de INRAE, Domaine de Vilvert, 78352 JOUY-EN-JOSAS Cedex\\
  France\\
  %(ORCiD if desired)\\
  \email{shuet.inra@gmail.com}}

\address{Marie-Luce Taupin\\
  UMR 8071 Laboratoire de Mathématiques et Modélisation d'Évry (LaMME)\\
  Bâtiment IBGBI de l'Université d'Evry Val d'Essonne, 23 Bd de France, 91037 Evry CEDEX \\
  France\\
  %(ORCiD if desired)\\
  \email{marie-luce.taupin@univ-evry.fr}}
