@article{aronszajn50reproducing,
  added-at = {2008-03-10T11:21:00.000+0100},
  author = {Aronszajn, N.},
  biburl = {https://www.bibsonomy.org/bibtex/2024c71f807cbf95a8fb6b934c01f4919/sb3000},
  description = {CiteULike: Theory of reproducing kernels},
  interhash = {5f0e5e40a1512aa0b21f287a39b81b31},
  intrahash = {024c71f807cbf95a8fb6b934c01f4919},
  journal = {Transactions of the American Mathematical Society},
  keywords = {kernel},
  number = 3,
  pages = {337--404},
  timestamp = {2010-10-07T14:13:58.000+0200},
  title = {{Theory of Reproducing Kernels}},
  url = {http://dx.doi.org/10.2307/1990404},
  volume = 68,
  year = 1950
}
@Article{RcppEigen,
    title = {{Fast and Elegant Numerical Linear Algebra Using the
      RcppEigen Package}},
    author = {Douglas Bates and Dirk Eddelbuettel},
    journal = {Journal of Statistical Software},
    year = {2013},
    volume = {52},
    number = {5},
    pages = {1--24},
    url = {http://www.jstatsoft.org/v52/i05/},
  }
@book{Berlinet2004ReproducingKH,
  title={{Reproducing Kernel Hilbert Spaces in Probability and Statistics}},
  author={Berlinet, A. and Thomas-Agnan, C.},
  isbn={9781402076794},
  lccn={2003064182},
  doi={https://doi.org/10.1007/978-1-4419-9096-9},
  year={2003},
  publisher={Springer US}
}
@article{BlatmanSudret,
author={Blatman, G. and Sudret, B.},
title={Adaptive sparse polynomial chaos expansion based on least angle regression},
journal={Journal of computational Physics},
year={2011},
volume={230},
pages={2345-2367}
}

@article{10.1561/2200000016, 
author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan}, 
title = {{Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers}}, 
year = {2011}, 
issue_date = {January 2011}, 
publisher = {Now Publishers Inc.}, 
address = {Hanover, MA, USA}, 
volume = {3}, 
number = {1}, 
issn = {1935-8237}, 
url = {https://doi.org/10.1561/2200000016}, 
doi = {10.1561/2200000016}, 
journal = {Found. Trends Mach. Learn.}, 
month = jan, 
pages = {1-122}, 
numpages = {122} 
}
@article{10.1561/2200000050, 
author = {Bubeck, S{\'e}bastien}, 
title = {{Convex Optimization: Algorithms and Complexity}}, 
year = {2015}, 
issue_date = {November 2015}, 
publisher = {Now Publishers Inc.}, 
address = {Hanover, MA, USA}, 
volume = {8}, number = {3-4}, 
issn = {1935-8237}, 
url = {https://doi.org/10.1561/2200000050}, 
doi = {10.1561/2200000050}, 
journal = {Found. Trends Mach. Learn.}, 
month = nov, 
pages = {231-357}, 
numpages = {127} 
}
@Article{mlegp,
    author = {Garrett M Dancik and Karin S Dorman},
    title = {mlegp: statistical analysis for computer models of
      biological systems using {R}},
    journal = {Bioinformatics},
    year = {2008},
    volume = {24},
    number = {17},
    pages = {1966}
  }
@article{Durrande2012,
abstract = {Gaussian Process models are often used for predicting and approximating expensive experiments. However, the number of observations required for building such models may become unrealistic when the input dimension increases. In oder to avoid the curse of dimensionality, a popular approach in multivariate smoothing is to make simplifying assumptions like additivity. The ambition of the present work is to give an insight into a family of covariance kernels that allows combining the features of Gaussian Process modeling with the advantages of generalized additive models, and to describe some properties of the resulting models.},
affiliation = {School of mathematics and statistics, University of Sheffield, Sheffield S3 7RH, UK, Ecole Nationale SupÃ©rieure des Mines, FAYOL-EMSE, LSTI, F-42023 Saint-Etienne, France; Institute of Mathematical Statistics and Actuarial Science, University of Berne, Alpeneggstrasse 22, 3012 Bern, Switzerland; Ecole Nationale SupÃ©rieure des Mines, FAYOL-EMSE, LSTI, F-42023 Saint-Etienne, France},
author = {Durrande, N. and Ginsbourger, D. and Roustant, O.},
journal = {Annales de la faculté des sciences de Toulouse Mathématiques},
keywords = {additive covariance kernels; Gaussian process modeling; additive Kriging},
language = {eng},
month = {4},
number = {3},
pages = {481-499},
publisher = {UniversitÃ© Paul Sabatier, Toulouse},
title = {{Additive Covariance kernels for high-dimensional Gaussian Process modeling}},
url = {http://eudml.org/doc/251000},
volume = {21},
year = {2012},
}
@article{DURRANDE201357,
title = "{ANOVA} kernels and {RKHS} of zero mean functions for model-based sensitivity analysis",
journal = "Journal of Multivariate Analysis",
volume = "115",
pages = "57 - 67",
year = "2013",
issn = "0047-259X",
doi = "https://doi.org/10.1016/j.jmva.2012.08.016",
url = "http://www.sciencedirect.com/science/article/pii/S0047259X1200214X",
author = "N. Durrande and D. Ginsbourger and O. Roustant and L. Carraro",
keywords = "Gaussian process regression, Global sensitivity analysis, HoeffdingÃ¢ÂÂSobol decomposition, SS-ANOVA",
abstract = "Given a reproducing kernel Hilbert space (H,Ã£ÂÂ.,.Ã£ÂÂ) of real-valued functions and a suitable measure ÃÅ over the source space DÃ¢ÂÂR, we decompose H as the sum of a subspace of centered functions for ÃÅ and its orthogonal in H. This decomposition leads to a special case of ANOVA kernels, for which the functional ANOVA representation of the best predictor can be elegantly derived, either in an interpolation or regularization framework. The proposed kernels appear to be particularly convenient for analyzing the effect of each (group of) variable(s) and computing sensitivity indices without recursivity."
}
@book{Eddelbuettel:2013:SRC:2517725,
 author = {Eddelbuettel, Dirk},
 title = {{Seamless R and C++ Integration with Rcpp}},
 year = {2013},
 isbn = {1461468671, 9781461468677},
 publisher = {Springer Publishing Company, Incorporated},
} 
@Manual{RcppGSL,
    title = {RcppGSL: 'Rcpp' Integration for 'GNU GSL' Vectors and Matrices},
    author = {Dirk Eddelbuettel and Romain Francois},
    year = {2019},
    note = {R package version 0.3.7},
    url = {https://cran.r-project.org/web/packages/RcppGSL/},  
}
@article{10.2307/1390712,
 ISSN = {10618600},
 URL = {http://www.jstor.org/stable/1390712},
 abstract = {Bridge regression, a special family of penalized regressions of a penalty function ?|? j|? with ? ? 1, is considered. A general approach to solve for the bridge estimator is developed. A new algorithm for the lasso (? = 1) is obtained by studying the structure of the bridge estimators. The shrinkage parameter ? and the tuning parameter ? are selected via generalized cross-validation (GCV). Comparison between the bridge model (? ? 1) and several other shrinkage models, namely the ordinary least squares regression (? = 0), the lasso (? = 1) and ridge regression (? = 2), is made through a simulation study. It is shown that the bridge regression performs well compared to the lasso and ridge regression. These methods are demonstrated through an analysis of a prostate cancer data. Some computational advantages and limitations are discussed.},
 author = {Wenjiang J. Fu},
 journal = {Journal of Computational and Graphical Statistics},
 number = {3},
 pages = {397--416},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
 title = {Penalized {R}egressions: The {B}ridge versus the {L}asso},
 volume = {7},
 year = {1998}
}
@misc{galassi2018scientific,
  added-at = {2018-01-12T09:26:17.000+0100},
  author = {Galassi, M. and Davies, J. and Theiler, J. and Gough, B. and Jungman, G. and Alken, P. and Booth, M. and Rossi, F. and Ulerich, R.},
  biburl = {https://www.bibsonomy.org/bibtex/2e20f2399022535753901fb218da52347/peter.ralph},
  interhash = {7af761771e032e87387361e33ef7fb3b},
  intrahash = {e20f2399022535753901fb218da52347},
  keywords = {gsl software},
  timestamp = {2018-01-12T09:26:34.000+0100},
  title = {{GNU Scientific Library Reference Manual}},
  url = {http://www.gnu.org/software/gsl/},
  year = 2018
}
@book{giraud2014introduction,
  title={Introduction to High-Dimensional Statistics},
  author={Giraud, C.},
  isbn={9781482237948},
  lccn={2015002096},
  series={Chapman \& Hall/CRC Monographs on Statistics \& Applied Probability},
  url={https://books.google.fr/books?id=qRuVoAEACAAJ},
  year={2014},
  publisher={Taylor \& Francis}
}
@article{10.1214/18-BA1133,
author = {Mengyang Gu},
title = {{Jointly Robust Prior for Gaussian Stochastic Process in Emulation, Calibration and Variable Selection}},
volume = {14},
journal = {Bayesian Analysis},
number = {3},
publisher = {International Society for Bayesian Analysis},
pages = {857 -- 885},
keywords = {computer model, posterior propriety, reference prior, tail rate},
year = {2019},
doi = {10.1214/18-BA1133},
URL = {https://doi.org/10.1214/18-BA1133}
}
@article{10.1214/17-AOS1648,
author = {Mengyang Gu and Xiaojing Wang and James O. Berger},
title = {{Robust Gaussian stochastic process emulation}},
volume = {46},
journal = {The Annals of Statistics},
number = {6A},
publisher = {Institute of Mathematical Statistics},
pages = {3038 -- 3066},
keywords = {Anisotropic covariance, emulation, Gaussian stochastic process, objective priors, posterior propriety, robust parameter estimation},
year = {2018},
doi = {10.1214/17-AOS1648},
URL = {https://doi.org/10.1214/17-AOS1648}
}
@article{Gu_2019,
   title={{RobustGaSP: Robust Gaussian Stochastic Process Emulation in R}},
   volume={11},
   ISSN={2073-4859},
   url={http://dx.doi.org/10.32614/rj-2019-011},
   DOI={10.32614/rj-2019-011},
   number={1},
   journal={The R Journal},
   publisher={The R Foundation},
   author={Gu, Mengyang and Palomo, Jesus and Berger, James, O.},
   year={2019},
   pages={112}
}
@MISC{eigenweb,
  author = {Ga\"{e}l Guennebaud and Beno\^{i}t Jacob and others},
  title = {Eigen v3},
  howpublished = {http://eigen.tuxfamily.org},
  year = {2010}
 }
@Manual{rkhsmeamodpackage,
    title = {RKHSMetaMod: Ridge Group Sparse Optimization Problem for Estimation of a Meta Model Based on Reproducing Kernel Hilbert Spaces},
    author = {Halaleh Kamari},
    year = {2019},
    url = {https://CRAN.R-project.org/package=RKHSMetaMod},
    note = {R package version 1.1},
  }
@Manual{gglspkg,
    title = {gglasso: Group Lasso Penalized Learning Using a Unified BMD Algorithm},
    author = {Yi Yang and Hui Zou and Sahir Bhatnagar},
    year = {2020},
    url = {https://CRAN.R-project.org/package=gglasso},
    note = {R package version 1.5},
  }  
@book{Hastie:2015:SLS:2834535,
 author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
 title = {Statistical Learning with Sparsity: The Lasso and Generalizations},
 year = {2015},
 isbn = {1498712169, 9781498712163},
 publisher = {Chapman \& Hall/CRC},
}
@article{HOMMA19961,
title = "Importance measures in global sensitivity analysis of nonlinear models",
journal = "Reliability Engineering \& System Safety",
volume = "52",
number = "1",
pages = "1 - 17",
year = "1996",
issn = "0951-8320",
doi = "https://doi.org/10.1016/0951-8320(96)00002-6",
url = "http://www.sciencedirect.com/science/article/pii/0951832096000026",
author = "Toshimitsu Homma and Andrea Saltelli",
abstract = "The present paper deals with a new method of global sensitivity analysis of nonlinear models. This is based on a measure of importance to calculate the fractional contribution of the input parameters to the variance of the model prediction. Measures of importance in sensitivity analysis have been suggested by several authors, whose work is reviewed in this article. More emphasis is given to the developments of sensitivity indices by the Russian mathematician I.M. Sobol'. Given that Sobol' treatment of the measure of importance is the most general, his formalism is employed throughout this paper where conceptual and computational improvements of the method are presented. The computational novelty of this study is the introduction of the âtotal effectâ parameter index. This index provides a measure of the total effect of a given parameter, including all the possible synergetic terms between that parameter and all the others. Rank transformation of the data is also introduced in order to increase the reproducibility of the method. These methods are tested on a few analytical and computer models. The main conclusion of this work is the identification of a sensitivity analysis methodology which is both flexible, accurate and informative, and which can be achieved at reasonable computational cost."
}
@article{huet:hal-01434895,
	author = {Huet, S. and Taupin, M.-L.},
	title = {Metamodel construction for sensitivity analysis},
	DOI= "10.1051/proc/201760027",
	url= "https://doi.org/10.1051/proc/201760027",
	journal = {ESAIM: Procs},
	year = 2017,
	volume = 60,
	pages = "27-69",
}
@article{marrel:hal-00525489,
  TITLE = {Global sensitivity analysis of stochastic computer models with joint metamodels},
  AUTHOR = {Marrel, Amandine and Iooss, Bertrand and Da Veiga, S{\'e}bastien and Ribatet, Mathieu},
  URL = {https://hal.archives-ouvertes.fr/hal-00525489},
  JOURNAL = {{Statistics and Computing}},
  PUBLISHER = {{Springer Verlag (Germany)}},
  VOLUME = {22},
  PAGES = {833-847},
  YEAR = {2012},
  KEYWORDS = {Computer experiment ; Generalized additive model ; Gaussian process ; Joint modeling ; Sobol indices ; Uncertainty},
  PDF = {https://hal.archives-ouvertes.fr/hal-00525489/file/sobol_joint_SC3.pdf},
  HAL_ID = {hal-00525489},
  HAL_VERSION = {v2},
}
@ARTICLE{Kennedy00bayesiancalibration,
    author = {Marc C. Kennedy and Anthony O'Hagan},
    title = {{Bayesian Calibration of Computer Models}},
    journal = {Journal of the Royal Statistical Society, Series B, Methodological},
    year = {2000},
    volume = {63},
    pages = {425--464}
}
@article{kimeldorf1970,
author = "Kimeldorf, George S. and Wahba, Grace",
doi = "10.1214/aoms/1177697089",
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "04",
number = "2",
pages = "495--502",
publisher = "The Institute of Mathematical Statistics",
title = "{A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines}",
url = "http://dx.doi.org/10.1214/aoms/1177697089",
volume = "41",
year = "1970"
}
@article{KLEIJNEN2009707,
title = "Kriging metamodeling in simulation: A review",
journal = "European Journal of Operational Research",
volume = "192",
number = "3",
pages = "707 - 716",
year = "2009",
issn = "0377-2217",
doi = "https://doi.org/10.1016/j.ejor.2007.10.013",
url = "http://www.sciencedirect.com/science/article/pii/S0377221707010090",
author = "Kleijnen, Jack P. C.",
keywords = "Kriging, Metamodel, Response surface, Interpolation, Optimization, Design",
abstract = "This article reviews Kriging (also called spatial correlation modeling). It presents the basic Kriging assumptions and formulasâcontrasting Kriging and classic linear regression metamodels. Furthermore, it extends Kriging to random simulation, and discusses bootstrapping to estimate the variance of the Kriging predictor. Besides classic one-shot statistical designs such as Latin Hypercube Sampling, it reviews sequentialized and customized designs for sensitivity analysis and optimization. It ends with topics for future research."
}
@book{Kleijnen:2007:DAS:1554802,
 author = {Kleijnen, Jack P. C.},
 title = {Design and Analysis of Simulation Experiments},
 year = {2007},
 isbn = {0387718125, 9780387718125},
 edition = {1st},
 publisher = {Springer Publishing Company, Incorporated},
}
@article{doi:10.1137/130926869,
author = {Le Gratiet, Loic. and Cannamela, Claire. and Iooss, Bertrand.},
title = {{A Bayesian Approach for Global Sensitivity Analysis of (Multifidelity) Computer Codes}},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
volume = {2},
number = {1},
pages = {336-363},
year = {2014},
doi = {10.1137/130926869},

URL = { 
        https://doi.org/10.1137/130926869
    
},
eprint = { 
        https://doi.org/10.1137/130926869
    
}

}
@Inbook{LeGratiet2017,
author="Le Gratiet, Lo{\"i}c
and Marelli, Stefano
and Sudret, Bruno",
editor="Ghanem, Roger
and Higdon, David
and Owhadi, Houman",
title="Metamodel-Based Sensitivity Analysis: Polynomial Chaos Expansions and Gaussian Processes",
bookTitle="Handbook of Uncertainty Quantification",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="1289--1325",
abstract="Global sensitivity analysis is now established as a powerful approach for determining the key random input parameters that drive the uncertainty of model output predictions. Yet the classical computation of the so-called Sobol' indices is based on Monte Carlo simulation, which is not affordable when computationally expensive models are used, as it is the case in most applications in engineering and applied sciences. In this respect metamodels such as polynomial chaos expansions (PCE) and Gaussian processes (GP) have received tremendous attention in the last few years, as they allow one to replace the original, taxing model by a surrogate which is built from an experimental design of limited size. Then the surrogate can be used to compute the sensitivity indices in negligible time. In this chapter an introduction to each technique is given, with an emphasis on their strengths and limitations in the context of global sensitivity analysis. In particular, Sobol' (resp. total Sobol') indices can be computed analytically from the PCE coefficients. In contrast, confidence intervals on sensitivity indices can be derived straightforwardly from the properties of GPs. The performance of the two techniques is finally compared on three well-known analytical benchmarks (Ishigami, G-Sobol', and Morris functions) as well as on a realistic engineering application (deflection of a truss structure).",
isbn="978-3-319-12385-1",
doi="10.1007/978-3-319-12385-1_38",
url="https://doi.org/10.1007/978-3-319-12385-1_38"
}
@article{lin2006,
author = "Lin, Yi and Zhang, Hao Helen",
doi = "10.1214/009053606000000722",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "10",
number = "5",
pages = "2272--2297",
publisher = "The Institute of Mathematical Statistics",
title = "Component selection and smoothing in multivariate nonparametric regression",
url = "https://doi.org/10.1214/009053606000000722",
volume = "34",
year = "2006"
}
@incollection{NIPS2008_3616,
title = {Nonparametric regression and classification with joint sparsity constraints},
author = {Liu, Han and Wasserman, Larry and John D. Lafferty},
booktitle = {Advances in Neural Information Processing Systems 21},
editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
pages = {969--976},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3616-nonparametric-regression-and-classification-with-joint-sparsity-constraints.pdf}
}
@inbook{doi:10.1061/9780784413609.257,
author = {Stefano Marelli and Bruno Sudret },
title = {UQLab: A Framework for Uncertainty Quantification in Matlab},
booktitle = {Vulnerability, Uncertainty, and Risk},
chapter = {},
pages = {2554-2563},
doi = {10.1061/9780784413609.257},
URL = {https://ascelibrary.org/doi/abs/10.1061/9780784413609.257},
eprint = {https://ascelibrary.org/doi/pdf/10.1061/9780784413609.257},
year = {2014},
abstract = { Uncertainty quantification is a rapidly growing field in computer simulation-based scientific applications. The UQLab project aims at the development of a Matlab-based software framework for uncertainty quantification. It is designed to encourage both academic researchers and field engineers to use and develop advanced and innovative algorithms for uncertainty quantification, possibly exploiting modern distributed computing facilities. Ease of use, extendibility and handling of non-intrusive stochastic methods are core elements of its development philosophy. The modular platform comprises a highly optimized core probabilistic modelling engine and a simple programming interface that provides unified access to heterogeneous high performance computing resources. Finally, it provides a content-management system that allows users to easily develop additional custom modules within the framework. In this contribution, we intend to demonstrate the features of the platform at its current development stage. }
}
@article{MARREL2009742,
title = "{Calculations of Sobol indices for the Gaussian process metamodel}",
journal = "Reliability Engineering \& System Safety",
volume = "94",
number = "3",
pages = "742 - 751",
year = "2009",
issn = "0951-8320",
doi = "https://doi.org/10.1016/j.ress.2008.07.008",
url = "http://www.sciencedirect.com/science/article/pii/S0951832008001981",
author = "Amandine Marrel and Bertrand Iooss and BÃ©atrice Laurent and Olivier Roustant",
keywords = "Gaussian process, Covariance, Metamodel, Sensitivity analysis, Uncertainty, Computer code",
abstract = "Global sensitivity analysis of complex numerical models can be performed by calculating variance-based importance measures of the input variables, such as the Sobol indices. However, these techniques, requiring a large number of model evaluations, are often unacceptable for time expensive computer codes. A well-known and widely used decision consists in replacing the computer code by a metamodel, predicting the model responses with a negligible computation time and rending straightforward the estimation of Sobol indices. In this paper, we discuss about the Gaussian process model which gives analytical expressions of Sobol indices. Two approaches are studied to compute the Sobol indices: the first based on the predictor of the Gaussian process model and the second based on the global stochastic process model. Comparisons between the two estimates, made on analytical examples, show the superiority of the second approach in terms of convergence and robustness. Moreover, the second approach allows to integrate the modeling error of the Gaussian process model by directly giving some confidence intervals on the Sobol indices. These techniques are finally applied to a real case of hydrogeological modeling."
}
@article{JSSv042i11,
   author = {Mebane, WR. and Sekhon, JS.},
   title = {{Genetic Optimization Using Derivatives: The rgenoud Package for R}},
   journal = {Journal of Statistical Software, Articles},
   volume = {42},
   number = {11},
   year = {2011},
   keywords = {},
   abstract = {genoud is an R function that combines evolutionary algorithm methods with a derivative-based (quasi-Newton) method to solve difficult optimization problems. genoud may also be used for optimization problems for which derivatives do not exist. genoud solves problems that are nonlinear or perhaps even discontinuous in the parameters of the function to be optimized. When the function to be optimized (for example, a log-likelihood) is nonlinear in the model's parameters, the function will generally not be globally concave and may have irregularities such as saddlepoints or discontinuities. Optimization methods that rely on derivatives of the objective function may be unable to find any optimum at all. Multiple local optima may exist, so that there is no guarantee that a derivative-based method will converge to the global optimum. On the other hand, algorithms that do not use derivative information (such as pure genetic algorithms) are for many problems needlessly poor at local hill climbing. Most statistical problems are regular in a neighborhood of the solution. Therefore, for some portion of the search space, derivative information is useful. The function supports parallel processing on multiple CPUs on a single machine or a cluster of computers.},
   issn = {1548-7660},
   pages = {1--26},
   doi = {10.18637/jss.v042.i11},
   url = {https://www.jstatsoft.org/v042/i11}
}
@article{MeiGeeBue08,
  added-at = {2010-03-15T12:35:12.000+0100},
  author = {Meier, Lukas and van de Geer, Sara and B\"uhlmann, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/23568c6c2dfac0f42ecfbce56784babf7/for916},
  doi = {10.1111/j.1467-9868.2007.00627.x},
  interhash = {1f0199e3944e88ee66ec4d253642deaf},
  intrahash = {3568c6c2dfac0f42ecfbce56784babf7},
  journal = {Journal of the Royal Statistical Society. Series B},
  keywords = {C1},
  number = 1,
  pages = {53-71},
  timestamp = {2014-02-26T21:25:57.000+0100},
  title = {The group lasso for logistic regression},
  volume = 70,
  year = 2008
}
@Manual{grppkg,
    title = {grplasso: Fitting User-Specified Models with Group Lasso Penalty},
    author = {Meier, Lukas},
    year = {2020},
    url = {https://CRAN.R-project.org/package=grplasso},
    note = {R package version 0.4-7},
  }
@article{meier2009,
author = {Meier, Lukas and van de Geer, Sara and B\"uhlmann, Peter},
doi = {10.1214/09-AOS692},
fjournal = {The Annals of Statistics},
journal = {Ann. Statist.},
month = {12},
number = {6B},
pages = {3779--3821},
publisher = {The Institute of Mathematical Statistics},
title = {High-dimensional additive modeling},
url = {https://doi.org/10.1214/09-AOS692},
volume = {37},
year = {2009}
}
@article{doi:10.1111/j.1467-9868.2004.05304.x,
author = {Oakley, Jeremy E. and O'Hagan, Anthony},
title = {{Probabilistic sensitivity analysis of complex models: a Bayesian approach}},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {66},
number = {3},
pages = {751-769},
keywords = {Bayesian inference, Computer model, Gaussian process, Sensitivity analysis, Uncertainty analysis},
doi = {10.1111/j.1467-9868.2004.05304.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2004.05304.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2004.05304.x},
abstract = {Summary.â In many areas of science and technology, mathematical models are built to simulate complex real world phenomena. Such models are typically implemented in large computer programs and are also very complex, such that the way that the model responds to changes in its inputs is not transparent. Sensitivity analysis is concerned with understanding how changes in the model inputs influence the outputs. This may be motivated simply by a wish to understand the implications of a complex model but often arises because there is uncertainty about the true values of the inputs that should be used for a particular application. A broad range of measures have been advocated in the literature to quantify and describe the sensitivity of a model's output to variation in its inputs. In practice the most commonly used measures are those that are based on formulating uncertainty in the model inputs by a joint probability distribution and then analysing the induced uncertainty in outputs, an approach which is known as probabilistic sensitivity analysis. We present a Bayesian framework which unifies the various tools of prob- abilistic sensitivity analysis. The Bayesian approach is computationally highly efficient. It allows effective sensitivity analysis to be achieved by using far smaller numbers of model runs than standard Monte Carlo methods. Furthermore, all measures of interest may be computed from a single set of runs.},
year = {2004}
} 
@article{PIANOSI201580,
title = "{A Matlab toolbox for Global Sensitivity Analysis}",
journal = "Environmental Modelling \& Software",
volume = "70",
pages = "80 - 85",
year = "2015",
issn = "1364-8152",
doi = "https://doi.org/10.1016/j.envsoft.2015.04.009",
url = "http://www.sciencedirect.com/science/article/pii/S1364815215001188",
author = "Francesca Pianosi and Fanny Sarrazin and Thorsten Wagener",
keywords = "Global Sensitivity Analysis, Matlab, Octave, Open-source software",
abstract = "Global Sensitivity Analysis (GSA) is increasingly used in the development and assessment of environmental models. Here we present a Matlab/Octave toolbox for the application of GSA, called SAFE (Sensitivity Analysis For Everybody). It implements several established GSA methods and allows for easily integrating others. All methods implemented in SAFE support the assessment of the robustness and convergence of sensitivity indices. Furthermore, SAFE includes numerous visualisation tools for the effective investigation and communication of GSA results. The toolbox is designed to make GSA accessible to non-specialist users, and to provide a fully commented code for more experienced users to complement their own tools. The documentation includes a set of workflow scripts with practical guidelines on how to apply GSA and how to use the toolbox. SAFE is open source and freely available for academic and non-commercial purpose. Ultimately, SAFE aims at contributing towards improving the diffusion and quality of GSA practice in the environmental modelling community."
}
@inproceedings{NIPS2009_3688,
  title={Lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness},
  author={Garvesh Raskutti and Martin J. Wainwright and Bin Yu},
  booktitle={Advances in Neural Information Processing Systems},
  year={2009}
}
@article{Raskutti:2012:MRS:2503308.2188398,
 author = {Raskutti, Garvesh and Wainwright, Martin J. and Yu, Bin},
 title = {{Minimax-optimal Rates for Sparse Additive Models over Kernel Classes via Convex Programming}},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2012},
 volume = {13},
 number = {1},
 month = feb,
 year = {2012},
 issn = {1532-4435},
 pages = {389--427},
 numpages = {39},
 url = {http://dl.acm.org/citation.cfm?id=2503308.2188398},
 acmid = {2188398},
 publisher = {JMLR.org},
 keywords = {convex, kernel, minimax, non-parametric, sparsity},
} 
@article{Ravikumar,
author = {Ravikumar, Pradeep and Lafferty, John and Liu, Han and Wasserman, Larry},
title = {Sparse additive models},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {71},
number = {5},
pages = {1009-1030},
keywords = {Additive models, Lasso, Non-parametric regression, Sparsity},
doi = {10.1111/j.1467-9868.2009.00718.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2009.00718.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2009.00718.x},
abstract = {Summary.â We present a new class of methods for high dimensional non-parametric regression and classification called sparse additive models. Our methods combine ideas from sparse linear modelling and additive non-parametric regression. We derive an algorithm for fitting the models that is practical and effective even when the number of covariates is larger than the sample size. Sparse additive models are essentially a functional version of the grouped lasso of Yuan and Lin. They are also closely related to the COSSO model of Lin and Zhang but decouple smoothing and sparsity, enabling the use of arbitrary non-parametric smoothers. We give an analysis of the theoretical properties of sparse additive models and present empirical results on synthetic and real data, showing that they can be effective in fitting sparse non-parametric models in high dimensional data.},
year = {2009}
}
@article{JSSv051i01,
   author = {Olivier Roustant and David Ginsbourger and Yves Deville},
   title = {{DiceKriging, DiceOptim: Two R Packages for the Analysis of Computer Experiments by Kriging-Based Metamodeling and Optimization}},
   journal = {Journal of Statistical Software, Articles},
   volume = {51},
   number = {1},
   year = {2012},
   keywords = {},
   abstract = {We present two recently released R packages, DiceKriging and DiceOptim, for the approximation and the optimization of expensive-to-evaluate deterministic functions. Following a self-contained mini tutorial on Kriging-based approximation and optimization, the functionalities of both packages are detailed and demonstrated in two distinct sections. In particular, the versatility of DiceKriging with respect to trend and noise specifications, covariance parameter estimation, as well as conditional and unconditional simulations are illustrated on the basis of several reproducible numerical experiments. We then put to the fore the implementation of sequential and parallel optimization strategies relying on the expected improvement criterion on the occasion of DiceOptim’s presentation. An appendix is dedicated to complementary mathematical and computational details.},
   issn = {1548-7660},
   pages = {1--55},
   doi = {10.18637/jss.v051.i01},
   url = {https://www.jstatsoft.org/v051/i01}
}  
@book{saitoh1988theory,
  title={Theory of reproducing kernels and its applications},
  author={Saitoh, S.},
  isbn={9780582035645},
  lccn={88018725},
  series={Pitman research notes in mathematics series},
  url={https://books.google.fr/books?id=YzxPAQAAIAAJ},
  year={1988},
  publisher={Longman Scientific \& Technical}
}
@article{SALTELLI2002280,
title = "Making best use of model evaluations to compute sensitivity indices",
journal = "Computer Physics Communications",
volume = "145",
number = "2",
pages = "280 - 297",
year = "2002",
issn = "0010-4655",
doi = "https://doi.org/10.1016/S0010-4655(02)00280-1",
url = "http://www.sciencedirect.com/science/article/pii/S0010465502002801",
author = "Andrea Saltelli",
keywords = "Sensitivity analysis, Sensitivity measures, Sensitivity indices, Importance measures"
}
@article{doi:10.1080/00401706.1999.10485594,
author = {Saltelli, A. and Tarantola, S. and Chan, K. P.-S.},
title = {{A Quantitative Model-Independent Method for Global Sensitivity Analysis of Model Output}},
journal = {Technometrics},
volume = {41},
number = {1},
pages = {39-56},
year  = {1999},
publisher = {Taylor & Francis},
doi = {10.1080/00401706.1999.10485594},

URL = { 
        https://www.tandfonline.com/doi/abs/10.1080/00401706.1999.10485594
    
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1080/00401706.1999.10485594
    
}

}
@book{saltelli2009sensitivity,
  title={Sensitivity Analysis},
  author={Saltelli, A. and Chan, K. and Scott, E.M.},
  isbn={9780470743829},
  lccn={00023093},
  url={https://books.google.fr/books?id=gOcePwAACAAJ},
  year={2009},
  publisher={Wiley}
}
@book{schoutens2000stochastic,
  title={Stochastic Processes and Orthogonal Polynomials},
  author={Schoutens, W.},
  isbn={9780387950150},
  lccn={00022019},
  series={Lecture Notes in Statistics},
  url={https://books.google.fr/books?id=V2BS\_Dmp0XoC},
  year={2000},
  publisher={Springer New York}
}
@inproceedings{Sobol1993SensitivityEF,
  title={{Sensitivity Estimates for Nonlinear Mathematical Models}},
  booktitle={Sensitivity Estimates for Nonlinear Mathematical Models},
  author={Ilya M. Sobol},
  year={1993}
}
@article{doi:10.1137/S1064827503424505,
author = {Soize, C. and Ghanem, R.},
title = {{Physical Systems with Random Uncertainties: Chaos Representations with Arbitrary Probability Measure}},
journal = {SIAM Journal on Scientific Computing},
volume = {26},
number = {2},
pages = {395-410},
year = {2004},
doi = {10.1137/S1064827503424505},

URL = { 
        https://doi.org/10.1137/S1064827503424505
    
},
eprint = { 
        https://doi.org/10.1137/S1064827503424505
    
}

}
@article{SUDRET2008964,
title = "Global sensitivity analysis using polynomial chaos expansions",
journal = "Reliability Engineering \& System Safety",
volume = "93",
number = "7",
pages = "964 - 979",
year = "2008",
note = "Bayesian Networks in Dependability",
issn = "0951-8320",
doi = "https://doi.org/10.1016/j.ress.2007.04.002",
url = "http://www.sciencedirect.com/science/article/pii/S0951832007001329",
author = "Bruno Sudret",
keywords = "Global sensitivity analysis, Sobolâ indices, Analysis of variance, Polynomial chaos, Generalized chaos, Regression, Stochastic finite elements",
abstract = "Global sensitivity analysis (SA) aims at quantifying the respective effects of input random variables (or combinations thereof) onto the variance of the response of a physical or mathematical model. Among the abundant literature on sensitivity measures, the Sobolâ indices have received much attention since they provide accurate information for most models. The paper introduces generalized polynomial chaos expansions (PCE) to build surrogate models that allow one to compute the Sobolâ indices analytically as a post-processing of the PCE coefficients. Thus the computational cost of the sensitivity indices practically reduces to that of estimating the PCE coefficients. An original non intrusive regression-based approach is proposed, together with an experimental design of minimal size. Various application examples illustrate the approach, both from the field of global SA (i.e. well-known benchmark problems) and from the field of stochastic mechanics. The proposed method gives accurate results for various examples that involve up to eight input random variables, at a computational cost which is 2â3 orders of magnitude smaller than the traditional Monte Carlo-based evaluation of the Sobolâ indices."
}
@book{vaart_1998, 
place={Cambridge}, 
series={Cambridge Series in Statistical and Probabilistic Mathematics}, 
title={Asymptotic Statistics}, 
DOI={10.1017/CBO9780511802256}, 
publisher={Cambridge University Press}, 
author={Van der Vaart, A. W.}, 
year={1998}, 
collection={Cambridge Series in Statistical and Probabilistic Mathematics}
}
@article{10.2307/1269548,
 ISSN = {00401706},
 URL = {http://www.jstor.org/stable/1269548},
 abstract = {Many scientific phenomena are now investigated by complex computer models or codes. Given the input values, the code produces one or more outputs via a complex mathematical model. Often the code is expensive to run, and it may be necessary to build a computationally cheaper predictor to enable, for example, optimization of the inputs. If there are many input factors, an initial step in building a predictor is identifying (screening) the active factors. We model the output of the computer code as the realization of a stochastic process. This model has a number of advantages. First, it provides a statistical basis, via the likelihood, for a stepwise algorithm to determine the important factors. Second, it is very flexible, allowing nonlinear and interaction effects to emerge without explicitly modeling such effects. Third, the same data are used for screening and building the predictor, so expensive runs are efficiently used. We illustrate the methodology with two examples, both having 20 input variables. In these examples, we identify the important variables, detect curvature and interactions, and produce a useful predictor with 30-50 runs of the computer code.},
 author = {William J. Welch and Robert. J. Buck and Jerome Sacks and Henry P. Wynn and Toby J. Mitchell and Max D. Morris},
 journal = {Technometrics},
 number = {1},
 pages = {15--25},
 publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
 title = {{Screening, Predicting, and Computer Experiments}},
 volume = {34},
 year = {1992}
}
@article{10.2307/2371268,
 ISSN = {00029327, 10806377},
 URL = {http://www.jstor.org/stable/2371268},
 author = {Norbert Wiener},
 journal = {American Journal of Mathematics},
 number = {4},
 pages = {897--936},
 publisher = {Johns Hopkins University Press},
 title = {{The Homogeneous Chaos}},
 volume = {60},
 year = {1938}
}
@article{Yang:2015:FUA:2833490.2833520,
 author = {Yang, Yi and Zou, Hui},
 title = {{A Fast Unified Algorithm for Solving Group-lasso Penalize Learning Problems}},
 journal = {Statistics and Computing},
 issue_date = {November  2015},
 volume = {25},
 number = {6},
 month = nov,
 year = {2015},
 issn = {0960-3174},
 pages = {1129--1141},
 numpages = {13},
 url = {http://dx.doi.org/10.1007/s11222-014-9498-5},
 doi = {10.1007/s11222-014-9498-5},
 acmid = {2833520},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {Group lasso, Groupwise descent, Large margin classifiers, MM principle, SLEP, grplasso},
} 
@article{doi:10.1111/j.1467-9868.2005.00532.x,
author = {Yuan, Ming and Lin, Yi},
title = {Model selection and estimation in regression with grouped variables},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {68},
number = {1},
pages = {49-67},
keywords = {Analysis of variance, Lasso, Least angle regression, Non-negative garrotte, Piecewise linear solution path},
doi = {10.1111/j.1467-9868.2005.00532.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00532.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2005.00532.x},
abstract = {Summary.â We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
year = {2006}
}
@Manual{sensitivity,
    title = {sensitivity: Global Sensitivity Analysis of Model Outputs},
    author = {Bertrand Iooss and Sebastien Da Veiga and Alexandre Janon and Gilles Pujol},
    year = {2020},
    note = {R package version 1.19.0},
    url = {https://CRAN.R-project.org/package=sensitivity},
}
@Manual{lhspackage,
    title = {lhs: Latin Hypercube Samples},
    author = {Rob Carnell},
    year = {2021},
    url = {https://CRAN.R-project.org/package=lhs},
    note = {R package version 1.1.3},
  }
