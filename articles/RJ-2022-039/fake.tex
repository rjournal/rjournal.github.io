% !TeX root = RJwrapper.tex
\title{The Concordance Test, an Alternative to Kruskal-Wallis Based on the Kendall-$\tau$ Distance: An R Package}
\author{by Javier Alcaraz, Laura Anton-Sanchez and Juan Francisco Monge}

\maketitle

\abstract{
%An abstract of less than 150 words.
The Kendall rank correlation coefficient, based on the Kendall-$\tau$ distance, is used to measure the ordinal association between two measurements. In this paper, we introduce a new coefficient also based on the Kendall-$\tau$ distance, the Concordance coefficient, and a test to measure whether different samples come from the same distribution. This work also presents a new R package, \pkg{ConcordanceTest}, with the implementation of the proposed coefficient. We illustrate the use of the Concordance coefficient to measure the ordinal association between quantity and quality measures when two or more samples are considered. In this sense, the Concordance coefficient can be seen as a generalization of the Kendall rank correlation coefficient and an alternative to the non-parametric mean rank-based methods for comparing two or more samples. A comparison of the proposed Concordance coefficient and the classical Kruskal-Wallis statistic is presented through a comparison of the exact distributions of both statistics.
}

%Section title in sentence case
%Section title in sentence case
\section{Introduction} \label{sec:intro}

When we have a sample of observations of a given population it may be difficult to assume that they come from a certain distribution since we may not always have any type of information about the variable under study and when we do, it may not be enough to determine the type of distribution. In these cases, parametric inference is inappropriate. Moreover, this type of technique may be unsuitable should the observations not fulfill any of the basic assumptions on which they are based; normality and a large quantity of data.

Violation of the necessary assumptions in parametric statistics necessitates the use of non-parametric statistics. Non-parametric tests do not depend on the definition of a distribution function or statistical parameters such as mean, variance, etc. The use of non-parametric tests, despite being less powerful, is also adequate when there are not enough observations available, when data are non-normal data or when ordinal data are being analyzed.

Although the first steps in non-parametric statistics began earlier, it was not until the 1930s that a systematic study in this field appeared. \cite{Fisher1935} introduced the permutation test or randomization test as a simple way to compute the sampling distribution for any test statistic under the null hypothesis that does not establish any effect on all possible outcomes. Over the next two decades some of the main non-parametric tests emerged, \cite{Friedman1940,Kendall1939,Kendall1938,Kruskal1958,Kruskal1952,Wilcoxon1947,Pitman1937,Wilcoxon1945}, among others.

The main advantages of the non-parametric tests are: the data can be nonnumerical observations while they can be classified according to some criterion, they are usually  easy to calculate and do not make any hypothesis about the distribution of the population from which the samples are taken. We can also cite two drawbacks: the non-parametric tests are less precise than other statistical models and they are based on the order of the elements in the sample and this order will likely stay the same even if the numerical data change. 

There are many non-parametric tests in the literature, which can basically be classified into four categories depending on whether: it is a test to compare two or more than two related samples or a test for comparing related or unrelated samples. Examples of the most used non-parametric tests in the literature for each of these four situations are the following: the \emph{Wilcoxon signed-rank test} \citep{Wilcoxon1945} for comparing two related samples, the \emph{Mann-Whitney (Wilcoxon) test} \citep{Wilcoxon1947} for comparing two unrelated samples, the  \emph{Friedman test} \citep{Friedman1940} for comparing three or more related samples, and the \emph{Kruskal-Wallis test} \citep{Kruskal1952} for comparing three or more unrelated samples. Several methods that exploit some characteristic of the samples have appeared in the literature in recent years, such as \cite{Alhakim2008, Terpstra2003}. 

It is also possible to measure the degree of association of two variables through a non-parametric approach, in that sense we can mention the Kendall rank correlation coefficient \citep{Kendall1938} and the Spearman rank correlation coefficient \citep{Spearman1904}.

In \cite{Aparicio2020}, the  authors introduce the \emph{Kendall-$\tau$ partition ranking}; given a ranking of elements of a set and given a disjoint partition of the same set, the Kendall-$\tau$ partition ranking is the induced linear order of the subsets of the partition which follows from the given ranking of elements of a set. In this work, we propose to use the Kendall-$\tau$ distance as a concordance measure between the different samples in an ordered set of observations. In this regard, the proposed measure, which we call \emph{Concordance coefficient}, can be considered as an extension of the Kendall rank correlation coefficient when more than two samples are considered. The main difference between the proposed measure and the previous ones, is the consideration of the Kendall-$\tau$ distance instead of ranks, which use classical methods. We also propose a significance test in order to determine when more than two samples come from the same distribution, and present a comparison with the classical Kruskal-Wallis method. We illustrate the use of the proposed coefficient with a new R package, \CRANpkg{ConcordanceTest} \citep{ConcordanceTest2022}, which is freely available from the Comprehensive R Archive Network (CRAN). Actually, R establishes the state of the art in statistical software. There are currently packages for all the non-parametric tests mentioned above, for example: the \CRANpkg{Kendall} package \citep{KendallPackage2011}, which deals with the Kendall rank correlation coefficient; the \CRANpkg{pspearman} package \citep{pspearmanPackage2014}, with the Spearman rank correlation coefficient; or the \pkg{stats} package: an R Core Team and contributors' worldwide package that contains many of the non-parametric tests for comparing two or more, related or unrelated, samples. The Kendall-$\tau$ distance, on which the proposed coefficient is based, is one of the most used in distance-based models, for which there are also recent alternatives in R. See, for example, the \CRANpkg{PerMallows} \citep{Irurozki2016}, \CRANpkg{rankdist} \citep{Qian2019} or \CRANpkg{BayesMallows} packages \citep{Sorensen2020}.

The remainder of this paper is organized as follows. After a brief review in the next section of the main features of the  Kendall rank correlation coefficient and the Kruskal-Wallis statistic, in the following two sections we present the coefficient we propose in this work and illustrate its use with our \pkg{ConcordanceTest} package. Specifically, in the third section we introduce the Concordance coefficient while in the fourth section the related statistical test is presented. The fifth section includes a comparison between the Kruskal-Wallis test in the \pkg{stats} package and that presented in this work. Some final remarks follow in the last section. Appendix A presents an example of the probability distribution of the Concordance coefficient and the Kruskal-Wallis statistic. Appendix B deals with a comparison between the probability density function of the Concordance coefficient and the Kruskal-Wallis statistic for several experiments. Finally, Appendix C presents some details of how the p-values for the Concordance coefficient have been calculated and shows some critical values and exact p-values.

\section{Non-parametric tests} \label{sec:nonparametric}

This section presents the Kendall rank correlation coefficient \citep{Kendall1938}, a coefficient to measure the relationship between two samples ordinally, and the Kruskal-Wallis statistical test \citep{Kruskal1952}, which is a rank-based statistical test to measure whether different samples come from the same distribution, without assuming a given distribution for the population.

Only these two non-parametric tests are presented in detail, since the test proposed in this paper uses the Kendall-$\tau$ distance and it can be seen as an extension of the Kendall rank correlation coefficient when more than two samples are considered, and it is presented as an alternative to the Kruskal-Wallis statistical test.

The Kendall rank correlation coefficient is a non-parametric measure of correlation. This measure is based on the Kendall-$\tau$ distance between two permutations of $n$ elements. The Kendall-$\tau$ distance ($d_{K\text{-}\tau}$) is defined as the number or pairwise disagreements between two permutations $\pi_1$ and $\pi_2$. For instance, if we have three elements, the distance from permutation 123 to permutations 132, 231 and 321 is 1, 2 and 3 respectively. The maximum number of disagreements that may occur between two permutations of $n$ elements is $n(n-1)/2$ and, in this case, all the values of permutation $\pi_1$  are in the reverse order of $\pi_2$. 

The Kendall rank correlation coefficient between permutations $\pi_1$ and $\pi_2$, denoted by $\tau$, is defined by 
$$ \tau = 1 -  2 \frac{d_{K\text{-}\tau(\pi_1,\pi_2)}}{n(n-1)/2}.$$
The Kendall rank correlation  coefficient is used as a statistical test to determine whether there is a relationship or dependence between two random variables. The main advantages of this coefficient are: the data can be non-numerical observations if they can be ordered, it is easy to calculate, and the associated statistical test does not assume a known distribution of the population from which the samples are taken. 

The Kruskal-Wallis test is a non-parametric statistical method to study whether different samples come from the same population. The test is the extension of the Mann-Whitney Test \citep{Wilcoxon1947} when we have more than two samples or groups. 
The following example illustrates the Kruskal-Wallis test when comparing three samples.

\begin{exmp} \label{ex:1}
Let us assume that the effectiveness of three different treatments ($A$, $B$, $C$) has been measured for 6 individuals, two individuals being assigned to each of the treatments, with the effectiveness of each treatment being measured ordinally. We could obtain the result shown in Table~\ref{t:example}, where, for example, the effectiveness of treatment $A$ has been rated in first and third place. 
\begin{table}[h]\centering
\begin{tabular}{ccccccc}
 & $A$ & $B$ & $A$ & $C$ & $C$ & $B$ \\ \hline
 Rank & 1 & 2 & 3 & 4 & 5 & 6  \\ \hline 
 \end{tabular}
  \caption{Result for an experiment with 6 people and 3 treatments.}
  \label{t:example}
\end{table}
\end{exmp} 

The Kruskal-Wallis statistic is determined by the difference between the ranks of the individuals in each category with the average rank. In our example, the average rank of the test is $\overline{R}=3.5$, while the average rank of each of the three treatments are $\overline{R}_A=2$, $\overline{R}_B=4$ and $\overline{R}_C=4.5$. The Kruskal-Wallis statistic, denoted by $H$, is based on the calculation of the distance of each rank to the average rank, which can be expressed as follows:
$$ H =  -3(n+1)+\frac{12}{n(n+1)}\sum_{i=1}^k \frac{R_i^2}{n_i},$$
where  $n$ is the number of observations in the $k$ samples, $n_i$ is the number of observations in the $i$-th sample and $R_i$ is the sum of the ranks in the $i$-th sample. In our example, the value of the Kruskal-Wallis statistic is:
$$ H =  -3(n+1)+\frac{12}{n(n+1)}\sum_{i=1}^k \frac{R_i^2}{n_i} = -3(6+1)+\frac{12}{6(6+1)}\left (\frac{4^2}{2} + \frac{8^2}{2} + \frac{9^2}{2}  \right ) = 2. $$
Table~\ref{t:prob_KW_222} shows the probability distribution of the Kruskal-Wallis statistic for 3 treatments, each with 2 patients. Appendix A presents the Kruskal-Wallis statistic for all possible results in the experiment for 3 treatments with 2 people in each. In \cite{Spurrier2003}, the author compares different methods for approximating the null probability points. 

\begin{table}[h]	\centering\footnotesize
\begin{tabular}{cc} 
   $H$   &$ Prob$           \\ \toprule
0.00 & 0.06667 \\ \cmidrule(l){1-2}
0.29 &0.13333\\\cmidrule(l){1-2}
0.86 & 0.13333\\\cmidrule(l){1-2}
1.14 & 0.13333\\\cmidrule(l){1-2}
2.00     &  0.13333\\\cmidrule(l){1-2}
2.57 & 0.06667\\\cmidrule(l){1-2}
3.43 & 0.13333\\\cmidrule(l){1-2}
3.71 & 0.13333 \\\cmidrule(l){1-2}
4.57 & 0.06667 \\  \bottomrule[0.2mm]
\end{tabular}
\caption{Probability distribution for the Kruskal-Wallis statistic ($H$), with sample sizes $N = (2,2,2)$.}
\label{t:prob_KW_222}
\end{table}

\section{The Concordance coefficient $\tau_c$} \label{sec:CC}

In \cite{Aparicio2020}, the authors introduce the \emph{Kendall-$\tau$ partition ranking};  given a ranking of elements of a set and given a disjoint partition of the same set, the Kendall-$\tau$ partition ranking is  the induced  linear order of the subsets of the partition which follows from the given ranking of elements of a set. The Kendall-$\tau$ partition ranking presents an ordinal alternative to the mean-based ranking that uses a pseudo-cardinal scale. Let  $\pi$ be permutation of the elements of set $V$ and let $V_1$, $V_2$, $\ldots$, $V_k$ be a partition of $V$ then, the Kendall-$\tau$ distance from permutation $\pi$ is given by 
$$ d_{K\text{-}\tau} = \displaystyle \min \{ d_{K\text{-}\tau}(\rho, \pi):\text{ elements in }V_r\text{ are consecutively listed in }\rho,\,\, \forall r \}.$$

This distance is also called the  disorder of permutation $\pi$. For the calculation of the disorder of a permutation of elements, in \cite{Aparicio2020}, the authors establish that the distance or disorder of a permutation of elements
$\pi = (a|a|b|b|a|c|a|b|c|\cdots|c|a|b)$ is given by the solution of the Linear Ordering Problem (LOP) with the preference matrix $M$, where the element $m_{ab}$ of matrix $M$ indicates the number of times that an element $a$ of sample $A$ precedes an element $b$ of sample $B$ in the order $\pi$. The solution of the Linear Ordering Problem gives us a new order in the elements of $\pi$, the closest to  $\pi$, in which all the elements belonging to the same sample are listed consecutively. The book publication by \cite{Marti2011} provides an exhaustive study of the Linear Ordering Problem. 

The authors \cite{Aparicio2020} present the properties of the Kendall-$\tau$ partition ranking and compare it with classical mean and median-based rank approaches. Those properties are extracted from social choice theory and are adapted to a partition ranking, see \cite{arrow1951,Kemeny1959,zahid2015}. Two of these properties are only true for the Kendall-$\tau$ partition ranking: the \emph{Condorcet} and \emph{Deletion Independence} properties. The Condorcet property establishes that the most preferred subset must be listed before any other in any ranking; and the Deletion Independence property establishes that if any subset is removed, then the induced order of subsets does not change. In permutation $\pi=(c|c|c|b|b|a|a|c|c)$ the set $C$ is a condorcet winner, the most preferred set, but $B$ has a lesser mean rank value than set $C$ if set $A$ is not considered in the comparison; therefore, the permutation $\pi=(c|c|c|b|b|a|a|c|c)$ gives an example where ranking subsets from ranks is not very reliable. 

From \cite{Aparicio2020}, the maximum number of disagreements that may occur in a permutation of $n$ elements (where the elements are classified in $k$ subsets $V_1, V_2,\ldots, V_k$ of sizes $n_1, n_2,\ldots, n_k$ respectively) is $\sum_{r=1}^k\sum_{s =r+1}^k n_r \, n_s - (GP_{b} +\sum_{r=1}^k\sum_{s =r+1}^k \displaystyle\lfloor\frac{n_r n_s}{2}\displaystyle\rfloor )$, where $GP_{b}$ is the Generalized Pentagonal Number of $b$, and $b$ the number of subsets $V_k$ with odd cardinality.   The Generalized Pentagonal number $GP_{b}$, for $b\in \mathbb{N}$, is 
$$GP_b=\left\{ \begin{array}{ll}\displaystyle\frac{\ell(3\ell -1)}{2} & b=2\ell \,\, (b \,\, \text{even}),\\  & \\\displaystyle\frac{\ell(3\ell +1)}{2} & b=2\ell +1 \,\, (b \,\, \text{odd}).\\ \end{array} \right. $$
This maximum number of  disagreements (the maximum disorder) in a permutation $\pi$ of elements, allows us to define a relative disorder coefficient of permutation $\pi$ as
$$ relative\,  disorder(\pi) = \frac{\displaystyle d_{K\text{-}\tau}(\pi)}{\displaystyle\sum_{r=1}^k\sum_{s =r+1}^k  n_r \, n_s   -  (GP_{b} +\sum_{r=1}^k\sum_{s =r+1}^k\displaystyle\lfloor\frac{n_r n_s}{2}\displaystyle\rfloor )}.$$

\begin{definition}
	We define the  Concordance coefficient ($\tau_c$) of permutation $\pi$ as
	$$\tau_c= 1- relative\, disorder(\pi) = 1- \frac{\displaystyle d_{K\text{-}\tau}(\pi)}{\displaystyle\sum_{r=1}^k\sum_{s =r+1}^k  n_r \, n_s   -  (GP_{b} +\sum_{r=1}^k\sum_{s =r+1}^k\displaystyle\lfloor\frac{n_r n_s}{2}\displaystyle\rfloor )} .$$
\end{definition}

The  Concordance coefficient ($\tau_c$) provides a measure of independence in the $k$ samples, where $\tau_c$ is a value between 0 and 1, taking the value of 1 when there is a total order between the samples, and 0 when the disorder is maximum. In this sense, the Concordance coefficient can be seen as a generalization of the Kendall rank correlation coefficient when we have more than two samples. Given that the Concordance coefficient satisfies the properties mentioned above, we consider it is more appropriate for measuring differences between samples than a rank-based method, such as Kruskal-Wallis'.

\setcounter{exmp}{0}
\begin{exmp}[Cont.] 
Continuing with the data in Example~\ref{ex:1}, the results of the experiment provide the following order or permutation of the treatments $\pi=(a|b|a|c|c|b|)$.
\end{exmp} 

Given the order of individuals $\pi=(a|b|a|c|c|b|)$, the ordering between individuals that leaves individuals with the same treatment together is ordination ($a$ $a$ $b$ $b$ $c$ $c$) or ($a$ $a$ $c$ $c$ $b$ $b$). Both ordinations only need 3 pairwise disagreements from the permutation $\pi$. In order to find the permutation of elements (equal elements listed consecutively) closer to a given permutation, it is sufficient to solve the Linear Ordering Problem (LOP) with the preference matrix defined above. In this example, said matrix is:

\setlength{\tabcolsep}{1mm}
\begin{center}
\begin{tabular}{cccc}
&\,\,\, \,\, $A$ & $B$ & $C$\,\,\,\, \\ 
$A$&\multirow{3}{*}{  $ \left(\begin{array}{r}  $--$ \\1  \\  0 \end{array}\right.$}  &\multirow{3}{*}{  $ \begin{array}{c}3   \\ $--$  \\  2\end{array}$}   &\multirow{3}{*}{  $ \left.\begin{array}{c}4   \\2  \\  $--$ \end{array}\right)$}  \\
$B$&&&\\
$C$&&&\\
 \end{tabular},\end{center}
 
where each element of the matrix $m_{ij}$ represents the number of times an individual of a treatment $i$ precedes an individual of the treatment $j$. The solution of the LOP is the permutation of treatments which maximizes the preferences of order in the experiment, that is, in this example, the permutations of treatments $(A\ B\ C)$ or $(A\ C\ B)$ retain 9 preferences expressed in the order of individuals represented by the permutation $\pi$. Therefore, the distance of the permutation $\pi$ to a total order between treatments is $\sum_{i<j} n_i n_j -9 =3$. This distance, which is the number of pairwise disagreements needed in a permutation of elements to reach a permutation that establishes a total order between treatments, is denominated the disorder of a permutation by the authors of the work by \cite{Aparicio2020}\footnote{If the number of samples is small, we can evaluate all the possibilities in order to obtain the solution of the Linear Ordering Problem, for example, if we have 3 samples the number of feasible solutions for the LOP is $3!=6$.}.

Then, the relative  disorder of permutation $\pi$ can be evaluated as 
$$ relative \,disorder(\pi) = \frac{\displaystyle d_{K\text{-}\tau}(\pi)}{\displaystyle\sum_{r=1}^k\sum_{s =r+1}^k  n_r \, n_s   -  (GP_{b} +\sum_{r=1}^k\sum_{s =r+1}^k\displaystyle\lfloor\frac{n_r n_s}{2}\displaystyle\rfloor )} =\frac{\displaystyle 3}{\displaystyle 12  -  (0
+6  )} = \frac{3}{6}=\frac{1}{2},$$
and the Concordance coefficient
$$\tau_c= 1- relative\,disorder(\pi) = 1-\frac{1}{2}=\frac{1}{2}.$$ 
Notice that no set of this example has odd cardinality, therefore the pentagonal number is $GP_0=0$.

Table~\ref{t:prob_DK_222} shows the probability distribution of the disorder and the Concordance coefficient for 3 treatments with 2 patients each. Appendix A presents the disorder and the Concordance coefficient for all possible results in the experiment with sample sizes $N = (2,2,2)$. Figure~\ref{fig:distprob_222} compares the probability distribution of the Concordance coefficient and the Kruskal-Wallis statistic, for 3 treatments and 2 people in each treatment. Notice that some Kruskal-Wallis statistic values ($H$=2.57) are less probable than large ones. 

\begin{table}[h] 	\centering\footnotesize
\begin{tabular}{ccc} 
    $dis$ & $\tau_c$   &$ Prob$           \\ \toprule
6	&	0.0000	&	0.06667\\ \cmidrule(l){1-3}
5	&	0.1667	&	0.13333	\\ \cmidrule(l){1-3}
4	&	0.3333	&	0.20000 	\\ \cmidrule(l){1-3}
3	&	0.5000	&	0.20000\\ \cmidrule(l){1-3}
2	&	0.6667	&	0.20000	\\ \cmidrule(l){1-3}
1	&	0.8333	&	0.13333	\\ \cmidrule(l){1-3}
0	&	1.0000	&	0.06667	\\ \bottomrule[0.2mm]
\end{tabular}
\caption{Probability distribution of the disorder ($dis$) and the Concordance coefficient ($\tau_c$), with sample sizes $N = (2,2,2)$.}
\label{t:prob_DK_222}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=10cm]{CC_H_222.pdf}
\caption{\label{fig:distprob_222} Probability distribution of the Concordance coefficient ($\tau_c$=1-relative disorder) and the Kruskal-Wallis statistic ($H$), with sample sizes $N = (2,2,2)$.}
\end{figure}
\subsection{The Concordance coefficient in \pkg{ConcordanceTest} package}

The R package we have developed allows to calculate both the Concordance coefficient and the Kruskal-Wallis statistic in order to facilitate their comparison. Given the high combinatorial degree of the problem of ordering samples of populations, some of the functions implemented in the package can perform the calculations exactly, exploring the entire sample space or possibilities, or they can approximate the sample space or possibilities by simulation.

The  \pkg{ConcordanceTest} package can be installed from CRAN:

\begin{example}
install.packages("ConcordanceTest")
library("ConcordanceTest")
\end{example}

and its functions can perform the calculations related only to the Concordance coefficient (default option, specified with the parameter \code{H}=0) or do them also for the Kruskal-Wallis statistic (\code{H}=1), allowing their comparison. 

To obtain the probability distribution of the statistics, it is necessary to have the set of all possible permutations that can occur in the result of the experiment that we want to analyze (90=6!/2!2!2! in Example~\ref{ex:1}). This can be obtained through the function \code{Permutations\_With\_Repetition()}, which has been developed and included in the \pkg{ConcordanceTest} package. 


The function \code{CT\_Distribution()} calculates the probability distribution of the Concordance coefficient and the Kruskal-Wallis statistic. The set of possibilities (sample space) grows very quickly with the number of elements and with the number of sets and, in some cases, to calculate the probability distribution in an exact way becomes unaffordable, making it necessary to approximate calculations. Both an exact and an approximate calculation (default option) can be done using the function \code{CT\_Distribution()}. It is used as follows:
\begin{example}
CT_Distribution(Sample_Sizes, Num_Sim = 10000, H = 0, verbose = TRUE)
\end{example}
where \code{Sample\_Sizes} is a numeric vector $(n_1,\ldots,n_k)$ containing the number of repetitions of each element, i.e., the size of each sample in the experiment. \code{Num\_Sim} is the number of simulations to be performed in order to obtain the probability distribution of the statistics (10,000 by default). If \code{Num\_Sim} is set to 0, the probability distribution tables are obtained exactly using the function \code{Permutations\_With\_Repetition()}. \code{H} is the parameter specifying whether the calculations must also be performed for the Kruskal-Wallis statistic, and \code{verbose} is a logical parameter that indicates whether some progress report of the simulations should be given.


\setcounter{exmp}{0}
\begin{exmp}[Cont.] 
Using the function \code{CT\_Distribution()} with \code{Num\_Sim} equal to 0, we could obtain the probability distribution of the Kruskal-Wallis statistic and the Concordance coefficient in Example~\ref{ex:1} (Tables~\ref{t:prob_KW_222} and \ref{t:prob_DK_222}, respectively) in an exact way. As shown in this example, we can also approximate the probability distributions of Example~\ref{ex:1} by simulating, for example, 25,000 permutations of 3 treatments with 2 patients each. Note that, for reproducibility, we always initialize the generator for pseudo-random numbers when the results rely on simulation.
\end{exmp} 
 
\begin{example}
set.seed(12)
Sample_Sizes <- c(2,2,2)
CT_Distribution(Sample_Sizes, Num_Sim = 25000, H = 1)

$C_freq
     disorder  Concordance coefficient Frequency Probability
[1,]        6                     0.00         6      0.0667
[2,]        5                     0.17        12      0.1333
[3,]        4                     0.33        18      0.2000
[4,]        3                     0.50        18      0.2000
[5,]        2                     0.67        18      0.2000
[6,]        1                     0.83        12      0.1333
[7,]        0                     1.00         6      0.0667

$H_freq
      H Statistic Frequency Probability
 [1,]        0.00         6      0.0667
 [2,]        0.29        12      0.1333
 [3,]        0.86        12      0.1333
 [4,]        1.14        12      0.1333
 [5,]        2.00        12      0.1333
 [6,]        2.57         6      0.0667
 [7,]        3.43        12      0.1333
 [8,]        3.71        12      0.1333
 [9,]        4.57         6      0.0667
\end{example}

The function \code{CT\_Distribution()} returns two elements. \code{C\_freq} is a matrix with the probability distribution of the Concordance coefficient. Each row in the matrix contains the disorder, the value of the Concordance coefficient $\tau_c$, the frequency and its probability.  \code{H\_freq} (only returned if \code{H} = 1) is a matrix with the probability distribution of the Kruskal-Wallis statistic. Each row in the matrix contains the value of the statistic $H$, the frequency and its probability. The results obtained by the function \code{CT\_Distribution()} are the same as those previously shown in Table~\ref{t:prob_DK_222} and Table~\ref{t:prob_KW_222} of Example~\ref{ex:1}.

\section{Concordance test} \label{sec:ConcordanceTest}

In this section, we present the Concordance test in order to evaluate when  different samples come from the same population distribution. The randomization test introduced by \cite{Fisher1935} establishes a framework for the statistical test based on permutations, see also \cite{Box1980,Stern1990,Welch1990}.  

If all the samples come from the same distribution, then all possible ways to rank $n$ observations divided into $k$ samples have the same probability of occurring. If a result of the experiment provides an order of the observations with a high disorder, it will support the idea that all observations come from the same population. On the contrary, a result with a small disorder will go against the claim that the observations come from the same population. In this way, we propose to consider samples that come from the same distribution as null hypothesis, while the alternative hypothesis is that some of the samples come from a different distribution.

\begin{description}
\item[$H_0$:] There is no difference among the $k$ populations.
\item[$H_a$:]  At least one of the populations differs from the other populations. 
\end{description}

The decision rule is to reject the null hypothesis if the disorder in the permutation of observations is small, equivalently if the Concordance coefficient $\tau_c$ is close to one. We reject the null hypothesis $H_0$ at the significance level $\alpha$ if $\tau_c$ is greater than the percentile $(1-\alpha)\%$ of the probability distribution of $\tau_c$. 

The following example illustrates the use of the Concordance test proposed in this work and compares it with the classical Kruskal-Wallis non-parametric test. The comparison will be made first considering that there are no ties and then modifying the data in the example so that ties appear.

\begin{exmp} \label{ex:18patients} Suppose we have applied three treatments to 18 patients, measuring the number of hours it takes these patients to recover. The results are shown in Table~\ref{t:example2}. \\

\begin{table}[h] \centering
\begin{tabular}{ccccccccccccc} \toprule
 & \multicolumn{10}{c}{Hours}\\ \hline
Treatment A & 12 & 13 & 15 & 20& 23 & 28 & 30 & 32 & 40 & 48 \\ \hline
Treatment B & 29& 31& 49 & 52 & 54 & & & & & \\ \hline
Treatment C & 24 & 26 & 44 & & & & & & &\\ \bottomrule[0.2mm]
\end{tabular}
 \caption{Result for an experiment with 18 patients and 3 treatments.}
  \label{t:example2}
\end{table}
\end{exmp}

\subsubsection{Concordance test:}

The experiment ranks the patients in the following ranking $$ (a\ a\ a\ a\ a\ c\ c\ a\ b\ a\ b\ a\ a\ c\ a\ b\ b\ b).$$

If we perform the contrast using the disorder statistic or the Concordance coefficient $\tau_c$, we must calculate the permutation of treatments that maximizes the order between patients obtained in the experiment. The matrix of preferences between treatments observed is as follows:

\begin{center}	\begin{tabular}{cccc}
		&\,\,\, \,\, $A$ & $B$ & $C$\,\,\,\, \\ 
		$A$&\multirow{3}{*}{  $ \left(\begin{array}{r}  $--$ \\7 \\  11 \end{array}\right.$}  &\multirow{3}{*}{  $ \begin{array}{c}43   \\ $--$  \\  13\end{array}$}   &\multirow{3}{*}{  $ \left.\begin{array}{c}19   \\2  \\  $--$ \end{array}\right)$}  \\
		$B$&&&\\
		$C$&&&\\
	\end{tabular}\end{center}

The order between treatments that maximizes the order between patients corresponds to the order $(A\ C\ B)$, satisfying 75 of the 95 preferences contained in the matrix, where the value 75 is the solution of the Linear Ordering Problem (LOP)\footnote{The solution of LOP for this example is the permutation of sets that maximizes the preferences in the preference  matrix. It is sufficient to compare the 6 possibilities, $(A\ B\ C) = 64 $, $(A\ C\ B) = 75 $, $(B\ A\ C) = 28$, $(B\ C\ A) = 20$, $(C\ A\ B) = 67$ and $(C\ B\ A) = 31$. }. 
 Therefore, exactly 20 = 95-75 is the number of pairwise disagreements necessary to order the samples and obtain the order (ACB), that is, the disorder is 20. The greatest disorder that an order of elements can have with samples of 10, 5 and 3 elements is given by:
 $\sum_{r=1}^3\sum_{s =r+1}^3 n_r \, n_s - (GP_{b} +\sum_{r=1}^3\sum_{s =r+1}^3 \displaystyle\lfloor\frac{n_r n_s}{2}\displaystyle\rfloor ) = 95 - (1 + 47) = 47 $, 
therefore the Concordance coefficient is $\tau_c = 1-20/47= 0.574$. The p-value of the disorder 20 or, equivalently, of the Concordance coefficient $\tau_c=0.574$ is 0.049272\footnote{Tables of p-values for the Concordance coefficient $\tau_c$ are in Appendix C.},  therefore, at a level of significance less than 5\% we can reject the null hypothesis of equality in treatments. 

\subsubsection{Kruskal-Wallis test:}

The treatments A, B and C have average ranks of 7.3, 14.2 and 9, respectively, and the sum of ranks are $R_A=73$, $R_B=71$ and $R_C=27$. 

The Kruskal-Wallis statistic is given by:
$$H = -3(n+1)+\frac{12}{n(n+1)}\sum \frac{R_i^2}{n_i}= -3(18+1)+\frac{12}{18(18+1)}\left( \frac{73^2}{10} +\frac{71^2}{5}+\frac{27^2}{3}\right)=5.6$$

In \cite{Meyer2015}, exact values for the Kruskal-Wallis contrast at different levels of significance are found. We can conclude by looking at the tables that the p-value of the $H$ statistic is greater than 0.05, therefore, we cannot reject the null hypothesis that the treatments are equally effective.

Comparing both methods, the Concordance and Kruskal-Wallis tests provide similar results about the statistic but the conclusion differs. 


\begin{exmp} \label{ex:3} Suppose we have the same experiment as in Example~\ref{ex:18patients} but with ties. The results are shown in Table~\ref{t:example3}. Ties are in bold.\\

\begin{table}[h] \centering
\begin{tabular}{ccccccccccccc} \toprule
 & \multicolumn{10}{c}{Hours}\\ \hline
Treatment A & 12 & 13 & 15 & 20& {\bf 24} & {\bf 29} & 30 & 32 & 40 & {\bf 49} \\ \hline
Treatment B & {\bf 29}& 31& {\bf 49} & 52 & 54 & & & & & \\ \hline
Treatment C & {\bf 24} & 26 & 44 & & & & & & &\\ \bottomrule[0.2mm]
\end{tabular}
 \caption{Result for an experiment with 18 patients and 3 treatments. Example with ties.}
  \label{t:example3}
\end{table}
\end{exmp}


\subsubsection{Concordance test with ties:}

The results of the experiment order the individuals according to the sequence: 
$$ (a\ a\ a\ a\ (a\ c)\ c\ (a\ b)\ a\ b\ a\ a\ c\ (a\ b)\ b\ b) $$
where the elements grouped in the order indicates that they tie. There are $8$ different possibilities in order to undo ties in the ranking of elements. If the same probability is assumed for all of them, the expected preference matrix between treatments is given distributing the preference in the comparison of repeated observations with the same weight, that is, assigning the value 0.5 to each of the treatments when two tied units are compared. The preference matrix for this example would be as follows:

\begin{center}	\begin{tabular}{cccc}
		&\,\,\, \,\, $A$ & $B$ & $C$\,\,\,\, \\ 
		$A$&\multirow{3}{*}{  $ \left(\begin{array}{r}  $--$ \\8 \\  11.5 \end{array}\right.$}  &\multirow{3}{*}{  $ \begin{array}{c}42   \\ $--$  \\  13\end{array}$}   &\multirow{3}{*}{  $ \left.\begin{array}{c}18.5   \\2  \\  $--$ \end{array}\right)$}  \\
		$B$&&&\\
		$C$&&&\\
	\end{tabular}\end{center}
	
Note that the previous matrix represents the matrix of expected preferences if all permutations of items with ties in which they are undone are considered, with the same probability of tie between elements.

The order between treatments that maximizes the order between patients, corresponds to the order $(A\ C\ B)$, satisfying 73.5 of the 95 preferences contained in the matrix, where 73.5 is the solution of the Linear Ordering Problem. Therefore, 21.5 = 95-73.5 is the expected number of pairwise disagreements necessary to order the samples and obtain the order $(A\ C\ B)$, that is, the disorder is 21.5 or, equivalently, the Concordance coefficient is $\tau = 1-21.5/47= 0.543 $, a value with a significance greater than 0.05, $p - value > 0.05$.  In this case, the observed data do not show significant evidence 
in favor of a difference in the effectiveness of treatments.

\subsubsection{Kruskal-Wallis test with ties:}

The treatments A, B and C have average ranks of 7.45, 14 and 8.83, respectively, and the sum of ranks are $R_A=74.5$, $R_B=70$ and $R_C=26.5$.

The Kruskal-Wallis statistic is given by:
$$H = -3(n+1)+\frac{12}{n(n+1)}\sum \frac{R_i^2}{n_i}= -3(18+1)+\frac{12}{18(18+1)}\left( \frac{74.5^2}{10} +\frac{70^2}{5}+\frac{26.5^2}{3}\right)=5.074$$

If we make the adjustment in the statistic for ties, we get:
$$\tilde{H} = \frac{H}{\displaystyle 1-\frac{ \sum_(t_i^3-t_i)}{N^3-N}} = \frac{5.074}{\displaystyle 1-\frac{(2^3-2)+(2^3-2)+(2^3-2)}{18^3-18}}=\displaystyle 5.0897$$

where $t_i$ is the number of ties of each value.
 
In this case, the Kruskal-Wallis test provides the same conclusion as the Concordance test; uncertainty being greater when we have ties. 

\subsection{Concordance test in \pkg{ConcordanceTest} package}

The  \pkg{ConcordanceTest} R-package  allows to perform the hypothesis test for testing whether samples originate from the same distribution with the function \code{CT\_Hypothesis\_Test()}, which carries out  the calculations by simulation. It is used as follows:
\begin{example}
CT_Hypothesis_Test(Sample_List, Num_Sim = 10000, H = 0, verbose = TRUE)
\end{example}
where \code{Sample\_List} is a list of numeric data vectors with the elements of each sample, \code{Num\_Sim} is the number of used simulations (10,000 by default), \code{H} specifies whether the Kruskal-Wallis test must also be done, and \code{verbose} is a logical parameter that indicates whether some progress report of the simulations should be given.

\setcounter{exmp}{1}
\begin{exmp}[Cont.] 
We use the \pkg{ConcordanceTest} package to perform the Concordance and Kruskal-Wallis tests of Example~\ref{ex:18patients}. We use 25,000 simulations.
\end{exmp} 
%
\begin{example}
set.seed(12)
A <- c(12,13,15,20,23,28,30,32,40,48)
B <- c(29,31,49,52,54)
C <- c(24,26,44)
Sample_List <- list(A, B, C)
CT_Hypothesis_Test(Sample_List, Num_Sim = 25000, H = 1)

$results
                        Statistic p-value
Concordance coefficient     0.574 0.04928
Kruskal Wallis              5.600 0.05292

$C_p_value
[1] 0.04928

$H_p_value
[1] 0.05292
\end{example}

The function \code{CT\_Hypothesis\_Test()} provides the value of the statistics together with the p-value associated with each of them. The result of the Kruskal-Wallis test is only returned if \code{H} = 1. Note that the approximated p-values obtained by simulation are close to the exact ones, 0.04927 and 0.05223 for the Concordance coefficient and the Kruskal-Wallis statistic, respectively. 

An alternative to the contrast performed with the function \code{CT\_Hypothesis\_Test()} is to obtain the critical values of our contrast. This can be done with the \pkg{ConcordanceTest} package both in an exact or approximate way, using the function \code{CT\_Critical\_Values()}. It is used as follows:
\begin{example}
CT_Critical_Values(Sample_Sizes, Num_Sim = 10000, H = 0, verbose = TRUE)
\end{example}
where \code{Sample\_Sizes} is a numeric vector $(n_1,\ldots,n_k)$ containing the number of repetitions of each element, i.e., the size of each sample in the experiment. \code{Num\_Sim} is the number of simulations carried out in order to obtain the probability distribution of the statistics (10,000 by default). If \code{Num\_Sim} is set to 0, the critical values are obtained in an exact way. Otherwise they are obtained by simulation. \code{H} is the parameter specifying whether the critical values of the Kruskal-Wallis test must be calculated and returned, and \code{verbose} is a logical parameter that indicates whether some progress report of the simulations should be given.

The function returns a list with two elements. \code{C\_results} are the critical values and p-values for a desired significance levels of 0.1, .05 and .01 of the Concordance coefficient, and \code{H\_results} are the critical values and p-values of the Kruskal-Wallis statistic (only returned if H = 1). 

\setcounter{exmp}{1}
\begin{exmp}[Cont.] 
We show the results of the function \code{CT\_Critical\_Values()} with sample sizes $N=(10,5,3)$ and 25,000 simulations. The results allow us to compare the test statistics with different significance levels.
\end{exmp} 

\begin{example}
set.seed(12)
Sample_Sizes <- c(10,5,3)
CT_Critical_Values(Sample_Sizes, Num_Sim = 25000, H = 1)

$C_results
              |  disorder |  Concordance coefficient |  p-value
Sig level .10          23                       0.51     0.0954
Sig level .05          20                       0.57     0.0492
Sig level .01          14                       0.70     0.0096

$H_results
              |  H Statistic |  p-value
Sig level .10           4.55     0.0995
Sig level .05           5.72     0.0497
Sig level .01           7.78     0.0097
\end{example}

To obtain the Concordance coefficient and the Kruskal-Wallis statistic from the result of an experiment, the \pkg{ConcordanceTest} package has the function \code{CT\_Coefficient()}. This function is useful when we only want to obtain the value of the statistic to check its significance using statistical tables. The function \code{CT\_Coefficient()} is used as follows:
\begin{example}
CT_Coefficient(Sample_List, H = 0)
\end{example}
where \code{Sample\_List} is a list of numeric data vectors with the elements of each sample, and \code{H} is defined as usual.

\setcounter{exmp}{1}
\begin{exmp}[Cont.] 
We show the results of the function \code{CT\_Coefficient()} for the data in Example~\ref{ex:18patients}.
\end{exmp}

\begin{example}
A <- c(12,13,15,20,23,28,30,32,40,48)
B <- c(29,31,49,52,54)
C <- c(24,26,44)
Sample_List <- list(A, B, C)
CT_Coefficient(Sample_List, H = 1)

$Sample_Sizes
[1] 10  5  3

$order_elements
 [1] 1 1 1 1 1 3 3 1 2 1 2 1 1 3 1 2 2 2

$disorder
[1] 20

$Concordance_Coefficient
[1] 0.5744681

$H_Statistic
[1] 5.6
\end{example}

The function \code{CT\_Coefficient()} returns a list with the following elements: \code{Sample\_Sizes} is a numeric vector with the sample sizes, \code{order\_elements} is a numeric vector containing the elements order, \code{disorder} is the disorder of the permutation given by \code{order\_elements}, \code{Concordance\_Coefficient} is the value of the Concordance coefficient $\tau_c$, that is, 1 minus the relative disorder of the permutation given by \code{order\_elements}, and \code{H\_Statistic} is the Kruskal-Wallis statistic (only returned if \code{H} = 1).

Note that we can also solve problems with ties (as in Example~\ref{ex:3}) with the \pkg{ConcordanceTest} package.

\subsection{Other functions in the \pkg{ConcordanceTest} package}

The graphical visualization of the probability distributions of the Concordance coefficient and the Kruskal-Wallis statistic can be done with the function \code{CT\_Probability\_Plot()}. It is used as follows:
\begin{example}
CT_Probability_Plot(C_freq = NULL, H_freq = NULL)
\end{example}

Using the function \code{CT\_Density\_Plot()} of the \pkg{ConcordanceTest} package, we can make an approximate representation of the density functions of the statistics, assuming that the probability distributions represent a sample of a continuous variable. It is used as follows:
\begin{example}
CT_Density_Plot(C_freq = NULL, H_freq = NULL)
\end{example}

In both functions, \code{C\_freq} is the probability distribution of the Concordance coefficient and \code{H\_freq} is the probability distribution of the Kruskal-Wallis statistic, obtained exactly or approximately with the function \code{CT\_Distribution()}. The function \code{CT\_Probability\_Plot()} can represent both probability distributions or only one of them (if it only receives the parameter \code{C\_freq} or \code{H\_freq}). Equivalently, the function \code{CT\_Density\_Plot()} can represent both density distributions or only one of them. Appendix B presents the empirical density probability functions for several experiments, where sample sizes vary form $N=(4,4)$ to $N=(5,5,4,4,4,4,4)$.

\setcounter{exmp}{1}
\begin{exmp}[Cont.] 
Graphical visualization of the probability distributions and the density distributions of Example~\ref{ex:18patients} generated by simulation. The first row of Figure~\ref{fig:1053} compares the probability distribution of the Concordance coefficient and the Kruskal-Wallis statistic. The second row of Figure~\ref{fig:1053} shows the probability density function of the Concordance coefficient (continuous line) and the Kruskal-Wallis statistic (dashed line). Note that the $H$ statistic has been normalized between 0 and 1.
\end{exmp} 

\begin{example}
set.seed(12)
Sample_Sizes <- c(10,5,3)
ProbDistr <- CT_Distribution(Sample_Sizes, Num_Sim = 25000, H = 1)
layout(matrix(c(1,3,2,3), ncol=2))
CT_Probability_Plot(C_freq = ProbDistr$C_freq, H_freq = ProbDistr$H_freq)
CT_Density_Plot(C_freq = ProbDistr$C_freq, H_freq = ProbDistr$H_freq)
\end{example}

\begin{figure}[h]
\centering
\includegraphics[width=11.5cm]{ProbabilityAndDensityDistributions}
\caption{\label{fig:1053} Probability distributions (first row) and density distributions (second row) of the Concordance coefficient ($\tau_c$=1-relative disorder) and the Kruskal-Wallis statistic ($H$), with sample sizes $N=(10,5,3)$.}
\end{figure}

As we mentioned in Figure~\ref{fig:distprob_222}, Figure~\ref{fig:1053} also shows that similar values of the Kruskal-Wallis statistic present very different probabilities, and this leads to a less smooth function than that presented by the Concordance coefficient. We can also see that the Concordance coefficient presents a more symmetrical distribution. This performance is generalized and, therefore, we consider that the Concordance coefficient is more reliable than the Kruskal-Wallis statistic.

The \pkg{ConcordanceTest} package also contains the function \code{LOP()}, which solves the Linear Ordering Problem from a square data matrix. This function allows to calculate the disorder of a permutation of elements from the preference matrix induced by that permutation and, therefore, it is necessary for the calculation of the Concordance coefficient. The function \code{LOP()} is used by functions \code{CT\_Distribution()}, \code{CT\_Hypothesis\_Test()} and \code{CT\_Coefficient()}. It is used as follows: 
\begin{example}
LOP(mat_LOP)
\end{example}
where \code{mat\_LOP} is the preference matrix defining the Linear Ordering Problem, a numeric square matrix for which we want to obtain the permutation of rows/columns that maximizes the sum of the elements above the main diagonal.

The function \code{LOP()} returns a list with the following elements: \code{obj\_val} is the optimal value of the solution of the Linear Ordering Problem, that is, the sum of the elements above the main diagonal under the permutation rows/columns solution, \code{permutation} is the solution of the Linear Ordering Problem, that is, the rows/columns permutation, and \code{permutation\_matrix} is the optimal permutation matrix of the Linear Ordering Problem.

\setcounter{exmp}{1}
\begin{exmp}[Cont.]
The matrix of preferences between treatments observed in Example~\ref{ex:18patients} was:

\begin{center}	\begin{tabular}{cccc}
		&\,\,\, \,\, $A$ & $B$ & $C$\,\,\,\, \\ 
		$A$&\multirow{3}{*}{  $ \left(\begin{array}{r}  $--$ \\7 \\  11 \end{array}\right.$}  &\multirow{3}{*}{  $ \begin{array}{c}43   \\ $--$  \\  13\end{array}$}   &\multirow{3}{*}{  $ \left.\begin{array}{c}19   \\2  \\  $--$ \end{array}\right)$}  \\
		$B$&&&\\
		$C$&&&\\
	\end{tabular}\end{center}
	
If we apply the function \code{LOP()} on this preference matrix we obtain the following results: 
\end{exmp}
%
\begin{example}
mat_LOP <- matrix(c(0,7,11,43,0,13,19,2,0), nrow=3)
LOP(mat_LOP)

$obj_val
[1] 75

$permutation
[1] 1 3 2

$permutation_matrix
     [,1] [,2] [,3]
[1,]    0    1    1
[2,]    0    0    0
[3,]    0    1    0
\end{example}

As we saw previously, the order between treatments that maximizes the order between patients corresponds to the order $(A\ C\ B)$ (\code{permutation} = 1 3 2), satisfying \code{obj\_val} = 75 of the preferences contained in the matrix.

 
\section{Comparison with \code{kruskal.test()} function from \pkg{stats} package}\label{sec:kruskal.test}

The well-known \pkg{stats} package contains, among many other functions, the function \code{kruskal.test()} that performs a Kruskal-Wallis rank sum test. In this section, we compare the results obtained with the \pkg{ConcordanceTest} package presented in this work and the function \code{kruskal.test()}, making use of the dataset from \cite{Hollander1973} referenced in the \code{kruskal.test()} examples.

\setcounter{exmp}{3}
\begin{exmp} \label{ex:4}
Comparison of \code{kruskal.test()} (\pkg{stats} package) and \code{CT\_Hypothesis\_Test()} functions with 25,000 simulations (\pkg{ConcordanceTest} package) using the dataset from \cite{Hollander1973}.

\begin{example}

results
                        Statistic p-value
Concordance coefficient     0.188 0.78408
Kruskal-Wallis              0.771 0.71080

$C_p_value
[1] 0.78408

$H_p_value
[1] 0.7108

\end{example}
\end{exmp}

\bibliography{ConcordanceTestRJ}


\address{Javier Alcaraz\\
  Center of Operations Research, Miguel Hern\'andez University\\
  Avda. Universidad sn, 03202 Elche (Spain)\\
  E-mail: \email{jalcaraz@umh.es}\\
}

\vspace{-0.1cm}

\address{Laura Anton-Sanchez\\
	Center of Operations Research, Miguel Hern\'andez University\\
  Avda. Universidad sn, 03202 Elche (Spain)\\
  E-mail: \email{l.anton@umh.es}\\
}

\vspace{-0.1cm}

\address{Juan Francisco Monge\\
	Center of Operations Research, Miguel Hern\'andez University\\
  Avda. Universidad sn, 03202 Elche (Spain)\\
  E-mail: \email{monge@umh.es}\\
}
