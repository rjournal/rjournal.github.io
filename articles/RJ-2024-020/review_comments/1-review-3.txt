Authors provided a new R package "mcmsupply" for estimating contraceptive method market supply shares. The package is built on top of the model described in Paper: Estimating the proportion of modern contraceptives supplied by the public and private sectors using a Bayesian hierarchical penalized spline model, on arXiv, and Population Association of America's 2022 annual meeting. 

The package is written carefully, with examples for different use cases described both in paper and the docs of the package itself.   It combines the raw data, data cleaning, and model estimation processes, and it is easy to use to a new user without strong background in this topic. 

However, there are a few comments I would like to raise:

1. The first comment, and I believe it should be the most vital one, is the importance of the work. The whole work is based on the model described in the 2022 paper above on PAA and arXiv, and the main contribution of the model is to provide an estimation on the proportion of different sources of modern contraceptive supplies from different sectors, with limited number of survey data. However, even in the case that we could get an exact estimate of the proportions of different sectors, it would not be able to describe a bigger picture on the accessibility of the model contraceptive supplies for women living in low income countries. It is much more important to understand the prevalence of these supplies, instead of the suppliers of those supplies. Indeed, we may be able to infer from the proportion changes of these supplies sources, but the relationship might be obscure, and is not discussed in both papers, especially for this one.

In my opinion, this paper will be more important if the 2022 paper on arXiv is published by some journals with a careful peer-reviewing process. With that, it could show that the model is important to the scientific society, and the development of a package for that model will be important then.

2. I would like to discuss a bit more about the use of models. In the 2022 paper, authors provided out of sample analysis to prove the superiority of the models over the linear models. The results was not very surprising to me, as the data doesn't look to be linear, and the OOS results show significantly worse performance in RMSE, while the MAE was not that bad in the linear model. This is likely due to the cases when the trend reverses for the last observes, the linear model will be off much more than the spline model, and results in huge errors. 

However, I am not very excited about comparing with 0 correlation results. From the validation results, I can't see the superiority of the main model over zero-correlation model. Most of the metrics: nominal coverages, MAE, RMSE, Mean errors and PI width are all similar. Authors explained in several paragraphs without showing why the full model is more suitable to this problem. This usually means that we should choose the simple over the complex ones. Authors need to explain the selection of the model with at least one convincing point.

Lastly, another weaker comment is: Using splines have its limitation as well. Splines is not very reliable for its extrapolation performances, and most likely the extrapolation will be done by following the last pieces trend, and smoothing the curve. That's shown in the case studies of the paper. It might not be appropriate to ask for some more validation studies (such as leave more data out) due to the limitation of the data, but the reasoning of using splines models over some parametric models is not strong enough for me.

3. Back to this package. In general the package was written well. I not only tried to play with the package, but also went through carefully about the internal codes of the package. Authors drafted those codes carefully, and the users are not hard to re-use them by following the manual and this paper. There are some parts of design I would like to raise to improve the package as well as this paper:

- For data: Authors need to explain better about the required format of the user-input data, if a new researcher wants to do the estimate with his own data. Indeed, authors provided some samples, but a better exercise should also include a section in the paper describing the required columns, formatting, and other information to this data;

- The model specification: The authors set the number of knots to 12 by default. However, there were no clear reasoning of this selection. It's good that researchers could change those easily in the function get_modelinputs, but justification of these selections are necessary.

- For all estimation processes, authors should provide seed so that users could reproduce the results under the same seed. This is important for many cases, including but not limited to debugging, model development etc. 

- Data cleaning process should be explained better. Authors should add at least two parts. The first is to explain what steps authors have taken in get_data function to change from the original data to the data ready for get inputs. This includes dropping NaN, renaming, data filtering (dplyr::filter(n_Commercial_medical>=10 | n_Public>=10 | n_Other>=10)), etc. This should be important for users to test their own data and should be described at least in the Appendix. The second is that: the users should provide some examples in the paper that could clearly shows different output of running some codes in case study. For example, in case study 1, after the code snippet cleaned_natdata <- get_data(national=TRUE), authors should print the cleanedd_natdata (include/exclude the args could both work). In this way, this paper will be more self-sufficient, as currently readers of that need to set up the R environment and run these code snippets to understand the story from the authors.

- For the model estimation, some summary functions to the jags output should be provided. This could be used to diagnose the convergence evaluation, initial value dependency, acceptance rate and thinning etcs. Also some tests (such as raftery.test) should be conducted and it is better to embed them in this package. Right now, we could only use the plot to evaluate that without any statistical testing. It is thus not clear why we need to choose the certain number of iterations, burnin and thinning. Also, some trace plots or some other plotting to diagnose the results should be provided.

- Lastly some coding logics could be improved. For example, in the internal_functions.R, we see multiple times of else {next}. This seems to be useless (as the if-statement finishes without else, it will automatically go to the next loop.) Also, for some functions such as region_index_fun, authors should use which function in R to generate more readable code. (Such as for function sector_index_fun, we could write something like my_data$index_sector[j] <- which(my_sectors == my_data$sector_category[j]) to get same results with one single for-loop).


