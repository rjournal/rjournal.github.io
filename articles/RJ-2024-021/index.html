<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { color: #00769e; background-color: #f1f3f5; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #00769e; } /* Normal */
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #657422; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #00769e; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #00769e; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #00769e; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #5e5e5e; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

<style>
  div.csl-bib-body { }
  div.csl-entry {
    clear: both;
      margin-bottom: 0em;
    }
  .hanging div.csl-entry {
    margin-left:2em;
    text-indent:-2em;
  }
  div.csl-left-margin {
    min-width:2em;
    float:left;
  }
  div.csl-right-inline {
    margin-left:2em;
    padding-left:1em;
  }
  div.csl-indent {
    margin-left: 2em;
  }
</style>

  <!--radix_placeholder_meta_tags-->
<title>Pomdp: A Computational Infrastructure for Partially Observable Markov Decision Processes</title>

<meta property="description" itemprop="description" content="Many important problems involve decision-making under uncertainty. For example, a medical professional needs to make decisions about  the best treatment option based on limited information about the  current state of the patient and uncertainty about outcomes. Different approaches have been developed by the applied mathematics, operations research, and artificial intelligence communities to address  this difficult class of decision-making problems. This paper presents the pomdp package, which provides a computational infrastructure for an approach called the partially  observable Markov decision process (POMDP), which models the problem as a discrete-time stochastic control process. The package lets the user specify POMDPs using familiar R syntax, apply state-of-the-art POMDP solvers, and then take full advantage of R&#39;s range of capabilities,  including statistical analysis, simulation, and visualization, to work with the resulting models."/>

<link rel="canonical" href="https://doi.org/10.32614/RJ-2024-021/"/>
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>
<link rel="icon" type="image/vnd.microsoft.icon" href="../../resources/favicon.ico"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2025-03-08"/>
<meta property="article:created" itemprop="dateCreated" content="2025-03-08"/>
<meta name="article:author" content="Michael Hahsler"/>
<meta name="article:author" content="Anthony R. Cassandra"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="Pomdp: A Computational Infrastructure for Partially Observable Markov Decision Processes"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="Many important problems involve decision-making under uncertainty. For example, a medical professional needs to make decisions about  the best treatment option based on limited information about the  current state of the patient and uncertainty about outcomes. Different approaches have been developed by the applied mathematics, operations research, and artificial intelligence communities to address  this difficult class of decision-making problems. This paper presents the pomdp package, which provides a computational infrastructure for an approach called the partially  observable Markov decision process (POMDP), which models the problem as a discrete-time stochastic control process. The package lets the user specify POMDPs using familiar R syntax, apply state-of-the-art POMDP solvers, and then take full advantage of R&#39;s range of capabilities,  including statistical analysis, simulation, and visualization, to work with the resulting models."/>
<meta property="og:url" content="https://doi.org/10.32614/RJ-2024-021/"/>
<meta property="og:image" content="https://journal.r-project.org/articles/RJ-2024-021/RJ-2024-021_files/figure-html5/tiger-transition-1.png"/>
<meta property="og:image:width" content="1536"/>
<meta property="og:image:height" content="960"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="The R Journal"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary_large_image"/>
<meta property="twitter:title" content="Pomdp: A Computational Infrastructure for Partially Observable Markov Decision Processes"/>
<meta property="twitter:description" content="Many important problems involve decision-making under uncertainty. For example, a medical professional needs to make decisions about  the best treatment option based on limited information about the  current state of the patient and uncertainty about outcomes. Different approaches have been developed by the applied mathematics, operations research, and artificial intelligence communities to address  this difficult class of decision-making problems. This paper presents the pomdp package, which provides a computational infrastructure for an approach called the partially  observable Markov decision process (POMDP), which models the problem as a discrete-time stochastic control process. The package lets the user specify POMDPs using familiar R syntax, apply state-of-the-art POMDP solvers, and then take full advantage of R&#39;s range of capabilities,  including statistical analysis, simulation, and visualization, to work with the resulting models."/>
<meta property="twitter:url" content="https://doi.org/10.32614/RJ-2024-021/"/>
<meta property="twitter:image" content="https://journal.r-project.org/articles/RJ-2024-021/RJ-2024-021_files/figure-html5/tiger-transition-1.png"/>
<meta property="twitter:image:width" content="1536"/>
<meta property="twitter:image:height" content="960"/>

<!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
<meta name="citation_title" content="Pomdp: A Computational Infrastructure for Partially Observable Markov Decision Processes"/>
<meta name="citation_fulltext_html_url" content="https://doi.org/10.32614/RJ-2024-021"/>
<meta name="citation_pdf_url" content="RJ-2024-021.pdf"/>
<meta name="citation_volume" content="16"/>
<meta name="citation_issue" content="2"/>
<meta name="citation_doi" content="10.32614/RJ-2024-021"/>
<meta name="citation_journal_title" content="The R Journal"/>
<meta name="citation_issn" content="2073-4859"/>
<meta name="citation_firstpage" content="116"/>
<meta name="citation_lastpage" content="133"/>
<meta name="citation_fulltext_world_readable" content=""/>
<meta name="citation_online_date" content="2025/03/08"/>
<meta name="citation_publication_date" content="2025/03/08"/>
<meta name="citation_author" content="Michael Hahsler"/>
<meta name="citation_author_institution" content="Southern Methodist University"/>
<meta name="citation_author" content="Anthony R. Cassandra"/>
<meta name="citation_author_institution" content="POMDP, Inc"/>
<!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Optimal control of Markov processes with incomplete state information;citation_volume=10;citation_issn=0022-247X;citation_author=K. J Åström"/>
  <meta name="citation_reference" content="citation_title=APPL: Approximate POMDP planning toolkit;citation_author=APPL: Approximate POMDP planning toolkit"/>
  <meta name="citation_reference" content="citation_title=ZMDP: Software for POMDP and MDP planning;citation_author=T. Smith"/>
  <meta name="citation_reference" content="citation_title=pyPOMDP: POMDP implementation in Python;citation_author=Bastian Migge;citation_author=Oliver Stollmann"/>
  <meta name="citation_reference" content="citation_title=JuliaPOMDP: POMDP packages for Julia;citation_author=JuliaPOMDP: POMDP packages for Julia"/>
  <meta name="citation_reference" content="citation_title=The POMDP page;citation_author=Anthony R. Cassandra"/>
  <meta name="citation_reference" content="citation_title=Exact and approximate algorithms for partially observable markov decision processes;citation_publisher=Brown University;citation_author=Anthony R. Cassandra"/>
  <meta name="citation_reference" content="citation_title=Planning and acting in partially observable stochastic domains;citation_volume=101;citation_doi=10.1016/S0004-3702(98)00023-X;citation_issn=0004-3702;citation_author=Leslie Pack Kaelbling;citation_author=Michael L. Littman;citation_author=Anthony R. Cassandra"/>
  <meta name="citation_reference" content="citation_title=Acting optimally in partially observable stochastic domains;citation_author=Anthony R. Cassandra;citation_author=Leslie Pack Kaelbling;citation_author=Michael L. Littman"/>
  <meta name="citation_reference" content="citation_title=The optimal control of partially observable Markov decision processes;citation_publisher=Stanford, California;citation_author=E. J. Sondik"/>
  <meta name="citation_reference" content="citation_title=The optimal control of partially observable Markov decision processes over a finite horizon;citation_volume=21;citation_author=R. D. Smallwood;citation_author=E. J. Sondik"/>
  <meta name="citation_reference" content="citation_title=A survey of POMDP applications;citation_publisher=Microelectronics; Computer Technology Corporation (MCC);citation_number=MCC-INSL-111-98;citation_author=Anthony R. Cassandra"/>
  <meta name="citation_reference" content="citation_title=Incremental pruning: A simple, fast, exact method for partially observable Markov decision processes;citation_author=Anthony R. Cassandra;citation_author=Michael L. Littman;citation_author=Nevin Lianwen Zhang"/>
  <meta name="citation_reference" content="citation_title=Learning policies for partially observable environments: Scaling up;citation_publisher=Morgan Kaufmann Publishers Inc.;citation_author=Michael L. Littman;citation_author=Anthony R. Cassandra;citation_author=Leslie Pack Kaelbling"/>
  <meta name="citation_reference" content="citation_title=Point-based value iteration: An anytime algorithm for POMDPs;citation_publisher=Morgan Kaufmann Publishers Inc.;citation_author=Joelle Pineau;citation_author=Geoff Gordon;citation_author=Sebastian Thrun"/>
  <meta name="citation_reference" content="citation_title=Planning in stochastic domains: Problem characteristics and approximation;citation_publisher=Hong Kong University;citation_number=HKUST-CS96-31;citation_author=Nevin L. Zhang;citation_author=Wenju Liu"/>
  <meta name="citation_reference" content="citation_title=SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces;citation_author=Hanna Kurniawati;citation_author=David Hsu;citation_author=Wee Sun Lee"/>
  <meta name="citation_reference" content="citation_title=Sarsop: Approximate POMDP planning software;citation_author=Carl Boettiger;citation_author=Jeroen Ooms;citation_author=Milad Memarzadeh"/>
  <meta name="citation_reference" content="citation_title=Pomdp: Infrastructure for partially observable markov decision processes (POMDP);citation_author=Michael Hahsler"/>
  <meta name="citation_reference" content="citation_title=pomdpSolve: Interface to ’pomdp-solve’ for partially observable markov decision processes;citation_author=Michael Hahsler;citation_author=Anthony R. Cassandra"/>
  <meta name="citation_reference" content="citation_title=Matrix: Sparse and dense matrix classes and methods;citation_author=Douglas Bates;citation_author=Martin Maechler;citation_author=Mikael Jagan"/>
  <meta name="citation_reference" content="citation_title=A survey of partially observable Markov decision processes: Theory, models, and algorithms;citation_volume=28;citation_author=G. E. Monahan"/>
  <meta name="citation_reference" content="citation_title=Reinforcement learning: An introduction;citation_publisher=The MIT Press;citation_author=Richard S. Sutton;citation_author=Andrew G. Barto"/>
  <meta name="citation_reference" content="citation_title=An analytics-driven approach for optimal individualized diabetes screening;citation_volume=30;citation_doi=10.1111/poms.13422;citation_issn=1937-5956;citation_author=Farzad Kamalzadeh;citation_author=Vishal Ahuja;citation_author=Michael Hahsler;citation_author=Michael E. Bowen"/>
  <meta name="citation_reference" content="citation_title=Markov decision processes: Discrete stochastic dynamic programming;citation_author=Martin L. Puterman"/>
  <meta name="citation_reference" content="citation_title=ReinforcementLearning: Model-free reinforcement learning;citation_author=Nicolas Proellochs;citation_author=Stefan Feuerriegel"/>
  <meta name="citation_reference" content="citation_title=The igraph software package for complex network research;citation_volume=Complex Systems;citation_author=Gabor Csardi;citation_author=Tamas Nepusz"/>
  <meta name="citation_reference" content="citation_title=ForceAtlas2, a continuous graph layout algorithm for handy network visualization designed for the Gephi software;citation_publisher=Public Library of Science;citation_volume=9;citation_doi=10.1371/journal.pone.0098679;citation_author=Mathieu Jacomy;citation_author=Tommaso Venturini;citation_author=Sebastien Heymann;citation_author=Mathieu Bastian"/>
  <meta name="citation_reference" content="citation_title=Value-function approximations for POMDPs;citation_volume=13;citation_doi=https://doi.org/10.1613/jair.678;citation_author=Milos Hauskrecht"/>
  <meta name="citation_reference" content="citation_title=Seamless R and C++ integration with Rcpp;citation_publisher=Springer;citation_doi=10.1007/978-1-4614-6868-4;citation_author=Dirk Eddelbuettel"/>
  <meta name="citation_reference" content="citation_title=Foreach: Provides foreach looping construct;citation_author=Steve Weston;citation_author=Foreach: Provides foreach looping construct"/>
  <meta name="citation_reference" content="citation_title=visNetwork: Network visualization using ’vis.js’ library;citation_author=Benoit Thieurmel;citation_author=visNetwork: Network visualization using ’vis.js’ library"/>
  <meta name="citation_reference" content="citation_title=ggplot2: Elegant graphics for data analysis;citation_publisher=Springer-Verlag New York;citation_author=Hadley Wickham"/>
  <meta name="citation_reference" content="citation_title=The complexity of optimal small policies;citation_publisher=INFORMS;citation_volume=25;citation_issn=0364-765X;citation_author=Martin Mundhenk"/>
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","date","description","author","type","output","bibliography","date_received","volume","issue","slug","draft","journal","pdf_url","citation_url","doi","creative_commons","packages","CTV","csl","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["Pomdp: A Computational Infrastructure for Partially Observable Markov Decision Processes"]},{"type":"character","attributes":{},"value":["2025-03-08"]},{"type":"character","attributes":{},"value":["Many important problems involve decision-making under uncertainty. For example, a medical professional needs to make decisions about  the best treatment option based on limited information about the  current state of the patient and uncertainty about outcomes. Different approaches have been developed by the applied mathematics, operations research, and artificial intelligence communities to address  this difficult class of decision-making problems. This paper presents the pomdp package, which provides a computational infrastructure for an approach called the partially  observable Markov decision process (POMDP), which models the problem as a discrete-time stochastic control process. The package lets the user specify POMDPs using familiar R syntax, apply state-of-the-art POMDP solvers, and then take full advantage of R's range of capabilities,  including statistical analysis, simulation, and visualization, to work with the resulting models."]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","address","url","orcid_id","email"]}},"value":[{"type":"character","attributes":{},"value":["Michael Hahsler"]},{"type":"character","attributes":{},"value":["Southern Methodist University"]},{"type":"character","attributes":{},"value":["Department of Computer Science","Dallas, TX, USA"]},{"type":"character","attributes":{},"value":["https://michael.hahsler.net"]},{"type":"character","attributes":{},"value":["0000-0003-2716-1405"]},{"type":"character","attributes":{},"value":["mhahsler@lyle.smu.edu"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url","email","affiliation","address"]}},"value":[{"type":"character","attributes":{},"value":["Anthony R. Cassandra"]},{"type":"character","attributes":{},"value":["https://tonycassandra.com"]},{"type":"character","attributes":{},"value":["tony.cassandra@gmail.com"]},{"type":"character","attributes":{},"value":["POMDP, Inc"]},{"type":"character","attributes":{},"value":["Austin, TX, USA"]}]}]},{"type":"character","attributes":{},"value":["package"]},{"type":"character","attributes":{},"value":["rjtools::rjournal_article"]},{"type":"character","attributes":{},"value":["pomdp.bib"]},{"type":"character","attributes":{},"value":["2023-09-24"]},{"type":"integer","attributes":{},"value":[16]},{"type":"integer","attributes":{},"value":[2]},{"type":"character","attributes":{},"value":["RJ-2024-021"]},{"type":"logical","attributes":{},"value":[false]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","issn","firstpage","lastpage"]}},"value":[{"type":"character","attributes":{},"value":["The R Journal"]},{"type":"character","attributes":{},"value":["2073-4859"]},{"type":"integer","attributes":{},"value":[116]},{"type":"integer","attributes":{},"value":[133]}]},{"type":"character","attributes":{},"value":["RJ-2024-021.pdf"]},{"type":"character","attributes":{},"value":["https://doi.org/10.32614/RJ-2024-021"]},{"type":"character","attributes":{},"value":["10.32614/RJ-2024-021"]},{"type":"character","attributes":{},"value":["CC BY"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["cran","bioc"]}},"value":[{"type":"character","attributes":{},"value":["ReinforcementLearning","sarsop","pomdpSolve","pomdp","Matrix","Rcpp","foreach","igraph","visNetwork","ggplot2"]},{"type":"list","attributes":{},"value":[]}]},{"type":"character","attributes":{},"value":["ChemPhys","DynamicVisualizations","Econometrics","GraphicalModels","HighPerformanceComputing","NetworkAnalysis","NumericalMathematics","Optimization","Phylogenetics","Spatial","TeachingStatistics"]},{"type":"character","attributes":{},"value":["/home/mitchell/R/x86_64-pc-linux-gnu-library/4.4/rjtools/rjournal.csl"]},{"type":"character","attributes":{},"value":["https://doi.org/10.32614/RJ-2024-021/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["pomdp_files/figure-html5.zip","pomdp_files/figure-html5/tiger-finite-horizon-1.png","pomdp_files/figure-html5/tiger-finite-horizon-99-1.png","pomdp_files/figure-html5/tiger-infinite-horizon-1.png","pomdp_files/figure-html5/tiger-simulation-1.png","pomdp_files/figure-html5/tiger-transition-1.png","pomdp_files/figure-html5/tiger-value-function-1.png","pomdp_files/figure-latex/tiger-finite-horizon-1.png","pomdp_files/figure-latex/tiger-finite-horizon-99-1.png","pomdp_files/figure-latex/tiger-infinite-horizon-1.png","pomdp_files/figure-latex/tiger-simulation-1.png","pomdp_files/figure-latex/tiger-transition-1.png","pomdp_files/figure-latex/tiger-value-function-1.png","pomdp.bib","pomdp.tex","RJ-2024-021_files/anchor-4.2.2/anchor.min.js","RJ-2024-021_files/bowser-1.9.3/bowser.min.js","RJ-2024-021_files/distill-2.2.21/template.v2.js","RJ-2024-021_files/figure-html5/tiger-finite-horizon-1.png","RJ-2024-021_files/figure-html5/tiger-finite-horizon-99-1.png","RJ-2024-021_files/figure-html5/tiger-infinite-horizon-1.png","RJ-2024-021_files/figure-html5/tiger-simulation-1.png","RJ-2024-021_files/figure-html5/tiger-transition-1.png","RJ-2024-021_files/figure-html5/tiger-value-function-1.png","RJ-2024-021_files/header-attrs-2.29/header-attrs.js","RJ-2024-021_files/jquery-3.6.0/jquery-3.6.0.js","RJ-2024-021_files/jquery-3.6.0/jquery-3.6.0.min.js","RJ-2024-021_files/jquery-3.6.0/jquery-3.6.0.min.map","RJ-2024-021_files/popper-2.6.0/popper.min.js","RJ-2024-021_files/tippy-6.2.7/tippy-bundle.umd.min.js","RJ-2024-021_files/tippy-6.2.7/tippy-light-border.css","RJ-2024-021_files/tippy-6.2.7/tippy.css","RJ-2024-021_files/tippy-6.2.7/tippy.umd.min.js","RJ-2024-021_files/webcomponents-2.0.0/webcomponents.js","RJ-2024-021.log","RJ-2024-021.tex","RJ-2024-021.zip","RJournal.sty","RJwrapper.log","RJwrapper.tex"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      tolerance: 5,
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

/* adjust viewport for navbar height */
/* helps vertically center bootstrap (non-distill) content */
.min-vh-100 {
  min-height: calc(100vh - 100px) !important;
}

</style>

<script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="../../site_libs/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
  font-size: 100%;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

hr.section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  margin: 0px;
}


d-byline {
  border-top: none;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
  border-top: none;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: auto;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

d-article details {
  grid-column: text;
  margin-bottom: 0.8em;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

/* tweak for Pandoc numbered line within distill */
d-article pre.numberSource code > span {
    left: -2em;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}

/* Citation hover box */

.tippy-box[data-theme~=light-border] {
  background-color: rgba(250, 250, 250, 0.95);
}

.tippy-content > p {
  margin-bottom: 0;
  padding: 2px;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}

/* Styles for listing layout (hide title) */
.layout-listing d-title, .layout-listing .d-title {
  display: none;
}

/* Styles for posts lists (not auto-injected) */


.posts-with-sidebar {
  padding-left: 45px;
  padding-right: 45px;
}

.posts-list .description h2,
.posts-list .description p {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

.posts-list .description h2 {
  font-weight: 700;
  border-bottom: none;
  padding-bottom: 0;
}

.posts-list h2.post-tag {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  padding-bottom: 12px;
}
.posts-list {
  margin-top: 60px;
  margin-bottom: 24px;
}

.posts-list .post-preview {
  text-decoration: none;
  overflow: hidden;
  display: block;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding: 24px 0;
}

.post-preview-last {
  border-bottom: none !important;
}

.posts-list .posts-list-caption {
  grid-column: screen;
  font-weight: 400;
}

.posts-list .post-preview h2 {
  margin: 0 0 6px 0;
  line-height: 1.2em;
  font-style: normal;
  font-size: 24px;
}

.posts-list .post-preview p {
  margin: 0 0 12px 0;
  line-height: 1.4em;
  font-size: 16px;
}

.posts-list .post-preview .thumbnail {
  box-sizing: border-box;
  margin-bottom: 24px;
  position: relative;
  max-width: 500px;
}
.posts-list .post-preview img {
  width: 100%;
  display: block;
}

.posts-list .metadata {
  font-size: 12px;
  line-height: 1.4em;
  margin-bottom: 18px;
}

.posts-list .metadata > * {
  display: inline-block;
}

.posts-list .metadata .publishedDate {
  margin-right: 2em;
}

.posts-list .metadata .dt-authors {
  display: block;
  margin-top: 0.3em;
  margin-right: 2em;
}

.posts-list .dt-tags {
  display: block;
  line-height: 1em;
}

.posts-list .dt-tags .dt-tag {
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0.3em 0.4em;
  margin-right: 0.2em;
  margin-bottom: 0.4em;
  font-size: 60%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

.posts-list img {
  opacity: 1;
}

.posts-list img[data-src] {
  opacity: 0;
}

.posts-more {
  clear: both;
}


.posts-sidebar {
  font-size: 16px;
}

.posts-sidebar h3 {
  font-size: 16px;
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 400;
  text-transform: uppercase;
}

.sidebar-section {
  margin-bottom: 30px;
}

.categories ul {
  list-style-type: none;
  margin: 0;
  padding: 0;
}

.categories li {
  color: rgba(0, 0, 0, 0.8);
  margin-bottom: 0;
}

.categories li>a {
  border-bottom: none;
}

.categories li>a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
}

.categories .active {
  font-weight: 600;
}

.categories .category-count {
  color: rgba(0, 0, 0, 0.4);
}


@media(min-width: 768px) {
  .posts-list .post-preview h2 {
    font-size: 26px;
  }
  .posts-list .post-preview .thumbnail {
    float: right;
    width: 30%;
    margin-bottom: 0;
  }
  .posts-list .post-preview .description {
    float: left;
    width: 45%;
  }
  .posts-list .post-preview .metadata {
    float: left;
    width: 20%;
    margin-top: 8px;
  }
  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.5em;
    font-size: 16px;
  }
  .posts-with-sidebar .posts-list {
    float: left;
    width: 75%;
  }
  .posts-with-sidebar .posts-sidebar {
    float: right;
    width: 20%;
    margin-top: 60px;
    padding-top: 24px;
    padding-bottom: 24px;
  }
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

.downlevel .posts-list .post-preview {
  color: inherit;
}



</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // separator
  var separator = '<hr class="section-separator" style="clear: both"/>';
  // prepend separator above appendix
  $('.d-byline').before(separator);
  $('.d-article').before(separator);

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('details, div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme, except when numbering line
  // in code chunk
  $('pre:not(.numberLines) code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      var author_name = front_matter.authors[i].author
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', author_name ? 'ORCID ID for ' + author_name : 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // hoverable references
    $('span.citation[data-cites]').each(function() {
      const citeChild = $(this).children()[0]
      // Do not process if @xyz has been used without escaping and without bibliography activated
      // https://github.com/rstudio/distill/issues/466
      if (citeChild === undefined) return true

      if (citeChild.nodeName == "D-FOOTNOTE") {
        var fn = citeChild
        $(this).html(fn.shadowRoot.querySelector("sup"))
        $(this).id = fn.id
        fn.remove()
      }
      var refs = $(this).attr('data-cites').split(" ");
      var refHtml = refs.map(function(ref) {
        // Could use CSS.escape too here, we insure backward compatibility in navigator
        return "<p>" + $('div[id="ref-' + ref + '"]').html() + "</p>";
      }).join("\n");
      window.tippy(this, {
        allowHTML: true,
        content: refHtml,
        maxWidth: 500,
        interactive: true,
        interactiveBorder: 10,
        theme: 'light-border',
        placement: 'bottom-start'
      });
    });

    // fix footnotes in tables (#411)
    // replacing broken distill.pub feature
    $('table d-footnote').each(function() {
      // we replace internal showAtNode methode which is triggered when hovering a footnote
      this.hoverBox.showAtNode = function(node) {
        // ported from https://github.com/distillpub/template/pull/105/files
        calcOffset = function(elem) {
            let x = elem.offsetLeft;
            let y = elem.offsetTop;
            // Traverse upwards until an `absolute` element is found or `elem`
            // becomes null.
            while (elem = elem.offsetParent && elem.style.position != 'absolute') {
                x += elem.offsetLeft;
                y += elem.offsetTop;
            }

            return { left: x, top: y };
        }
        // https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetTop
        const bbox = node.getBoundingClientRect();
        const offset = calcOffset(node);
        this.show([offset.left + bbox.width, offset.top + bbox.height]);
      }
    })

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    // ignore leaflet img layers (#106)
    figures = figures.filter(':not(img[class*="leaflet"])')
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace(/^index[.]html/, "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<style type="text/css">
/* base variables */

/* Edit the CSS properties in this file to create a custom
   Distill theme. Only edit values in the right column
   for each row; values shown are the CSS defaults.
   To return any property to the default,
   you may set its value to: unset
   All rows must end with a semi-colon.                      */

/* Optional: embed custom fonts here with `@import`          */
/* This must remain at the top of this file.                 */



html {
  /*-- Main font sizes --*/
  --title-size:      50px;
  --body-size:       1.06rem;
  --code-size:       14px;
  --aside-size:      12px;
  --fig-cap-size:    13px;
  /*-- Main font colors --*/
  --title-color:     #000000;
  --header-color:    rgba(0, 0, 0, 0.8);
  --body-color:      rgba(0, 0, 0, 0.8);
  --aside-color:     rgba(0, 0, 0, 0.6);
  --fig-cap-color:   rgba(0, 0, 0, 0.6);
  /*-- Specify custom fonts ~~~ must be imported above   --*/
  --heading-font:    sans-serif;
  --mono-font:       monospace;
  --body-font:       sans-serif;
  --navbar-font:     sans-serif;  /* websites + blogs only */
}

/*-- ARTICLE METADATA --*/
d-byline {
  --heading-size:    0.6rem;
  --heading-color:   rgba(0, 0, 0, 0.5);
  --body-size:       0.8rem;
  --body-color:      rgba(0, 0, 0, 0.8);
}

/*-- ARTICLE TABLE OF CONTENTS --*/
.d-contents {
  --heading-size:    18px;
  --contents-size:   13px;
}

/*-- ARTICLE APPENDIX --*/
d-appendix {
  --heading-size:    15px;
  --heading-color:   rgba(0, 0, 0, 0.65);
  --text-size:       0.8em;
  --text-color:      rgba(0, 0, 0, 0.5);
}

/*-- WEBSITE HEADER + FOOTER --*/
/* These properties only apply to Distill sites and blogs  */

.distill-site-header {
  --title-size:       18px;
  --text-color:       rgba(255, 255, 255, 0.8);
  --text-size:        15px;
  --hover-color:      white;
  --bkgd-color:       #0F2E3D;
}

.distill-site-footer {
  --text-color:       rgba(255, 255, 255, 0.8);
  --text-size:        15px;
  --hover-color:      white;
  --bkgd-color:       #0F2E3D;
}

/*-- Additional custom styles --*/
/* Add any additional CSS rules below                      */
</style>
<style type="text/css">
/* base variables */

/* Edit the CSS properties in this file to create a custom
   Distill theme. Only edit values in the right column
   for each row; values shown are the CSS defaults.
   To return any property to the default,
   you may set its value to: unset
   All rows must end with a semi-colon.                      */

/* Optional: embed custom fonts here with `@import`          */
/* This must remain at the top of this file.                 */



html {
  /*-- Main font sizes --*/
  --title-size:      50px;
  --body-size:       1.06rem;
  --code-size:       14px;
  --aside-size:      12px;
  --fig-cap-size:    13px;
  /*-- Main font colors --*/
  --title-color:     #000000;
  --header-color:    rgba(0, 0, 0, 0.8);
  --body-color:      rgba(0, 0, 0, 0.8);
  --aside-color:     rgba(0, 0, 0, 0.6);
  --fig-cap-color:   rgba(0, 0, 0, 0.6);
  /*-- Specify custom fonts ~~~ must be imported above   --*/
  --heading-font:    sans-serif;
  --mono-font:       monospace;
  --body-font:       sans-serif;
  --navbar-font:     sans-serif;  /* websites + blogs only */
}

/*-- ARTICLE METADATA --*/
d-byline {
  --heading-size:    0.6rem;
  --heading-color:   rgba(0, 0, 0, 0.5);
  --body-size:       0.8rem;
  --body-color:      rgba(0, 0, 0, 0.8);
}

/*-- ARTICLE TABLE OF CONTENTS --*/
.d-contents {
  --heading-size:    18px;
  --contents-size:   13px;
}

/*-- ARTICLE APPENDIX --*/
d-appendix {
  --heading-size:    15px;
  --heading-color:   rgba(0, 0, 0, 0.65);
  --text-size:       0.8em;
  --text-color:      rgba(0, 0, 0, 0.7);
}

/*-- WEBSITE HEADER + FOOTER --*/
/* These properties only apply to Distill sites and blogs  */

.distill-site-header {
  --title-size:       18px;
  --text-color:       rgba(0, 0, 0, 0.8);
  --text-size:        15px;
  --hover-color:      black;
  --bkgd-color:       #ffffff;
}

.distill-site-footer {
  --text-color:       rgba(0, 0, 0, 0.8);
  --text-size:        15px;
  --hover-color:      black;
  --bkgd-color:       #fafafa;
}

/*-- Additional custom styles --*/
/* Add any additional CSS rules below                      */

.nav-right > a {
  text-transform: uppercase;
}

d-title h1, d-title p, d-title figure,
d-abstract p, d-abstract b {
  grid-column: page;
}

.rj-blue {
  color: #2467bb;
}

ul li {
  line-height: 1.6;
  margin-top: 0em;
  margin-bottom: 0em;
}</style>
<style type="text/css">
/* base style */

/* FONT FAMILIES */

:root {
  --heading-default: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  --mono-default: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  --body-default: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

body,
.posts-list .post-preview p,
.posts-list .description p {
  font-family: var(--body-font), var(--body-default);
}

h1, h2, h3, h4, h5, h6,
.posts-list .post-preview h2,
.posts-list .description h2 {
  font-family: var(--heading-font), var(--heading-default);
}

d-article div.sourceCode code,
d-article pre code {
  font-family: var(--mono-font), var(--mono-default);
}


/*-- TITLE --*/
d-title h1,
.posts-list > h1 {
  color: var(--title-color, black);
}

d-title h1 {
  font-size: var(--title-size, 50px);
}

/*-- HEADERS --*/
d-article h1,
d-article h2,
d-article h3,
d-article h4,
d-article h5,
d-article h6 {
  color: var(--header-color, rgba(0, 0, 0, 0.8));
}

/*-- BODY --*/
d-article > p,  /* only text inside of <p> tags */
d-article > ul, /* lists */
d-article > ol {
  color: var(--body-color, rgba(0, 0, 0, 0.8));
  font-size: var(--body-size, 1.06rem);
}


/*-- CODE --*/
d-article div.sourceCode code,
d-article pre code {
  font-size: var(--code-size, 14px);
}

/*-- ASIDE --*/
d-article aside {
  font-size: var(--aside-size, 12px);
  color: var(--aside-color, rgba(0, 0, 0, 0.6));
}

/*-- FIGURE CAPTIONS --*/
figure .caption,
figure figcaption,
.figure .caption {
  font-size: var(--fig-cap-size, 13px);
  color: var(--fig-cap-color, rgba(0, 0, 0, 0.6));
}

/*-- METADATA --*/
d-byline h3 {
  font-size: var(--heading-size, 0.6rem);
  color: var(--heading-color, rgba(0, 0, 0, 0.5));
}

d-byline {
  font-size: var(--body-size, 0.8rem);
  color: var(--body-color, rgba(0, 0, 0, 0.8));
}

d-byline a,
d-article d-byline a {
  color: var(--body-color, rgba(0, 0, 0, 0.8));
}

/*-- TABLE OF CONTENTS --*/
.d-contents nav h3 {
  font-size: var(--heading-size, 18px);
}

.d-contents nav a {
  font-size: var(--contents-size, 13px);
}

/*-- APPENDIX --*/
d-appendix h3 {
  font-size: var(--heading-size, 15px);
  color: var(--heading-color, rgba(0, 0, 0, 0.65));
}

d-appendix {
  font-size: var(--text-size, 0.8em);
  color: var(--text-color, rgba(0, 0, 0, 0.5));
}

d-appendix d-footnote-list a.footnote-backlink {
  color: var(--text-color, rgba(0, 0, 0, 0.5));
}

/*-- WEBSITE HEADER + FOOTER --*/
.distill-site-header .title {
  font-size: var(--title-size, 18px);
  font-family: var(--navbar-font), var(--heading-default);
}

.distill-site-header a,
.nav-dropdown .nav-dropbtn {
  font-family: var(--navbar-font), var(--heading-default);
}

.nav-dropdown .nav-dropbtn {
  color: var(--text-color, rgba(255, 255, 255, 0.8));
  font-size: var(--text-size, 15px);
}

.distill-site-header a:hover,
.nav-dropdown:hover .nav-dropbtn {
  color: var(--hover-color, white);
}

.distill-site-header {
  font-size: var(--text-size, 15px);
  color: var(--text-color, rgba(255, 255, 255, 0.8));
  background-color: var(--bkgd-color, #0F2E3D);
}

.distill-site-footer {
  font-size: var(--text-size, 15px);
  color: var(--text-color, rgba(255, 255, 255, 0.8));
  background-color: var(--bkgd-color, #0F2E3D);
}

.distill-site-footer a:hover {
  color: var(--hover-color, white);
}</style>
<!--/radix_placeholder_distill-->
  <script src="../../site_libs/header-attrs-2.29/header-attrs.js"></script>
  <script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="../../site_libs/popper-2.6.0/popper.min.js"></script>
  <link href="../../site_libs/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="../../site_libs/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="../../site_libs/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="../../site_libs/anchor-4.2.2/anchor.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<!--/radix_placeholder_site_in_header-->
  <script>
    $(function() {
      console.log("Starting...")

      // Mathjax config (add automatic linebreaks when supported)
      // MathJax = {
      //    tex: {
      //        inlineMath: [['$', '$'], ['\\(', '\\)']],
      //        displayMath: [['$$', '$$'], ['\\[', '\\]']],
      //        tags: 'ams',
      //        multline: true,
      //    },
      //    options: {
      //        linebreaks: { automatic: true },
      //    },
      // };

      // Always show Published - distill hides it if not set
      function show_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'visible');
      }

      show_byline_column('Published')

      // tweak function
      var rmd_meta = JSON.parse($("#radix-rmarkdown-metadata").html());
      function get_meta(name, meta) {
        var ind = meta.attributes.names.value.findIndex((e) => e == name)
        var val = meta.value[ind]
        if (val.type != 'list') {
          return val.value.toString()
        }
        return val
      }

      // tweak description
      // Add clickable tags
      const slug = get_meta('slug', rmd_meta)
      const cite_url = get_meta('citation_url', rmd_meta)

      var title = $("d-title").text

      const buttons = $('<div class="dt-tags" style="grid-column: page;">')
      buttons.append('<a href="#citation" class="dt-tag"><i class="fas fa-quote-left"></i> Cite</a>')
      buttons.append('<a href="' + slug + '.pdf" class="dt-tag"><i class="fas fa-file-pdf"></i> PDF</a>')
      buttons.append('<a href="' + slug + '.zip" class="dt-tag"><i class="fas fa-file-zipper"></i> Supplement</a>')

      // adds Abstract: in front of the first <p> in the title section --
      // unless it happens to be the subtitle (FIXME: this is a bad hack - can't distill do this?)
      var tpar = $("d-title p:not(:empty)").filter(function() {
        return !$(this).hasClass("subtitle");
      }).first();
      if (tpar) {
        const abstract = $('<d-abstract>')
        abstract.append('<b>Abstract:</b><br>')
        abstract.append(tpar) // Move description to d-abstract
        $("d-title p:empty").remove() // Remove empty paragraphs after title
        abstract.append(buttons)
        abstract.insertAfter($('d-title')) // Add abstract section after title */
      }

      // tweak by-line
      var byline = $("d-byline div.byline")
      ind = rmd_meta.attributes.names.value.findIndex((e) => e == "journal")
      const journal = get_meta('journal', rmd_meta)
      const volume = get_meta('volume', rmd_meta)
      const issue = get_meta('issue', rmd_meta)
      const jrtitle = get_meta('title', journal)
      const year = ((jrtitle == "R News") ? 2000 : 2008) + parseInt(volume)
      const firstpage = get_meta('firstpage', journal)
      const lastpage = get_meta('lastpage', journal)
      byline.append('<div class="rjournal grid">')
      $('div.rjournal').append('<h3>Volume</h3>')
      $('div.rjournal').append('<h3>Pages</h3>')
      $('div.rjournal').append('<a class="volume" href="../../issues/'+year+'-'+issue+'">'+volume+'/'+issue+'</a>')
      $('div.rjournal').append('<p class="pages">'+firstpage+' - '+lastpage+'</p>')

      const received_date = new Date(get_meta('date_received', rmd_meta))
      byline.find('h3:contains("Published")').parent().append('<h3>Received</h3><p>'+received_date.toLocaleDateString('en-US', {month: 'short'})+' '+received_date.getDate()+', '+received_date.getFullYear()+'</p>')

    })
  </script>

  <style>
      /*
    .nav-dropdown-content .nav-dropdown-header {
      text-transform: lowercase;
    }
    */

    d-byline .byline {
      grid-template-columns: 2fr 2fr 2fr 2fr;
    }

    d-byline .rjournal {
      grid-column-end: span 2;
      grid-template-columns: 1fr 1fr;
      margin-bottom: 0;
    }

    d-title h1, d-title p, d-title figure,
    d-abstract p, d-abstract b {
      grid-column: page;
    }

    d-title .dt-tags {
      grid-column: page;
    }

    .dt-tags .dt-tag {
      text-transform: lowercase;
    }

    d-article h1 {
      line-height: 1.1em;
    }

    d-abstract p, d-article p {
      text-align: justify;
    }

    @media(min-width: 1000px) {
      .d-contents.d-contents-float {
        justify-self: end;
      }

      nav.toc {
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }

    .posts-list .dt-tags .dt-tag {
      text-transform: lowercase;
    }

    @keyframes highlight-target {
      0% {
        background-color: #ffa;
      }
      66% {
        background-color: #ffa;
      }
      100% {
        background-color: none;
      }
    }

    d-article :target, d-appendix :target {
       animation: highlight-target 3s;
    }

    .header-section-number {
      margin-right: 0.5em;
    }
    
    d-appendix .citation-appendix,
    .d-appendix .citation-appendix {
      color: rgb(60, 60, 60);
    }

    d-article h2 {
      border-bottom: 0px solid rgba(0, 0, 0, 0.1);
      padding-bottom: 0rem;
    }
    d-article h3 {
      font-size: 20px;
    }
    d-article h4 {
      font-size: 18px;
      text-transform: none;
    }

    @media (min-width: 1024px) {
      d-article h2 {
        font-size: 32px;
      }
      d-article h3 {
        font-size: 24px;
      }
      d-article h4 {
        font-size: 20px;
      }
    }
  </style>


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Pomdp: A Computational Infrastructure for Partially Observable Markov Decision Processes","description":"Many important problems involve decision-making under uncertainty. For example, a medical professional needs to make decisions about  the best treatment option based on limited information about the  current state of the patient and uncertainty about outcomes. Different approaches have been developed by the applied mathematics, operations research, and artificial intelligence communities to address  this difficult class of decision-making problems. This paper presents the pomdp package, which provides a computational infrastructure for an approach called the partially  observable Markov decision process (POMDP), which models the problem as a discrete-time stochastic control process. The package lets the user specify POMDPs using familiar R syntax, apply state-of-the-art POMDP solvers, and then take full advantage of R's range of capabilities,  including statistical analysis, simulation, and visualization, to work with the resulting models.","doi":"10.32614/RJ-2024-021","authors":[{"author":"Michael Hahsler","authorURL":"https://michael.hahsler.net","affiliation":"Southern Methodist University","affiliationURL":"#","orcidID":"0000-0003-2716-1405"},{"author":"Anthony R. Cassandra","authorURL":"https://tonycassandra.com","affiliation":"POMDP, Inc","affiliationURL":"#","orcidID":""}],"publishedDate":"2025-03-08T00:00:00.000+00:00","citationText":"Hahsler & Cassandra, 2025"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<a class="logo" href="../../index.html">
<img src="../../resources/rlogo.png" alt="Logo"/>
</a>
<a href="../../index.html" class="title">The R Journal</a>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../issues/2024-4">Current</a>
<a href="../../issues.html">Issues</a>
<a href="../../news.html">News</a>
<a href="../../submissions.html">Submit</a>
<a href="../../editors.html">Editorial board</a>
<a href="../../articles.xml">
<i class="fa fa-rss" aria-hidden="true"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Pomdp: A Computational Infrastructure for Partially Observable Markov Decision Processes</h1>

<!--radix_placeholder_categories-->
<!--/radix_placeholder_categories-->
<p><p>Many important problems involve decision-making under uncertainty. For example, a medical professional needs to make decisions about the best treatment option based on limited information about the current state of the patient and uncertainty about outcomes. Different approaches have been developed by the applied mathematics, operations research, and artificial intelligence communities to address this difficult class of decision-making problems. This paper presents the pomdp package, which provides a computational infrastructure for an approach called the partially observable Markov decision process (POMDP), which models the problem as a discrete-time stochastic control process. The package lets the user specify POMDPs using familiar R syntax, apply state-of-the-art POMDP solvers, and then take full advantage of R’s range of capabilities, including statistical analysis, simulation, and visualization, to work with the resulting models.</p></p>
</div>

<div class="d-byline">
  Michael Hahsler <a href="https://michael.hahsler.net" class="uri">https://michael.hahsler.net</a> (Southern Methodist University)
  
,   Anthony R. Cassandra <a href="https://tonycassandra.com" class="uri">https://tonycassandra.com</a> (POMDP, Inc)
  
<br/>2025-03-08
</div>

<div class="d-article">
<h2 data-number="1" id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>Many important problems require decision-making without perfect information,
and where decisions made today will affect the future.
For example, in diabetes prevention and care, the primary care
provider needs to make decisions about screening, early interventions
like suggesting lifestyle modification, and eventually
medication for disease management
based on a patient’s available medical history. Especially,
screening and lifestyle modifications need to be used early on to
be effective in preventing severe and debilitating diseases later on.
This is clearly a difficult problem that involves uncertainty and
requires a long-term view. We have studied this problem
using the partially observable Markov decision process approach in
<span class="citation" data-cites="hahsler:Kamalzadeh:2021">(<a href="#ref-hahsler:Kamalzadeh:2021" role="doc-biblioref">Kamalzadeh et al. 2021</a>)</span> and, in the absence of solvers for R,
we started the development of the <code>pomdp</code>
package described in this paper.</p>
<p>A Markov decision process (MDP)
is a discrete-time stochastic control process that models
how an agent decides on what actions to take when facing an environment
whose dynamics can be adequately modeled by a Markov process that can be
affected by the agent’s behavior <span class="citation" data-cites="Puterman1994">(<a href="#ref-Puterman1994" role="doc-biblioref">Puterman 1994</a>)</span>.
That is, the environment transitions between a set of states
where transition probabilities only depend on the current state and
are conditioned on the agent’s actions.
Over time, the agent receives rewards depending on the actions and the
environment’s state. The agent’s objective is to make a plan that
maximizes its total reward earned.
A plan can be expressed as a mapping of each possible state
to the best action in that state.
The best possible plan is often called the optimal policy.
For MDP problems, the agent is always aware of
the state of the environment and can make decisions directly following such a policy.</p>
<p>A partially observable Markov decision process (POMDP) generalizes the
concept of the MDP to model more realistic situations where the agent
cannot directly observe the environment’s state.
Here, the agent must
infer the current state using observations that are only probabilistically linked to
the underlying state. The agent can form a belief about what states
it may be in and update its belief when new observations are made.
In this setting, the agent has to base its actions on its current belief.
A POMDP can be modeled as a <em>belief MDP</em> where the underlying Markov model uses belief states instead of the original states of the environment.
While the original state space is typically modeled as a finite set of states, making MDPs readily solvable using dynamic programming, the agent’s belief is represented by a probability distribution over the states in the form of a continuous probability simplex
and are therefore much more challenging to solve.
The volume of the believe space that POMDPs are operating in grows
exponentially with the number of underlying states.
This is called the curse of dimensionality which means that working with
problems with a
realistic number of states typically requires the use of approximate algorithms.</p>
<p>Karl Johan Åström first described Markov decision processes with
a discrete state space and imperfect information in 1965
<span class="citation" data-cites="Astrom1965">(<a href="#ref-Astrom1965" role="doc-biblioref">Åström 1965</a>)</span>. The model was also studied by the operations research community where the acronym POMDP was introduced <span class="citation" data-cites="Smallwood1973">(<a href="#ref-Smallwood1973" role="doc-biblioref">Smallwood and Sondik 1973</a>)</span>.
More recently, the POMDP framework was adapted for automated planning problems in artificial intelligence <span class="citation" data-cites="Kaelbling1998">(<a href="#ref-Kaelbling1998" role="doc-biblioref">Kaelbling et al. 1998</a>)</span>.
The POMDP framework is a popular choice when a known Markov process can adequately approximate system
dynamics and the reward function is known. POMDPs have been successfully applied to
model various real-world sequential decision processes.
Examples include numerous industrial, scientific, business, medical and military applications where an optimal or near-optimal policy is needed. This includes important applications like machine maintenance scheduling, computer vision, medical diagnosis, and many more. A detailed review of applications can be found in <span class="citation" data-cites="Cassandra1998b">(<a href="#ref-Cassandra1998b" role="doc-biblioref">Cassandra 1998a</a>)</span>.</p>
<p>While the (PO)MDP framework is used to find an optimal or near optimal policy, given
a model of system dynamics, the related class of model-free reinforcement learning algorithms, more specifically, temporal difference learning, Q-learning and its deep learning variations <span class="citation" data-cites="Sutton1998">(<a href="#ref-Sutton1998" role="doc-biblioref">Sutton and Barto 2018</a>)</span>,
learn unknown system dynamics and
the reward function directly from interactions with the environment.
Reinforcement learning methods typically require observable states and perform a large amount of exploration, where the agent performs sub-optimal actions to learn about the environment.
Q-learning and some algorithms are already available in R packages like <a href="https://cran.r-project.org/package=ReinforcementLearning">ReinforcementLearning</a> <span class="citation" data-cites="Proellochs2020">(<a href="#ref-Proellochs2020" role="doc-biblioref">Proellochs and Feuerriegel 2020</a>)</span>.
While these model-free approaches are very powerful for many artificial intelligence applications, they may not be appropriate
for situations where experts already possess a reasonable
amount of knowledge about the system dynamics and where
the cost of sub-optimal actions is very high. For example, the cost of administering the wrong medication in a medical setting
due to exploration by a pure reinforcement learning approach may not be acceptable and a model-based approach like a POMDP is more appropriate.
The R package described in this paper exclusively focuses on planning with POMDP.</p>
<p>While POMDPs are well studied, the complexity of solving all but very small problems
limits its application.
Recent spectacular advances in artificial intelligence applications have lead to more interest in POMDPs, as
shown in the development of new approximate algorithms and
by the frameworks available for various programming languages:</p>
<ul>
<li>pomdp-solve <span class="citation" data-cites="Cassandra2015">(<a href="#ref-Cassandra2015" role="doc-biblioref">Cassandra 2015</a>)</span> is a C program to solve POMDPs using exact and approximate solvers.</li>
<li>APPL <span class="citation" data-cites="APPL2022">(<a href="#ref-APPL2022" role="doc-biblioref">APPL Team 2022</a>)</span> provides the fast point-based POMDP solver SARSOP in C++.</li>
<li>ZMDP software <span class="citation" data-cites="ZMDP2009">(<a href="#ref-ZMDP2009" role="doc-biblioref">Smith 2009</a>)</span> implements several approximate value iteration algorithms in C++.</li>
<li>pyPOMDP <span class="citation" data-cites="pypomdp2013">(<a href="#ref-pypomdp2013" role="doc-biblioref">Migge and Stollmann 2013</a>)</span> is a Python 2.x toolbox for solving POMDPs.</li>
<li>JuliaPOMDP <span class="citation" data-cites="juliapomdp2022">(<a href="#ref-juliapomdp2022" role="doc-biblioref">JuliaPOMDP Team 2022</a>)</span> is a set of packages for defining and solving MDPs and POMDPs using the Julia programming language.</li>
</ul>
<p>R activity around POMDPs has also picked up with the packages <a href="https://cran.r-project.org/package=sarsop">sarsop</a> <span class="citation" data-cites="Bottiger2021">(<a href="#ref-Bottiger2021" role="doc-biblioref">Boettiger et al. 2021</a>)</span> and <a href="https://cran.r-project.org/package=pomdpSolve">pomdpSolve</a> <span class="citation" data-cites="Hahsler2022b">(<a href="#ref-Hahsler2022b" role="doc-biblioref">Hahsler and Cassandra 2022</a>)</span> which interface the
two popular POMDP solver programs APPL and pomdp-solver.</p>
<p>In this paper, we present <a href="https://cran.r-project.org/package=pomdp">pomdp</a> <span class="citation" data-cites="Hahsler2022">(<a href="#ref-Hahsler2022" role="doc-biblioref">Hahsler 2023</a>)</span> which was co-developed with
<a href="https://cran.r-project.org/package=pomdpSolve">pomdpSolve</a> <span class="citation" data-cites="Hahsler2022b">(<a href="#ref-Hahsler2022b" role="doc-biblioref">Hahsler and Cassandra 2022</a>)</span> to provide R users with
a consistent and flexible infrastructure for solving and working with
POMDPs. The package can be used to work with larger POMDP problems but is limited
by the capability of the used solvers. Larger problems also typically lead to
very complicated policies which can be executed by an automatic agent but
are not very helpful for a human user. This paper
focuses on features for smaller problems that yield simpler policies. Such models and
policies are better suited for human experts who want to understand the problem and
are interested in improved decision making.
For example, a medical researcher who tries to
develop easy-to-follow guidelines for doctors based on experiments with a POMDP model
is looking for a relatively simple and robust model with a simple and
understandable policy.
This typically means
to consider a model with few states and a small number of different observations.
For example, we have used the package to study diabetes
prevention by creating a very small, simplified model
to obtain a policy that is actionable in a primary care setting <span class="citation" data-cites="hahsler:Kamalzadeh:2021">(<a href="#ref-hahsler:Kamalzadeh:2021" role="doc-biblioref">Kamalzadeh et al. 2021</a>)</span>.
A second use of smaller models is in a classroom or self-study setting where the <a href="https://cran.r-project.org/package=pomdp">pomdp</a>
package can be used to demonstrate and study how POMDP models, solvers, and resulting
policies work.</p>
<h2 data-number="2" id="background-for-partially-observable-markov-decision-processes"><span class="header-section-number">2</span> Background for partially observable Markov decision processes</h2>
<p>A POMDP is a discrete-time stochastic control process that can formally be described by the 7-tuple
<span class="math display">\[\mathcal{P} = (S, A, T, R, \Omega , O, \gamma),\]</span> where</p>
<ul>
<li><span class="math inline">\(S = \{s_1, s_2, \dots, s_n\}\)</span> is the set of partially observable states of the environment,</li>
<li><span class="math inline">\(A = \{a_1, a_2, \dots, a_m\}\)</span> is the set of available actions,</li>
<li><span class="math inline">\(T\)</span> describes the system dynamics as the set of transition probabilities <span class="math inline">\(T(s&#39; \mid s,a)\)</span>
the state transition <span class="math inline">\(s \rightarrow s&#39;\)</span> conditioned on taking
ion <span class="math inline">\(a\)</span>.</li>
<li><span class="math inline">\(R: S \times A \times S \rightarrow \mathbb{R}\)</span> is the reward function which can
depend on the<br />
the state transition (previous and new state) and the action,</li>
<li><span class="math inline">\(\Omega = \{o_1, o_2, \dots, o_k\}\)</span> is the set of possible observations,</li>
<li><span class="math inline">\(O\)</span> defines the probabilistic connection of observations with the reached states <span class="math inline">\(s&#39;\)</span> as the set of observation probabilities <span class="math inline">\(O(o \mid a, s&#39;)\)</span> conditioned on the action <span class="math inline">\(a\)</span> taken to reach <span class="math inline">\(s&#39;\)</span>, and</li>
<li><span class="math inline">\(\gamma \in [0, 1]\)</span> is the discount factor modeling how much the agent prefers immediate rewards over later rewards.</li>
</ul>
<p>The used notation follows largely <span class="citation" data-cites="Kaelbling1998">(<a href="#ref-Kaelbling1998" role="doc-biblioref">Kaelbling et al. 1998</a>)</span>. Several variations of
this notation can be found in the literature.
Sets are often set in calligraphic font and it is also common to see the
observation model denoted by <span class="math inline">\(Z\)</span> instead of <span class="math inline">\(O\)</span>.</p>
<p>The control process proceeds in discrete time steps called epochs as follows.
At each time epoch <span class="math inline">\(t\)</span>, the environment is in some
unknown state <span class="math inline">\(s \in S\)</span>.
The agent chooses an action <span class="math inline">\(a \in A\)</span>, which causes the environment to
transition to state <span class="math inline">\(s&#39; \in S\)</span> with probability <span class="math inline">\(T(s&#39; \mid s,a)\)</span>. Simultaneously, the agent receives an observation <span class="math inline">\(o \in \Omega\)</span>, which
depends on the action and the new state of the environment following
the conditional probability distribution <span class="math inline">\(O(o \mid a, s&#39;)\)</span>. Finally, the agent receives a reward <span class="math inline">\(R(s,a,s&#39;)\)</span> depending on the transition. This
process repeats till a specified time horizon is reached. Often,
as in the equation below, an infinite horizon
is used.
The goal of the agent is to plan a policy that prescribes actions that
maximize the expected sum of discounted future rewards, i.e., she
chooses at each time <span class="math inline">\(t\)</span> the action that maximizes</p>
<p><span class="math display">\[\mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r_t\right],\]</span></p>
<p>where <span class="math inline">\(r_t = R(s_t, a_t, s_{t+1})\)</span> is the reward at epoch <span class="math inline">\(t\)</span> which depends on the state transition and the action at that time.
Since state transitions are stochastic,
the expectation is taken over all trajectories that the process may take.
Infinite horizon problems are guaranteed to converge if the discount factor <span class="math inline">\(\gamma &lt; 1\)</span>.
For a finite time horizon, the expectation is calculated over the sum up to the end of the
time horizon and a discounted expected final reward (called terminal value) may be added in the final epoch.</p>
<p>In a POMDP, the agent does not know the state the system is in, but it has to use
observations
to form a belief of what states the system could be in. This belief is called a belief state <span class="math inline">\(b \in B\)</span> and is
represented in the form of a probability distribution over the states. <span class="math inline">\(B\)</span> is the infinite set of all possible belief states
forming a <span class="math inline">\(|S|-1\)</span> simplex.
The agent starts with an initial belief <span class="math inline">\(b_0\)</span> (often a uniform distribution) and then updates the belief when new
observations are available.
In each epoch, after observing <span class="math inline">\(o\)</span>, the
agent can perform a simple Bayesian update where the
updated belief for being in state <span class="math inline">\(s&#39;\)</span> written as <span class="math inline">\(b&#39;(s&#39;)\)</span> is</p>
<p><span class="math display">\[
b&#39;(s&#39;) = \eta\ O(o | a, s&#39;) \sum_{s \in S} T(s&#39; | s,a) b(s),
\]</span>
and</p>
<p><span class="math display">\[
\eta = \frac{1}{\sum_{s&#39; \in S}\left( O(o | a, s&#39;) \sum_{s \in S} T(s&#39; | s,a) b(s)\right)}
\]</span>
normalizes the new belief state so all probabilities add up to one.</p>
<p>Regular MDPs have (under some assumptions) a deterministic optimal policy that prescribes an optimal action for
each state <span class="citation" data-cites="Puterman1994">(<a href="#ref-Puterman1994" role="doc-biblioref">Puterman 1994</a>)</span>. Even though
the actual states are not observable for POMDPs, POMDPs also have a
deterministic optimal policy that prescribes an optimal action for
each belief state.
A policy is a mapping <span class="math inline">\(\pi: B \rightarrow A\)</span> that prescribes
for each belief state an action.
The optimal policy is given by</p>
<p><span class="math display">\[\pi^* = \mathrm{argmax}_\pi \  V^\pi(b_0)\]</span></p>
<p>with</p>
<p><span class="math display">\[V^\pi(b_0) =   \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t r_t  \ \bigg\vert \ \pi, b_0 \right].\]</span></p>
<p><span class="math inline">\(V^\pi(b_0)\)</span>
is called the value function given policy <span class="math inline">\(\pi\)</span> and the agent’s
initial belief <span class="math inline">\(b_0 \in B\)</span>. The value function for any MDP or POMDP is a piecewise linear function that can be described by the
highest-reward segments of
a set of intersecting hyperplanes. The parameters for these hyperplanes are typically called <span class="math inline">\(\alpha\)</span>-vectors and
are a compact way to specify
both, the value function and the policy of the solution of a problem.</p>
<p>For the infinite-horizon case, the policy converges for <span class="math inline">\(\gamma &lt; 1\)</span>
to a policy that is independent of the time step and the initial belief. In this case, the policy can be visualized as
a directed graph called the policy graph. Each node of the graph is related to a hyperplane and represents the part of the belief space
where this hyperplane produces the highest reward in the
value function. Each node is labeled with the action to be taken given by the policy.
The outgoing edges are labeled with observations and specify to what segment of the value function the agent will transition
given the previous segment, the action and the observation.
The formulation can be easily extended to the finite-horizon case. However, the finite-horizon policy depends on the initial belief and the epoch. The finite-horizon policy forms a policy tree, where each level represents an epoch.</p>
<p>It has to be mentioned that finding optimal policies for POMDPs is known to be
a prohibitively difficult problem because the belief space grows exponentially with the number of states.
This issue is called the <em>curse of dimensionality</em> in dynamic programming.
Mundenk <span class="citation" data-cites="Mundhenk2000">(<a href="#ref-Mundhenk2000" role="doc-biblioref">Mundhenk 2000</a>)</span> has shown that finding the optimal policy for POMDPs is, in general, an
<span class="math inline">\(\text{NP}^\text{PP}\)</span>-complete problem which means that it is at least as difficult as the hardest
problems in <span class="math inline">\(\text{NP}\)</span>.
Therefore, exact algorithms can be only used for extremely small problems that are typically
of very limited use in practice.
More useful algorithms fall into the classes of approximate value iteration and approximate policy iteration <span class="citation" data-cites="Cassandra1998 Hauskrecht2000">(<a href="#ref-Cassandra1998" role="doc-biblioref">Cassandra 1998b</a>; <a href="#ref-Hauskrecht2000" role="doc-biblioref">Hauskrecht 2000</a>)</span>, which often find good solutions for larger problems.
To use POMDPs successfully,
the researcher typically needs to experiment with simplifying the problem description
and choosing an acceptable level of approximation by the algorithm.</p>
<p>The solution of POMDPs can be used to guide the agent’s actions. Automatic agents can follow very complicated
policies. Humans often prefer simpler policies, even if they are not optimal but good enough
to robustly improve outcomes. Simpler policies also result from problem simplification and allowing for a larger degree of approximation by the solver algorithm.</p>
<!-- Once a policy is found, the agent, starting with its initial belief, uses observations to update its belief state -->
<!-- and executes the prescribed action for its current belief state.  -->
<h2 data-number="3" id="implementation"><span class="header-section-number">3</span> Implementation</h2>
<p>Package <a href="https://cran.r-project.org/package=pomdp">pomdp</a> includes a convenient and consistent way for users to define all components of a POMDP
model using familiar R syntax, solve the problem using several methods and then analyze and visualize the results.
An important design decision is to separate the tasks of defining a problem and analyzing the policy from
the actual solver.
The separation between the infrastructure in
package <a href="https://cran.r-project.org/package=pomdp">pomdp</a> and the
solver code makes sure that additional solvers can be easily added in the future.
Solver code is typically interfaced by writing a standard problem definition file, running an external process,
and reading the results back. This way of interfacing solvers has several advantages:</p>
<ul>
<li>Using an external process, rather than directly interfacing the code in R ensures that memory issues for larger problems do not compromise the running R process itself.</li>
<li>The separation lets the solver use any available parallelization technique without imposing limitations
by R.</li>
<li>Most existing solver software accepts a standard problem definition file format.</li>
<li>The problem definition and the results are typically very small and fast to write and read compared to the significant amount of time used by the solver.</li>
<li>Separating problem definition and result analysis from the actual solver lets the user solve larger problems
on a dedicated server.</li>
</ul>
<p>For communication with the solver, the package supports the widely used POMDP <span class="citation" data-cites="Cassandra2015">(<a href="#ref-Cassandra2015" role="doc-biblioref">Cassandra 2015</a>)</span> file specification and can use POMDPX files <span class="citation" data-cites="APPL2022">(<a href="#ref-APPL2022" role="doc-biblioref">APPL Team 2022</a>)</span> via package <a href="https://cran.r-project.org/package=sarsop">sarsop</a>. This means that
new algorithms that use these formats can be easily interfaced in the future, and that
problems already formulated in these formats can be directly solved using the package.
The authors also provide an initial set of solvers with the companion package <a href="https://cran.r-project.org/package=pomdpSolve">pomdpSolve</a> which
provides an easy-to-install distribution of the well-known fast C implementation of a set of solvers originally developed by one of the co-authors <span class="citation" data-cites="Cassandra2015">(<a href="#ref-Cassandra2015" role="doc-biblioref">Cassandra 2015</a>)</span>.
The package <a href="https://cran.r-project.org/package=pomdp">pomdp</a> currently provides access to the following algorithms:</p>
<ul>
<li>Exact value iteration
<ul>
<li>Enumeration algorithm <span class="citation" data-cites="Sondik1971 Monahan1982">(<a href="#ref-Sondik1971" role="doc-biblioref">Sondik 1971</a>; <a href="#ref-Monahan1982" role="doc-biblioref">Monahan 1982</a>)</span>.</li>
<li>Two pass algorithm <span class="citation" data-cites="Sondik1971">(<a href="#ref-Sondik1971" role="doc-biblioref">Sondik 1971</a>)</span>.</li>
<li>Witness algorithm <span class="citation" data-cites="Littman1995">(<a href="#ref-Littman1995" role="doc-biblioref">Littman et al. 1995</a>)</span>.</li>
<li>Incremental pruning algorithm <span class="citation" data-cites="Zhang1996 Cassandra1997">(<a href="#ref-Zhang1996" role="doc-biblioref">Zhang and Liu 1996</a>; <a href="#ref-Cassandra1997" role="doc-biblioref">Cassandra et al. 1997</a>)</span>.</li>
</ul></li>
<li>Approximate value iteration
<ul>
<li>Finite grid algorithm <span class="citation" data-cites="Cassandra2015">(<a href="#ref-Cassandra2015" role="doc-biblioref">Cassandra 2015</a>)</span>, a variation of point-based value iteration to solve larger POMDPs (PBVI; see <span class="citation" data-cites="Pineau2003">(<a href="#ref-Pineau2003" role="doc-biblioref">Pineau et al. 2003</a>)</span>) without dynamic belief set expansion.</li>
<li>SARSOP <span class="citation" data-cites="Kurniawati2008">(<a href="#ref-Kurniawati2008" role="doc-biblioref">Kurniawati et al. 2008</a>)</span>, Successive Approximations of the
Reachable Space under Optimal Policies, a point-based algorithm
that approximates optimally reachable belief spaces for
infinite-horizon problems (via the third-party
R package <a href="https://cran.r-project.org/package=sarsop">sarsop</a> <span class="citation" data-cites="Bottiger2021">(<a href="#ref-Bottiger2021" role="doc-biblioref">Boettiger et al. 2021</a>)</span>).</li>
</ul></li>
</ul>
<p>While exact methods can only solve very small problems, PBVI and
SARSOP can efficiently find approximate solutions for larger
problems with thousands of states and hundreds of different
observations. <a href="https://cran.r-project.org/package=pomdp">pomdp</a> uses by default the finite grid
algorithm.</p>
<p>The <a href="https://cran.r-project.org/package=pomdp">pomdp</a> package provides efficient support by using</p>
<ul>
<li>sparse matrix representation based on the <a href="https://cran.r-project.org/package=Matrix">Matrix</a> package <span class="citation" data-cites="Bates2022">(<a href="#ref-Bates2022" role="doc-biblioref">Bates et al. 2022</a>)</span> for large transition and observation
matrices of low density,</li>
<li>fast matrix operations,</li>
<li>fast C++ implementations of loops using <a href="https://cran.r-project.org/package=Rcpp">Rcpp</a> <span class="citation" data-cites="Eddelbuettel2013">(<a href="#ref-Eddelbuettel2013" role="doc-biblioref">Eddelbuettel 2013</a>)</span>, and</li>
<li>parallel execution using <a href="https://cran.r-project.org/package=foreach">foreach</a> <span class="citation" data-cites="Microsoft2022">(<a href="#ref-Microsoft2022" role="doc-biblioref">Microsoft and Weston 2022</a>)</span>.</li>
</ul>
<p>The package implements many auxiliary functions to analyze and visualize POMDPs and their solution.
For example, to sample from the belief space, simulate trajectories through a
POMDP and estimate beliefs, fast C++ implementations (using <a href="https://cran.r-project.org/package=Rcpp">Rcpp</a> <span class="citation" data-cites="Eddelbuettel2013">(<a href="#ref-Eddelbuettel2013" role="doc-biblioref">Eddelbuettel 2013</a>)</span>) and
support for parallel execution using <a href="https://cran.r-project.org/package=foreach">foreach</a> <span class="citation" data-cites="Microsoft2022">(<a href="#ref-Microsoft2022" role="doc-biblioref">Microsoft and Weston 2022</a>)</span> are provided.
To represent and visualize policy graphs the widely used and powerful <a href="https://cran.r-project.org/package=igraph">igraph</a> package <span class="citation" data-cites="igraph2006">(<a href="#ref-igraph2006" role="doc-biblioref">Csardi and Nepusz 2006</a>)</span>
with its advanced layout options is used. Interactive policy graphs can be produced based on the
<a href="https://cran.r-project.org/package=visNetwork">visNetwork</a> <span class="citation" data-cites="Almende2022">(<a href="#ref-Almende2022" role="doc-biblioref">Almende B.V. and Contributors and Thieurmel 2022</a>)</span>.
While the package does not directly provide functions to create <a href="https://cran.r-project.org/package=ggplot2">ggplot2</a> visualizations <span class="citation" data-cites="Wickham2016">(<a href="#ref-Wickham2016" role="doc-biblioref">Wickham 2016</a>)</span> to
avoid installing the large number of packages needed, the manual pages provide examples.</p>
<p>Solving a new POMDP problem with the <a href="https://cran.r-project.org/package=pomdp">pomdp</a> package consists of the following steps:</p>
<ol type="1">
<li>Define a POMDP problem using the creator function <code>POMDP()</code> using R syntax,</li>
<li>solve the problem using <code>solve_POMDP()</code> which calls an external solver, and</li>
<li>analyze and visualize the results with functions like <code>reward()</code>,
<code>plot_policy_graph()</code>, and <code>plot_value_function()</code>.</li>
</ol>
<p>We will now discuss these steps in more detail and then present the complete code for a small toy example.</p>
<h3 data-number="3.1" id="defining-a-pomdp-problem"><span class="header-section-number">3.1</span> Defining a POMDP problem</h3>
<p>The <code>POMDP()</code> creator function has as its arguments the
7-tuple <span class="math inline">\((S, A, T, R, \Omega , O, \gamma)\)</span>,
the time horizon with terminal values, the
initial belief state <span class="math inline">\(b_0\)</span> and a name for the model. Default values are an infinite time
horizon (which has no terminal values), and an initial belief state given by a uniform distribution over all states.</p>
<p>While specifying most parts of the POMDP is straightforward, some arguments can be specified for convenience in
several different ways. Transition probabilities,
observation probabilities
and the reward function
can be
specified in several ways:</p>
<ul>
<li><p>A named list of dense or sparse matrices or the keywords <code>"identity"</code> and <code>"uniform"</code> representing the
probabilities or rewards organized by action.</p></li>
<li><p>As a <code>data.frame</code> representing a table with states, actions and the probabilities or reward values created with the helper functions</p>
<ul>
<li><code>T_(action, start.state, end.state, probability)</code>,</li>
<li><code>O_(action, end.state, observation, probability)</code> and</li>
<li><code>R_(action, start.state, end.state, observation, value)</code>.</li>
</ul>
<p><code>NA</code> is used to mean that a value applies to all actions, states or observations.</p></li>
<li><p>An R function with the same arguments as <code>T_()</code>, <code>O_()</code> or <code>R_()</code> that
returns the probability or reward.</p></li>
</ul>
<p>More details can be found in the manual page for the constructor function <code>POMDP()</code>.</p>
<h3 data-number="3.2" id="accessing-model-data"><span class="header-section-number">3.2</span> Accessing Model Data</h3>
<p>Several parts of the POMDP description can be defined in different ways. In particular, transition probabilities, observation
probabilities, rewards, and the start belief can be defined using dense matrices, sparse matrices, data frames, functions, keywords or a mixture of all of these. The decision to specify different parts of the description using different formats is
typically a result of how it is easier for the user to specify the part of the model.
For example, transition and observation matrices can typically be represented efficiently as sparse matrices and the keywords <code>uniform</code> and <code>identity</code>, while rewards are typically more compactly specified as a data frame with rows describing the reward for
a subset of action/state/observation combinations.</p>
<p>To write code that performs computation using this information requires a way to access the data in a unified
way. The package provides accessor functions like:</p>
<ul>
<li><p><code>start_vector()</code> translates the initial probability vector description into a numeric vector.</p></li>
<li><p>Transition probabilities, observation probabilities, and rewards can be accessed using functions ending in <code>_matrix()</code>.
Given an action, a matrix is returned.
The user can request a dense or sparse matrix using the logical parameter <code>sparse</code>.
To reduce the overhead associated with representing dense matrices in sparse format, sparse matrices are only returned
if the density of the matrix is below 50%. The user can also specify <code>sparse = NULL</code>,
which will return the data in the way it was specified by the user (e.g., a data frame). This saves the cost of conversion.
Functions ending in <code>_val()</code> can be used to access individual values directly.</p></li>
</ul>
<p>To allow a user-implemented algorithm direct access to the data in a uniform way, the function <code>normalize_POMDP()</code> can be used to create a
new POMDP definition where transition probabilities, observation probabilities, rewards, and the start belief are
consistently translated to (lists of) matrices and numeric vectors. Similar access
facilities for C++ developers are also available in the package source code.</p>
<h3 data-number="3.3" id="solving-a-pomdp"><span class="header-section-number">3.3</span> Solving a POMDP</h3>
<p>POMDP problems are solved with the function <code>solve_POMDP()</code>. This function uses the low-level interface in the companion package
<a href="https://cran.r-project.org/package=pomdpSolve">pomdpSolve</a> to solve a pomdp using the pomdp-solve software and return a solved instance of the POMDP problem.
Since the low-level interfaces vary between solvers, pomdp will provide additional functions for other popular solvers.
For example, for using the SARSOP solver interfaced in package <a href="https://cran.r-project.org/package=sarsop">sarsop</a>, a function <code>solve_SARSOP()</code> is provided.</p>
<p>Solving POMDPs is often done by trial-and-error while simplifying the problem description to make it
tractable. This means that we need to be able to interrupt the solver when it is running too long or
when it runs out of memory. To accomplish this, the problem is transferred to the solver
by writing a POMDP or POMDPX file, the solver software is then run in a separate process, and the results are read back. This approach results in a more robust interface since the R process is not compromised by a solver that runs out of memory or is interrupted due to too long run time. However, note that writing a large problem description file
can be quite slow.</p>
<p>The <code>solve_POMDP()</code> and <code>solve_SARSOP()</code> functions require a POMDP model and then
allow the user to specify or overwrite model parameters that are often used in experimentation
like the horizon, the discount rate, and the initial belief state. Additionally, solver-specific parameters
like the used algorithm for pomdp-solve can also be specified.</p>
<h3 data-number="3.4" id="analyzing-the-solution"><span class="header-section-number">3.4</span> Analyzing the solution</h3>
<p>The function <code>solve_POMDP()</code> returns a solved instance of the POMDP as a list that contains the original
problem definition and an additional element
containing the solution including if the solution has converged, the total expected reward given the initial belief, and the <span class="math inline">\(\alpha\)</span>-vectors representing the value function <span class="math inline">\(V^\pi\)</span> and the policy <span class="math inline">\(\pi\)</span>.
Keeping the problem definition and the solution together allows the user to
resolve an already solved problem multiple times experimenting with different initial beliefs, horizons or
discount rates,
and also to perform
analysis that requires both the problem definition and the solution.</p>
<p>An example of such an analysis is to simulate trajectories for a solved POMDP by following
an <span class="math inline">\(\epsilon\)</span>-greedy policy.
An <span class="math inline">\(\epsilon\)</span>-greedy policy follows the policy given in the solution but with a probability of <span class="math inline">\(\epsilon\)</span> uses a random action instead,
which can lead to exploring parts of the belief space that would not be reached by using only the policy.
Such a simulation needs access to the policy in the solution but also to the original problem description
(transaction and observation probabilities). This simulation is implemented in function <code>simulate_POMDP()</code>
and includes fast C++ code using <a href="https://cran.r-project.org/package=Rcpp">Rcpp</a> <span class="citation" data-cites="Eddelbuettel2013">(<a href="#ref-Eddelbuettel2013" role="doc-biblioref">Eddelbuettel 2013</a>)</span> and a native R implementation supporting
sparse matrix representation and sparse matrix operations.
Both implementations support parallelization using <a href="https://cran.r-project.org/package=foreach">foreach</a> <span class="citation" data-cites="Microsoft2022">(<a href="#ref-Microsoft2022" role="doc-biblioref">Microsoft and Weston 2022</a>)</span>
to speed up the simulation.</p>
<p>Often it is also interesting to test the robustness
of a policy on slightly modified problem descriptions or to test
the performance of a manually created policy. These experiments are supported
using function <code>add_policy()</code> which provides a convenient way to combine POMDP problem descriptions with compatible policies.</p>
<p>While the list elements of the solution can be directly accessed, several convenient access and visualization functions are provided.
We provide a <code>plot_value_function()</code> that visualizes the piecewise linear value function giving the reward over the belief space simplex as a line chart for two-state problems. Function <code>plot_belief_space()</code>
provides a more flexible visualization of the reward, the policy-based action, or the policy graph node
over the whole belief space. A three-state problem has a belief space of the form of a 2-simplex which is a triangle
and the visualization uses a ternary plot.
The belief space from more than three states cannot be directly visualized, however, projections
can be visualized by fixing the probabilities for all but two or three states.</p>
<p>The function <code>policy()</code> returns the policy as a table (data frame) consisting of
one row for each value function segment with the <span class="math inline">\(\alpha\)</span>-vector and the prescribed action as the last column.
If the policy depends on the epoch, then a list of tables is returned, one for
each epoch.
If the policy corresponds to a realizable conditional control plan, then
the policy can also be converted into an <a href="https://cran.r-project.org/package=igraph">igraph</a> object using the function
<code>policy_graph()</code> and visualized using the function <code>plot_policy_graph()</code>.
The policy graph shows the prescribed actions and how observations change the agent’s belief state.
This is often very useful for understanding the
policy. For general finite-horizon policies, the policy graph is a
policy tree where each level in the tree represents successive epochs.
Such trees are often too large to visualize directly, but the <a href="https://cran.r-project.org/package=igraph">igraph</a> object
can be used in many advanced R packages for network analysis or exported for analysis with external tools.</p>
<p>Further,
individual belief updates, the optimal action and the expected reward given a belief can be calculated using
<code>update_belief()</code>,
<code>optimal_action()</code>, and
<code>reward()</code>. Together with the unified accessor functions and the POMDP specifications,
the user can use these functions
to implement more sophisticated R-based analysis.
The source package also contains C++ implementations of these and the accessor functions. These can be used by an advanced
R developer to write fast analysis code or implement custom solvers.</p>
<h3 data-number="3.5" id="time-dependent-pomdps"><span class="header-section-number">3.5</span> Time-dependent POMDPs</h3>
<p>For some real-world problems, the transition probabilities, observation probabilities, or
rewards may change depending on the epoch. For example, in a medical application, the
transition probability modeling the chance of getting an infection may increase with
the age of the patient.
While the general definition of POMDPs can be easily extended to allow time-dependent transition probabilities, observation
probabilities and reward functions to model changes in the modeled system,
most existing solvers use fixed matrices.</p>
<p>The package <a href="https://cran.r-project.org/package=pomdp">pomdp</a> adds a simple mechanism to support time dependence.
Time dependence of transition probabilities, observation probabilities and the reward structure can be modeled by considering a set of episodes representing epochs with the same settings and then solving these
episodes in reverse order with the accumulated discounted reward of each episode used as the final reward for the preceding episode.
Details on how to specify episodes in time-dependent POMDPs can be found in the <a href="https://cran.r-project.org/package=pomdp">pomdp</a> manual pages.</p>
<h2 data-number="4" id="toy-example-the-tiger-problem"><span class="header-section-number">4</span> Toy Example: The Tiger problem</h2>
<p>We will demonstrate how to use the package with a popular toy problem called the Tiger Problem <span class="citation" data-cites="Cassandra1994">(<a href="#ref-Cassandra1994" role="doc-biblioref">Cassandra et al. 1994</a>)</span>.
This example is often used to introduce students to POMDPs.
The problem is defined as:</p>
<p>An agent is facing two closed doors, and a tiger is put with equal
probability behind one of the two doors represented by the environment states
<code>tiger-left</code> and <code>tiger-right</code> while treasure is put behind the other
door. The available actions are <code>listen</code> for tiger noises or opening a
door (actions <code>open-left</code> and <code>open-right</code>).
Listening is neither free
(the action has a reward of -1) nor is it entirely accurate. There is
a 15% probability that the agent hears the tiger behind the left door
while it is behind the right door and vice versa. If the
agent opens the door with the tiger, it will get hurt (a reward
of -100), but if it opens the door with the treasure, it will receive
a positive reward of 10. After a door is opened, the problem
resets (i.e., the tiger is again randomly assigned to a door), and the agent gets another try.
This makes it an infinite horizon problem and we use a discount factor
of .75 to guarantee convergence.</p>
<h3 data-number="4.1" id="specifying-the-tiger-problem"><span class="header-section-number">4.1</span> Specifying the Tiger problem</h3>
<p>The problem can be specified using the function <code>POMDP()</code>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='st'><a href='https://github.com/mhahsler/pomdp'>"pomdp"</a></span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>Tiger</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/POMDP.html'>POMDP</a></span><span class='op'>(</span></span>
<span>  name <span class='op'>=</span> <span class='st'>"Tiger Problem"</span>,</span>
<span>  discount <span class='op'>=</span> <span class='fl'>0.75</span>,</span>
<span>  states <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='st'>"tiger-left"</span> , <span class='st'>"tiger-right"</span><span class='op'>)</span>,</span>
<span>  actions <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='st'>"listen"</span>, <span class='st'>"open-left"</span>, <span class='st'>"open-right"</span><span class='op'>)</span>,</span>
<span>  observations <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='st'>"tiger-left"</span>, <span class='st'>"tiger-right"</span><span class='op'>)</span>,</span>
<span>  start <span class='op'>=</span> <span class='st'>"uniform"</span>,</span>
<span>  </span>
<span>  transition_prob <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span></span>
<span>    <span class='st'>"listen"</span> <span class='op'>=</span> <span class='st'>"identity"</span>, </span>
<span>    <span class='st'>"open-left"</span> <span class='op'>=</span> <span class='st'>"uniform"</span>, </span>
<span>    <span class='st'>"open-right"</span> <span class='op'>=</span> <span class='st'>"uniform"</span><span class='op'>)</span>,</span>
<span></span>
<span>  observation_prob <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span></span>
<span>    <span class='st'>"listen"</span> <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/matrix.html'>matrix</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>0.85</span>, <span class='fl'>0.15</span>, <span class='fl'>0.15</span>, <span class='fl'>0.85</span><span class='op'>)</span>, nrow <span class='op'>=</span> <span class='fl'>2</span>, byrow <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span>, </span>
<span>    <span class='st'>"open-left"</span> <span class='op'>=</span> <span class='st'>"uniform"</span>,</span>
<span>    <span class='st'>"open-right"</span> <span class='op'>=</span> <span class='st'>"uniform"</span><span class='op'>)</span>,</span>
<span>    </span>
<span>  reward <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/cbind.html'>rbind</a></span><span class='op'>(</span></span>
<span>    <span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/POMDP.html'>R_</a></span><span class='op'>(</span><span class='st'>"listen"</span>,     <span class='cn'>NA</span>,            <span class='cn'>NA</span>,  <span class='cn'>NA</span>,   <span class='op'>-</span><span class='fl'>1</span><span class='op'>)</span>,</span>
<span>    <span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/POMDP.html'>R_</a></span><span class='op'>(</span><span class='st'>"open-left"</span>,  <span class='st'>"tiger-left"</span>,  <span class='cn'>NA</span> , <span class='cn'>NA</span>, <span class='op'>-</span><span class='fl'>100</span><span class='op'>)</span>,</span>
<span>    <span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/POMDP.html'>R_</a></span><span class='op'>(</span><span class='st'>"open-left"</span>,  <span class='st'>"tiger-right"</span>, <span class='cn'>NA</span> , <span class='cn'>NA</span>,   <span class='fl'>10</span><span class='op'>)</span>,</span>
<span>    <span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/POMDP.html'>R_</a></span><span class='op'>(</span><span class='st'>"open-right"</span>, <span class='st'>"tiger-left"</span>,  <span class='cn'>NA</span> , <span class='cn'>NA</span>,   <span class='fl'>10</span><span class='op'>)</span>,</span>
<span>    <span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/POMDP.html'>R_</a></span><span class='op'>(</span><span class='st'>"open-right"</span>, <span class='st'>"tiger-right"</span>, <span class='cn'>NA</span> , <span class='cn'>NA</span>, <span class='op'>-</span><span class='fl'>100</span><span class='op'>)</span></span>
<span>  <span class='op'>)</span></span>
<span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>Note that we use for each component the most convenient specification method. For observations and transitions, we use a
list of distribution keywords and a matrix, while for the rewards, a data frame created with the <code>R_()</code> function is used.
The <code>R_()</code> function accepts the arguments <code>action</code>,
<code>start.state</code>, <code>end.state</code>,
<code>observation</code>, and the reward <code>value</code>.
A missing value of <code>NA</code> indicates that the reward is valid for any
state or observation.</p>
<p>The transition model can be visualized as a graph.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>g</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/transition_graph.html'>transition_graph</a></span><span class='op'>(</span><span class='va'>Tiger</span><span class='op'>)</span></span>
<span></span>
<span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://r.igraph.org/'>igraph</a></span><span class='op'>)</span></span>
<span><span class='fu'><a href='https://rdrr.io/r/graphics/plot.default.html'>plot</a></span><span class='op'>(</span><span class='va'>g</span>,</span>
<span> layout <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/cbind.html'>rbind</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='op'>-</span><span class='fl'>1</span>, <span class='fl'>0</span><span class='op'>)</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>1</span>, <span class='fl'>0</span><span class='op'>)</span><span class='op'>)</span>, rescale <span class='op'>=</span> <span class='cn'>FALSE</span>,</span>
<span> edge.curved <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/plot_policy_graph.html'>curve_multiple_directed</a></span><span class='op'>(</span><span class='va'>g</span>, <span class='fl'>.8</span><span class='op'>)</span>,</span>
<span> edge.loop.angle <span class='op'>=</span> <span class='va'>pi</span> <span class='op'>/</span> <span class='fl'>2</span>,</span>
<span> vertex.size <span class='op'>=</span> <span class='fl'>65</span></span>
<span><span class='op'>)</span></span></code></pre>
</div>
<div class="figure"><span style="display:block;" id="fig:tiger-transition"></span>
<img src="RJ-2024-021_files/figure-html5/tiger-transition-1.png" alt="A graph showing the transition model of the Tiger problem." width="100%" />
<p class="caption">
Figure 1: Transition model of the Tiger problem.
</p>
</div>
</div>
<p>The vertices in Figure <a href="#fig:tiger-transition">1</a> represent the states and
the edges show transitions labeled with actions and the associated transition probabilities in parentheses. Multiple parallel transitions are
collapsed into a single arrow with several labels to simplify the visualization.
The graph shows that the action <code>listen</code> stays with a probability of 1
in the same state (i.e., listening does not move the tiger). The actions <code>open-left</code> and <code>open-right</code> lead to
a reset of the problem which assigns the tiger randomly to a state. This is represented by the transitions with a
probability of .5.</p>
<p>For more complicated transition models, individual graphs for each action or interactive
graphs using <a href="https://cran.r-project.org/package=visNetwork">visNetwork</a> can also be plotted.</p>
<h3 data-number="4.2" id="solving-the-tiger-problem-for-an-infinite-time-horizon"><span class="header-section-number">4.2</span> Solving the Tiger problem for an infinite time horizon</h3>
<p>To solve the problem, we use the default method (pomdp-solve’s finite grid method interfaced
in package <a href="https://cran.r-project.org/package=pomdpSolve">pomdpSolve</a>)
which performs a form of point-based value iteration that can find
approximate solutions for larger problems.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>sol</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/solve_POMDP.html'>solve_POMDP</a></span><span class='op'>(</span><span class='va'>Tiger</span><span class='op'>)</span></span>
<span><span class='va'>sol</span></span></code></pre>
</div>
<pre><code>POMDP, list - Tiger Problem
  Discount factor: 0.75
  Horizon: Inf epochs
  Size: 2 states / 3 actions / 2 obs.
  Start: uniform
  Solved:
    Method: &#39;grid&#39;
    Solution converged: TRUE
    # of alpha vectors: 5
    Total expected reward: 1.933439

  List components: &#39;name&#39;, &#39;discount&#39;, &#39;horizon&#39;, &#39;states&#39;,
    &#39;actions&#39;, &#39;observations&#39;, &#39;transition_prob&#39;,
    &#39;observation_prob&#39;, &#39;reward&#39;, &#39;start&#39;, &#39;info&#39;, &#39;solution&#39;</code></pre>
</div>
<p>The solver returns an object of class POMDP, which contains the solution as an
additional list component. The print function displays important information like the
used discount factor, the horizon, if the solution has converged and the total expected reward. In this case, the total expected discounted reward for following the policy starting from the initial belief is 1.933.
Note that
the optimal policy for infinite-horizon does not depend on the
initial belief.
The reward for other initial beliefs can be calculated using the
<code>reward()</code> function. For example, the expected reward for a correct
belief that the tiger starts to the left with a probability of 90% is:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/reward.html'>reward</a></span><span class='op'>(</span><span class='va'>sol</span>, belief <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>0.9</span>, <span class='fl'>0.1</span><span class='op'>)</span><span class='op'>)</span> </span></code></pre>
</div>
<pre><code>[1] 4.779814</code></pre>
</div>
<h3 data-number="4.3" id="inspecting-the-policy"><span class="header-section-number">4.3</span> Inspecting the Policy</h3>
<p>The policy of a solved POMDP is a set of <span class="math inline">\(\alpha\)</span>-vectors
representing a segment of the value function and the
associated best action.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/policy.html'>policy</a></span><span class='op'>(</span><span class='va'>sol</span><span class='op'>)</span></span></code></pre>
</div>
<pre><code>  tiger-left tiger-right     action
1 -98.549921   11.450079  open-left
2 -10.854299    6.516937     listen
3   1.933439    1.933439     listen
4   6.516937  -10.854299     listen
5  11.450079  -98.549921 open-right</code></pre>
</div>
<p>The returned policy is a list where each element represents the <span class="math inline">\(\alpha\)</span>-vectors
for an epoch. The policy above has only one list element since the solution
converged to a solution that is independent of the epoch.</p>
<p>Smaller policies that
correspond to a conditional plan
can also be represented as a graph using a custom plot function.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/plot_policy_graph.html'>plot_policy_graph</a></span><span class='op'>(</span><span class='va'>sol</span><span class='op'>)</span></span></code></pre>
</div>
<div class="figure"><span style="display:block;" id="fig:tiger-infinite-horizon"></span>
<img src="RJ-2024-021_files/figure-html5/tiger-infinite-horizon-1.png" alt="A graph showing the infinite-horizon solution of the Tiger problem." width="100%" />
<p class="caption">
Figure 2: The policy graph for the converged infinite-horizon solution of the Tiger problem.
</p>
</div>
</div>
<p>The function uses the <a href="https://cran.r-project.org/package=igraph">igraph</a> package <span class="citation" data-cites="igraph2006">(<a href="#ref-igraph2006" role="doc-biblioref">Csardi and Nepusz 2006</a>)</span> to produce the layout.
Figure <a href="#fig:tiger-infinite-horizon">2</a> shows the graph for the optimal policy returned
by the solver for the Tiger problem.
Each node in the policy graph represents an <span class="math inline">\(\alpha\)</span>-vector and is labeled by the
action prescribed by the policy.
Each segment covers a part of the belief space which represents how much the agent knows about the location of
the tiger based on all previous observations.
We use a pie chart inside each node to show a representative belief point that belongs to the segment.
This makes it easier to compare the beliefs in different nodes with each other.
The representative belief points are found with the function
<code>estimate_belief_for_nodes()</code> which uses the solver output and searches along policy trajectories.</p>
<p>It is easy to interpret smaller policy graphs. Figure <a href="#fig:tiger-infinite-horizon">2</a> shows that without prior information,
the agent starts at the node marked with initial belief. In this case,
the agent believes there is a 50/50 chance that the tiger is behind
either door. The optimal action is displayed inside the state and, in this
case, is to listen. The arcs are labeled with observations. Let us
assume that the observation is tiger-left. The agent follows the
appropriate arc and ends in a node representing the new range of belief states with a higher probability of the tiger being to the left.
However, the optimal action is still to listen. If the agent again hears
the tiger on the left then it ends up in a node that has a
belief of close to 100% that the tiger is to the left and open-right is the optimal
action. The arcs back from the nodes with the open actions to the
initial state reset the problem and let the agent start over.</p>
<p>Typically, small and compact policy graphs are preferable in practice because they make
the policy easier to understand for the decision maker and also easier to follow.
For large, more complicated policy graphs, representation as a graph is difficult
leading to issues with node layout and too many crossing vertices.
The package can also plot the
graph as an interactive HTML widget with movable vertices (see the manual page for <code>plot_policy_graph()</code>)
to let the user arrange the graph manually.
Larger policy graphs can also be exported in common formats
like graphML to be displayed and analyzed in large-scale network analysis tools like Gephi <span class="citation" data-cites="Jacomy2014">(<a href="#ref-Jacomy2014" role="doc-biblioref">Jacomy et al. 2014</a>)</span>.</p>
<p>The Tiger problem environment has only two states (tiger-left and tiger-right) with a
belief space forming a 1-simplex which is a line going from a probability of 1 that the tiger is left
to a probability of 1 that the tiger is right. Therefore, we can visualize the piecewise linear
convex value function as a simple line chart with the belief on the x-axis.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/value_function.html'>plot_value_function</a></span><span class='op'>(</span><span class='va'>sol</span>, ylim <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>0</span>,<span class='fl'>20</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
<div class="figure"><span style="display:block;" id="fig:tiger-value-function"></span>
<img src="RJ-2024-021_files/figure-html5/tiger-value-function-1.png" alt="A line chart with the piecewise linear segments of the value funciton." width="100%" />
<p class="caption">
Figure 3: The value function for the solution of the converged Tiger problem.
</p>
</div>
</div>
<p>Figure <a href="#fig:tiger-value-function">3</a> shows the value function.
The x-axis represents the belief, the lines represent the nodes in the policy graph (the numbers in the legend match the numbers in the graph in Figure <a href="#fig:tiger-value-function">3</a>),
and the piecewise linear value function consists of the line segments with the highest reward.
The optimal action for each segment is shown in the legend.
This visualization function is mostly provided to study small textbook examples
with two states. A more versatile function is <code>plot_belief_space()</code> which can produce
ternary plots for problems with three or more states by projecting the belief
space on three states.</p>
<p>Auxiliary functions provided in the package let the user perform many analyses. For example, we simulate
trajectories through the POMDP belief space by following the policy and estimating the distribution of the
agent’s belief.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>sim</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/simulate_POMDP.html'>simulate_POMDP</a></span><span class='op'>(</span><span class='va'>sol</span>, n <span class='op'>=</span> <span class='fl'>50</span>, horizon <span class='op'>=</span> <span class='fl'>5</span>, </span>
<span>  belief <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>.5</span>, <span class='fl'>.5</span><span class='op'>)</span>, return_beliefs <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span></span>
<span><span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/plot_belief_space.html'>plot_belief_space</a></span><span class='op'>(</span><span class='va'>sol</span>, sample <span class='op'>=</span> <span class='va'>sim</span><span class='op'>$</span><span class='va'>belief_states</span>, ylim <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>0</span>, <span class='fl'>12</span><span class='op'>)</span>, </span>
<span>                  jitter <span class='op'>=</span> <span class='fl'>5</span><span class='op'>)</span></span>
<span><span class='fu'><a href='https://rdrr.io/r/graphics/lines.html'>lines</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/stats/density.html'>density</a></span><span class='op'>(</span><span class='va'>sim</span><span class='op'>$</span><span class='va'>belief_states</span><span class='op'>[</span>, <span class='fl'>1</span><span class='op'>]</span>, bw <span class='op'>=</span> <span class='fl'>.01</span>, from <span class='op'>=</span> <span class='fl'>0</span> , to <span class='op'>=</span> <span class='fl'>1</span><span class='op'>)</span><span class='op'>)</span> </span>
<span><span class='fu'><a href='https://rdrr.io/r/graphics/axis.html'>axis</a></span><span class='op'>(</span><span class='fl'>2</span><span class='op'>)</span>; <span class='fu'><a href='https://rdrr.io/r/graphics/title.html'>title</a></span><span class='op'>(</span>ylab <span class='op'>=</span> <span class='st'>"Density"</span><span class='op'>)</span></span></code></pre>
</div>
<div class="figure"><span style="display:block;" id="fig:tiger-simulation"></span>
<img src="RJ-2024-021_files/figure-html5/tiger-simulation-1.png" alt="A dot chart showing the distribution of reached belief states." width="100%" />
<p class="caption">
Figure 4: Belief states reached in 50 simulated trajectories of horizon 5.
</p>
</div>
</div>
<p>Figure <a href="#fig:tiger-simulation">4</a> shows the five beliefs that are
reached in the trajectories as dots and uses jitter and a density estimate to show how
much time the agent has spent in the simulation in different parts of the belief space. The color of the dots indicates the actions chosen by the policy.</p>
<h3 data-number="4.4" id="solving-the-tiger-problem-for-a-finite-time-horizon"><span class="header-section-number">4.4</span> Solving the Tiger problem for a finite time horizon</h3>
<p>To demonstrate how to solve a POMDP problem with a finite time horizon,
we set the horizon to 4 epochs, which means that the agent starts with its initial
belief and can perform only four actions.
The grid-based method used before
finds the optimal policy, but for finite time horizon problems with negative rewards, the
value function and the calculated expected reward is only valid when the solution converges.
To avoid this issue, we use here the incremental pruning
algorithm <span class="citation" data-cites="Zhang1996 Cassandra1997">(<a href="#ref-Zhang1996" role="doc-biblioref">Zhang and Liu 1996</a>; <a href="#ref-Cassandra1997" role="doc-biblioref">Cassandra et al. 1997</a>)</span>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>sol</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/solve_POMDP.html'>solve_POMDP</a></span><span class='op'>(</span>model <span class='op'>=</span> <span class='va'>Tiger</span>, horizon <span class='op'>=</span> <span class='fl'>4</span>, method <span class='op'>=</span> <span class='st'>"incprune"</span><span class='op'>)</span></span>
<span><span class='va'>sol</span></span></code></pre>
</div>
<pre><code>POMDP, list - Tiger Problem
  Discount factor: 0.75
  Horizon: 4 epochs
  Size: 2 states / 3 actions / 2 obs.
  Start: uniform
  Solved:
    Method: &#39;incprune&#39;
    Solution converged: FALSE
    # of alpha vectors: 26
    Total expected reward: 0.483125

  List components: &#39;name&#39;, &#39;discount&#39;, &#39;horizon&#39;, &#39;states&#39;,
    &#39;actions&#39;, &#39;observations&#39;, &#39;transition_prob&#39;,
    &#39;observation_prob&#39;, &#39;reward&#39;, &#39;start&#39;, &#39;info&#39;, &#39;solution&#39;</code></pre>
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/policy.html'>policy</a></span><span class='op'>(</span><span class='va'>sol</span><span class='op'>)</span></span></code></pre>
</div>
<pre><code>[[1]]
  tiger-left tiger-right     action
1 -99.321250   10.678750  open-left
2 -11.820719    4.640094     listen
3  -2.734955    2.600990     listen
4  -1.137420    1.595135     listen
5   0.483125    0.483125     listen
6   1.595135   -1.137420     listen
7   2.600990   -2.734955     listen
8   4.640094  -11.820719     listen
9  10.678750  -99.321250 open-right

[[2]]
   tiger-left tiger-right     action
1 -101.312500    8.687500  open-left
2  -20.550156    5.488906     listen
3  -13.450000    4.700000     listen
4   -3.565469    2.157969     listen
5    0.905000    0.905000     listen
6    2.157969   -3.565469     listen
7    4.700000  -13.450000     listen
8    5.488906  -20.550156     listen
9    8.687500 -101.312500 open-right

[[3]]
  tiger-left tiger-right     action
1  -100.7500      9.2500  open-left
2   -12.8875      5.2625     listen
3    -1.7500     -1.7500     listen
4     5.2625    -12.8875     listen
5     9.2500   -100.7500 open-right

[[4]]
  tiger-left tiger-right     action
1       -100          10  open-left
2         -1          -1     listen
3         10        -100 open-right</code></pre>
</div>
<p>The policy has four elements, one for each epoch. Is easier to understand the
policy by visualizing it as a graph.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/plot_policy_graph.html'>plot_policy_graph</a></span><span class='op'>(</span><span class='va'>sol</span><span class='op'>)</span></span></code></pre>
</div>
<div class="figure"><span style="display:block;" id="fig:tiger-finite-horizon"></span>
<img src="RJ-2024-021_files/figure-html5/tiger-finite-horizon-1.png" alt="A graph showing the finite-horizon solution of the Tiger problem." width="100%" />
<p class="caption">
Figure 5: Policy tree for the Tiger problem solved with a horizon of 4 and a uniform initial belief.
</p>
</div>
</div>
<p>The resulting policy graph is shown in Figure <a href="#fig:tiger-finite-horizon">5</a>
as a tree with four levels, one for each time epoch. The plot function automatically
uses a tree layout and adds the epoch as the first number to the node labels. By default,
it also simplifies the representation by hiding belief states which cannot be reached
form the start belief and, therefore, there are more entries in the policy above
than there are nodes in the graph.
The root node of the tree represents the initial belief
used in the model. The model starts with a uniform initial belief represented by the evenly split pie chart.
The policy shows that
the optimal strategy is
to listen and open a door only if we hear the tiger behind the same door twice.
Interestingly, it is optimal never to open a door in
the last epoch. The reason is that we cannot reach a sufficiently high belief of the tiger being behind a single door.
The expected reward of this policy starting at a uniform initial belief is
0.483.</p>
<p>Policy trees for finite-horizon problems are dependent on the agent’s
initial belief. To show this, we produce a new policy tree
for an initial belief of 99% that the tiger is to the left by overwriting the initial belief in the model definition.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>sol</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/solve_POMDP.html'>solve_POMDP</a></span><span class='op'>(</span>model <span class='op'>=</span> <span class='va'>Tiger</span>, horizon <span class='op'>=</span> <span class='fl'>4</span>, </span>
<span>  initial_belief <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>.99</span>, <span class='fl'>.01</span><span class='op'>)</span>, method <span class='op'>=</span> <span class='st'>"incprune"</span><span class='op'>)</span></span>
<span><span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/reward.html'>reward</a></span><span class='op'>(</span><span class='va'>sol</span>, belief <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>.99</span>, <span class='fl'>.01</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
<pre><code>[1] 9.57875</code></pre>
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'><a href='https://rdrr.io/pkg/pomdp/man/plot_policy_graph.html'>plot_policy_graph</a></span><span class='op'>(</span><span class='va'>sol</span>, belief <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>.99</span>, <span class='fl'>.01</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
<div class="figure"><span style="display:block;" id="fig:tiger-finite-horizon-99"></span>
<img src="RJ-2024-021_files/figure-html5/tiger-finite-horizon-99-1.png" alt="A graph showing the finite-horizon solution of the Tiger problem." width="100%" />
<p class="caption">
Figure 6: Policy tree for the Tiger problem solved with a horizon of 4 and an initial belief of 99 percent that the tiger is to the left.
</p>
</div>
</div>
<p>The resulting policy graph with an initial belief indicating that we are very sure that the tiger
is to the left is shown in Figure <a href="#fig:tiger-finite-horizon-99">6</a>. The graph
indicates that it is optimal to open the right door right away and then wait if we hear the tiger twice in the same location before we open the other door.
Under the strong belief, the agent also expects a much higher reward of
9.579 for the optimal policy.</p>
<h2 data-number="5" id="summary"><span class="header-section-number">5</span> Summary</h2>
<p>Partially observable Markov decision processes are an important modeling technique useful
for many applications.
Easily accessible software to solve POMDP problems is crucial to support applied research and instruction in fields including artificial intelligence and operations research. Most existing libraries need advanced technical expertise to install and offer minimal support to analyze the results. The <a href="https://cran.r-project.org/package=pomdp">pomdp</a> package fills this gap by providing an easily accessible platform to perform experiments and analyze POMDP problems and the resulting policies.</p>
<p>This paper used a minimalist toy example to show the functionality of the package in
a concise way.
Studying and visualizing complicated policies with hundreds or thousands of belief states
is an important topic that has received less attention than improving solver algorithms.
R provides a wide range of tools to compare, analyze, and cluster belief states. We plan
to investigate the use of these techniques to support
explainability of more complicated policies and will implement corresponding functions in
future releases of the package <a href="https://cran.r-project.org/package=pomdp">pomdp</a>.</p>
<h2 data-number="6" id="acknowledgments"><span class="header-section-number">6</span> Acknowledgments</h2>
<p>Farzad Kamalzadeh participated in the development of an early version of the pomdp package
and used it for several applications.
He was supported by a Graduate Fellowship and a Niemi Center Fellowship, both at SMU.
His and Michael Hahsler’s work was also supported in part by
the National Institute of Standards and Technology (NIST) under grant number 60NANB17D180.</p>
<p>The authors would also like to thank Carl Boettiger for maintaining the package
<a href="https://cran.r-project.org/package=sarsop">sarsop</a> and the anonymous reviewers for their valuable insights.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<h3 class="appendix" data-number="6.1" id="supplementary-materials"><span class="header-section-number">6.1</span> Supplementary materials</h3>
<p>Supplementary materials are available in addition to this article. It can be downloaded at
<a href="RJ-2024-021.zip">RJ-2024-021.zip</a></p>
<h3 class="appendix" data-number="6.2" id="cran-packages-used"><span class="header-section-number">6.2</span> CRAN packages used</h3>
<p><a href="https://cran.r-project.org/package=ReinforcementLearning">ReinforcementLearning</a>, <a href="https://cran.r-project.org/package=sarsop">sarsop</a>, <a href="https://cran.r-project.org/package=pomdpSolve">pomdpSolve</a>, <a href="https://cran.r-project.org/package=pomdp">pomdp</a>, <a href="https://cran.r-project.org/package=Matrix">Matrix</a>, <a href="https://cran.r-project.org/package=Rcpp">Rcpp</a>, <a href="https://cran.r-project.org/package=foreach">foreach</a>, <a href="https://cran.r-project.org/package=igraph">igraph</a>, <a href="https://cran.r-project.org/package=visNetwork">visNetwork</a>, <a href="https://cran.r-project.org/package=ggplot2">ggplot2</a></p>
<h3 class="appendix" data-number="6.3" id="cran-task-views-implied-by-cited-packages"><span class="header-section-number">6.3</span> CRAN Task Views implied by cited packages</h3>
<p><a href="https://cran.r-project.org/view=ChemPhys">ChemPhys</a>, <a href="https://cran.r-project.org/view=DynamicVisualizations">DynamicVisualizations</a>, <a href="https://cran.r-project.org/view=Econometrics">Econometrics</a>, <a href="https://cran.r-project.org/view=GraphicalModels">GraphicalModels</a>, <a href="https://cran.r-project.org/view=HighPerformanceComputing">HighPerformanceComputing</a>, <a href="https://cran.r-project.org/view=NetworkAnalysis">NetworkAnalysis</a>, <a href="https://cran.r-project.org/view=NumericalMathematics">NumericalMathematics</a>, <a href="https://cran.r-project.org/view=Optimization">Optimization</a>, <a href="https://cran.r-project.org/view=Phylogenetics">Phylogenetics</a>, <a href="https://cran.r-project.org/view=Spatial">Spatial</a>, <a href="https://cran.r-project.org/view=TeachingStatistics">TeachingStatistics</a></p>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Almende2022" class="csl-entry" role="listitem">
Almende B.V. and Contributors and B. Thieurmel. <em>visNetwork: Network visualization using ’vis.js’ library.</em> 2022. URL <a href="https://CRAN.R-project.org/package=visNetwork">https://CRAN.R-project.org/package=visNetwork</a>. R package version 2.1.2.
</div>
<div id="ref-APPL2022" class="csl-entry" role="listitem">
APPL Team. <span>APPL</span>: <span>A</span>pproximate <span>POMDP</span> planning toolkit. 2022. URL <a href="https://bigbird.comp.nus.edu.sg/pmwiki/farm/appl/">https://bigbird.comp.nus.edu.sg/pmwiki/farm/appl/</a>.
</div>
<div id="ref-Astrom1965" class="csl-entry" role="listitem">
K. J. Åström. Optimal control of <span>M</span>arkov processes with incomplete state information. <em>Journal of Mathematical Analysis and Applications</em>, 10(1): 174–205, 1965.
</div>
<div id="ref-Bates2022" class="csl-entry" role="listitem">
D. Bates, M. Maechler and M. Jagan. <em>Matrix: Sparse and dense matrix classes and methods.</em> 2022. URL <a href="https://CRAN.R-project.org/package=Matrix">https://CRAN.R-project.org/package=Matrix</a>. R package version 1.5-3.
</div>
<div id="ref-Bottiger2021" class="csl-entry" role="listitem">
C. Boettiger, J. Ooms and M. Memarzadeh. <em>Sarsop: <span>A</span>pproximate <span>POMDP</span> planning software.</em> 2021. URL <a href="https://CRAN.R-project.org/package=sarsop">https://CRAN.R-project.org/package=sarsop</a>. <span>R</span> package version 0.6.9.
</div>
<div id="ref-Cassandra1998b" class="csl-entry" role="listitem">
A. R. Cassandra. A survey of <span>POMDP</span> applications. MCC-INSL-111-98. Microelectronics; Computer Technology Corporation (MCC). 1998a. Presented at the <span>AAAI</span> Fall Symposium.
</div>
<div id="ref-Cassandra1998" class="csl-entry" role="listitem">
A. R. Cassandra. Exact and approximate algorithms for partially observable markov decision processes. 1998b. AAI9830418.
</div>
<div id="ref-Cassandra2015" class="csl-entry" role="listitem">
A. R. Cassandra. The <span>POMDP</span> page. 2015. URL <a href="https://www.pomdp.org">https://www.pomdp.org</a>.
</div>
<div id="ref-Cassandra1994" class="csl-entry" role="listitem">
A. R. Cassandra, L. P. Kaelbling and M. L. Littman. Acting optimally in partially observable stochastic domains. In <em>Proceedings of the twelfth national conference on artificial intelligence</em>, 1994. Seattle, WA. <span>AAAI</span> Classic Paper Award, 2013.
</div>
<div id="ref-Cassandra1997" class="csl-entry" role="listitem">
A. R. Cassandra, M. L. Littman and N. L. Zhang. Incremental pruning: <span>A</span> simple, fast, exact method for partially observable <span>M</span>arkov decision processes. In <em>UAI’97: Proceedings of the thirteenth conference on uncertainty in artificial intelligence</em>, pages. 54--61 1997.
</div>
<div id="ref-igraph2006" class="csl-entry" role="listitem">
G. Csardi and T. Nepusz. The igraph software package for complex network research. <em>InterJournal</em>, Complex Systems: 1695, 2006. URL <a href="https://igraph.org">https://igraph.org</a>.
</div>
<div id="ref-Eddelbuettel2013" class="csl-entry" role="listitem">
D. Eddelbuettel. <em>Seamless <span>R</span> and <span>C++</span> integration with <span>Rcpp</span>.</em> New York: Springer, 2013. DOI <a href="https://doi.org/10.1007/978-1-4614-6868-4">10.1007/978-1-4614-6868-4</a>. ISBN 978-1-4614-6867-7.
</div>
<div id="ref-Hahsler2022" class="csl-entry" role="listitem">
M. Hahsler. <em>Pomdp: Infrastructure for partially observable markov decision processes (POMDP).</em> 2023. URL <a href="https://github.com/mhahsler/pomdp">https://github.com/mhahsler/pomdp</a>. R package version 1.1.3.
</div>
<div id="ref-Hahsler2022b" class="csl-entry" role="listitem">
M. Hahsler and A. R. Cassandra. <em>pomdpSolve: Interface to ’pomdp-solve’ for partially observable markov decision processes.</em> 2022. URL <a href="https://github.com/mhahsler/pomdpSolve">https://github.com/mhahsler/pomdpSolve</a>. R package version 1.0.2.
</div>
<div id="ref-Hauskrecht2000" class="csl-entry" role="listitem">
M. Hauskrecht. Value-function approximations for <span>POMDPs</span>. <em>Journal Of Artificial Intelligence Research</em>, 13: 33–94, 2000. DOI <a href="https://doi.org/10.1613/jair.678">https://doi.org/10.1613/jair.678</a>.
</div>
<div id="ref-Jacomy2014" class="csl-entry" role="listitem">
M. Jacomy, T. Venturini, S. Heymann and M. Bastian. ForceAtlas2, a continuous graph layout algorithm for handy network visualization designed for the <span>G</span>ephi software. <em>PLOS ONE</em>, 9(6): 1–12, 2014. DOI <a href="https://doi.org/10.1371/journal.pone.0098679">10.1371/journal.pone.0098679</a>.
</div>
<div id="ref-juliapomdp2022" class="csl-entry" role="listitem">
JuliaPOMDP Team. <span>JuliaPOMDP:</span> <span>POMDP</span> packages for <span>Julia</span>. 2022. URL <a href="https://github.com/JuliaPOMDP">https://github.com/JuliaPOMDP</a>.
</div>
<div id="ref-Kaelbling1998" class="csl-entry" role="listitem">
L. P. Kaelbling, M. L. Littman and A. R. Cassandra. Planning and acting in partially observable stochastic domains. <em>Artificial Intelligence</em>, 101(1): 99–134, 1998. DOI <a href="https://doi.org/10.1016/S0004-3702(98)00023-X">10.1016/S0004-3702(98)00023-X</a>.
</div>
<div id="ref-hahsler:Kamalzadeh:2021" class="csl-entry" role="listitem">
F. Kamalzadeh, V. Ahuja, M. Hahsler and M. E. Bowen. An analytics-driven approach for optimal individualized diabetes screening. <em>Production and Operations Management</em>, 30(9): 3161–3191, 2021. URL <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/poms.13422">https://onlinelibrary.wiley.com/doi/abs/10.1111/poms.13422</a>.
</div>
<div id="ref-Kurniawati2008" class="csl-entry" role="listitem">
H. Kurniawati, D. Hsu and W. S. Lee. <span>SARSOP:</span> <span>E</span>fficient point-based POMDP planning by approximating optimally reachable belief spaces. In <em>In proc. Robotics: Science and systems</em>, 2008.
</div>
<div id="ref-Littman1995" class="csl-entry" role="listitem">
M. L. Littman, A. R. Cassandra and L. P. Kaelbling. Learning policies for partially observable environments: <span>S</span>caling up. In <em>Proceedings of the twelfth international conference on international conference on machine learning</em>, pages. 362–370 1995. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc. ISBN 1558603778.
</div>
<div id="ref-Microsoft2022" class="csl-entry" role="listitem">
Microsoft and S. Weston. <em>Foreach: Provides foreach looping construct.</em> 2022. URL <a href="https://CRAN.R-project.org/package=foreach">https://CRAN.R-project.org/package=foreach</a>. R package version 1.5.2.
</div>
<div id="ref-pypomdp2013" class="csl-entry" role="listitem">
B. Migge and O. Stollmann. <span class="nocase">pyPOMDP:</span> <span>POMDP</span> implementation in <span>Python</span>. 2013. URL <a href="https://bitbucket.org/bami/pypomdp/src/master/">https://bitbucket.org/bami/pypomdp/src/master/</a>.
</div>
<div id="ref-Monahan1982" class="csl-entry" role="listitem">
G. E. Monahan. A survey of partially observable <span>M</span>arkov decision processes: <span>T</span>heory, models, and algorithms. <em>Management Science</em>, 28(1): 1–16, 1982.
</div>
<div id="ref-Mundhenk2000" class="csl-entry" role="listitem">
M. Mundhenk. The complexity of optimal small policies. <em>Math. Oper. Res.</em>, 25(1): 118–129, 2000.
</div>
<div id="ref-Pineau2003" class="csl-entry" role="listitem">
J. Pineau, G. Gordon and S. Thrun. Point-based value iteration: <span>A</span>n anytime algorithm for <span>POMDPs</span>. In <em>Proceedings of the 18th international joint conference on artificial intelligence</em>, pages. 1025–1030 2003. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.
</div>
<div id="ref-Proellochs2020" class="csl-entry" role="listitem">
N. Proellochs and S. Feuerriegel. <em>ReinforcementLearning: <span>M</span>odel-free reinforcement learning.</em> 2020. URL <a href="https://CRAN.R-project.org/package=ReinforcementLearning">https://CRAN.R-project.org/package=ReinforcementLearning</a>. <span>R</span> package version 1.0.5.
</div>
<div id="ref-Puterman1994" class="csl-entry" role="listitem">
M. L. Puterman. Markov decision processes: <span>D</span>iscrete stochastic dynamic programming. In <em>Wiley series in probability and statistics</em>, 1994.
</div>
<div id="ref-Smallwood1973" class="csl-entry" role="listitem">
R. D. Smallwood and E. J. Sondik. The optimal control of partially observable <span>M</span>arkov decision processes over a finite horizon. <em>Operations Research</em>, 21(5): 1071–88, 1973.
</div>
<div id="ref-ZMDP2009" class="csl-entry" role="listitem">
T. Smith. <span>ZMDP</span>: <span>S</span>oftware for <span>POMDP</span> and <span>MDP</span> planning. 2009. URL <a href="https://github.com/trey0/zmdp">https://github.com/trey0/zmdp</a>.
</div>
<div id="ref-Sondik1971" class="csl-entry" role="listitem">
E. J. Sondik. The optimal control of partially observable <span>M</span>arkov decision processes. 1971.
</div>
<div id="ref-Sutton1998" class="csl-entry" role="listitem">
R. S. Sutton and A. G. Barto. <em>Reinforcement learning: <span>A</span>n introduction.</em> Second The MIT Press, 2018.
</div>
<div id="ref-Wickham2016" class="csl-entry" role="listitem">
H. Wickham. <em>ggplot2: Elegant graphics for data analysis.</em> Springer-Verlag New York, 2016. URL <a href="https://ggplot2.tidyverse.org">https://ggplot2.tidyverse.org</a>.
</div>
<div id="ref-Zhang1996" class="csl-entry" role="listitem">
N. L. Zhang and W. Liu. Planning in stochastic domains: <span>P</span>roblem characteristics and approximation. HKUST-CS96-31. Hong Kong University. 1996.
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="references">References</h3>
  <div id="references-listing"></div>
  <h3 id="reuse">Reuse</h3>
  <p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
  <h3 id="citation">Citation</h3>
  <p>For attribution, please cite this work as</p>
  <pre class="citation-appendix short">Hahsler &amp; Cassandra, "Pomdp: A Computational Infrastructure for Partially Observable Markov Decision Processes", The R Journal, 2025</pre>
  <p>BibTeX citation</p>
  <pre class="citation-appendix long">@article{RJ-2024-021,
  author = {Hahsler, Michael and Cassandra, Anthony R.},
  title = {Pomdp: A Computational Infrastructure for Partially Observable Markov Decision Processes},
  journal = {The R Journal},
  year = {2025},
  note = {https://doi.org/10.32614/RJ-2024-021},
  doi = {10.32614/RJ-2024-021},
  volume = {16},
  issue = {2},
  issn = {2073-4859},
  pages = {116-133}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<!--radix_placeholder_navigation_after_body--><html><body>
<div class="distill-site-nav distill-site-footer">
<p>© The R Foundation, <a href="mailto:r-journal@r-project.org">web page
contact</a>.</p>
</div>
<!--/radix_placeholder_navigation_after_body-->
</body></html>


</body>

</html>
