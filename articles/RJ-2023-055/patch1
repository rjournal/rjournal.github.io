diff --git a/Proofs/2023-2/2022-25/filename.tex b/Proofs/2023-2/2022-25/filename.tex
index 3c4c577c3..c5f97bd9b 100644
--- a/Proofs/2023-2/2022-25/filename.tex
+++ b/Proofs/2023-2/2022-25/filename.tex
@@ -64,7 +64,7 @@ Finally, we assign to the temporal random effect term, $w_{jt}$, a Gaussian rand
 w_{jt} \mid w_{(j-1)t}, \tau_w \sim \text{Normal}(w_{(j-1)t},\tau_w^{-1}).
 \]
 
-The Bayesian representation of the above model is completed once we select priors for the fixed effects $\beta_0$ and {\boldmath$\beta$} and the hyperparameters: $\tau_{\epsilon}, \tau_z, \tau_b, \tau_w,$ and $\phi$. For the fixed effects we selected minimally informative Normal distributions, whereas for the hyperparameters we specified "penalising complexity" (PC)  priors \citep{simpson2017penalising}. PC priors are defined by penalising deviations from a ``base'' model (e.g.,~specified in terms of a specific value of the hyperparameters) and have the effect of regularising inference, while not implying too strong prior information. Technically, PC priors imply an exponential distribution on a function of the Kullback–Leibler divergence between the base model and an alternative model in which the relevant parameter is unrestricted. This translates to a suitable ``minimally informative'', regularising prior on the natural scale of the parameter.
+The Bayesian representation of the above model is completed once we select priors for the fixed effects $\beta_0$ and {\boldmath$\beta$} and the hyperparameters: $\tau_{\epsilon}, \tau_z, \tau_b, \tau_w,$ and $\phi$. For the fixed effects we selected minimally informative Normal distributions, whereas for the hyperparameters we specified "penalising complexity" (PC)  priors \citep{simpson2017penalising}. PC priors are defined by penalising deviations from a ``base'' model (e.g.,~specified in terms of a specific value of the hyperparameters) and have the effect of regularising inference, while not implying too strong prior information. Technically, PC priors imply an exponential distribution on a function of the Kullback--Leibler divergence between the base model and an alternative model in which the relevant parameter is unrestricted. This translates to a suitable ``minimally informative'', regularising prior on the natural scale of the parameter.
 
 In order to quantify the weekly excess mortality at sub-national level for specific age-sex population groups, we need to predict the number of deaths that would be expected if the COVID-19 pandemic had not occurred. In Bayesian analysis, this can be performed by drawing random samples from the posterior predictive distribution (that is, the distribution of unobserved values conditional on the observed values from previous years). Specifically, letting $\pmb{\theta}$ be the model parameters, $\mathcal{D}$ be the observed data, and $y_{jst^{*}}$ be the count of deaths that we want to predict, we have:
 
@@ -250,7 +250,7 @@ UL = quantile(c_across(V1:V1000), probs= 0.975)) %>%
 	select(ID_space, median, LL, UL) %>% 
 	head()
 		
-# A tibble: 107 × 4
+# A tibble: 107 x 4
 # Rowwise:  ID_space
 # ID_space median    LL    UL
 # <chr>     <dbl> <dbl> <dbl>
@@ -264,7 +264,7 @@ UL = quantile(c_across(V1:V1000), probs= 0.975)) %>%
 # 8 008          86   68   107 
 # 9 009         113   93   135.
 # 10 010         341  301.  380 
-# … with 97 more rows
+# ... with 97 more rows
 \end{example}
 	
 \subsection{Excess mortality}
