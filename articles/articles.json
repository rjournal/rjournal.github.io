[
  {
    "path": "articles/RJ-2023-032/",
    "title": "markovMSM: An R Package for Checking the Markov Condition in Multi-State Survival Data",
    "description": "Multi-state models can be used to describe processes in which an individual moves through a finite number of states in continuous time. These models allow a detailed view of the evolution or recovery of the process and can be used to study the effect of a vector of explanatory variables on the transition intensities or to obtain prediction probabilities of future events after a given event history. In both cases, before using these models, we have to evaluate whether the Markov assumption is tenable. This paper introduces the [markovMSM](https://CRAN.R-project.org/package=markovMSM) package, a software application for R, which considers tests of the Markov assumption that are applicable to general multi-state models. Three approaches using existing methodology are considered: a simple method based on including covariates depending on the history; methods based on measuring the discrepancy of the non-Markov estimators of the transition probabilities to the Markovian Aalen-Johansen estimators; and, finally, methods that were developed by considering summaries from families of log-rank statistics where individuals are grouped by the state occupied by the process at a particular time point. The main functionalities of the [markovMSM](https://CRAN.R-project.org/package=markovMSM) package are illustrated using real data examples.",
    "author": [
      {
        "name": "Gustavo Soutinho",
        "url": {}
      },
      {
        "name": "Luís Meira-Machado",
        "url": {}
      }
    ],
    "date": "2023-09-24",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-032.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-039/",
    "title": "A Framework for Producing Small Area Estimates Based on Area-Level Models in R",
    "description": "The R package [emdi](https://CRAN.R-project.org/package=emdi) facilitates the estimation of regionally disaggregated indicators using small area estimation methods and provides tools for model building, diagnostics, presenting, and exporting the results. The package version 1.1.7 includes unit-level small area models that rely on access to micro data. The area-level model by @Fay1979 and various extensions have been added to the package since the release of version 2.0.0. These extensions include (a) area-level models with back-transformations, (b) spatial and robust extensions, (c) adjusted variance estimation methods, and (d) area-level models that account for measurement errors. Corresponding mean squared error estimators are implemented for assessing the uncertainty. User-friendly tools like a stepwise variable selection, model diagnostics, benchmarking options, high quality maps and results exportation options enable a complete analysis procedure. The functionality of the package is illustrated by examples based on synthetic data for Austrian districts.",
    "author": [
      {
        "name": "Sylvia Harmening",
        "url": {}
      },
      {
        "name": "Ann-Kristin Kreutzmann",
        "url": {}
      },
      {
        "name": "Sören Schmidt",
        "url": {}
      },
      {
        "name": "Nicola Salvati",
        "url": {}
      },
      {
        "name": "Timo Schmid",
        "url": {}
      }
    ],
    "date": "2023-09-24",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-039.zip\n\n\nR. E. Fay and R. A. Herriot. Estimates of income for small places: An application of James-Stein procedures to census data. Journal of the American Statistical Association, 74(366): 269–277, 1979. URL https://doi.org/10.1080/01621459.1979.10482505.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-031/",
    "title": "Onlineforecast: An R Package for Adaptive and Recursive Forecasting",
    "description": "Systems that rely on forecasts to make decisions, e.g. control or energy trading systems, require frequent updates of the forecasts. Usually, the forecasts are updated whenever new observations become available, hence in an online setting. We present the [R]{.sans-serif} package [[onlineforecast](https://onlineforecasting.org)]{.sans-serif} that provides a generalized setup of data and models for online forecasting. It has functionality for time-adaptive fitting of dynamical and non-linear models. The setup is tailored to enable the effective use of forecasts as model inputs, e.g. numerical weather forecast. Users can create new models for their particular applications and run models in an operational setting. The package also allows users to easily replace parts of the setup, e.g. using new methods for estimation. The package comes with comprehensive vignettes and examples of online forecasting applications in energy systems, but can easily be applied for online forecasting in all fields.",
    "author": [
      {
        "name": "Peder Bacher",
        "url": {}
      },
      {
        "name": "Hjörleifur G. Bergsteinsson",
        "url": {}
      },
      {
        "name": "Linde Frölke",
        "url": {}
      },
      {
        "name": "Mikkel L. Sørensen",
        "url": {}
      },
      {
        "name": "Julian Lemos-Vinasco",
        "url": {}
      },
      {
        "name": "Jon Liisberg",
        "url": {}
      },
      {
        "name": "Jan Kloppenborg Møller",
        "url": {}
      },
      {
        "name": "Henrik Aalborg Nielsen",
        "url": {}
      },
      {
        "name": "Henrik Madsen",
        "url": {}
      }
    ],
    "date": "2023-09-07",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-033/",
    "title": "Robust Functional Linear Regression Models",
    "description": "With advancements in technology and data storage, the availability of functional data whose sample observations are recorded over a continuum, such as time, wavelength, space grids, and depth, progressively increases in almost all scientific branches. The functional linear regression models, including scalar-on-function and function-on-function, have become popular tools for exploring the functional relationships between the scalar response-functional predictors and functional response-functional predictors, respectively. However, most existing estimation strategies are based on non-robust estimators that are seriously hindered by outlying observations, which are common in applied research. In the case of outliers, the non-robust methods lead to undesirable estimation and prediction results. Using a readily-available [R]{.sans-serif} package robflreg, this paper presents several robust methods build upon the functional principal component analysis for modeling and predicting scalar-on-function and function-on-function regression models in the presence of outliers. The methods are demonstrated via simulated and empirical datasets.",
    "author": [
      {
        "name": "Ufuk Beyaztas",
        "url": {}
      },
      {
        "name": "Han Lin Shang",
        "url": {}
      }
    ],
    "date": "2023-09-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-033.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-021/",
    "title": "A Hexagon Tile Map Algorithm for Displaying Spatial Data",
    "description": "Spatial distributions have been presented on alternative representations of geography, such as cartograms, for many years. In modern times, interactivity and animation have allowed alternative displays to play a larger role. Alternative representations have been popularised by online news sites, and digital atlases with a focus on public consumption. Applications are increasingly widespread, especially in the areas of disease mapping, and election results. The algorithm presented here creates a display that uses tessellated hexagons to represent a set of spatial polygons, and is implemented in the R package called sugarbag. It allocates these hexagons in a manner that preserves the spatial relationship of the geographic units, in light of their positions to points of interest. The display showcases spatial distributions, by emphasising the small geographical regions that are often difficult to locate on geographic maps.",
    "author": [
      {
        "name": "Stephanie Kobakian",
        "url": {}
      },
      {
        "name": "Dianne Cook",
        "url": {}
      },
      {
        "name": "Earl Duncan",
        "url": {}
      }
    ],
    "date": "2023-08-26",
    "categories": [],
    "contents": "\n\n\n\n\n\n\nIntroduction\n\nMany cancer atlases present geospatial cancer data on a choropleth map display. The Australian Cancer Atlas (Cancer Council Queensland, Queensland University of Technology, and Cooperative Research Centre for Spatial Information 2018) is a recent addition to the many cancer atlas maps worldwide. The ground-breaking atlas for Australia presents a central map that shows the landmass overlaid with administrative boundaries. This choropleth display can highlight the geographic patterns in geospatially related cancer statistics (Moore and Carpenter 1999).\n\nOver time, the population density in major cities has increased as residents have gathered to live near urban areas (Dorling 2011). Populations tend to be distributed unevenly across geographic regions. When comparing sets of administrative geographic units such as states or electorates, area size is seldom proportional to the population size. In a choropleth map display, the geographic units are coloured to represent the value of the statistic for each unit (Tufte 1990). This can cause some values to be emphasised over others, and allows choropleth map displays to misrepresent the spatial distributions of human related statistics due to area-size bias (Skowronnek 2016).\nThe Australian Cancer Atlas is an online, interactive display of Australian cancer statistics in each of the Statistical Areas at Level 2 (SA2s), used by the Australian Bureau of Statistics (2011). The dataset of estimated standardised incidence ratios (SIRs) of thyroid cancer for females was downloaded from the publicly accessible Australian Cancer Atlas website and presented in Figure 1, a choropleth map that uses colour to display the distribution. The Australian choropleth map display draws attention to the expanse of dark and light blue areas across the rural communities in all states. The SA2s on the east coast around Brisbane and in northern New South Wales stand out as more orange and red. However, this display neglects the vast amount of Australian residents living in the densely populated capital cities.\n\n\n\n\n\n\nFigure 1: A choropleth map of thyroid incidence among females across the Statistical Areas of Australia at Level 2. Blue indicates lower than average and red indicates higher than average incidence. A cluster of high incidence is visible on the east coast.\n\n\n\n\nThe solutions to this visualisation problem begin with the geography. Alternative maps can be created to shift the focus from land area and shape to the value of the statistics (Dougenik et al. 1985) in a cartogram display. Cartogram methods apply different transformations to the geographic areas, to highlight the values of the statistic of interest. Alternative maps can result in a distortion of the map space to represent features of the distribution across the areas (Dougenik et al. 1985) as the statistic of interest is used to determine the cartogram layout.\nAlternative mapping methods, like cartograms implemented in cartogram (Jeworutzki 2020) for R (R Core Team 2012), promote better understanding of the spatial distribution of a variable across the population, by representing the population in each administrative area fairly (Levison and Haddon Jr 1965). This acknowledges that the number of residents can be different and also recognises that each area, or person within it is equally important.\nThis paper contains a discussion of existing mapping practices, followed by details of the algorithm. The implementation section explains the algorithm available in the sugarbag package. How to utilise animation with the hexagon map is described. The paper finishes with a summary and possible new directions for the algorithm.\nExisting mapping practices\n\nTypically, chloropleth maps are used to orient the users within the geographic context. However, the strong salience of the land mass can hide or downplay the features of the distribution in densely populated communities due to the small size on the display (Dorling 2011). The unique shapes of boundaries can be helpful for orienting users but may not contribute to their understanding of the spatial disease distribution as many of the communities are not visible in a choropleth display (Dorling 2012).\nAdministrative areas are often used to aggregate census data and used to understand demographics within communities of the Australian population. Each SA2 (Australian Bureau of Statistics 2011) was designed to represent a community. This collection of communities presents an opportunity for explicit map transformations to improve communication of spatial distributions (Kocmoud and House 1998). In Figure 2 the choropleth map can be seen underneath each alternative map display to allow for comparisons to be made.\nThere are several established alternative visualisation methods. Rectangular cartograms (Kreveld and Speckmann 2007) and Dorling cartograms (Dorling 2011) implemented in cartogram and tile maps implemented in tilemaps (Rosenberg 2020), all use one simple shape to represent each geographic unit. They all minimise the salience of the size or shape of geographic areas. These alternative map displays highlight the relationship between neighbours, preserve connections, and discard the unique shapes of the administrative boundaries. Figure 2 shows a collection of alternative map displays, this includes a) a contiguous cartogram, b) a non-contiguous cartogram, and c) a Dorling cartogram.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: The three displays show alternative maps of the Australian state of Tasmania at SA2 level: (a) contiguous cartogram, (b) non-contiguous cartogram and (c) Dorling cartogram of Tasmania. The contiguous cartogram looks like the state has an hourglass figure, while the non-contiguous cartogram shrinks areas into invisibility. The Dorling expands the metropolitan regions.\n\n\n\nWhen communicating information that is relevant to the population, each member of the population can be given equal representation by transforming the map (Dorling 2012). The connectedness of the units can be preserved by using transformations that maintain the connection between boundaries. The contiguous cartogram displayed in Figure 2a draws attention to smaller geographic units when they are rescaled according to the population (Walter 2001). These new shapes can now be coloured to represent a second variable. The algorithm uses the geographic shape of the areas and iterates toward sizing the areas to represent the population. This display can create twisted and unfamiliar shapes from the geographic units as the algorithms must satisfy the topology conditions, especially when there are communities located geographically far from their neighbours (Dorling 2012).\n\nThe non-contiguous cartogram in Figure 2b also uses the population to rescale the geographic units. Unlike the contiguous cartogram, the SA2 areas maintain their geographic shape, but they may not retain the connection with their neighbours. The population of the SA2 areas is used to scale the geographic units in the non-contiguous cartogram (Olson 1976). The amount of background space can be meaningful in non-contiguous cartograms (Keim et al. 2002). However, the relative size of units in comparison to the reference unit can lead to scaling issues. The size disparity between rural areas and urban areas result in reasonable display within the south eastern city of Hobart, but these units are not visible in the context of the Tasmania state map. Depending on the difference between the population and geographic land size, the amount of white space can also prevent meaningful understanding of the distribution (Dorling 2012).\n\nThe Dorling cartogram presents each geographic unit as a circle, the size of the circle is scaled according to the population value of each area (Dorling 2011). Figure 2c shows the Tasmania SA2 areas as an individual circle located as close as possible to the geographic centroid location. This map draws attention to the collection of coastal cities in Tasmania that were not apparent in Figure 2 a or b. This display also highlights the difference in the population of each unit, as there is some disparity in the sizes of the circles.\nAnother common practice is to use equal-sized polygons and treat all geographic units as equal. The Dorling algorithm can be induced to do this, to make equallly sized circles. Cano et al. (2015) describes a group of alternative maps called “mosaic cartograms” for tile displays. Similarly, the geography of USA has been transformed into a square-binned tile map manually and made available in the R package statebins (Rudis 2020).The algorithm described below generates an automatic layout of hexagon tiles, where a continguity constraint is relaxed. This gives equal weight to each geographic area even if the population differs.\nThe nature of Australian population boundaries means that there are many connecting neighbours of population areas. This solution was designed to visualise Australian population analysis, inspired by tile maps three sided triangles and four sided squares were considered, but this would not have allowed sufficient neighbours to connect. Hexagon tile maps utilise the tessellating features of hexagons to maximise the data ink and allow connectivity between six potential neighbours. Hexagons allow the alignment of geographic borders, this aides in comparison of colour values.\nAlgorithm\nThe purpose of this algorithm is to create an alternative map display that highlights distributional features across wide ranges of geographical area size and population density. There has been an increasing need for displays that recognise the large number of people that live in dense urban environments. The algorithm intends to maintain the spatial relationships of a group of geographic units using the relationship between each unit and the closest focal point. The algorithm allocates geographic units to a representative hexagon, in order of their proximity to the closest focal point.\nThe algorithm is implemented in the sugarbag package for R, named after the common name for the Trigona carbonaria bee – a species which builds flat layers of hexagonal brood cells, spiralling out from a central point (Vit et al. 2013). This unusual pattern also provided the inspiration for the algorithm, where the maps are constructed by building out from multiple focal points on the geographic map base in a spiral fashion.\nThe result of the algorithm applied to the same data represented in Figure 1 shows rates of thyroid cancer for females in each of the SA2 areas of Australia as hexagons in Figure 3.\nThe difference to the choropleth map in Figure 1 are clear. Now the higher thyroid cancer incidence in the densely populated areas in Sydney, Brisbane and Perth are visible. Interestingly, there is no clear divide between rural and urban SA2 areas, as many rural areas and the cities of Melbourne, Darwin, Adelaide and Hobart have low rates of thyroid incidence for females. The hexagon tile map display provides a more accurate representation of the spatial distribution of thyroid cancer incidence across Australia.\nFigure 3 highlights the density of Australian capital cities, as it draws attention to the many communities in Sydney, Melbourne and Hobart. This display also highlights the disparity in the burden of thyroid cancer for females in the communities of these cities. There are several collections of red hexagons in Sydney that represent the communities with much higher rates of diagnosis than the Australian average. Brisbane also experiences higher than average rates of diagnosis, but has more orange than red. The females in the cities of Adelaide and Perth show much lower rates of diagnosis.\nCompared to the choropleth map display in Figure 1, the low rates in the rural Australian communities no longer dominate the display. While the corresponding hexagons are still visible against the black background, the lower rates in rural Australia are less prominent.\n\n\n\n\n\nFigure 3: A hexagon tile map of female thyroid cancer incidence in Australia, the same data as shown in the choropleth map in Figure 1. The high incidence in several of the metropolitan regions (Brisbane, Sydney and Perth) can now be seen, along with numerous isolated spots.\n\n\n\nThere are several key steps in the creation of the hexagon tile map as described in the flow chart in Figure 4. First, derive the set of centroids from the polygons provided, then create the grid of hexagon locations. These two processes are described in the blue left column of the flow chart in figure 4. Each centroid can then be allocated to an available hexagon location. The steps for the allocation process are detailed in the right column of Figure 4. There are several filter steps to speed up the process of selecting an appropriate hexagon to represent each geographic unit. To make tessellated plots with the hexagon allocations, the point locations are converted into hexagon shapes.\n\n\n\nFigure 4: A flow diagram detailing the steps taken to create a hexagon tile map. There are two basic processes, one to make the grid, and the other to allocate centroids to grid points.\n\n\n\nImplementation\n\nHexagon tile maps can be useful for visualising distributions across a collection of geographic areas. However, these maps are not easy to create manually, especially as the number of areas increases. This algorithm was developed to automate the process, and reduce the workload involved in creating and implementing alternative displays. This allows map makers and data communicators to spend their time choosing the most effective display.\nThe sugarbag package contains a set of functions that help R users to create a hexagon tile map. The algorithm presented in the sugarbag package operates on a set of simple feature geometry objects , known as sf objects (Pebesma 2018). This package allows R users to create sf objects by importing polygons stored in various formats. Users should provide a set of polygons that define geographic units by their administrative boundaries. The functions arrange the geographic units in order of proximity to a set of locations provided, such as the centre of major cities. The centroid location of each geographic unit is used to measure the proximity. It emphasises the major cities as population hubs, rather than emphasizing the size of large, rural geographic units.\nThe user can tweak the parameters of the hexagon map using additional arguments to the function, but these options may affect the speed of the algorithm.\nThe hexagon size may need adjustments depending on the density of the population; users can provide an appropriate hexagon size and re-run the algorithm. The buffer distance may need to be increased if the coastal cities need to extend beyond the geographic land mass.\nAlgorithm steps\nThe package can be installed from CRAN and the development version can be installed from the GitHub repository: https://github.com/srkobakian/sugarbag.\nThe following steps create the hexagon tile map for all the Statistical Areas at Level 2 in Tasmania. These steps can be executed by the main function, , or can be run separately for more flexibility.\nIf a user would like to perform steps of the algorithm themselves, additional user input will be needed for the functions that perform each step. For example, if the user wishes to use a set of centroids, rather than polygons, the function can be used directly.\nThe set of SA2 polygons for Tasmania, using the 2011 definition of SA2s, was accessed from the absmapsdata package (Mackey 2022). A single column of the data set is used to identify the unique areas. In this case, the unique SA2 names for each SA2 have been used.\nThe Australian capital cities are used as focal points to allocate each geographic area around the closest capital city. Hobart will be the focal point for this example because only the state of Tasmania is being processed.\n\n\n\nThe buffer distance, hexagon size, hexagon amount to filter and width of angle are parameters that will be determined within , if they are not provided. They are created as they are needed throughout the following example.\nThe results from various steps in the process are illustrated in Figure 5.\nDerive the set of centroid points\nThe set of polygons should be provided as an sf object, this is a data frame containing a column. The function can assist in creating this object for use in R.\nThe centroids can be derived from an sf object using the function:\n\n\n\nStep 1: Create the hexagon grid points\nA grid is created to allow tessellation of the hexagons that represent the geographic units. For a hexagon tile map, the grid of possible hexagon locations is made using the function. It uses the centroids, the hexagon size and the buffer distance. Figure 5 1a shows the initial locations of grid points created for Tasmania.\ncentroids <- create_centroids(\n  shp_sf = sa2 %>% filter(state_name_2011 == \"Tasmania\"), \n  sf_id = \"sa2_name_2011\")\n(a) Creating a tessellated grid\nA set of longitude columns, and latitude rows are created to define the locations of the hexagons. The distance between each row and column is the size specified by . Equally spaced columns are created from the minimum longitude minus the buffer distance, up to the maximum longitude plus the buffer distance. Similarly, the rows are created from the latitude values and the buffer distance. A unique hexagon location is created from all intersections of the longitude columns and latitude rows. Figure 5 1a shows the hexagon grid after every second latitude row on the grid is shifted right, by half of the hexagon size.\ngrid <- create_grid(centroids = centroids, hex_size = 0.2, buffer_dist = 2)\n\n\n\n(b) Filtering the grid\nNot all of the grid points will be used, especially if islands result in a large grid space. To filter the grid for appropriate hexagon locations for allocation, the function will call the function. It finds the grid points needed to best capture the set of centroids on a hexagon tile map. Reducing the buffer size will decrease the amount of time that the algorithm needs to run for, as there will be less points over the water to consider for each centroid allocation.\nFor each centroid location, the closest latitude row and longitude column are found. Then rows and columns of centroids are divided into 20 groups. The number of rows in each latitude group and the number of columns in each longitude group are used as the width of rolling windows. The first rolling window step finds the minimum and maximum centroid values within each of the sliding window groups of longitude columns, and the groups of latitude rows. The second rolling window step finds the average of the rolling minimum and maximum centroid values, for the longitude columns and latitude rows.\nThe hexagon grid points are kept only if they fall within the rolling average of the minimum and maximum centroid values after accounting for the buffer distance, for each row and column of the grid. Figure 5 1b displays remaining hexagon grid points after applying the buffer filter. The sparsely populated South-West region of National Park has fewer points available in the water compared to the South-East region near the city of Hobart.\n\n\n\nCentroid to focal point distance\nThe distance is calculated between each centroid in the set, and each of the focal points provided. The order for allocation is determined by the distance between the polygon centroid and it’s closest focal point. For this example, this distance is only calculated for the capital city of Hobart, represented in Figure 5 2a and 2b as a white cross.\nStep 2: Allocate each centroid to a hexagon grid point\nTo allocate all centroids the set of polygon centroids and the hexagon map grid are required. The polygon centroids are ordered from the centroid closest to the focal point(s), to the furthest. This preserves spatial relationships with the focal point, as the inner city areas are allocated first and placed closest to the capital, the areas that are further will then be accommodated. The following example considers the first of the Statistical Areas at Level 2. Within the algorithm, these steps are repeated for each polygon.\nhex_allocated <- allocate(centroids = centroids,\n                          sf_id = \"SA2_NAME16\",\n                          hex_grid = grid,\n                          hex_size = 0.2, # same size used in create_grid\n                          hex_filter = 10,\n                          use_neighbours = tas_sa2,\n                          focal_points = capital_cities,\n                          # same column used in create_centroids\n                          width = 30, verbose = TRUE)\n\n\n\n(a) Filter the grid for unassigned hexagon points\nAfter each centroid is located, it is removed from the set of grid points, and is no longer considered in the next step. Keeping only the available hexagon points prevents multiple geographic units from being allocated to the same hexagon. This is demonstrated in Figure 5 2a and 2b by the black hexagons that represent the seven closest polygons to the capital city of Hobart. As the allocation process begins for the eighth closest centroid there are seven unavailable hexagon locations.\n(b) Filter the grid points for those closest to the centroid\nThe algorithm creates a circle of points, by only keeping points within a certain radial distance around the original centroid location. Only the hexagons which are close to the centroid and have not been assigned are considered. The number of possible hexagon locations to consider for a centroid is determined by the hexagon filter. This is the maximum number of hexagons between the centroid and the furthest considered hexagon. It is used to subset possible grid points to only those surrounding the polygon centroid within an appropriate range. A smaller distance will increase speed, but can decrease accuracy when width of the angle increases.\n\n\n\nThe parameter is used to take a slice of the remaining points. The slice centres on the angle from the focal point to centroid location. This uses the angle from the closest capital city, to the current centroid as seen in Figure 5 2a. This allows the spatial relationship to be preserved, even when it is allocated to a hexagon that is further from the focal point then the original centroid location.\n\n\n\nFigure 5: Illustration of key steps of the algorithm: (1a) full hexagon grid is created first; (1b) buffer is applied, shown as dark green circles, to accommodate irregularly shaped regions; (2a, 2b) allocation process, relative the center of Hobart, showing the 8th centroid to be allocated. The relationship between Hobart (the cross) and the centroid (the purple triangle) is used to filter the potential locations from the grid within a wedge. Hexagons already allocated are shown in black, and the purple circle indicates the hexagon to be assigned to the 8th centroid.\n\n\n\nIf no available hexagon grid point is found within the original filter distance and angle, the distance is expanded and only when a maximum distance is reached will the angle expand to accommodate more possible grid points.\nBy default the angle filter for the hexagon grid points create plus and minus 30 degree bounds of the angle from the focal point to the geographic centroid. This will increase if no points can be found within the distance. The default angle of 30 was chosen to allow the algorithm to find hexagons that best maintained the spatial relationship between the focal point and geographic centroid.\n\n\nUser choices\n\n\nOnly two inputs are necessary to begin the algorithm; the shapefile, and the ID variable. The ID variable should uniquely identify each geographic unit in the shapefile.\nThe centroids are derived from the shapefile. The number of centroids within the geographic map is used to determine an appropriate hexagon size if one is not provided. The centroids are used to construct a grid. The grid initially covers the entire map, encompassing all the centroid locations and extending in all directions up to the buffer distance. This buffer distance can help to account for densely populated coastal areas, allowing the hexagon locations to spread beyond the coastline.\nThe centroid set and hexagon tile map grid are necessary for the allocation process. Additionally, a set of reference locations can be provided as focal points, typically compact and/or densely populated areas such as major cities. The algorithm will use the focal points to create the order of allocation, prioritising the closest centroid locations to the focal points. The user can also specify the variable that should be used to determine the order for the allocation.\nWhen allocating representative hexagons, the width parameter can be used to determine the flexibility of positioning. Using the relationship with the nearest focal point, a larger width parameter will increase the amount of available hexagons nearer to the original centroid of the geographic unit. A smaller width will maintain the orientation from the focal point to the centroid when selecting the hexagon location. However, this could mean it is placed further away.\nAnimation\nCreating an animation connecting these two displays can allow users to grasp the meaning of the connected hexagons. It will also highlight the density of the communities using the rapid expansion of the inner-city areas, these hexagons will move the furthest and will move rapidly in the animation from their geographic locations. The rapid decrease in size of the large rural areas can show the large size of their collective landmass. The gganimate (Pedersen and Robinson 2019) package can be used to make an animation. It connects the polygons for each area in the two displays using the variable, the names of the statistical areas, and creates intermediate displays as they geographic areas shrink or expand to the chosen hexagon size.\nDiscussion\nThe present work describes an algorithm called the hexagon tile map.\nIt also shows how this algorithm addresses the potential issues found with contiguous, non-contiguous and Dorling cartograms when applied to the geography of Australia. It achieves a display that effectively presents the density of the population in small geographic areas and de-emphasises the large rural geographic space between the densely populated capital cities. The hexagon tile map display acknowledges that the amount of residents can differ but each administrative area is equally important. This solution uses equally sized areas, and maintain neighbourhood boundary connections.\n\nThe hexagon tile map display acknowledges that the amount of residents can differ but each administrative area is equally important. This solution uses equally sized areas, and maintain neighbourhood boundary connections. The sugarbag package for R automates the creation of tessellated hexagon tile maps by providing an algorithm to design these displays. The Australian application preserves spatial relationships, and emphasises capital cities.\nHowever, similar to the choropleth map display, the tessellation does not allow the size of the hexagons to represent another variable. The algorithm is heavily dependent on the location and number of focal points used, as this determines the order of allocation. With careful consideration of the choropleth map, the small geographic inner city areas may not have been noticed by viewers, but the hexagon tile map display emphasises them. The communities in northern Tasmania and the Northern territory do not draw attention because of their size as in the choropleth, but their colour is still noticeably below average when contrasted with the hexagons in New South Wales.\n\nAustralia serves as a good use case for the algorithm as it represents an extremely sparse geography and small dense population areas which cannot be satisfactorily represented by existing cartogram-like algorithms. However, the algorithm is sufficiently general to apply to other geographic regions, and indeed has been applied to make hexagon maps of the UK, USA and France.\nThere are many possible extensions to the algorithm that might prove valuable in the future. The buffer allows extension beyond the furthest centroids, but there is no mechanism to protect borders by forcing centroids to be located within the geographic borders of a set of regions (e.g. country or state). The algorithm is also currently limited to representing each geographic area with one hexagon only. In the filter step for final selection of a hexagon grid point for each centroid, a direct angle constraint is currently implemented, but it may narrowly miss potentially better hexagon allocation. Some other constraints, such as a logarithmic function may help to refine the final allocation to hexagons closer to the original centroid location. In a country like Australia, the vast empty interior produces a hexagon map with many small isolated hexagons, even when the city centres are hugely expanded. It could be useful to allow variable sized hexagons, primarily to increase the size of the isolated hexagons to improve their visibility.\nThis new algorithm provides an effective start point for automatically creating hexagon tile maps for any spatial polygon data, and contributes to an extensive body of work that encourages the use of alternative map displays for statistical displays of spatial data.\n\nCRAN packages used\ncartogram, sugarbag, tilemaps, statebins, sf, gganimate\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, TeachingStatistics\n\n\nAustralian Bureau of Statistics. Statistical Area Level 2 (SA2) ASGS Ed 2011 Digital Boundaries in ESRI Shapefile Format., 2011.\n\n\nCancer Council Queensland, Queensland University of Technology, and Cooperative Research Centre for Spatial Information. Australian Cancer Atlas. 2018. URL https://atlas.cancer.org.au.\n\n\nR. G. Cano, K. Buchin, T. Castermans, A. Pieterse, W. Sonke and B. Speckmann. Mosaic Drawings and Cartograms. In Computer graphics forum, pages. 361–370 2015. Wiley Online Library.\n\n\nD. Dorling. Area cartograms: Their use and creation. In Concepts and techniques in modern geography (CATMOG), pages. 252–260 2011. ISBN 9780470979587. DOI 10.1002/9780470979587.ch33.\n\n\nD. Dorling. The Visualisation of Spatial Social Structure. John Wiley; Sons Ltd, 2012.\n\n\nJ. A. Dougenik, N. R. Chrisman and D. R. Niemeyer. An Algorithm to Construct Continuous Area Cartograms. The Professional Geographer, 37(1): 75–81, 1985. URL https://doi.org/10.1111/j.0033-0124.1985.00075.x.\n\n\nS. Jeworutzki. Cartogram: Create cartograms with r. 2020. URL https://CRAN.R-project.org/package=cartogram. R package version 0.2.2.\n\n\nD. A. Keim, S. C. North, C. Panse and J. Schneidewind. Efficient Cartogram Generation: A Comparison. In IEEE Symposium on Information Visualization, 2002. INFOVIS 2002, pages. 33–36 2002. IEEE. ISBN 076951751X.\n\n\nC. Kocmoud and D. House. A Constraint-based Approach to Constructing Continuous Cartograms. In Proc. Symp. Spatial data handling, pages. 236–246 1998.\n\n\nM. van Kreveld and B. Speckmann. On Rectangular Cartograms. Computational Geometry, 37(3): 175–187, 2007. URL http://www.sciencedirect.com/science/article/pii/S0925772106000770. Special Issue on the 20th European Workshop on Computational Geometry.\n\n\nM. E. Levison and W. Haddon Jr. The Area Adjusted Map. An Epidemiologic Device. Public Health Reports, 80: 55–59, 1965.\n\n\nW. Mackey. Absmapsdata: A catalogue of ready-to-use ASGS (and other) sf objects. 2022. R package version 1.3.3.\n\n\nD. A. Moore and T. E. Carpenter. Spatial Analytical Methods and Geographic Information Systems: Use in Health Research and Epidemiology. Epidemiologic Reviews, 21(2): 143–161, 1999. URL https://doi.org/10.1093/oxfordjournals.epirev.a017993.\n\n\nJ. M. Olson. Noncontiguous Area Cartograms. The Professional Geographer, 28(4): 371–380, 1976. URL https://doi.org/10.1111/j.0033-0124.1976.00371.x.\n\n\nE. Pebesma. Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal, 10(1): 439–446, 2018. URL https://doi.org/10.32614/RJ-2018-009.\n\n\nT. L. Pedersen and D. Robinson. gganimate: A Grammar of Animated Graphics. 2019. URL https://CRAN.R-project.org/package=gganimate. R package version 1.0.3.\n\n\nR Core Team. R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing, 2012. URL http://www.R-project.org/. ISBN 3-900051-07-0.\n\n\nK. Rosenberg. Tilemaps: Generate tile maps. 2020. URL https://CRAN.R-project.org/package=cartogram. R package version 0.2.0.\n\n\nB. Rudis. Statebins: Create united states uniform cartogram heatmaps. 2020. URL https://CRAN.R-project.org/package=statebins. R package version 1.4.0.\n\n\nA. Skowronnek. Beyond Choropleth Maps – A Review of Techniques to Visualize Quantitative Areal Geodata., 2016. URL https://alsino.io/static/papers/BeyondChoropleths_AlsinoSkowronnek.pdf.\n\n\nE. R. Tufte. Envisioning information. Graphics Press, 1990.\n\n\nP. Vit, S. R. M. Pedro and D. Roubik. Pot-Honey: A legacy of stingless bees. Springer-Verlag New York, 2013. URL http://dx.doi.org/10.1007/978-1-4614-4960-7.\n\n\nS. D. Walter. Disease Mapping: A Historical Perspective. Oxford University Press, 2001. DOI https://dx.doi.org/10.1093/acprof:oso/9780198515326.003.0012.\n\n\n\n\n",
    "preview": "articles/RJ-2023-021/RJ-2023-021_files/figure-html5/choro-1.png",
    "last_modified": "2023-11-07T21:31:41+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2400
  },
  {
    "path": "articles/RJ-2023-022/",
    "title": "A Clustering Algorithm to Organize Satellite Hotspot Data for the Purpose of Tracking Bushfires Remotely",
    "description": "This paper proposes a spatiotemporal clustering algorithm and its implementation in the R package spotoroo. This work is motivated by the catastrophic bushfires in Australia throughout the summer of 2019-2020 and made possible by the availability of satellite hotspot data. The algorithm is inspired by two existing spatiotemporal clustering algorithms but makes enhancements to cluster points spatially in conjunction with their movement across consecutive time periods. It also allows for the adjustment of key parameters, if required, for different locations and satellite data sources. Bushfire data from Victoria, Australia, is used to illustrate the algorithm and its use within the package.",
    "author": [
      {
        "name": "Weihao Li",
        "url": {}
      },
      {
        "name": "Emily Dodwell",
        "url": {}
      },
      {
        "name": "Dianne Cook",
        "url": "http://www.dicook.org"
      }
    ],
    "date": "2023-08-26",
    "categories": [],
    "contents": "\n\n\n\nIntroduction\nThe 2019-2020 Australia bushfire season was catastrophic in the scale of damage caused to agricultural resources, property, infrastructure, and ecological systems. By the end of 2020, the devastation attributable to these Black Summer fires totalled 33 lives lost, almost 19 million hectares of land burned, over 3,000 homes destroyed and AUD $1.7 billion in insurance losses, as well as an estimated 1 billion animals killed, including half of Kangaroo Island’s population of koalas (Filkov et al. 2020). According to the Australian Government Bureau of Meteorology (2021), 2019 was the warmest year on record in Australia, and the period from 2013-2020 represents eight of the ten warmest years in recorded history. There is concern and expectation that impacts of climate change – including more extreme temperatures, persistent drought, and changes in plant growth and landscape drying – will worsen conditions for extreme bushfires (CSIRO and Australian Government Bureau of Meteorology 2020; Deb et al. 2020). Contributing to the problem is that dry lightning represents the main source of natural ignition, and fires that start in remote areas deep in the temperate forests are difficult to access and monitor (Abram et al. 2021). Therefore, opportunities to detect fire ignitions, monitor bushfire spread, and understand movement patterns in remote areas are important for developing effective strategies to mitigate bushfire impact.\nRemote satellite data provides a potential solution to the challenge of active fire detection and monitoring. Development of algorithms that process satellite imagery into hotspots – points that represent likely fires – is an active area of research (see for example Giglio et al. (2016), Xu and Zhong (2017), Wickramasinghe et al. (2016), Jang et al. (2019)). Throughout this paper and the associated package, we make use of the Japan Aerospace Exploration Agency (JAXA) Himawari-8 satellite wildfire product (P-Tree System 2020) that identifies the location and fire radiative power (FRP) of hotspots across East Asia and Australia according to an algorithm developed by Kurihara et al. (2020). It contains records of 1,989,572 hotspots from October 2019 to March 2020 with a 0.02\\(^o\\) (~2km) spatial resolution and 10-minute temporal resolution. Detection of bushfire ignition and movement requires the clustering of satellite hotspots into meaningful clusters, which may then be considered in their entirety or summarized by a trajectory.\nIn this paper, we present a spatiotemporal clustering algorithm to organize hotspots into groups in order to estimate bushfire ignition locations and track bushfire movements over space and time. Inspired by two existing clustering algorithms, namely Density Based Spatial Clustering of Applications with Noise (DBSCAN) (Ester et al. 1996) and Fire Spread Reconstruction (FSR) (Loboda and Csiszar 2007), our algorithm adopts the notion of noise from DBSCAN, while drawing upon the fire movement dynamics presented in FSR. We generalize the latter’s specification of spatiotemporal parameters, thereby providing an intuitive, straightforward, and extendable approach to the complex problem of bushfire identification and monitoring that may be applied to any satellite wildfire product. In clustering hotspots into bushfires of arbitrary shape and size, we aim to capture key fire behavior:\nfire evolution occurs only forwards in time,\nfires can smolder undetectably for some time and then flare up again,\nfires can merge with other bushfires, and\nsolitary hotspots, which we classify as noise, may not represent true fires or are otherwise very brief ignitions that do not spread.\nThis algorithm is implemented in the R package spotoroo, available on CRAN. By enabling the user to cluster satellite hotspot data across space and time, this software provides the ability to relate findings to key factors in bushfire ignition (e.g. weather, proximity to roads and campsites, and fuel sources) and patterns in their spread.\nThe core functionality of this spatiotemporal clustering algorithm determines whether a hotspot represents a new ignition point or a continuation of an existing bushfire by comparing and combining cluster membership information via incremental updates from one time window to the next. Our algorithm first slices the hotspot data by its temporal dimension into fixed time intervals, according to a user-defined time step. Doing so divides the overall spatiotemporal clustering task into many smaller spatial clustering tasks that may be completed in parallel. Within each time window, which can be considered a static snapshot of hotspots observed across a user-specified number of time intervals, hotspots that fall within the threshold of a user-defined spatial metric of each other are joined into a cluster. Then, proceeding sequentially, we identify whether or not a hotspot was observed in the previous time window. If so, it retains its cluster membership from the previous time window; if not, the hotspot adopts the membership of the nearest hotspot with which it has been clustered. If no such neighbor exists, a hotspot represents the start of a new fire. It is important to note that each hotspot does not necessarily represent an individual fire, so those clusters that do not pass the threshold of a minimum number of hotspots or exist for a minimum amount of time are labelled noise.\nAs emphasized by Kisilevich et al. (2009), the selection of spatial resolution and time granularity – and relevance of domain knowledge in their choice – are imperative to the understanding and interpretation of resulting clusters. These choices can influence the shape and number of clusters discovered. In the case of satellite hotspot data, these parameters depend on the spatial resolution and temporal frequency at which images are captured. We suggest that parameter tuning can be assessed using a visual heuristic, which also enables selection of appropriate values in a unit-free manner, independent of a satellite’s spatial and temporal resolutions.\nThis paper is organized as follows. The next section provides an introduction to the literature on spatiotemporal clustering and applications to bushfire modeling. Section Algorithm details the steps of the clustering algorithm, and Section Package introduces its implementation in spotoroo on CRAN, including demonstration of the package’s key functions and visualization capabilities. We illustrate the clustering algorithm’s functionality to study bushfire ignition and spread in Victoria, Australia throughout the 2020 bushfire season in Application, and describe a visual heuristic to inform parameter selection. Finally, we give a brief conclusion of the paper and discuss potential opportunities for use of the clustering algorithm.\nBackground\nHan et al. (2012) overviews clustering methods and groups algorithms into five types: partitioning, hierarchical, density-based, grid-based, and model-based methods. Clustering of hotspot data lends itself nicely to hierarchical and density-based methods because they allow for the identification of clusters of various shapes and sizes without requiring that the user pre-specify the number of clusters. Particularly, density-based methods have the notion of noise, which is convenient for eliminating non-fire events from the clustering result. We therefore focus on a review of density-based methods and refer the reader to Han et al. (2012) for algorithms in other categories and Kisilevich et al. (2009) for appropriate extensions to spatiotemporal data.\nSpatiotemporal clustering based on DBSCAN\nDensity-based methods separate regions constituting a high density of points from low-density regions by identifying pairwise distances between points, and then requiring that a threshold for their grouping be satisfied (Han et al. 2012). Density Based Spatial Clustering of Applications with Noise (DBSCAN) (Ester et al. 1996) is an implementation of this methodology designed to address three challenges of clustering algorithms: (1) requirements of domain knowledge to determine the hyperparameters, (2) arbitrary shape of clusters and (3) computational efficiency. DBSCAN defines a maximum radius to construct a neighborhood around each point. It distinguishes between a core point, for which the number of points that fall in its neighborhood meets a minimum threshold, and a boundary point, whose neighborhood does not meet this threshold, but can be reached via overlapping neighborhoods extending from the core point. Intersecting neighborhoods are defined to be a cluster, while points that cannot be assigned to a cluster are identified as noise. DBSCAN also provides a heuristic to inform selection of threshold and cluster size.\nWhat is often identified as a limitation of DBSCAN – its inability to differentiate between clusters of different densities and those adjacent to each other (Birant and Kut 2007) – is of less concern for the application to satellite data, which by nature is a set of points corresponding to the equidistant center of pixels on grid of latitudes and longitudes. However, its application to spatiotemporal clustering problems, which contain at least three components – spatial location (e.g. latitude and longitude) and time – requires specification of temporal granularity and treatment of temporal similarity (Kisilevich et al. 2009). As such, several extensions to DBSCAN’s spatial clustering functionality have been proposed for spatiotemporal clustering solutions.\nST-DBSCAN (Birant and Kut 2007) was developed as an extension of DBSCAN’s functionality to cluster points according to their non-spatial, spatial, and temporal attributes, and it simultaneously addresses DBSCAN’s limitations regarding identification of clusters of varying densities and differentiation of adjacent clusters. ST-DBSCAN also introduces a second metric that considers similarity of variables associated with temporal neighbors.\nAnother spatiotemporal extension of DBSCAN called MC1 was developed by Kalnis et al. (2005). The authors slice the data into snapshots along the time dimension and define moving clusters using the percentage of common objects shared by clusters in two consecutive snapshots.\nThe clustering result is obtained by linking the outputs of DBSCAN applied on each snapshot with the definition of moving clusters. This algorithm enables tracking of moving clusters such as a convoy of cars moving in a city efficiently. However, the definition of moving clusters assumes that data points are individual objects with unique IDs that can move throughout time, which does not translate to hotspots.\nSpatiotemporal clustering with FSR\nSatellite hotspot data has been effectively clustered and visualized using DBSCAN, as demonstrated in studies by Nisa et al. (2014) and Hermawati and Sitanggang (2016). However, it should be noted that DBSCAN does not enable the tracking of fire ignition and movement over time.\nFSR (Loboda and Csiszar 2007) addresses this limitation with a focus on fire dynamics, which aim to characterize the ignition location, spatial progression, and rate of movement of individual fire events. It introduces what is effectively a hierarchical-based clustering algorithm to identify fire spread in the Russian boreal forest based on active fire detection from MODIS (Moderate Resolution Imaging Spectroradiometer), which has a temporal frequency of six hours. The algorithm proposed by the authors constructs a tree based on three rules: (1) the earliest observed hotspot is the root of the tree, (2) any node is within a 2.5km radius from its parent and (3) any node is observed no later than four days from its parent. When the tree is closed and there are still unassigned hotspots, the algorithm continues at the earliest unassigned hotspot to construct a new tree. Finally, each tree is a cluster, and the earliest observed hotspot(s) is defined as the ignition point (i.e. a cluster may have multiple ignition points).\nThe selection of parameters for FSR is tailored to the specific region and data product being utilized. As a result, these parameter settings cannot be immediately generalized or applied to other sources of satellite hotspot data.\nLimitations of existing methods for the purposes of bushfire monitoring\nWhen considered in the context of clustering hotspot data, the existing methods discussed in the previous section are not ideal. Here we clarify these limitations that have led to different choices in our algorithm, particularly in combining the clustering results of two consecutive time periods.\nST-DBSCAN’s consideration of the similarity of variables associated with consecutive time periods adds an unnecessary complexity for the spatiotemporal clustering of hotspots, for which we focus on three variables (latitude, longitude, time). It also does not guarantee that clusters are processed temporally, which is required to determine bushfire ignition sites. For this reason, we have developed a different handling of the temporal variable for our hotspot clustering.\nMC1 applies DBSCAN to each snapshot of data, which has the potential to treat some fire ignitions as noise. It is possible for a fire ignition to be initially identified by only a few hotspots, which may not be enough to meet DBSCAN’s density constraint. While we believe it is important to introduce the concept of noise to filter out non-fire events, their identification and removal should not occur at each temporal snapshot for this reason. We have therefore implemented the noise filter differently.\nWe also consider how best to handle the spatial convergence of clusters, that is, when two or more clusters from a previous time period merge together in the current time period. In such situations, both MC1 and FSR assign only one membership to the cluster of hotspots in the current time period. The consequence of such treatment is twofold: spatial coverage of one fire may increase dramatically in a short amount of time, which may not accurately reflect the natural speed of a bushfire’s spread, convergence or shifts in directions. This is not necessarily appropriate in light of our goal of identifying bushfire ignition locations. For this reason, and in contrast to MC1 and FSR, our algorithm inherently enables tracking of individual fires through their potential merging by keeping separate cluster ids in the current time period. This is because cluster membership is informed by a hotspot’s appearance in previous time periods. In the event that multiple hotspots are observed in a cluster’s first appearance, we calculate their centroid and record it as the ignition location.\nAlgorithm\nOur spatiotemporal clustering algorithm consists of four steps: (1) divide hotspots into equal time intervals, (2) cluster hotspots spatially across a series of time intervals, which we will refer to as a time window, (3) combine cluster information between consecutive time windows, and (4) identify hotspots that represent noise so they can be filtered out. These four steps are described in detail in this section.\n1. Divide hotspots into intervals\nBecause fires progress through time and our aim is to determine the ignition point of any particular bushfire, time is a critical component for determining a fire’s starting point and tracking its subsequent progression. To manage this analysis, it is convenient to partition time into intervals to support the spatial clustering of hotspots and eventual combination of results with those of future intervals.\nSelection of an appropriate time interval for clustering hotspot data represents a balance between maintaining the resolution of the original satellite data (which may record observations every five minutes or every hour or every four hours, for example) and computational efficiency of the algorithm. For example, a time interval of half an hour produces twice the number of iterations of the spatial clustering algorithm than a time interval of an hour; computation time is approximately linear with number of iterations. Himawari-8 hotspot data is recorded every 10 minutes; using this original temporal granularity has the potential to introduce significant noise, and would represent many more iterations of the clustering algorithm. For mitigation of this noise, we may choose an interval of 60 minutes = 1 hour. We then maintain an overall index of each hour across our entire data set, denoted as \\(t=1,...,T\\) where T is the total number of hours in the data set.\nA hotspot is a likely indication of a burning fire, although it is possible that a hotspot may disappear between two consecutive intervals, perhaps due to cloud cover or moisture that dampens the fire such that it becomes temporarily undetectable. The algorithm must take these considerations into account when combining information across consecutive time intervals. We need a parameter to measure how long a fire may burn undetected even if we don’t see a hotspot; that is, the amount of time a bushfire is considered to be active but unaccounted for in the hotspot data.\nFor this reason, the parameter \\(activeTime\\) is defined to reflect in intervals the maximum amount of time a fire may stay smoldering but undetectable by satellite before flaring up again. For example, if it is reasonable to assume that a bushfire not observed for 24 hours has burned out, the implication on \\(activeTime\\) is as follows: if time is divided into half hour intervals, the parameter \\(activeTime\\) will be 48, whereas with hour intervals, \\(activeTime\\) is 24. The \\(activeTime\\) parameter is a unitless integer.\nLet \\(\\boldsymbol{S}_t\\) be a series of time intervals defined by\n\\[\\boldsymbol{S}_t = [max(1,t-activeTime),t]\\quad t = 1,2,...,T,\\]\nwhere \\(max(.)\\) is the maximum function, \\(t\\) is an integer time index, and \\(T\\) is the integer length of the time window.\nSuppose the data set contains \\(T\\) hours of hotspot data, our time interval is one hour, and \\(activeTime = 24\\). This produces a sequence of time windows\n\\(\\boldsymbol{S}_1,\\boldsymbol{S}_2,\\ldots,\\boldsymbol{S}_{T}\\), where\n\\[\\begin{align*}\n\\boldsymbol{S}_1 &= [1,1],\\\\\n\\boldsymbol{S}_2 &= [1,2],\\\\\n&...\\\\\n\\boldsymbol{S}_{25} &= [1,25],\\\\\n\\boldsymbol{S}_{26} &= [2,26],\\\\\n&...\\\\\n\\boldsymbol{S}_{47} &= [23,47],\\\\\n\\boldsymbol{S}_{48} &= [24,48],\\\\\n&...\\\\\n\\boldsymbol{S}_{T} &= [T-24,T].\n\\end{align*}\\]\n2. Cluster hotspots spatially within each time window\nWe next cluster the hotspots within each time window. Each time window represents a static unified view of all hotspots observed across 25 consecutive time intervals (as of \\(\\boldsymbol{S}_{25}\\)). The clustering within a time window disregards the specific hourly time index associated with each observed hotspot to focus exclusively on the spatial relationship between them. A hierarchical-based method with a maximum distance stopping criterion \\(adjDist\\) is applied.\nThe parameter \\(adjDist\\) is used to represent the maximum distance between two adjacent hotspots, which are connected to form a graph. For example, the Himawari-8 hotspot data collection has 0.02\\(^o\\) resolution, which corresponds to approximately 2000m. Thus, \\(adjDist\\) is set to 3000m to indicate diagonal distance between observations. Every connected component of the graph is then considered as an individual cluster.\nGiven \\(adjDist>0~m\\) and a window \\(\\boldsymbol{S}_t\\), the algorithm performs the following substeps:\nAppend a randomly selected hotspot \\(h_i\\) to a empty list \\(\\boldsymbol{L}\\), where \\(h_i\\) is the \\(i\\)th hotspot in the time window \\(\\boldsymbol{S}_t\\). Let point \\(\\boldsymbol{P}\\) be the first item of the list \\(\\boldsymbol{L}\\).\nFor every \\(h_i \\notin \\boldsymbol{L}\\), if \\(geodesic(h_i, \\boldsymbol{P})\\leq adjDist\\), append \\(h_i\\) to the list \\(\\boldsymbol{L}\\).\nLet point \\(\\boldsymbol{P}\\) be the next item of the list \\(\\boldsymbol{L}\\).\nRepeat (b) and (c) until the point \\(\\boldsymbol{P}\\) reaches the end of the list \\(\\boldsymbol{L}\\).\nFor all hotspots \\(h_i \\in \\boldsymbol{L}\\), assign a new membership to them. Remove these hotspots from the time window \\(\\boldsymbol{S}_t\\). Repeat (a) to (e) until time window \\(\\boldsymbol{S}_t\\) is empty.\nRecover the time window \\(\\boldsymbol{S}_t\\) and record the memberships.\nFigure 1 provides an example of this step.\n\n\n\nFigure 1: Illustration showing Step 2 of the clustering algorithm on a sample of 20 hotspots in one time window \\(\\boldsymbol{S}_t\\). Initially (a), a hotspot is selected randomly (\\(\\boldsymbol{P}\\)) in order to seed a cluster. The circle indicates the maximum neighborhood distance (\\(adjDist\\)). Nearby hotspots as shown in red are clustered with \\(\\boldsymbol{P}\\) (b) to initialize list \\(\\boldsymbol{L}\\). The neighborhood is moved following every point in that collected list \\(\\boldsymbol{L}\\) and new observations are added (c), until there no more points that can be grouped (d). Then a new hotspot is selected external to the existing cluster, and the process is repeated (e). At the end, all the hotspots will be clustered (f).\n\n\n\n3. Update memberships for next time window\nStep 3 assigns membership to hotspots in the next time window (\\(t=2, ..., T\\) ) based on memberships in the previous window, as illustrated by Figure 2. The algorithm performs the following substeps:\nCollect the hotspots, \\(h_i\\), belonging to \\(\\boldsymbol{S}_{t}\\), where some may have also been present in \\(\\boldsymbol{S}_{t-1}\\), into the set \\(\\boldsymbol{H}_t = \\{h_1,h_2,...\\}\\). Hotspots that were present in \\(\\boldsymbol{S}_{t-1}\\) are labelled, and new hotspots are unlabeled.\nConstruct the connectivity graph, based on \\(adjDist\\), measuring proximity between all hotspots in \\(\\boldsymbol{H}_t\\). There may be some hotspots from multiple \\(\\boldsymbol{S}_{t-1}\\) clusters connected together here, due to new hotspots appearing between previous hotspots. This is resolved at the next step.\nFor all unlabeled hotspots, use the label of the closest labelled point, if it is connected in the graph. Any unlabeled hotspots not connected to labelled hotspots are assigned a new label (cluster 5 in Figure 2). Also, any connected graph where memberships from multiple \\(\\boldsymbol{S}_{t-1}\\) clusters occur are effectively split into the same multiple clusters (clusters 2 and 4 in Figure 2). This would correspond to two existing fires burning into each other, but they keep their original label. These fires might die at this time, or they might progress in different directions.\nThis consideration of cluster memberships in consecutive time windows enables us to capture the long-term behavior of individual fires, namely whether they go undetected for 24 hours or more. Intuitively, a fire is undetected for 24 hours if a cluster is represented in \\(\\boldsymbol{S}_{t-1}\\) but not \\(\\boldsymbol{S}_{t}\\); that is, all hotspots in the cluster as of time window \\(\\boldsymbol{S}_{t-1}\\) were from time index \\(t-25\\). By moving through the temporal sequence of time windows this way, we gradually identify the ignition time and location of new fires by considering hotspot activity in a region over the last 24 hours.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Illustration of clustering Step 3, which involves combining results from one time window to the next. There are 33 hotspots at \\(\\boldsymbol{S}_t\\), where 20 (green) of them have been previously clustered at \\(\\boldsymbol{S}_{t-1}\\) (Figure 1 f) and 13 (orange) of them are new hotspots. The connected graph show the clustering in this time window. Hotspots previously clustered at \\(\\boldsymbol{S}_{t-1}\\) keep their cluster labels. The 13 new hotspots are assigned labels of the nearest hotspot’s cluster label. This might mean that a big cluster \\(\\boldsymbol{S}_t\\) (indicated by the graph) would be split back into two, if it corresponded to two clusters at \\(\\boldsymbol{S}_{t-1}\\) (e.g. clusters 2, 4). New clusters of hotspots are assigned a new label (e.g. cluster 5).\n\n\n\n4. Handle noise in the clustering result\nAfter performing step 3, all hotspots will have been assigned a membership label. However, there may exist small clusters, where small refers to number of associated hotspots and/or length of time for which a cluster is observed. These small clusters are less important for bushfire monitoring due to their limited spread, and we may be less certain that they represent true fires. To address this issue, we provide a noise filter as the last step.\nParameter \\(minPts\\) specifies the minimum number of hotspots a cluster can contain and parameter \\(minTime\\) specifies the minimum amount of time for which a cluster can exist and still be considered a bushfire. Any cluster that does not satisfy these two conditions will be reassigned membership label \\(-1\\) to indicate they represent noise.\nResult\nThe result of the spatiotemporal clustering algorithm applied to the hotspot data is a vector of memberships with length equal to the number of observations in the data.\nPackage\nOur spatiotemporal clustering algorithm detailed in the previous section is implemented in the R package spotoroo, which is available for download on CRAN:\ninstall.packages(\"spotoroo\")\nThe development version can be installed from Github:\ndevtools::install_github(\"TengMCing/spotoroo\")\nThe following demonstration of the package’s functionality assumes spotoroo and dplyr have been loaded. We also want to load the built-in data set hotspots.\nlibrary(spotoroo)\nlibrary(dplyr)\ndata(hotspots)\nThe hotspots data is a subset of the original Himawari-8 wildfire product and contains 1070 observations observed in the state of Victoria in Australia from December 2019 through February 2020.\nTo create this illustrative data set, records from the original October 2019-March 2020 data set were filtered to include only those hotspots in the boundary of Victoria’s borders with an irradiance over 100 watts per square meter. This threshold of the hotspot’s intensity is proposed by landscape ecologist and spatial scientist Dr. Grant Williamson (2020) to reduce the likelihood of including hotspots that do not represent true fires in our analysis. These two processing steps capture 75,936 observations, for which we preserve fields longitude, latitude, and observed date and time; we sample approximately 1% of these records for the package’s final hotspots data set.\nPerforming spatiotemporal cluster analysis\nThe main function of this package is hotspot_cluster(), which implements the spatiotemporal clustering algorithm on satellite hotspot data input by the user.\nThis function accepts a data set of hotspots (hotspots) which must contain fields corresponding to longitude (lon), latitude (lat), and observed time (obsTime). Arguments activeTime, adjDist, minPts, and minTime were previously defined in the Algorithm section, and represent parameters for the algorithm’s functionality. Arguments timeUnit and timestep represent the conversion from a hotspot’s observed time to its corresponding integer time index.\nWe illustrate the use of function hotspot_cluster() below. Our hotspots data set has columns with names that correspond to the first four arguments. Parameters timeUnit and timeStep define the difference between two adjacent time indices as \\(1\\) hour, and activeTime is \\(24\\) time indexes, adjDist is \\(3000\\) meters, minPts is \\(4\\) hotspots, and minTime is \\(3\\) time indices. That is, we consider any cluster that lasts longer than \\(3\\) hours and contains at least \\(4\\) hotspots to be a bushfire, although since the definition of a bushfire may vary according to geography, these parameters may be adjusted accordingly by the user.\n\n\n\nresult <- hotspot_cluster(hotspots = hotspots,\n                          lon = \"lon\",\n                          lat = \"lat\",\n                          obsTime = \"obsTime\",\n                          activeTime = 24,\n                          adjDist = 3000,\n                          minPts = 4,\n                          minTime = 3,\n                          timeUnit = \"h\",\n                          timeStep = 1)\nThis function returns a spotoroo object, which is a three-object list containing two data.frames – hotspots and ignition – and a list called setting. Printing this output tells us that of the 1070 original hotspots, 10 were identified as noise points, and the remaining 1060 were clustered into 6 clusters.\n\n\nresult\n\nℹ spotoroo object: 6 clusters | 1070 hot spots (including noise points)\n\nWithin result, the first item is a hotspots data frame. This contains the three fields from our original input data (namely, lon, lat, and obsTime), as well as each hotspot’s assigned time index (timeID) and cluster membership. Each hotspot is also associated with the calculated distance from and time since its associated fire’s ignition, distToIgnition and timeFromIgnition, respectively. Any hotspot with a membership of \\(-1\\) is identified as noise (noise == TRUE).\nOur original hotspot data was recorded at 10-minute intervals, and we defined each time interval to be one hour. A hotspot’s timeID is assigned starting from the first observed timestamp in the data. For example, 2019-12-29 13:10:00 is the earliest obsTime in hotspots and begins the hour for which timeID == 1; any hotspot observed from 2019-12-29 14:10:00 to 2019-12-29 15:00 is assigned timeID == 2, and so on. This enumeration continues through the last hour for which we observe hotspots in the data.\n\n\nresult$hotspots %>% arrange(obsTime) %>% glimpse()\n\nRows: 1,070\nColumns: 10\n$ lon                  <dbl> 149.30, 149.30, 149.32, 149.30, 149.30,…\n$ lat                  <dbl> -37.75999, -37.78000, -37.78000, -37.75…\n$ obsTime              <dttm> 2019-12-29 13:10:00, 2019-12-29 13:10:…\n$ timeID               <int> 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 4, 4, 5, …\n$ membership           <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ noise                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ distToIgnition       <dbl> 1111.885, 1111.885, 2080.914, 1111.885,…\n$ distToIgnitionUnit   <chr> \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\",…\n$ timeFromIgnition     <drtn> 0.0000000 hours, 0.0000000 hours, 0.33…\n$ timeFromIgnitionUnit <chr> \"h\", \"h\", \"h\", \"h\", \"h\", \"h\", \"h\", \"h\",…\n\nThe second item within result is the ignition data set, which contains one row for each cluster identified by the algorithm. Each cluster’s row captures information regarding its ignition location, observed time of ignition, number of hotspots in the cluster, and for how long the cluster was observed. The coordinates of each ignition point represent the earliest observed hotspot associated with the cluster, or the calculated centroid if there are multiple hotspots observed in the first time index associated with the cluster.\n\n\nglimpse(result$ignition)\n\nRows: 6\nColumns: 8\n$ membership         <int> 1, 2, 3, 4, 5, 6\n$ lon                <dbl> 149.3000, 146.7200, 149.0200, 149.1600, 1…\n$ lat                <dbl> -37.77000, -36.84000, -37.42000, -37.2900…\n$ obsTime            <dttm> 2019-12-29 13:10:00, 2020-01-08 01:40:00,…\n$ timeID             <int> 1, 229, 258, 280, 327, 859\n$ obsInCluster       <dbl> 146, 165, 126, 256, 111, 256\n$ clusterTimeLen     <drtn> 116.1667 hours, 148.3333 hours, 146.3333…\n$ clusterTimeLenUnit <chr> \"h\", \"h\", \"h\", \"h\", \"h\", \"h\"\n\nThe final object contained in result is a list that records the user-input values for the algorithm’s parameters.\n\n\nresult$setting\n\n$activeTime\n[1] 24\n\n$adjDist\n[1] 3000\n\n$minPts\n[1] 4\n\n$ignitionCenter\n[1] \"mean\"\n\n$timeUnit\n[1] \"h\"\n\n$timeStep\n[1] 1\n\nThe user can run the function summary() to return key distributions that summarize the clustering results. For clusters, it captures the distribution of the number of hotspots in a cluster and the duration of the cluster (in hours). Similarly, for hotspots, it captures the distribution of their distance to ignition, both spatially (in meters) and temporally (in hours). It also notes the number of noise observations identified by the clustering algorithm.\nExtracting a subset of clusters\nSome users may prefer to engage with a data frame of the results, rather than parsing the spotoroo list object described above. For this purpose, the package provides a function extract_fire(), which converts a spotoroo object to a data.frame by collapsing the hotspots and ignition objects. A new field, type, records whether a row in this data frame represents a hotspot, ignition point, or noise point. Cluster information is available for each hotspot that was not identified to be noise. Users can include or disregard noise points by toggling the argument noise = TRUE.\n\n\nall_fires <- extract_fire(result, noise = TRUE)\nall_fires %>% arrange(obsTime) %>% glimpse()\n\nRows: 1,076\nColumns: 14\n$ lon                  <dbl> 149.30, 149.30, 149.30, 149.32, 149.30,…\n$ lat                  <dbl> -37.75999, -37.78000, -37.77000, -37.78…\n$ obsTime              <dttm> 2019-12-29 13:10:00, 2019-12-29 13:10:…\n$ timeID               <int> 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 4, 4, …\n$ membership           <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ noise                <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ distToIgnition       <dbl> 1111.885, 1111.885, 0.000, 2080.914, 11…\n$ distToIgnitionUnit   <chr> \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\",…\n$ timeFromIgnition     <drtn> 0.0000000 hours, 0.0000000 hours, 0.00…\n$ timeFromIgnitionUnit <chr> \"h\", \"h\", \"h\", \"h\", \"h\", \"h\", \"h\", \"h\",…\n$ type                 <chr> \"hotspot\", \"hotspot\", \"ignition\", \"hots…\n$ obsInCluster         <dbl> 146, 146, 146, 146, 146, 146, 146, 146,…\n$ clusterTimeLen       <drtn> 116.1667 hours, 116.1667 hours, 116.16…\n$ clusterTimeLenUnit   <chr> \"h\", \"h\", \"h\", \"h\", \"h\", \"h\", \"h\", \"h\",…\n\nBy providing a vector of indices to the argument cluster, the function will extract the corresponding clusters from the clustering result.\n\n\nfire_1_and_2 <- extract_fire(result, cluster = c(1, 2), noise = FALSE)\n\n\nVisualizing the clustering result\nThe package provides three basic methods to visualize the clustering results, all of which can be produced by the function plot(). Users can draw advanced graphics using the results provided by the algorithm.\nDefault plot for visualizing the spatial distribution of clusters\nThe default plot allows for visualization of the spatial distribution of the fires. As shown in Figure 3, it presents the hotspots of each cluster identified by the algorithm with a single color. The black dot on top of each colored set of points is the ignition location of that fire.\nThis graphical representation only provides a static view of the hotspot data. To get a dynamic view, users can draw subplots grouped by time.\n\n\nplot(result, bg = plot_vic_map())\n\n\n\nFigure 3: This is the default plot for visualizing the spatial distribution of clusters. In the results shown there are six clusters, which correspond to six fires, shown using different colors. The black dots indicate the ignition site for each fire.\n\n\n\nFire movement plot for visualizing the fire dynamics\nFigure 4 shows the path of each fire’s movement, which is produced by setting the plot argument type = 'mov'. The fire movement is computed by the get_fire_mov() function. Its argument step controls the number of time intervals combined for calculation of the hotspots’ centroid for the purpose of this visualization. Using a small value of step will produce a complex path; for example, step = 1 means that the centroid of hotspots in each hour will be calculated to trace the fire’s path, while step = 12 combines intervals over the course of half a day.\nNote that centroids won’t adequately summarize unusual cluster shapes, and it may be necessary to change the visualization to show a full shape with perhaps a convex hull. This would make for a complicated visualization of temporal movement, though. Also if a fire spreads simultaneously in different directions, the centroids would remain in approximately the same locations.\n\n\nplot(result, type = \"mov\", step = 12)\n\n\n\nFigure 4: This is the fire movement plot for visualizing the fire dynamics. Here there are six clusters, corresponding to six different fires. The path between the ignition point and the end point is drawn with black line, where the triangle is the ignition point and the circle is the end point. (Note that the aspect ratio of the plot reflects the relative spatial ratio of latitude and longitude.)\n\n\n\nIn this plot, hotspots are slightly jittered from their original recorded locations on the spatial grid so repeat observations at a specific latitude and longitude are visible. The triangle identifies the fire’s ignition point and the circle is the fire’s final location. This provides us with a view into the distribution of hotspots over the course of the fire, as well as the bushfire’s overall movement. For example, fire 1 ended southeast of its original ignition location.\nTimeline plot for providing an overview of the bushfire season\nFigure 5 shows a timeline of fires produced by setting plot argument type = 'timeline', which enables study of the intensity of the bushfire season. Hotspots’ observed times are plotted along the horizontal line corresponding to their assigned clusters. The green curve along the top captures the temporal density of all fires throughout the season. Noise points are represented with orange dots. The plots demonstrates that the majority of fires in our sample data set burned in mid-January.\n\n\nplot(result, type = \"timeline\")\n\n\n\nFigure 5: This is the timeline plot for providing an overview of the bushfire season. The x-axis is the date and the y-axis is the cluster membership. The observed time of hotspots are shown as dot plots (green). The density plot at the top display the temporal frequency of fire occurrence over the timeframe. The dot plot at the bottom (orange) shows the observed time of hotspots that are considered to be noise.\n\n\n\nApplication\nWe illustrate the use of the algorithm by applying it to the entirety of the 75,936 hotspots in Victoria. The spatial distribution of these hotspots for October 2019-March 2020 is shown in Figure 6. This example takes around 5 minutes to run on a 2020 Apple MacBook Pro.\n\n\n\nClustering the Victorian hotspot data\nTo perform the clustering algorithm on the Victorian hotspot data, we first convert the observed time of each hotspot to its corresponding time index by setting the time difference between two successive indices to be \\(1\\) hour via parameters timeStep and timeUnit. As initially discussed in Algorithm, because the original Himawari-8 data is recorded at 10-minute intervals, reassigning each hotspot to its corresponding hourly time index does not cause great loss of information, but serves to significantly shorten the computation time. We then let activeTime be \\(24\\) time indices and adjDist be \\(3000\\) meters; see Selecting parameter values for further discussion of and guidance regarding selection of these choices. Finally, minPts is \\(3\\) hotspots and minTime is \\(3\\) time indices.\nresult <- hotspot_cluster(hotspots = vic_hotspots,\n                          lon = \"lon\",\n                          lat = \"lat\",\n                          obsTime = \"obsTime\",\n                          activeTime = 24,\n                          adjDist = 3000,\n                          minPts = 4,\n                          minTime = 3,\n                          timeUnit = \"h\",\n                          timeStep = 1)\nThe clustering result shows that 407 bushfires are identified among the 75,936 hotspots. This result is reasonable, as historically the number of bushfires reported in Victoria in a year ranges from about \\(200\\) to \\(700\\) (Department of Environment, Land, Water & Planning 2019).\n\n\nresult\n\nℹ spotoroo object: 407 clusters | 75936 hot spots (including noise points)\n\nDetermining the ignition point and time for individual fires\nThe clustering result provides the ignition location of the bushfire represented by each cluster, and the plot() function now enables us to examine the spatial distribution of bushfire ignitions over the course of the entire summer. The result is given in Figure 6. From this plot, we observe that the large majority of fires burned in eastern Victoria, with a smattering in the south west. These areas are primarily forests and mountains. Very few fires started in or around city of Melbourne, which is located along the southern coast in the middle of the state.\nNote that because there are now too many fires to color each of them uniquely (as done previously in Figure 3, the default plot function colors each cluster of fires black with their respective ignition points in red. This change in plot output occurs once the number of clusters is greater than nine.\nplot(result, bg = plot_vic_map(), hotspot = TRUE)\nThe ignition time of individual fires and representation of how long they burned is produced with the following code, the output of which is Figure 7. This plot shows that the majority of fires were ignited from late December 2019 to early January 2020. We observe a significant number of hotspots identified to be noise points in mid-December, which may imply there are some undetected fire events that were short-lived.\nplot(result, type = \"timeline\", mainBreak = \"1 month\", dateLabel = \"%b %d, %y\")`. \n\n\n\nFigure 6:  The distribution of hotspots (black) and bushfire ignitions (red) in Victoria during 2019-2020 Australian bushfire season. The spatial distribution of the ignition locations suggest that most of the fires were observed in the east of Victoria.\n\n\n\n\n\n\nFigure 7: Timeline of fires observed in Victoria during the 2019-2020 Australian bushfire season. Clustered hotspots are shown as dotplots (green). The density display of the timeline shows that most fires started in late December and early January. Noise is shown at the bottom (orange), with the dashed lines indicating the density. This plot shows there is a significant number of hotspots that could be considered to be noise, especially in mid-December. It might also suggest that there are lots of short-lived and spatially constrained fires.\n\n\n\nTracking fire movement\nWe further study the movement of the four most intensive fires, as identified by the number of hotspots clustered together. These fires burned in the eastern part of Victoria, and the output of the following code is shown in Figure 8. These fires burned during the time period from December 18th through January 4th. According to the plot, 3 out of the 4 fires moved over the course of their burning, while the center of fire 163 does not appear to move much. This is because simultaneous spread in different directions kept the centroid of the cluster in a similar location over time.\n\n\nplot(result, \n     type = \"mov\", \n     cluster = order(result$ignition$obsInCluster,\n                     decreasing = TRUE)[1:4], \n     step = 12, \n     bg = plot_vic_map())\n\n\n\nFigure 8: Examining the dynamics of the four most intensive fires in Victoria during the 2019-2020 Australian bushfire season. All of the fires covered similar spatial areas over their lifetimes, but the trajectory was quite different. Fire 163 may have spread in many directions simultaneously over the time period, as indicated by the near constant location of the centroid.\n\n\n\nSelecting parameter values\nWe previously introduced two key parameters, \\(adjDist\\) and \\(activeTime\\), in our explanation of the algorithm. While their optimal choice is unknown, a visualization tool enables their tuning such that the user can select reasonable values.\nConsider the relationships between \\(adjDist\\), \\(activeTime\\), and the percentage of hotspots observed to be noise points. Increase of either \\(adjDist\\) or \\(activeTime\\) represents greater spatial and temporal tolerance, respectively, either of which usually reduces the percentage of noise points. However, if the noise points are spatially and temporally far from the meaningful clusters, increasing either of these two parameters may not significantly reduce the number of clusters. Therefore, to identify what are likely real noise points, we find the smallest values for each of \\(adjDist\\) and \\(activeTime\\) after which the percentage of noise points no longer decreases significantly. Based on this logic, we develop a visualization tool inspired by the scree plot (Cattell 1966) used in the principal component analysis and the sorted k-dist plot used in DBSCAN (Ester et al. 1996). Similar to these two plots, users need to determine the values of \\(activeTime\\) and \\(adjDist\\) for which the greatest decrease in the percentage of noise points is captured. Figure 9 shows the parameter tuning process using this visualization tool. To produce these plots, a grid of parameter values must be evaluated, which can be computationally intensive. For this reason, we do not include this tool in the spotoroo, but leave it for the user to build. The final choice of \\(activeTime\\) is \\(24\\) hours and \\(adjDist\\) is \\(3000\\) meters.\n\n\n\nFigure 9: Parameter tuning plots, where the best choice of parameter is at a large drop in the percentage of noise points. Here, these are at \\(AdjDist = 3000\\) and \\(activeTime = 24\\).\n\n\n\nConclusions\nIn this paper, we have proposed a spatiotemporal clustering algorithm to organize satellite hotspot data for the purpose of determining points of bushfire ignition and tracking their movement remotely. This algorithm can be used to cluster hotspots into bushfires of arbitrary shape and size, while following some basic characteristics of bushfire dynamics. It is necessary to tune the parameters of the algorithm. This can be done visually, as demonstrated in the previous section’s example application, or determined by expert knowledge of fire dynamics for a geography of interest.\nWe have provided an implementation of the algorithm in the R package spotoroo, which supports three types of visualizations for presenting different aspects of the bushfire data: the spatial distribution of fires, the movement of individual bushfires, and the timeline of fire ignitions and length of burning. In addition, we have illustrated the use of this package in studying the 2019-2020 Australian bushfire season. The main benefit of this work is that information otherwise infeasible to collect about fires in remote areas can be extracted from satellite hotspot data and processed with the use of this package.\nWe have previously discussed the ways in which existing algorithms for spatial clustering that incorporate a temporal component are not suitable for our intended purpose. Furthermore, there are no R packages on CRAN that provide implementations of ST-DBSCAN, FSR or MC1 in order to run a direct comparison of computational efficiency or clustering performance. There is an implementation of DBSCAN in R package dbscan that runs extremely fast (< 10 seconds) on hotspot data from our example application with the proper choice of parameters. However, it clusters over 30% of the hotspots into one fire, which is not very helpful for tracking fire ignition and spread.\nThere are possible modifications that could be made to our algorithm for the purpose of improving quality of the clustering result or reduced computation time. We might consider alternative clustering techniques – for example, grid-based clustering – at each time window. It may also be possible to use an adaptable \\(adjDist\\) at different time windows. We leave these extensions for future research.\nMore complex analyses can be conducted when clustering results are merged with data such as recreation sites, fire stations, roads, weather and vegetation types. An analysis of this type to predict the causes of bushfires in Victoria during the 2019-2020 bushfire season is described in Cook (2020), which yields meaningful suggestions for future fire prevention. We hope others can build upon this work to make use of satellite hotspot data for bushfire research, especially in the context of future bushfire planning and prevention.\nAcknowledgements\nThis work was made possible through funding by the Australian Centre of Excellence for Mathematical & Statistical Frontiers, which supported Emily’s travel.\nThis paper was written in rmarkdown with knitr and rjtools. All analyses were performed using R version 4.2.2 (R Core Team 2022). Data were processed with readr and dplyr. Plots were produced with ggplot2, ggforce and patchwork. The code and data to reproduce this paper are available on GitHub.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-022.zip\nCRAN packages used\nspotoroo, dplyr, dbscan, rmarkdown, knitr, rjtools, readr, ggplot2, ggforce, patchwork\nCRAN Task Views implied by cited packages\nCluster, Databases, ModelDeployment, Phylogenetics, ReproducibleResearch, Spatial, TeachingStatistics, WebTechnologies\n\n\nN. J. Abram, B. J. Henley, A. Sen Gupta, T. J. R. Lippmann, H. Clarke, A. J. Dowdy, J. J. Sharples, R. H. Nolan, T. Zhang, M. J. Wooster, et al. Connections of Climate Change and Variability to Large and Extreme Forest Fires in Southeast Australia. Communications Earth & Environment, 2(1): 8, 2021. URL https://doi.org/10.1038/s43247-020-00065-8.\n\n\nAustralian Government Bureau of Meteorology. Annual Climate Statement 2020. 2021. URL http://www.bom.gov.au/climate/current/annual/aus/.\n\n\nD. Birant and A. Kut. ST-DBSCAN: An Algorithm for Clustering Spatial-Temporal Data. Data & Knowledge Engineering, 60(1): 208–221, 2007. URL https://doi.org/10.1016/j.datak.2006.01.013.\n\n\nR. B. Cattell. The Scree Test for the Number of Factors. Multivariate Behavioral Research, 1(2): 245–276, 1966.\n\n\nD. Cook. Open Data Shows Lightning, Not Arson, Was the Likely Cause of Most Victorian Bushfires Last Summer. 2020. URL https://theconversation.com/open-data-shows-lightning-not-arson-was-the-likely-cause-of-most-victorian-bushfires-last-summer-151912.\n\n\nCSIRO and Australian Government Bureau of Meteorology. State of the Climate 2020. 2020. URL http://www.bom.gov.au/state-of-the-climate/documents/State-of-the-Climate-2020.pdf.\n\n\nP. Deb, H. Moradkhani, P. Abbaszadeh, A. S. Kiem, J. Engstrom, D. Keellings and A. Sharma. Causes of the Widespread 2019-2020 Australian Bushfire Season. Earth’s Future, 8(11): e2020EF001671, 2020. URL https://doi.org/10.1029/2020EF001671.\n\n\nDepartment of Environment, Land, Water & Planning. Fire Origins - Current and Historical. 2019. URL https://discover.data.vic.gov.au/dataset/fire-origins-current-and-historical.\n\n\nM. Ester, H.-P. Kriegel, J. Sander and X. Xu. A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, pages. 226–231 1996. Portland, Oregon: AAAI Press. URL https://dl.acm.org/doi/10.5555/3001460.3001507.\n\n\nA. I. Filkov, T. Ngo, S. Matthews, S. Telfer and T. D. Penman. Impact of Australia’s Catastrophic 2019/20 Bushfire Season on Communities and Environment. Retrospective Analysis and Current Trends. Journal of Safety Science and Resilience, 1(1): 44–56, 2020. URL https://doi.org/10.1016/j.jnlssr.2020.06.009.\n\n\nL. Giglio, W. Schroeder and C. O. Justice. The Collection 6 MODIS Active Fire Detection Algorithm and Fire Products. Remote Sensing of Environment, 178: 31–41, 2016. URL https://doi.org/10.1016/j.rse.2016.02.054.\n\n\nJ. Han, M. Kamber and J. Pei. Data Mining: Concepts and Techniques, 3rd ed. Morgan Kaufman, 2012. URL https://www.sciencedirect.com/book/9780123814791/data-mining-concepts-and-techniques.\n\n\nR. Hermawati and I. S. Sitanggang. Web-Based Clustering Application Using Shiny Framework and DBSCAN Algorithm for Hotspots Data in Peatland in Sumatra. Procedia Environmental Sciences, 33: 317–323, 2016. URL https://doi.org/10.1016/j.proenv.2016.03.082.\n\n\nE. Jang, Y. Kang, J. Im, D.-W. Lee, J. Yoon and S.-K. Kim. Detection and Monitoring of Forest Fires Using Himawari-8 Geostationary Satellite Data in South Korea. Remote Sensing, 11(3): 2019. URL https://doi.org/10.3390/rs11030271.\n\n\nP. Kalnis, N. Mamoulis and S. Bakiras. On Discovering Moving Clusters in Spatio-Temporal Data. In Advances in Spatial and Temporal Databases: 9th International Symposium, SSTD 2005, Angra dos Reis, Brazil, August 22-24, 2005. Proceedings 9, pages. 364–381 2005. Springer. URL https://link.springer.com/chapter/10.1007/11535331_21.\n\n\nS. Kisilevich, F. Mansmann, M. Nanni and S. Rinzivillo. Spatio-Temporal Clustering. In Data Mining and Knowledge Discovery Handbook, pages. 855–874 2009. Springer. URL https://link.springer.com/chapter/10.1007/978-0-387-09823-4_44.\n\n\nY. Kurihara, K. Tanada, H. Murakami and M. Kachi. Australian Bushfire Captured by AHI/Himawari-8 and SGLI/GCOM-C. In Proceedings of the JpGU-AGU Joint Meeting, 2020. URL https://www.eorc.jaxa.jp/ptree/documents/Poster_H8Wfire_JpGU2020.pdf.\n\n\nT. V. Loboda and I. A. Csiszar. Reconstruction of Fire Spread Within Wildland Fire Events in Northern Eurasia From the MODIS Active Fire Product. Global and Planetary Change, 56(3): 258–273, 2007. URL https://doi.org/10.1016/j.gloplacha.2006.07.015.\n\n\nK. K. Nisa, H. A. Andrianto and R. Mardhiyyah. Hotspot Clustering Using DBSCAN Algorithm and Shiny Web Framework. In 2014 International Conference on Advanced Computer Science and Information System, pages. 129–132 2014. URL https://doi.org/10.1109/ICACSIS.2014.7065840.\n\n\nP-Tree System. JAXA Himawari Monitor - User’s Guide. 2020. URL https://www.eorc.jaxa.jp/ptree/userguide.html.\n\n\nR Core Team. R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing, 2022. URL https://www.R-project.org/.\n\n\nC. H. Wickramasinghe, S. Jones, K. Reinke and L. Wallace. Development of a Multi-Spatial Resolution Approach to the Surveillance of Active Fire Lines Using Himawari-8. Remote Sensing, 8(11): 2016. URL https://doi.org/10.3390/rs8110932.\n\n\nG. Williamson. Example Code to Generate Animation Frames of Himawari-8 Hotspots. 2020. URL https://gist.github.com/ozjimbob/80254988922140fec4c06e3a43d069a6.\n\n\nG. Xu and X. Zhong. Real-Time Wildfire Detection and Tracking in Australia Using Geostationary Satellite: Himawari-8. Remote Sensing Letters, 8(11): 1052–1061, 2017. URL https://doi.org/10.1080/2150704X.2017.1350303.\n\n\n\n\n",
    "preview": "articles/RJ-2023-022/RJ-2023-022_files/figure-html5/step2figs-1.png",
    "last_modified": "2023-11-07T21:31:41+00:00",
    "input_file": {},
    "preview_width": 5760,
    "preview_height": 1440
  },
  {
    "path": "articles/RJ-2023-023/",
    "title": "asteRisk - Integration and Analysis of Satellite Positional Data in R",
    "description": "Over the past few years, the amount of artificial satellites orbiting Earth has grown fast, with close to a thousand new launches per year. Reliable calculation of the evolution of the satellites' position over time is required in order to efficiently plan the launch and operation of such satellites, as well as to avoid collisions that could lead to considerable losses and generation of harmful space debris. Here, we present asteRisk, the first R package for analysis of the trajectory of satellites. The package provides native implementations of different methods to calculate the orbit of satellites, as well as tools for importing standard file formats typically used to store satellite position data and to convert satellite coordinates between different frames of reference. Such functionalities provide the foundation for integrating orbital data and astrodynamics analysis in R.",
    "author": [
      {
        "name": "Rafael Ayala",
        "url": {}
      },
      {
        "name": "Daniel Ayala",
        "url": {}
      },
      {
        "name": "Lara Sellés Vidal",
        "url": {}
      },
      {
        "name": "David Ruiz",
        "url": {}
      }
    ],
    "date": "2023-08-26",
    "categories": [],
    "contents": "\n1 Introduction\nSince the launch of Sputnik 1 in 1957, the developments in space technology have enabled a fast growth of the amount of satellites placed in orbit around Earth. Such growth has become especially prominent in the last decade, as evidenced by the fact that over a third of the 11,697 satellite launches performed until September 2021 occurred over the last 5 years (United Nations). The number of organizations behind satellite launches has also considerably expanded, and currently includes not only government and military agencies, but also private entities. This is largely due to technological developments, such as miniaturized satellites, that have made the process of placing a satellite on orbit around Earth increasingly accessible (Toorian et al. 2008; Bouwmeester and Guo 2010; Kirillin et al. 2015).\nPlanning the launch, operation and orbital maneuvers of satellites requires predictions of the trajectory that a satellite will follow along time from a known state vector (a set of parameters that define the position, velocity and acceleration of the satellite at a given instant). Such methods are also useful for studying the simultaneous evolution of multiple satellites, with the aim to predict and avoid collisions. This is of special interest in the light of the increasing density of Earth-orbiting satellites, since uncontrolled collisions could lead to the onset of Kessler syndrome (Kessler and Cour-Palais 1978; Kessler et al. 2010), a positive-feedback scenario where the generated space debris impacts on other objects, producing further cascading collisions. This would eventually hinder space operations in some orbits for very long periods of time. For these reasons, multiple models have been developed to calculate the evolution (propagate) of the position of a satellite over time from a set of initial, known conditions.\nWe introduce asteRisk, an R package that aims to provide a suite for astrodynamics analysis in R. To that extent, implementations of some of the most frequently applied orbital propagators are provided, including the SGP4 and SDP4 models (Dong and Chang-yin 2010), as well as a high-precision numerical orbital propagator. Additionally, utilities for reading file formats commonly used to distribute satellite positions (ephemeris) are provided. Satellite ephemerides are relatively scarce compared to other types of positional data (such as that available for standard aviation) (Schäfer et al. 2014), due to the much higher technical complexity of the equipment required to obtain experimental observations that allow the calculation of the position of satellites, as well as to the sensitivity level of such information. As a consequence, only limited sources of data are available for most satellites, such as CelesTrak (Kelso) and Space-Track (SAIC), with the notable exception of Global Navigation Satellite Systems (GNSS) and Planet Labs nanosatellites (Foster et al. 2015). In spite of the scarcity of the data, the provided orbital propagators can extend its scope by calculating future and past ephemeris from a given known state vector. The package also provides tools for the calculation of orbital parameters from the coordinates and velocity of a satellite and vice versa, as well as for the conversion of coordinates between different frames of reference. Some of the functionalities provided with asteRisk require large data tables. These are provided in the accessory data package asteRiskData, which is distributed through a drat repository and can be installed by running .\nIn the following sections, we describe the features of the package and provide examples of application to real data. Firstly, the supported file formats are described, together with examples of data sources, in Section File formats and data sources. Next, we introduce the different orbital propagators currently implemented in Section Orbital propagators. Finally, the different available frames of reference are presented, and conversion between them is demonstrated in Section Frames of reference.\n2 File formats and data sources\nAs previously mentioned, positional information for satellites is relatively scarce compared to non-space aircraft. CelesTrak and Space-Track distribute data for nearly all well-known, non-classified, Earth-orbiting satellites in TLE format. Additionally, NASA provides information about GNSS satellites through the Crustal Dynamics Data Information System (CDDIS) (Noll 2010) in RINEX format. asteRisk provides functionalities for reading-in both types of files, which consist of plain text files structured according to the definitions provided by the organizations that implement each standard.\nTLE files\nThe TLE (Two/Three Line Element) format was originally implemented by the North American Aerospace Deffense Command (NORAD), and has become the standard format for distributing positional information about Earth-orbiting satellites.\nThe format consists of an initial optional line with up to 24 characters (title line) with the name of the satellite corresponding to the TLE file, followed by two 69-character lines that include the information required to calculate orbital elements (a set of parameters that describe the orbit of an object at a given time point), as well as some additional metadata about the satellite (Kelso,). It should be noted that a TLE file can contain multiple TLE data structures, which are concatenated in the same file with no additional separator.\nTLE files can be read with the function, which receives the path to a file containing one or more TLEs. Both TLEs with and without the additional title line are supported, but all the TLEs in the file should be of the same type. Alternatively, the can be used to obtain the same information from a character vector where each element is a string representing a TLE (including the new-line characters).\nIt should be noted that TLE files were designed to be used in conjunction with the SGP4 or SDP4 orbital propagators (described in detail in Section The SGP4 model). This is because the values provided in TLEs are not osculating/Keplerian orbital elements of a satellite, but instead mean elements calculated to best fit multiple observations by the entity distributing the file. Therefore, as a general rule it is not recommended to use the information obtained from a TLE file as input to orbital propagators other than SGP4/SDP4. However, it is often the case that TLEs are the single available source of information for a given satellite. In such circumstances, if the application of other propagators is desired, it is advised to apply the SGP4/SDP4 model to propagate the orbit to the same instant corresponding to the values of orbital elements distributed in the TLE. This will generate position and velocity values in Cartesian coordinates that are better suited to be used as input for other algorithms, although unpredictable errors are still likely to be present (Janson et al. 2020).\nIn the following example, we read a file containing TLEs retrieved from CelesTrak for the International Space Station (ISS) and a Molniya satellite (a satellite model for military and communications purposes launched by the Soviet Union and later by the Russian Federation from 1965 to 2004):\n\n\n## In this example, we read a test TLE file containing 2 TLEs, one for the\n## Zarya module of the International Space Station and another one for a\n## Molniya satellite\n\ntest_TLEs <- readTLE(\"./data/testTLEs.tle\")\n\n## We can now check the mean orbital elements contained in each TLE. For \n## example, we can calculate the orbital period from the mean motion, provided \n## in revolutions/day\n\nmeanMotion_ISS <- test_TLEs[[1]]$meanMotion\n1/meanMotion_ISS * 24 * 60\n\n## An orbital period of around 93 minutes is obtained for the ISS, in accordance\n## with expectations\n\n## Let us check some characteristic parameters of the Molniya satellite\n\nmeanMotion_Molniya <- test_TLEs[[2]]$meanMotion\n1/meanMotion_Molniya * 24 * 60\ntest_TLEs[[2]]$eccentricity\n\n## The Molniya satellite has a period of around 715 minutes and an eccentricity\n## of 0.74, in accordance with the elliptical Molniya orbits in which such\n## satellites were placed\n\n\nRINEX navigation files\nRINEX (Receiver INdependent EXchange) (Gurtner and Estey 2007) is a standard defining multiple formats to distribute satellite navigation systems data, including GNSS. The standard defines three types of files for navigation, observation and meteorological data. Among these, navigation data files provide positional information about satellites.\nRINEX defines multiple navigation file formats for different constellations of satellites: GPS, GLONASS, Galileo, BeiDou, IRNSS/NavIC and other regional satellite-based augmentation systems (SBAS). The current version of asteRisk supports GPS and GLONASS navigation files, which can be read respectively with functions and . Both functions receive as an argument the path to a RINEX navigation file, and return a list with the values for the elements contained in the file. In the case of GLONASS navigation files, these include directly Cartesian coordinates values for the position, velocity and acceleration of the satellite in the ITRF system of coordinates. On the other hand, GPS navigation files provide values for the osculating orbital elements of the satellite in the GCRF system of coordinates, which can be converted to Cartesian coordinates. Functions and automatically perform corrections of satellite time to obtain the corresponding accurate UTC times following the procedures described in the specifications of both GNSS (Flores; Space Device Engineering). In the case of GPS RINEX navigation files, conversion of orbital elements to Cartesian coordinates in the ITRF frame is also performed. Additionally, due to the small scale of some of the involved clock corrections (below microseconds), the corrected ephemeris times in UTC are returned as objects of class from the nanotime package.\nIn the following example, a RINEX navigation file for a GPS satellite obtained from the CDDIS is read. A TLE file from CelesTrak for the same satellite at the same instant is also read, and the obtained orbital elements are compared:\n\n\n## Let us read a GPS RINEX navigation file containing a single message\n\nGPS_RINEX <- readGPSNavigationRINEX(\"./data/RINEX_GPS_1.rxn\")\n\n## The resulting list comprises 2 elements: \"header\" (which is common for all\n## navigation messages present in the file) and \"messages\" (which is a list of\n## lists, with one element per message in the top-level list and each of these\n## containing elements for the different pieces of information provided in the\n## navigation messages). Since there is only one message in the read file,\n## \"messages\" is a list of length 1. We can retrieve orbital parameters from it.\n## Note that the values for angle quantities are in radians, which we convert \n## to degrees here. The mean motion is in radians per second, which we convert\n## to revolutions per day\n\nlength(GPS_RINEX$messages) # 1\nGPS_RINEX$messages[[1]]$correctedMeanMotion * 86400/(2*pi) # 2.005735\nGPS_RINEX$messages[[1]]$eccentricity # 0.002080154\nGPS_RINEX$messages[[1]]$inclination * 180/pi # 55.58636\nGPS_RINEX$messages[[1]]$meanAnomaly * 180/pi # -37.86007\nGPS_RINEX$messages[[1]]$perigeeArgument * 180/pi # 175.6259\nGPS_RINEX$messages[[1]]$ascension * 180/pi # -22.95967\n\n## Let us now read a TLE for the same satellite at approximately the same time\n\nGPS_TLE <- readTLE(\"./data/TLE_GPS.tle\")\n\n## We can verify that both the TLE and the RINEX file correspond to the same\n## satellite by comparing the PRN codes, which is in both cases 18. A PRN code\n## is an identifier unique to each satellite of the GPS constellation.\n\nGPS_RINEX$messages[[1]]$satellitePRNCode\nGPS_TLE$objectName\n\n## We can now check the mean orbital elements provided the TLE file\n\nGPS_TLE$meanMotion # 2.005681\nGPS_TLE$eccentricity # 0.0020743\nGPS_TLE$inclination # 55.5757\nGPS_TLE$meanAnomaly # -46.0714\nGPS_TLE$perigeeArgument # 181.977\nGPS_TLE$ascension # 37.1706\n\n## As we can see, mean motion, eccentricity, inclination and argument of perigee\n## are very similar between the two files. The value for mean anomaly, a \n## measurement of where the satellite is along its orbital path, are also\n## similar if we convert both of them to the [0, 2*pi) range. However, the \n## values for the longitude of the ascending node differ significantly. This\n## is due to the fact that the orbital elements provided in the TLE file are\n## defined in the TEME frame of reference, while the values in the RINEX file\n## are defined in the ITRF frame of reference.\n\n\n3 Orbital propagators\nAn orbital propagator is a mathematical model for calculating the position of a satellite at future or past time points given a known state vector at a certain time. Multiple propagators have been developed, which differ in the underlying forces being considered and the extent to which assumptions are made to simplify the model. The different propagators therefore offer varying degrees of accuracy and computational costs, with the two often being inversely related.\nThree main types of propagators exist: numerical, analytical and semi-analytical. Numerical propagators implement a formulation of a set of forces that act on the satellite, which leads to an expression for the acceleration of the satellite at a given instant (Flores et al. 2021). Numerical integrators are then used to solve the Ordinary Differential Equation (ODE) that defines acceleration as the second-order time derivative of position. While numerical propagators offer the highest accuracy provided that the relevant forces acting on the satellite are correctly modeled, they also have the highest computational costs (especially if implicit integration methods are applied).\nConversely, analytical propagators rely on a set of assumptions and simplifications to obtain closed-form solutions that can be directly evaluated at any given instant as a function of time, the initial known state vector and a set of model parameters. These offer the advantage of having much lower computational costs than numerical propagators, but at the expense of reduced accuracy, particularly for long-term predictions (Deprit 1981; Meeus 1991; Lara et al. 2014).\nSemi-analytical methods are an intermediate approach, whereby short-period motions are modeled analytically, and long-term secular effects are solved with numerical integrators (Morand et al. 2013; Lara et al. 2016; Lévy et al. 2021).\nIn its current version (1.2.0), asteRisk provides implementation of two of the most widely applied analytical propagators, the SGP4 and SDP4 models, as well as a numerical high-precision orbital propagator (HPOP).\nThe SGP4 model\nThe SGP4 (Simplified General Perturbation 4) model was developed by Ken Cranford in 1970 (Lane and Hoots 1979), based on previous theoretical developments by Lane and Cranford (Lane 1965) whereby the Earth gravitational field is modeled with spherical harmonics up to degree 4 and atmospheric drag is modeled as being spherically symmetric, static, and with density following a power-law decay as altitude increases. The model is mainly designed to be applied to near-Earth satellites with an orbital period of 225 minutes or less, corresponding to an altitude of 5877.5 km if the orbit is assumed to be circular.\nThe native R implementation available in asteRisk is based on the C++ implementation by Vallado (Vallado), which includes minor corrections over the original FORTRAN implementation (Hoots et al. 1988). The validity of the implementation has been verified by comparing the results obtained with previous implementations for a set of test cases (Appendix: Test cases for SGP4/SDP4). The propagator can be applied through the function. The function receives as input the following mean orbital elements of the satellite, typically obtained from TLE files: mean motion (in radians/min), mean eccentricity (dimensionless; ranging from 0, a perfectly circular orbit, to 1, a parabolic trajectory), mean orbital inclination (in radians), mean anomaly (in radians), mean argument of the perigee (in radians) and mean longitude of the ascending node (in radians).\n\n\nTable 1: Description of\nthe arguments taken as input by the SGP4 propagator.\n\n\nArgument of sgp4 function\n\n\nOrbital parameter\n\n\nUnits\n\n\nDescription\n\n\nn0\n\n\nMean motion\n\n\n\\(radians/min\\)\n\n\nAngular speed of the satellite\n\n\ne0\n\n\nEccentricity\n\n\nDimensionless\n\n\nValue between 0 and 1 measuring\nhow much the orbit deviates from a circular shape, with 0 indicating a\nperfectly circular orbit and 1 an extreme case of parabolic trajectory\n\n\ni0\n\n\nInclination\n\n\n\\(radians\\)\n\n\nAngle between the orbital plane of the satellite and the equatorial plane\n\n\nM0\n\n\nMean anomaly\n\n\n\\(radians\\)\n\n\nAngle between the direction of the perigee and the hypothetical point where\nthe object would be if it was moving in a circular orbit with the same period\nas its true orbit after the same amount of time since it last crossed the\nperigee had ellapsed. Therefore, 0 denotes that the object is at the perigee\n\n\nomega0\n\n\nArgument of perigee\n\n\n\\(radians\\)\n\n\nAngle between the direction of the ascending node and the direction of the\nperigee\n\n\nOMEGA0\n\n\nLongitude of the ascending node\n\n\n\\(radians\\)\n\n\nAngle between the direction of the ascending node (the point where\nthe satellite crosses the equatorial plane moving north) and the direction of\nthe First Point of Aries (which indicates the location of the vernal equinox)\n\n\nBstar\n\n\n–\n\n\n\\(Earth\\) \\(radii^{-1}\\)\n\n\nDrag coefficient of the satellite, which indicates how susceptible it is to\natmospheric drag\n\n\ninitialDateTime\n\n\n–\n\n\nUTC date-time string\n\n\nTime corresponding to the provided state vector of the\nsatellite\n\n\ntargetTime\n\n\n–\n\n\nminutes or UTC date-time string\n\n\nTime at which propagation should be performed, as a date-time\nstring or in minutes from the initial time\n\n\nkeplerAccuracy\n\n\n–\n\n\nDimensionless\n\n\nAccuracy to consider Kepler’s\nequation solved. If two consecutive solutions differ by a value lower than this\naccuracy, integration is considered to have converged\n\n\nmaxKeplerIterations\n\n\n–\n\n\nDimensionless\n\n\nMaximum number of\niterations after which fixed-point integration of Kepler’s equation will stop\n\n\nThe function also receives as an argument \\(B^*\\) in units of inverse Earth radii, a modified ballistic coefficient for the satellite which indicates how susceptible it is to atmospheric drag. The target times at which the position of the satellite should be calculated can be specified either as absolute date-time strings in UTC time, or as minutes from the time corresponding to the known state vector. The function outputs the position and velocity of the satellite at the target times in Cartesian coordinates in the TEME (True Equator, Mean Equinox) frame of reference (Section TEME).\nIn the following example, we demonstrate the application of the SGP4 model to propagate the orbit of the ISS from the previously read TLE:\n\n\n## We can use the mean orbital elements of the TLE of the ISS to propagate its\n## position. It should be kept in mind that the mean motion must be input in \n## radians per minute, and the mean inclination, anomaly, argument of perigee\n## and longitude of the ascending node must be provided in radians.\n## Let us propagate the orbit of the ISS for 465 minutes, equivalent to 5\n## orbital periods\n\nISS_TLE <- test_TLEs[[1]]\n\ntarget_times_ISS <- seq(0, 465, by=5)\n\nresults_position_matrix_ISS <- matrix(nrow=length(target_times_ISS), ncol=3)\nresults_velocity_matrix_ISS <- matrix(nrow=length(target_times_ISS), ncol=3)\n\nfor(i in 1:length(target_times_ISS)) {\n    new_result <- sgp4(n0=revDay2radMin(ISS_TLE$meanMotion),\n                       e0=ISS_TLE$eccentricity,\n                       i0=deg2rad(ISS_TLE$inclination),\n                       M0=deg2rad(ISS_TLE$meanAnomaly),\n                       omega0=deg2rad(ISS_TLE$perigeeArgument),\n                       OMEGA0=deg2rad(ISS_TLE$ascension),\n                       Bstar=ISS_TLE$Bstar,\n                       initialDateTime=ISS_TLE$dateTime,\n                       targetTime = target_times_ISS[i])\n    results_position_matrix_ISS[i,] <- new_result[[1]]\n    results_velocity_matrix_ISS[i,] <- new_result[[2]]\n}\n\nresults_position_matrix_ISS = cbind(results_position_matrix_ISS, target_times_ISS)\ncolnames(results_position_matrix_ISS) <- c(\"x\", \"y\", \"z\", \"time\")\n\n## We can now visualize the resulting trajectory using a plotly animation\n## In order to create the animation, we must first define a function to create\n## the accumulated dataframe required for the animation, which indicates the \n## trajectory up to each frame. Frames are defined by propagation time\n\naccumulate_by <- function(dat, var) {\n    var <- f_eval(var, dat)\n    lvls <- plotly:::getLevels(var)\n    dats <- lapply(seq_along(lvls), function(x) {\n        cbind(dat[var %in% lvls[seq(1, x)], ], frame = lvls[[x]])\n    })\n    bind_rows(dats)\n}\n\naccumulated_df_ISS <- accumulate_by(as.data.frame(results_position_matrix_ISS), ~time)\n\n## We can then create a plotly animation\n\norbit_animation_ISS <- plot_ly(accumulated_df_ISS, x = ~x, y=~y, z=~z, type = \"scatter3d\",\n                           mode=\"lines+marker\", opacity=0.8, line=list(width = 6, \n                                                                 color = ~time, \n                                                                 reverscale = FALSE), \n                           frame= ~frame, showlegend=FALSE)\n\norbit_animation_ISS <- layout(orbit_animation_ISS, scene = list(\n    xaxis=list(range=c(-7000, 7000)),\n    yaxis=list(range=c(-7000, 7000)),\n    zaxis=list(range=c(-7000, 7000))))\n\n## We can also create an animation of a static spheric mesh to represent the\n## Earth, and add it to the orbit animation\n## First we generate Cartesian coordinates for a sphere of radius equal to\n## the radius of Earth. Coordinates are generated along meridians and parallels\n## that can be plotted as lines\n\nsphere_theta <- seq(0, 2*pi, length.out=20)\nsphere_phi <- seq(0, pi, length.out=20)\nsphere_radius <- 6371\nsphere_x <- sphere_y <- sphere_z <- numeric(0)\n\nfor(theta in sphere_theta) {\n    for(phi in sphere_phi) {\n        sphere_x <- c(sphere_x, sphere_radius * cos(theta) * sin(phi))\n        sphere_y <- c(sphere_y, sphere_radius * sin(theta) * sin(phi))\n        sphere_z <- c(sphere_z, sphere_radius * cos(phi))\n    }\n    sphere_x <- c(sphere_x, NULL)\n    sphere_y <- c(sphere_y, NULL)\n    sphere_z <- c(sphere_z, NULL)\n}\n\nfor(phi in sphere_phi) {\n    for(theta in sphere_theta) {\n        sphere_x <- c(sphere_x, sphere_radius * cos(theta) * sin(phi))\n        sphere_y <- c(sphere_y, sphere_radius * sin(theta) * sin(phi))\n        sphere_z <- c(sphere_z, sphere_radius * cos(phi))\n    }\n    sphere_x <- c(sphere_x, NULL)\n    sphere_y <- c(sphere_y, NULL)\n    sphere_z <- c(sphere_z, NULL)\n}\n\n## Then, we generate an extended dataframe with repetitions of the coordinates\n## for a number of times equal to the number of frames in the orbit animation.\n## We include a frame column to specify the frame corresponding to each sphere,\n## matching the frame numbers\n\nsphere_df <- data.frame(x = sphere_x, y = sphere_y, z = sphere_z)\nsphere_df_ext_ISS <- sphere_df[rep(seq_len(nrow(sphere_df)), \n                                   length(target_times_ISS)), ]\nsphere_df_ext_ISS <- cbind(sphere_df_ext_ISS, \n                           rep(target_times_ISS, each=nrow(sphere_df)))\ncolnames(sphere_df_ext_ISS) <- c(\"x\", \"y\", \"z\", \"frame\")\n\n## We can then use the extended dataframe to create an animation of a static\n## sphere\n\nsphere_animated_ISS <- plot_ly(sphere_df_ext_ISS, x=~x, y=~y, z=~z, frame=~frame, \n                           type=\"scatter3d\", mode=\"lines\", \n                           line=list(color='rgb(0,0,255)'), hoverinfo=\"skip\",\n                           showlegend=FALSE)\nsphere_animated_ISS <- layout(sphere_animated_ISS, scene = list(\n    xaxis = list(showspikes=FALSE), \n    yaxis = list(showspikes=FALSE),\n    zaxis = list(showspikes=FALSE)))\n\n## The two animations can then be combined and used to visualize the orbit,\n## which as we can see is relatively close to the surface of Earth. This is in\n## accordance with the ISS being on a LEO (Low Earth Orbit)\n\ncombined_animation_ISS <- suppressWarnings(subplot(orbit_animation_ISS, \n                                                   sphere_animated_ISS))\ncombined_animation_ISS <- animation_opts(combined_animation_ISS, frame=50)\ncombined_animation_ISS <- layout(combined_animation_ISS, scene = list(\n    aspectmode = \"cube\"))\ncombined_animation_ISS\n\n\n\n\n\n\n\n\n\nFigure 1: Trajectory of the International Space Station (ISS) calculated with the SGP4 propagator. The ISS is in a low Earth orbit, around 400 km above Earth’s surface.\n\n\n\nThe SDP4 model\nThe SDP4 (Simplified Deep-space Perturbation 4) model (Hujsak 1979) is an extension of the SGP4 model designed to be applied to deep-space satellites, which are considered to be those with an orbital period over 225 minutes. The model introduces corrections to account for the gravitational effects exerted by the Sun and the Moon, as well as Earth gravitational resonance effects for orbits with periods of 12 h or 24 h. However, it employs a simplified atmospheric drag model, and therefore SGP4 should still be the preferred choice for near-Earth satellites.\nThe SDP4 implementation provided with asteRisk is also based on the revised C++ implementation by Vallado (Vallado). It can be accessed through the function , which takes the same arguments as input and returns an output in the same format as function . Alternatively, the function can be used to automatically determine and apply the most appropriate model based on the orbital period of the satellite (SGP4 for orbital periods below 225 minutes, and SDP4 for larger ones). The validity of our implementation has also been verified with previously published test cases (Appendix: Test cases for SGP4/SDP4).\nIn the following example, we demonstrate the application of the SDP4 model to propagate the orbit of the Molniya satellite, which follows a highly elliptical orbit with a period of approximately 12 h:\n\n\n## Let us now propagate the position of a Molniya satellite, which should follow\n## a highly elliptical orbit and go to distances much farther from Earth than \n## the ISS. We will propagate the orbit for 715 minutes, one orbital period. In\n## this case, we use the SDP4 propagator\n\nmolniya_TLE <- test_TLEs[[2]]\n\ntarget_times_Molniya <- seq(0, 715, by=5)\n\nresults_position_matrix_Molniya <- matrix(nrow=length(target_times_Molniya), ncol=3)\nresults_velocity_matrix_Molniya <- matrix(nrow=length(target_times_Molniya), ncol=3)\n\nfor(i in 1:length(target_times_Molniya)) {\n    new_result <- sdp4(n0=revDay2radMin(molniya_TLE$meanMotion),\n                       e0=molniya_TLE$eccentricity,\n                       i0=deg2rad(molniya_TLE$inclination),\n                       M0=deg2rad(molniya_TLE$meanAnomaly),\n                       omega0=deg2rad(molniya_TLE$perigeeArgument),\n                       OMEGA0=deg2rad(molniya_TLE$ascension),\n                       Bstar=molniya_TLE$Bstar,\n                       initialDateTime=molniya_TLE$dateTime,\n                       targetTime = target_times_Molniya[i])\n    results_position_matrix_Molniya[i,] <- new_result[[1]]\n    results_velocity_matrix_Molniya[i,] <- new_result[[2]]\n}\n\nresults_position_matrix_Molniya = cbind(results_position_matrix_Molniya, \n                                        target_times_Molniya)\ncolnames(results_position_matrix_Molniya) <- c(\"x\", \"y\", \"z\", \"time\")\n\n## We can follow a similar procedure as for the ISS to generate an animation\n## of the trajectory of the Molniya satellite\n\naccumulated_df_Molniya <- accumulate_by(\n    as.data.frame(results_position_matrix_Molniya), ~time)\n\n## We can then create a plotly animation\n\norbit_animation_Molniya <- plot_ly(accumulated_df_Molniya, x = ~x, y=~y, z=~z, \n                                   type = \"scatter3d\", mode=\"lines+marker\", \n                                   opacity=0.8, line=list(width = 6, \n                                                          color = ~time,\n                                                          reverscale = FALSE), \n                                   frame= ~frame, showlegend=FALSE)\n\nsphere_df_ext_Molniya <- sphere_df[rep(seq_len(nrow(sphere_df)),\n                                       length(target_times_Molniya)), ]\nsphere_df_ext_Molniya <- cbind(sphere_df_ext_Molniya,\n                               rep(target_times_Molniya, each=nrow(sphere_df)))\ncolnames(sphere_df_ext_Molniya) <- c(\"x\", \"y\", \"z\", \"frame\")\n\nsphere_animated_Molniya <- plot_ly(sphere_df_ext_Molniya, x=~x, y=~y, z=~z,\n                                   frame=~frame, type=\"scatter3d\", mode=\"lines\",\n                                   line=list(color='rgb(0,0,255)'),\n                                   hoverinfo=\"skip\", showlegend=FALSE)\nsphere_animated_Molniya <- layout(sphere_animated_Molniya,\n                                  scene = list(xaxis = list(showspikes=FALSE), \n                                               yaxis = list(showspikes=FALSE),\n                                               zaxis = list(showspikes=FALSE)))\n\ncombined_animation_Molniya <- suppressWarnings(subplot(sphere_animated_Molniya,\n                                                       orbit_animation_Molniya))\ncombined_animation_Molniya <- animation_opts(combined_animation_Molniya, frame=15)\ncombined_animation_Molniya <- layout(combined_animation_Molniya, scene = list(\n    aspectmode = \"manual\",\n    aspectratio = list(x=1, y=(7000+24500)/(15000+21000), \n                       z=(41000 + 7000)/(15000+21000)),\n    xaxis=list(range=c(-21000, 15000)),\n    yaxis=list(range=c(-24500, 7000)),\n    zaxis=list(range=c(-7000, 41000))))\n\n## We can now verify that the satellite follows a highly elliptical orbit. It can\n## also be seen that, as expected, the satellite moves faster at the perigee\n## (when it is closest to Earth) and slower at the apogee (when it is the\n## farthest from Earth)\n\ncombined_animation_Molniya\n\n\n\n\n\n\n\n\n\nFigure 2: Trajectory of a Molniya satellite calculated with the SDP4 propagator. The satellite follows a highly elliptical orbit.\n\n\n\nNumerical high-precision orbital propagator\nasteRisk also provides a numerical orbital propagator. The implemented high-precision orbital propagator relies on the following fundamental motion equation:\n\\[\n\\begin{aligned}\n    \\mathbf{a} = \\frac{\\partial^2 \\mathbf{p}}{\\partial t^2}\n\\end{aligned}\n\\]\nWhere \\(\\mathbf{a}\\) denotes the acceleration, \\(\\mathbf{p}\\) the position, and \\(t\\) time. It can be simplified to a set of 6 first-order differential equations, where \\(p_x, p_y, p_z\\), \\(v_x, v_y, v_z\\) and \\(a_x, a_y, a_z\\) denote, respectively, orthogonal components of the position, velocity and acceleration:\n\\[\n\\begin{aligned}\nv_x = \\frac{\\partial p_x}{\\partial t}, v_y = \\frac{\\partial p_y}{\\partial t}, v_z = \\frac{\\partial p_z}{\\partial t} \\\\\na_x = \\frac{\\partial v_x}{\\partial t}, a_y = \\frac{\\partial v_y}{\\partial t}, a_z = \\frac{\\partial v_z}{\\partial t}\n\\end{aligned}\n\\]\nIn the current implementation, acceleration is calculated by considering the following forces known to play a significant role in orbital mechanics (Montenbruck and Gill 2012), comparable to the force field found in other HPOP implementations (Maisonobe et al. 2010; AGI; Mahooti):\nEarth gravitational attraction. The gravity field generated by Earth is calculated according to the GGM03S model (Tapley et al. 2007), using zonal, tesseral and sectoral spherical harmonics up to a degree and order of 180. The effects of ocean and solid Earth tides on the gravity field are also taken into account.\nThird-body gravitational attractions. The gravitational attractions exerted by the Sun, the Moon, the other 7 main planets of the Solar System (Mercury, Venus, Mars, Jupiter, Saturn, Uranus and Neptune) and Pluto are considered. All of them are modelled as point masses, and their positions are calculated using Jet Propulsion Laboratory Development Ephemeris 436 (Folkner et al. 2014).\nSolar radiation pressure. This is the force generated by the impact of photons from solar radiation on the satellite. A double conical shadow model to account for the reduction in the apparent solar disk seen by a satellite due to the Earth and the Moon (both of which are assumed to be spherical) eclipses is implemented (Li et al. 2019).\nAtmospheric drag. The drag effect caused by the atmosphere of Earth is calculated using the NRLMSISE-00 (Picone et al. 2002) atmospheric model.\nAdditionally, a correction to the acceleration generated by the described forces to account for relativistic effects caused by the mass of the Earth is included (Montenbruck and Gill 2012). In the current implementation, only the Schwarzschild effect (Roh et al. 2016; Sośnica et al. 2021) is considered.\nThe HPOP can be accessed through the function, which receives as inputs the initial position and velocity of the satellite, the time to which these correspond, a set of target times at which the position and velocity of the satellite should be calculated, the mass of the satellite, the effective area of the surface of the satellite subjected to drag and radiation pressure, and drag and reflectivity coefficients, that describe respectively how much the satellite is affected by drag and radiation pressure forces (Knocke et al. 1988).\nThe system of differential equations is solved with the deSolve package. By default, uses the RADAU5 integrator, an implicit Runge-Kutta method of order 5 with adaptive step size (Wanner and Hairer 1996). However, additional arguments accepted by the function of the deSolve package can be directly provided to to modify the values of the parameters for the integrator or to specify a different integrator that will be used to solve the motion equations.\nIn the following example, we apply the HPOP to the GPS satellite for which we previously read a RINEX navigation file, and compare the results with the real final position obtained by reading another RINEX file:\n\n\n## We will use the ephemeris broadcasted in the previously read RINEX file\n## for GPS satellite with PRN code 18 to obtain initial conditions for\n## propagation. It should be noted that the HPOP requires Earth orientation\n## parameters and other space data, which are provided through the companion\n## asteRiskData package. Therefore, we must first install it if it is not\n## already available:\n\nif (!requireNamespace(\"asteRiskData\", quietly = TRUE)) {\n    install.packages('asteRiskData', repos='https://rafael-ayala.github.io/drat/')\n}\n\n## The initial position and velocity obtained from the RINEX file are in the\n## ITRF frame, and must first be converted to the GCRF frame (more details \n## about each frame are given in the following section). In order to perform\n## such corrections, the latest Earth Orientation Parameters (which contain\n## information about the precise location of the Earth rotation axis for every\n## day) are required. These can be retrieved by running getLatestSpaceData(),\n## after which they will be automatically used by all the functions that require\n## them\n\nGPS_RINEX_initialMessage <- GPS_RINEX$messages[[1]]\n\ngetLatestSpaceData()\ninitialEphemerisDateTime <- format(GPS_RINEX_initialMessage$ephemerisUTCTime, \n                                   \"%Y-%m-%d %H:%M:%EXS\")\ninitialStateGCRF <- ITRFtoGCRF(GPS_RINEX_initialMessage$position_ITRF, \n                               GPS_RINEX_initialMessage$velocity_ITRF,\n                               initialEphemerisDateTime)\n\n## Additionally, we also require some parameters describing the physical\n## properties of the satellite. By November 2021, GPS satellite with PRN code\n## 18 is USA-293, launched in August 2019 and belonging to GPS block III.\n## Such satellites have an on-orbit mass of around 2600 kg. As an approximation,\n## we can model GPS block III satellites as a central body with dimensions of\n## 2.2 m x 1.8 m x 4.2 m with solar arrays of a total surface of 28.2 m2, as \n## described by Steigenberger et al., 2020.  The central body therefore has\n## faces with areas of 9.24 m2, 7.56 m2 and 3.96 m2. Without considering a more\n## complex attitude model, the mean cross-sectional area of the satellite can\n## be calculated as the sum of all these 4 areas divided by 2. We will use this\n## value as the effective area for drag (which will anyway be negligible at the\n## altitude of a GPS orbit). For the effective area for radiation pressure, we\n## will use the mean cross-sectional area of the central body plus the full\n## area of the solar arrays, since GPS block III satellites are equipped with\n## systems to maintain them oriented towards the Sun. Finally, we will use the\n## commonly employed values of 2.2 and 1.2 for drag and radiation coefficients.\n## We will propagate the orbit for 12 hours at every minute.\n\npropagationTimes <- seq(0, 43200, by=60)\n\n## It should be noted that the following can take a few minutes to run\n\nhpop_results_GPS <- hpop(position=initialStateGCRF$position,\n                         velocity=initialStateGCRF$velocity,\n                         dateTime=initialEphemerisDateTime,\n                         times=propagationTimes,\n                         satelliteMass=2600,\n                         dragArea=(9.24 + 7.56 + 3.96 + 28.2)/2,\n                         radiationArea=(9.24 + 7.56 + 3.96)/2 + 28.2,\n                         dragCoefficient=2.2,\n                         radiationCoefficient=1.2)\n\n## We can now read another RINEX navigation file for the same satellite with\n## the position 12 hours later, and compare it with the calculated one\n\nGPS_RINEX_2 <- readGPSNavigationRINEX(\"./data/RINEX_GPS_2.rxn\")\n\nGPS_RINEX_endMessage <- GPS_RINEX_2$messages[[1]]\nendEphemerisDateTime <- format(GPS_RINEX_endMessage$ephemerisUTCTime, \n                               \"%Y-%m-%d %H:%M:%EXS\")\nendStateGCRF <- ITRFtoGCRF(GPS_RINEX_endMessage$position_ITRF, \n                           GPS_RINEX_endMessage$velocity_ITRF,\n                           endEphemerisDateTime)\n\ndistance <- sqrt(\n    sum(\n        (endStateGCRF$position - hpop_results_GPS[nrow(hpop_results_GPS), 2:4])^2))\nprint(distance) # 19.7806\n\n## The calculated position is less than 20 m away from the real position, thanks\n## To the high precision of the propagator. We can also visualize the trajectory\n## as previously with a plotly animation. To keep consistency, we first convert\n## the distance units to km and propagation times to minutes\n\npropagated_positions_GPS <- cbind(hpop_results_GPS[, 2:4]/1000,\n                                  hpop_results_GPS[, 1]/60)\ncolnames(propagated_positions_GPS) <- c(\"x\", \"y\", \"z\", \"time\")\n\n## We will sample the trajectory once every 10 minutes\n\npropagated_positions_GPS <- propagated_positions_GPS[\n    seq(1, nrow(propagated_positions_GPS), by=10), ]\naccumulated_df_GPS <- accumulate_by(as.data.frame(propagated_positions_GPS), ~time)\n\norbit_animation_GPS <- plot_ly(accumulated_df_GPS, x = ~x, y=~y, z=~z,\n                               type = \"scatter3d\", mode=\"lines+marker\", \n                               opacity=0.8, line=list(width = 6,\n                                                      color = ~time,\n                                                      reverscale = FALSE),\n                               frame= ~frame, showlegend=FALSE)\n\norbit_animation_GPS <- layout(orbit_animation_GPS, scene = list(\n    xaxis=list(range=c(-27500, 27500)),\n    yaxis=list(range=c(-27500, 27500)),\n    zaxis=list(range=c(-27500, 27500))))\n\nsphere_df_ext_GPS <- sphere_df[rep(seq_len(nrow(sphere_df)), \n                                   length(propagated_positions_GPS[, \"time\"])), ]\nsphere_df_ext_GPS <- cbind(sphere_df_ext_GPS, \n                           rep(propagated_positions_GPS[, \"time\"],\n                               each=nrow(sphere_df)))\ncolnames(sphere_df_ext_GPS) <- c(\"x\", \"y\", \"z\", \"frame\")\n\nsphere_animated_GPS <- plot_ly(sphere_df_ext_GPS, x=~x, y=~y, z=~z, frame=~frame, \n                               type=\"scatter3d\", mode=\"lines\", \n                               line=list(color='rgb(0,0,255)'), hoverinfo=\"skip\",\n                               showlegend=FALSE)\nsphere_animated_GPS <- layout(sphere_animated_GPS,\n                              scene = list(xaxis = list(showspikes=FALSE), \n                                           yaxis = list(showspikes=FALSE),\n                                           zaxis = list(showspikes=FALSE)))\n\ncombined_animation_GPS <- suppressWarnings(subplot(sphere_animated_GPS,\n                                                   orbit_animation_GPS))\ncombined_animation_GPS <- animation_opts(combined_animation_GPS, frame=15)\ncombined_animation_GPS <- layout(combined_animation_GPS, scene = list(\n    aspectmode = \"cube\"))\ncombined_animation_GPS\n\n\n\n\n\n\n\n\n\nFigure 3: Trajectory of a GPS satellite calculated with the high-precision numerical propagator. It follows an almost circular orbit at an altitude of approximately 20200 km, which allows it to cover a larger portion of Earth at any given time.\n\n\n\n4 Frames of reference\nThere are multiple frames of reference available to describe the position of objects in space. These differ in the location of the origin of coordinates, the presence or absence of acceleration for the reference frame itself and the orientation of the planes defining the main axes. Inertial frames are those that do not experience any acceleration, while non-inertial frames exhibit some sort of acceleration, such as rotation. The different available frames offer advantages and disadvantages for different applications. For example, a non-inertial frame that rotates with Earth is useful when analyzing the ground-track projected by a satellite onto the surface of Earth along its orbit, while inertial frames make the description of the forces acting on a satellite simpler.\nasteRisk provides functions to convert between the frames of reference most relevant for the determination of Earth-orbiting satellites: ITRF (International Terrestrial Reference Frame), GCRF (Geocentric Celestial Reference Frame), TEME (True Equator Mean Equinox) and geodetic coordinates (latitude, longitude and altitude). Additionally, functions to obtain the orbital parameters of a satellite with known position and velocity in these frames are provided.\nITRF\nThe International Terrestrial Reference Frame (ITRF) (Cartography and Geodesy) is an Earth-Centered, Earth-Fixed (ECEF) frame of reference, i.e. a non-inertial Cartesian coordinate system that rotates with Earth. The origin of coordinates is placed at the center of mass of Earth, with the Z-axis extending along the true North as defined by the IERS reference pole (which differs slightly from the axis of rotation of Earth at a given instant due to its slight wobbling known as polar motion (Sandoval-Romero and Argueta-Diaz 2010)). The X-axis extends towards the intersection between the equator and the Greenwich meridian at any time (the point with 0º latitude and 0º longitude), and the Y-axis is set such that a right-handed orthogonal coordinate system is completed. As a consequence of these features, the position of an object fixed with respect to the surface of the Earth does not change with time.\nGCRF\nThe Geocentric Celestial Reference Frame (GCRF) (Petit and Luzum 2010) is an inertial Cartesian coordinate system where the origin of coordinates is placed at the center of mass of the Earth and the axes are aligned with those of the International Celestial Reference System, which are defined based on the measurement of the position of extragalactic radio sources through very-long baseline interferometry. As a consequence, the axes are non-rotating, unlike in the ITRF frame. Even though GCRF, like any other body-centered frame of reference, is not a truly inertial frame of reference (given, for example, the fact that Earth itself is constantly orbiting around the Sun), it can be considered as such for Earth-orbiting satellites.\nTEME\nThe True Equator Mean Equinox (TEME) frame is another Earth-centered inertial frame of reference used mainly by the SGP4/SDP4 orbital propagators to output coordinates. The origin of coordinates is also placed at the center of mass of the Earth, and the Z-axis extends towards the position of the Celestial Ephemeris Pole (CEP). The location of the CEP matches the mean instantaneous axis of rotation averaged over a period of 2 days (which makes the CEP ignore the diurnal and quasi-diurnal polar motions) (Capitaine et al. 1985). The X-axis extends towards the so-called uniform equinox, which is calculated by finding the intersection between the ecliptic (the plane defined by the orbit of the Earth around the Sun) and the mean equator (the plane containing Earth’s equatorial line at a given time calculated by taking into consideration only precession, i.e., averaging over nutation effects), and then projecting this intersection (the mean equinox) over the true equator (the plane perpendicular to the previously defined Z-axis, which is therefore the equatorial plane at the same time calculated taking into consideration both precession and nutation) (Seago and Vallado 2000). Due to the complexity of its definition, the TEME frame is not currently widely used, except in the context of the results of propagating TLE elements with the SGP4/SDP4 models. It can also be inferred from its definition that the axes of the frame will slightly rotate over time as a consequence of the precession and nutation of Earth, and therefore it is important to know which time is taken as the reference when setting up the frame. While an explicit definition has not been given by NORAD, it is generally accepted that the reference elements (CEP, true equator and mean equinox) for the TEME frame are taken to be those at the time for which orbital propagation is performed (Seago and Vallado 2000; Vallado et al. 2006).\nOrbital elements\nKeplerian orbital elements are a set of six parameters used to describe the orbits of celestial objects, including satellites. While satellites do not follow a perfectly Keplerian orbit (i.e., a conic section that would be followed if only the point-like gravitational attraction between the satellite and the central body was considered), their state at any point can be defined by the orbital parameters that they would have if they were located at the same\nposition with the same velocity following a perfectly Keplerian orbit (i.e., if perturbations caused by other forces were absent). These are called osculating orbital elements. Different parametrizations exist, with one of the most frequent sets of parameters being analogous to the mean orbital elements used by TLE previously described (it should be kept in mind that mean motion, semi major axis and orbital period provide the same information). Additional, alternative parameters exist for edge cases where some of the conventional ones are not well defined or meaningful. These are:\nTrue anomaly: unlike mean anomaly, true anomaly is the angle between the direction of the perigee and the actual position of the satellite.\nArgument of latitude: the angle between the equator and the position of the satellite. It is useful to define the position of satellites in circular orbits, where the argument of perigee and true anomaly are not well defined.\nLongitude of perigee: the angle between the vernal equinox and the perigee. It is useful for cases of orbits with 0 inclination, where the longitude of the ascending node and the argument of perigee are not well defined.\nTrue longitude: the angle between the vernal equinox and the position of the satellite. It is useful for cases of circular orbits with 0 inclination, where the longitude of the ascending node, the argument of perigee and true anomaly are not well defined.\nIt should also be noted that orbital elements are defined with respect to a set of elements of reference, such as the equatorial plane, which are usually chosen to define an Earth-centered inertial frame of reference. Although the specific choice of these must be taken into account when interpreting the meaning of a given set of orbital parameters, a complete set of six Keplerian orbital elements defines unequivocally the position and velocity of the satellite at a given time in a frame of reference with elements of reference equivalent to those used for the orbital elements. Therefore, osculating orbital elements can be used to calculate a corresponding set of Cartesian coordinates.\nConversion between different coordinate systems\nThe following table summarizes the functions available in asteRisk to convert coordinates between the different frames of reference previously described:\n\n\n\n\n\nTable 2: Available functions\nfor conversions between different systems of coordinates.\n\n\nOrigin system\n\n\nTarget system\n\n\nFunction\n\n\nITRF\n\n\nGCRF\n\n\nITRFtoGCRF\n\n\nITRF\n\n\nGeodetic\n\n\nITRFtoLATLON\n\n\nGCRF\n\n\nITRF\n\n\nGCRFtoITRF\n\n\nGCRF\n\n\nGeodetic\n\n\nGCRFtoLATLON\n\n\nGCRF\n\n\nOrbital elements\n\n\nECItoKOE\n\n\nTEME\n\n\nGCRF\n\n\nTEMEtoGCRF\n\n\nTEME\n\n\nITRF\n\n\nTEMEtoITRF\n\n\nTEME\n\n\nGeodetic\n\n\nTEMEtoLATLON\n\n\nTEME\n\n\nOrbital elements\n\n\nECItoKOE\n\n\nGeodetic\n\n\nGCRF\n\n\nLATLONtoGCRF\n\n\nGeodetic\n\n\nITRF\n\n\nLATLONtoITRF\n\n\nOrbital elements\n\n\nGCRF\n\n\nKOEtoECI\n\n\nOrbital elements\n\n\nTEME\n\n\nKOEtoECI\n\n\nIt should be noted that, while a velocity value is not strictly required to perform conversions between systems of Cartesian coordinates, it is required to convert to or from orbital elements.\nIn the following example, we apply multiple coordinate transformations to obtain a projection of the trajectory of the Molniya satellite previously analyzed over the surface of Earth:\n\n\n## We previously calculated the trajectory of a Molniya satellite using the SDP4\n## propagator. The output coordinates are given in the TEME frame. Let us first\n## convert the final position to the standard GCRF frame of reference. Note that\n## we need to add the propagation time to the original epoch specified in the\n## TLE to obtain the correct date and time for conversion of reference frame\n\nendState_Molniya_GCRF <- TEMEtoGCRF(results_position_matrix_Molniya[144, 1:3]*1000,\n                                    results_velocity_matrix_Molniya[144, 1:3]*1000,\n                                    as.character(as.POSIXct(\n                                        molniya_TLE$dateTime, tz=\"UTC\") + \n                                            60*results_position_matrix_Molniya[144,4]))\n\nprint(endState_Molniya_GCRF$position)\n\n[1] -14390694.30  -4572069.28    -95636.31\n\nprint(results_position_matrix_Molniya[144, 1:3]*1000)\n\n          x           y           z \n-14384056.0  -4592709.2   -104890.6 \n\n## The coordinates values are not wildly different, since differences are due to\n## the precession and nutation of Earth's rotation axis.\n## Let us know convert the coordinates to geodetic latitude and longitude to\n## visualize a projection of the trajectory over the surface of Earth\n\ngeodetic_matrix_Molniya <- matrix(nrow=nrow(results_position_matrix_Molniya), ncol=4)\n\nfor(i in 1:nrow(geodetic_matrix_Molniya)) {\n    new_dateTime <- as.character(as.POSIXct(molniya_TLE$dateTime, tz=\"UTC\") + \n                                     60*target_times_Molniya[i])\n    new_geodetic <- TEMEtoLATLON(results_position_matrix_Molniya[i, 1:3]*1000,\n                                 new_dateTime)\n    geodetic_matrix_Molniya[i, 1:3] <- new_geodetic\n    geodetic_matrix_Molniya[i, 4] <- target_times_Molniya[i]\n}\n\ncolnames(geodetic_matrix_Molniya) <- c(\"latitude\", \"longitude\", \"altitude\", \"time\")\n\n## We can now visualize the ground track of the satellite with ggmap\n\ngroundTrack_Molniya <- ggmap(get_map(c(left=-180, right=180, bottom=-80, top=80),\n                                     source = \"stamen\")) +\n    geom_segment(data=as.data.frame(geodetic_matrix_Molniya), \n                 aes(x=longitude, y=latitude, \n                     xend=c(tail(longitude, n=-1), NA), \n                     yend=c(tail(latitude, n=-1), NA)), \n                 na.rm=TRUE) +\n    geom_point(data=as.data.frame(geodetic_matrix_Molniya), aes(x=longitude, y=latitude), \n               color=\"blue\", size=0.3, alpha=0.8)\n\n## As we can see, Molniya satellites spend most of the time over high latitudes,\n## thanks to the fact that the apogee of their elliptical orbit is above such\n## regions. This property made them useful for the Soviet Union.\n\ngroundTrack_Molniya\n\n\n\nFigure 4: Ground track of a Molniya satellite. The satellite spent most of the time over regions of high latitude.\n\n\n\n5 Future work\nWhile the described functionalities provide a solid core for astrodynamics analysis in R, additional features are expected to be frequently added to asteRisk. Some of these will include:\nExpansion of the force model used by the HPOP to increase its accuracy. For example, by including albedo radiation pressure or considering the additional Lens-Thirring and De Sitter relativistic effects.\nAddition of functionalities to model and calculate orbital maneuvers.\nSupport for more complex models for satellite orientation during propagation with the HPOP.\nAdding support for the Orbit Data Messages standard formats (Space Data Systems).\nAddition of more reference frames, including non-Earth centered frames.\n6 Summary\nAccurate calculation of the position of satellites at a given time is a key part of the design of successful space missions and continued operation and maintenance of satellites already in orbit. asteRisk provides tools to perform such astrodynamics analyses in R. These include well-documented orbital propagators and functions to parse the file formats most commonly used to distribute satellite positional data, as well as to convert coordinates between different coordinate frames. The provided toolbox should be specially useful in combination with the publicly available orbital data sources and the multiple statistical analysis and machine-learning functionalities already available in the R ecosystem, facilitating the development of novel orbital propagators with higher accuracy and lower computational costs.\n7 Appendix: Test cases for SGP4/SDP4\nThe following code will run SGP4/SDP4 propagators on a series of standard test frequently used to test the validity of implementations of these models (Vallado et al. 2006). The results demonstrate that the implementation provided in asteRisk is in agreement up to at least tenths of millimeters with previous implementations.\n\n\n## First, we create a list with the target propagation times for each test case,\n## in the same order as the TLEs are given\n\ntarget_times_verification <- list(\n    \"5\"=seq(0, 4320, by=360),\n    \"4632\"=c(0, -5184, -5064, -4944, -4896),\n    \"6251\"=seq(0, 2880, by=120),\n    \"8195\"=seq(0, 2880, by=120),\n    \"9880\"=seq(0, 2880, by=120),\n    \"9998\"=c(0, seq(-1440, -720, by=60)),\n    \"11801\"=seq(0, 1440, by=720),\n    \"14128\"=seq(0, 2880, by=120),\n    \"16925\"=seq(0, 1440, by=120),\n    \"20413\"=c(0, seq(1440, 4320, by=120)),\n    \"21897\"=seq(0, 2880, by=120),\n    \"22312\"=c(0, seq(54.2028672, 474.2028672, by=20)),\n    \"22674\"=seq(0, 2880, by=120),\n    \"23177\"=seq(0, 1440, by=120),\n    \"23333\"=c(seq(0, 1560, by=120), 1600),\n    \"23599\"=seq(0, 720, by=20),\n    \"24208\"=seq(0, 1440, by=120),\n    \"25954\"=c(0, seq(-1440, 1440, by=120)),\n    \"26900\"=c(0, 9300, 9360, 9400),\n    \"26975\"=seq(0, 2880, by=120),\n    \"28057\"=seq(0, 2880, by=120),\n    \"28129\"=seq(0, 1440, by=120),\n    \"28350\"=seq(0, 1440, by=120),\n    \"28623\"=seq(0, 1440, by=120),\n    \"28626\"=seq(0, 1440, by=120),\n    \"28872\"=seq(0, 50, by=5),\n    \"29141\"=seq(0, 420, by=20),\n    \"29238\"=seq(0, 1440, by=120),\n    \"88888\"=seq(0, 1440, by=120),\n    \"33333\"=seq(0, 20, by=5),\n    \"33334\"=0,\n    \"33335\"=seq(0, 1440, by=20),\n    \"20413\"=c(0, seq(1844000, 1844340, by=5))\n)\n\n## We can now read the TLE file with the TLEs for all verification cases and\n## propagate them at the target times. We can store the results for each case\n## as a matrix of 7 columns where each row corresponds to a propagation time,\n## column 1 stores propagation times, columns 2 to 4 the propagated positions\n## and columns 5 to 7 the propagated velocities\n\nverification_TLEs <- readTLE(\"data/verificationTLEs.tle\")\nverification_results <- vector(mode=\"list\", length=length(verification_TLEs))\nnames(verification_results) <- names(target_times_verification)\n\nfor(i in 1:length(verification_TLEs)) {\n    verification_results[[i]] <- matrix(nrow=length(target_times_verification[[i]]),\n                                        ncol=7)\n    for(j in 1:length(target_times_verification[[i]])) {\n        propagation <- sgdp4(n0=revDay2radMin(verification_TLEs[[i]]$meanMotion),\n                             e0=verification_TLEs[[i]]$eccentricity,\n                             i0=deg2rad(verification_TLEs[[i]]$inclination),\n                             M0=deg2rad(verification_TLEs[[i]]$meanAnomaly),\n                             omega0=deg2rad(verification_TLEs[[i]]$perigeeArgument),\n                             OMEGA0=deg2rad(verification_TLEs[[i]]$ascension),\n                             Bstar=verification_TLEs[[i]]$Bstar,\n                             initialDateTime=verification_TLEs[[i]]$dateTime, \n                             targetTime=target_times_verification[[i]][j])\n            verification_results[[i]][j, 1] <- target_times_verification[[i]][j]\n            verification_results[[i]][j, 2:4] <- propagation$position\n            verification_results[[i]][j, 5:7] <- propagation$velocity\n    }\n}\n\n\n8 Acknowledgements\nThe development of this work is supported by the following grants: a KAKENHI Grant-in-Aid for Research Activity Start-up Grant Number 21K20645 to Rafael Ayala, a JSPS Postdoctoral Fellowship for Research in Japan (Standard) Grant Number P20810 to Lara Sellés Vidal (Overseas researcher under Postdoctoral Fellowship of Japan Society for the Promotion of Science), and grants by the Spanish Ministry of Science and Innovation (grant code PID2019-105471RB-I00) and the Regional Government of Andalusia (grant code P18-RT-1060) to David Ruiz.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-023.zip\nCRAN packages used\nasteRisk, drat, nanotime, deSolve\nCRAN Task Views implied by cited packages\nDifferentialEquations, Epidemiology, TimeSeries\n\n\nAGI. Systems tool kit (STK).,URL https://www.agi.com/products/stk. (accessed: 24.11.2021).\n\n\nJ. Bouwmeester and J. Guo. Survey of worldwide pico-and nanosatellite missions, distributions and subsystem technology. Acta Astronautica, 67(7-8): 854–862, 2010.\n\n\nN. Capitaine, J. Williams and P. Seidelmann. Clarifications concerning the definition and determination of the celestial ephemeris pole. Astronomy and Astrophysics, 146: 381–383, 1985.\n\n\nF. A. for Cartography and Geodesy. The international terrestrial reference frame (ITRF).,URL https://www.iers.org/IERS/EN/DataProducts/ITRF/itrf.html. (accessed: 26.11.2021).\n\n\nA. Deprit. The elimination of the parallax in satellite theory. Celestial Mechanics, 24(2): 111–153, 1981.\n\n\nW. Dong and Z. Chang-yin. An accuracy analysis of the SGP4/SDP4 model. Chinese Astronomy and Astrophysics, 34(1): 69–76, 2010.\n\n\nA. Flores. GPS interface specification.,URL https://www.gps.gov/technical/icwg/IS-GPS-200M.pdf. (accessed: 26.11.2021).\n\n\nR. Flores, B. M. Burhani and E. Fantino. A method for accurate and efficient propagation of satellite orbits: A case study for a molniya orbit. Alexandria Engineering Journal, 60(2): 2661–2676, 2021.\n\n\nW. M. Folkner, J. G. Williams, D. H. Boggs, R. S. Park and P. Kuchynka. The planetary and lunar ephemerides DE430 and DE431. Interplanetary Network Progress Report, 196(1): 2014.\n\n\nC. Foster, H. Hallam and J. Mason. Orbit determination and differential-drag control of planet labs CubeSat constellations. arXiv preprint arXiv:1509.03270, 2015.\n\n\nW. Gurtner and L. Estey. RINEX - the receiver independent exchange format-version 3.00. Astronomical Institute, University of Bern and UNAVCO, Bolulder, Colorado., 2007.\n\n\nF. R. Hoots, R. L. Roehrich and T. S. Kelso. Spacetrack report no. 3., 1988. URL https://celestrak.com/NORAD/documentation/spacetrk.pdf. (accessed: 07.10.2021).\n\n\nR. S. Hujsak. A restricted four body solution for resonating satellite with an oblate earth. In American institute of aeronautics and astronautics conference, 1979.\n\n\nS. W. Janson, E. Jaime, M. Sergio, L. R. Hissa, R. A. de Carvalho, W. H. Steyn, V. J. Lappas, F. T. Nardini, C. Michele, R. Alexander, et al. Nanosatellites: Space and ground technologies, operations and economics. 2020.\n\n\nT. S. Kelso. CelesTrack.,URL https://celestrak.com/. (accessed: 16.09.2021).\n\n\nT. S. Kelso. Frequently asked questions: Two-line element set format.,URL http://www.celestrak.com/columns/v04n03/. (accessed: 07.10.2021).\n\n\nT. S. Kelso. NORAD two-line element set format.,URL http://www.celestrak.com/NORAD/documentation/ADCOM\\%20DO\\%20Form\\%2012.pdf. (accessed: 07.10.2021).\n\n\nD. J. Kessler and B. G. Cour-Palais. Collision frequency of artificial satellites: The creation of a debris belt. Journal of Geophysical Research: Space Physics, 83(A6): 2637–2646, 1978.\n\n\nD. J. Kessler, N. L. Johnson, J. Liou and M. Matney. The kessler syndrome: Implications to future space operations. Advances in the Astronautical Sciences, 137(8): 2010, 2010.\n\n\nA. Kirillin, I. Belokonov, I. Timbai, A. Kramlikh, M. Melnik, E. Ustiugov, A. Egorov and S. Shafran. SSAU nanosatellite project for the navigation and control technologies demonstration. Procedia Engineering, 104: 97–106, 2015.\n\n\nP. Knocke, J. Ries and B. Tapley. Earth radiation pressure effects on satellites. In Astrodynamics conference, page. 4292 1988.\n\n\nM. Lane. The development of an artificial satellite theory using a power-law atmospheric density representation. In 2nd aerospace sciences meeting, page. 35 1965.\n\n\nM. H. Lane and F. R. Hoots. General perturbations theories derived from the 1965 lane drag theory. AEROSPACE DEFENSE COMMAND PETERSON AFB CO OFFICE OF ASTRODYNAMICS. 1979.\n\n\nM. Lara, J. F. San-Juan, D. Hautesserres and C. de Competence Technique. A semi-analytical orbit propagator program for highly elliptical orbits. Order, 10: 8, 2016.\n\n\nM. Lara, J. F. San-Juan and L. M. López-Ochoa. Delaunay variables approach to the elimination of the perigee in artificial satellite theory. Celestial Mechanics and Dynamical Astronomy, 120(1): 39–56, 2014.\n\n\nH. Lévy, É. Joffre, S. Lizy-Destrez and M. Zamaro. STORM: A semi-analytical orbit propagator for assessing the compliance with mars planetary protection requirements. Acta Astronautica, 2021.\n\n\nZ. Li, M. Ziebart, S. Bhattarai and D. Harrison. A shadow function model based on perspective projection and atmospheric effect for satellites in eclipse. Advances in Space Research, 63(3): 1347–1359, 2019.\n\n\nM. Mahooti. High precision orbit propagator matlab implementation.,URL https://jp.mathworks.com/matlabcentral/fileexchange/55167-high-precision-orbit-propagator. (accessed: 24.11.2021).\n\n\nL. Maisonobe, V. Pommier and P. Parraud. Orekit: An open source library for operational flight dynamics applications. In 4th international conference on astrodynamics tools and techniques, pages. 3–6 2010.\n\n\nJ. Meeus. Astronomical algorithms. Richmond, 1991.\n\n\nO. Montenbruck and E. Gill. Satellite orbits: Models, methods and applications. Springer Science & Business Media, 2012.\n\n\nV. Morand, J. C. Dolado-Perez, H. Fraysse, F. Delefie, J. Daquin and C. Dental. Semi analytical computation of partial derivatives and transition matrix using STELA software. In 6th ESA conference on space debris, 2013.\n\n\nC. E. Noll. The crustal dynamics data information system: A resource to support scientific analysis using space geodesy. Advances in Space Research, 45(12): 1421–1440, 2010.\n\n\nG. Petit and B. Luzum. IERS conventions (2010). Bureau International des Poids et mesures sevres (france). 2010.\n\n\nJ. Picone, A. Hedin, D. P. Drob and A. Aikin. NRLMSISE-00 empirical model of the atmosphere: Statistical comparisons and scientific issues. Journal of Geophysical Research: Space Physics, 107(A12): SIA–15, 2002.\n\n\nK.-M. Roh, S. M. Kopeikin and J.-H. Cho. Numerical simulation of the post-newtonian equations of motion for the near earth satellite with an application to the LARES satellite. Advances in Space Research, 58(11): 2255–2268, 2016.\n\n\nSAIC. Space-Track.,URL https://www.space-track.org/. (accessed: 16.09.2021).\n\n\nG. E. Sandoval-Romero and V. Argueta-Diaz. A simple theoretical comparison between two basic schemes in function of the earth’s north pole detection: The static method. Journal of Sensors, 2010: 2010.\n\n\nM. Schäfer, M. Strohmeier, V. Lenders, I. Martinovic and M. Wilhelm. Bringing up OpenSky: A large-scale ADS-B sensor network for research. In IPSN-14 proceedings of the 13th international symposium on information processing in sensor networks, pages. 83–94 2014. URL https://doi.org/10.1109/IPSN.2014.6846743.\n\n\nJ. Seago and D. Vallado. Coordinate frames of the US space object catalogs. In Astrodynamics specialist conference, page. 4025 2000.\n\n\nK. Sośnica, G. Bury, R. Zajdel, K. Kazmierski, J. Ventura-Traveset, R. Prieto-Cerdeira and L. Mendes. General relativistic effects acting on the orbits of galileo satellites. Celestial Mechanics and Dynamical Astronomy, 133(4): 1–31, 2021.\n\n\nC. C. for Space Data Systems. Orbit data messages - recommended standard.,URL https://public.ccsds.org/Pubs/502x0b2c1e2.pdf. (accessed: 26.11.2021).\n\n\nR. I. of Space Device Engineering. Global navigation satellite system - interface control document.,URL https://russianspacesystems.ru/wp-content/uploads/2016/08/ICD_GLONASS_eng_v5.1.pdf. (accessed: 26.11.2021).\n\n\nB. Tapley, J. Ries, S. Bettadpur, D. Chambers, M. Cheng, F. Condi and S. Poole. The GGM03 mean earth gravity model from GRACE. In AGU fall meeting abstracts, pages. G42A–03 2007.\n\n\nA. Toorian, K. Diaz and S. Lee. The CubeSat approach to space access. In 2008 IEEE aerospace conference, pages. 1–14 2008. IEEE.\n\n\nUnited Nations. Register of Objects Launched into Outer Space.,URL https://www.unoosa.org/oosa/en/spaceobjectregister/index.html. (accessed: 15.09.2021).\n\n\nD. Vallado. Astrodynamics software.,URL https://celestrak.com/software/vallado-sw.php. (accessed: 07.10.2021).\n\n\nD. A. Vallado, P. Crawford, R. Hujsak and T. Kelso. Revisiting spacetrack report# 3: Rev 1. In AIAA/AAS astrodynamics specialist conference and exhibit, 2006.\n\n\nG. Wanner and E. Hairer. Solving ordinary differential equations II. Springer Berlin Heidelberg, 1996.\n\n\n\n\n",
    "preview": "articles/RJ-2023-023/RJ-2023-023_files/figure-html5/groundtrack-Molniya-1.png",
    "last_modified": "2023-11-07T21:31:41+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "articles/RJ-2023-024/",
    "title": "gplsim: An R Package for Generalized Partially Linear Single-index Models",
    "description": "Generalized partially linear single-index models (GPLSIMs) are important tools in nonparametric regression. They extend popular generalized linear models to allow flexible nonlinear dependence on some predictors while overcoming the \"curse of dimensionality.\" We develop an R package gplsim that implements efficient spline estimation of GPLSIMs, proposed by [@yu_penalized_2002] and [@yu_penalised_2017], for a response variable from a general exponential family. The package builds upon the popular mgcv package for generalized additive models (GAMs) and provides functions that allow users to fit GPLSIMs with various link functions, select smoothing tuning parameter $\\lambda$ against generalized cross-validation or alternative choices, and visualize the estimated unknown univariate function of single-index term. In this paper, we discuss the implementation of gplsim in detail, and illustrate the use case through a sine-bump simulation study with various links and a real-data application to air pollution data.",
    "author": [
      {
        "name": "Tianhai Zu",
        "url": {}
      },
      {
        "name": "Yan Yu",
        "url": {}
      }
    ],
    "date": "2023-08-26",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-024.zip\n\n\nY. Yu and D. Ruppert. Penalized Spline Estimation for Partially Linear Single-Index Models. Journal of the American Statistical Association, 97(460): 1042–1054, 2002. URL https://www.jstor.org/stable/3085829 [online; last accessed April 5, 2020].\n\n\nY. Yu, C. Wu and Y. Zhang. Penalised spline estimation for generalised partially linear single-index models. Statistics and Computing, 27(2): 571–582, 2017. URL https://doi.org/10.1007/s11222-016-9639-0 [online; last accessed July 1, 2020].\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:41+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-025/",
    "title": "Non-Parametric Analysis of Spatial and Spatio-Temporal Point Patterns",
    "description": "The analysis of spatial and spatio-temporal point patterns is becoming increasingly necessary, given the rapid emergence of geographically and temporally indexed data in a wide range of fields. Non-parametric point pattern methods are a highly adaptable approach to answering questions about the real-world using complex data in the form of collections of points. Several methodological advances have been introduced in the last few years. This paper examines the current methodology, including the most recent developments in estimation and computation, and shows how various R packages can be combined to run a set of non-parametric point pattern analyses in a guided and intuitive way. An example of non-specific gastrointestinal disease reports in Hampshire, UK, from 2001 to 2003 is used to illustrate the methods, procedures and interpretations.",
    "author": [
      {
        "name": "Jonatan A. González",
        "url": {}
      },
      {
        "name": "Paula Moraga",
        "url": "http://www.paulamoraga.com"
      }
    ],
    "date": "2023-08-26",
    "categories": [],
    "contents": "\n1 Introduction\nA spatial point pattern \\(X=\\{\\mathbf{u}_i\\}_{i=1}^n\\) is a random collection of points observed within a bounded region of the plane, \\(W \\subset\\mathbb{R}^2\\). Spatial point patterns arise in a broad range of applied disciplines such as climatology, ecology, epidemiology, and seismology. A spatial point pattern can represent the locations of forest fires (Turner 2009), the occurrence of species (Moraga 2021) or the residence locations of people with a certain disease (Moraga and Montes 2011). When the points evolve in space and time, we call them spatio-temporal point patterns.\nIn this case, we treat the data as being generated as a snapshot in space-time, and they may be viewed as a spatial point process with a further (temporal) dimension (González et al. 2016). Similarly to the spatial case, we consider a spatio-temporal point pattern as a countable set of random points \\(\\{(\\mathbf{u}_i,v_i)\\}_{i=1}^n\\) (non-overlapping), where \\(\\mathbf{u}_i\\) represents the spatial location of the \\(i^{\\text{th}}\\) event and all of them belong to a bounded planar region \\(W \\subset\\mathbb{R}^2\\). Here, \\(v_i\\) is associated with time and it belongs to a compact positive interval \\(T\\subset \\mathbb{R}_+\\). \\(W \\times T\\) is usually called the spatio-temporal observation window. Examples of spatio-temporal point patterns include the locations of individuals with an infectious disease in which the time of infection is known (Diggle 2013), the starting (or ending) locations of tornadoes with their registered times (González et al. 2019) or the location and time at which some crime was committed (Rodrigues and Diggle 2012). Point pattern analyses may also be performed in smaller dimensions, such as straight segments (Daley and Vere-Jones 2003). For example, we can consider a time segment as a straight segment and record the exact time the heartbeat of a patient with arrhythmia occurs during an electrocardiogram. In addition, we can also have marked point patterns in which each of the points has some additional characteristics. For example, a marked point pattern may represent a pattern of beta-type ganglion cells in a cat’s retina that can be classified anatomically as the marks “on” or “off” (Wässle et al. 1981).\nWhen dealing with point patterns, it is desirable to learn as much as possible about the probability distribution that governs the occurrence of points in a given point pattern. This is a difficult task since, in general, there is only a single set of points or realisation of the point process. However, many methods have been developed that serve to estimate specific characteristics of the distribution, such as the intensity, which describes the expected number of points per unit area; or the clustering or aggregation behaviour, that is, whether the points have a particular attraction or repulsion mechanism. Statistical models for point processes can also be formulated, and these may consider previous knowledge to describe trends, variability, and different factors that may impact them. Point patterns are usually considered a random sample of a stochastic mechanism called point process. The most typical families of point processes are Poisson processes. There are two classes of Poisson processes; the homogeneous, whose realisations in any subset of the observation window are uniform random samples with a constant mean \\(\\lambda\\), and the inhomogeneous, obtained by replacing the intensity \\(\\lambda\\) by a spatial, temporal or spatio-temporal varying one (Illian et al. 2008; Diggle 2013). Homogeneous Poisson processes are benchmark models for spatial and spatio-temporal point pattern data, often called complete spatial random (CSR) or complete spatio-temporal random (CSTR) point processes; i.e., point processes whereby point events appear in a completely random fashion.\nA large body of literature has been developed in recent years to study point patterns and many statistical software packages have been developed for their estimation and analysis. For instance, packages such as spatial (Venables and Ripley 2002) or splancs (Rowlingson and Diggle 1993) initially developed for S-PLUS but available on CRAN are classics for displaying and analysing spatial point pattern data. Packages as sp (Pebesma and B. 2005; Bivand et al. 2013), or more recently, sf (Pebesma 2018), were created to provide a uniform interface for handling 2D and 3D data. spatstat (Baddeley and Turner 2005; Baddeley et al. 2015) is one of the most popular and improved packages for analysing spatial point patterns It is divided into several subpackages: spatstat.utils, spatstat.sparse, spatstat.data, spatstat.geom, spatstat.random, spatstat.explore, spatstat.model and spatstat.linnet. Some extra specialised packages such as smacpod or sparr (Davies et al. 2018) have methods for analysing case-control point patterns; or spatgraphs for graphs-related analyses of locations in 1D, 2D, 3D.\nEven though authors have considered the combination of space and time in theory, and nowadays, spatial data often includes a time component, only a few examples show this in practice. This problem may be due to the complexity of the computations, added to the fact that the authors tend to delve into particular contexts. There are some tools available for dealing with spatio-temporal point pattern data. The stpp package (Gabriel et al. 2013) covers many models and functional descriptors related to point process methods to study spatio-temporal phenomena. The PtProcess package (Harte 2010) was designed to fit time-indexed point process models in the same simplified fashion as generalised linear models software. The lgcp (Taylor et al. 2013), as well as the inla (Rue et al. 2009; Moraga 2019); http://www.r-inla.org and inlabru (Bachl et al. 2019) packages, are suitable options for statistical inference in space and time.\nIn this paper, we demonstrate how R (R Core Team 2023) can be used to study spatial and spatio-temporal point process data using a dataset of geographic locations and times of individuals with non-specific gastrointestinal infections in Hampshire, UK, from 2001 to 2003 (Diggle et al. 2003). We consider the data as realisations of spatial and spatio-temporal point processes that lack spatial and temporal homogeneity; i.e., the expected number of points in each area unit of the study region depends on their location and time. Consequently, the first problem we solve is the intensity estimation. We then investigate the nature of the stochastic interactions between the process points after adjusting for spatial and spatio-temporal inhomogeneity. We use second-order descriptors to describe these interactions.\nAlong with this paper, we employ several R packages to successfully analyse the complex point patterns seen in both the spatial and the spatio-temporal settings. For dealing with spatial point patterns, we mainly use the spatstat package. In addition, we use the sparr package for analysing the relative risk and the GET package (Myllymäki and Mrkvička 2020) for testing hypotheses with envelopes. For the spatio-temporal analysis, we use the stpp package. We also make use of other R packages such as parallel, foreach (Microsoft and Weston 2022b) and doParallel (Microsoft and Weston 2022a) to optimise the computing time by dividing the burden by several processors. An R script containing all the necessary code for reading the data and running the analyses shown in this paper is provided as supplementary material.\nSpatio-temporal point pattern data\nWe use the geographic locations and time information of individuals who had non-specific gastrointestinal infections in Hampshire, UK, between 2001 and 2003 (Diggle et al. 2003). These data come from the Ascertainment and Enhancement of Gastrointestinal Infection Surveillance and Statistics (AEGISS) project (Diggle et al. 2003), which, using the Hampshire area as a test, seeks to identify irregularities, i.e., high or low intensity spots in the spatio-temporal distribution of non-specific gastrointestinal infections in the UK.\nThe data consist of a collection of points \\(\\{(\\mathbf{u}_i,v_i)\\}_{i=1}^n\\), where \\(\\mathbf{u}_i\\) is a spatial coordinate with two components, i.e., \\(\\mathbf{u}_i = (a_i, b_i), a_i,b_i \\in \\mathbb{R}\\), and \\(v_i\\in \\mathbb{R}_+\\) is a temporal coordinate. These points represent the locations and times of daily healthcare providers reports of non-specific gastrointestinal disease in Hampshire from 2001 to 2003. After cleaning the data to remove non-Hampshire residents (out of the Hampshire region) and multiple (coincident) points, there were \\(n = 10443\\) cases of non-specific gastrointestinal disease by a phone-in triage service operating within the UK’s National Health Service. The command sequence\n\n\nlibrary(spatstat)\nlibrary(viridis)\nload(\"AegissData.RData\")\n\n\nloads the data in the local environment of R. Each location corresponds to the centroid of the unit post-code of the residential address of the caller. The unit of distance is one kilometre. The unit of time is one day, with day 1 corresponding to 1 January 2001. We can inspect the first spatio-temporal coordinates of our dataset by writing\n\n\nhead(as.data.frame(Aegiss))\n\n       x      y marks\n1 478.55 134.85     1\n2 458.15 107.35     1\n3 449.05 128.95     1\n4 450.75 106.95     1\n5 461.05  99.55     1\n6 446.35 109.15     1\n\nNotice that x and y are the coordinates. Times are not labelled as dates, but they are integers, and their column is named as ‘marks’. Finally, the planar region that encloses the points corresponds to a 120-sided polygon representing the boundary of the study region of Hampshire. To visualise the spatio-temporal point pattern, we can depict a spatial map using the symbolmap() function of the spatstat.geom package setting the times as colours. The colours are selected by using the viridis (Garnier et al. 2023) (Figure 1).\n\n\nTimes <- Aegiss$marks\ntimelabels <- as.Date(Times - 1, origin = \"2001-01-01\")\ncolmap <- viridis(length(Times))\nsy <- symbolmap(pch = 21, col = \"black\", bg = colmap, range = range(timelabels))\n\n\nWe use the locations and times of the point pattern and the population density of Hampshire to study the point process that generates these data. We obtain the mean population density of Hampshire from WorldPop (University of Southampton). The population information detailed for each squared kilometre is available in a yearly basis. Figure 1 shows the point pattern of the cases and the population density. First, we save only the spatial locations with no marks by writing X <- unmark(Aegiss). Then, we attach the date labels to the points using the operator %mark%. Note that symap = sy sets the colour and point shapes previously defined and that the six dates in the legend are a sample to represent the colour variation. To summarise the population density, given in a set of three images, 2001, 2002 and 2003, we use the function im.apply() of the package spatstat.geom to average them.\n\n\npar(mfrow = c(1, 2), mar = c(0,0,2,0.5)) \nX <- unmark(Aegiss)\nPopDens <- im.apply(Population, mean)\nplot(X %mark% timelabels, symap = sy, \n     main = \"Gastrointestinal disease reports\")\nplot(PopDens, col = viridis(prod(PopDens$dim)), log = T, box = F,\n     main = \"Population intensity\")\n\n\n\nFigure 1: Left: Locations of 10443 reports of non-specific gastrointestinal disease in Hampshire from January 2001 to July 2003. The time is treated as a quantitative mark where darker dots correspond to the older events, and lighter dots match the most recent reports. Right: Hampshire population density averaged over the years 2001-2003. The colour map is evenly-spaced on a logarithmic scale.\n\n\n\n2 Spatial analysis\nSpatial intensity and relative risk\nThe first-order intensity function or the intensity of a point process can be defined as the expected number of events per unit space, or mathematically (Diggle 2013),\n\\[\n\\lambda(\\mathbf{u})=\\lim_{|\\text{d}\\mathbf{u}|\\rightarrow 0}\\frac{\\mathbb{E}N(\\text{d}\\mathbf{u})}{|\\text{d}\\mathbf{u}|},\n\\]\nwhere \\(N()\\) denotes the number of points of a region. In the inhomogeneous case, we can compute a kernel smoothed intensity function from a point pattern \\(\\{\\mathbf{u}_i\\}_{i=1}^n\\) without the temporal coordinates. The estimator is given by\n\\[\\begin{equation}\n    \\hat{\\lambda}(\\mathbf{u})=\\frac{1}{e^2(\\mathbf{u})}\\sum_{i=1}^{n}\\kappa_{\\epsilon}^{2}(\\mathbf{u}-\\mathbf{u}_i), \\quad \\text{and} \\quad e^2(\\mathbf{u})=\\int_{W} \\kappa_{\\epsilon}^{2}(\\mathbf{s}-\\mathbf{u}) \\text{d} \\mathbf{s},\n    \\tag{1}\n\\end{equation}\\]\nwhere \\(\\mathbf{u}\\in W\\) and \\(\\kappa_{\\epsilon}^{2}\\) is a two-dimensional Gaussian kernel. \\(e()\\) is known as uniform edge correction and is intended to correct the bias of the estimation at the edges of the region (Baddeley et al. 2015). One of the most challenging considerations is selecting the bandwidth \\(\\epsilon\\); there is no single best procedure for doing that, but the estimation can be improved by using a spatially varying bandwidth despite its computational complexity (Davies and Baddeley 2018). For computing the adaptive estimate of the intensity function, we use the adaptative.density() function of the spatstat.explore package,\n\n\n# This takes roughly 12 seconds to be executed in an\n# iMAC 2019, 3GHz intel core i5 processor with 16GB of RAM (hereinafter)\nSpatialDens <- adaptive.density(X, method = \"kernel\", edge = T)\n\n\nThe estimated intensity is shown in Figure 2 (left). We can see a high concentration of cases (more than two per square kilometre) in Southampton and its coastal environs. More to the north, there are also local foci in Winchester (the centre of the study region) and the primary urban centres located in the north and northeast of the study region.\nThe kernel log relative risk or density ratio function is a descriptor aimed to compare two estimated densities on the study region \\(W\\). In epidemiology, it is handy to examine disease risk fluctuations based on case and control samples. In recent years, many improvements have been developed to the estimation methodology, including spatially adaptive smoothers and inference based on asymptotic theory (Davies et al. 2018). The related techniques are implemented in the sparr package. Given two spatial point patterns, \\(X\\), with intensity \\(\\hat{\\lambda}(\\mathbf{u}|X)\\) (that can be thought of as cases) and \\(Y\\), with intensity \\(\\hat{\\lambda}(\\mathbf{u}|Y)\\) (that can be thought of as controls); the estimated log relative risk is defined as\n\\[\\begin{equation}\n    \\hat{r}(\\mathbf{u})=\\log \\left\\{\\frac{\\hat{\\lambda}(\\mathbf{u}|X)}{\\hat{\\lambda}(\\mathbf{u}|Y) } \\cdot \\frac{n_Y}{n_X} \\right\\}, \\quad \\mathbf{u} \\in W,\n    \\tag{2}\n\\end{equation}\\]\nwhere \\(n_X\\) and \\(n_Y\\) are the number of points of \\(X\\) and \\(Y\\), respectively. When \\(\\hat{r}(\\mathbf{u}) \\approx 0\\), the densities are roughly the same; peaks greater than zero mean a higher local ratio of cases to controls, and \\(\\hat{r}(\\mathbf{u}) < 0\\) indicates lack of concentration of cases than controls.\nHere, we compute the relative risk by comparing the point pattern of gastrointestinal infections and a point pattern with controls derived from a random sample of the underlying population. We first generate a set of random controls by simulating \\(n=1443\\) locations with intensity proportional to the population density using the rpoispp() function of package spatstat.random. In order to do that, we calculate the population density as the population intensity normalised by its mass (its integral over the observation window) calculated by using the function integral.im() of the package spatstat.geom in each region of Hampshire. Then we can evaluate expressions involving one or more pixel images through the eval.im() function of package spatstat.geom,\n\n\nN <- integral.im(PopDens) \nn <- X$n\nControlDens <- eval.im((PopDens / N) * n)\nControls <- rpoispp(ControlDens)\n\n\nThen we assign Hampshire’s window to the new point pattern by using the Window() function of package spatstat.geom,\n\n\nWindow(Controls) <- X$window\n\n\nFor estimating the relative risk, we use the sparr package to perform an adaptive smoothing adequately designed to deal with the reduced smoothing in densely populated areas. The code\n\n\n# This takes roughly 22 seconds to be executed \nlibrary(sparr)\nRelativeRisk <- risk(f = X, g = Controls, adapt = T, \n                     pilot.symmetry = \"pooled\", tolerate = T)\n\n\ntakes both point patterns (cases f and controls g), computes an adaptive smoothing for estimating the risk, and computes asymptotic tolerance contours as per Hazelton and Davies (2009). Maps displayed in Figure 2 showing the intensity and relative risk estimates are produced by executing\n\n\npar(mfrow = c(1, 2), mar = c(0,0,2,0.5)) \nplot(SpatialDens, col = viridis(1200), auto.axes = F,\n     main = \"Adaptative kernel intensity estimate\", box = F)\nplot(RelativeRisk, auto.axes = F, tol.type = \"upper\", \n     main = \"Relative risk estimate\")\n\n\n\nFigure 2: Left: Adaptive bandwidth kernel estimate of intensity for the gastrointestinal data. The intensity is expressed as number of reports per squared kilometre. Right: Adaptive spatial log-relative risk of non-specific gastrointestinal disease, with asymptotic tolerance contours for elevated risk.\n\n\n\nFigure 2 (right) provides pointwise areas of significant high relative risk. The null hypothesis is \\(H_0: r(\\mathbf{u})=0\\), against an alternative of increased local risk \\(H_1:r(\\mathbf{u})>0\\) for a point \\(\\mathbf{u}\\in W\\). We superimpose the contours corresponding to significant values on the log relative risk surface. They are known as tolerance contours and allow us to identify areas of high risk (Kelsall and Diggle 1995). The relative abundance of gastrointestinal disease reports appears to be significantly higher in some areas where the intensity is not necessarily higher. Nor does the relative risk appear to increase in Southampton (located on the southern coast), the region’s most urbanised area.\nSecond-order descriptors\nPair correlation and \\(K\\)-functions\nThe second-order intensity function \\((\\lambda^{(2)}(\\mathbf{u}_1,\\mathbf{u}_2), \\mathbf{u}_1,\\mathbf{u}_2\\in W)\\) is, in point processes, analogous to the covariance function in classical statistics (Baddeley et al. 2015). Also known as product density function, the second-order intensity function represents the infinitesimal two-dimensional joint distribution of the points. A more friendly summary statistic is the pair correlation function, which is the standardised probability density that an event occurs in each of two small spatial volumes \\(\\text{d} \\mathbf{u}_1\\) and \\(\\text{d} \\mathbf{u}_2\\). For CSR (Complete Spatial Random) point processes, the covariance is zero, and the pair correlation function takes the value of one (Diggle 2013). Different values than these benchmarks indicate how likely a pair of events will occur at the specified locations than in a CSR process with the same intensity; i.e., larger values indicate clustering, and smaller values indicate regularity. The pair correlation is defined as\n\\[\\begin{equation}\n    g(\\mathbf{u}_1,\\mathbf{u}_2)=\\frac{\\lambda^{(2)}(\\mathbf{u}_1,\\mathbf{u}_2)}{\\lambda(\\mathbf{u}_1)\\lambda(\\mathbf{u}_2)}, \\qquad \\mathbf{u}_1,\\mathbf{u}_2\\in W.\n    \\tag{3}\n\\end{equation}\\]\nWhenever the intensity function of a point process is bounded away from zero and its pair correlation function depends only on the difference vector \\(r =||\\mathbf{u}_1 - \\mathbf{u}_2||\\), the point process is called second-order intensity reweighted stationary (SOIRS) and isotropic. For estimating the pair correlation function, consider a point pattern \\(X=\\{\\mathbf{u}_{i}\\}_{i=1}^n\\) with \\(n\\) points, then\n\\[\\begin{equation}\n    \\hat{g}(r)=\\frac{1}{2\\pi} \\sum_{i=1}^n \\sum_{j\\neq i}\n    \\frac{\\kappa_{\\epsilon}(||\\mathbf{u}_{i}-\\mathbf{u}_{j}||- r)}{\\hat{\\lambda} ( \\mathbf{u}_{i}) \\hat{\\lambda} (\\mathbf{u}_{j}) ||\\mathbf{u}_{i}-\\mathbf{u}_{j}|| w_{ij}}.\n    \\tag{4}\n\\end{equation}\\]\nSeveral parameters must be set before the estimation: a suitable interval for the spatial distances \\(r\\), a bandwidth \\(\\epsilon\\), a one-dimensional kernel \\(\\kappa_{\\epsilon}(\\cdot)\\) (Epanechnikov’s kernel is usually recommended), a type of divisor (factor \\(||\\mathbf{u}_{i}-\\mathbf{u}_{j}||\\) in the denominator of Eq. (4) can be replaced by \\(r\\), this option brings a pole, i.e., an indetermination, in \\(r=0\\) (Wong and Stoyan 2021)); and an edge correction \\(w_{ij}\\) correcting for the loss of information of outsider points close to the edges. spatstat usually has smart defaults for these parameters based on the knowledge of the descriptors, which many authors have contributed. For this part, we use a fixed-bandwidth kernel estimate, with bandwidth set by Scott’s rule of thumb (Scott 2015), through the function bw.scott() of the package spatstat.explore, as we need simulations where the intensity has to be estimated time after time. We employ the function density.ppp() of package spatstat.explore that provides a fixed bandwidth kernel smoothed intensity function from a point pattern according to Eq. (1).\n\n\nsigmaD <- bw.scott(X)\nMD <- density.ppp(X, diggle = T, positive = T, sigma = sigmaD)\nMP <- density.ppp(X, diggle = T, sigma = sigmaD, at = \"points\")\n\n\nIn the above implementation, diggle = T sets the Jones-Diggle edge correction (Diggle 1985), at = 'points' means that the intensity is computed only at the points of \\(X\\), and sigma is the bandwidth, which in this case is \\(3.5\\)km for the horizontal axis and \\(4.19\\)km for the vertical axis. In the case of the bandwidth for the pair correlation function, we employ a composite likelihood approach (Jalilian and Waagepetersen 2018) by using the function bw.pcf() of library spatstat.explore with the parameter cv.method = 'compLik'; the parameter divisor = 'd' refers to the term \\(||\\mathbf{u}_{i}-\\mathbf{u}_{j}||\\) in Eq. (4), which can be replaced by \\(r\\) (Jalilian and Waagepetersen 2018), and lambda is the estimate of the intensity. We then write\n\n\n# Time consuming: This takes roughly 24 minutes to be executed \nbwG <- bw.pcf(X, cv.method = \"compLik\", divisor = \"d\", lambda = MP)\n\n\nand obtain a bandwidth value of \\(2.7\\)km. Note that this function can warn of undetermined contributions to the pair correlation. These caveats come from the divisor, which, when too small, can conflict with the numerical tolerance of R and eventually lead to indeterminacy. The spatstat package fixes this by simply removing the problematic values.\nBefore going further with the estimation of the pair correlation function, let us take a look into another classical second-order descriptor, the inhomogeneous \\(K\\)-function, which in the SOIRS case, can be defined as\n\\[\n    K(r)=2\\pi \\int_0^r sg(s)\\text{d} s.\n\\]\nFor spatial Poisson point processes \\(K(r)=\\pi r^2\\). spatstat chooses a suitable range for \\(r\\) values based on some rules of thumb depending on geometric attributes of the process or the average intensity in the region. We decide to choose a proportion of 70% of the maximum value for \\(r\\) proposed by spatstat.explore through the rmax.rule() function, and set of 70 (71 including zero) different values for \\(r\\),\n\n\nr0 <- 0.7 * rmax.rule(\"K\", X$window, intensity(X))\nrr <- seq(0, r0, length.out = 71)\n\n\nThe classical estimator of the spatial \\(K\\)-function is given by\n\\[\\begin{equation*}\n    \\hat{K}(r)= \\sum_{i=1}^n \\sum_{j \\neq i}\n    \\frac{\\mathbf{1}\\left[|| \\mathbf{u}_{i}-\\mathbf{u}_{j} || \\leq r\\right]}{\\hat{\\lambda}(\\mathbf{u}_{i}) \\hat{\\lambda} (\\mathbf{u}_{j}) w_{ij}}.\n    \\tag{5}\n\\end{equation*}\\]\nWhere \\(\\mathbf{1}[\\cdot]\\) is the indicator function. An alternative descriptor defined as \\(L(r)-r= \\sqrt{K(r) / \\pi}-r\\), and its straightforward estimator \\(\\hat{L}(r)-r\\) are intended for stabilising the variance (Besag 1977).\nIn addition to the estimation of the second-order descriptors, their standard deviation and confidence intervals are desirable for the analysis; this is not always simple, not even in the Poisson case, as there is dependence between the contributions of different points to the descriptor. However, we can use Monte Carlo simulation for computing envelopes through computational efforts. We use command envelope() of the spatstat.explore for getting the observed descriptor as well as its simulated counterparts.\nWe note that although Monte Carlo testing is generally used in point process contexts, it becomes invalid when the null hypothesis is composite, i.e., when parameters, such as the intensity function, must be estimated prior to the computing of the envelopes, which is our case for inference about the second-order descriptors. For dealing with such composite hypotheses, an extra set of simulations is needed to perform a balanced two-stage test (Baddeley et al. 2017). The GET package performs global envelope tests (where the significance level is controlled simultaneously for all the functions) for the second-order descriptors and provides a graphical interpretation (see, e.g., Myllymäki et al. 2017). We perform a global rank envelope test with the extreme rank length measure and retrieve the \\(p\\)-value. We warn the reader that this procedure of computing envelopes and two-stage Monte Carlo testing is computationally demanding. We calculate the first set of \\(39\\) envelopes for the centred \\(L\\)-function. This selection gives 39 (simulated) + 1 (observed) independent samples; it comes from classical settings of point process simulation (Ripley 1977; Baddeley et al. 2015, 2017). We use the envelope.ppp() function of the spatstat.explore package; this function computes simulation envelopes of a summary function,\n\n\n# Time consuming: This takes roughly 3 minutes to be executed \nL1 <- envelope(X, nsim = 39, savefuns = TRUE, fun = \"Linhom\", diggle = T,\n               transform = expression(.-r), sigma = sigmaD, r = rr, \n               simulate = expression(rpoispp(lambda = MD)), verbose = F)\n\n\nThen we generate another set of inhomogeneous Poisson point patterns with the estimated intensity,\n\n\nSimpatterns <- rpoispp(lambda = MD, nsim = 39)\n\n\nWe design a function to calculate the intensity for each point pattern and to compute envelopes with point patterns simulated from the new intensity,\n\n\nsimL <- function(rep) {\n  sim_fit <- density.ppp(Simpatterns[[rep]], diggle = T, \n                         sigma = sigmaD, positive = T)\n  envelope(Simpatterns[[rep]], nsim = 39, savefuns = T, fun = \"Linhom\", \n           transform = expression(.-r), r = rr, diggle = T, sigma = sigmaD,\n           simulate = expression(rpoispp(lambda = sim_fit)))\n}\n\n\nNow we apply the function for every point pattern of the list to get a list of envelope objects. This process is very slow as 39 \\(L\\)-functions have to be computed for each point pattern. We employ the package foreach to use multiple processors to accelerate this procedure in combination with the parallel and doParallel packages. The function detectCores() of the package parallel automatically detects the computer’s CPU cores and the function registerDoParallel() of the doParallel package is used to register a parallel backend in the computer. The foreach() %dopar% function returns a list with the result of applying a function over every element of the main argument by using the registered cores of the computer processor,\n\n\n# Time consuming: This takes roughly 21 minutes to be executed \nlibrary(foreach)\nlibrary(doParallel)\nc0 <- parallel::makeCluster(detectCores() - 1)\ndoParallel::registerDoParallel(c0)\nL.ls <- foreach(i = 1:39, .packages = c(\"spatstat\")) %dopar% {simL(i)}\nparallel::stopCluster(c0)\n\n\nFinally, we perform the adjusted test using the function GET.composite() of the GET package and plot the envelopes. In this case, we are interested in establishing a possible positive departure of the observed centred \\(L\\)-function and its simulated counterparts. Thus, we set alternative = \"greater\", type = 'erl', which performs a rank envelope test based on extreme rank lengths (Myllymäki et al. 2017) based on several minimal pointwise ranks; and use the ggplot2 (Wickham 2016) package for the plots,\n\n\nlibrary(GET)\nlibrary(ggplot2)\nresL <- GET.composite(X = L1, X.ls = L.ls, type = \"erl\", \n                      alternative = \"greater\", savefuns = T)\n\n\n\n\n\nFigure 3: Graphical clustering tests of Hampshire data based on the \\(L\\)-functions. The gray bands represent the \\(95\\%\\) global envelope. Red dots stand for the second-order descriptors outside the envelopes.\n\n\n\nFigure 3 shows the output of the rank envelope test. The rejection is explained by the attractive behaviour of gastrointestinal disease cases at short distances. The red-dot line indicates this clustering behaviour up to 5km, which shows a departure from the narrow grey band that represents the 95% adjusted rank envelope, i.e., the rank envelope that considers the composite hypothesis. Note that the more simulations, the better results for the test (Myllymäki et al. 2017). The dashed black line represents the central function (the mean) of only the first set of simulations; note that this function is not the mean value of the grey ribbon as it comes from envelopes under the composite null hypothesis. We find a significant positive departure from the envelopes up to approximately \\(5\\)km; this means that observed data are more clustered in distances below \\(5\\)km than might be expected in Poisson point patterns with the same intensity, and this clustering is statistically significant.\n3 Spatio-temporal analysis\nSpatio-temporal intensity and relative risk\nFirst-order separability\nSpatio-temporal point processes analyses can be carried out when every data point has associated geographic and time information. In these cases, estimating the first-order intensity function constitutes a primary interest. The intensity function provides information about the joint distribution of the spatial and temporal locations. Usually, this distribution is simplified by assuming, for example, separability. A point process is referred to as when its spatio-temporal intensity function can be factorised as\n\\[\n    \\lambda(\\mathbf{u},v) = \\lambda_1(\\mathbf{u}) \\lambda_2(v), \\quad  \\mathbf{u} \\in W,v \\in T,\n\\]\nwhere \\(\\lambda_1(\\cdot)\\) and \\(\\lambda_2(\\cdot)\\) are non-negative functions of space and time, respectively. Authors often take separability for granted as it is a very pragmatic working assumption, but it should be tested before choosing an estimator. There are some specialised separability tests available in the literature. We use the package kernstadapt (González and Moraga 2022b) to test separability in our example by considering a division of the temporal interval \\(T=[1,1095]\\) into 16 disjoint equally-spaced sub-intervals. We split the spatial window \\(W\\) into 20 disjoint rectangular subsets with roughly the same area. As the separability of the intensity might be thought of as the independence of two random variables, it can be tested by a simple \\(\\chi^2\\)-test as proposed by Ghorbani et al. (2021), where the null hypothesis is separability. The test bases on the counts of points in disjoint spatial and temporal sub-regions; when there are expected counts of individual cells below 5, the authors recommend a Fisher’s test. We employ Fisher’s test calculating the \\(p\\)-value by Monte Carlo simulation (5000 replicates):\n\n\n# This takes roughly 20 seconds to be executed \nSepTest <- separability.test(Aegiss, nx = 5, ny = 4, nt = 16, nperm = 50000)\n\n\n\n\nSepTest\n\n\n    Separability test based on Fisher's for counting data\n\ndata:  Point pattern Aegiss\np-value = 0.00344\nalternative hypothesis: The point pattern Aegiss is not spatio-temporal separable\n\nAs the \\(p\\)-value is significant, we opt for estimating the spatio-temporal intensity in a non-separable fashion. We estimate the intensity by employing the following estimator, which uses a Gaussian kernel for all coordinates (Choi and Hall 1999; González et al. 2019; Ghorbani et al. 2021)\n\\[\\begin{equation}\n    \\hat{\\lambda}(\\mathbf{u},v)=\\sum_{i=1}^{n}\\frac{\\kappa_{\\epsilon}^{2}(\\mathbf{u}-\\mathbf{u}_i)\\kappa_{\\delta}^{1}(v-v_i)}{e^2(\\mathbf{u}_i)e^1(v_i)},\n    \\tag{6}\n\\end{equation}\\]\nwhere\n\\[\ne^2(\\mathbf{u}_i)=\\int_{W} \\kappa_{\\epsilon}^{2}(\\mathbf{u}_i-\\mathbf{s})\\text{d} \\mathbf{s}, \\quad \\text{and}\\quad e^1(v_i)=\\int_{T} \\kappa_{\\delta}^{1}(v_i-w)\\text{d} w;\n\\]\nand where \\(\\kappa_{\\epsilon}^{2}\\) and \\(\\kappa_{\\delta}^{1}\\) are the two- and one-dimensional Gaussian kernels with bandwidths \\(\\epsilon\\) and \\(\\delta\\).\nIn this case, we use a fixed spatial bandwidth calculated by a rule of thumb that depends on the short size of the window and a temporal one calculated through the function bw.nrd(). Notice that the spatio-temporal estimator given in Eq.(6) resembles the spatial one given in Eq.(1), with an extra kernel weight for the temporal component. On the other hand, as the spatial edge correction, due to Diggle (1985), is already implemented in the function density.ppp() of spatstat.explore, we must calculate the temporal one. We employ the generic function bw.nrd0() of package stats for the temporal bandwidth, which uses Silverman’s rule-of-thumb (Silverman 1986) for choosing the bandwidth of a Gaussian kernel density estimator. For computing the edge correction, we take advantage of the properties of the normal distribution noticing that \\(e^1(v_i)=\\mathbb{P}(Y_i \\in T)\\) where \\(Y_i \\sim N(v_i, \\delta^2)\\), indeed,\n\n\nTimes <- Aegiss$marks\nbwt <- bw.nrd0(Times)\nedgewt.t <- pnorm((max(Times) - Times) / bwt) - pnorm((min(Times)- Times) / bwt) \n\n\nThen we proceed to calculate the non-separable estimator. To quicken the computation, for each fixed time \\(t_0\\), we take temporal bins \\(t\\in [t_0-3\\delta, t_0+3\\delta]\\), as the normal distribution practically vanishes at larger or smaller values, then we compute the temporal kernel values and attach them to the spatial kernel as weights,\n\n\n# This takes roughly 44 seconds to be executed \nnonseparable <- function(time){\n  contrib <- (Times >= time - 3 * bwt) & (Times <= time + 3 * bwt)\n  Wh <- dnorm(x = Times[contrib], mean = time, sd = bwt) / edgewt.t[contrib]\n  density.ppp(X[contrib], weights = Wh, diggle = TRUE)\n}\nnonsep <- lapply(unique(Times), nonseparable)\n\n\nWe have 1047 intensity estimates corresponding to each of the 1047 different times; note that 1047 is the size of the temporal coordinates set and not the maximum temporal value (1095). We decide to plot a subset of them, namely the estimates corresponding to times \\(01/01/2001\\), \\(01/01/2002\\), \\(01/01/2003\\) and \\(12/31/2003\\).\n\n\nn.slices <- which(unique(timelabels) %in% as.Date(c(\"2001-01-01\", \"2002-01-01\", \n                                                    \"2003-01-01\", \"2003-12-31\")))\nSnap <- list(nonsep[[n.slices[1]]], nonsep[[n.slices[2]]], \n             nonsep[[n.slices[3]]], nonsep[[n.slices[4]]])\nplot.imlist(Snap, equal.ribbon = T, ncols = 4, box = F, main = \"\", log = F,\n            main.panel = unique(timelabels)[n.slices], col = viridis(1200),\n            ribscale = 100,  mar.panel=c(0, 0, 1, 1), panel.end = X$window)\n\n\n\nFigure 4: Temporal snapshots of the intensity \\(\\hat{\\lambda}(\\mathbf{u},v)\\), of non-specific gastrointestinal reports in Hampshire, estimated in a non-separable way through Gaussian kernels. The units are rescaled by \\(100\\).\n\n\n\nIn the spatio-temporal case (Figure 4), we can see a behaviour in the intensity of cases much lower than in the spatial case (Figure 2 (left)) due to the temporal granularity. This scale difference is due to the number of events per unit of time, which is not considered in the only spatial estimation. When considering the temporal dimension, the total points must be divided by the number of bins in the temporal grid where the estimation is made. Therefore, there are fewer points per unit area at each time than only when spatial coordinates are considered.\nThe intensity maintains some urban centres as foci over time, especially those in the south, for example, in Portsmouth, where there is a high concentration of cases between 2002 and 2003.\nIn the next section, we show how to compute spatio-temporal descriptors. To do that, we need to know the intensity values at every point of the point pattern. We can extract the intensity values by using the safelookup() of spatstat.geom. Then, we attach those values, and the dates, as a mark to each point by using the operator %mark%,\n\n\nlambda <- NULL\nfor (i in 1:length(nonsep)) {\n  lambda <- c(lambda, safelookup(nonsep[[i]], \n  Aegiss[marks(Aegiss) == unique(Times)[i]]))\n}\nPP <- X %mark% data.frame(time = Aegiss$marks, Lambda = lambda)\n\n\nSpatio-temporal relative risk\nThe concept of relative risk straightforwardly extends to the spatio-temporal domain. In this regard, we must make additional considerations; for example, the spatial relative risk can be conditioned to a specific time (Davies et al. 2018). Analogous to the classic probability, in the spatio-temporal context, the idea of conditioning on a specific time \\(v=t_0\\) comes naturally. This means that for a fixed instant \\(t_0\\), we consider the spatial density function given that time. Mathematically, it is calculated by normalising the spatio-temporal density by the density of the temporal coordinates evaluated at the fixed time \\(t_0\\). On the other hand, if it is not conditioned to a particular time, the risk function is a function of three variables (two spatial coordinates and one temporal), which can be drawn through snapshots. Thus, we have two possible risk functions, \\(r(\\mathbf{u},v)\\) (if plotted at a fixed time, it represents an unnormalised slice of the three-dimensional function) and \\(r(\\mathbf{u}|v=t_0)\\) (a spatial-only snapshot of relative risk).\nGiven that we have population information for 2001, 2002 and 2003, we generate controls from these three population densities considering uniform times within each year. Then, we superimpose the three control point patterns into one using the superimpose() function of the spatstat.geom package. We obtain a spatio-temporal point pattern of controls that will serve as input to calculate the denominator of the relative risk.\n\n\nNi <- lapply(Population, integral.im)\nni <- lapply(split(Aegiss, cut(Aegiss$marks, breaks = c(0, 365, 730, 1095))), \n             npoints)\nControlDensi <- mapply(function(D, N, n) {eval.im((D / N) * n)}, \n                       D = Population, N = Ni, n = ni, SIMPLIFY = F)\nControlsi <- mapply(function(L, Breaks){\n  PP <- rpoispp(lambda = L)\n  marks(PP) <- sample(x = (Breaks[1] + 1):Breaks[2], \n                      size = npoints(PP), replace = T)\n  return(PP)}, \n  L = ControlDensi, Breaks = list(c(0, 365), c(365, 730), c(730, 1095)), \n  SIMPLIFY = F)\n\nControlsi <- superimpose(as.solist(Controlsi))\nControlsi <- unmark(Controlsi) %mark% Controlsi$marks$origMarks\nWindow(Controlsi) <- Window(Aegiss)\n\n\nFor the spatio-temporal risk estimation exercise, we return to the sparr package. We follow the recommendations of (Davies et al. 2018) and set oversmoothing bandwidths (a rule based on an optimal bandwidth upper bound for the asymptotic mean integrated squared error of the density estimate, this may be done using the command OS.spattemp()). The spatio-temporal densities can analysed through the command spattemp.density() of the sparr package.\n\n\n# This takes roughly 4.87 minutes to be executed\nBr <- OS.spattemp(Aegiss)\nf.num <- spattemp.density(Aegiss, h = Br[1], lambda = Br[2])\nd.den <- spattemp.density(Controlsi, h = Br[1], lambda = Br[2])\n\n\nIn this case, interest may rise about the anomalous regions of spatio-temporal risk considering joint and conditional risks (given a time \\(t\\)); i.e., we are interested in testing the null hypotheses \\(H_0:r(\\mathbf{u},v)=0\\) or \\(H_0:r(\\mathbf{u},|v=t)=0\\) against the alternatives of high risks. We employ the spattemp.risk() function of sparr to obtain a spatiotemporal relative risk surface based on the ratio of the two kernel estimates. In addition, since doing Monte Carlo simulations to obtain \\(p\\)-values is very complicated from the computational point of view, asymptotic approximations are implemented in sparr by setting tolerate = T,\n\n\n# This takes roughly 5.47 minutes to be executed\nst.risk <- spattemp.risk(f = f.num, g = d.den, tolerate = T)\n\n\nWe proceed to extract the unconditional and conditional risk slices,\n\n\nrisk.slices <- spattemp.slice(st.risk, tt = n.slices)\n\n\nThe variable risk.slices contains a list of lists of images, each of which corresponds to the requested times in n.slices and has the unconditional and conditional risk estimates and the \\(p\\)-values surfaces. We can plot the image estimates as follows:\n\n\nplot.imlist(c(risk.slices$rr, risk.slices$rr.cond), equal.ribbon = T, ncols = 4, \n            box = F, main = \"\", main.panel = c(timelabels[n.slices], rep(\"\", 4)), \n            mar.panel = c(0, 0, 1, 1), col = rocket(1200), ribmar = c(1, 3, 1, 2), \n            ribwid = 0.4, panel.end = function(i,...){\n              contour(c(risk.slices$P, risk.slices$P.cond)[[i]], \n                      levels = 0.05, drawlabels = TRUE, ...)\n              plot(X$window, add = T)})\n\n\n\nFigure 5: Temporal snapshots of the unconditional risk \\(\\hat{r}(\\mathbf{u},v)\\) (top) and conditional risk \\(\\hat{r}(\\mathbf{u}|v= t)\\) (where \\(t\\) is \\(01/01/2001\\), \\(01/01/2002\\), \\(01/01/2003\\) and \\(12/31/2003\\)) (bottom) of non-specific gastrointestinal disease in Hampshire. Solid lines delineate a statistical significant elevated risk at the \\(5\\%\\) level.\n\n\n\nIn Figure 5, we can see how the high significant risk remains in the central Hampshire region. Although the two panels (top and bottom) look similar, there are differences. For example, in February 2001, we see how the conditional risk shows a larger statistically high area than its unconditional counterpart. We also see that the greatest risk is not present in the most densely populated areas (in this case, the south-central part of the region, where the capital is located).\nSecond-order descriptors\nPair correlation and \\(K\\)-functions\nLet \\(\\xi_1 = (\\mathbf{u}_1,v_1),\\xi_2 = (\\mathbf{u}_2,v_2)\\) be two points of \\(W\\times T\\). The second-order intensity function \\((\\lambda^{(2)}(\\xi_1,\\xi_2), \\xi_1,\\xi_2\\in W\\times T)\\) and the pair correlation function are defined in the spatio-temporal context, in the very same way as the spatial ones. For CSTR point processes, the pair correlation function takes the value of one. Values other than these benchmarks indicate how likely a pair of “extra” points will appear in a CSTR process with the same intensity at the specified locations; i.e., larger values indicate clustering, and smaller values indicate regularity. The pair correlation (Gabriel and Diggle 2009; Møller 2012; González et al. 2016) is defined as\n\\[\\begin{equation}\n    g(\\xi_1,\\xi_2)=\\frac{\\lambda^{(2)}(\\xi_1,\\xi_2)}{\\lambda(\\xi_1)\\lambda(\\xi_2)}, \\qquad \\xi_1,\\xi_2\\in W\\times T.\n    \\tag{7}\n\\end{equation}\\]\nSecond-order intensity reweighted stationarity (SOIRS) and isotropy are defined in the same vein as the spatial counterparts. For the estimation of the pair correlation function, consider a point pattern \\(X=\\{(\\mathbf{u}_{i},v_i)\\}_{i=1}^n\\) with \\(n\\) points, the spatio-temporal pair correlation function (Eq. (7)) may be estimated by\n\\[\n    \\hat{g}(r,t)=\\frac{1}{4\\pi r}\n    \\sum_{i=1}^n \\sum_{j\\neq i}\n    \\frac{\\kappa_{\\epsilon}(\\|\\mathbf{u}_{i}-\\mathbf{u}_{j}\\|- r)\\kappa'_{\\delta}(|v_{i}-v_{j}|-t)}{\\hat{\\lambda} \\left( \\mathbf{u}_{i},v_{i}\\right) \\hat{\\lambda} \\left(\\mathbf{u}_{j},v_{j}\\right) w_{ij}\n    }, \\quad r \\in [\\epsilon, r_0], t\\in [\\delta, t_0],\n\\]\nwhere \\(\\kappa_{\\epsilon}\\) and \\(\\kappa'_{\\delta}\\) are one-dimensional kernel functions with spatial and temporal bandwidths \\(\\epsilon\\) and \\(\\delta\\), respectively; \\(w_{ij}\\) represent edge-correction weights (González et al. 2016). Selecting the bandwidths is often complicated because the estimation depends on those parameters, which means a wrong decision could be disastrous. As the second-order descriptors are distance-based, we can take advantage of some techniques for bandwidth selection. For example, for the temporal kernel, we use the dpik() function from the KernSmooth package (González and Moraga 2022b), which uses the method proposed by (Sheather and Jones 1991) to estimate a suitable bandwidth,\n\n\nlibrary(KernSmooth)\ndt <- dist(unique(Times))\nht <- dpik(dt, kernel = \"epanech\", gridsize = 50, scalest = \"iqr\")\nht <- 2 * ht\n\n\nNote that we set Epanechnikov’s kernel as a default because the kernel selection is not as important as the bandwidth. We get a temporal bandwidth of \\(13.5\\) days (the output of the dpik() function). For the spatial bandwidth and upper bound \\(r_0\\), we use the same as obtained for the spatial summary functions. For the temporal upper bound \\(t_0\\), we set the 15% of the maximum temporal distances,\n\n\nt0 <- 0.15 * max(dt)\n\n\nWe use the stpp package to estimate the descriptor. We first create data in spatio-temporal point format using the command as.3dpoints(); then, we create a two-column matrix to specify the polygonal region containing the points and set the spatial and temporal domains for the descriptor. Finally, we use the PCFhat() function to compute the estimate of the pair correlation function,\n\n\nlibrary(stpp)\nFMD <- as.3dpoints(PP$x, PP$y, PP$marks$time)\ns.region <- as.matrix(data.frame(x = PP$window$bdry[[1]]$x, \n                                 y = PP$window$bdry[[1]]$y))\nhs <- bwG\nu1 <- seq(hs, r0, length.out = 71)[-1]\nv1 <- seq(ht, t0, length.out = 71)[-1]\n\n\n\n\n# Time consuming: This takes roughly 3.87 hours to be executed\ng <- PCFhat(xyt = FMD, s.region = s.region, t.region = range(Times), \n            lambda = lambda, dist = u1, times = v1, ks = \"epanech\", \n            kt = \"epanech\", hs = hs, ht = ht)\n\n\nTo visualise the descriptor, we use the persp3D() function of the plot3D package,\n\n\nlibrary(plot3D)\npar(mar = c(0,0,0,0))\npersp3D(x = u1, y = v1, z = g$pcf, facets = NA, curtain = F, col =  viridis(200), \n  colkey = F, bty = \"g\", pch = 20, cex = 1.5, theta = 40, phi = 5, \n  border = NA, ticktype = \"detailed\", cex.axis = 0.5,  zlab = \"\",\n  xlab = \"spatial distances\", ylab = \"temporal distances\")\n\n\n\nFigure 6: Spatio-temporal pair correlation function estimate of non-specific gastrointestinal disease data.\n\n\n\nThe \\(\\hat{g}(r,t)\\) surface, shown in Figure 6, describes the spatio-temporal interaction structure of the disease; it shows clustering (\\(\\hat{g}(r,t)\\geq 1\\)) for all the distances at all times with the appearance of high clustering at small distances (up to \\(5\\)km) that moves towards complete spatio-temporal randomness (CSTR) for more considerable distances. We then can suspect from this behaviour that the point pattern is only clustered in the spatial dimension, i.e., the interaction between gastrointestinal reports increases as long as they are close in space, but the clustering in time seems uniform along with the selected periods. We can also conclude that while the first-order intensity is not separable, perhaps the second-order one is. Note that the bandwidth selection explains the waves of the pair correlation function in the temporal dimension; narrow bandwidths produce spurious bumps in the estimate.\nOne of the most used second-order descriptor is the inhomogeneous spatio-temporal \\(K\\)-function Møller (2012). Under the SOIRS assumption, if \\(g(r,t)\\) is the pair correlation function, then the \\(K\\)-function is defined as\n\\[\n    K(r,t) = \\int_{\\mathbb{R}^2}\\int_{\\mathbb{R}} \\mathbf{1}[||\\mathbf{u}||\\leq r, |s|\\leq t] g(||\\mathbf{u}||,|s|)  \\text{d} \\mathbf{u} \\text{d} s, \\quad \\text{for }r>0,t>0.\n\\]\nIn the homogeneous case, \\(K(r,t)\\) is simply the expected number of additional points within distance \\(r\\) and time lag \\(t\\) from the origin, given that the point pattern has a point at the origin. For a CSTR point processes \\(K(r,t)= \\pi r^2 t\\). Considering a spatio-temporal point pattern \\(X\\) again, an estimator of \\(K(r,t)\\) is\n\\[\n    \\hat{K}(r,t)= \\frac{1}{|W\\times T|}\\sum_{i=1}^n \\sum_{j\\neq i}\n    \\frac{\\mathbf{1}\\left[ \\left\\Vert \\mathbf{u}_{i}-\\mathbf{u}_{j}\\right\\Vert \\leq r\\right]\\mathbf{1}\\left[\\left\\vert v_{i}-v_{j}\\right\\vert \\leq t\\right]}{\\hat{\\lambda} \\left( \\mathbf{u}_{i},v_{i}\\right) \\hat{\\lambda} \\left(\\mathbf{u}_{j},v_{j}\\right) w_{ij}}.\n\\]\nThere is an alternative estimator \\(\\hat{K}^*(r,t)\\) matching the original definition of (Gabriel 2014) that into account the past of the process. To compute the \\(K\\)-function we employ the STIKhat() function from the stpp package,\n\n\nu <- seq(0, r0, length.out = 71)\nv <- seq(0, t0, length.out = 71)\n\n\n\n\n# Time consuming: This takes roughly 55 minutes to be executed\nstik <- STIKhat(xyt = FMD, s.region = s.region, t.region = range(Times),\n                lambda = lambda, dist = u, times = v, infectious = F)\n\n\nThe parameter infectious controls the type of estimator, when it is set FALSE, past and future events are considered for the estimation. In this equation, the points \\(\\xi_i=(\\mathbf{u}_i, v_i)\\) are ordered so that \\(v_i<v_{i+1}\\) (ties are broken by random unrounding whenever necessary). To deal with temporal edge-effects, for each \\(t\\in T=[T_0,T_1]\\), \\(n_t\\) is the number of events for which \\(v_i\\leq T_1 - t\\). For spatial edge effects, the STIKhat() function employs Ripley’s method (Gabriel and Diggle 2009). For plotting the spatio-temporal \\(K\\)-function we write,\n\n\nKS <- stik$Khat - stik$Ktheo\npar(mar = c(0,0,0,0))\npersp3D(x = u[-1], y = v[-1], z = KS, facets = NA, zlab = \"\",\n  curtain = F, col =  viridis(100), colkey = F, bty = \"g\", pch = 20, \n  cex = 1.5, theta = 40, phi = 5, border = NA, ticktype = \"detailed\", \n  cex.axis = 0.5, xlab = \"spatial distances\", ylab = \"temporal distances\")\n\n\n\nFigure 7: Spatio-temporal pair correlation function estimate of non-specific gastrointestinal disease data.\n\n\n\nFigure 7 shows the estimated \\(K\\)-function. Since the \\(K\\)-function is centred on its theoretical value \\((2\\pi r^2 t)\\), we can easily interpret it; positive deviations suggest clustering, while negative deviations represent regularity or inhibition. In our case, we can see positive deviations that become larger as the spatial and temporal distances grow together. Therefore, as time passes and spatial distances become larger, the level of clustering also increases in our point pattern, at least in the range we have considered, that is, up to 7km in space and up to 5 months in time.\n4 Discussion\nThroughout this paper, we have shown how to analyse complex spatial and spatio-temporal point patterns through various techniques that aim to capture their first and second-order characteristics using several R packages. Specifically, we have toured very recent techniques such as variable bandwidths, relative risk inference, and composite hypotheses. The spatial methods presented are well-investigated and documented and can be easily applied using R. We have seen that some procedures, such as estimating \\(p\\)-values in composite hypotheses, require severe computational resources. The approximate computing time, including all the routines and set parameters suggested throughout the paper, is approximately six hours in a 3 GHz 6-Core Intel Core i5 iMac computer. This time depends on several factors, such as the number of simulations and the precision of desired outputs. It is mainly due to the number of points in the point pattern; the more data points, the more computational complexity, especially in the case of second-order descriptors, where pairwise calculations are required. Some R packages such as foreach can help accelerate the computations, even though these methods may quickly become unfeasible when the datasets have many points.\nWe have presented the most straightforward methods and the most widely known in the literature; we have done this, above all, because we intended to provide a solid and approachable learning resource. However, there are also alternative methods to deal with some of the descriptors we analyse in this document; for example, González and Moraga (2022a) presents a methodology to accurately and adaptively estimate the spatio-temporal intensity functions; this methodology can be applied using the kernstadapt package.\nThe analyses presented here can benefit future researchers who have initial questions about spatio-temporal point process data; these results can serve, for instance, as input for possible statistical models. We have not delved into statistical modelling and inference since many models exist in the spatio-temporal point processes literature. These statistical models can incorporate current knowledge of the phenomenon and many scientific considerations. Different models can be proposed and compared to decide which variables significantly influence the summary descriptors, making the results more informative (Baddeley et al. 2015). For example, one of the most flexible families of models with the most methodological advantages is the family of log-Gaussian point processes. Log-Gaussian Cox processes are models for point patterns that intend to capture the environmental variability (Diggle et al. 2013). They are spatio-temporal Poisson processes conditional on the realisation \\(\\lambda(\\mathbf{u},v)\\) of a stochastic intensity function \\(\\Lambda(\\mathbf{u},v), (\\mathbf{u},v)\\in \\mathbb{R}^2\\times \\mathbb{R}\\). In the SOIRS case, a convenient representation of the intensity is\n\\[\n    \\log \\left\\{ \\Lambda(\\mathbf{u},v) \\right\\}= \\lambda(\\mathbf{u},v)S(\\mathbf{u},v),\n\\]\nwhere \\(S(\\mathbf{u},v)\\) is a Gaussian stationary process with expectation \\(1\\) and covariance function \\(\\gamma(r,t)=\\sigma^2 s(r,t)\\), where \\(\\sigma^2\\) is the variance of \\(S(\\mathbf{u}, v)\\) and \\(s(\\cdot, \\cdot)\\) is a spatio-temporal correlation function, where \\(r\\) and \\(t\\) are spatial and temporal distances. Inference uses a discretised version of \\(S(\\mathbf{u},v)\\), namely \\(\\mathcal{S}\\), defined on a regular grid, where observations \\(X\\) are the cell counts, and the intensity of the process is assumed constant within each cell (Taylor and Diggle 2014). \\(\\mathcal{S}\\) is a finite collection of Gaussian random variables, so its properties imply a multivariate Gaussian density. The covariance matrix elements are calculated by evaluating \\(\\gamma(r,t)\\) at the centroids of the cells. Predictive inference about \\(\\mathcal{S}\\) requires samples from the conditional distribution of the latent field given the observations, namely \\([\\mathcal{S}|X]\\). For estimating the parameters of a space-time log-Gaussian Cox process, one can choose between computationally feasible methods, such as those based on moments or more demanding methods based on likelihood (Diggle et al. 2013). Inference can be achieved for example, by using Markov chain Monte Carlo (MCMC) for which the lgcp package can be used, and by using integrated nested Laplace approximation (INLA) with the inla and inlabru packages.\nTo conclude, the methods presented here are primarily concerned with understanding the probability distribution that generate points observed in space or time. The study of such a distribution involves many routines, concepts and contexts that can help answer scientific questions and make decisions based on the data or inferences about them. The inclusion of technological advancements in R has led to improved efficiency and speed in calculations. Additionally, R offers a versatile and reliable environment for implementing statistical procedures and creating graphical visualisations. Also, modern developments in spatial statistics related to point patterns have been made possible thanks to the widespread availability of reproducible R scripts for analyses. In this paper, we have made extensive use of the ability to apply, extend and modify procedures in R to analyse spatial and spatio-temporal point patterns in the context of epidemiology. These approaches are also helpful for analysing point pattern data that arise in various fields such as climatology, ecology or seismology.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-025.zip\nCRAN packages used\nspatial, splancs, sp, sf, spatstat, spatstat.utils, spatstat.sparse, spatstat.data, spatstat.geom, spatstat.random, spatstat.explore, spatstat.model, spatstat.linnet, smacpod, sparr, spatgraphs, stpp, PtProcess, lgcp, inlabru, GET, foreach, doParallel, viridis, ggplot2, kernstadapt, KernSmooth, plot3D\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, MixedModels, Phylogenetics, Spatial, SpatioTemporal, Survival, TeachingStatistics\n\n\nF. E. Bachl, F. Lindgren, D. L. Borchers and J. B. Illian. inlabru: An R package for Bayesian spatial modelling from ecological survey data. Methods in Ecology and Evolution, 10(6): 760–766, 2019.\n\n\nA. Baddeley, A. Hardegen, T. Lawrence, R. K. Milne, G. Nair and S. Rakshit. On two-stage Monte Carlo tests of composite hypotheses. Computational Statistics & Data Analysis, 114: 75–87, 2017.\n\n\nA. Baddeley, E. Rubak and R. Turner. Spatial point patterns: Methodology and applications with r. CRC Press, Boca Raton, Florida, 2015.\n\n\nA. Baddeley and R. Turner. spatstat: An R package for analyzing spatial point patterns. Journal of Statistical Software, 12(6): 1–42, 2005.\n\n\nJ. Besag. Contribution to the discussion of dr Ripley’s paper. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 39: 193–195, 1977.\n\n\nR. S. Bivand, E. Pebesma and V. Gómez-Rubio. Applied spatial data analysis with R. Second Springer, New York, 2013.\n\n\nE. Choi and P. Hall. Nonparametric approach to analysis of space-time data on earthquake occurrences. Journal of Computational and Graphical Statistics, 8: 733–748, 1999.\n\n\nD. Daley and D. Vere-Jones. An introduction to the theory of point processes: Volume i: Elementary theory and methods. Second Springer-Verlag, New York, 2003.\n\n\nT. M. Davies and A. Baddeley. Fast computation of spatially adaptive kernel estimates. Statistics and Computing, 28(4): 937–956, 2018.\n\n\nT. M. Davies, J. C. Marshall and M. L. Hazelton. Tutorial on kernel estimation of continuous spatial and spatiotemporal relative risk. Statistics in Medicine, 37(7): 1191–1221, 2018.\n\n\nP. J. Diggle. A kernel method for smoothing point process data. Journal of the Royal Statistical Society: Series C (Applied Statistics), 34(2): 138–147, 1985.\n\n\nP. J. Diggle. Statistical analysis of spatial and spatio-temporal point patterns. Third CRC Press, Boca Raton, Florida, 2013.\n\n\nP. J. Diggle, P. Moraga, B. Rowlingson and B. Taylor. Spatial and spatio-temporal log-Gaussian Cox processes: Extending the geostatistical paradigm. Statistical Science, 28: 542–563, 2013.\n\n\nP. Diggle, L. Knorr-Held, R. B, T. Su, P. Hawtin and T. Bryant. On-line monitoring of public health surveillance data. In Monitoring the health of populations: Statistical principlesand methods for public health surveillance, Eds B. R. and S. D. F. pages. 233–266 2003. Oxford University Press.\n\n\nE. Gabriel. Estimating second-order characteristics of inhomogeneous spatio-temporal point processes. Methodology and Computing in Applied Probability, 16: 411–431, 2014.\n\n\nE. Gabriel and P. J. Diggle. Second-order analysis of inhomogeneous spatio-temporal point process data. Statistica Neerlandica, 63(1): 43–51, 2009.\n\n\nE. Gabriel, B. Rowlingson and P. J. Diggle. stpp: An R package for plotting, simulating and analyzing spatio-temporal point patterns. Journal of Statistical Software, 53(2)(2): 1–29, 2013.\n\n\nS. Garnier, Simon, N. Ross, B. Rudis, M. Sciaini, A. Pedro and C. Scherer. viridis(Lite) - colorblind-friendly color maps for R. 2023. URL https://sjmgarnier.github.io/viridis/. viridis package version 0.6.3.\n\n\nM. Ghorbani, N. Vafaei, J. Dvořák and M. Myllymäki. Testing the first-order separability hypothesis for spatio-temporal point patterns. Computational Statistics & Data Analysis, 161: 107245, 2021.\n\n\nJ. A. González, U. Hahn and J. Mateu. Analysis of tornado reports through replicated spatiotemporal point patterns. Journal of the Royal Statistical Society: Series C (Applied Statistics), 69(1): 3–23, 2019.\n\n\nJ. A. González and P. Moraga. An adaptive kernel estimator for the intensity function of spatio-temporal point processes. 2022a. DOI 10.48550/ARXIV.2208.12026.\n\n\nJ. A. González and P. Moraga. kernstadapt: Spatio-temporal adaptive kernel estimators for intensities. 2022b. URL https://CRAN.R-project.org/package=kernstadapt. R package version 0.0.2.\n\n\nJ. A. González, F. J. Rodríguez-Cortés, O. Cronie and J. Mateu. Spatio-temporal point process statistics: A review. Spatial Statistics, 18: 505–544, 2016.\n\n\nD. Harte. PtProcess: An R package for modelling marked point processes indexed by time. Journal of Statistical Software, 35(8): 1–32, 2010.\n\n\nM. L. Hazelton and T. M. Davies. Inference based on kernel estimates of the relative risk function in geographical epidemiology. Biometrical Journal, 51(1): 98–109, 2009.\n\n\nJ. Illian, P. A. Penttinen, H. Stoyan and D. Stoyan. Statistical analysis and modelling of spatial point patterns. Wiley, 2008.\n\n\nA. Jalilian and R. Waagepetersen. Fast bandwidth selection for estimation of the pair correlation function. Journal of Statistical Computation and Simulation, 88(10): 2001–2011, 2018.\n\n\nJ. E. Kelsall and P. J. Diggle. Kernel estimation of relative risk. Bernoulli, 1(1/2): 3–16, 1995.\n\n\nMicrosoft and S. Weston. doParallel: Foreach parallel adaptor for the ’parallel’ package. 2022a. URL https://CRAN.R-project.org/package=doParallel. R package version 1.0.17.\n\n\nMicrosoft and S. Weston. foreach: Provides foreach looping construct. 2022b. URL https://CRAN.R-project.org/package=foreach. R package version 1.5.2.\n\n\nM. Møller J. And Ghorbani. Aspects of second-order analysis of structured inhomogeneous spatio-temporal point processes. Statistica Neerlandica, 66: 472–491, 2012.\n\n\nP. Moraga. Geospatial health data: Modeling and visualization with R-INLA and shiny. Chapman & Hall/CRC, Boca Raton, Florida, 2019.\n\n\nP. Moraga. Species distribution modeling using spatial point processes: A case study of sloth occurrence in Costa Rica. The R Journal, 12(2): 293–310, 2021.\n\n\nP. Moraga and F. Montes. Detection of spatial disease clusters with LISA functions. Statistics in Medicine, 30(10): 1057–1071, 2011.\n\n\nM. Myllymäki and T. Mrkvička. GET: Global envelopes in R. arXiv:1911.06583 [stat.ME], 2020. URL https://arxiv.org/abs/1911.06583.\n\n\nM. Myllymäki, T. Mrkvička, P. Grabarnik, H. Seijo and U. Hahn. Global envelope tests for spatial processes. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(2): 381–404, 2017.\n\n\nE. Pebesma. Simple features for R: Standardized support for spatial vector data. The R Journal, 10(1): 439–446, 2018.\n\n\nE. J. Pebesma and R. S. B. Classes and methods for spatial data in R. R News, 5(2): 9–13, 2005.\n\n\nR Core Team. R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing, 2023. URL https://www.R-project.org.\n\n\nB. D. Ripley. Modelling spatial patterns (with discussion). Journal of the Royal Statistical Society: Series B (Statistical Methodology), 39(2): 172–212, 1977.\n\n\nA. Rodrigues and P. J. Diggle. Bayesian estimation and prediction for inhomogeneous spatiotemporal log-Gaussian Cox processes using low-rank models, with application to criminal surveillance. Journal of the American Statistical Association, 107(497): 93–101, 2012.\n\n\nB. S. Rowlingson and P. J. Diggle. splancs: Spatial point pattern analysis code in S-PLUS. Computers & Geosciences, 19(5): 627–655, 1993.\n\n\nH. Rue, S. Martino and N. Chopin. Approximate bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(2): 319–392, 2009.\n\n\nD. W. Scott. Multivariate density estimation: Theory, practice, and visualization. Second John Wiley & Sons, 2015.\n\n\nS. J. Sheather and M. C. Jones. A reliable data-based bandwidth selection method for kernel density estimation. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 53(3): 683–690, 1991.\n\n\nB. W. Silverman. Density estimation for statistics and data analysis. CRC press, Boca Raton, Florida, 1986.\n\n\nB. M. Taylor, T. M. Davies, B. S. Rowlingson and P. J. Diggle. Lgcp: An R package for inference with spatial and spatio-temporal log-Gaussian Cox processes. Journal of Statistical Software, 52: 1–40, 2013.\n\n\nB. Taylor and P. Diggle. INLA or MCMC? A tutorial and comparative evaluation for spatial prediction in log-Gaussian Cox processes. Journal of Statistical Computation and Simulation, 84(10): 2266–2284, 2014.\n\n\nR. Turner. Point patterns of forest fire locations. Environmental and Ecological Statistics, 16(2): 197–223, 2009.\n\n\nW. N. Venables and B. D. Ripley. Modern applied statistics with s. Fourth Springer-Verlag, New York, 2002.\n\n\nH. Wässle, B. B. Boycott and R.-B. Illing. Morphology and mosaic of on-and off-beta cells in the cat retina and some functional considerations. Proceedings of the Royal Society of London. Series B. Biological Sciences, 212(1187): 177–195, 1981.\n\n\nH. Wickham. ggplot2: Elegant graphics for data analysis. Second Springer-Verlag, New York, 2016.\n\n\nK. Y. Wong and D. Stoyan. Poles of pair correlation functions: When they are real? Annals of the Institute of Statistical Mathematics, 73: 425–440, 2021.\n\n\n\n\n",
    "preview": "articles/RJ-2023-025/distill-preview.png",
    "last_modified": "2023-11-07T21:31:41+00:00",
    "input_file": {},
    "preview_width": 1824,
    "preview_height": 576
  },
  {
    "path": "articles/RJ-2023-026/",
    "title": "nlmeVPC: Visual Model Diagnosis for the Nonlinear Mixed Effect Model",
    "description": "A nonlinear mixed effects model is useful when the data are repeatedly measured within the same unit or correlated between units. Such models are widely used in medicine, disease mechanics, pharmacology, ecology, social science, psychology, etc. After fitting the nonlinear mixed effect model, model diagnostics are essential for verifying that the results are reliable. The visual predictive check (VPC) has recently been highlighted as a visual diagnostic tool for pharmacometric models. This method can also be applied to general nonlinear mixed effects models. However, functions for VPCs in existing R packages are specialized for pharmacometric model diagnosis, and are not suitable for general nonlinear mixed effect models. In this paper, we propose nlmeVPC, an R package for the visual diagnosis of various nonlinear mixed effect models. The nlmeVPC package allows for more diverse model diagnostics, including visual diagnostic tools that extend the concept of VPCs along with the capabilities of existing R packages.",
    "author": [
      {
        "name": "Eun-Hwa Kang",
        "url": {}
      },
      {
        "name": "Myungji Ko",
        "url": {}
      },
      {
        "name": "Eun-Kyung Lee",
        "url": {}
      }
    ],
    "date": "2023-08-26",
    "categories": [],
    "contents": "\n1 Introduction\nAfter fitting a model, diagnosing the fitted model is essential for verifying that the results are reliable (Nguyen et al. 2017). For linear models, the residuals are usually used to determine the goodness of fit of the fitted model. However, due to random effects and nonlinearity, residuals are less useful for diagnosing fit in nonlinear mixed effects models. Therefore, various diagnostic tools for this type of model have been developed. The nonlinear mixed effect model is useful when the data are repeatedly measured within the same unit, or the relationship between the dependent and independent variables is nonlinear. It is widely used in various fields, including medicine, disease mechanics, pharmacology, ecology, pharmacometrics, social science, and psychology (Pinheiro and Bates 2000; Davidian 2017). Recently, among the various diagnostic tools applicable to nonlinear mixed models, simulation-based diagnostic methods have been developed in the field of pharmacology (Karlsson and Savic 2007). The visual predictive check (VPC; Karlsson and Holford (2008)) is a critical diagnostic tool that visually tests for the adequacy of the fitted model. It allows for the diagnosis of fixed and random effects in mixed models (Karlsson and Holford 2008) in the original data space.\nCurrently, it is a widely used method for diagnosing population pharmacometric models:\nHeus et al. (2022) used the VPC method to evaluate their population pharmacokinetic models of vancomycin, Mudde et al. (2022) checked their final population PK model for each regimen of antituberculosis drug using the VPC method, and Otto et al. (2021) compares the predictive performance of parent-metabolite models of (S)-ketamine with the VPC method. This VPC method can be used for general nonlinear mixed effect models, including hierarchical models.\nThe psn (Lindbom et al. 2004) and xpose4 (Keizer et al. 2013) packages provide various diagnostic methods for pharmacometric models in R (R Core Team 2023), including the VPC plot. These packages were developed only for the pharmacometricians, and the users needed to use the NONMEM software (Bauer 2011) for generating inputs of functions in these two packages. However, NONMEM is licensed software, and it is mainly designed towards the analysis of population pharmacometric models. Therefore, it is not easy for nonpharmacometrician to use NONMEM and to draw the VPC plot through psn and xpose4. Recently, the vpc (Keizer 2021) package and nlmixr2 (Fidler et al. 2019) package have been developed to draw VPC plots in R without results from NONMEM. However, vpc package provides the function for drawing the original VPC plot only. nlmixr2 was developed initially for fitting general dynamic models. To check the fitted model, nlmixr2 provides a function to use graphical diagnostics in xpose4 with the nlmixr2 model object. Also, nlmixr2 uses the VPC plot in the vpc package with the nlmixr2 model object. Therefore, both packages only provide a function to draw the basic VPC plot, and the other newly developed simulation-based methods, including extensions of VPC, are not provided.\nWe have developed a new R package, nlmeVPC, to provide a suite for various visual checking methods for the nonlinear mixed models. This package includes various state-of-the-art model diagnostic methods based on the visual comparison between the original and simulated data. Most methods compare the statistics calculated from the observed data to the statistics from the simulated data. Percentiles, for example, the \\(10^{th}\\), \\(50^{th}\\), and \\(90^{th}\\) percentiles, are widely used to summarize relevant statistics from the observed and simulated data. We compare the similarities between the statistics from the observed data and those from the simulated data in two different spaces: the data space and the model space (Wickham et al. 2015). The original data comprise the data space. Usually, the data space is represented by the independent and dependent variables, such as time and blood concentration, in the pharmacokinetic data. On the other hand, the model space is composed of quantities obtained from the fitted model, for example, residuals and summary values from the fitted model. From this viewpoint, we categorize the well-known visual diagnostic tools into two categories. One method compares the observed and simulated data in the original data space, and the other in the model space. In the method with the data space, we developed functions for the original VPC (VPCgraph), the additive quantile regression VPC (aqrVPC), and the bootstrap VPC (bootVPC). In addition, we proposed a new VPC method: the average shifted VPC (asVPC). In the method with the model space, the coverage plot (coverageplot) and the quantified VPC (quantVPC) are included. For a more detailed diagnosis, we developed a coverage detailed plot (coverageDetailplot). In nlmeVPC, the ggplot2 package (Wickham 2016) is used to create all plots.\n2 Nonlinear mixed effect model\nThe general nonlinear mixed effect model is defined as follow:\n\\[\ny_{ij} = f(x_{ij}, \\theta, \\eta_i, \\epsilon_{ij}) \\\\\n\\eta_i \\sim N(0,\\Omega) \\nonumber\\\\\n\\epsilon_{ij}  \\sim N(0, \\Sigma)\\nonumber\n\\]\nwhere \\(y_{ij}\\) is the dependent variable of the \\(j^{th}\\) observation of the \\(i^{th}\\) individual, \\(x_{ij}\\) is the independent variable, \\(f\\) is the nonlinear function of \\(x_{ij}\\), \\(\\theta\\) is the population parameter, \\(\\eta_i\\) represents the variability of the individual \\(i\\), and \\(\\epsilon_{ij}\\) represents the random error. From the data, \\(\\theta\\) , \\(\\Omega\\), and \\(\\Sigma\\) are estimated. For the simulated data, the fitted model \\(y_{ij} = f(x_{ij}, \\hat{\\theta}, \\eta_i, \\epsilon_{ij})\\), \\(\\eta_i \\sim N(0,\\hat\\Omega)\\), \\(\\epsilon_{ij} \\sim N(0, \\hat\\Sigma)\\) are used.\n3 Validation in the data space\nThe VPC is the most popular model validation method in the pharmacometrics area. It was developed to diagnose population pharmacokinetic/pharmacodynamic models visually.\nThe main idea of the VPC is to compare the distribution of observations and the distribution of predicted values, where the distribution of predicted values is obtained from simulated data drawn from the fitted model. If the fitted model explains the observed data well, these two distributions should be similar.\nBoth distributions can be represented in the original data space that consists X axis as the independent variable and Y axis as the dependent variable.\nIt allows us to compare the observed data with the fitted model in the original data space. In nlmeVPC, we include an original VPC plot, an additive quantile regression VPC (Jamsen et al. 2018), and a bootstrap VPC (Post et al. 2008). We also proposed a new approach to draw the VPC: the average shifted VPC.\nVisual Predictive Check\nThe visual predictive check (VPC; Karlsson and Holford (2008)) is based on the principle that if the fitted model adequately describes the observed data, the distribution of the simulated data from the fitted model should be similar to the distribution of the observed data. There are several ways to compare the similarities between the distributions. In the VPC approach, profiles of quantiles are used. Two profiles are mainly used to compare the distributions of observations and predictions. One profile is from the upper bound of the prediction intervals, and the other is from the lower bound. These prediction intervals are calculated from the simulated data. 90\\(\\%\\) prediction intervals are usually used. For small and sparse samples, 80\\(\\%\\) prediction interval is also used. The lower and upper bounds of 80\\(\\%\\) prediction interval are the \\(10^{th}\\) and \\(90^{th}\\) percentiles of the simulated data. Figure 1(A) shows the “scatter” type of the VPC plot. Dots indicate the observed data. Two dashed blue lines represent profiles of the \\(10^{th}\\) and \\(90^{th}\\) percentiles of the simulated data, and the solid blue line represents the \\(50^{th}\\) percentile. If the fitted model represents the observed data well, most observed data should lie between profiles of \\(10^{th}\\) and \\(90^{th}\\) percentiles.\nFigure 1(B) is the “percentile” type of the VPC plot. In this plot, profiles of percentiles from the observed data are compared to profiles of percentiles from the simulated data. Two dashed red lines represent profiles of the \\(10^{th}\\) and \\(90^{th}\\) percentiles of the observed data, and the solid red line represents profiles of the \\(50^{th}\\) percentile of the observed data. If the fitted model represents the observed data well, two profiles in each percentile - one from the original data and the other from the simulated data - are similar.\nFigure 1(C) is the “CI” type of the VPC plot. The solid red line represents the \\(50^{th}\\) percentile of the observed data, and dashed red lines represent the \\(10^{th}\\) and \\(90^{th}\\) percentiles of the observed data. Light blue areas represent the 95\\(\\%\\) confidence areas of the \\(10^{th}\\) and \\(90^{th}\\) percentiles, and pink areas represent the 95\\(\\%\\) confidence areas of the \\(50^{th}\\) percentile. These confidence areas were calculated from the simulated data. After calculating percentiles in each simulated data, we find 95\\(\\%\\) confidence intervals for each percentile and use this to draw the areas. In this plot, it is necessary to verify that the profiles of the original data are in confidence areas of each profile from the simulated data in each percentile. If each percentile line of the observed data is in the corresponding confidence area, this can be evidence\nthat the fitted model represents the observed data quite well. Otherwise, the fitted model needs to be improved. The “CI” type of the VPC plot is the most widely used type in pharmacometrics.\nThe percentiles of the dependent variable are calculated in each bin. To estimate the percentiles accurately, enough data points need to lie within each bin. No binning means that the number of bins is equal to the number of different independent variable values. In this case, the VPC plot shows all details of the relationship between the independent and dependent variables. However, the resulting lines become too irregular to show meaningful trends or patterns.\nAs the number of bins decreases, the lines become smoother and more regular, however this can come at the loss of information if too much smoothing is used. Therefore, the selection of the best number of bins is crucial. The way of determining cutoffs for bin also plays an important role. Lavielle and Bleakley (2011) proposed a procedure for finding the optimal bin configuration automatically, including the optimal number of bins and the optimal bin cutoffs.\nVPCgraph provides the automatic binning with optK and makeCOVbin; here, optK finds the optimal number of bins, and makeCOVbin finds the optimal cutoffs of bins using Lavielle and Bleakley’s method.\n\n\n\nFigure 1: The visual predictive check plot. The solid red line represents the \\(50^{th}\\) percentile of the observed data, and dashed red lines represent the \\(10^{th}\\) and \\(90^{th}\\) percentiles of the observed data. The solid blue line represents the \\(50^{th}\\) percentile of the simularted data, and dashed blue lines represent the \\(10^{th}\\) and \\(90^{th}\\) percentiles of the simulated data. Light blue and pink areas represent the 95% confidence areas of the \\(10^{th}\\), \\(50^{th}\\) and \\(90^{th}\\) percentile lines.\n\n\n\nAdditive quantile regression VPC\nTo overcome the difficulties of making bins as well as determining the number of bins,\nJamsen et al. (2018) used additive quantile regression to calculate the quantiles of the observed and simulated data. This regression method makes it possible to estimate quantiles without discrete binning, which is especially useful when the data are insufficient, irregular, or inappropriate to configure the bins. To fit the additive quantile regression, we used the rqss function in the quantreg (Koenker 2023) package and developed the aqrVPC function to draw the VPC type plot with additive quantile regression. Figure 2 shows the additive quantile regression VPC plot. The solid and dashed lines represent the \\(10^{th}\\), \\(50^{th}\\), and \\(90^{th}\\) additive quantile regression lines of the observed data, and the pink and light blue areas represent the confidence areas of the additive quantile regression lines of the simulated data. Lines and areas in the additive quantile regression VPC plot are much smoother than those in the original VPC plot.\n\n\n\nFigure 2: The additive equantile VPC plot. Dots indicate the observed data. The solid and dashed blue lines represent the \\(10^{th}\\), \\(50^{th}\\), and \\(90^{th}\\) percentiles of the simulated data. The solid red line represents the \\(50^{th}\\) percentile line. Light blue and pink areas represent the 95% confidence areas of the \\(10^{th}\\), \\(50^{th}\\) and \\(90^{th}\\) percentile lines.\n\n\n\nBootstrap VPC\nThe bootstrap VPC (Post et al. 2008) compares the distribution of the simulated data to the distribution of the bootstrap samples drawn from the observed data.\nThis plot reflects the uncertainty of the observed data and allows for more objective comparisons with the predicted median.\nFigure 3 shows the bootstrap VPC plot using bootVPC. The solid and dashed blue lines represent the \\(10^{th}\\), \\(50^{th}\\), and \\(90^{th}\\) percentiles of the simulated data. The solid red line represents the \\(50^{th}\\) percentile line, and the pink areas represent the 95\\(\\%\\) confidence areas of the \\(50^{th}\\) percentile line, calculated from the bootstrap samples of the observed data. If the solid blue line and the solid red line are similar, the solid blue line is in the pink area, and the pink area is located between two dashed blue lines,\nthen this is evidence that the fitted model fit the observed data well.\n\n\n\nFigure 3: The bootstrap VPC plot. Dots indicate the observed data. The solid and dashed blue lines represent the \\(10^{th}\\), \\(50^{th}\\), and \\(90^{th}\\) percentiles of the simulated data. The solid red line represents the \\(50^{th}\\) percentile line, and the pink areas represent the 95% confidence areas of the \\(50^{th}\\) percentile line, calculated from the bootstrap samples of the observed data.\n\n\n\nAverage shifted VPC\nEven though binning mitigates the problem with highly irregular data, the VPC plot still has a precision problem with sparse data. In this paper, we propose a new approach to draw the adapted VPC plot from the average shifted histograms (Scott 1985).\nA histogram is a widely used method for displaying the density of a single continuous variable. However, histograms can look quite different based on different choice of bin width and anchor.\nThis requires computing an optimal bin width.\nTo overcome the problem with the choice of bin width and to obtain smoother estimates, Scott (1985) proposed the averaged shifted histogram (ASH). The idea behind ASH is to make \\(K\\) different histograms with different anchors and to combine them via a weighted average. For each histogram, the starting point is shifted by \\(d/K\\), where \\(d\\) is the width of the bin. We extend this idea to the VPC graph and propose the average shifted visual predictive check (asVPC) plot.\nThe binning of the VPC varies from the traditional binning in a histogram. The width of the bins is determined such that each bins contains the same, fixed number of observations, and as such the width of each bin is different. To apply the ASH idea to the VPC, we divide our data into \\(K*B\\) bins along with the independent variable, where each bin has a different width but the same number of observations. Here, \\(B\\) is the number of the original bins in the VPC plot, and \\(K\\) is the number of the histograms averaged in the asVPC plot.\nTo draw the VPC plots, the following information is needed from each bin:\nthe median of the independent variable and percentiles of the dependent variable of the observed data.\nthe median of the independent variable and percentiles of the dependent variable of the simulated data.\n95\\(\\%\\) confidence interval of percentiles (usually \\(10^{th}\\), \\(50^{th}\\), and \\(90^{th}\\) percentiles) of the dependent variable, calculated from the simulated data.\nThese values are also needed to produce confidence areas and percentile lines in the asVPC plot. Furthermore, additional steps are needed for asVPC. To find the median of the independent variable and percentiles of the dependent variable using the ASH algorithm, the following procedures are needed:\nDivide the independent variable into \\(N=K*B\\) bins.\nFor \\(i = 1, \\cdots, N\\),\nIf \\(i < K\\), combine \\(bin_1, \\cdots, bin_{i+K-1}\\)\\\nelse if \\(K\\le i\\le N-K\\) , combine \\(bin_{i-K+1}, \\cdots, bin_{i+K-1}\\)\\\nelse if \\(N-K < i\\) , combine \\(bin_{i-K+1}, \\cdots, bin_N\\)\nCalculate the median of the independent variable and the weighted percentiles of the dependent variable in the combined bin\n\nCollect the medians of the independent variable and the weighted percentiles of the dependent variable from 2, and connect them to the lines.\nWe can implement (A) and (B) by applying these procedures separately to the observed and simulated data. Additionally, (C) can be implemented using these procedures for each simulated dataset. First, we find the weighted percentiles, combine the results from each simulated dataset, and then calculate the 95\\(\\%\\) confidence intervals of each percentile. Using these three quantities (A), (B), and (C), we can draw the VPC type plot with the ASH approach, producing the asVPC plot.\nDetermining the weights\nIn the asVPC plot, the observations in each bin are combined using weights. Typically, the data near the center of the integrated bin have higher weights, and the data far from the center have smaller weights. This idea is used in the ASH algorithm as well as the density estimation literature. We suggest two different ways to apply weights for the asVPC calculation.\nBin-related weight: For each bin in the combined bin, K-1 bins in both directions have weights and the other bins have no weight. Use \\(\\frac{1}{K}\\), \\(\\cdots\\), \\(\\frac{i-1}{K}\\), \\(\\cdots\\), \\(\\frac{K-1}{K}\\), \\(1\\), \\(\\frac{K-1}{K}\\), \\(\\cdots\\), \\(\\frac{i-1}{K}\\), \\(\\cdots\\), \\(\\frac{1}{K}\\) as weights for 2K-1 bins. This approach is the same as the original ASH approach.\nDistance-related weight: Use the reciprocal of the distance from the center of the independent variable in each combined bin so that the points near the center have higher weights and the points far from the center have lower weights.\nFigure 4 shows the results from the asVPC function using bin-related weights and distance-related weights. The solid and dashed lines represent the average shifted quantile lines of the observed data, and the pink and light blue areas represent the confidence areas of the simulated data. The lines in the asVPC plot are smoother than those in the original VPC plot, and the confidence areas in the asVPC plot are thinner than those in the original VPC plot.\n\n\n\nFigure 4: The average shifted VPC plot. Dots indicate the observed data. The solid line represents the 50th quantiles of the observed data, and dashed lines represent the \\(10^{th}\\) and \\(90^{th}\\) percentiles of the observed data. Light blue and pink areas represent the 95% confidence areas of the \\(10^{th}\\), \\(50^{th}\\), and \\(90^{th}\\) percentiles.\n\n\n\n4 Validation in the model space\nTo validate the fitted model in the model space, we need to choose appropriate statistics to visualize and describe them in the model space. In nlmeVPC, we use the same statistics, that is quantiles, as the VPC plot in the data space, and compare them in two ways - numerically and visually. The numerical predictive check and the coverage plot are two commonly used methods in pharmacometrics. However, there is a limitation to detecting the illness of the fitted model. These methods combine the results of all ranges of the independent variable, and it is helpful to see the overall fitness. However, it is not easy to detect the detailed discrepancy between the fitted model and the observed data. To overcome this limitation, we developed a new plot, the coverage detailed plot, to compensate for the shortness of the coverage plot.\nNumerical predictive check\nThe VPC plot visually compares the observed data to the simulated percentiles in the data space. On the other hand, the numerical predictive check (NPC; Wang and Zhang (2012); Karlsson and Savic (2007))\nnumerically compares the observed data to the simulated data. For a given level of prediction (for example, 90\\(\\%\\)), the predicted interval is calculated using the simulated data, and the number of the observed data points outside of the prediction interval is counted, both below the lower bound and above the upper bound. The expected number of points below the lower bound of the predicted interval (for example, 5\\(\\%\\) of observations) is also calculated and compared to the observed number. If these two numbers are similar, the suggested model is suitable (Maharaj et al. 2019).\nNumericalCheck provides information summarizing the results of the NPC for various prediction levels.\nPI is the prediction level \\(\\times\\) 100 and n is the number of observations.\n\"Expected points below PI\" and \"Expected points above PI\" are respectively the expected numbers below and above the PI; \"points below PI\" and \"points above PI\" respectively represent the numbers of points below and above the PI;\n\"95%CIBelowFrom(%)\" and \"95%CIBelowTo(%)\" represent the 95\\(\\%\\) confidence interval of \"points below PI(%)\"; and \"95%CIAboveFrom(%)\" and \"95%CIAboveTo(%)\" represent the 95\\(\\%\\) confidence interval of \"points above PI(%)\". If \"points below PI(%)\" is in the\n95\\(\\%\\) confidence intervals of \"points below PI(%)\" and is similar to \"Expected points below PI\", and if\n\"points above PI(%)\" is in the\n95\\(\\%\\) confidence intervals of \"points above PI(%)\" and is similar to \"Expected points above PI\", this is the evidence that the fitted model explains the data well.\n\n\nNumericalCheck(origdata,simdata,pred.level=c(0,0.2,0.4,0.6,0.8,0.9),N_xbin=8)$NPC\n\n  PI   n Expected points below PI points below PI points below PI(%)\n1  0 132                     66.0              57           43.18182\n2 20 132                     52.8              46           34.84848\n3 40 132                     39.6              37           28.03030\n4 60 132                     26.4              27           20.45455\n5 80 132                     13.2              17           12.87879\n6 90 132                      6.6               7            5.30303\n  95%CIBelowFrom(%) 95%CIBelowTo(%) Expected points above PI\n1        35.6060606        56.81818                     66.0\n2        26.8750000        48.88258                     52.8\n3        20.4545455        36.76136                     39.6\n4        11.3636364        26.15530                     26.4\n5         3.7878788        16.66667                     13.2\n6         0.3598485        10.64394                      6.6\n  points above PI points above PI(%) 95%CIAboveFrom(%)\n1              63          47.727273         34.090909\n2              49          37.121212         25.359848\n3              35          26.515152         16.666667\n4              28          21.212121         10.965909\n5              12           9.090909          2.632576\n6               5           3.787879          1.117424\n  95%CIAboveTo(%)\n1       55.303030\n2       46.609848\n3       36.003788\n4       25.000000\n5       15.549242\n6        8.333333\n\nCoverage plot\nThe result of the NPC is a table with many values, which, while useful, can be difficult to parse visually. The coverage plot (Karlsson and Holford 2008) was developed to help visually check the fitted model with the NPC result.\nIn each level of the predicted interval, the ratios between the expected number of data points (Exp) outside the prediction interval and the observed number of data points (Obs)\noutside the prediction interval are calculated. These ratios are calculated separately for the upper and lower bound of the prediction interval. For example, when the prediction level is 90, a 90\\(\\%\\) prediction interval is used, and 10\\(\\%\\) of the observations are expected to locate outside this prediction interval. To be more precise, 5\\(\\%\\) of observations are expected to be above the upper limit, and 5\\(\\%\\) of observations are expected to be below the lower limit.\nThe coverage plot with the NPC result can diagnose a model using multiple prediction intervals. The X-axis represents the prediction level, and the Y-axis represents Obs/Exp. The closer Obs/Exp is to 1, the more appropriate the model is. Furthermore, the confidence intervals of Obs/Exp values are obtained using simulated data and then expressed together in the plot for a more objective comparison. This plot can provide more information than the VPC plot, which interprets only a couple of quantiles - usually the 10\\(\\%\\), 50\\(\\%\\), and 90\\(\\%\\) percentiles.\nThe xpose4 package (Keizer et al. 2013) provides a coverage plot. However,\nto draw the coverage plot using xpose4, PsN (Lindbom et al. 2004) software is needed to calculate the NPC result. Therefore, we developed coverageplot to draw the coverage plot using the results from NumericalCheck.\nFigure 5(A) shows the coverage plot. The X-axis shows the level of the prediction interval. The Y-axis show the ratio between the observed number of data and the expected number of data of the lower and upper parts in each level of the prediction interval. The white line is the reference line, and the gray area represents the confidence areas of the ratios in each prediction level. If the solid lines are near the white line or in the gray area, we can conclude that the suggested model is suitable.\nCoverage detailed plot\nUnlike the VPC plot, which represents the data space, the information in the observed data space does not come together in the coverage plot, which makes it difficult to determine whether the model is overestimated, underestimated, or adequate in the specific region of the data space. To overcome this limitation of the coverage plot, we propose a new method called the coverage detailed plot.\nThe percentages of observations above the prediction interval are calculated in each bin of the independent variable. Additionally, the percentages of observations below the prediction interval are calculated. The white dots in the plot represent the expected percentages. If the percentages of upper and lower observations are near the white dots, we can conclude that the suggested model is suitable for the specific prediction interval.\nFigure 5(B) is the result of coverageDetailplot when the prediction level is 80\\(\\%\\). The white dots represent the expected percentages of the lower and upper the prediction intervals, 10\\(\\%\\), and 90\\(\\%\\), respectively. The upper and lower percentages of observation in each time bin are shown in darker gray. The left bin(before 0.045 hours) shows all light gray in the coverage detailed plot, and it is quite different patterns from the expected one. However, it is mainly due to the characteristics of this example data. All observations in this bin are 0. It makes the lower and upper bound of the prediction interval all 0, and the lower and upper percentages become 0.\n\n\n\nFigure 5: The coverage plot and the coverage detailed plot for the 80% prediction interval. In the coverage plot, the X-axis is the level of the prediction interval. The Y-axis is the ratio between the number of observed data and the number of expected data of the lower and upper parts in each level of the prediction interval. The white line is the reference line, and the gray area represents the confidence area of the ratios. If the solid lines are near the white line, we can conclude that the suggested model is suitable. In the coverage detailed plot, the white dots represent the expected percentages of lower and upper prediction intervals of, 10%, and 90%, respectively. The upper and lower percentages of observation in each time bin are darker gray.\n\n\n\nQuantified VPC\nPost et al. (2008) proposed the quantified VPC (QVPC). It expanded the existing VPC, including information about missing data.\nThe QVPC plot visually represents actual and unavailable observations around the predicted medians through the form of percent, regardless of the observed data’s density or shape. Here, “unavailable observations” refer to all kinds of missing values and unobserved data that occur for various reasons, including deviations and unexpected omissions.\nIf the model is appropriate, observations at each time bin are allocated randomly around the predicted median of the model. In the QVPC plot, white dots represent the model’s predicted median in each bin. If the borderlines above and below are close to the white dots, we can conclude that the fitted model describes the observed data well.\nFigure 6 shows the result of quantVPC. The darker gray areas represent the percentages below the median. The lighter gray areas represent the percentage above. The brightest gray areas represent the percent unavailable in each time bin. The white dots represent the ideal location where the above and the below percentages meet. In this example, there is no missing value.\n\n\n\nFigure 6: The quantified VPC plot. The darker gray areas represent the percentages below the median, the lighter gray areas represent the percentage above, and the brightest gray areas represent the percent unavailable in each time bin. The white dots represent the ideal location where the above and the below percentages meet.\n\n\n\n\n\nTable 1: List of functions to check the validity of model with the observed data and the simulated data\n\n\nFunction\n\n\nDescription\n\n\nVPCgraph\n\n\ndraw the original VPC plot\n\n\naqrVPC\n\n\ndraw the additive quantile regression VPC plot\n\n\nbootVPC\n\n\ndraw the bootstrap VPC plot\n\n\nasVPC\n\n\ndraw VPC using ASH method\n\n\nNujmericalCheck\n\n\ncalculate the numbers to check coverage in each prediction level\n\n\ncoverageplot\n\n\nplot the result of NumericalCheck\n\n\ncoverageDetailplot\n\n\nplot for checking specific prediction level\n\n\nquantVPC\n\n\nplot for the quantified VPC\n\n\n\n\nTable 2: Summary of the arguments of functions in nlmeVPC\n\n\nArgument\n\n\nDescription\n\n\norig_data\n\n\nA data frame of original data with X and Y variable\n\n\nsim_data\n\n\nA matrix of simulated data with only Y values collected\n\n\ntype\n\n\nType of VPC graph; ‘CI’, ‘percentile’, or ‘scatter’\n\n\nweight_method\n\n\nThe way to put weights in asVPC; ‘bin’ or ‘distance’\n\n\nN_xbin\n\n\nNumber of bins in X variable\n\n\nprobs\n\n\nA numeric vector of probabilities\n\n\nconf.level\n\n\nConfidence level of the interval\n\n\nX_name\n\n\nName of X variable inorig_data\n\n\nY_name\n\n\nName of Y variable in orig_data\n\n\nsubject_name\n\n\nName of subject variable in orig_data\n\n\nMissingDV\n\n\nName of missing indicator variable in orig_data\n\n\nDV_point\n\n\nDraw point (X, Y) in the plot if TRUE; omit if FALSE\n\n\nCIvpc_type\n\n\nType of CI area in VPC graph; ‘line’ or ‘segment’\n\n\nbin_grid\n\n\nDraw grid lines for binning in X variable if TRUE; omit if FALSE\n\n\nplot_caption\n\n\nPut caption with additional information if TRUE; omit if FALSE\n\n\nplot_flag\n\n\nDraw plot if TRUE; generate data for drawing plot if FALSE\n\n\nlinesize\n\n\nSize of line in the plot\n\n\npointsize\n\n\nSize of point in the plot\n\n\ncaptionsize\n\n\nSize of caption\n\n\nmaxK\n\n\nThe maximum number of bins\n\n\nKmethod\n\n\nThe way to calculate the penalty in automatic binning; ‘cluster’ or ‘kernel’\n\n\nbeta\n\n\nAdditional parameter for automatic binning, used in optK function\n\n\nlambda\n\n\nAdditional parameter for automatic binning, used in optK function\n\n\n5 The nlmeVPC package: structure and functionality\nTable 1 shows the list of functions and Table 2 shows the list of arguments used in the functions of nlmeVPC. The following codes are for Figure 1 to Figure 6.\n\n\nlibrary(nlmeVPC)\ndata(origdata)\ndata(simdata)\n\noptK(origdata$TIME)$K\n\n[1] 8\n\n\n\n# Figure 1\n\nVPCgraph(origdata,simdata,N_xbin=8,type=\"scatter\")+\n  labs(title=\"(A) Visual Predictive Check : scatter\",caption=\"\")\nVPCgraph(origdata,simdata,N_xbin=8,type=\"percentile\")+\n  labs(title=\"(B) Visual Predictive Check : percentile\",caption=\"\")\nVPCgraph(origdata,simdata,N_xbin=8,type=\"CI\")+\n  labs(title=\"(C) Visual Predictive Check : CI\",caption=\"\")\n  \n# Figure 2\n\naqrVPC(origdata,simdata) +labs(caption=\"\")\n\n# Figure 3\n\nbootVPC(origdata,simdata,N_xbin=8)+labs(caption=\"\")\n \n# Figure 4\n\nasVPC(origdata,simdata,type=\"CI\",N_xbin=8,N_hist=3,weight_method=\"bin\")+labs(caption=\"\")\nasVPC(origdata,simdata,type=\"CI\",N_xbin=8,N_hist=3,weight_method=\"distance\")+labs(caption=\"\")\n  \n# Numerical Predictive Check\n \nNumericalCheck(origdata,simdata,N_xbin=8,pred.level=c(0,0.2,0.4,0.6,0.8,0.9))$NPC\n\n# Figure 5\n \n\ncoverageplot(origdata,simdata,N_xbin=8) +ggtitle(\"(A) Coverage Plot\")\ncoverageDetailplot(origdata,simdata,N_xbin=8,predL=0.8) +\nggtitle(\"(B) Coverage Detailed plot: PI = 80\")\n \n# Figure 6\n \nquantVPC(origdata,simdata)\n\n\nWe use an example to show how to use functions in nlmeVPC and how they work.\nExample\nThe origdata in nlmeVPC is from an experiment on the pharmacokinetics of theophylline. Twelve patients were given oral doses of theophylline, and blood concentrations were measured at 11 time points over the next 25 hours. Each patient had different time points.\nWe consider the following first-order absorption one-compartment model:\n\\[\ny_{ij}= \\frac{Amt_i * Ke_i *Ka_i}{Cl_i} \\left(\\exp( -Ke_i * TIME_{ij})-\\exp(-Ka_i * TIME_{ij})\\right) +\\varepsilon_{ij}.\n\\]\nIn this model, \\(y_{ij}\\) is the theophylline concentration at \\(TIME_{ij}\\) after an initial dose of \\(Amt_i\\). The pharmacokinetic parameters are the absorption rate constant \\(Ka\\), the elimination rate constant \\(Ke\\), and the clearance \\(Cl\\).\nIn this example, two different models are fitted and diagnosed using functions in the nlmeVPC package. In Model 1, \\(Ka\\) and \\(Cl\\) are considered as random effects. In Model 2, \\(Ke\\) is considered as random effect, and \\(Ka\\) and \\(Cl\\) are considered only as a fixed effect. The nlme (Pinheiro et al. 2022) and nlraa (Miguez 2022) packages are used to fit the nonlinear mixed models and to generate the simulated data from the fitted model.\n\n\nlibrary(nlme)\nlibrary(nlraa)\nlibrary(nlmeVPC)\ndata(origdata)\norigdataT <- groupedData(DV~TIME|ID,origdata)\n\n# Model 1 (True) \nT.nlme <- nlme(DV ~ exp(lKe+lKa-lCl)*AMT*\n               (exp(-exp(lKe)*TIME) - exp(-exp(lKa)*TIME))/\n               (exp(lKa)-exp(lKe)), data=origdataT,\n             fixed=lKa+lKe+lCl~1,\n             random=lKa+lCl~1,\n             start=c(lKe=-2,lKa=1.5,lCl=-3))\nset.seed(123456)\nsim.T <- simulate_nlme(object=T.nlme,nsim=100,psim=3,level=1,value=\"data.frame\",data = NULL)\nsimdata.T <- matrix(sim.T$sim.y,ncol=100)\n\n# Model 2 (Wrong)\nF.nlme <- nlme(DV ~ exp(lKe+lKa-lCl)*AMT*\n                (exp(-exp(lKe)*TIME) - exp(-exp(lKa)*TIME))/\n                (exp(lKa)-exp(lKe)), data=origdataT,\n              fixed=lKa+lKe+lCl~1,\n              random=lKe~1,\n              start=c(lKe=-2,lKa=1.5,lCl=-3))\n\nsim.F <- simulate_nlme(object=F.nlme,nsim=100,psim=3,level=1,value=\"data.frame\",data = NULL)\nsimdata.F <- matrix(sim.F$sim.y,ncol=100)\n\n\n\n\n# Figure 7\n \nVPCgraph(origdata,simdata.T,type=\"CI\",N_xbin=8)+labs(title=\"Model 1\",caption=\"\")\nVPCgraph(origdata,simdata.F,type=\"CI\",N_xbin=8)+labs(title=\"Model 2\",caption=\"\")\n  \n# Figure 8\n \naqrVPC(origdata,simdata.T)+labs(title=\"Model 1\",caption=\"\")\naqrVPC(origdata,simdata.F)+labs(title=\"Model 2\",caption=\"\")\n  \n# Figure 9\n \nasVPC(origdata,simdata.T,type=\"CI\",weight_method=\"distance\",N_xbin=8)+labs(title=\"Model 1\",caption=\"\")\nasVPC(origdata,simdata.F,type=\"CI\",weight_method=\"distance\",N_xbin=8)+labs(title=\"Model 2\",caption=\"\")\n\n# Figure 10\n \nbootVPC(origdata,simdata.T,N_xbin=8)+labs(title=\"Model 1\",caption=\"\")\nbootVPC(origdata,simdata.F,N_xbin=8)+labs(title=\"Model 2\",caption=\"\")\n  \n# Figure 11\n \ncoverageplot(origdata,simdata.T,conf.level=0.9,N_xbin=8)+labs(title=\"Model 1\")\ncoverageplot(origdata,simdata.F,conf.level=0.9,N_xbin=8)+labs(title=\"Model 2\")\n  \n# Figure 12\n \ncoverageDetailplot(origdata,simdata.T,predL=0.5,N_xbin=8)+labs(title=\"Model 1\")\ncoverageDetailplot(origdata,simdata.F,predL=0.5,N_xbin=8)+labs(title=\"Model 2\")\n  \n# Figure 13\n \ncoverageDetailplot(origdata,simdata.T,predL=0.8,N_xbin=8)+labs(title=\"Model 1\")\ncoverageDetailplot(origdata,simdata.F,predL=0.8,N_xbin=8)+labs(title=\"Model 2\")\n  \n# Figure 14\n \nquantVPC(origdata,simdata.T,N_xbin=8)+labs(title=\"Model 1\")\nquantVPC(origdata,simdata.F,N_xbin=8)+labs(title=\"Model 2\")\n\n\nFigure 7 shows the VPCgraph for Model 1 and Model 2.\nThe solid line represents the 50\\(^{th}\\) percentile of the observed data, and the dashed lines represent the 10\\(^{th}\\) and 90\\(^{th}\\) percentiles of the observed data. In Model 1, all quantile lines (the solid and dashed lines) are in the confidence area. However, the 10\\(^{th}\\) and 90\\(^{th}\\) percentiles in Model 2 are mostly outside the confidence area, especially at 0 to 5 hours.\n\n\n\nFigure 7: The VPC plots of Model 1 and Model 2.\n\n\n\nFigure 8 shows the results of aqrVPC, and Figure 9 shows the results of asVPC for Model 1 and Model 2. The additive quantile regression VPCs and the average shifted VPCs show similar patterns to the original VPCs. Model 1 shows all quantile lines in the confidence area, and the 10\\(^{th}\\) and 90\\(^{th}\\) percentiles in Model 2 are mostly outside the confidence area.\n\n\n\nFigure 8: The additive quantile regression VPC plots for Model 1 and Model 2.\n\n\n\n\n\n\nFigure 9: The average shifted VPC plots for Model 1 and Model 2.\n\n\n\nFigure 10 shows the results of bootVPC for Model 1 and Model 2. The solid and dashed blue lines show the \\(10^{th}\\), \\(50^{th}\\), and \\(90^{th}\\) percentiles of the simulated data. The solid red line represents the \\(50^{th}\\) percentile line of the observed data, and the pink areas represent the 95\\(\\%\\) confidence areas of the \\(50^{th}\\) percentile line, calculated from the bootstrap samples of the observed data.\nThe solid blue line is in the pink area, and the two solid red and blue lines are almost identical in both models.\nThe dashed blue lines in Model 1 cover most of the observed data. However, the dashed blue lines in Model 2 do not cover the observed data. Many observed data points lie outside these dashed blue lines in Model 2, especially in 0 to 10 hours.\n\n\n\nFigure 10: The bootstrap VPC plots for Model 1 and Model 2.\n\n\n\n\n\n\nFigure 11: The coverage plots for Model 1 and Model 2.\n\n\n\nFigure 11 shows the coverageplot results for Model 1 and Model 2. The lines are in the gray area and close to the white line in Model 1. However, the lines in Model 2 are not in the gray area, especially when the PI value is large. Figures 12 and 13 show the results of coverageDetailplot for Model 1 and Model 2 when PIs are 50\\(\\%\\) and 80\\(\\%\\).\nThe upper and lower percentages in both figures are close to the white points in Model 1. On the other hand, the upper percentages of the most time bins are far from the white points in Model 2, especially the time bin (3.54,5.28] when PI = 50\\(\\%\\). When PI = 80\\(\\%\\), most upper and lower percentages are far from the white points.\n\n\n\nFigure 12: The coverage detailed plots for Model 1 and Model 2 when PI=50%.\n\n\n\n\n\n\nFigure 13: The coverage detailed plots for Model 1 and Model 2 when PI=80%.\n\n\n\nFigure 14 shows the results of quantVPC for Model 1 and Model 2. In Model 2, the right tail area (after 11 hours) looks quite different from the expected pattern. The above percentages are much larger than the below percentages.\nThe results from Figure 7 through Figure 14 show that Model 1 explains the origdata quite well. However, Model 2 shows different patterns than Model 1 in most figures. We can conclude that Model 1 is better than Model 2, and treating \\(Ka\\) and \\(Cl\\) as random effects is better.\n\n\n\nFigure 14: The quantified VPC plots for Model 1 and Model 2.\n\n\n\n6 Summary\nThis paper introduces the nlmeVPC package. The VPC and its extensions are useful for validating the nonlinear mixed effect model. The nlmeVPC package provides various visual diagnostic tools for the nonlinear mixed effect model in two different approaches: validation in data space and model space. Both approaches are valuable. Validation in data space can compare the fitted model with the original data, and validation in model space provides detailed comparisons in various ways. In the nlmeVPC package, we also provide new approaches - asVPC and coverageDetailplot. Here, asVPC provides a more precise VPC plot, and coverageDetailplot provides the detailed feature of the fitted model that is not captured in the coverage plot. Even though the coverage plot does not show any problem with the fitted model, the coverage detailed plot can reveal the problem in a specific region (Figures 10, 11, and 12).\n7 Acknowledgements\nThis work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (2018R1A2B6001251). This paper was prepared by extracting part of Eun-Hwa Kang and Myungji Ko’s master thesis.\n\nCRAN packages used\nnlmeVPC, xpose4, vpc, nlmixr2, ggplot2, quantreg, nlme, nlraa\nCRAN Task Views implied by cited packages\nAgriculture, ChemPhys, DifferentialEquations, Econometrics, Environmetrics, Finance, HighPerformanceComputing, MixedModels, OfficialStatistics, Optimization, Pharmacokinetics, Phylogenetics, Psychometrics, ReproducibleResearch, Robust, Spatial, SpatioTemporal, Survival, TeachingStatistics\n\n\nR. J. Bauer. NONMEM users guide introduction to NONMEM 7.2.0. ICON Development Solutions Ellicott City, MD, 2011. URL https://www.iconplc.com/innovation/nonmem/.\n\n\nM. Davidian. Nonlinear models for repeated measurement data. Routledge, 2017.\n\n\nM. Fidler, J. J. Wilkins, R. Hooijmaijers, T. M. Post, R. Schoemaker, M. N. Trame, Y. Xiong and W. Wang. Nonlinear mixed-effects model development and simulation using nlmixr and related r open-source packages. CPT: pharmacometrics & systems pharmacology, 8(9): 621–633, 2019. URL https://doi.org/10.1002/psp4.12445.\n\n\nA. Heus, D. W. Uster, V. Grootaert, N. Vermeulen, A. Somers, D. H. In’t Veld, S. G. Wicha and P. A. De Cock. Model-informed precision dosing of vancomycin via continuous infusion: A clinical fit-for-purpose evaluation of published PK models. International Journal of Antimicrobial Agents, 59(5): 106579, 2022. URL https://doi.org/10.1016/j.ijantimicag.2022.106579.\n\n\nK. M. Jamsen, K. Patel, K. Nieforth and C. M. Kirkpatrick. A regression approach to visual predictive checks for population pharmacometric models. CPT: pharmacometrics & systems pharmacology, 7(10): 678–686, 2018. URL https://doi.org/10.1002/psp4.12319.\n\n\nM. O. Karlsson and N. Holford. A tutorial on visual predictive checks. In 17th meeting of the population approach group in europe, marseille, france, page abstr, 2008. URL https://www.page-meeting.org/?abstract=1434.\n\n\nM. Karlsson and R. Savic. Diagnosing model diagnostics. Clinical Pharmacology & Therapeutics, 82(1): 17–20, 2007. URL https://doi.org/10.1038/sj.clpt.6100241.\n\n\nR. Keizer. Vpc: Create visual predictive checks. R package version 1.2.2. 2021. URL https://cran.r-project.org/web/packages/vpc/index.html.\n\n\nR. J. Keizer, M. Karlsson and A. Hooker. Modeling and simulation workbench for NONMEM: Tutorial on pirana, PsN, and xpose. CPT: pharmacometrics & systems pharmacology, 2(6): 1–9, 2013. URL https://doi.org/10.1038/psp.2013.24.\n\n\nR. Koenker. Quantreg: Quantile regression. R package version 5.95. 2023. URL https://cran.r-project.org/web/packages/quantreg/index.html.\n\n\nM. Lavielle and K. Bleakley. Automatic data binning for improved visual diagnosis of pharmacometric models. Journal of pharmacokinetics and pharmacodynamics, 38(6): 861–871, 2011. URL https://doi.org/10.1007/s10928-011-9223-3.\n\n\nL. Lindbom, J. Ribbing and E. N. Jonsson. Perl-speaks-NONMEM (PsN)—a perl module for NONMEM related programming. Computer methods and programs in biomedicine, 75(2): 85–94, 2004. URL https://doi.org/10.1016/j.cmpb.2003.11.003.\n\n\nA. R. Maharaj, H. Wu, C. P. Hornik and M. Cohen-Wolkowiez. Pitfalls of using numerical predictive checks for population physiologically-based pharmacokinetic model evaluation. Journal of pharmacokinetics and pharmacodynamics, 46(3): 263–272, 2019. URL https://doi.org/10.1007/s10928-019-09636-5.\n\n\nF. Miguez. Nlraa: Nonlinear regression for agricultural applications. R package version 1.5. 2022. URL https://cran.r-project.org/web/packages/nlraa/index.html.\n\n\nS. E. Mudde, R. Ayoun Alsoud, A. van der Meijden, A. M. Upton, M. U. Lotlikar, U. S. Simonsson, H. I. Bax and J. E. de Steenwinkel. Predictive modeling to study the treatment-shortening potential of novel tuberculosis drug regimens, toward bundling of preclinical data. The Journal of Infectious Diseases, 225(11): 1876–1885, 2022. URL https://doi.org/10.1093/infdis/jiab101.\n\n\nT. Nguyen, M.-S. Mouksassi, N. Holford, N. Al-Huniti, I. Freedman, A. C. Hooker, J. John, M. O. Karlsson, D. Mould, J. Pérez Ruixo, et al. Model evaluation of continuous data pharmacometric models: Metrics and graphics. CPT: pharmacometrics & systems pharmacology, 6(2): 87–109, 2017. URL https://doi.org/10.1002/psp4.12161.\n\n\nM. Otto, K. Bergmann, G. Jacobs and M. J. van Esdonk. Predictive performance of parent-metabolite population pharmacokinetic models of (s)-ketamine in healthy volunteers. European journal of clinical pharmacology, 77(8): 1181–1192, 2021. URL https://doi.org/10.1007/s00228-021-03104-1.\n\n\nJ. C. Pinheiro and D. M. Bates. Mixed-effects models in s and s-PLUS. Springer New York, 2000. URL https://doi.org/10.1007/978-1-4419-0318-1.\n\n\nJ. Pinheiro, D. Bates, S. DebRoy, D. Sarkar, S. Heisterkamp, B. Van Willigen and R. Maintainer. Nlme: Linear and nonlinear mixed effects models. R package version 3.1-159. 2022. URL https://cran.r-project.org/web/packages/nlme/index.html.\n\n\nT. M. Post, J. I. Freijer, B. A. Ploeger and M. Danhof. Extensions to the visual predictive check to facilitate model performance evaluation. Journal of pharmacokinetics and pharmacodynamics, 35(2): 185–202, 2008. URL https://doi.org/10.1007/s10928-007-9081-1.\n\n\nR Core Team. R: A language and environment for statistical computing. 2023. URL https://www.R-project.org/.\n\n\nD. W. Scott. Average shifted histograms: Effective nonparametric density estimations in several dimensions. Annals of Statistics, 13(3): 1024–1040, 1985. URL https://doi.org/10.1214/aos/1176349654.\n\n\nD. D. Wang and S. Zhang. Standardized visual predictive check versus visual predictive check for model evaluation. The Journal of Clinical Pharmacology, 52(1): 39–54, 2012. URL https://doi.org/10.1177/0091270010390040.\n\n\nH. Wickham. ggplot2 : Elegance graphics for data analysis. Springer, 2016.\n\n\nH. Wickham, D. Cook and H. Hofmann. Visualizing statistical models: Removing the blindfold. Statistical Analysis and Data Mining: The ASA Data Science Journal, 8(4): 203–225, 2015. URL https://doi.org/10.1002/sam.11271.\n\n\n\n\n",
    "preview": "articles/RJ-2023-026/distill-preview.png",
    "last_modified": "2023-11-07T21:31:41+00:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 2304
  },
  {
    "path": "articles/RJ-2023-027/",
    "title": "Likelihood Ratio Test-Based Drug Safety Assessment using R Package pvLRT",
    "description": "Medical product safety continues to be a key concern of the twenty-first century. Several spontaneous adverse events reporting databases established across the world continuously collect and archive adverse events data on various medical products. Determining signals of disproportional reporting (SDR) of product/adverse event pairs from these large-scale databases require the use of principled statistical techniques. Likelihood ratio test (LRT)-based approaches are particularly noteworthy in this context as they permit objective SDR detection without requiring ad hoc thresholds. However, their implementation is non-trivial due to analytical complexities, which necessitate the use of computation-heavy methods. Here we introduce R package pvLRT which implements a suite of LRT approaches, along with various post-processing and graphical summary functions, to facilitate simplified use of the methodologies. Detailed examples are provided to illustrate the package through analyses of three real product safety datasets obtained from publicly available FDA FAERS and VAERS databases.",
    "author": [
      {
        "name": "Saptarshi Chakraborty",
        "url": {}
      },
      {
        "name": "Marianthi Markatou",
        "url": {}
      },
      {
        "name": "Robert Ball",
        "url": {}
      }
    ],
    "date": "2023-08-26",
    "categories": [],
    "contents": "\n1 Introduction\nAdverse events of medical products are a major concern worldwide, with serious global health implications particularly in the post-pandemic world. While clinical trials conducted pre-licensure remain the primary source of safety information about medical products, and other data sources such as claims and electronic health records are increasingly being used post-licensure, they exhibit several limitations. First, detecting extremely rare adverse events in clinical trials is difficult due to their strict inclusion/exclusion criteria and their relatively small counts compared to the population which will eventually receive the product. Second, randomized clinical trials for detecting multiple adverse events may be logistically and ethically infeasible. Despite having known issues such as self-selection, confounding, and missing data, etc., large-scale observational studies have been used in surveillance and post-licensure epidemiological studies to supplement traditional clinical trial-based approaches (Markatou and Ball 2014). Thanks to the proliferation of electronic health record systems worldwide, a vast trove of such data are now available and can collectively provide critical first alerts for emerging medical product safety concerns. Curated large-scale databases such as US Food and Drug Administration (FDA)’s Adverse Event Reporting System (FAERS) and Vaccine Adverse Event Reporting System (VAERS), the European Medicines Agency’s Eudravigilance, and the World Health Organization’s VigiBase provide collective lists of drugs/vaccines and adverse events from large number of reports, and thus are invaluable resources for product safety assessment.\nMining these databases to assess medical product safety is however challenging due to limitations of the data (e.g., biases in reporting, presence of duplicate reports, and missing/incomplete information) and the lack of a universally accepted statistical methodology. Several approaches to detecting signals of disproportionate reporting (SDR) as an indicator of a possible safety concern from adverse event data exist; these constitute a part of the pharmacovigilance process for assessing whether a medical product is the cause of an adverse event. Of the existing approaches, Proportional Relative Reporting Proportion (PRR), Reporting Odds Ratio (ROR), Multi-item Gamma Poisson Shrinker (MGPS), Bayesian Confidence Propagation Neural Network (BCPNN) and their false discovery rate (FDR)-adjusted variants are notable. Many of these approaches require one or both types of thresholding: (a) on the number of reports needed for the method to operate, and (b) on the underlying statistic to identify important/severe adverse events. However, determining appropriate thresholds remains a key challenge for these approaches, and in practice ad hoc thresholds are often used. This is problematic because both too low and too high thresholds lead to drastically different detection rates, and no separate measure is usually available to assess correctness of the detected signals. A principled approach permitting data-driven threshold determination with optimal guarantees on the detected signals is therefore essential to ensure statistical validity of results.\nThe LRT-based approaches to signal detection in the medical product safety datasets (Huang et al. 2011, 2017; Zhao et al. 2018; Ding et al. 2020; Chakraborty et al. 2022) provide a highly rigorous suite of statistical methods to address these problems. To provide a high-level summary, these methods assume a Poisson/zero-inflated Poisson model to parametrize associations between drugs and their adverse events. Then conditional on the total reported number of cases for each adverse event and each medical product, these methods quantify signal strength in the observed number of reports per adverse event/medical product pair through a (likelihood ratio) test statistic. Finally, significance of the pair is determined by comparing the observed test statistic to its null (independence) sampling distribution. There are several key advantages of these formal hypothesis test-based approaches which make them ideal for practical use. First, they aid coherent frequetist sampling distribution-based quantification of signal strengths, ensuring strong statistical and probabilistic rigor of the results. Second, they do not require the use of ad hoc thresholds for the observed report counts; instead, they quantify significance through probabilistic measures of uncertainty, e.g., \\(p\\)-values. This ensures that the identified signals are valid up to a pre-specified level of tolerance (usually 0.05). Third, because they are based on the highly formal likelihood ratio test theory for null hypothesis significance testing, they can rigorously control the type I error and the false discovery rate, while ensuring high power and sensitivity for signal detection. Moreover, by appropriately defining a maximum likelihood ratio test statistic, the method achieves automated FDR adjustment without requiring any separate \\(p\\)-value adjustment step.\n\n\nTable 1: Existing R packages on CRAN with functionalities for pharmacovigilance.\n\n\nPackage\n\n\nMethod\n\n\nNotes\n\n\nPhViD: Pharmacovigilance Signal Detection\n\n\nPRR, ROR, BCPNN, GPS\n\n\nAimed towards drug safety.\n\n\nopenEBGM: EBGM Disproportionality Scores for Adverse Event Data Mining\n\n\nMGPS\n\n\nAimed towards drug safety.\n\n\nsglr: An R package for power and boundary calculations in pre-licensure vaccine trials using a sequential generalized likelihood ratio test\n\n\nSequential Generalized Likelihood Ratio decision boundaries\n\n\nAimed towards sequential testing-based vaccine safety assessments as used by the FDA.\n\n\nSequential: Exact sequential analysis for Binomial and Poisson data\n\n\nMax SPRT statistic\n\n\nAimed towards sequential testing-based vaccine safety assessments as used by the FDA.\n\n\nAEenrich: Vaccine adverse event enrichment tests\n\n\nModified Fisher’s exact test (for pre-selected significant AEs); b) Modified Kolmogorov Smirnov statistic\n\n\nAimed towards sequential testing-based vaccine safety assessments as used by the FDA.\n\n\nmds: Medical Devices Surveillance\n\n\nData preprocessing\n\n\nProvides functions for handling messy/unstructured medical devices data.\n\n\npvLRT: A suite of likelihood ratio test based methods to use in pharmacovigilance\n\n\n(Pseudo) LRT approaches based on log-linear models\n\n\nOur package.\n\n\nThere currently exist a few R packages in CRAN with functionalities relevant for medical product safety. This includes the packages PhViD (Ahmed and Poncet 2016), openEBGM (Canida and Ihrie 2017), AEenrich (Li et al. 2021), Sequential (Silva and Kulldorff 2021), SPRT sglr (Narasimhan and Shih 2012), and mds (Chung 2020). Among these PhViD and openEBGM provide functionalities for spontaneous adverse event data-driven pharmacovigilance: PhViD implements methods such as PRR, ROR, and BCPNN, and openEBGM implements the method of MGPS. By contrast, the other packages provide sequential testing-based approaches to vaccine safety. Table 1 lists these packages and their functionalities together with some high-level notes on their targeted uses. We note however that none of these packages implement LRT-based approaches to drug safety based on spontaneous adverse event data as considered in pvLRT. Indeed the development of pvLRT was motivated by the lack of easily accessible and comprehensive open source computational solutions to the LRT-based pharmacovigilance approaches. An important common ingredient of these LRT-based methods is a computation-intensive Monte Carlo simulation step required to facilitate (exact/non-asymptotic) inference from the analytically intractable null sampling distributions of the relevant test statistics. However, this requirement precludes immediate applications of the methods by pharmacovigilance practitioners in the absence of easily accessible software implementations. This is particularly relevant for zero-inflation based models where, in addition to the computation-heavy Monte Carlo step additional numerical optimization steps are necessary for the estimation of the zero-inflation parameter. We have developed pvLRT (Chakraborty and Markatou 2022) to cater to these needs while ensuring proper statistical rigor in the implementations.\nThe following are the major contributions of the pvLRT package. First, the package serves as a comprehensive software implementation of several LRT-based methods proposed over the past decade, including some recently developed methods. Both Poisson and Zero-inflated Poisson (ZIP) based models are implemented, and tests of both single and multiple simultaneous drugs (or medical products) are provided. Formal model comparison methods, such as Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC), can be used on the model fits to aid data-driven selection of Poisson vs. ZIP models on individual datasets. Second, three visual summary plots for the test results , namely bubble plot, barplot, and heatmap, are provided for immediate visualization. These plots provide information on sample size and test statistic and p-values for all AE/drug pairs tested, and hence permit quick exploration of signals in a (possibly large) dataset. These visualizations use ggplot2 (Wickham 2016) as the plotting engine, and hence its attributes (color, text size, etc.) can be easily modified post hoc by changing some of the ggplot2 graphical sttributes. In addition, leveraging the modern R ecosystem, the resultant plots can be easily made interactive via additional functions from other packages, such as ggplotly() from the package plotly (Sievert 2020). Third, aside from implementations of likelihood ratio tests, simple functions for random contingency table data generation from the Poisson and zero-inflated Poisson models are provided. This is particularly helpful in simulation experiments where different statistical methods for pharmacovigilance are compared. Finally, we have included processed contingency tables of AE/drug reports for two specific drug groups, viz. statins and Gadolinium-based Contrast Agents (GBCA; Zhao et al. (2018)), raw AE/drug incidence data, and a vaccine/AE dataset on rotavirus vaccines as R datasets. The drug data were obtained from the publicly available FDA FAERS database for the quarters 2014 Q1 – 2020 Q3 (for the processed contingency tables) and 2022 Q3 (raw incidence data), and the vaccine data were obtained from the FDA VAERS database for the year 1999; the package provides a convenient approach to accessing these processed data, particularly beneficial in methodological studies.\nWe note here that pvLRT is primarily an analysis package providing functions implementing statistical methodologies and subsequent post-processing and visualizations of results. The input data for the main analysis functions in pvLRT are always assumed to be pre-processed contingency tables (matrix-like objects) enumerating adverse event report counts with AEs along the rows and Drugs (or other medical products) along the columns. The package provides a convenience function to convert raw AE/drug incidence data (e.g., those downloaded from the FDA FAERS database) into analysis-ready processed contingency tables; however, no other functions for input raw data exploration and pre-processing, e.g. for filtering specific AEs or Drugs of interest or grouping specific AEs or drugs together are provided. We have deliberately made this design choice to (a) make the scope of the package well-defined, and (b) encourage the user to explore the raw adverse event data well before analyzing them. The modern R ecosystem contains several excellent general-purpose packages/package-collections, including data.table (Dowle and Srinivasan 2021) and tidyverse (Wickham et al. 2019) that provide a suite of principled and easy to use data pre-processing, munging, and visualization functions. We highly recommend leveraging these packages/functions to understand and preprocess the data well, before likelihood ratio tests are performed using pvLRT.\nThe purpose of the current article is to provide a high-level overview of the pvLRT package with detailed notes on its use, and guided examples with real world adverse event data exemplifying the use of LRT-based methodologies for pharmacovigilance. We note however that this article is not meant to serve as a comprehensive manual for pvLRT; all functions/objects provided in pvLRT come with detailed documentation, and this documentation serves as the definitive resource for pvLRT. It is also important to note that the LRT-based disproportionality analysis exemplified herein is only one part of the collective medical product/adverse event relationship assessment process. Current pharmacovigilance practice for medical product safety assessment includes review of summary statistics, disproportionality scores, cases summaries, and other sources of information about the medical products and the adverse events. The remainder of the article is organized as follows. We begin with a brief review of the LRT-based approaches to pharmacovigilance implemented in pvLRT. We then exemplify pvLRT by analyzing three sets of real pharmacovigilance datasets, two concerning drug safety and one concerning vaccine safety. We conclude the article with a brief discussion and some potential future directions.\n2 A brief review of the Likelihood Ratio test-based approaches to pharmacovigilance\nThis section reviews the underlying theories behind the likelihood ratio test (LRT)-based methods for drug safety assessment. We begin by fixing our notation. Consider a drug safety database cataloging \\(I\\) AEs detected among \\(J\\) medical products. Hereinafter we describe the methodologies in terms of ‘drugs’ as the medical products of concern; however, the methodologies trivially extend to other medical products including vaccines and medical devices. Let \\(n_{ij}\\) denote the number of reported cases for the \\(i\\)-th AE in the \\(j\\)-th drug. For the \\(I \\times J\\) contingency table \\((n_{ij})\\), let \\(n_{i\\bullet} = \\sum_{j=1}^{J} n_{ij}\\) and \\(n_{\\bullet j} = \\sum_{i=1}^{I} n_{ij}\\) denote the \\(i\\)-th row total and \\(j\\)-th column total respectively, \\(i = 1, \\dots, I\\); \\(j = 1, \\dots, J\\); and let \\(n_{\\bullet \\bullet}\\) denote the grand total. Interest lies in determining whether the observed report count \\(n_{ij}\\) for \\((i, j)\\)-th AE/drug pair is substantially larger than what is expected had there been no association between the \\(i\\)-th AE and the \\(j\\)-th drug. Within the null hypothesis significance testing framework this is achieved by testing a hypothesis of absence of a ``signal” against that of its presence at the corresponding AE/drug pair. We formally define these hypotheses and the underlying parametric models below.\nModel and Parametrization for LRT-based pharmacovigilance.\nA Poisson log-linear model for LRT-based SDR detection has been proposed in the literature to parametrize AE/drug associations based on the observed report counts \\(\\{n_{ij}\\}\\). At the outset we note that these AE/drug associations can also be studied through a series of logistic regression models each with binary occurrences of each individual AE as the response and the presence/absence of various drugs (or other medical products) as predictors. When a Poisson model is assumed for the report counts \\(\\{n_{ij}\\}\\), the resulting log-linear model and the collective logistic regression models lead to identical maximum likelihood-based inferences (Agresti 2013). However, the log linear model is more flexible than the logistic models in that, by relaxing the underlying Poisson assumption for the report counts, it can handle a richer set of data. Two different parametrizations, namely the reporting proportion (Huang et al. 2011) and the relative reporting proportion (Dumouchel 1999), have been considered within the log-linear modeling framework for pharmacovigilance. These parametrizations differ by how they define and handle “signals”. In the reporting proportion parametrization a signal is determined at a specific AE for a given drug if its reporting proportion is substantially larger than the overall reporting proportions of all other AEs combined. In contrast, in the relative reporting proportion parametrization one focuses on the relative reporting proportion of a specific AE for a given drug and tests whether it is substantially larger than 1. The latter parametrization does not require consideration of the category of “all other AEs combined” explicitly in each comparison, and hence has a simpler interpretation. In applications the two parametrizations incur similar computational expenses, and produce virtually identical results when the true data generating process is Poisson. However, the results may vary substantially when there is a model misspecification, including the presence of zero-inflation, i.e., excess zero-report counts that are inherently different from the sampling zero counts governed by the Poisson law and are produced by an exogenous process. A note on different types of zero-inflation possibly occurring in adverse event datasets is provided in the next paragraph. Our recent methodological study (Chakraborty et al. 2022) has shown that the relative reporting proportion-based Poisson LRT is more robust against zero-inflated data than its reporting proportion based counterpart. Extensions of the Poisson LRTs have been proposed to explicitly handle excess zeros under a zero-inflated Poisson (ZIP) model. The relative reporting proportion parametrization, within a pseudo likelihood ratio testing framework, aids a straightforward extension to the ZIP model that adds only a small overhead (one single additional optimization of a smooth univariate function) to the overall computational burden, and produces a test statistic whose functional form is identical to the ordinary (non-zero inflated) Poisson LRT. This computational simplicity makes the approach highly scalable for large data sizes, similar to the Poisson LRT. Collectively, these simplifications aid the construction of a unified LRT framework based on the relative reporting proportion-based parametrization that can simultaneously handle a Poisson model and a ZIP model, with near identical computational burden. By contrast, the reporting proportion-based LRT under a ZIP model suffers from a substantially increased computation burden (Huang et al. 2017) which hinders its scalability. For these reasons, both in this article and in the pvLRT package we primarily focus on the relative reporting proportion parametrization; we do note however that the pvLRT package does provide an implementation of the reporting proportion parametrization under an ordinary Poisson model setup.\nA brief note on excess zeros or zero-inflation in medical product safety data.\nWe now briefly note how zero-inflation occurs in pharmacovigilance. In adverse event reporting datasets, an important source of excess zero report counts is structural zeros associated with AE/drug pairs that are physically impossible to occur. If structural zeros are present, then the corresponding cells in a contingency table enumerating the number of reported cases of various AE/drug pairs will always be zero, regardless of the total number of reported counts. Moreover, inference on these structural zero positions can be made from the observed data: intuitively, a cell with a positive report count in any observed table cannot be a structural zero, and only an observed zero cell can be a structural zero. This is in contrast with another source of zero-inflation, namely data corruption, that may also occur in adverse events data. Here, due to noise in data recording process some report counts in the AE/drug contingency tables are randomly recorded as zero. Under this setting, there is no population level true zero positions, and the zero inflated positions vary from one sample to another. In this article we primarily focus on the structural zero-type zero-inflation, as it is the primary type of zero-inflation that the FAERS datasets stored in pvLRT feature. Indeed, the FAERS data undergo several rounds of rigorous reviews and checks to safeguard against data corruption. However, we note that pvLRT does provide functions for handling corruption-type zero-inflation; interested readers are referred to the the documentation of the pvlrt() function.\nParametric hypothesis testing for signals in pharmacovigilance based on a Zero-inflated Poisson (ZIP) model.\nTo facilitate a unified treatment we consider a zero-inflated Poisson model to describe the test procedure. A discrete random variable \\(X\\) is said to follow a ZIP\\((\\theta, \\omega)\\) distribution where \\(\\omega \\in [0, 1)\\) and \\(\\theta > 0\\) if the probability mass function of \\(X\\) is given by \\[\n\\operatorname{P}(X = x) = \\begin{cases}\n\\omega +  (1 - \\omega) \\exp(-\\theta), & x = 0 \\\\\n(1 - \\omega) \\exp(-\\theta) \\frac{\\theta^x}{x!}, & x = 1, 2, 3, \\dots\n\\end{cases},\n\\] i.e., if the distribution of \\(X\\) is a \\((1-\\omega)\\) and \\(\\omega\\) mixture of a Poisson(\\(\\theta\\)) distribution and a point mass at zero. The model reduces to an ordinary Poisson model when the zero-inflation probability \\(\\omega = 0\\). For adverse event data report counts we consider the following zero-inflated Poisson model for the observed count \\(n_{i j}\\) for the \\(i\\)-th AE of a specific drug \\(j\\): \\(n_{i j} \\sim \\text{ZIP}(\\lambda_{i j} \\times E_{i j}, \\omega_{j})\\), where \\(E_{i j} = n_{i \\bullet} n_{\\bullet j}/n_{\\bullet \\bullet }\\) is the expected number of reports for the \\((i, j)\\)-th pair when there is no association between the pair, and \\(\\lambda_{i j}\\) is the relative reporting proportion. Under this set up, null hypothesis significance tests are performed to determine signals at specific pairs. Suppose we are interested in determining significant adverse events among \\(K\\) out of the \\(J\\) drugs, labeled \\(1, \\dots, K\\). The global null hypothesis is \\(H_0^{1:K}: \\{\\lambda_{i j} = 1\\) for all \\(i = 1, \\dots, I\\) and \\(j = 1, \\dots, J\\)} and is tested against the global alternative \\(H_a^{1:K} = \\{ \\lambda_{i j} > 1\\) for at least one \\((i, j): i = 1, \\dots, I; j = 1, \\dots, K\\}\\) which represents the hypothesis of ``at least one signal’’. If the global null hypothesis is rejected in favor of the global alternative, our focus then shifts on identifying the individual AE/drug pairs with strong associations. This is achieved by post hoc test of the global null hypothesis against the individual alternative hypotheses \\(H_{a, i j}: \\lambda_{i j} > 1\\).\nPseudo Likelihood Ratio Test.\nA pseudo likelihood ratio test of the above hypotheses involves computation of the individual pseudo likelihood ratios \\[\\begin{equation*}\n{\\text{LR}}_{i j} =\n\\begin{cases}\n1 & n_{i j} = 0 \\\\\n\\exp(-(\\hat \\lambda_{i j}-1) E_{i j}) \\hat \\lambda_{i j}^{n_{i j}} & n_{i j} > 0\n\\end{cases},\n\\end{equation*}\\] where \\(\\hat \\lambda_{i j} = \\max\\{n_{i j}/E_{i j}, 1\\}\\) is the maximum likelihood estimator of \\(\\lambda_{i j}\\) under \\(H_0^{1:K} \\cup H_a^{1:K}\\), and subsequently, the maximum likelihood ratio statistic \\(\\text{MLR}^{1:K} = \\max_{i = 1, \\dots, I; j = 1, \\dots, K} \\text{LR}_{i j}\\). The global null is rejected in favor of the global alternative hypothesis if \\(\\text{MLR}^{1:K}\\) is large, in which case subsequent post hoc tests of individual alternatives \\(H_{a, i j}\\) can be performed based on the observed values of \\({\\text{LR}}_{i j}\\). Computation of the global and post hoc \\(p\\)-values are described in the following paragraph. Note that the computed or “observed” values of \\({\\text{LR}}_{i j}\\) aid rigorous statistical quantification of the signal strength in a pair \\((i, j)\\). Thus, we can use these quantities to rank the individual AE/drug pairs in terms of their signal strengths detected in the dataset under consideration. This is particularly useful for determining, the “most prominent” adverse events for a specific drug or a groups of drugs as detected in a dataset. Note also that the functional form of \\(\\text{LR}_{i j}\\) and hence that of \\(\\text{MLR}^{1:K}\\) does not contain the zero-inflation parameters \\({\\omega_j}\\). However, the null sampling distributions necessary for performing the hypothesis tests do involve them. Computational approximations of these null sampling distributions are described next.\nBootstrapped null sampling distribution of the pseudo likelihood ratio test statistic, and computing \\(p\\)-values.\nThe null sampling distributions of the individual likelihood ratio statistics \\(\\{\\text{LR}_{i j}\\}\\) and the maximum likelihood ratio statistic \\(\\{\\text{MLR}^{1:K}\\}\\) are analytically intractable, and they must be approximated using computational techniques to facilitate inference. In pvLRT we use a parametric bootstrap resampling scheme for this purpose (Davison and Hinkley 1997). To this end, zero-inflated Poisson models are fitted to the observed data under the global null hypothesis \\(H_{0, ij}^{1:K}\\), and null resampled counts \\(\\{\\tilde n_{i j}\\}\\) are subsequently generated from the fitted model. Fitting zero-inflated Poisson models requires estimation of the zero-inflation parameters \\(\\{\\omega_j\\}\\) from the observed data. In pvLRT the maximum likelihood estimates of \\(\\{\\omega_j\\}\\) are used for this purpose, which are obtained by maximizing the profile likelihood for \\(\\omega_j\\):\\[\nl_j(\\omega) = (I - I_{0j}) \\log (1 - \\omega) + \\sum_{\\{i: n_{i j} = 0\\}} \\log \\left(\\omega + (1-\\omega) \\exp(-E_{i j}) \\right),\n\\] for \\(j=1, \\dots, J\\). Here \\(I_{0 j}\\) denotes the number of zero \\(n_{ij}\\) values for each drug \\(j\\). The conditional (posterior) probability of encountering a zero-inflation at the \\((i, j)\\) pair, given the corresponding observed count \\(n_{ij}\\) is subsequently estimated and computed as: \\[\\begin{aligned}\n\\hat \\eta_{i j}^\\text{obs}=\n\\begin{cases}\n0, & n_{i j} > 0 \\\\\n\\frac{\\hat \\omega_{j}^\\text{obs}}{\\hat \\omega_{j}^\\text{obs} + (1-\\hat \\omega_{j}^\\text{obs})\\exp( - E_{i j})} & n_{i j} = 0\n\\end{cases},\n\\end{aligned}\n\\] for all \\(i = 1, \\dots, I\\) and \\(j = 1, \\dots, J\\). Then, using these estimated conditional zero-inflation probabilities, null parametric bootstrap resamples are generated from \\(\\tilde n_{i j} \\sim \\text{ZIP}(E_{i j}, \\hat \\eta_{i j})\\). Under an ordinary Poisson model, the parameters \\(\\omega_{j}\\) and \\(\\eta_{i j}\\) do not arise (they are equal to zero from a ZIP model perspective), and the null bootstrap resamples are simply generated from \\(\\tilde n_{i j} \\sim \\text{Poisson}(E_{i j})\\); hence, the bootstrap null resamples become exact null resamples. In either case, these null resamples \\(\\tilde n_{ij}\\) produce null resampled values of the test statistic \\(\\text{MLR}^{1:K}\\), and they collectively approximate the corresponding sampling distributions. The \\(p\\)-value for the global null against the global alternative hypothesis is \\(P^{1:K} = \\operatorname{P}(\\text{MLR}^{1:K}\\geq \\text{MLR}^{1:K, \\text{obs}} \\mid H_{0}^{1:K})\\), and can be approximated by the proportion of the null resampled \\(\\text{MLR}^{1:K}\\) values that exceed the observed value of \\(\\text{MLR}^{1:K, \\text{obs}}\\): \\[\nP^{1:K, \\text{computed}} = \\frac{1 + \\sum_{h=1}^M \\mathbb{1}\\left(\\widetilde{\\text{MLR}}^{1:K, (h)} \\geq \\text{MLR}^{1:K, \\text{obs}} \\right)}{M+1},\n\\] where \\(\\{\\widetilde{\\text{MLR}}^{1:K, (h)}: h = 1, \\dots, M\\}\\) are bootstrapped resampled null values of \\(\\text{MLR}^{1:K}\\). The global null can be rejected at level \\(\\alpha\\) in favor of the global alternative hypothesis if \\(P^{1:K, \\text{computed}} < \\alpha\\). The post hoc \\(p\\)-values \\(\\{P_{ij} = \\operatorname{P}(\\text{MLR}^{1:K}\\geq \\text{LR}_{ij}^{\\text{obs}} \\mid H_{0}^{1:K}) \\}\\) for the individual alternative hypotheses \\(\\{H_{a, i j}\\}\\) are also approximated using the same null resampled values of \\(\\text{MLR}^{1:K}\\): \\[\nP^{\\text{computed}}_{ij} = \\frac{1 + \\sum_{h=1}^M \\mathbb{1}\\left(\\widetilde{\\text{MLR}}^{1:K, (h)} \\geq \\text{LR}_{ij}^{\\text{obs}} \\right)}{M+1}.\n\\] Note that these post hoc \\(p\\)-values are based on the null sampling distribution of the same random test statistic \\(\\text{MLR}^{1:K}\\); only the thresholds where the tail probabilities are evaluated are different (the individual observed \\(\\text{LR}_{ij}^{\\text{obs}}\\) values). Consequently, no inflation in the overall type I error occurs due to multiplicity of hypothesis tests with independent test statistics. This also ensures that the false discovery rate of the entire procedure is controlled at the same level as the overall type I error.\nModel selection: determining whether to use a Poisson model or a ZIP model on a given dataset.\nAs noted before, the ZIP model above assumes that the report counts for a specific drug \\(j\\) are zero-inflated with probability \\(\\omega_{j}\\). If \\(\\omega_{j}\\) is small, then the ZIP model effectively reduces to an ordinary Poisson model; however in general, the ZIP model produces probabilities that are different from the Poisson probabilities. A natural question of particular practical importance is therefore which of these two models should be used on a given dataset. Clearly, if there is knowledge about the existence (or non-existence) of structural zero pairs in a given dataset (the exact positions of the structural zeros may possibly be unknown), then one should use a ZIP (ordinary Poisson) model. However, in practice no such knowledge may be available, and the optimal model must be determined entirely on the basis of the observed data. In other words, one must address what is known as a model selection problem. Because we are in a parametric likelihood framework, a convenient and canonical approach would involve the use of some penalized likelihood model selection criterion, such as the AIC or the BIC (Sakamoto et al. 1986) defined as:\\[\n\\operatorname{AIC}(k) = -2 \\log(\\hat{L}) + k p,\n\\] where \\(\\hat{L}\\) denotes the maximized value of the likelihood function for a given model, \\(k\\) is a penalty parameter, and \\(p\\) is the total number of parameters in the model. The choice \\(k = 2\\) produces the classical AIC, and \\(k = \\log N\\) where \\(N\\) denotes the total number of data points, produces the BIC. The model with the minimum AIC (or BIC) is regarded as optimal. Note that in the current context for both the Poisson and the ZIP models, \\(\\log(\\hat{L})\\) is obtained from the unrestricted model with each \\(\\{\\lambda_{ij}\\}\\) assuming values in \\([1, \\infty)\\); the total number of parameters are \\(p = I \\times J\\) in the Poisson model and \\((I + 1)J\\) in the ZIP model, and the total sample size \\(N\\) is the grand total \\(n_{\\bullet \\bullet}\\) of the contingency table.\nTesting for zero-inflation among individual drugs.\nThe above model selection procedure addresses the question of whether to use the ZIP model or an ordinary Poisson model for a given dataset. When a ZIP model is determined to be optimal, interest may then lie in determining the statistical strength/significance of individual zero-inflation parameters \\(\\{\\omega_{j}\\}\\) specific to the individual drugs. In a null hypothesis significance testing framework, this can be translated as a test of \\(H_{0, j^*}^\\omega: \\omega_{j^*} = 0\\) against \\(H_{a, j^*}^\\omega : \\omega_{j^*} > 0\\), for each drug \\(j^*\\). The likelihood ratio test statistic for this problem is \\(\\text{LR}_{j^*}^\\omega = \\exp \\left[l_j(\\hat \\omega_{j^*}) - l_j(0) \\right]\\), where \\(l_{j^*}(\\omega)\\) is the profile log likelihood function for \\(\\omega_{j^*}\\) as described before, and \\(\\hat \\omega_{j^*}\\) is the corresponding maximum likelihood estimator obtained by maximizing \\(l_{j^*}(\\omega)\\). Similar to the LRT for \\(\\lambda_{ij}\\) described before, the null sampling distribution of \\(\\text{LR}_{j^*}^\\omega\\) is analytically intractable, but can be approximated using a parametric bootstrap. To this end, null resamples \\(\\{\\tilde n_{ij^*}\\}\\) are generated from the \\(\\text{ZIP}(\\hat \\lambda_{i j^*} E_{i j^*}, \\omega_{j^*} = 0) \\equiv \\text{Poisson}(\\hat \\lambda_{i j^*} E_{i j^*})\\) distribution, where \\(\\hat \\lambda{i j^*} = \\max\\{n_{ij^*}/E_{ij^*}, 1\\}\\) is the maximum likelihood estimator of \\(\\lambda_{ij^*}\\). From these null resamples \\(\\{\\tilde{n}_{i j}\\}\\), null values of the likelihood ratio test statistics \\(\\{\\widetilde{\\text{LR}}_{j^*}^\\omega\\}\\) are computed. The \\(p\\)-value of the test of \\(H_{0, j^*}^\\omega\\) against \\(H_{a, j^*}^\\omega\\) is subsequently computed using Monte Carlo estimated probability (proportion) of null \\(\\{\\widetilde{\\text{LR}}_{j^*}^\\omega\\}\\) values that are bigger than or equal to the “observed” value \\(\\text{LR}_{j^*}^{\\omega,\\ \\text{obs}}\\). If zero-inflation in several drugs \\({j^*}\\) are tested simultaneously, then the overall \\(p\\)-value needs to be adjusted for multiplicity; in pvLRT the Benjamini-Hochberg adjusted \\(p\\)-values (\\(q\\)-values; Benjamini and Hochberg (1995)) are used by default for this purpose.\n3 Adverse Event Data Analysis with pvLRT\nIn this section we provide data analysis examples based on four real datasets contained in the pvLRT package. These datasets are stored as pre-processed contingency tables (in matrix-like forms) cataloging the aggregated report count for each AE/drug pair. To produce such contingency tables from raw adverse event data the pvLRT function convert_raw_to_contin_table() may be used. We note, however, that care must be taken before summarizing raw data into such contingency tables. First, a decision needs to be made regarding which reports to include in the analysis, e.g., only consider primary and/or secondary suspect drugs or also include concomitant and interacting drugs as cataloged in the FAERS database. The processed data stored in pvLRT only considers reports for the primary and secondary suspect drugs. Second, one needs to identify which drugs to include in the “Other Drug” category which plays the key role of “baseline” drugs. Appropriate specification of this baseline category is required because of its contribution to the total report counts for each AE, even though the AE/baseline drug category associations are not of primary importance. Typically, one identifies a handful of drugs of particular interest (e.g., the statins as discussed in the examples below) and combines all remaining drugs as baseline (Ding et al. 2020). Finally, one needs to determine which AEs to include in the testing. The proposed LRT-based methods permit strong control over the global type I error and false discovery rates while ensuring high power (Chakraborty et al. 2022), regardless of the total number of AEs. However, computation of the bootstrap p-values become increasingly more expensive with a growing number of AEs, and thus in practice one may encounter certain upper bounds to the total number of AEs that can be tested at a given time. We refer to Ding et al. (2020) for a detailed discussion on these considerations. The analyses provided below (divided into subsections) showcase examples with both small (\\(I = 46\\)) and large (\\(I \\approx 6000\\)) number of AEs. We start by loading the pvLRT package into in R using library().\n\n\nlibrary(pvLRT)\n\n\nAnalysis of the statin dataset with 47 adverse events\nWe begin with an analysis of the statin dataset labeled statin46, previously identified as being associated with the statin drugs. There are 47 rows in the data, corresponding to these 46 adverse events, and a collapsed category labeled “other” that consists of all other adverse events tabulated in the FAERS database during this time span. An older version of the data with the same 46 adverse events have been previously considered in (Ding et al. 2020); our analysis that follows corroborates findings from that study. To begin the analysis, we first load the data into an R session and view the first few rows and columns with the following codes:\n\n\ndata(\"statin46\")\nhead(statin46)[, 1:3]\n\n                                          Atorvastatin Fluvastatin\nAcute Kidney Injury                               1353          42\nAnuria                                              71           0\nBlood Calcium Decreased                             14           2\nBlood Creatine Phosphokinase Abnormal               34           0\nBlood Creatine Phosphokinase Increased            1175         125\nBlood Creatine Phosphokinase Mm Increased            2           0\n                                          Lovastatin\nAcute Kidney Injury                                7\nAnuria                                             0\nBlood Calcium Decreased                            0\nBlood Creatine Phosphokinase Abnormal              0\nBlood Creatine Phosphokinase Increased            32\nBlood Creatine Phosphokinase Mm Increased          0\n\nOur interest lies in finding the most important adverse events of these 6 statin drugs. We note at the outset that the adverse events potentially caused by these drugs cannot be considered in isolation due to similarity among functions of statins, and thus we must consider all 6 drugs simultaneously while determining their important adverse events. This is achieved by performing a simultaneous test for all drugs of interest collectively within an extended LRT framework, which ensures that the overall type-I error of the process is preserved and the false discovery rate is controlled. See the previous section for a note on the statistical properties of the extended LRT test.\nAnalysis based on an ordinary Poisson model.\nWe first perform an LRT analysis based on the ordinary Poisson model on this dataset. The R codes for performing this task are as follows.\n\n\nset.seed(100) \ntest_statin46_poisson <- pvlrt(   \n  statin46,   \n  zi_prob = 0,   \n  test_drug_idx = 1:6   \n)\n\n\nThe first argument specifies the contingency table on which the LRT is to be performed, which is statin46 in the current analysis. The argument zi_prob = 0 pre-specifies the zero-inflation probability in the model for each drug to be 0, thus reducing the model to an ordinary Poisson model. Note that the ordinary Poisson model-based test can also be invoked through the wrapper function lrt_poisson(). Under the hood, lrt_poisson() sets zi_prob = 0 and then calls pvlrt() itself. Next, the argument test_drug_idx = 1:6 indicates that hypothesis tests are to be performed only on the first 6 columns, i.e., excluding the last column for “other” drugs. By default, all drugs supplied in test_drug_idx are tested simultaneously as a single group/class in an extended LRT framework; if other class/group structures exist among the drugs or if the drugs are to be tested separately (i.e., each tested drug forms its own class) those can be passed through drug_class_idx.\nRunning the code above creates a pvlrt S3 object. A pvlrt object is simply a reclassified matrix of computed (log) LRTs; it has the same dimensions as the input contin_table, with each cell containing the computed test statistic value for the corresponding AE/Drug pair. However, in addition to the usual dimnames attributes, a pvlrt object has several other attributes including the p-values associated with these log LRTs and estimates for zero-inflation parameters, and some meta-data including the type of test performed, the input data, etc. The printing method for pvlrt objects provides a high-level textual summary of the test. To glance at the overall results from the above LR tests, we simply print test_statin46_poisson on the R console:\n\n\ntest_statin46_poisson\n\nRelative reporting rate (lambda)-based ordinary-LRT on 47 AE &\n7 drugs. Hypothesis tests performed on 6 drugs using\nparametric bootstrap.\n\nRunning time of the original pvlrt call: 1.475351 secs\n\nNo zi considered in the model.\nTotal 110 AE-drug pairs are significant at level = 0.05.\n\nExtract all LR statistics and p-values using `summary()`.\n\nThe above output shows that there are 110 significant AE/drug pairs at level \\(\\alpha = 0.05\\). The likelihood ratio test statistics and \\(p\\)-values for all AE/drug pairs can be extracted using summary(). This will produce a data table, with each row providing the sample size, likelihood ratio, and \\(p\\)-value for each AE/drug pair present in the data1:\n\n\n# summary(test_statin46_poisson) \n\n\nThe output is omitted for brevity. Note that for pairs that are not tested (here, all entries in the 7-th column of the statin46 table) the test results will be missing (denoted by NA in R). pvLRT currently has implementations for three visual summary methods for the test results, namely bubble-plot, barplot, and heatmap. These can either be accessed explicitly through the functions bubbleplot_pvrlt, barplot (which is an S3 method for pvlrt objects), and heatmap_pvlrt, or directly through the plot method for pvlrt objects with the specifications of type = \"bubbleplot\" or type = \"barplot\", or type = \"barplot\" respectively. As noted before in the Introduction, these plots are constructed using the ggplot2 package (Wickham 2016). As a result, several aspects of the produced plots can be updated post hoc, as facilitated by the principles of grammar of graphics that ggplot2 implements. Examples of resizing axes label text sizes for pvlrt plots are provided below.\nIn the following we first create a bubble-plot top 15 AE/drug pairs across all 6 statin drugs, with pairs being ranked by the magnitudes of their likelihood ratio test statistic values. The figure is stored in pl_bubble_statin46_poisson, which is visualized upon printing.\n\n\npl_bubble_statin46_poisson <- plot(\n  test_statin46_poisson,\n  type = \"bubbleplot\",\n  x_axis_measure = \"lrstat\",\n  size_measure = \"n\",\n  fill_measure = \"p_value\",\n  AE = 15\n)\n\n\nThe arguments used in plot() above are described as follows. The first two arguments specify the pvlrt object under consideration and the type of plot to be made. All three visualizations implemented (bubbleplot, barplot and heatmap) display the test results by plotting AEs across the rows and Drugs across the columns, in their respective plots. The arguments x_axis_measure = \"lrstat\", size_measure = \"n\", and fill_measure = \"p_value\" indicate that the log LRT values will be displayed along the x-axis, the circle sizes in the bubbleplot will represent the corresponding sample size of an AE/drug pair, and that the circles will be color coded according to the p-values of the corresponding test statistics, respectively. Note that the three dimensions (lrstat, n and p_value) displayed through these three arguments can be interchanged. The argument AE = 15 indicates that the top 15 adverse events with the largest log likelihood ratio test statistic values are to be plotted. Here these “largest” values are obtained by comparing across all drugs tested during the original pvlrt run. Instead of looking at “all” drugs while determining the top AEs, one can also focus on a specific subset of drugs by supplying the appropriate subset through the argument Drug. Moreover, instead of looking at the top adverse events, one can also explicitly specify which adverse events to display by directly passing their names through the same AE argument. When passing names through AE and Drug, partial string patterns can be supplied and then matched against their full names via regular expressions; this is achieved by setting grep = TRUE. For example, specifying AE = \"muscle\" and setting grep = TRUE will show all adverse events that have the character \"muscle\" in its name. We refer to the package documentation for further details on these two arguments. To visualize the plot produced, we can simply print pl_bubble_statin46_poisson on the R console or save it to a graphic device via ggsave. As noted before, post hoc editing of various graphical parameters in pl_heat_statin46_poisson can be made using ggplot2 functions. Note that to use ggplot2 functions, we need to load the ggplot2 package. Below we show how the axes label sizes and legend text sizes can be modified. (Changing the default labels etc. may cause ggplot2 to throw some warnings). We refer to the ggplot2 package documentations and vignettes for further details on modifications of ggplot2 objects and saving ggplot2 objects to graphic devices.\n\n\nlibrary(ggplot2)\npl_bubble_statin46_poisson +\n  theme(\n    axis.text.y = element_text(size = 13, face = \"bold\"),\n    axis.text.x = element_text(size = 9, face = \"bold\"),\n    legend.title = element_text(size = 12, face = \"bold\"),\n    strip.text = element_text(size = 14, face = \"bold\"),\n    legend.position = \"top\"\n  )\n\n\n\nThe above bubbleplot visualizes three simultaneous dimensions, viz., lrstat, n and p_value along the \\(x\\)-axis, circle size, and circle color respectively. An alternative mode of visualization is made by paneled barplots which can visualize two simultaneous dimensions, e.g., the \\(x\\)-axis showing the log likelihood ratio statistic, and the colors representing p-values as in the bubbleplot. The sample sizes can be displayed on the bar as text, by specifying show_n = TRUE. The following R codes create the barplot.\n\n\npl_bar_statin46_poisson <- plot(\n  test_statin46_poisson,\n  type = \"barplot\",\n  fill_measure = \"p_value\",\n  x_axis_measure = \"lrstat\",\n  AE = 15,\n  show_n = TRUE,\n  border_color = \"black\"\n) \npl_bar_statin46_poisson +\n  theme(\n    axis.text.y = element_text(size = 13, face = \"bold\"),\n    axis.text.x = element_text(size = 13, face = \"bold\"),\n    axis.title.x = element_text(size = 14, face = \"bold\"),\n    legend.title = element_text(size = 12, face = \"bold\"),\n    strip.text = element_text(size = 14, face = \"bold\"),\n    legend.position = \"top\"\n  )\n\n\n\nFinally, we can visualize only one dimension, say p_value as cell-colors in a heatmap, and inscribe as texts the other two dimensions, i.e., lrstat and n (by specifying show_n = TRUE and show_lrstat = TRUE). The following R codes perform this task.\n\n\npl_heat_statin46_poisson <- plot(\n  test_statin46_poisson,\n  type = \"heatmap\",\n  fill_measure = \"p_value\",\n  AE = 15,\n  show_n = TRUE,\n  show_lrstat = TRUE,\n  border_color = \"black\"\n)\npl_heat_statin46_poisson +\n  theme(\n    axis.text = element_text(size = 13, face = \"bold\"),\n    legend.title = element_text(size = 12, face = \"bold\"),\n    strip.text = element_text(size = 14, face = \"bold\"),\n    legend.position = \"top\"\n  )\n\n\n\nNote that all three plots above can be made interactive using external R packages; consider, e.g., plotly::ggplotly(pl_bubble_statin46_poisson). Such interactive plots can be particularly useful in exploratory analyses performed on large adverse event datasets. We refer to the plotly (Sievert 2020) package documentation for further details on plotly::ggplotly and other interactive plotting functions. An interactive version of the above figure is displayed below.\n\n\n## install plotly if not available\n# if (!requireNamespace(\"plotly\")) install.packages(\"plotly\")\n# interactive heatmap. turn all inscribed texts off\nplotly::ggplotly(pl_heat_statin46_poisson)\n\n\n\nThe following observations are made from the above plots. First, Myalgia appears to be the AE with the largest computed log likelihood ratio (LR) statistic across all 6 statin drugs combined. It has the largest LR in Atorvastatin, followed by Simvastatin, Rosuvastatin, Pravastatin, Fluvastatin, and Lovastatin. Next is Rhabdomyolysis, which displays a near similar pattern as Myalgia in terms of computed signal/association strength among the six drugs, except here Lovastatin has a slightly larger computed LR value than Fluvastatin. Second, not all of the 15 adverse events are statistically significant across all six statin drugs. For example, Necrotising Myositis is a statistically significant adverse event of Atorvastatin, Simvastatin, and Rosuvastatin, but not for Pravastatin, Fluvastatin, and Lovastatin. Third, the sample size \\(n_{i j}\\) of a specific AE/drug pair \\((i, j)\\) alone does not provide much information about its strength of association; for example, the pairs (Muscle Necrosis, Rosuvastatin), (Myositis, Lovastatin), (Chromaturia, Fluvastatin), and (Necrotising Myositis, Rosuvastatin) all have sample sizes \\(n_{ij} = 10\\), but their computed likelihood ratio statistics, and hence the strengths of their associations vary. Moreover, a pair with a smaller sample size may harbor a stronger signal than one with a larger sample size; for example for Fluvastatin, Chromaturia has a sample size of \\(n_{ij} = 10\\) and Myoglobin Blood Increased has a sample size of \\(n_{ij} = 4\\); yet the former AE has a smaller LRT statistic value than the latter.\nAnalysis based on a zero-inflated Poisson model.\nWe now consider zero-inflated Poisson model-based LRTs to identify signals in the statin46 data. To this end, the same pvlrt function can be used, with two changes in the parameters: (a) the argument zi_prob is now set to NULL, which ensures that the zero-inflation parameters are estimated from the data, and (b) the logical argument test_zi is now set to TRUE, which specifies that a separate (pseudo) likelihood ratio test for the significance of each drug-specific zero inflation parameter will be performed. The following are the R codes used:\n\n\nset.seed(100) \ntest_statin46_zip <- pvlrt(   \n  statin46,   \n  test_drug_idx = 1:6,   \n  zi_prob = NULL,\n  test_zi = TRUE\n)\n\n\nA textual summary of the analysis can be obtained by printing the output of test_statin46_zip on the R console:\n\n\ntest_statin46_zip\n\nRelative reporting rate (lambda)-based pseudo-LRT on 47 AE & 7\ndrugs. Hypothesis tests performed on 6 drugs using parametric\nbootstrap.\n\nRunning time of the original pvlrt call: 6.557479 secs\n\nDrug-specific estimated zi probabilities: 0 (Atorvastatin),\n0.07 (Fluvastatin), 0.16 (Lovastatin), 0.06 (Pravastatin), 0\n(Rosuvastatin), 0 (Simvastatin), 0 (Other)\n\nLR test of significance for zi probabilities: LRstat= 0.00,\nq>0.90 (Atorvastatin), LRstat= 5.28, q<0.001 (Fluvastatin),\nLRstat= 3.81, q<0.001 (Lovastatin), LRstat=32.49, q<0.001\n(Pravastatin), LRstat= 0.00, q>0.90 (Rosuvastatin), LRstat=\n0.00, q>0.90 (Simvastatin), LRstat= 0.00, q=<NA> (Other)\nTotal 110 AE-drug pairs are significant at level = 0.05.\n\nExtract all LR statistics and p-values using `summary()`.\n\nIt follows from the output that the estimated drug-specific zero-inflation probabilities are significantly different from zero (q-value or FDR adjusted \\(p\\)-value < 0.05) only for Fluvastatin and Lovastatin, where the point estimates of the zero-inflation parameters are \\(\\omega = 0.07\\) and \\(\\omega = 0.16\\) respectively. The total number of significant AE/drug is 110, which is the same as the number of significant pairs detected from the Poisson model. To see which 110 pairs detected to be significant in the two models, we can use the function extract_significant_pairs to extract a data.table containing the test results of significant AE/drug pairs and at a given level \\(\\alpha = 0.05\\) from the two models, and subsequently compare these results.\n\n\n# extract_significant_pairs(test_statin46_poisson)\n# extract_significant_pairs(test_statin46_zip)\nall.equal(\n  extract_significant_pairs(test_statin46_poisson),\n  extract_significant_pairs(test_statin46_zip)\n)\n\n[1] \"Column 'p_value': Mean relative difference: 0.06278027\"\n\nAs it appears the above two pvlrt objects differ only in the computed \\(p\\)-values, but not in any other fields/statistics. That the computed LR statistics are identical is not surprising, since the functional form of the test statistic is exactly the same for the two models. The \\(p\\)-values, on the other hand, depend on the null sampling distributions of LR statistics, which are different for the two models. Moreover, here the \\(p\\)-values are numerically computed using Monte Carlo methods based on bootstrap resampling, which means that even for the same null sampling distributions, the computed \\(p\\)-values may show random variations between two instances of the computation. However, that there is no difference between the identified significant (computed \\(p\\)-value < 0.05) AE/drug pairs for the two models essentially indicates concordance of the signals identified.\nWhile the two models identify the same AE/drug pairs to be significant at the \\(0.05\\) level, they may differ in terms of predictive accuracy/goodness of fit, which can be compared through model selection criteria such as BIC. The logLik methods for pvlrt objects implemented in the package (see the documentation for logLik.pvlrt) allow automated computations of the BIC and AIC of the fitted models, using the stats S3 functions BIC and AIC. Below we use these two functions to compare the predictive accuracies of the fitted Poisson and ZIP models.\n\n\ncbind(BIC(test_statin46_poisson, test_statin46_zip), \n      AIC(test_statin46_poisson, test_statin46_zip))[, -3] # remove duplicated df col\n\n                       df      BIC      AIC\ntest_statin46_poisson 329 15962.47 10707.02\ntest_statin46_zip     336 16005.13 10637.85\n\nIt follows that the Poisson model has smaller BIC but larger AIC than the ZIP model for the current dataset. This essentially implies that the Poisson model and the ZIP model both perform equally well in modeling the statin46 dataset.\nAnalysis of the full statin data with 6039 adverse events\nWe next analyze the full statin dataset with all 6039 adverse events. This dataset differs from the smaller statin46 dataset in that it explicitly catalogs report-counts of many adverse events that are grouped together as “Other AE” in statin46. The first few rows of the bigger statin dataset can be displayed using head(statin); we omit the output for brevity.\n\n\ndata(\"statin\")\n# head(statin)\n\n\nIn the following we first run a Poisson LRT, followed by a ZIP LRT, and then compare the two models. The model that fits the data better (assessed through BIC/AIC) will subsequently be used for inference. The following commands run a Poisson LRT on the statin dataset, with the default 10,000 resamples for computation of \\(p\\)-values. The remaining arguments are analogous to the case of analysis of statin46. Once created, the pvlrt objects will output brief textual summary of the results; the outputs are omitted for brevity.\n\n\nset.seed(100)\ntest_statin_poisson <- pvlrt(\n  statin,\n  zi_prob = 0,\n  test_drug_idx = 1:6\n)\n# test_statin_poisson # print the results\n\n\nNext we run the zero-inflated Poisson model. Codes are as follows.\n\n\nset.seed(100)\ntest_statin_zip <- pvlrt(\n  statin,\n  test_drug_idx = 1:6,\n  zi_prob = NULL,\n  test_zi = TRUE\n)\n# test_statin_zip # print the results\n\n\n\n\n\n\nAs it appears, in this bigger dataset all drugs (except the “other” group) have > 10% zero inflation probabilities with statistically significant FDR adjusted \\(p\\)-values (\\(q\\)-values). This indicates that the zero-inflated Poisson model based test would likely be more appropriate here. We formally check this via model comparison using AIC and BIC as follows:\n\n\ncbind(BIC(test_statin_poisson, test_statin_zip),\n      AIC(test_statin_poisson, test_statin_zip))[, -3] #remove duplicated df col\n\n                       df     BIC      AIC\ntest_statin_poisson 42273 1063654 388384.2\ntest_statin_zip     42280 1043314 367931.7\n\nIt follows that here the zero-inflated Poisson model has both smaller AIC and BIC than the Poisson model, thus confirming our suspicion that the former model is a better fit for the data. We shall therefore conduct the subsequent analysis based on the zero-inflated Poisson model-based tests.\nTo get a visual understanding of the results we plot a heatmap of the \\(p\\)-values with LR test statistics and samples sizes inscribed as texts:\n\n\npl_heat_statin_zip <- heatmap_pvlrt(\n  test_statin_zip,\n  AE = 15,\n  show_n = FALSE,\n  show_lrstat = FALSE\n) \nplotly::ggplotly(pl_heat_statin_zip)\n\n\n\nThe following observations are made. First, (Type 2 Diabetes Mellitus, Atorvastatin) appears to be the most prominent association in terms of LR statistic (log LR = 8.0632317^{4}) across all AE/drug pairs. The next four prominent signals are observed in (Myalgia, Atorvastatin), (Myalgia, Simvastatin), (Rhabdomyolysis, Atorvastatin), and (Myalgia, Rosuvastatin) respectively.\nAnalysis of the full gbca data with 1707 adverse events\nWe next analyze the full gbca dataset with all 1707 adverse events and 9 drugs. This example arises in the context of an FDA evaluation of Gadolinium-based contrast agents (GBCA) from which FDA concluded there were no adverse events from GBCA retention (https://www.fda.gov/drugs/postmarket-drug-safety-information-patients-and-providers/information-gadolinium-based-contrast-agents). Several disproportionality methods were compared in the FAERS database (Zhao et al. 2018) using the GBCAs as an example. We use this example to further illustrate the LRT methods, and assess their computational efficiency in datasets with many zero-report counts. The observed proportions of zero counts are noticeably large in most drugs:\n\n\ndata(\"gbca\")\n# head(gbca) # show the first few rows\n\n\n\n\nobs_prop_0 <- apply(gbca, 2, function(x) mean(x == 0))\nround(obs_prop_0, 2)\n\n     Gadobenate      Gadobutrol     Gadodiamide    Gadofosveset \n           0.60            0.42            0.71            0.99 \n  Gadopentetate      Gadoterate     Gadoteridol Gadoversetamide \n           0.61            0.51            0.70            0.87 \n     Gadoxetate           Other \n           0.82            0.00 \n\nThese large proportions of zero counts provide a heuristic justification for using a ZIP-model based LRT in this example: because the total zero counts comprise both true/structural zero counts (zero-inflation) and sampling zero counts, some substantial parts of these observed zeros may be attributable to zero inflation. To formalize this intuition, in the following we first run a Poisson LRT, followed by a ZIP LRT, and then compare goodness of fits for the two models in the dataset. The model that fits the data better (assessed through BIC/AIC) will be subsequently used for inference. We begin with the Poisson LRT, with the default 10,000 resamples for computation of \\(p\\)-values. The codes are as follows. The resulting output, when printed on console will display textual summary of test results which we omit for brevity.\n\n\nset.seed(100)\ntest_gbca_poisson <- pvlrt(\n  gbca,\n  zi_prob = 0,\n  test_drug_idx = 1:9\n)\n# test_gbca_poisson ## print results\n\n\n\n\n\n\nNext we run the zero-inflated Poisson model. Codes are as follows.\n\n\nset.seed(100)\ntest_gbca_zip <- pvlrt(\n  gbca,\n  test_drug_idx = 1:9,\n  zi_prob = NULL,\n  test_zi = TRUE\n)\n\n\n\n\n# print results\ntest_gbca_zip\n\nRelative reporting rate (lambda)-based pseudo-LRT on 1707 AE &\n10 drugs. Hypothesis tests performed on 9 drugs using\nparametric bootstrap.\n\nRunning time of the original pvlrt call: 1.048432 mins\n\nDrug-specific estimated zi probabilities: 0.39 (Gadobenate),\n0.2 (Gadobutrol), 0.52 (Gadodiamide), 0.44 (Gadofosveset),\n0.41 (Gadopentetate), 0.26 (Gadoterate), 0.32 (Gadoteridol),\n0.76 (Gadoversetamide), 0.66 (Gadoxetate), 0 (Other)\n\nLR test of significance for zi probabilities: LRstat=2318.57,\nq<0.001 (Gadobenate), LRstat=1377.85, q<0.001 (Gadobutrol),\nLRstat=2671.46, q<0.001 (Gadodiamide), LRstat= 4.23, q=0.13\n(Gadofosveset), LRstat=2112.43, q<0.001 (Gadopentetate),\nLRstat=1160.36, q<0.001 (Gadoterate), LRstat= 433.91, q<0.001\n(Gadoteridol), LRstat=3671.24, q<0.001 (Gadoversetamide),\nLRstat=2284.60, q<0.001 (Gadoxetate), LRstat= 0.00, q=<NA>\n(Other)\nTotal 203 AE-drug pairs are significant at level = 0.05.\n\nExtract all LR statistics and p-values using `summary()`.\n\nIt follows that several drug groups have substantially large (> 40%) zero inflation probabilities with statistically significant FDR adjusted \\(p\\)-values (\\(q\\)-values). This agrees with the earlier exploratory observation of large proportions of observed zeros. Similar to the full statin data analysis, it therefore seems that the zero-inflated Poisson model would be more appropriate here. We use AIC and BIC to formally check this:\n\n\ncbind(BIC(test_gbca_poisson, test_gbca_zip),\n      AIC(test_gbca_poisson, test_gbca_zip))[, -3] # remove \n\n                     df      BIC      AIC\ntest_gbca_poisson 17070 416208.7 147441.5\ntest_gbca_zip     17080 384316.9 115392.2\n\nIndeed, both AIC and BIC are smaller for the zero-inflated Poisson model than the Poisson model. We therefore use the former model for subsequent analysis.\nTo get a visual understanding of the results we plot a heatmap of the \\(p\\)-values with LR test statistics and samples sizes inscribed as texts:\n\n\npl_heat_gbca_zip <- heatmap_pvlrt(\n  test_gbca_zip,\n  AE = 15,\n  show_n = FALSE,\n  show_lrstat = TRUE\n) \nplotly::ggplotly(pl_heat_gbca_zip)\n\n\n\nThe following observations are made. First, (Contrast Media Deposition, Gadopentetate) appears to be the most prominent AE/drug pair in terms of LR statistic (log LR = 2084.09) across all AE/drug pairs. The next four prominent signals are observed in (Gadolinium Deposition Disease, Gadopentetate), (Gadolinium Deposition Disease, Gadodiamide), (Gadolinium Deposition Disease, Gadoversetamide), and (Gadolinium Deposition Disease, Gadobenate) respectively.\nAnalysis of the rotavirus vaccine datasets rv, rvold, and rvyoung\nFinally, to exemplify the use of pvLRT in analyzing vaccine safety we consider rotavirus vaccine datasets. These vaccines are an important example for vaccine safety assessment. Intussusception, a telescoping of the intestines that can cause serious illness, had led to the withdrawal of the first rotavirus vaccines from the market. Several AEs have since been identified in analyses of the FDA/CDC VAERS data (Niu et al. 2001; Haber et al. 2004) as being associated with intusscusception. Below in our analysis we consider several of these vaccine/AE combinations; these combinations are well-known and well-studied and hence they aid informative assessment of the performance of the disproportionality methods. In pvLRT, the rotavirus vaccine datasets are obtained from the FDA VAERS database for the year of 1999, and they summarize adverse event report counts for two vaccine groups, namely the rotavirus vaccine (“Rv”) and “Other” vaccines (37 different vaccines combined). pvLRT contains three such datasets namely rvold, rvyoung, and rv tabulating the number of occurrences of 727, 346, and 794 AEs among individuals of “old” (age \\(\\geq 1\\) year), “young” (age \\(< 1\\) year), and “combined” (collective old and young) age groups respectively, with most counts in the combined dataset arising from the young age group. Our interest here lies in identifying the most prominent AEs of the rotavirus vaccine and then comparing them across the three datasets/age groups. The rotavirus data has been previously analyzed in the literature using other approaches such as the EBGM (Niu et al. 2001) and proportional morbidity analysis for vaccine safety (Haber et al. 2004). Here we will utilize the LRT method as implemented in pvLRT: first, in each dataset we will perform an LRT with an optimally selected model (Poisson or ZIP); subsequently, based on the LRT results we will determine a few prominent AEs with the largest LR statistics in each dataset and compare these AEs across datasets.\nWe first fit Poisson and ZIP models to the three datasets:\n\n\nset.seed(100)\ntest_rv_poisson_list <- lapply(\n  list(old = rvold, young = rvyoung, combined = rv),\n  function(this_data) {\n    pvlrt(\n      this_data,\n      zi_prob = 0,\n      test_drug_idx = 1\n    )\n  }\n)\n\n\n\n\nset.seed(100)\ntest_rv_zip_list <- lapply(\n  list(old = rvold, young = rvyoung, combined = rv),\n  function(this_data) {\n    pvlrt(\n      this_data,\n      test_drug_idx = 1,\n      zi_prob = NULL\n    )\n  }\n)\n\n\nThe objects test_rv_poisson_list and test_rv_zip_list list the Poisson and ZIP LRT results for the three datasets respectively. The estimated zero-inflation probabilities for the “Rv” vaccine can be extracted from the latter list as follows. In the code chunks below we use the magrittr (Bache and Wickham 2020) forward pipe %>% infix operator for better readability of chained computations. The operator is automatically loaded with pvLRT; see the magrittr package manual for further details on the pipe operator.\n\n\nrv_zi_prob <- test_rv_zip_list %>% \n  sapply(function(test) extract_zi_probability(test)[\"Rv\"]) %>% \n  round(3)\nrv_zi_prob\n\n     old.Rv    young.Rv combined.Rv \n      0.248       0.000       0.215 \n\nIt follows from the above estimated zero inflation probabilities that a ZIP model could be appropriate for the combined and the old age groups whereas an ordinary Poisson model is likely adequate for the young age groups. We formalize the model selection via AIC and BIC, and find the optimal models for the three datasets. Codes are as follows.\n\n\ncompare_models_rv <- mapply(\n  function(test_poisson, test_zip, data_name) {\n    AIC_both <- AIC(test_poisson, test_zip)$AIC\n    BIC_both <- BIC(test_poisson, test_zip)$BIC\n    data.frame(\n      name = data_name, \n      AIC_poisson = AIC_both[1],\n      BIC_poisson = BIC_both[1],\n      AIC_zip = AIC_both[2],\n      BIC_zip = BIC_both[2],\n      optimal_AIC = c(\"poisson\", \"zip\")[which.min(AIC_both)],\n      optimal_BIC = c(\"poisson\", \"zip\")[which.min(AIC_both)]\n    )\n  },\n  test_rv_poisson_list,\n  test_rv_zip_list,\n  names(test_rv_poisson_list),\n  SIMPLIFY = FALSE\n) %>% \n  unname() %>% \n  do.call(rbind, .)\n\ncompare_models_rv\n\n      name AIC_poisson BIC_poisson  AIC_zip  BIC_zip optimal_AIC\n1      old    5749.153   18631.535 5751.783 18651.88     poisson\n2    young    3829.600    9000.445 3833.600  9019.39     poisson\n3 combined    8308.330   22800.414 8051.049 22561.38         zip\n  optimal_BIC\n1     poisson\n2     poisson\n3         zip\n\nIt follows from the above AIC and BIC values that while the ZIP model is a clear winner for the combined age group dataset, the Poisson model displays slightly smaller AIC and BIC values in the other two datasets. We therefore first create a list of optimal tests from these two models fitted to the three datasets, and use these optimal models for subsequent determination of prominent AEs as follows:\n\n\ntest_rv_optimal_list <- list(\n  old = test_rv_poisson_list$old,\n  young = test_rv_poisson_list$young,\n  combined = test_rv_zip_list$combined\n)\n\n\nWe may extract the statistically significant AE/vaccine pairs (\\(p\\)-value < 0.05) from these tests as follows.\n\n\nsignif_pairs_rv <- lapply(test_rv_zip_list, extract_significant_pairs) \n\n\n\n\n# print for summarized results\nsignif_pairs_rv\n\n$old\n                             AE Drug n    lrstat    p_value\n1: Gastrointestinal Haemorrhage   Rv 2 10.768628 0.00049995\n2:                    Diarrhoea   Rv 4 10.386030 0.00049995\n3:       Intestinal Obstruction   Rv 1  7.087679 0.03909609\n4:       Secondary Transmission   Rv 1  7.087679 0.03909609\n\n$young\n                             AE Drug   n    lrstat    p_value\n1:                    Diarrhoea   Rv 114 35.680505 0.00009999\n2: Gastrointestinal Haemorrhage   Rv  88 33.708041 0.00009999\n3:                     Vomiting   Rv 129 22.512090 0.00009999\n4:              Intussusception   Rv  32 11.980492 0.00009999\n5:               Abdominal Pain   Rv  40 11.911067 0.00009999\n6:                  Dehydration   Rv  37 10.643293 0.00009999\n7:              Gastroenteritis   Rv  21  6.150401 0.00809919\n8:       Intestinal Obstruction   Rv  19  5.457423 0.02109789\n\n$combined\n                              AE Drug   n     lrstat    p_value\n 1:                    Diarrhoea   Rv 135 154.320951 0.00009999\n 2: Gastrointestinal Haemorrhage   Rv  91 153.884769 0.00009999\n 3:                     Vomiting   Rv 134  99.195439 0.00009999\n 4:              Intussusception   Rv  34  58.610616 0.00009999\n 5:                    Agitation   Rv 105  45.670818 0.00009999\n 6:                  Dehydration   Rv  39  42.857250 0.00009999\n 7:       Intestinal Obstruction   Rv  21  34.022145 0.00009999\n 8:               Abdominal Pain   Rv  43  33.794857 0.00009999\n 9:              Gastroenteritis   Rv  23  33.200344 0.00009999\n10:              Abnormal Faeces   Rv  16  23.570006 0.00009999\n11:                 Constipation   Rv  18  23.020318 0.00009999\n12:                     Anorexia   Rv  45  22.292411 0.00009999\n13:             Weight Decreased   Rv  13  13.939082 0.00019998\n14:       Diarrhoea Haemorrhagic   Rv   7  13.573366 0.00039996\n15:                   Flatulence   Rv  10  13.093022 0.00079992\n16:                   Somnolence   Rv  37  13.077281 0.00079992\n17:                      Melaena   Rv   7  12.255722 0.00099990\n18:         Abdominal Distension   Rv   8  12.226474 0.00109989\n19:    Gastrointestinal Disorder   Rv  16  10.774175 0.00369963\n20:                 Haematemesis   Rv   4   9.818926 0.00809919\n21:                       Sepsis   Rv  10   8.919282 0.01769823\n                              AE Drug   n     lrstat    p_value\n\nFor visualization, we consider heatmaps for top 15 AEs (in terms of LRT statistics) in the three datasets, and place them side by side. Because pvLRT uses ggplot2 as its plotting engine, these individual heatmaps can be easily combined via external ggplot2 post-processing packages such as patchwork (Pedersen 2020). Codes are as follows:\n\n\npl_heatmap_rv_list <- mapply(\n  function(this_test, this_name) {\n    heatmap_pvlrt(\n      this_test,\n      AE = 15,\n      show_n = TRUE,\n      show_lrstat = TRUE\n    ) + \n      ggtitle(paste(\"age group:\\n\", this_name)) + \n      theme(plot.title = element_text(hjust = 0.5))\n  },\n  test_rv_optimal_list,\n  names(test_rv_optimal_list),\n  SIMPLIFY = FALSE\n)\n## install patchwork if not installed\n# if (!requireNamespace(\"patchwork\")) install.packages(\"pacthwork\")\nlibrary(patchwork)\npl_heatmap_rv_comb <- pl_heatmap_rv_list$old + \n  pl_heatmap_rv_list$young + \n  pl_heatmap_rv_list$combined + \n  # combine the legends\n  plot_layout(guides = \"collect\") & \n  # unified coloring scheme, \n  # see the 'fill_gradient_range' argument for heatmap_pvlrt\n  scale_fill_gradientn(\n    colors = c(\"darkred\", \"white\"), \n    limits = c(0, 1)\n  ) &\n  theme(\n    axis.text = element_text(size = 16, face = \"bold\"),\n    legend.title = element_text(size = 16, face = \"bold\"),\n    legend.text = element_text(size = 14),\n    title = element_text(size = 16, face = \"bold\")\n  )\n\npl_heatmap_rv_comb\n\n\n\nThe following observations are made from the above heatmaps and associated test results (stored in signif_pairs_rv). First, in terms of LR statistics and \\(p\\)-values, the “old” age group has only 4 statistically significant (\\(p\\)-value \\(< 0.05\\)) AEs, viz., Gastrointestinal Haemorrhage, Diarrhoea, Intestinal Obstruction, and Secondary Transmission. Note, however, the extremely small sample sizes for these AEs within the dataset; care must therefore be taken while interpreting these results. By contrast, the young and the combined age group data sets contain sufficient sample sizes. There are 8 and 21 significant AEs in the young and the combined age group datasets respectively. These prominent AEs for the combined and the young age groups agree, which is expected as the most reported cases in the combined dataset correspond to the young age group. Several of these prominent AEs detected in our analysis associate with Intussusception, which has been determined to be of particular concern in the literature (Niu et al. 2001; Haber et al. 2004). However, unlike the approaches used in those studies, our LRT-based approach for signal detection does not require ad hoc thresholding, and provides rigorous statistical guarantees on the results obtained.\n4 Discussion and Future Directions\nLikelihood ratio test (LRT)-based approaches to pharmacovigilance constitute a class of rigorous statistical methods that permit objective signal determination without the need for specifying ad hoc thresholds. The lack of comprehensive software implementing the LRT methods (in R or otherwise) however has hindered their applicability in practice – especially in cases where the adverse event count data are zero-inflated.\nThe package pvLRT is our contribution to this area, providing a platform that implements a suite of formal statistical tools and post-processing functions for analyzing medical product safety datasets, and is readily accessible to practitioners. There are various directions in which the software can be extended in future releases. Some possible features/updates under consideration include the following.\nParallelize bootstrap resampling. This may permit substantial computational gains in sizable datasets. Care must however be taken while generating random draws in the parallel workers to ensure reproducibility and validity (see the discussion of page 5, Section 6 of the R package parallel (R Core Team 2022)).\nInclude additional processed datasets, and expand the current documentations with examples on these additional datasets.\nCreate visualizations for the estimated zero inflation probabilities along with the LRT results.\n5 Acknowledgment\nDr. Markatou acknowledges grant support from KALEIDA Health Foundation (Troup Fund) and BAA HDDISWP#97, FDA. These resources supported the work of the University at Buffalo affiliated authors.\n\n\n\n\nCRAN packages used\n<<x[2]>>, PhViD, openEBGM, sglr, Sequential, SPRT, AEenrich, mds, pvLRT, ggplot2, plotly, data.table, tidyverse, magrittr, patchwork, parallel\nCRAN Task Views implied by cited packages\nBayesian, Finance, HighPerformanceComputing, Phylogenetics, Spatial, TeachingStatistics, TimeSeries, WebTechnologies\n\n\nA. Agresti. Categorical Data Analysis. John Wiley & Sons, 2013. Google-Books-ID: 6PHHE1Cr44AC.\n\n\nI. Ahmed and A. Poncet. PhViD: an R package for PharmacoVigilance signal Detection. 2016. R package version 1.0.8.\n\n\nS. M. Bache and H. Wickham. Magrittr: A forward-pipe operator for r. 2020. URL https://CRAN.R-project.org/package=magrittr. R package version 2.0.1.\n\n\nY. Benjamini and Y. Hochberg. Controlling the false discovery rate: A practical and powerful approach to multiple testing. Journal of the Royal Statistical Society: Series B (Methodological), 1995. DOI 10.1111/j.2517-6161.1995.tb02031.x.\n\n\nT. Canida and J. Ihrie. openEBGM: An R Implementation of the Gamma-Poisson Shrinker Data Mining Model. The R Journal, 9(2): 499–519, 2017. URL https://journal.r-project.org/archive/2017/RJ-2017-063/index.html.\n\n\nS. Chakraborty, A. Liu, R. Ball and M. Markatou. On the use of the likelihood ratio test methodology in pharmacovigilance. Statistics in Medicine, 41(27): 5395–5420, 2022.\n\n\nS. Chakraborty and M. Markatou. pvLRT: Likelihood ratio test-based approaches to pharmacovigilance. 2022. R package version 0.4.\n\n\nG. Chung. mds: Medical Devices Surveillance. 2020. URL https://CRAN.R-project.org/package=mds. R package version 0.3.2.\n\n\nA. C. Davison and D. V. Hinkley. Bootstrap methods and their application. Cambridge: Cambridge University Press, 1997. DOI 10.1017/CBO9780511802843.\n\n\nY. Ding, M. Markatou and R. Ball. An evaluation of statistical approaches to postmarketing surveillance. Statistics in Medicine, 39(7): 845–874, 2020. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8447.\n\n\nM. Dowle and A. Srinivasan. Data.table: Extension of ‘data.frame‘. 2021. URL https://CRAN.R-project.org/package=data.table. R package version 1.14.2.\n\n\nW. Dumouchel. Bayesian data mining in large frequency tables, with an application to the FDA spontaneous reporting system. The American Statistician, 53(3): 177–190, 1999. URL https://www.tandfonline.com/doi/abs/10.1080/00031305.1999.10474456.\n\n\nP. Haber, R. T. Chen, L. R. Zanardi, G. T. Mootrey, R. English, M. M. Braun and the VAERS Working Group. An analysis of rotavirus vaccine reports to the vaccine adverse event reporting system: More than intussusception alone? Pediatrics, 113(4): e353–e359, 2004. URL https://doi.org/10.1542/peds.113.4.e353.\n\n\nL. Huang, J. Zalkikar and R. C. Tiwari. A likelihood ratio test based method for signal detection with application to FDA’s drug safety data. Journal of the American Statistical Association, 106(496): 1230–1241, 2011. URL https://doi.org/10.1198/jasa.2011.ap10243.\n\n\nL. Huang, D. Zheng, J. Zalkikar and R. Tiwari. Zero-inflated poisson model based likelihood ratio test for drug safety signal detection. Statistical Methods in Medical Research, 26(1): 471–488, 2017. URL https://doi.org/10.1177/0962280214549590.\n\n\nS. Li, H. Chen, L. Zhao and M. Kleinsasser. AEenrich: Adverse Event Enrichment Tests. 2021. URL https://CRAN.R-project.org/package=AEenrich. R package version 1.1.0.\n\n\nM. Markatou and R. Ball. A pattern discovery framework for adverse event evaluation and inference in spontaneous reporting systems. Stat. Anal. Data Min., 2014. DOI 10.1002/sam.11233.\n\n\nB. Narasimhan and M.-C. Shih. sglr: An r package for computing the boundaries for sequential generalized likelihood ratio test for pre-licensure vaccine studies. 2012. URL http://CRAN.R-project.org/package=sglr. R package version 0.7.\n\n\nM. T. Niu, D. E. Erwin and M. M. Braun. Data mining in the US Vaccine Adverse Event Reporting System (VAERS): early detection of intussusception and other events after rotavirus vaccination. Vaccine, 19(32): 4627–4634, 2001. URL https://www.sciencedirect.com/science/article/pii/S0264410X01002377.\n\n\nT. L. Pedersen. Patchwork: The composer of plots. 2020. URL https://CRAN.R-project.org/package=patchwork. R package version 1.1.1.\n\n\nR Core Team. Package “parallel.” 2022. (Accessed on 08/04/2023).\n\n\nY. Sakamoto, M. Ishiguro and G. Kitagawa. Akaike information criterion statistics. Dordrecht, The Netherlands: D. Reidel, 81(10.5555): 26853, 1986. Publisher: Taylor & Francis.\n\n\nC. Sievert. Interactive web-based data visualization with r, plotly, and shiny. Chapman; Hall/CRC, 2020. URL https://plotly-r.com.\n\n\nI. R. Silva and M. Kulldorff. Sequential: Exact Sequential Analysis for Poisson and Binomial Data. 2021. URL https://CRAN.R-project.org/package=Sequential. R package version 4.1.\n\n\nH. Wickham. ggplot2: Elegant graphics for data analysis. Springer-Verlag New York, 2016. URL https://ggplot2.tidyverse.org.\n\n\nH. Wickham, M. Averick, J. Bryan, W. Chang, L. D. McGowan, R. François, G. Grolemund, A. Hayes, L. Henry, J. Hester, et al. Welcome to the tidyverse. Journal of Open Source Software, 4(43): 1686, 2019. DOI 10.21105/joss.01686.\n\n\nY. Zhao, M. Yi and R. C. Tiwari. Extended likelihood ratio test-based methods for signal detection in a drug class with application to FDA’s adverse event reporting system database. Statistical Methods in Medical Research, 27(3): 876–890, 2018. URL https://doi.org/10.1177/0962280216646678. Publisher: SAGE Publications Ltd STM.\n\n\nThe default printing specifications for a data.table object applies to the resulting summary, which abbreviates the output to show results for the top 5 and bottom 5 AE/drug pairs (arranged with respect to the computed LRT values) on the console for large summary tables. To print more pairs, use e.g., summary(test_statin46_poisson) %>% print(n) with a larger n. Storing the summary output to an object, e.g., summary_test <- summary(test_statin46_poisson) allows access to results from all pairs.↩︎\n",
    "preview": "articles/RJ-2023-027/RJ-2023-027_files/figure-html5/show-heatmap-poisson-statin46-1.png",
    "last_modified": "2023-11-07T21:31:41+00:00",
    "input_file": {},
    "preview_width": 2496,
    "preview_height": 960
  },
  {
    "path": "articles/RJ-2023-028/",
    "title": "GCPBayes:  An R package for studying Cross-Phenotype Genetic Associations with Group-level Bayesian Meta-Analysis",
    "description": "Several R packages have been developed to study cross-phenotypes associations (or pleiotropy) at the SNP-level, based on summary statistics data from genome-wide association studies (GWAS). However, none of them allow for consideration of the underlying group structure of the data. We developed an R package, entitled GCPBayes (Group level Bayesian Meta-Analysis for Studying Cross-Phenotype Genetic Associations), introduced by Baghfalaki et al. (2021), that implements continuous and Dirac spike priors for group selection, and also a Bayesian sparse group selection approach with hierarchical spike and slab priors, to select important variables at the group level and within the groups. The methods use summary statistics data from association studies or individual level data as inputs, and perform Bayesian meta-analysis approaches across multiple phenotypes to detect pleiotropy at both group-level (e.g., at the gene or pathway level) and within group (e.g., at the SNP level).",
    "author": [
      {
        "name": "Taban Baghfalaki",
        "url": {}
      },
      {
        "name": "Pierre-Emmanuel Sugier",
        "url": {}
      },
      {
        "name": "Yazdan Asgari",
        "url": {}
      },
      {
        "name": "Thérèse Truong",
        "url": {}
      },
      {
        "name": "Benoit Liquet",
        "url": {}
      }
    ],
    "date": "2023-08-26",
    "categories": [],
    "contents": "\n1 Introduction\nMany cross-phenotype associations have been identified in genome-wide association studies (GWAS), providing evidence that pleiotropy (the idea that a single genetic variant, usually a Single-Nucleotide\nPolymorphism (SNP), is affecting multiple traits) is a common phenomenon in human complex traits (Watanabe et al. 2019). As a result, pleiotropy has attracted a great deal of attention among genetic epidemiologists (see, Solovieff et al. 2013; Verma et al. 2016; Hackinger and Zeggini 2017; Lu et al. 2017; Krapohl et al. 2018; Liu and Lin 2018; Majumdar et al. 2018; Trochet et al. 2019; Ray and Chatterjee 2020; Baghfalaki et al. 2021; Broc et al. 2021), and several R packages have been developed to perform meta-analysis methods adapted for cross-phenotype association detection based on summary statistics data from GWAS as input (i.e., estimated effect size, standard error, and p-value of the association between each SNP and a trait, Ray and Chatterjee 2020). It should be noted that pleiotropy is only one possible explanation of cross-phenotype association, but for simplicity, we will use the term pleiotropy for cross-phenotype association to describe any cross-phenotype association in this paper. Below is a list of the most common methods for detecting pleiotropy between multiple traits.\nASSET (Bhattacharjee et al. 2012) is a frequentist method, which is an extension of standard fixed effects meta-analysis that considers the effects of a variable (such as a SNP) in each study (representing a phenotype) to be in either the same direction or opposite directions, allowing for detection of antagonistic pleiotropic effects. This approach uses summary statistics and does not take into account group structure. In the case of GWAS, the outputs of this approach are a p-value of the global association of each SNP with all studies (or phenotypes), and an optimal subset of non-null studies or phenotypes that are associated with each SNP.\nGPA (Chung et al. 2014) implements a flexible statistical framework for the joint analysis of multiple GWAS and\nits integration with various genetic and genomic data. It implements a flexible parametric mixture modeling approach for such integrative analysis and also provides hypothesis testing procedures for\npleiotropy and annotation enrichment.\nPLACO is a recently created R package using GWAS summary statistics data to identify pleiotropic signals between two traits (Ray and Chatterjee 2020). The computed method derives a composite null hypothesis\nthat ‘at most one trait is associated with the genetic variant’ vs. the alternative that ‘both traits are associated’, based on the product of the Z-statistics of the genetic variants across two studies. CPBayes (Majumdar et al. 2018) is a Bayesian meta-analysis approach that uses univariate spike and slab prior and performs the MCMC technique via a Gibbs sampler. CPBayes uses the summary statistics data for a SNP across multiple traits. Two different measures are estimated for evaluating the overall pleiotropic association:\nthe local false discovery rate and the Bayes factor. The optimal subset of associated traits underlying a pleiotropic signal is defined as the maximum a posteriori (MAP) estimate. The marginal trait-specific posterior probability of association (PPA), the direction of associations, and the credible interval of true genetic effects, are some examples of additional insights into pleiotropic signals that are provided by CPBayes.\nAll packages developed for these summary statistics-based methods are only designed to test pleiotropy at the variable-level (SNP-level). Though the structure of the common mechanisms shared by multiple phenotypes can be more complex, for example, different variants in the same locus can be associated with multiple traits\nand affect the same gene, and therefore can have an impact on the same trait.\nThus, extending CPBayes to the gene or pathway level, which takes into account the group structure of the data, could provide additional power to detect pleiotropic signals between multiple diseases. By incorporating prior biological information in GWAS, such as group structure information (gene or pathway), our approaches could uncover new pleiotropic signals (Li et al. 2020; Baghfalaki et al. 2021).\nIn this paper, we present the GCPBayes R package (Bayesian meta-analysis of pleiotropic effects using group structure) for studying pleiotropy between multiple phenotypes using GWAS data by considering group structure information from prior biological knowledge. This package is able to consider and detect pleiotropy at both the variable-level and group-level. The inputs to the developed functions are SNP-level summary statistics data\nderived from GWAS by considering all the estimated regression coefficients of the variables (SNPs) in a group (a gene or a pathway) and its covariance matrix.\nThe methods that can be performed by the package offer a Bayesian paradigm using multivariate spike and slab priors for group-level pleiotropy using either continuous spike (CS, George and McCulloch 1993) or Dirac spike (DS, Mitchell and Beauchamp 1988) formulations. Also, a hierarchical sparsity prior (HS, Xu et al. 2015) using two levels of Dirac spike and slab components to achieve group and within-group selection can be applied. Both tests for the global null hypothesis of no association and for detecting pleiotropy are included in the package.\ngroup-level and variable-level are considered.\nThe free RStudio interface is advised to be used with\nGCPBayes. The GCPBayes package can be installed by typing:\n\n\n  install.packages(\"GCPBayes\")\n\n\nTo be used, the GCPBayes package has to be loaded each time R or RStudio are opened, via the following code:\n\n\n  library(GCPBayes)\n\n\nThe paper is organized as follows:\nThe next section (“Data structure”) describes how a data should look in order to be used by functions of the GCPBayes package.\nThe section “Usage” presents the different functions of GCPBayes. Some details for using different case studies and a practical guideline for the detection of pleiotropic effects are provided.\nThe “Guidelines” section includes a statistical inference pipeline and recommendations for carrying out the various stages of a pleiotropy analysis with CRANpkg and Bayes.\nThe computational time of the functions of the GCPBayes package is explored using real data in the “Computational time for GCPBayes” section. Finally, in the “Concluding remarks” section, we discuss some\nremarks about and limitations of GCPBayes. Besides, future works for a newer version of the package are discussed at the end of the section. In addition, the paper includes two appendices.\nThe first is\n“Material and method”, where notations and models are described briefly.\nThe second one describes the design of the simulated data and their corresponding R codes, which are also embedded in the package.\n2 Data structure\nGCPBayes is intended to work with summary statistics data. Though summary statistics can be derived from individual-level data when available, this allows for taking into account correlations between variables to improve the accuracy of the method.\nIt should be noted that GCPBayes is not restricted to a specific type of outcome. Summary statistics data can be estimated through various kinds of models (linear, logistic, Cox, etc.) and based on different types of phenotypes (categorical or continuous traits). Therefore, it is possible to use the GCPBayes package for genetic studies, gene expression analyses, and other omics of studies.\nIn this paper, we will illustrate the use of our method in genetic studies of breast and thyroid cancers. We will explore the cross-phenotype association between these two traits and SNPs located in one gene in particular, PARP2. So, this section includes two different scenarios. The first part\nfirst part deals with summary statistics on the association between breast cancer risk or thyroid cancer risk and SNPs located in the PARP2 gene (Example 1). The second part considers the individual level data from which the summary statistics of Example 1 were derived as input\nand then explores multicollinearity through the data (Example 2), and computes summary statistics and the covariance matrix from the individual-level data for PARP2 gene (Example 3).\nWe will consider the estimated regression coefficients and their covariance matrices from all studies: \\[{\\hat{{\\beta}}_k},~{\\hat{{\\Sigma}} _k} = {\\mathop{\\rm cov}} ({\\hat{{ \\beta}} _k}),~~k = 1, \\cdots ,K.\\]\nwhere \\({\\hat{{\\beta}}_k}\\) is an \\(m-\\)dimensional vector containing the regression coefficients from the \\(k^{th}\\) study for a given gene (or pathway) and \\({\\hat{{\\Sigma}} _k}\\) is the corresponding covariance matrix. If only summary statistics are available, the covariance matrix is replaced by a diagonal matrix with an estimated variance of the regression coefficients.\nSummary statistics level data\nGWAS summary statistics data are available through various public databases such as GWAS Catalog.\nIn most cases, summary statistics include effect size estimate (beta), standard error, and p-value for each SNP.\nThe GCPBayes package accepts inputs related to betas and standard errors as lists:\nspecifically, a list of regression coefficients (betas) and a list of the matrices of the computed estimates of the variance of the regression coefficients.\nExample 1: Inputs of GCPBayes using summary statistics from two case-control studies (on breast and thyroid cancers) for PARP2 gene\nGWAS summary statistics are available through various public databases, such as the GWAS Catalog. In most cases, a summary of the statistical data includes the effect size estimate (beta), standard error, and p-value for each SNP:\n\n\nlibrary(GCPBayes)\ndata(PARP2_summary)\nBreast <- PARP2_summary$Breast\nThyroid <- PARP2_summary$Thyroid\ngenename <- \"PARP2\"\nsnpnames <- rownames(Breast)\nBetah <- list(Breast$beta,Thyroid$beta)\nSigmah <- list(diag(Breast$se^2),diag(Thyroid$se^2))\nprint(Betah,digits=2)\n\n[[1]]\n[1] -0.081  0.073 -0.346 -0.222  0.095  0.166\n\n[[2]]\n[1] -0.033 -0.470  0.398  0.276  0.160  0.040\n\nprint(Sigmah,digits=2)\n\n[[1]]\n      [,1]  [,2]  [,3]  [,4]   [,5]  [,6]\n[1,] 0.029 0.000 0.000 0.000 0.0000 0.000\n[2,] 0.000 0.056 0.000 0.000 0.0000 0.000\n[3,] 0.000 0.000 0.061 0.000 0.0000 0.000\n[4,] 0.000 0.000 0.000 0.031 0.0000 0.000\n[5,] 0.000 0.000 0.000 0.000 0.0064 0.000\n[6,] 0.000 0.000 0.000 0.000 0.0000 0.051\n\n[[2]]\n      [,1] [,2]  [,3] [,4]   [,5]  [,6]\n[1,] 0.031 0.00 0.000 0.00 0.0000 0.000\n[2,] 0.000 0.16 0.000 0.00 0.0000 0.000\n[3,] 0.000 0.00 0.068 0.00 0.0000 0.000\n[4,] 0.000 0.00 0.000 0.04 0.0000 0.000\n[5,] 0.000 0.00 0.000 0.00 0.0063 0.000\n[6,] 0.000 0.00 0.000 0.00 0.0000 0.043\n\nThis gene included six SNPs (rs3093872, rs3093921, rs1713411, rs3093926, rs3093930, and rs878156). So, the Betah list must include summary statistics for six elements for every study, and Sigmah list must contain a diagonal matrix of [\\(6\\times 6\\)] for each study. A user can check the data using the “print” method.\nIndividual level data\nWhen individual level data are available, summary statistics should be computed using external packages. Here, we explore the available approaches and R packages for computing the summary statistics.\nLet \\({Y}_1,\\cdots,{Y}_K\\) be the response variables of \\(K\\) studies, and \\({Y}_k=(Y_{k1},\\cdots,Y_{kn_k})^\\prime\\) represent the \\(n_k\\) phenotype observations, \\(k = 1, cdots, Y_K\\). For study \\(k\\), let the genetic information be organized into \\(G\\) groups for research purposes. For example, this can be a set of SNPs belonging to a gene or a set of SNPs belonging to a set of genes acting together in the same biological pathway. We denote \\({Z}_{kg}\\) as a \\(n_k\\times m_g-\\) matrix of the \\(m_g\\) covariates for group \\(g, ~g=1,2,\\cdots,G\\). As all inferences are performed gene by gene, we remove the index \\(g\\) and consider \\(m\\) as the number of variables in each specific group for simplicity. Considering one of the groups, we assume that a generalized linear model (GLM, McCullagh 2019)\nis fitted separately for each study as follows:\n\\[\\begin{eqnarray}\\label{e1}\n\\eta \\left({\\rm{E}}\\left( {{{{Y}}_k}{\\rm{ }}} \\right)\\right) ={\\alpha _{k}}{1}_{n_k} + {Z}_{k}{{\\beta} _{k}},~~k = 1,\\cdots,K,\n\\end{eqnarray}\\]\nwhere \\(\\eta(\\cdot)\\) is a link function, \\(\\alpha_{k}\\) is the intercept of the model and \\({1}_{n_k}\\) is a vector of \\(n_k\\) ones. Here \\({{\\beta}}_{k}=({{\\beta}}_{k1},\\cdots,{{\\beta}}_{km})^\\prime\\) denotes the \\(m\\)-dimensional regression coefficients for the group and the \\(k^{th}\\) study.\nThe “glm” function from R can be applied to fit generalized linear models and so get the summary statistics or parameter estimation of model 1 (Dunn and Smyth 2018).\nIf the phenotype is binary, then 1 is the usual logistic or probit model (Agresti 2018).\nIf the phenotype is continuous, model 1 is reduced to multiple linear regression and can also be fitted by the “lm” function from R which will provide the same results as the “glm” function.\nThe “vcov” function can be applied to get the estimated covariance matrix of the regression coefficients, whereas the “glm” and “lm” functions calculate standard errors, which yield only the diagonal elements.\nAnother strategy should be considered in the case of multicollinearity. Multicollinearity is the presence of near-linear relationships among variables (Malo et al. 2008).\nThis phenomenon is widespread in genetic data, where non-random association of alleles at different loci in a given population is frequent, introducing large structures of correlation between SNPs. It is known as linkage disequilibrium (LD).\nLD can lead to inaccurate estimates of the regression coefficients, and also inflate the standard errors of the regression coefficients (Saleh et al. 2019).\nMulticollinearity can be visualized by drawing pairwise scatter plots of variables\nor by inspection of the correlation matrix.\nThe variance inflation factor (VIF) can be used to assess the presence of multicollinearity (Fox and Weisberg 2018). It can be computed by the “vif” function from the car package (Fox et al. 2012). Most research papers consider a VIF > 10 as an indicator of multicollinearity (Menard 2002; Vittinghoff et al. 2011; Gareth et al. 2013; Johnston et al. 2018), but some choose a more conservative threshold of 5 (Gareth et al. 2013) or even 2.5 (Johnston et al. 2018).\nExample 2: Exploring multicollinearity/linkage disequilibrium in PARP2 gene using individual level data\nIn this example, we show the existence of multicollinearity in the data. Here, we consider the individual-level data from which the summary statistics of Example 1 were derived. The summary statistics and the corresponding VIFs for both studies could be obtained using the usual “glm” function as follows:\n\n\nlibrary(GCPBayes)\nlibrary(car)\ndata(PARP2)\nBreast <- PARP2$Breast\nThyroid <- PARP2$Thyroid\nFit1 <- glm(y1~ ., family=binomial(link=\"logit\"),data=Breast)\nprint(vif(Fit1))\n\nrs3093872 rs3093921 rs1713411 rs3093926 rs3093930  rs878156 \n 1.696966  1.584775  3.095049  2.700594  1.347861  4.438011 \n\nFit2 <- glm(y2~ ., family=binomial(link=\"logit\"),data=Thyroid)\nprint(vif(Fit2))\n\nrs3093872 rs3093921 rs1713411 rs3093926 rs3093930  rs878156 \n 1.565507  1.478632  3.552804  3.465883  1.397995  5.490866 \n\nA high VIF value (>5) for one SNP indicates multicollinearity in the data. Hence, the user should not use the “glm” function to obtain correct estimates of regression coefficients and their standard errors. Instead, the user could perform Ridge regression (Hilt and Seegrist 1977). Although many R packages are available to perform Ridge regression, only lrmest, ridge and lmridge can estimate the standard errors of the regression coefficients. Only lmridge, and only for the linear model, can compute their covariance matrix using the “vcov” or “vcov.lmridge” functions.\nSince a usual approach for obtaining the covariance matrix is to apply a very time-consuming Bootstrap method (Efron 1992), we recommend performing a Bayesian hierarchical GLM by using Gaussian priors to get summary statistics of each group (gene/pathway).\nThus, a user could apply the bayesreg package (Makalic and Schmidt 2016) or the BhGLM package (Yi et al. 2019) that provides fast and stable algorithms to estimate parameters for high-dimensional clinical and genomic data as well as highly correlated variables.\nExample 3: Inputs of GCPBayes using individual level data from two case-control studies (on breast and thyroid cancers) for PARP2 gene\nWe propose to estimate the regression coefficients and their covariance matrices using a Bayesian hierarchical logistic model, which would lead to more powerful inputs for GCPBayes (as we mentioned earlier) than the inputs used in Example 1:\n\n\nlibrary(GCPBayes)\ndata(PARP2)\nBreast <- PARP2$Breast\nThyroid <- PARP2$Thyroid\nFit1 <- bayesreg::bayesreg(as.factor(y1)~ ., model = \"logistic\", prior = \"ridge\",data=Breast)\nBetah1 <-  Fit1$mu.beta\nSigmah1 <- cov(t(Fit1$beta))\nFit2 <- bayesreg::bayesreg(as.factor(y2)~ ., model = \"logistic\", prior = \"ridge\",data=Thyroid)\nBetah2 <-  Fit2$mu.beta\nSigmah2 <- cov(t(Fit2$beta))\nBetah <- list(Betah1,Betah2)\nSigmah <- list(Sigmah1,Sigmah2)\n#print(Betah,digits=1)\n#print(Sigmah,digits=1)\n\n\nBy running the code above, we can estimate the regression coefficients and their covariance matrices, using the individual-level data from the two studies. The results can be checked using the last two “print” commands.\nAs a final note in this section, we demonstrated in a previous study (Baghfalaki et al. 2021) that running GCPBayes on summary statistics data could result in a loss of power because it requires consideration of a diagonal covariance matrix of the effects (without information on off-diagonal components). So, statistically speaking, when individual-level data are available, we recommend computing the covariance matrix of the effects and using it as inputs to GCPBayes in order to increase the power of the method.\n3 Usage\nIn this section, we describe how to use the GCPBayes package that includes four functions: DS, CS, HS, and MCMCplot.\nFirst, Examples 4-7 show the outputs of each of the four functions while applying GCPBayes to the summary statistics described in Example 1 to test whether the gene PARP2 is associated with both thyroid cancer and breast cancer risk. Then, in Example 8, we compared results obtained with the DS function applied to summary statistics of PARP2 gene with those obtained with individual-level data (as described in Examples 2-3) as input. Finally, in Example 9, we show an example of output for the gene DNAJC1 which was significantly associated with breast and thyroid cancers using the DS function. The inputs are summary statistics from two larger studies than those used in Example 1 and will be detailed below.\nIn addition to the real datasets described in Examples 1-3 and Example 9, three more simulated data are provided in Appendix B and are embedded in the GCPBayes package, including summary statistics level data for \\(K=5\\) studies with binary outcome,\nindividual level data for \\(K=3\\) studies with continuous outcome and individual level data for \\(K=2\\) studies for survival outcomes and gene expression data. All commands used for the examples considered in this section are available through the R documentation of the GCPBayes package.\nDS function\nDS runs a Gibbs sampler for a multivariate Bayesian sparse group selection model with a Dirac spike prior for the detection of pleiotropic effects on the traits. As we mentioned in the previous section, the DS function is designed to use estimated regression coefficients and their estimated covariance matrices from \\(K\\) studies. The statistical details of the DS approach are given in Appendix A. Besides, more details can be found at (Baghfalaki et al. 2021).\nThe DS function and its parameters have the following format:\n\n\n  DS(Betah, Sigmah, kappa0, sigma20, m, K, niter = 2000, burnin = 1000, nthin = 2, \n      nchains = 2, a1 = 0.1, a2 = 0.1, d1 = 0.1, d2 = 0.1, snpnames, genename)\n\n\nwhere\nBetah: A list containing \\(m\\)-dimensional vectors of the regression coefficients for \\(K\\) studies.\nSigmah: A list containing the \\(m \\times m\\)-dimensional positive definite covariance matrices which are the estimated covariance matrices of K studies. If individual-level data are not available, it corresponds to the diagonal matrices with the estimated variance of the regression coefficients for the K studies.\nkappa0: Initial value for kappa such that its dimension is equal to the number of Markov chains (nchains).\nAlthough, the best strategy for choosing the initial values are previous studies, however, the domain for choosing any value is equal to the domain of the priors which is given in the hierarchical setup A.2 in Appendix A.\nKappa0 can be in range from 0 to 1.\nsigma20: Initial value for sigma2 such that its dimension is equal to nchains. The domain for initial values of sigma2 is \\(\\Re^+\\).\nm: Number of variables in the group.\nK: Number of traits.\nniter: Number of iterations for the Gibbs sampler.\nburnin: Number of burn-in iterations. The burnin=1000 value is the default.\nnthin: The lag of the iterations used for the posterior analysis (or thinning rate). nthin=2 is the default value.\nnchains: Number of Markov chains. When nchains > 1, the function calculates the Gelman-Rubin convergence statistic (Gelman et al. 1992; Brooks and Gelman 1998). Default value is nchains=2.\na1, a2: Hyperparameters of kappa. Default values are a1=0.1 and a2=0.1.\nd1, d2: Hyperparameters of sigma2. d1 = 0.1 and d2 = 0.1 are the default values.\nsnpnames: Variable names for the group.\ngenename: The group’s name.\nExample 4: Usage of the DS function to analyse pleiotropy between breast and thyroid cancers at PARP2 gene\nHere we consider the individual data for the PARP2 gene from the two case-control studies on breast and thyroid cancer (Examples 2-3) as input. As we explained earlier, first we estimated the effect size of the six SNPs in PARP2 and the corresponding covariance matrix for both studies. Then, we applied the DS function to the results.\n\n\nlibrary(GCPBayes)\ndata(PARP2)\nBreast <- PARP2$Breast\nThyroid <- PARP2$Thyroid\ngenename <- \"PARP2\"\nsnpnames <- names(PARP2$Breast)[-1]\nFit1 <- bayesreg::bayesreg(as.factor(y1)~ ., model = \"logistic\", prior = \"ridge\",data=Breast)\nBetah1 <-  Fit1$mu.beta\nSigmah1 <- cov(t(Fit1$beta))\nFit2 <- bayesreg::bayesreg(as.factor(y2)~ ., model = \"logistic\", prior = \"ridge\",data=Thyroid)\nBetah2 <-  Fit2$mu.beta\nSigmah2 <- cov(t(Fit2$beta))\nBetah <- list(Betah1,Betah2)\nSigmah <- list(Sigmah1,Sigmah2)\n\n\nTo detect pleiotropic effects for this gene, two chains with 2,000 iterations and a burn-in period of 1,000 iterations were used.\n\n\nset.seed(123)\nRES1 <- DS(Betah, Sigmah, kappa0=c(0.2,0.5), sigma20=c(1,2),\n           m=6, K=2, niter=2000, burnin=1000, nthin=2, nchains=2,\n           a1=0.1, a2=0.1, d1=0.1, d2=0.1, snpnames, genename)\n\n\nThe result of the DS function was saved in the “RES1” list, which includes four lists (MCMCChain, Summary, Criteria, and Indicator).\nThe “MCMCChain” list contains posterior samples for the unknown parameters, while the “Summary” list includes summary statistics of the posterior samples for the unknown parameters including the name of the SNP, the posterior mean, the posterior standard deviation, the quantile 2.5\\(\\%\\), median, the quantile 97.5\\(\\%\\) and the Gelman-Rubin convergence statistic (Gelman et al. 1992; BGR, Brooks and Gelman 1998).\nUsing a “print” command for the “Summary” list would result in two lists showing Bayesian estimation for all analyzed SNPs in both studies.\n\n\nprint(RES1$Summary,digits=2)\n\n\nIn order to check if the chains converged to a stationary distribution or not, a user could look at the BGR column. In our example, as the values of BGR are not close to one for some of the variables, the chains do not converge to a stationary distribution. To check this criteria, we also repeat the DS function with \\(niter=20000\\) and \\(burnin=10000\\).\n\n\nset.seed(123)\nRES1 <- DS(Betah, Sigmah, kappa0=c(0.2,0.5), sigma20=c(1,1.5),\n           m=6, K=2, niter=20000, burnin=10000, nthin=2, nchains=2,\n           a1=0.1, a2=0.1, d1=0.1, d2=0.1, snpnames, genename)\nprint(RES1$Summary,digits=2)\n\n\nAs it is clear, after running the DS function with new parameters, the values of the BGR column now confirm convergence to the stationary distribution.\nNote that in the case of \\(nchain=1\\), the BGR column can never be computed.\nThe two other output lists of the “RES1” (“Criteria” and “Indicator”) contain information for checking group and variable pleiotropy, respectively.\nFor each SNP, the ‘Criteria’ list contains the name of the gene, name of the SNP, the posterior probability of association (PPA), the logarithm (to base 10) of\nthe Bayes factor (BF), the local false discovery rate (locFDR) and \\(\\theta\\) of equation (4) of the Guidelines section. A user could check it with the following command:\n\n\nprint(RES1$Criteria,digits=2)\n\n$`Name of Gene`\n[1] \"PARP2\"\n\n$`Name of SNPs`\n[1] \"rs3093872\" \"rs3093921\" \"rs1713411\" \"rs3093926\" \"rs3093930\"\n[6] \"rs878156\" \n\n$PPA\n$PPA[[1]]\n[1] 0\n\n$PPA[[2]]\n[1] 0.036\n\n\n$log10BF\n[1] -1.9\n\n$lBFDR\n[1] 0.96\n\n$theta\n[1] 5.8e-06\n\nBased on the PPA1 and PPA2 values, the gene PARP2 isnot significantly associated with the traits in the study.\nThis is also confirmed by the values of lBFDR and BF. One can conclude that the global null hypothesis of no association at the group level cannot be rejected. Now, we can check the results of the pleiotropic effect at the group level by considering the value of theta. By considering the threshold of \\(0.5\\), it shows that there is no group-level pleiotropic effect for textit{PARP2} gene.\nAlso, an “Indicator” list is given for checking variable pleiotropy using two different statistics: (\\(95\\%\\) credible interval and the median threshold. Each row of the lists includes the name of the SNP, a binary indicator for the significant regression coefficient of each study (0=no,1=yes) and the total number of significant regression coefficients in the studies. Finally, the last column indicates whether the test for pleiotropy at the variable level is significant or not.\n\n\nprint(RES1$Indicator,digits=2)\n\n\nIn addition, summaryDS is a generic function that generates DS function result summaries:\n\n\nsummaryDS(RES1)\n\n$`Name of Gene`\n[1] \"PARP2\"\n\n$`Number of SNPs`\n[1] 6\n\n$`Name of SNPs`\n[1] \"rs3093872\" \"rs3093921\" \"rs1713411\" \"rs3093926\" \"rs3093930\"\n[6] \"rs878156\" \n\n$log10BF\n[1] -1.890008\n\n$lBFDR\n[1] 0.9627912\n\n$theta\n[1] 5.839709e-06\n\n$`Significance based on CI`\n          Study 1 Study 2 Total Pleiotropic effect\nrs3093872       0       0     0                 No\nrs3093921       0       0     0                 No\nrs1713411       0       0     0                 No\nrs3093926       0       0     0                 No\nrs3093930       0       0     0                 No\nrs878156        0       0     0                 No\n\n$`Significance based on median thresholding`\n          Study 1 Study 2 Total Pleiotropic effect\nrs3093872       0       0     0                 No\nrs3093921       0       0     0                 No\nrs1713411       0       0     0                 No\nrs3093926       0       0     0                 No\nrs3093930       0       0     0                 No\nrs878156        0       0     0                 No\n\nAs can be seen, there is no variable-level pleiotropic effect for this gene, which means that none of its SNPs were significant in both studies.\nCS function\nThe CS runs a Gibbs sampler for a multivariate Bayesian sparse group selection model with a continuous spike prior for the detection of pleiotropic effects on K traits. CS uses the same inputs as the DS function.\nThe details of the approach are given in the material and methods section (Appendix A). More information can be found at (Baghfalaki et al. 2021).\nThe CS function has the following format in general:\n\n\nCS(Betah, Sigmah, kappa0, tau20, zeta0, m, K, niter = 1000, burnin = 500,\n   nthin = 2, nchains = 2, a1 = a1, a2 = a2, c1 = c1, c2 = c2, sigma2 = 10^-3,\n   snpnames = snpnames, genename = genename)\n\n\nwhere Betah, Sigmah, kappa0, m, K, niter, burnin,\nnthin, nchains, a1, a2, snpnames and genename are the same as those introduced for the DS function. For the other arguments we have\nzeta0: Initial value for zeta. For the elicitation of initial values of zeta, first the regression coefficients and their standard errors for each study are considered. After that, by adjusting their p-values, we can find the initial values of zeta, \\(k=1, cdots, and K\\). As a result, if one group contains at least one non-null signal, the group is a non-null and zeta0\\([k]=1\\), otherwise zeta0\\([k]=0\\).\nc1, c2: Hyperparameters of tau2. Default values are c1=0.1 and c2=0.1.\nsigma2: The variance of the spike (multivariate normal distribution with a diagonal covariance matrix with small variance) represents the null effect distribution. The default setting is \\(10-3\\).\nExample 5: Using the CS method to analyze pleiotropy between breast and thyroid cancers at PARP2 gene\nFor this example, we consider the same data used for Example 4 as input. So, first, we estimated the regression coefficients for each SNP and corresponding covariance matrix for both studies (the same estimated regression coefficients and covariance matrices were computed in Example 4),\nand then, we performed the CS function on the results.\nTo get the values for \\(xi(0)_k\\), a user must first calculate the p-values and initial values of \\(xi_k\\) for each regression coefficient using the code below (for more information, see (Baghfalaki et al. 2021):\n\n\nK <- 2\nm <- 6\npvalue <- matrix(0,K,m)\nfor(k in 1:K){\n pvalue[k,] <- 2*pnorm(-abs(Betah[[k]]/sqrt(diag(Sigmah[[k]]))))\n}\nzinit <- rep(0,K)\nfor(j in 1:K){\n index <- 1:m\n PVALUE <- p.adjust(pvalue[j,])\n SIGNALS <- index[PVALUE<0.05]\n modelf1 <- rep(0,m)\n modelf1[SIGNALS] <- 1\n if(max(modelf1)==1)(zinit[j] <- 1)\n}\n\n\nThe initial value is a two-dimensional vector (as K=2) of 0 or 1:\n\n\nprint(zinit)\n\n[1] 0 0\n\nNow, we could use the CS function to look for a pleiotropic signal:\n\n\nset.seed(123)\nRES1 <- CS(Betah, Sigmah, kappa0=c(0.2,0.5), tau20=c(1,2), zeta0=zinit,\n      m=m, K=K, niter=2000, burnin=1000, nthin=2, nchains=2,\n      a1=0.1, a2=0.1, c1=0.1, c2=0.1, sigma2=10^-3, snpnames, genename)\n\n\nThe output of CS (here RES1) is the same as DS and has similar interpretations, summaryCS is a generic function used to produce result summaries of the CS function.\nHS function\nThe HS function runs a Gibbs sampler for a multivariate Bayesian sparse group selection model with hierarchical spike prior for the detection of pleiotropic effects associated with the traits at group-level and variable-level. As in the case of\nCS and DS, this function is designed to use summary statistics as inputs, containing estimated regression coefficients and their estimated covariance matrices. The following is a general usage example of the HS function:\n\n\nHS(Betah, Sigmah, kappa0 = kappa0, kappastar0 = kappastar0, sigma20 = sigma20, s20 = s20,\n    m, K, niter = 1000, burnin = 500, nthin = 2, nchains = 2, a1 = 0.1, a2 = 0.1,\n    d1 = 0.1, d2 = 0.1, c1 = 1, c2 = 1, e2 = 1, snpnames, genename)\n\n\nwhere Betah, Sigmah, kappa0, sigma20, m, K, niter, burnin,\nnthin, nchains, a1, a2, d1, d2, snpnames, and genename have similar definitions as the DS function. For the other arguments we have\nkappastar0: Initial value for kappastar such that its dimension is equal to nchains. The domain for initial values of kappastar0 is \\((0,1)\\).\ns20: Initial value for s2 such that its dimension is equal to nchains such that the domain for initial values of s2 is \\(\\Re^+\\).\nc1, c2: Hyperparameters of the kappastar. Defaults are c1=0.1 and c2=0.1.\ne2: Initial value for doing the Monte Carlo EM algorithm to estimate the hyperparameter of s2.\nExample 6: Use of the HS method to analyze pleiotropy between breast and thyroid cancers at PARP2 gene\nThe same data as in Example 4 is used as input. We used the same estimated regression coefficients and their covariance matrices.\n\n\nlibrary(GCPBayes)\ndata(PARP2)\nBreast <- PARP2$Breast\nThyroid <- PARP2$Thyroid\ngenename <- \"PARP2\"\nsnpnames <- names(PARP2$Breast)[-1]\nFit1 <- bayesreg::bayesreg(as.factor(y1)~ ., model = \"logistic\", prior = \"ridge\",data=Breast)\nBetah1 <-  Fit1$mu.beta\nSigmah1 <- cov(t(Fit1$beta))\nFit2 <- bayesreg::bayesreg(as.factor(y2)~ ., model = \"logistic\", prior = \"ridge\",data=Thyroid)\nBetah2 <-  Fit2$mu.beta\nSigmah2 <- cov(t(Fit2$beta))\nBetah <- list(Betah1,Betah2)\nSigmah <- list(Sigmah1,Sigmah2)\n\n\n\n\nset.seed(123)\nRES <- HS(Betah, Sigmah, kappa0=c(0.5,0.3), kappastar0=c(0.5,0.3), sigma20=c(2,1), s20=c(1,2),\n     m=6, K=2, niter=2000, burnin=1000, nthin=2, nchains=2,\n     a1=0.1, a2=0.1, d1=0.1, d2=0.1, c1=1, c2=1, e2=1, snpnames, genename)\n\n\nIn this example, the HS function is applied to detect group and variable pleiotropic signals. The outputs of the HS function based on median threshold are as follows:\n\n\n summaryHS(RES1)\n\n\nThe overall output of HS (RES1) is the same as that of DS and has similar interpretations.\nMCMCplot function\nMonitoring the convergence of the MCMC statistic is important and the Gelman-Rubin statistic is not the only way to assess convergence. MCMCplot presents some visual plots, such as trace, auto-correlation function (ACF) and posterior density plots to check the convergence of the MCMC chains.\nThe trace plot is\nthe plot of the generated values over the course of the iterations. If\nall values fall within a uniform bandwidth around the posterior mean and do not display any periodicity or non-stationary trends, we can assume convergence. The density plot represents the signal’s estimated posterior distribution. Also, monitoring ACF plots is very useful since low or high values indicate fast or slow convergence, respectively. In fact, we need to monitor the ACF of the MCMC generated sample and select a sampling lag larger than 1 [=L]. Then, we can produce an independent sample by keeping the first generated values in every batch of L iterations, which is called the thinning rate.\nIn practice, the ACF should be close to zero for a sufficiently large number of lags.\nThe inputs of the MCMCplot function are the generated results by DS, CS, or HS function, the number of studies for drawing plots, the number of MCMC chains in the result, and\nthe names of favorable SNPs for drawing the plots.\nUp to 10 plots can be plotted simultaneously. In general, a usage for the MCMCplot function is as follows:\nMCMCplot(Result, k, nchains, whichsnps, betatype, acftype, dencol, denlty, denbg)\nwhere\nResult: All of the results produced by the DS, CS, and HS functions.\nk: The number of plotting studies, where k = 1, 2,…, K.\nnchains: The number of Markov chains that were run to produce Result.\nwhich SNPs: The SNPs’ names\nbetatype: The type of plot desired. The following values are possible: “p” for points, “l” for lines, “b” for both points and lines, “c” for empty points joined by lines, “o” for overplotted points and lines, “s” and “S” for stair steps, and “h” for histogram-like vertical lines. Finally, “n” does not produce any points or lines.\nacftype: String giving the type of ACF to be computed. Allowed values are “correlation” (the default), “covariance” or “partial” will be partially matched.\ndencol: The color used to fill out the density plot.\ndenlty: The line type to be used in the density plot.\ndenbg The color to be used for the background of the density plot.\nExample 7: Using the MCMCplot function to check convergence of the DS method\nFor this example, the output of the DS function for PARP2 gene is used when drawing the MCMCplot. We select this gene because it is a special case because it required more MCMC iterations to converge.\nTo run this example, a user needs to run the first fourteen lines of code in Example 4 (briefly, these commands compute regression coefficients and covariance matrices based on the individual-level data).\nThen the DS function needs to be run as follows:\n\n\nlibrary(GCPBayes)\ndata(PARP2)\nBreast <- PARP2$Breast\nThyroid <- PARP2$Thyroid\ngenename <- \"PARP2\"\nsnpnames <- names(PARP2$Breast)[-1]\nFit1 <- bayesreg::bayesreg(as.factor(y1)~ ., model = \"logistic\", prior = \"ridge\",data=Breast)\nBetah1 <-  Fit1$mu.beta\nSigmah1 <- cov(t(Fit1$beta))\nFit2 <- bayesreg::bayesreg(as.factor(y2)~ ., model = \"logistic\", prior = \"ridge\",data=Thyroid)\nBetah2 <-  Fit2$mu.beta\nSigmah2 <- cov(t(Fit2$beta))\nBetah <- list(Betah1,Betah2)\nSigmah <- list(Sigmah1,Sigmah2)\n\nset.seed(123)\nRES1 <- DS(Betah, Sigmah, kappa0=c(0.2,0.5), sigma20=c(1,2),\n           m=6, K=2, niter=200, burnin=100, nthin=1, nchains=2,\n           a1=0.1, a2=0.1, d1=0.1, d2=0.1, snpnames, genename)\nMCMCplot(Result = RES1, k = 2, nchains = 2, whichsnps = snpnames,\n                     betatype = \"l\",\n                     acftype = \"correlation\",\n                     dencol = \"white\", denlty = 1, denbg = \"white\")\n\n\nset.seed(123)\nRES1 <- DS(Betah, Sigmah, kappa0=c(0.2,0.5), sigma20=c(1,2),\n           m=6, K=2, niter=2000, burnin=1000, nthin=2, nchains=2,\n           a1=0.1, a2=0.1, d1=0.1, d2=0.1, snpnames, genename)\nMCMCplot(Result = RES1, k = 2, nchains = 2, whichsnps = snpnames,\n                     betatype = \"l\",\n                     acftype = \"correlation\",\n                     dencol = \"white\", denlty = 1, denbg = \"white\")\n\n\nThe plots for the thyroid cancer study with 200 MCMC iterations and 100 burn-in are given in Figure 2.\nThe top row of the figure shows strong periodicities in the posterior samples for some SNPs, which are confirmed by their corresponding ACF plots (last row). As a result, more samples are needed to achieve convergence for the PARP2 gene. In the second DS command, the number of iterations was increased to 2000; half of this number was considered burn-in, and the thinning rate was set to 2. The results are shown in Figure 3\nand the resulting regularities in the plot implies that increasing the number of iterations has made the model converge successfully.\n\n\n\nFigure 1: Trace, density and ACF plot for the posterior samples of the regression coefficients (niter=200, burnin=100, nthin=2) for thyroid study. Based on the top and last rows, the results do not show a convergence.\n\n\n\n\n\n\nFigure 2: Trace, density and ACF plot for the posterior samples of the regression coefficients (niter=2000, burnin=1000, nthin=5) for thyroid study. Based on the top and last rows, the results imply a convergence.\n\n\n\nComparing GCPBayes results using summary statistics level versus individual level data as input\nAs we mentioned earlier, we showed that the performance of the GCPBayes package is better when using non-diagonal\ncovariance matrices (individual-level data) compared to diagonal covariance matrices (summary level data, Baghfalaki et al. (2021)). In this section, we show two examples to deal with such differences.\nExample 8: Comparing DS function outputs using summary statistics level versus individual-level data of PARP2 gene as inputs\nFirst, we consider summary statistics level data for PARP2 gene as an input of the GCPBayes package and run the DS function for this data as follows:\n\n\nlibrary(GCPBayes)\ndata(PARP2_summary)\nBreast <- PARP2_summary$Breast\nThyroid <- PARP2_summary$Thyroid\ngenename <- \"PARP2\"\nsnpnames <- rownames(Breast)\nBetah <- list(Breast$beta,Thyroid$beta)\nSigmah <- list(diag(Breast$se^2),diag(Thyroid$se^2))\nprint(Betah,digits=2)\nprint(Sigmah,digits=2)\nset.seed(123)\nRES_sum <- DS(Betah, Sigmah, kappa0=c(0.2,0.5), sigma20=c(1,2),\n                            m=6, K=2, niter=2000, burnin=1000, nthin=2, nchains=2,\n                            a1=0.1, a2=0.1, d1=0.1, d2=0.1, snpnames, genename)\nprint(RES_sum$Criteria,digits=2)\n\n\nNow, we use individual data for the PARP2 gene,\nthen compute regression coefficients and covariance matrices based on the individual data and used it as the input to the GCPBayes package. Finally, we run the DS function for this data as follows:\n\n\nlibrary(GCPBayes)\ndata(PARP2)\nBreast <- PARP2$Breast\nThyroid <- PARP2$Thyroid\ngenename <- \"PARP2\"\nsnpnames <- names(PARP2$Breast)[-1]\nFit1 <- bayesreg::bayesreg(as.factor(y1)~ ., model = \"logistic\", prior = \"ridge\",data=Breast)\nBetah1 <-  Fit1$mu.beta\nSigmah1 <- cov(t(Fit1$beta))\nFit2 <- bayesreg::bayesreg(as.factor(y2)~ ., model = \"logistic\", prior = \"ridge\",data=Thyroid)\nBetah2 <-  Fit2$mu.beta\nSigmah2 <- cov(t(Fit2$beta))\nBetah <- list(Betah1,Betah2)\nSigmah <- list(Sigmah1,Sigmah2)\nset.seed(123)\nRES_ind <- DS(Betah, Sigmah, kappa0=c(0.2,0.5), sigma20=c(1,2),\n                            m=6, K=2, niter=2000, burnin=1000, nthin=2, nchains=2,\n                            a1=0.1, a2=0.1, d1=0.1, d2=0.1, snpnames, genename)\n#print(RES_ind$Criteria,digits=2)\n\n\nBy comparing the results for these two scenarios, it is clear that using individual-level data,\nwhich includes more information about the SNP correlations,\nwould lead to less biased results and hence better signals at the end.\nExample 9: Investigation of pleiotropic effect between breast cancer and thyroid cancer at DNAJC1 gene in large datasets\nFor the final example, we consider the summary statistics for the DNAJC1 protein-coding gene, specifically betas and standard errors, for fourteen SNPs from two studies (on breast and thyroid cancers), and try to investigate any potential pleiotropic signal at group and individual levels. The summary statistics of the breast and thyroid cancer studies were extracted from the Breast Cancer Association Consortium (BCAC) (Zhang et al. 2020) and the EPITHYR consortium (Truong et al. 2021), respectively.\nThe data is embedded in the GCPBayes package, and a user can run the DS function as follows:\n\n\nlibrary(GCPBayes)\ndata(DNAJC1)\nBreast <- DNAJC1$Breast\nThyroid <- DNAJC1$Thyroid\ngenename <- \"DNAJC1\"\nsnpnames <- Breast$snp\nBetah <- list(Breast$beta,Thyroid$beta)\nSigmah <- list(diag(Breast$se^2),diag(Thyroid$se^2))\nK <- 2\nm <- 14\nset.seed(123)\nRES1 <- DS(Betah, Sigmah, kappa0=c(0.2,0.5), sigma20=c(1,2),\n            m=m, K=K, niter=2000, burnin=1000, nthin=2, nchains=2,\n            a1=0.1, a2=0.1, d1=0.1, d2=0.1, snpnames, genename)\n\n\nNow, we run the following command for testing the global null hypothesis:\n\n\nprint(RES1$Criteria)\n\n$`Name of Gene`\n[1] \"DNAJC1\"\n\n$`Name of SNPs`\n [1] \"rs10740997\" \"rs10764330\" \"rs10828266\" \"rs12570400\" \"rs1970467\" \n [6] \"rs2666762\"  \"rs2807967\"  \"rs35759613\" \"rs3951780\"  \"rs4747438\" \n[11] \"rs60773921\" \"rs6650129\"  \"rs7075508\"  \"rs77353976\"\n\n$PPA\n$PPA[[1]]\n[1] 1\n\n$PPA[[2]]\n[1] 1\n\n\n$log10BF\n[1] 47.29012\n\n$lBFDR\n[1] 1.709075e-48\n\n$theta\n[1] 0.9998224\n\nBased on the values of log10BF and lBFDR, the null hypothesis is rejected. Thus, we check the value of theta which is equal to 0.999, and conclude that the gene has a group pleiotropic effect.\nA user should run the following command to test for variable pleiotropic effects:\n\n\nprint(RES1$Indicator$`Significant studies and Pleiotropic effect based on CI`)\n\n           Study 1 Study 2 Total Pleiotropic effect\nrs10740997       1       0     1                 No\nrs10764330       1       0     1                 No\nrs10828266       1       0     1                 No\nrs12570400       1       0     1                 No\nrs1970467        1       0     1                 No\nrs2666762        1       1     2                Yes\nrs2807967        1       0     1                 No\nrs35759613       1       0     1                 No\nrs3951780        1       0     1                 No\nrs4747438        1       1     2                Yes\nrs60773921       1       0     1                 No\nrs6650129        1       0     1                 No\nrs7075508        1       0     1                 No\nrs77353976       0       0     0                 No\n\nBased on the results, two SNPs are significantly associated with both traits (rs4747438, and rs2666762).\n4 Guidelines\nIn this section, we provide some guidelines for working with the GCPBayes package and for extracting potential pleiotropic effects at group and variable-levels.\nWe first give some practical advice on which methods to consider and in what order, and then we provide a decision pipeline based on statistical inferences, firstly for a primary analysis by using DS (or CS), and secondly for a precision analysis by using HS.\nPractical hints\nWe do not recommend that\na user do initial exploration by using the HS function because it may take a long time to compute when working with large datasets.\nBesides, the HS function does not show better power in for detecting group pleiotropy compared to the other methods (DS and CS).\nIn addition, DS and CS methods have similar strategies, but we showed that DS performed better than CS during the simulations for all tested scenarios (Baghfalaki et al. 2021).\nThat is why we suggest the user first performs the\nthe pleiotropic analysis on all groups in the data by using the DS function (Figure 3).\nThus, in order to select groups with probable pleiotropic effects, we recommend performing a first batch of analyses by using the DS method, with a reasonable number of iterations\nsuch that most of the groups converge, using 2 chains to compute the BGR and test the convergence of the method.\nWe suggest performing the method with 2,000 iterations and 1,000 burn-in.\nThen, if the BGR values for one or more SNPs in the group are significantly different\nfrom 1 (values between 0.9 and 1.1 are typically used as threshold for reasonable convergence),\nthe method should be run again for the group using higher number for iterations and burn-in.\nWe recommend to repeatedly double these values until the convergence threshold is reached.\nStatistical inference pipeline for primary analysis\nWe provide a diagram in Figure 3 that helps the user detect group and variable pleiotropic effects using the GCPBayes package.\nThe first step for detection of a pleiotropic signal is to test the global null hypothesis of no association against the global alternative hypothesis of association with at least one trait by considering DS (or CS).\nConsidering the CS method of the package (please refer to the corresponding equations in Appendix A), we define the following global null hypothesis of no association as follows:\n\\[\\begin{eqnarray}\\label{testC}\n{\\rm{H}_0}:{\\xi_1} = {\\xi_2} = \\cdots = {\\xi_K} = 0{\\rm{~~~~~  }} \\textrm{versus }\n{\\rm{H}_1}:{\\rm{at~ least~ one ~}}{{\\xi}_k} \\ne 0,k = 1,\\cdots,K.\n\\end{eqnarray}\\]\nTo perform this test, the package computes two quantities. Let et \\(P(\\rm{H}_0|\\cal{D})\\) be the posterior probability of the null hypothesis, i.e. the probability of making a false discovery for a non-null effect, which (Efron 2012) refers to as the \\(\\cal{D}\\) local Bayesian false discovery rate (denoted by \\(lBFDR\\)). The phrase ``local” comes from a single point, \\(\\{0\\}\\), as the domain of the null hypothesis tested (Efron 2012). Note that small values of \\(lBFDR\\) show strong evidence for the existence of a substantive effect.\nWe recommend using the usual value of 0.05 as a threshold to\ndecide whether to reject the null hypothesis or not.\nThe Bayes factor (BF), which describes the evidence of \\(\\rm{H}_1\\) versus \\(\\rm{H}_0\\) could also be considered.\nIn contrast to the lBFDR, a high Bayes factor (BF) value indicates strong evidence in favor of \\(\\rm{H}_1\\). We recommend considering a value of 1 as a threshold (or log10BF > 0).\nThe marginal study-specific posterior probability of association (PPA) proposed by Majumdar et al. (Majumdar et al. 2018) is also computed to quantify the relative contribution of a study underlying the signal.\nThis is defined for each study \\(k\\) by \\({PPA _k} = \\frac{1}{M}\\sum\\limits_{r = 1}^M {\\xi _k^{(r)}}\\).\nThis corresponds to the probability that the study \\(k\\) was drawn from the slab distribution, i.e. the probability that the group has a non-zero effect.\nFor checking any association by using DS, the global test is defined by:\n\\[\\begin{eqnarray}\\label{testD}\n\\begin{array}{l}\n{\\rm{H}_0}:{\\beta _1} = \\cdots = {\\beta _K} = 0, \\rm{versus~~}\n{H_1}:{\\rm{at~ least ~one~}}{\\beta _k} \\ne 0,{\\rm{ ~}}k = 1,\\cdots,K.\n\\end{array}\n\\end{eqnarray}\\]\nThe existence of an association when using DS can be defined in the same way as for CS.\nIf there is no sufficient evidence for \\(H_1\\) (i.e., a non-null effect in at least one study), then there is no need to further consider group pleiotropy or variable pleiotropy.\nIf there is evidence for \\(H_1\\), the user can calculate the value of the DS function’s \\(theta\\). The strategy to detect group pleiotropic effects based on CS is to compute \\(\\theta\\) as follows:\n\\[\\begin{eqnarray}\\label{theta}\n\\theta=P\\left(\\sum_{k=1}^{K} \\xi_k\\geq 2 | \\hat{\\beta}_k, \\hat{\\Sigma}_k,~k=1,\\cdots,K\\right).\n\\end{eqnarray}\\]\nNote that for computing \\(\\theta\\) for DS, we can follow the same strategy as for CS. Hence, \\(\\theta\\) represents the probability of having a non-zero effect in at least two studies.\nThus, it is reasonable to consider a threshold \\(t\\) of 0.5 to statistically determine whether there is pleiotropy or not. However, a user wishing to generate many tentative hypotheses could choose a less stringent threshold (i.e., 0.1).\nConversely, a user could choose a very strict threshold (i.e., 0.9) to select only the most certain groups.\nIf \\(\\theta \\leq t\\), no pleiotropic effect is detected at the group level, and thus no pleiotropy signal is present at the variable level.\nHowever, if \\(\\theta> t\\), we can consider the group to have a pleiotropic effect.\nIt should be mentioned that when a pleiotropic effect is detected for a group, we can investigate the pleiotropy for each variable in the group.\nIn this case, as CS and DS are defined group by group, the use of Bayesian point estimations (the means of the posterior distributions) and \\(95\\%\\) credible intervals from the DS function can be used as criteria for variable selection.\nThe \\(j^{th}\\) variable has a pleiotropic effect if at least two \\(\\beta_{kj}\\)s are chosen. For example, assume that a pleiotropic effect is detected in an analysis of two studies.\nThe posterior estimates \\(\\hat{\\beta}_{1j}\\) and \\(\\hat{\\beta}_{2j}\\) can then be used as a pleiotropy decision tool for the \\(j^{th}\\) variable \\(j=1,2,\\cdots,m\\).Hence, if \\(\\beta_{1j}\\) and \\(\\beta_{2j}\\) are two non-zero signals,\nThen the \\(j^{th}\\) variable is declared to have a pleiotropic effect; otherwise, the \\(j^{th}\\) variable has no pleiotropic effect.\nStatistical inferences for analysis by using HS\nBecause HS has shown a higher power to detect pleiotropy at the variable-level in simulations, we recommend using the HS approach for the groups initially detected as pleiotropic via the DS function.\nA user can move down the \\(\\theta\\) threshold in order to explore potential pleiotropic effects that could have been detected with a larger sample size (i.e. to consider \\(\\theta \\geq 0.1\\) from DS).\nThen, the user can compare the results at the variable-level for DS and HS (Figure 3).\nUsing this overall strategy, a user would have to run the HS method on a smaller number of groups and thus avoids dealing with long computational time of the HS function for all groups at once.\nTo detect a pleiotropic signal for the \\(j^{th}\\) variable, the posterior estimates \\(\\hat{\\beta}_{kj}\\), \\(j=1,2,\\cdots,m\\), \\(k=1,2,\\cdots,K\\), and \\(k=1,2,\\cdots,K\\) can be used.\nWe consider the variable \\(j\\) to have pleiotropic effect if and only if there are at least two non-zero signals among the \\(K\\) studies.\nIt should be noted that, in addition to a \\(95\\%\\) CI, the sparseness of the posterior median of the regression coefficient \\(\\beta_{kj}, ~k=1,\\cdots,K,~j=1,2,\\cdots,m_g\\) can be used for this purpose.\nPleiotropy should be considered at the variable level based on the posterior median: for example, the \\(j^{th}\\) variable can be considered pleiotropic if the posterior median of a \\(\\beta_{kj}\\) is different from 0 for at least two studies \\(k\\).\nFinally, pleiotropy is considered at the group level if at least one pleiotropic variable is found within the group. Moreover, HS allows for the detection of pleiotropy at the group-level in other situations. Note that the pleiotropy effect can take on different shapes.\nAs presented in (Solovieff et al. 2013), biological pleiotropy at the gene level can occur when two different signals within the same gene are associated with two different phenotypes.\nThis can affect both phenotypes in a similar way.\nTherefore, we also consider pleiotropy at the group-level when an association is detected (not necessarily detected as pleiotropic) for different variables within the group in at least two different studies.\n5 Computational time for GCPBayes\nIn this section, the computational time of GCPBayes package is explored\nby re-analysing of the experimental data for nine groups (genes) described in (Baghfalaki et al. 2021).\nThe analysis has been conducted using a PC with a Core(TM) i5-4690K CPU \\(@\\) 3.50 GHz processor and 8 GB of memory. For this purpose, we consider nine genes with different numbers of variables (SNPs).\nTo fit the models, we used 2,000 MCMC iterations and discarded half as burn-in. Table 1 represents the results of computational times using the GCPBayes package for analyzing the data. As expected, the computational time increases when the number of variables within a group increases. The computational time for the DS method is the shortest, and the more complex HS model HS has the longest computational time, as expected. Another exploration of computational time for the GCPBayes package, based on simulation studies, see (Baghfalaki et al. 2021).\nWe have also applied GCPBayes on whole genome data of breast and ovarian cancers (summary statistics data). Running GCPBayes over all coding genes (number of genes: 18,244 and number of SNPs: 1 to 9,595) took about 17 days (on a PC with Intel \\(®\\) Core(TM) i7-1165G7 \\(@\\) 2.80 GHz, 32 GB RAM). However,\nthe user takes advantage of parallelization (using “foreach” command),\nthe computational time can be reduced. For instance, the computational time was about 40 hours using the same PC but running six genes in each loop (i.e., using six CPUs instead of one).\n\n\n\nFigure 3: A diagram for detecting group and variable pleiotropic effects using GCPBayes package. For more information see the text.\n\n\n\n6 Concluding remarks\nWe have developed the R package GCPBayes that implements several Bayesian meta-analysis methods for studying cross-phenotype genetic associations (pleiotropy), taking into account group structure from prior biological knowledge. GCPBayes offers multivariate spike and slab priors for group-level pleiotropy, using either DS or CS formulations, and for pleiotropy at both group and variable-level using the hierarchical spike prior (HS) formulation. These approaches take into account\nthe heterogeneity in size and direction of the genetic effects across traits. Although DS and CS are not designed for variable selection, these methods do allow us to\ninvestigate the pleiotropy for each variable in the group by using \\(95\\%\\) credible intervals and the median threshold as criteria for variable selection in groups for which a pleiotropic effect has been detected. On the other hand, the posterior estimates can be used to detect a pleiotropic signal for each variable using HS. Thus, a variable is considered to have a pleiotropic effect if and only if there are at least two non-zero signals among the studies. For this purpose, in addition to the \\(95\\%\\) credible intervals, the sparseness of the posterior median of the regression coefficients is reported. Also, it should be mentioned that GCPBayes package provides marginal trait-specific posterior probability of association of each study (PPA), direction of associations for each variable, Bayes factor and local Bayesian false discovery rate of the global hypothesis, a statistical criterion for detection of group pleiotropic signal (\\(\\theta\\)), credible interval and median of the variables, summary information of the variables, and the generated MCMC samples.\nBesides, as we mentioned, HS allows us to consider the pleiotropy at the group-level if at least one pleiotropic variable is detected within the group. It also allows detection of pleiotropy at group-level without pleiotropy at variable-level if two or more different variables are significant in different studies (Baghfalaki et al. 2021).\nThis might happen if,\nfor example, the same gene (group) may have a similar effect on several phenotypes through different genetic markers (Solovieff et al. 2013).\nWe provided nine examples to demonstrate the different ways of working with the package using both simulated and real data. In addition, three more examples are provided in Appendix B to show the ability of the GCPBayes package to deal with various types of input data. As we mentioned earlier, while the input of the package is summary statistics data, it is possible to use the package for investigating pleiotropy in various kinds of phenotypes. So, a user look for pleiotropic signals in different types of phenotypes, including categorical, continuous, mix categorical and continuous, survival data, without any limitation.\nHowever, there are still some limitations that a user should consider while working with the package. For example, for more accurate detection of pleiotropic signals, a large enough sample size for the input data should be provided. This means having a larger sample size in the individual-level data\n(from which the summary statistics are calculated from) would lead to better accuracy of the method for detecting pleiotropic signals.\nBesides, the GCPBayes package is designed for uncorrelated studies (no overlapping samples between studies). So, an improvement of the package for correlated studies could be considered for the future.\nAlso, GCPBayes can deal with correlated samples in uncorrelated studies as it uses summary statistics. Though the user should exercise caution, the summary statistics has been calculated using methods which take into account such correlations between samples.\nMore user-friendly functions for preparing the data, for pre-selecting, and assigning key variables into groups before applying the GCPBayes methods, are currently in development.\nFurthermore, since the main focus of the package is on the detection of pleiotropic effects using GWAS data, we are currently developing guidelines to optimize\nthe package for use in real-world large-scale data by taking into account factors such as optimal\niterations of the MCMC step for convergence and the development of a general strategy for dealing with\ngenes with a large number of SNPs or large sizes.\nAcknowledgements\nThe authors acknowledge the calculus center MCIA (Mésocentre de Calcul Intensif Aquitain) for providing its facilities. The `Ligue contre le Cancer’ is acknowledged as well for its support for the “Cross Cancer Genomic Investigation of Pleiotropy project”. The INSERM and Aviesan ITMO cancer are acknowledged as well for their support for “Advanced Machine Learning Algorithms for leveraging Pleiotropy effect project”. This project is also part of the INSERM cross-cutting project GOLD.\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-028.zip\nCRAN packages used\nGCPBayes, CPBayes, car, lrmest, ridge, lmridge, bayesreg, BhGLM\nCRAN Task Views implied by cited packages\nBayesian, Econometrics, Finance, MetaAnalysis, MixedModels, TeachingStatistics\nBioconductor packages used\nASSET, GPA\n\n\nA. Agresti. An introduction to categorical data analysis. John Wiley & Sons, 2018.\n\n\nT. Baghfalaki, P.-E. Sugier, T. Truong, A. N. Pettitt, K. Mengersen and B. Liquet. Bayesian meta-analysis models for cross cancer genomic investigation of pleiotropic effects using group structure. Statistics in Medicine, 40(6): 1498–1518, 2021.\n\n\nS. Bhattacharjee, P. Rajaraman, K. B. Jacobs, W. A. Wheeler, B. S. Melin, P. Hartge, M. Yeager, C. C. Chung, S. J. Chanock and N. Chatterjee. A subset-based approach improves power and interpretation for the combined analysis of genetic association studies of heterogeneous traits. The American Journal of Human Genetics, 90(5): 821–835, 2012.\n\n\nC. Broc, T. Truong and B. Liquet. Penalized partial least squares for pleiotropy. BMC bioinformatics, 22(1): 1–31, 2021.\n\n\nS. P. Brooks and A. Gelman. General methods for monitoring convergence of iterative simulations. Journal of computational and graphical statistics, 7(4): 434–455, 1998.\n\n\nD. Chung, C. Yang, C. Li, J. Gelernter and H. Zhao. GPA: A statistical approach to prioritizing GWAS results by integrating pleiotropy and annotation. PLoS Genet, 10(11): e1004787, 2014.\n\n\nP. K. Dunn and G. K. Smyth. Generalized linear models with examples in r. Springer, 2018.\n\n\nB. Efron. Bootstrap methods: Another look at the jackknife. In Breakthroughs in statistics, pages. 569–593 1992. Springer.\n\n\nB. Efron. Large-scale inference: Empirical bayes methods for estimation, testing, and prediction. Cambridge University Press, 2012.\n\n\nJ. Fox and S. Weisberg. An r companion to applied regression. Sage publications, 2018.\n\n\nJ. Fox, S. Weisberg, D. Adler, D. Bates, G. Baud-Bovy, S. Ellison, D. Firth, M. Friendly, G. Gorjanc, S. Graves, et al. Package “car.” Vienna: R Foundation for Statistical Computing, 16: 2012.\n\n\nJ. Gareth, W. Daniela, H. Trevor and T. Robert. An introduction to statistical learning: With applications in r. Spinger, 2013.\n\n\nA. Gelman, D. B. Rubin, et al. Inference from iterative simulation using multiple sequences. Statistical science, 7(4): 457–472, 1992.\n\n\nE. I. George and R. E. McCulloch. Variable selection via gibbs sampling. Journal of the American Statistical Association, 88(423): 881–889, 1993.\n\n\nS. Hackinger and E. Zeggini. Statistical methods to detect pleiotropy in human complex traits. Open biology, 7(11): 170125, 2017.\n\n\nD. E. Hilt and D. W. Seegrist. Ridge, a computer program for calculating ridge regression estimates. Department of Agriculture, Forest Service, Northeastern Forest Experiment …, 1977.\n\n\nR. Johnston, K. Jones and D. Manley. Confounding and collinearity in regression analysis: A cautionary tale and an alternative procedure, illustrated by studies of british voting behaviour. Quality & quantity, 52(4): 1957–1976, 2018.\n\n\nE. Krapohl, H. Patel, S. Newhouse, C. J. Curtis, S. von Stumm, P. S. Dale, D. Zabaneh, G. Breen, P. F. OReilly and R. Plomin. Multi-polygenic score approach to trait prediction. Molecular psychiatry, 23(5): 1368–1374, 2018.\n\n\nX. Li, Z. Li, H. Zhou, S. M. Gaynor, Y. Liu, H. Chen, R. Sun, R. Dey, D. K. Arnett, S. Aslibekyan, et al. Dynamic incorporation of multiple in silico functional annotations empowers rare variant association analysis of large whole-genome sequencing studies at scale. Nature genetics, 52(9): 969–983, 2020.\n\n\nZ. Liu and X. Lin. Multiple phenotype association tests using summary statistics in genome-wide association studies. Biometrics, 74(1): 165–175, 2018.\n\n\nQ. Lu, B. Li, D. Ou, M. Erlendsdottir, R. L. Powles, T. Jiang, Y. Hu, D. Chang, C. Jin, W. Dai, et al. A powerful approach to estimating annotation-stratified genetic covariance via GWAS summary statistics. The American Journal of Human Genetics, 101(6): 939–964, 2017.\n\n\nA. Majumdar, T. Haldar, S. Bhattacharya and J. S. Witte. An efficient bayesian meta-analysis approach for studying cross-phenotype genetic associations. PLoS genetics, 14(2): e1007139, 2018.\n\n\nE. Makalic and D. F. Schmidt. High-dimensional bayesian regularised regression with the BayesReg package. arXiv preprint arXiv:1611.06649, 2016.\n\n\nN. Malo, O. Libiger and N. J. Schork. Accommodating linkage disequilibrium in genetic-association analyses via ridge regression. The American Journal of Human Genetics, 82(2): 375–385, 2008.\n\n\nP. McCullagh. Generalized linear models. Routledge, 2019.\n\n\nS. Menard. Applied logistic regression analysis. Sage, 2002.\n\n\nT. J. Mitchell and J. J. Beauchamp. Bayesian variable selection in linear regression. Journal of the American Statistical Association, 83(404): 1023–1032, 1988.\n\n\nD. Ray and N. Chatterjee. A powerful method for pleiotropic analysis under composite null hypothesis identifies novel shared loci between type 2 diabetes and prostate cancer. PLoS genetics, 16(12): e1009218, 2020.\n\n\nA. M. E. Saleh, M. Arashi and B. G. Kibria. Theory of ridge regression estimation with applications. John Wiley & Sons, 2019.\n\n\nN. Solovieff, C. Cotsapas, P. H. Lee, S. M. Purcell and J. W. Smoller. Pleiotropy in complex traits: Challenges and strategies. Nature Reviews Genetics, 14(7): 483–495, 2013.\n\n\nH. Trochet, M. Pirinen, G. Band, L. Jostins, G. McVean and C. C. Spencer. Bayesian meta-analysis across genome-wide association studies of diverse phenotypes. Genetic epidemiology, 43(5): 532–547, 2019.\n\n\nT. Truong, F. Lesueur, P.-E. Sugier, J. Guibon, C. Xhaard, M. Karimi, O. Kulkarni, E. A. Lucotte, D. Bacq-Daian, A. Boland-Auge, et al. Multiethnic genome-wide association study of differentiated thyroid cancer in the EPITHYR consortium. International Journal of Cancer, 148(12): 2935–2946, 2021.\n\n\nA. Verma, S. S. Verma, S. A. Pendergrass, D. C. Crawford, D. R. Crosslin, H. Kuivaniemi, W. S. Bush, Y. Bradford, I. Kullo, S. J. Bielinski, et al. eMERGE phenome-wide association study (PheWAS) identifies clinical associations and pleiotropy for stop-gain variants. BMC Medical Genomics, 9(1): 19–25, 2016.\n\n\nE. Vittinghoff, D. V. Glidden, S. C. Shiboski and C. E. McCulloch. Regression methods in biostatistics: Linear, logistic, survival, and repeated measures models. Springer Science & Business Media, 2011.\n\n\nK. Watanabe, S. Stringer, O. Frei, et al. A global overview of pleiotropy and genetic architecture in complex traits. Nature Genetics, 51(9): 1339–1348, 2019.\n\n\nX. Xu, M. Ghosh, et al. Bayesian variable selection and estimation for group lasso. Bayesian Analysis, 10(4): 909–936, 2015.\n\n\nN. Yi, Z. Tang, X. Zhang and B. Guo. BhGLM: Bayesian hierarchical GLMs and survival models, with applications to genomics and epidemiology. Bioinformatics, 35(8): 1419–1421, 2019.\n\n\nH. Zhang, T. U. Ahearn, J. Lecarpentier, D. Barnes, J. Beesley, G. Qi, X. Jiang, T. A. O’Mara, N. Zhao, M. K. Bolla, et al. Genome-wide association study identifies 32 novel breast cancer susceptibility loci from overall and subtype-specific analyses. Nature genetics, 52(6): 572–581, 2020.\n\n\n\n\n",
    "preview": "articles/RJ-2023-028/distill-preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 1500,
    "preview_height": 694
  },
  {
    "path": "articles/RJ-2023-029/",
    "title": "rankFD: An R Software Package for Nonparametric Analysis of General Factorial Designs",
    "description": "Many experiments can be modeled by a factorial design which allows statistical analysis of main factors and their interactions. A plethora of parametric inference procedures have been developed, for instance based on normality and additivity of the effects. However, often, it is not reasonable to assume a parametric model, or even normality, and effects may not be expressed well in terms of location shifts. In these situations, the use of a fully nonparametric model may be advisable. Nevertheless, until very recently, the straightforward application of nonparametric methods in complex designs has been hampered by the lack of a comprehensive R package. This gap has now been closed by the novel R-package [rankFD](https://CRAN.R-project.org/package=rankFD) that implements current state of the art nonparametric ranking methods for the analysis of factorial designs. In this paper, we describe its use, along with detailed interpretations of the results.",
    "author": [
      {
        "name": "Frank Konietschke",
        "url": {}
      },
      {
        "name": "Edgar Brunner",
        "url": {}
      }
    ],
    "date": "2023-08-26",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-029.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-030/",
    "title": "The segmetric Package: Metrics for Assessing Segmentation Accuracy for Geospatial Data",
    "description": "Segmentation methods are a valuable tool for exploring spatial data by identifying objects based on images' features. However, proper segmentation assessment is critical for obtaining high-quality results and running well-tuned segmentation algorithms Usually, various metrics are used to inform different types of errors that dominate the results. We describe a new R package, [segmetric](https://CRAN.R-project.org/package=segmetric), for assessing and analyzing the geospatial segmentation of satellite images. This package unifies code and knowledge spread across different software implementations and research papers to provide a variety of supervised segmentation metrics available in the literature. It also allows users to create their own metrics to evaluate the accuracy of segmented objects based on reference polygons. We hope this package helps to fulfill some of the needs of the R community that works with Earth Observation data.",
    "author": [
      {
        "name": "Rolf Simoes",
        "url": {}
      },
      {
        "name": "Alber Sanchez",
        "url": {}
      },
      {
        "name": "Michelle C. A. Picoli",
        "url": {}
      },
      {
        "name": "Patrick Meyfroidt",
        "url": {}
      }
    ],
    "date": "2023-08-26",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-034/",
    "title": "Fairness Audits and Debiasing Using mlr3fairness",
    "description": "Given an increase in data-driven automated decision-making based on machine learning  (ML) models, it is imperative that, along with tools to develop and improve such models, there are sufficient capabilities to analyze and assess models with respect to potential biases. We present the package mlr3fairness, a collection of metrics and methods that allow for the assessment of bias in machine learning models. Our package implements a variety of widely used fairness metrics that can be used to audit models for potential biases, along with a set of visualizations that can help to provide additional insights into such biases.  mlr3fairness furthermore integrates bias mitigation methods for machine learning models through data pre-processing or post-processing of predictions.  These allow practitioners to trade off performance and fairness metrics that are appropriate for their use case.",
    "author": [
      {
        "name": "Florian Pfisterer",
        "url": {}
      },
      {
        "name": "Siyi Wei",
        "url": {}
      },
      {
        "name": "Sebastian Vollmer",
        "url": {}
      },
      {
        "name": "Michel Lang",
        "url": {}
      },
      {
        "name": "Bernd Bischl",
        "url": {}
      }
    ],
    "date": "2023-08-26",
    "categories": [],
    "contents": "\n1 Introduction\nHumans are increasingly subject to data-driven automated decision-making.\nThose automated procedures, such as credit risk assessments, are often applied using predictive models (Galindo and Tamayo 2000; Kozodoi et al. 2022), often profoundly affecting individual’s lives.\nIt is therefore important that, along with tools to develop and improve such models, we also develop sufficient capabilities to analyze and assess models not only with respect to their robustness and predictive performance but also with respect to potential biases.\nThis is highlighted by the European General Data Protection Regulation (GDPR) which requires data to be processed fairly.\nPopular modelling frameworks for the R language (R Core Team 2021) such as caret (Kuhn 2021), tidymodels (Kuhn and Wickham 2020), SuperLearner (Polley et al. 2021), or mlr (Bischl et al. 2016) implement a plethora of metrics to measure performance, but fairness metrics are widely missing.\nThis lack of availability can be detrimental to obtaining fair and unbiased models if the result is to forgo bias audits due to the considerable complexity of implementing such metrics.\nConsequently, there exists a considerable necessity for R packages to (a) implement such metrics, and (b) to connect these metrics to existing ML frameworks.\nIf biases are detected and need to be mitigated, we might furthermore want to employ bias mitigation techniques that tightly integrate with the fitting and evaluation of the resulting models in order to obtain trade-offs between a model’s fairness and utility (e.g., predictive accuracy).\nIn this article, we present the mlr3fairness package which builds upon the ML framework mlr3 (Lang et al. 2019).\nOur extension contains fairness metrics, fairness visualizations, and model-agnostic pre- and post-processing operators that aim to reduce biases in ML models.\nAdditionally, mlr3fairness comes with reporting functionality that assists the user in documenting data and ML models, as well as in performing fairness audits.\nIn the remainder of the article, we first provide an introduction to fairness in ML to raise awareness of biases that can arise due to the use of ML models.\nNext, we introduce the mlr3fairness package, followed by an extensive case study, showcasing the capabilities of mlr3fairness.\nWe conclude with a summary.\n2 Fairness in Machine Learning\nStudies have found that data-driven automated decision-making systems often improve over human expertise (Dawes et al. (1989)) and high-stakes decisions can therefore be enhanced using data-driven systems.\nThis often does not only improve predictions, but can also make decisions more efficient through automation.\nSuch systems, often without human oversight, are now ubiquitous in everyday life (O’neil 2016; Eubanks 2018; Noble 2018).\nTo provide further examples, ML-driven systems are used for highly influential decisions such as loan accommodations (Chen 2018; Turner and McBurnett 2019), job applications (Schumann et al. 2020), healthcare (Topol 2019), and criminal sentencing (Angwin et al. 2016; Corbett-Davies et al. 2017; Berk et al. 2018).\nWith this proliferation, such decisions have become subject to scrutiny as a result of prominent inadequacies or failures, for example in the case of the COMPAS recidivism prediction system (Angwin et al. 2016).\nWithout proper auditing, those models can unintentionally result in negative consequences for individuals, often from underprivileged groups (Barocas et al. 2019).\nSeveral sources of such biases are worth mentioning in this context:\nData often contains historical biases such as gender or racial stereotypes, that – if picked up by the model – will be replicated into the future.\nSimilarly, unprivileged populations are often not represented in data due to sampling biases leading to models that perform well in groups sufficiently represented in the data but worse on others (Buolamwini and Gebru 2018) – this includes a higher rate of missing data.\nOther biases include biases in how labels and data are measured (Bao et al. 2021) as well as feedback loops where repeated decisions affect the population subject to such decisions.\nFor an in-depth discussion and further sources of biases, the interested reader is referred to available surveys of the field (Barocas et al. 2019; Mehrabi et al. 2021; Mitchell et al. 2021).\nQuantifying fairness\nWe now turn to the question of how we can detect whether disparities exist in a model and if so, how they can be quantified.\nWhat constitutes a fair model depends on a society’s ethical values and which normative position we take, resulting in different metrics that are applied to a problem at hand.\nIn this article, we focus on a subgroup of these, so-called statistical group fairness metrics.\nFirst, the observations are grouped by a sensitive attribute \\(A\\) (\\(A = 0\\) vs. \\(A = 1\\)), which, e.g., is an identifier for a person’s race or a person’s gender.\nFor the sake of simplicity, we consider a binary classification scenario and a binary sensitive attribute.\nEach observation has an associated label \\(Y, Y \\in \\{0, 1\\}\\), and we aim to predict, e.g., whether a defendant was caught re-offending.\nA system then makes a prediction \\(\\hat{Y}, \\hat{Y} \\in \\{0,1\\}\\), with the goal to predict whether an individual might re-offend.\nWe assume that \\(Y = 1\\) is the favored outcome in the following exposition.\nWhile we do not describe them in detail, the concepts discussed in the following often extend naturally to more complex scenarios including multi-class classification, regression or survival analysis.\nSimilarly, metrics can be extended to settings that require consideration of multiple possibly intersecting sensitive attributes.\nWe now provide and discuss groups of metrics that require either Separation or Independence (Barocas et al. 2019) to provide further intuition regarding core concepts and possible applications.\nSeparation\nOne group of widely used fairness notions requires Separation: \\(\\hat{Y} \\perp A | Y\\).\nIn order for separation to hold, the prediction \\(\\hat{Y}\\) has to be independent of \\(A\\) given the true label \\(Y\\).\nThis essentially requires that some notion of model error, e.g., accuracy or false positive rate, is equal across groups \\(A\\).\nFrom this notion, we can derive several metrics that come with different implications.\nIt is important to note that those metrics can only meaningfully identify biases under the assumption that no disparities exist in the data or that they are legally justified.\nFor example, if societal biases lead to disparate measurements of an observed quantity (e.g. SAT scores) for individuals with the same underlying ability, separation based metrics might not identify existing biases.\nFor this reason, Wachter et al. (2020) refer to those metrics as bias-preserving metrics since underlying disparities are not addressed.\nEqualized Odds\nA predictor \\(\\hat{Y}\\) satisfies equalized odds with respect to a sensitive attribute \\(A\\) and observed outcome \\(Y\\), if \\(\\hat{Y}\\) and \\(A\\) are conditionally independent given \\(Y\\):\n\\[\\begin{equation}\n\\mathbb{P}\\left(\\hat{Y} = 1 \\mid A = 0, Y = y\\right) = \\mathbb{P}\\left(\\hat{Y} = 1 \\mid A = 1, Y = y\\right), \\quad y \\in \\{0,1\\}.\n\\tag{1}\n\\end{equation}\\]\nIn short, we require that the true positive rates (TPR) and false positive rates (FPR) across both groups \\(A = 0\\) and \\(A = 1\\) are equal.\nThis intuitively requires, e.g., in the case of university admission, independent of the sensitive attribute, equal chances for qualified individuals to be accepted and unqualified individuals to be rejected.\nSimilar measures have been proposed based on equalized false positive rates (Chouldechova 2017) and false omission rates (Berk et al. 2018), depending on the scenario and societal context.\nEquality of Opportunity\nA predictor \\(\\hat{Y}\\) satisfies equality of opportunity with respect to a sensitive attribute \\(A\\) and observed outcome \\(Y\\), if \\(\\hat{Y}\\) and \\(A\\) are conditionally independent for \\(Y = 1\\).\nThis is a relaxation of the aforementioned equalized odds essentially only requiring equal TPRs:\n\\[\\begin{equation}\n\\mathbb{P}\\left(\\hat{Y} = 1 \\mid A = 0, Y = 1\\right) = \\mathbb{P}\\left(\\hat{Y} = 1 \\mid A = 1, Y = 1\\right).\n\\tag{2}\n\\end{equation}\\]\nIntuitively, this only requires that, independent of the sensitive attribute, qualified individuals have the same chance of being accepted.\nPerformance Parity\nA more general formulation can be applied when we require parity of some performance metric across groups.\nTo provide an example, Buolamwini and Gebru (2018) compare accuracy across intersectional subgroups, essentially arguing that model performance should be equal across groups:\n\\[\\begin{equation}\n\\mathbb{P}\\left(\\hat{Y} = Y \\mid A = 0\\right) = \\mathbb{P}\\left(\\hat{Y} = Y \\mid A = 1\\right).\n\\tag{3}\n\\end{equation}\\]\nThis intuitively requires that the model should work equally well for all groups, i.e., individuals are correctly accepted or denied at the same rate, independent of the predicted attribute.\nThis notion can be extended across supervised learning settings and performance metrics, leading to considerations of equal mean squared error, e.g., in a regression setting.\nIndependence\nThe second group of fairness metrics is given by so-called bias-transforming metrics (Wachter et al. 2020).\nThey require that decision rates, such as the positive rate, are equal across groups.\nThis notion can identify biases, e.g., those which arise from societal biases, that manifest in different base rates across groups.\nAt the same time, employing such notions poses a considerable risk, as blindly optimizing for demographic parity might result in predictors that, for example jail innocent people from an advantaged group in order to achieve parity across both groups (Dwork et al. 2012; Berk et al. 2018).\nA predictor \\(\\hat{Y}\\) satisfies demographic parity (Calders and Verwer 2010) with respect to a sensitive attribute \\(A\\) and observed outcome \\(Y\\), if \\(\\hat{Y}\\) and \\(A\\) are conditionally independent:\n\\[\\begin{equation}\n\\mathbb{P}\\left(\\hat{Y} = 1 \\mid A = 0\\right) = \\mathbb{P}\\left(\\hat{Y} = 1 \\mid A = 1\\right).\n\\tag{4}\n\\end{equation}\\]\nIn contrast to the previous definitions, this requires that the chance of being accepted is equal across groups.\nFairness metrics\nIn order to encode the requirements in equations (1) - (4) into a fairness metric, we encode differences between measured quantities in two groups.\nFor a performance metric \\(M\\), e.g., the true positive rate (TPR), we calculate the difference in the metric across the two groups:\n\\[\n\\Delta_{\\mathrm{M}} = \\mathrm{M}_{A=0} - \\mathrm{M}_{A=1}.\n\\]\nWhen \\(\\Delta_{\\mathrm{M}}\\) significantly deviates from \\(0\\), this indicates a fairness violation with respect to the fairness notion described in \\(M\\).\nTo provide an example, with \\(\\mathbb{P}\\left(\\hat{Y} = 1 \\mid A = \\star, Y = 1\\right)\\) denoted with \\(\\mathrm{TPR}_{A=\\star}\\), we calculate the difference in TPR between the two groups:\n\\[\n\\Delta_{\\mathrm{TPR}} = \\mathrm{TPR}_{A=0} - \\mathrm{TPR}_{A=1}.\n\\]\nWhen \\(\\Delta_{\\mathrm{TPR}}\\) now significantly deviates from \\(0\\), the prediction \\(\\hat{Y}\\) violates the requirement for equality of opportunity formulated above.\nIt is important to note that in practice, we might not be able to perfectly satisfy a given metric, e.g., due to stochasticity in data and labels.\nInstead, to provide a binary conclusion regarding fairness, a model could be considered fair if \\(|\\Delta_{\\mathrm{TPR}}| < \\epsilon\\) for a given threshold \\(\\epsilon > 0\\), e.g., \\(\\epsilon = 0.05\\).\nThis allows for small deviations from perfect fairness due to variance in the estimation of \\(\\mathrm{TPR}_{A=\\star}\\) or additional sources of bias.\nHowever, choosing appropriate thresholds is difficult, and widely used values for \\(\\epsilon\\) such as \\(0.05\\) are arbitrary and do not translate to legal doctrines, e.g., disparate impact (Watkins et al. 2022).\nA more in-depth treatment of metrics is given by (Saleiro et al. 2018; Barocas et al. 2019; Kim et al. 2020; Wachter et al. 2020; Mehrabi et al. 2021).\nSelecting fairness metrics\nWhile the aforementioned metrics are conceptually similar, they encode different beliefs of what constitutes fair in a given scenario.\nWachter et al. (2020) differentiate between bias-preserving and bias-transforming metrics:\nBias-preserving metrics such as equalized odds and equality of opportunity require that errors made by a model are equal across groups.\nThis can help to detect biases stemming, from imbalances in the sampling or under- and overfitting in ML models, but might be problematic in cases where labels are biased.\nTo provide an example, police enforcement and subsequent arrests of violent re-offenders might be different across ZIP code areas, a proxy for race.\nThis might lead to situations where observed labels \\(Y\\) suffer from differential measurement bias strongly correlated with race (Bao et al. 2021).\nBias-preserving metrics do not take such disparities into account and might, therefore (wrongly) lead to the conclusion that a given model is fair.\nBias-transforming methods, in contrast, do not depend on labels and might therefore not suffer from this problem.\nThey can help detect biases arising from different base rates across populations, arising, e.g., from aforementioned biases in the labelling or as a consequence of structural discrimination.\nDeciding which metrics to use constitutes a value judgement and requires careful assessment of the societal context a decision-making system is deployed in.\nA discussion of different metrics and their applicability can be found in the Aequitas Fairness Toolkit (Saleiro et al. 2018) which also provides guidance towards selecting a metric via the Aequitas Fairness Tree.\nWachter et al. (2020) recommend using bias-transforming metrics and provide a checklist that can guide the choice of fairness metric.\nCorbett-Davies and Goel (2018), on the other hand, point out several limitations of available metrics and argue for grounding decisions in real-world quantities in addition to abstract fairness metrics.\nSimilarly, Friedler et al. (2016) emphasize the need to differentiate between constructs we aim to measure (e.g., job-related knowledge) and the observed quantity that can be measured in practice (e.g., years in a job) when trying to automate decisions, since disparities in how constructs translate to observed quantities might suffer from bias.\nTo provide an example, individuals with similar abilities might exhibit different measured quantities (grades) due to structural bias, e.g., differential access to after-school tutoring programs.\nThe dangers of fairness metrics\nWe want to stress that overly trusting in metrics can be dangerous and that fairness metrics cannot and should not be used to prove or guarantee fairness.\nWhether a selected fairness notion (and a corresponding numerical value) is actually fair depends on the societal context in which a decision is made and which action should be derived from a given prediction.\nTherefore, selecting the correct fairness metric requires a thorough understanding of the societal context of a decision, as well as the possible implications of such decisions.\nTo provide an example, in some cases discrepancies in positive predictions might be justified or even desired, as they, for example, allow for a more nuanced, gender-specific diagnosis (Cirillo et al. 2020).\nFurthermore, fairness metrics might not detect biases in more fine-grained subgroups, e.g., at the intersection of multiple sensitive attributes.\nIt is also important to note that fairness metrics merely provide a reduction of the aforementioned fairness notions into mathematical objectives.\nAs such, they require a variety of abstraction steps that might invalidate the metric (Watkins et al. 2022), as they, for example, require that the data is a large enough and representative sample of exactly the population that we aim to investigate.\nFurthermore, practitioners need to look beyond the model, and also at the data used for training and the process of data and label acquisition.\nIf the data for example exhibit disparate measurement errors in the features or labels, valid fairness assessments can become impossible.\nSimilarly, feedback loops might arise from a prediction leading to changes in the data collected in the future.\nEven an initially fair model might then lead to adverse effects in the long term (Schwöbel and Remmers 2022).\nNote that the fairness definitions presented above serve a dual purpose (Wachter et al. 2020):\nFirst, as a diagnostic tool to detect disparities.\nThis allows for assessing whether a model has inherited biases, e.g., from historical disparities reflected in the data.\nThe second purpose is as a basis for model selection and making fair decisions in practice.\nIn this setting, fairness notions are employed to audit ML models or to select which model should be used in practice.\nIn this setting, it is important to note that fairness metrics should not be used as the sole basis for making decisions about whether to employ a given ML model or to assess whether a given system is fair.\nWe therefore explicitly encourage using the presented metrics for exploratory purposes.\nOther notions of fairness\nIn addition to statistical group fairness notions introduced above, several additional fairness notions exist.\nThe notion of individual fairness was proposed by Dwork et al. (2012).\nIts core idea comes from the principle of treating similar cases similarly and different cases differently.\nIn contrast to statistical group fairness notions, this notion allows assessing fairness at an individual level and\nwould therefore allow determining whether an individual is treated fairly.\nA more in-depth treatment of individual fairness notions is, given in Binns (2020).\nSimilarly, a variety of causal fairness notions exist (c.f. Kilbertus et al. (2017)).\nThey argue that assessing fairness requires incorporating causal relationships in the data and propose a variety of causal fairness metrics based on a directed acyclic graph describing relationships in the data.\nFairness constraints\nStatistical group fairness notions suffer from two further problems in practice:\nFirst, it might be hard to exactly satisfy the required fairness notions, e.g., due to a limited amount of data available for evaluation.\nSecondly, only requiring fairness might lead to degenerate solutions (Corbett-Davies and Goel 2018) or models that have low utility, e.g., in separating good and bad credit risk.\nOne approach to take this into account is to employ models which maximize utility but satisfy some maximum constraint on potential unfairness.\nThis can be achieved via constraints on the employed fairness measure, e.g. \\(|\\Delta_M| \\leq \\epsilon\\) requiring that the absolute difference in a metric \\(M\\) between groups\nis smaller than a chosen value \\(\\epsilon\\).\nIn the following, we denote the fairness metric we want to minimize with \\(\\Delta_M\\) and a performance metric with \\(\\rho\\).\n\\[\n  \\rho_{|\\Delta_M| \\leq \\epsilon} = \\left\\{\n\\begin{array}{ll}\n\\rho         & |\\Delta_M| \\leq \\epsilon      \\\\\n- |\\Delta_M| &  \\textrm{else.}                \\\\\n\\end{array}\n\\right.\n\\]\nNote that this assumes that the fairness metric \\(\\rho\\) is strictly positive and should be maximized.\nThis approach is similar in spirit to the approach of Perrone et al., (2021) who optimize the constrained expected improvement \\(cEI = \\mathbb{P}(|\\Delta_M| \\leq \\epsilon) \\cdot \\rho\\).\nHowever, it is not immediately clear how the constraint \\(\\epsilon\\) should be chosen.\nAn alternative, therefore, is to employ multi-objective optimization to investigate available trade-offs between performance and accuracy metrics.\nThis can be done via mlr3tuning (Becker et al. 2023a) which contains functionality to tune models for multiple metrics, described in more detail in the mlr3book (Bernd Bischl 2024).\nThe result of multi-objective optimization then is the Pareto-set: A list of models which optimally trade off the specified objectives.\nBias mitigation\nIf biases are detected in a model, we might now be interested in improving models in order to potentially mitigate such biases.\nBias in models might arise from a variety of sources, so a careful understanding of the data, data quality and distribution might lead to approaches that can help in decreasing biases, e.g. through the collection of better or additional data or a better balancing of sensitive groups.\nSimilarly, biases might arise from the model, through under- or overfitting and more careful tuning of model hyperparameters might help with improving fairness.\nEspecially if the goal is to satisfy bias-transforming metrics, a better solution might often be to address fairness problems in the real world instead of relying on algorithmic interventions to solve fairness. This might lead to more robust, long-term solutions instead of temporarily addressing issues via algorithmic interventions.\nIn addition, a variety of algorithmic bias mitigation techniques, that might help with obtaining fairer models have been proposed.\nTheir goal is to reduce measured gaps in fairness, either via data pre-processing, employing models that incorporate fairness, or by applying post-processing techniques to a model’s predictions.\nPopular examples of such techniques include computing instance weights before training (Kamiran and Calders 2012), where each observation is weighted proportional to the inverse frequency of its label and sensitive attribute.\nOther methods work by directly learning fair models that incorporate fairness constraints into the fitting procedure (Zafar et al. 2017) or by adapting model predictions, e.g., Hardt, Price, and Srebro (2016) propose to randomly flip a small fraction of predictions in each group given by \\(\\hat{Y}\\) and \\(A\\), such that fairness metrics are satisfied in expectation.\nSince bias mitigation techniques are often tailored towards a particular fairness metric, the optimal choice is often not trivial and a combination of algorithms and bias mitigation techniques determined via tuning might result in an optimal model.\nBias-mitigation techniques, as proposed above, have the goal of mitigating fairness issues, as measured by fairness metrics.\nIn practice, this usually comes with several drawbacks:\nFirst, bias-mitigation strategies often lead to a decrease in a classifier’s predictive performance (Corbett-Davies and Goel 2018).\nIn addition, processing schemes can worsen interpretability or introduce stochasticity during prediction (see, e.g., Hardt et al. (2016)).\nFurthermore, we want to caution against favouring bias-mitigation techniques over policy interventions that tackle biases at their root cause.\nA different set of risks is posed by fairwashing (Aivodji et al. 2019), i.e., finding fair explanations or satisfying fairness metrics for otherwise unfair models.\nIf biases are only addressed at a given moment and without regard for downstream effects, they might simultaneously lead to a decrease in predictive performance in the near term and to negative consequences for the sensitive group in the long term (Schwöbel and Remmers 2022).\n3 The mlr3fairness package\nIn this section, we first give an overview of related software.\nNext, we give a very brief introduction to the mlr3 ecosystem of packages.\nFinally, the implemented extensions for fairness are presented.\nRelated software\nSeveral R packages provide similar capabilities to our software, but mostly focus on fairness metrics and visualization.\nThe fairness package (Kozodoi and V. Varga 2021) allows for the calculation of a variety of fairness metrics, while aif360 (Bellamy et al. 2019) wraps the Python aif360 module allowing for the computation of fairness metrics and several bias mitigation techniques, but has only limited interoperability with R objects such as s.\nThe fairmodels (Wiśniewski and Biecek 2022) package again allows for the computation of fairness metrics for classification and regression settings as well as several bias mitigation techniques.\nIt tightly integrates with DALEX (Biecek 2018) to gain further insight using interpretability techniques.\nOutside R, in Python, the fairlearn module (Bird et al. 2020) provides ample functionality to study a wide variety of metrics, bias mitigation with respect to a variety of pre-, in- and post-processing methods as well as to visualize differences.\nIt furthermore provides a fairlearn dashboard providing a comprehensive fairness report.\nThe aif360 (Bellamy et al. 2019) module similarly provides metrics as well as bias mitigation techniques while the aequitas fairness toolkit (Saleiro et al. 2018) provides similar capabilities.\nInteroperability with the scikit-learn (Pedregosa et al. 2011) ML framework allows for bias mitigation for a wide variety of ML models in all aforementioned systems.\nSimilar capabilities are also available in Julia’s Fairness.jl (Agrawal et al. 2020a) library.\nThe mlr3 ecosystem\nmlr3fairness is tightly integrated into the ecosystem of packages around the ML framework mlr3 (Lang et al. 2019).\nmlr3 provides the infrastructure to fit, resample, and evaluate over 100 ML algorithms using a unified API.\nPackages from the ecosystem can be installed and updated via the mlr3verse (Lang and Schratz 2023) package.\nMultiple extension packages bring numerous additional advantages and extra functionality.\nIn the context of fairness, the following extension packages deserve special mention:\nmlr3pipelines (Binder et al. 2021) for pre- and postprocessing via pipelining.\nThis allows composing bias mitigation techniques with arbitrary ML algorithms shipped with mlr3 as well as fusing ML algorithms with pre-processing steps such as imputation or class balancing.\nIt furthermore integrates with mcboost (Pfisterer et al. 2021), which implements additional bias mitigation methods.\nWe present an example in the supplementary material.\nmlr3tuning and bbotk (Becker et al. 2023b) for its extensive tuning capabilities.\nmlr3proba (Sonabend et al. 2021) for survival analysis.\nmlr3benchmark for post-hoc analysis of benchmarked approaches.\nmlr3oml as a connector to OpenML (Vanschoren et al. 2014), an online scientific platform for collaborative ML.\nIn order to provide the required understanding for mlr3, we briefly introduce some terminology and syntax.\nA full introduction can be found in the mlr3 book (Bernd Bischl 2024).\nA Task in mlr3 is a basic building block holding the data, storing covariates and the target variable along with some meta-information.\nThe shorthand constructor function tsk() can be used to quickly access example tasks shipped with mlr3 or mlr3fairness.\nIn the following chunk, we retrieve the binary classification task with id \"adult_train\" from the package.\nIt contains a part of the Adult data set (Dua and Graff 2017).\nThe task is to predict whether an individual earns more than $50.000 per year.\nThe column \"sex\" is set as a binary sensitive attribute with levels \"Female\" and \"Male\".\n\n\nlibrary(\"mlr3verse\")\nlibrary(\"mlr3fairness\")\n\ntask = tsk(\"adult_train\")\nprint(task)\n\n<TaskClassif:adult_train> (30718 x 13)\n* Target: target\n* Properties: twoclass\n* Features (12):\n  - fct (7): education, marital_status, occupation, race,\n    relationship, sex, workclass\n  - int (5): age, capital_gain, capital_loss, education_num,\n    hours_per_week\n* Protected attribute: sex\n\nThe second building block is the Learner.\nIt is a wrapper around an ML algorithm, e.g., an implementation of logistic regression or a decision tree.\nIt can be trained on a Task and used for obtaining a Prediction on an independent test set which can subsequently be scored using a Measure to get an estimate for the predictive performance on new data.\nThe shorthand constructors lrn() and msr() allow for the instantiation of implemented Learners and Measures, respectively.\nIn the following example, we will first instantiate a learner, then split our data into a train and test set, afterwards train it on the train set of the dataset and finally evaluate predictions on held-out test data.\nThe train-test split in this case is given by row indices, here stored in the idx variable.\n\n\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nidx = partition(task)\nlearner$train(task, idx$train)\nprediction = learner$predict(task, idx$test)\n\n\nWe then employ the classif.acc measure which measures the accuracy of a prediction compared to the true label:\n\n\nmeasure = msr(\"classif.acc\")\nprediction$score(measure)\n\nclassif.acc \n     0.8382 \n\nIn the example above, we obtain an accuracy score of 0.8382, meaning our ML model correctly classifies roughly 84 % of the samples in the test data.\nAs the split into training set and test set is stochastic, the procedure should be repeated multiple times for smaller datasets (Bischl et al. 2012) and the resulting performance values should be aggregated.\nThis process is called resampling, and can easily be performed with the resample() function, yielding a ResampleResult object.\nIn the following, we employ 10-fold cross-validation as a resampling strategy:\n\n\nresampling = rsmp(\"cv\", folds = 10)\nrr = resample(task, learner, resampling)\n\n\nWe can call the aggregate method on the ResampleResult to obtain the accuracy aggregated across all \\(10\\) replications.\n\n\nrr$aggregate(measure)\n\nclassif.acc \n     0.8408 \n\nHere, we obtain an accuracy of 0.8408, so slightly higher than previous scores, due to using a larger fraction of the data.\nFurthermore, this estimate has a lower variance (as it is an aggregate) at the cost of additional computation time.\nTo properly compare competing modelling approaches, candidates can be benchmarked against each other using the benchmark() function (yielding a BenchmarkResult).\nIn the following, we compare the decision tree from above to a logistic regression model.\nTo do this, we use the benchmark_grid function to compare the two Learners across the same Task and resampling procedure.\nFinally, we aggregate the measured scores each learner obtains on each cross-validation split using the $aggregate() function.\n\n\nlearner2 = lrn(\"classif.log_reg\", predict_type = \"prob\")\n\ngrid = benchmark_grid(task, list(learner, learner2), resampling)\nbmr = benchmark(grid)\n\nbmr$aggregate(measure)[, .(learner_id, classif.acc)]\n\n        learner_id classif.acc\n1:   classif.rpart      0.8408\n2: classif.log_reg      0.8467\n\nAfter running the benchmark, we can again call .$aggregate to obtain aggregated scores.\nThe mlr3viz package comes with several ready-made visualizations for objects from mlr3 via ggplot2’s (Wickham 2016) autoplot function.\nFor a BenchmarkResult, the autoplot function provides a Box-plot comparison of performances across the cross-validation folds for each Learner.\nFigure 1 contains the box-plot comparison.\nWe can see that log_reg has higher accuracy and lower interquartile range across the 10 folds, and we might therefore want to prefer the log_reg model.\n\n\n\nFigure 1: Model comparison based on accuracy for decision trees (rpart) and logistic regression (log_reg) across resampling splits.\n\n\n\nSelecting the sensitive attribute\nFor a given task, we can select one or multiple sensitive attributes.\nIn mlr3, the sensitive attribute is identified by the column role pta and can be set as follows:\n\n\ntask$set_col_roles(\"marital_status\", add_to = \"pta\")\n\n\nIn the example above, we add the \"marital_status\" as an additional sensitive attribute.\nThis information is then automatically passed on when the task is used, e.g., when computing fairness metrics.\nIf more than one sensitive attribute is specified, metrics will be computed based on intersecting groups formed by the columns.\nQuantifying fairness\nWith the mlr3fairness package loaded, fairness measures can be constructed via msr() like any other measure in mlr3.\nThey are listed with prefix fairness, and simply calling msr() without any arguments will return a list of all available measures.\nTable 1 provides an overview of some popular fairness measures which are readily available.\nTable 1:  Overview of fairness metrics available with mlr3fairness.\nkey\ndescription\nfairness.acc\nAccuracy equality \nfairness.mse\nMean squared error equality (Regression)\nfairness.eod\nEqualized odds \nfairness.tpr\nTrue positive rate equality / Equality of opportunity \nfairness.fpr\nFalse positive rate equality / Predictive equality \nfairness.tnr\nTrue negative rate equality\nfairness.fnr\nFalse negative rate equality \nfairness.fomr\nFalse omission rate equality \nfairness.tnr\nNegative predictive value equality\nfairness.tnr\nPositive predictive value equality\nfairness.cv\nDemographic parity / Equalized positive rates \nfairness.pp\nPredictive parity / Equalized precision \nfairness.{tp, fp, tn, fn}\nEqual true positives, false positives, true negatives, false negatives\nfairness.acc_eod=.05\nAccuracy under equalized odds constraint \nfairness.acc_ppv=.05\nAccuracy under ppv constraint \nFurthermore, new custom fairness measures can be easily implemented, either by implementing them directly or by composing them from existing metrics.\nThis process is extensively documented in an accompanying measures vignette available with the package.\nHere we choose the binary accuracy measure \"classif.acc\" and the equalized odds metric from above using \"fairness.eod\":\nThe constructed list of measures can then be used to score a Prediction, a ResampleResult or BenchmarkResult, e.g.\n\n\nmeasures = list(msr(\"classif.acc\"), msr(\"fairness.eod\"))\nrr$aggregate(measures)\n\n            classif.acc fairness.equalized_odds \n                0.84078                 0.07939 \n\n\n\n\nWe can clearly see a comparatively large difference in equalized odds at around 0.08.\nThis means, that in total, the false positive rates (FPR) and true positive rates (TPR) on average differ by ~0.08, indicating that our model might exhibit a bias.\nLooking at the individual components yields a clearer picture.\nHere, we are looking at the confusion matrices of the combined predictions of the 10 folds, grouped by sensitive attribute:\n\n\nfairness_tensor(rr)\n\n$Male\n        truth\nresponse   <=50K    >50K\n   <=50K 0.43030 0.10033\n   >50K  0.03408 0.11202\n\n$Female\n        truth\nresponse    <=50K     >50K\n   <=50K 0.282668 0.020900\n   >50K  0.003907 0.015789\n\nPlotting the prediction density or comparing measures graphically often provides additional insights:\nFor example, in Figure 2, we can see that Females are more often predicted to earn below $50.000.\nSimilarly, we can see that both equality in FPR and TPR differ considerably.\n\n\nfairness_prediction_density(prediction, task)\ncompare_metrics(prediction, msrs(c(\"fairness.fpr\", \"fairness.tpr\", \"fairness.eod\")), task)\n\n\n\n\n\nFigure 2: Visualizing predictions of the decision tree model. Left: Prediction densities for the negative class for Female and Male. Right: Fairness metrics comparison for FPR, TPR, EOd metrics. Plots show a higher likelihood for the ‘<50k’ class for females resulting in fairness metrics different from 0.\n\n\n\nBias mitigation\nAs mentioned above, several ways to improve a model’s fairness exist.\nWhile non-technical interventions, such as e.g. collecting more data should be preferred,\nmlr3fairness provides several bias mitigation techniques that can be used together with a Learner to obtain fairer models.\nTable 2 provides an overview of implemented bias mitigation techniques.\nThey are implemented as PipeOps from the mlr3pipelines package and can be\ncombined with arbitrary learners using the %>>% operator to build a pipeline that can later be trained.\nIn the following example, we show how to combine a learner with a reweighing scheme (reweighing_wts) or alternatively how to post-process predictions using the equalized odds debiasing (EOd) strategy.\nAn introduction to mlr3pipelines is available in the corresponding mlr3book chapter (Bernd Bischl 2024).\n\n\npo(\"reweighing_wts\") %>>% lrn(\"classif.glmnet\")\npo(\"learner_cv\", lrn(\"classif.glmnet\")) %>>% po(\"EOd\")\n\n\nTable 2:  Overview of bias mitigation techniques available in mlr3fairness.\nKey\nDescription\nType\nReference\nEOd\nEqualized-Odds Debiasing\nPostprocessing\nHardt et al. (2016)\nreweighing_os\nReweighing (Oversampling)\nPreprocessing\nKamiran and Calders (2012)\nreweighing_wts\nReweighing (Instance weights)\nPreprocessing\nKamiran and Calders (2012)\nIt is simple for users or package developers to extend mlr3fairness with additional\nbias mitigation methods – as an example, the mcboost package adds further post-processing methods\nthat can improve fairness.\nAlong with pipeline operators, mlr3fairness contains several machine learning algorithms listed in table 3 that can directly incorporate\nfairness constraints. They can similarly be constructed using the lrn() shorthand.\nTable 3:  Overview of fair ML algorithms available with mlr3fairness.\nKey\nPackage\nReference\nregr.fairfrrm\nfairml\nScutari et al. (2021)\nclassif.fairfgrrm\nfairml\nScutari et al. (2021)\nregr.fairzlm\nfairml\nZafar et al. (2017)\nclassif.fairzlrm\nfairml\nZafar et al. (2017)\nregr.fairnclm\nfairml\nKomiyama et al. (2018)\nReports\nBecause fairness aspects can not always be investigated based on the fairness definitions above (e.g., due to biased sampling or labelling procedures), it is important to document data collection and the resulting data as well as the models resulting from this data.\nInforming auditors about those aspects of a deployed model can lead to better assessments of a model’s fairness.\nQuestionnaires for ML models (Mitchell et al. 2019) and data sets (Gebru et al. 2021) have been proposed in literature.\nWe further add automated report templates using R markdown (Xie et al. 2020) for data sets and ML models.\nIn addition, we provide a template for a fairness report which includes many fairness metrics and visualizations to provide a good starting point for generating a fairness report inspired by the Aequitas Toolkit (Saleiro et al. 2018).\nA preview for the different reports can be obtained from the Reports vignette in the package documentation.\nTable 4:  Overview of reports generated by mlr3fairness.\nReport\nDescription\nReference\nreport_modelcard()\nModelcard for ML models\nMitchell et al. (2019)\nreport_datasheet()\nDatasheet for data sets\nGebru et al. (2021)\nreport_fairness()\nFairness Report\n–\n4 Case study\nIn order to demonstrate a full workflow, we conduct full bias assessment and bias mitigation on the popular adult data set (Dua and Graff 2017).\nThe goal is to predict whether an individual’s income is larger than $\\(50.000\\) with the sensitive attribute being gender.\nThe data set is included with mlr3fairness, separated into a train and test task and can be instantiated using tsk(\"adult_train\") and tsk(\"adult_test\"), respectively.\nAs a fairness metric, we consider true positive parity which calls for equality in the true positive rates across groups, in this case the sex variable.\nWe furthermore are interested in the model’s utility, here measured as its classification accuracy.\n\n\nlibrary(\"mlr3verse\")\nlibrary(\"mlr3fairness\")\n\ntask = tsk(\"adult_train\")\nprint(task)\n\n<TaskClassif:adult_train> (30718 x 13)\n* Target: target\n* Properties: twoclass\n* Features (12):\n  - fct (7): education, marital_status, occupation, race,\n    relationship, sex, workclass\n  - int (5): age, capital_gain, capital_loss, education_num,\n    hours_per_week\n* Protected attribute: sex\n\nmeasures = msrs(c(\"fairness.tpr\", \"classif.acc\"))\n\n\nIn order to get an initial perspective, we benchmark three models using 3-fold cross-validation each:\na classification tree from the rpart package,\na penalized logistic regression from the glmnet package and\na penalized logistic regression from the glmnet package, but with reweighing pre-\nprocessing.\nThe logistic regression in the latter two approaches does not support operating on factor features natively, therefore we pre-process the data with a feature encoder from mlr3pipelines.\nTo achieve this, we connect the feature encoder po(\"encode\") with the learner using the %>>% operator.\nThis encodes factor variables into integers using dummy encoding.\nWe then evaluate all three learners on the adult_train data using 3-fold cross-validation by building up a grid of experiments we want to run using benchmark_grid.\nThis grid is then executed using the benchmark function, and we can aggregate the performance and fairness metric scores via the $aggregate() function.\n\n\nset.seed(4321)\nlearners = list(\n    lrn(\"classif.rpart\"),\n    po(\"encode\") %>>% lrn(\"classif.glmnet\"),\n    po(\"encode\") %>>% po(\"reweighing_wts\") %>>% lrn(\"classif.glmnet\")\n)\n\ngrid = benchmark_grid(\n  tasks = tsks(\"adult_train\"),\n  learners = learners,\n  resamplings = rsmp(\"cv\", folds = 3)\n)\n\nbmr1 = benchmark(grid)\nbmr1$aggregate(measures)[, c(4, 7, 8)]\n\n                             learner_id fairness.tpr classif.acc\n1:                        classif.rpart     0.059767      0.8408\n2:                encode.classif.glmnet     0.070781      0.8411\n3: encode.reweighing_wts.classif.glmnet     0.004732      0.8351\n\nThe pre-processing step of reweighing already improved the fairness while sacrificing only a tiny bit of performance.\nTo see if we can further improve, we use mlr3tuning to jointly tune all hyperparameters of the glmnet model as well as our reweighing hyperparameter.\nIn order to do this, we use an AutoTuner from mlr3tuning; a model that tunes its own hyperparameters during training.\nThe full code for setting up this model can be found in the appendix.\nAn AutoTuner requires a specific metric to tune for.\nHere, we define a fairness-thresholded accuracy metric. We set \\(\\epsilon = 0.01\\) as a threshold:\n\\[\n  if \\; |\\Delta_{EOd}| \\leq \\epsilon: \\textrm{accuracy} \\;\\; else: \\;  - |\\Delta_{EOd}|.\n\\]\n\n\nmetric = msr(\"fairness.constraint\",\n    performance_measure = msr(\"classif.acc\"),\n    fairness_measure = msr(\"fairness.eod\"),\n    epsilon = 0.01\n)\n\n\n\n\n\nWe then design the pipeline and the hyperparameters we want to tune over.\nIn the following example, we choose tuning_iters = 3 and set a small range for the hyperparameters in vals to shorten the run time of the tuning procedure.\nIn real settings, this parameter would be set to a larger number, such as \\(100\\).\nTo construct a self-tuning learner, we construct an AutoTuner that takes as input a learner, the resampling procedure and metric used for tuning as well as the tuning strategy along with a termination criterion (here how many tuning iterations should be run).\nIn addition, we provide a new id for the learner to beautify subsequent printing and visualization.\nWe can then use this self-tuning learner like any other learner and benchmark it using benchmark as described above.\n\n\ntuning_iters = 3\nat = AutoTuner$new(lrn, rsmp(\"holdout\"),\n    metric,\n    tuner = mlr3tuning::tnr(\"random_search\"),\n    terminator = trm(\"evals\", n_evals = tuning_iters)\n)\nat$id = \"glmnet_weighted_tuned\"\n\ngrd = benchmark_grid(\n  tasks = tsks(\"adult_train\"),\n  learners = list(at),\n  resamplings = rsmp(\"cv\", folds = 3)\n)\n\nbmr2 = benchmark(grd, store_models = TRUE)\nbmr2$aggregate(measures)[, c(4, 7, 8)]\n\n              learner_id fairness.tpr classif.acc\n1: glmnet_weighted_tuned     0.009486      0.8385\n\nThe result improves w.r.t. accuracy while only slightly decreasing the measured fairness.\nNote that the generalization error is estimated using a holdout strategy during training and slight violations of the desired threshold \\(\\epsilon\\) should therefore be considered (Feurer et al. 2023).\nThe results of both benchmark experiments can then be collected and jointly visualized in Figure 3 visualizing accuracy and fairness of models in our benchmark.\nIn addition to aggregate scores (denoted by a cross) individual iterations of the 3-fold Cross-Validation (represented by points) are shown to visualize variations in the individual results.\n\n\n\nFigure 3: Fairness-Accuracy tradeoff for 3-fold CV on the adult train set.\n\n\n\n\n\nbmr$aggregate(measures)[, c(4, 7, 8)]\n\n                             learner_id fairness.tpr classif.acc\n1:                        classif.rpart     0.059767      0.8408\n2:                encode.classif.glmnet     0.070781      0.8411\n3: encode.reweighing_wts.classif.glmnet     0.004732      0.8351\n4:                glmnet_weighted_tuned     0.009486      0.8385\n\nEspecially when considering optimizing accuracy while still retaining a fair model, tuning can be helpful and further improve upon available trade-offs.\nIn this example, the AutoTuner improves w.r.t. the fairness metric while offering accuracy comparable with the simple glmnet model.\nThis can be observed from the fairness accuracy tradeoff shown in Figure 3.\nWhether the achieved accuracy is sufficient needs to be determined, e.g. from a business context.\nFor now, we assume that the model obtained from the AutoTuner is the model we might want to use going forward.\nHaving decided on a final model, we can now train the final model on the full training data\n\n\nat_lrn = bmr$learners$learner[[4]]\nat_lrn$train(tsk(\"adult_train\"))\n\n\nand predict on the held out test set available for the Adult dataset to obtain a final estimate.\nThis is important since estimating fairness metrics often incurs significant variance (Agrawal et al. 2020b) and evaluation of the test-set provides us with an unbiased estimate of model performance after the previous model selection step.\n\n\ntest = tsk(\"adult_test\")\nat_lrn$predict(test)$score(measures, test)\n\nfairness.tpr  classif.acc \n     0.07141      0.84375 \n\nOn the held-out test set, the fairness constraint is slightly violated which can happen due to the comparatively large variance in the estimation of fairness metrics.\n5 Summary\nThe large-scale availability and use of automated decision making systems have resulted in growing concerns for a lack of fairness in the decisions made by such systems.\nAs a result, fairness auditing methods, that allow for investigating (un-)fairness in such systems are an important step towards improving the auditability of deployed systems.\nFor ease of use, it is especially important, that they provide interoperability with machine learning toolkits that allows for ease of use and integration into model evaluation and tuning.\nIn future work we plan on implementing several tools that further support the user w.r.t. pinpointing potential fairness issues in the data, especially through the help of interpretability tools, such as the iml (Molnar et al. 2018) package.\nWe furthermore aim to implement additional fairness metrics from the realm of `individual fairness’ (Dwork et al. 2012) and `conditional demographic parity’ (Wachter et al. 2020).\n\n6 Appendix\nTuning the ML pipeline\n\n\n\nWe include the full code to construct the AutoTuner with additional details\nand comments below.\nWe first load all required packages and use mlr3’s interaction with the future (Bengtsson 2021) package\nto automatically distribute the tuning to all available cores in parallel by setting a plan.\nSee the documentation of future for platform-specific hints regarding parallelization.\n\n\nlibrary(mlr3misc)\nlibrary(mlr3)\nlibrary(mlr3pipelines)\nlibrary(mlr3fairness)\nlibrary(mlr3tuning)\n\n# Enable parallelization utilizing all cores\nfuture::plan(\"multicore\")\n\n\n\n\n\nWe then instantiate an ML pipeline using mlr3pipelines.\nThis connects several modelling steps, in our case categorical encoding, reweighing and a final learner using the\n%>>% (double caret) operator, ultimately forming a new learner.\nThis learner can then subsequently be fit on a Task. We use the po(<key>) shorthand to construct a new\npipeline operator from a dictionary of implemented operators.\nWe conduct categorical encoding because glmnet can not naturally handle categorical variables, and we therefore have\nto encode them (in our case using one-hot encoding).\n\n\n# Define the learner pipeline.\nlrn = as_learner(po(\"encode\") %>>% po(\"reweighing_wts\") %>>%\n  po(\"learner\", lrn(\"classif.glmnet\")))\n\n\nIn addition, we have to specify the hyperparameter space our Tuner should tune over.\nWe do this by defining a list of values with a to_tune() token specifying the range.\nNote, that hyperparameter names are prefixed with the respective operation’s id.\n\n\n# Define the parameter space to optimize over\nvals = list(\n  reweighing_wts.alpha = to_tune(0.75, 1),\n  classif.glmnet.alpha = to_tune(0.5, 1),\n  classif.glmnet.s = to_tune(1e-4, 1e-2, logscale = TRUE)\n)\n\n# Add search space to the learner\nlrn$param_set$values = insert_named(lrn$param_set$values, vals)\n\n\nBefore we now train the model, we again specify a metric we aim to satisfy, here we would like the equalized odds difference to be smaller than \\(0.1\\).\nIn this case, we set a constraint on the equalized odds difference comprised of the differences in true positive rate (TPR) and false positive rate (FPR):\n\\[\n\\Delta_{EOd} = \\frac{|\\textrm{TPR}_{sex = M} - \\textrm{TPR}_{sex = F}| + |\\textrm{FPR}_{sex = M} - \\textrm{FPR}_{sex = F}|}{2}.\n\\]\nThis can be done using the fairness.constraint measure.\n\n\nmetric = msr(\"fairness.constraint\",\n    performance_measure = msr(\"classif.acc\"),\n    fairness_measure = msr(\"fairness.eod\"),\n    epsilon = 0.1\n)\n\n\nWe can now instantiate a new AutoTuner using lrn defined above by additionally providing arguments specifying the tuning strategy, in our case random search, the measure to optimize for as well as the number of tuning steps.\n\n\nmetric = msr(\"fairness.constraint\",\n    performance_measure = msr(\"classif.acc\"),\n    fairness_measure = msr(\"fairness.eod\"),\n    epsilon = 0.1\n)\n\nat = AutoTuner$new(\n  learner = lrn, # The learner\n  resampling = rsmp(\"holdout\"), # inner resampling strategy\n  measure = metric, # the metric to optimize for\n  tuner = mlr3tuning::tnr(\"random_search\"), # tuning strategy\n  terminator = trm(\"evals\", n_evals = 30) # number of tuning steps\n)\n\n\nThe so-constructed AutoTuner can now be used on any classification Task!\nAdditional information regarding the AutoTuner is again available in the corresponding mlr3book chapter.\nIn the following example, we will apply it to the Adult task and train our model.\nThis will perform a tuning loop for the specified number of evaluations and\nautomatically retrain the best found parameters on the full data.\n\n\nat$train(tsk(\"adult_train\"))\n\n\nAfter training, we can look at the best models found, here ordered by our metric.\nNote, that our metric reports the negative constraint violation if the constraint is violated and the accuracy in case the constraint is satisfied.\n\n\nhead(at$archive$data[order(fairness.acc_equalized_odds_cstrt), 1:4])\n\n   reweighing_wts.alpha classif.glmnet.alpha classif.glmnet.s\n1:               0.9503               0.5138           -4.654\n2:               0.8160               0.9289           -5.314\n3:               0.7507               0.6784           -5.786\n4:               0.9694               0.9219           -6.198\n5:               0.8254               0.8826           -7.241\n6:               0.8808               0.7362           -7.311\n   fairness.acc_equalized_odds_cstrt\n1:                            0.8411\n2:                            0.8445\n3:                            0.8447\n4:                            0.8448\n5:                            0.8448\n6:                            0.8448\n\nWe can then use the tuned model to assess our metric on the held out data:\n\n\nprd = at$predict(tsk(\"adult_test\"))\nprd$score(c(metric, msr(\"classif.acc\"), msr(\"fairness.eod\")),  tsk(\"adult_test\"))\n\nfairness.acc_equalized_odds_cstrt                       classif.acc \n                          0.83976                           0.83976 \n          fairness.equalized_odds \n                          0.07512 \n\nSo our tuned model manages to obtain an accuracy of ~0.84 while satisfying the specified constraint of \\(\\Delta_{EOd} < 0.1\\).\nSo to summarize, we have tuned a model to optimize accuracy with respect to a constraint on a selected fairness metric using an AutoTuner.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-034.zip\nCRAN packages used\nmlr3fairness, caret, tidymodels, SuperLearner, mlr, mlr3, mlr3tuning, fairness, aif360, fairmodels, DALEX, mlr3verse, mlr3pipelines, mcboost, bbotk, mlr3benchmark, mlr3oml, mlr3viz, ggplot2, rpart, glmnet, iml, future\nCRAN Task Views implied by cited packages\nBayesian, Environmetrics, HighPerformanceComputing, MachineLearning, Phylogenetics, Spatial, Survival, TeachingStatistics\n\n\nA. Agrawal, J. Chen, S. Vollmer and A. Blaom. Fairness.jl. 2020a. URL https://github.com/ashryaagr/Fairness.jl.\n\n\nA. Agrawal, F. Pfisterer, B. Bischl, J. Chen, S. Sood, S. Shah, F. Buet-Golfouse, B. A. Mateen and S. Vollmer. Debiasing classifiers: Is reality at variance with expectation? arXiv:2011.02407, 2020b.\n\n\nU. Aivodji, H. Arai, O. Fortineau, S. Gambs, S. Hara and A. Tapp. Fairwashing: The risk of rationalization. In International conference on machine learning, pages. 161–170 2019.\n\n\nJ. Angwin, J. Larson, S. Mattu and L. Kichner. Machine bias. 2016. URL https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\n\n\nM. Bao, A. Zhou, S. A. Zottola, B. Brubach, S. Desmarais, A. S. Horowitz, K. Lum and S. Venkatasubramanian. It’s COMPASlicated: The messy relationship between RAI datasets and algorithmic fairness benchmarks. In Thirty-fifth conference on neural information processing systems datasets and benchmarks track (round 1), 2021.\n\n\nS. Barocas, M. Hardt and A. Narayanan. Fairness and machine learning. fairmlbook.org, 2019. URL https://fairmlbook.org/.\n\n\nM. Becker, M. Lang, J. Richter, B. Bischl and D. Schalk. mlr3tuning: Hyperparameter optimization for ’mlr3’. 2023a. https://mlr3tuning.mlr-org.com, https://github.com/mlr-org/mlr3tuning.\n\n\nM. Becker, J. Richter, M. Lang, B. Bischl and M. Binder. Bbotk: Black-box optimization toolkit. 2023b. https://bbotk.mlr-org.com, https://github.com/mlr-org/bbotk.\n\n\nR. K. Bellamy, K. Dey, M. Hind, S. C. Hoffman, S. Houde, K. Kannan, P. Lohia, J. Martino, S. Mehta, A. Mojsilović, et al. AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. IBM Journal of Research and Development, 63(4/5): 4–1, 2019. URL https://aif360.mybluemix.net/.\n\n\nH. Bengtsson. A unifying framework for parallel and distributed processing in R using futures. The R Journal, 13(2): 208–227, 2021. URL https://doi.org/10.32614/RJ-2021-048.\n\n\nR. Berk, H. Heidari, S. Jabbari, M. Kearns and A. Roth. Fairness in Criminal Justice Risk Assessments: The State of the Art. Sociological Methods & Research, 2018. DOI 10.1177/0049124118782533.\n\n\nL. K. Bernd Bischl Raphael Sonabend, ed. Applied machine learning using mlr3 in R. CRC Press, 2024. URL https://mlr3book.mlr-org.com.\n\n\nP. Biecek. DALEX: Explainers for Complex Predictive Models in R. Journal of Machine Learning Research, 19(84): 1–5, 2018. URL https://jmlr.org/papers/v19/18-416.html.\n\n\nM. Binder, F. Pfisterer, M. Lang, L. Schneider, L. Kotthoff and B. Bischl. mlr3pipelines - Flexible Machine Learning Pipelines in R. Journal of Machine Learning Research, 22(184): 1–7, 2021. URL https://jmlr.org/papers/v22/21-0281.html.\n\n\nR. Binns. On the apparent conflict between individual and group fairness. In Proceedings of the conference on fairness, accountability, and transparency, pages. 514–524 2020. New York, NY, USA: Association for Computing Machinery. DOI 10.1145/3351095.3372864.\n\n\nS. Bird, M. Dudík, R. Edgar, B. Horn, R. Lutz, V. Milan, M. Sameki, H. Wallach and K. Walker. Fairlearn: A toolkit for assessing and improving fairness in AI. MSR-TR-2020-32. Microsoft. 2020. URL https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/.\n\n\nB. Bischl, M. Lang, L. Kotthoff, J. Schiffner, J. Richter, E. Studerus, G. Casalicchio and Z. M. Jones. mlr: Machine learning in R. Journal of Machine Learning Research, 17(170): 1–5, 2016. URL https://jmlr.org/papers/v17/15-066.html.\n\n\nB. Bischl, O. Mersmann, H. Trautmann and C. Weihs. Resampling methods for meta-model validation with recommendations for evolutionary computation. Evolutionary Computation, 20(2): 249–275, 2012.\n\n\nJ. Buolamwini and T. Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Proceedings of the conference on fairness, accountability, and transparency, pages. 77–91 2018. PMLR.\n\n\nT. Calders and S. Verwer. Three naive Bayes approaches for discrimination-free classification. Data Mining and Knowledge Discovery, 21(2): 277–292, 2010. DOI 10.1007/s10618-010-0190-x.\n\n\nJ. Chen. Fair lending needs explainable models for responsible recommendation. In Proceedings of the 2nd FATREC workshop on responsible recommendation, 2018.\n\n\nA. Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big Data, 5(2): 153–163, 2017. DOI 10.1089/big.2016.0047.\n\n\nD. Cirillo, S. Catuara-Solarz, C. Morey, E. Guney, L. Subirats, S. Mellino, A. Gigante, A. Valencia, M. J. Rementeria, A. S. Chadha, et al. Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare. NPJ Digital Medicine, 3(1): 1–11, 2020. DOI 10.1038/s41746-020-0288-5.\n\n\nS. Corbett-Davies and S. Goel. The measure and mismeasure of fairness: A critical review of fair machine learning. arXiv:1808.00023, 2018.\n\n\nS. Corbett-Davies, E. Pierson, A. Feller, S. Goel and A. Huq. Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pages. 797–806 2017. New York, NY, USA: Association for Computing Machinery. DOI 10.1145/3097983.3098095.\n\n\nR. M. Dawes, D. Faust and P. E. Meehl. Clinical versus actuarial judgment. Science, 243(4899): 1668–1674, 1989. DOI 10.1126/science.2648573.\n\n\nD. Dua and C. Graff. UCI machine learning repository. 2017. URL http://archive.ics.uci.edu/ml.\n\n\nC. Dwork, M. Hardt, T. Pitassi, O. Reingold and R. Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pages. 214–226 2012.\n\n\nV. Eubanks. Automating inequality: How high-tech tools profile, police, and punish the poor. St. Martin’s Press, 2018.\n\n\nM. Feurer, K. Eggensperger, E. Bergman, F. Pfisterer, B. Bischl and F. Hutter. Mind the gap: Measuring generalization performance across multiple objectives. In Advances in intelligent data analysis XXI, pages. 130–142 2023.\n\n\nS. A. Friedler, C. Scheidegger and S. Venkatasubramanian. On the (im)possibility of fairness. arXiv:1609.07236, 2016.\n\n\nJ. Galindo and P. Tamayo. Credit risk assessment using statistical and machine learning: Basic methodology and risk modeling applications. Computational Economics, 15(1): 107–143, 2000.\n\n\nT. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. D. Iii and K. Crawford. Datasheets for datasets. Communications of the ACM, 64(12): 86–92, 2021.\n\n\nM. Hardt, E. Price and N. Srebro. Equality of opportunity in supervised learning. Advances in Neural Information Processing Systems, 29: 3315–3323, 2016.\n\n\nF. Kamiran and T. Calders. Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems, 33(1): 1–33, 2012.\n\n\nN. Kilbertus, M. Rojas Carulla, G. Parascandolo, M. Hardt, D. Janzing and B. Schölkopf. Avoiding discrimination through causal reasoning. Advances in Neural Information Processing Systems, 30: 2017.\n\n\nJ. S. Kim, J. Chen and A. Talwalkar. Fact: A diagnostic for group fairness trade-offs. In International conference on machine learning, pages. 5264–5274 2020. PMLR.\n\n\nJ. Komiyama, A. Takeda, J. Honda and H. Shimao. Nonconvex optimization for regression with fairness constraints. In International conference on machine learning, pages. 2737–2746 2018.\n\n\nN. Kozodoi, J. Jacob and S. Lessmann. Fairness in credit scoring: Assessment, implementation and profit implications. European Journal of Operational Research, 297(3): 1083–1094, 2022. DOI 10.1016/j.ejor.2021.06.023.\n\n\nN. Kozodoi and T. V. Varga. Fairness: Algorithmic fairness metrics. 2021. URL https://CRAN.R-project.org/package=fairness. R package version 1.2.1.\n\n\nM. Kuhn. Caret: Classification and regression training. 2021. URL https://CRAN.R-project.org/package=caret. R package version 6.0-88.\n\n\nM. Kuhn and H. Wickham. Tidymodels: A collection of packages for modeling and machine learning using tidyverse principles. 2020. URL https://www.tidymodels.org.\n\n\nM. Lang, M. Binder, J. Richter, P. Schratz, F. Pfisterer, S. Coors, Q. Au, G. Casalicchio, L. Kotthoff and B. Bischl. mlr3: A modern object-oriented machine learning framework in R. Journal of Open Source Software, 2019. DOI 10.21105/joss.01903.\n\n\nM. Lang and P. Schratz. mlr3verse: Easily install and load the ’mlr3’ package family. 2023. https://mlr3verse.mlr-org.com, https://github.com/mlr-org/mlr3verse.\n\n\nN. Mehrabi, F. Morstatter, N. Saxena, K. Lerman and A. Galstyan. A survey on bias and fairness in machine learning. ACM Computing Surveys (CSUR), 54(6): 1–35, 2021.\n\n\nM. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson, E. Spitzer, I. D. Raji and T. Gebru. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency, pages. 220–229 2019. New York, NY, USA: PMLR; Association for Computing Machinery. DOI 10.1145/3287560.3287596.\n\n\nS. Mitchell, E. Potash, S. Barocas, A. D’Amour and K. Lum. Algorithmic fairness: Choices, assumptions, and definitions. Annual Review of Statistics and Its Application, 8: 141–163, 2021. DOI 10.1146/annurev-statistics-042720-125902.\n\n\nC. Molnar, B. Bischl and G. Casalicchio. iml: An R package for interpretable machine learning. JOSS, 3(26): 786, 2018. URL https://joss.theoj.org/papers/10.21105/joss.00786.\n\n\nS. U. Noble. Algorithms of oppression. New York University Press, 2018.\n\n\nC. O’neil. Weapons of math destruction: How big data increases inequality and threatens democracy. Crown, 2016.\n\n\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. Journal of Machine Learning Research, 12: 2825–2830, 2011.\n\n\nV. Perrone, M. Donini, M. B. Zafar, R. Schmucker, K. Kenthapadi and C. Archambeau. Fair Bayesian Optimization. In Proceedings of the 2021 AAAI/ACM conference on AI, ethics, and society, pages. 854–863 2021.\n\n\nF. Pfisterer, C. Kern, S. Dandl, M. Sun, M. P. Kim and B. Bischl. Mcboost: Multi-calibration boosting for R. Journal of Open Source Software, 6(64): 3453, 2021. URL https://joss.theoj.org/papers/10.21105/joss.03453.\n\n\nE. Polley, E. LeDell, C. Kennedy and M. van der Laan. SuperLearner: Super learner prediction. 2021. URL https://CRAN.R-project.org/package=SuperLearner. R package version 2.0-28.\n\n\nR Core Team. R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing, 2021. URL http://www.R-project.org/.\n\n\nP. Saleiro, B. Kuester, L. Hinkson, J. London, A. Stevens, A. Anisfeld, K. T. Rodolfa and R. Ghani. Aequitas: A bias and fairness audit toolkit. arXiv:1811.05577, 2018.\n\n\nC. Schumann, J. S. Foster, N. Mattei and J. P. Dickerson. We need fairness and explainability in algorithmic hiring. In Proceedings of the 19th international conference on autonomous agents and MultiAgent systems, pages. 1716–1720 2020. Auckland, New Zealand: International Foundation for Autonomous Agents; Multiagent Systems.\n\n\nP. Schwöbel and P. Remmers. The long arc of fairness: Formalisations and ethical discourse. In Proceedings of the conference on fairness, accountability, and transparency, pages. 2179–2188 2022. New York, NY, USA: Association for Computing Machinery. DOI 10.1145/3531146.3534635.\n\n\nM. Scutari, F. Panero and M. Proissl. Achieving fairness with a simple ridge penalty. arXiv:2105.13817, 2021.\n\n\nR. Sonabend, F. J. Király, A. Bender, B. Bischl and M. Lang. mlr3proba: An R Package for Machine Learning in Survival Analysis. Bioinformatics, 2021. DOI 10.1093/bioinformatics/btab039.\n\n\nE. J. Topol. High-performance medicine: The convergence of human and artificial intelligence. Nature Medicine, 25(1): 44–56, 2019. DOI 10.1038/s41591-018-0300-7.\n\n\nM. Turner and M. McBurnett. Predictive models with explanatory concepts: A general framework for explaining machine learning credit risk models that simultaneously increases predictive power. In Proceedings of the 15th credit scoring and credit control conference, 2019. URL https://crc.business-school.ed.ac.uk/wp-content/uploads/sites/55/2019/07/C12-Predictive-Models-with-Explanatory-Concepts-McBurnett.pdf.\n\n\nJ. Vanschoren, J. N. van Rijn, B. Bischl and L. Torgo. OpenML. ACM SIGKDD Explorations Newsletter, 15(2): 49–60, 2014. DOI 10.1145/2641190.2641198.\n\n\nS. Wachter, B. Mittelstadt and C. Russell. Bias preservation in machine learning: The legality of fairness metrics under EU non-discrimination law. West Virginia Law Review, 123: 2020.\n\n\nE. A. Watkins, M. McKenna and J. Chen. The four-fifths rule is not disparate impact: A woeful tale of epistemic trespassing in algorithmic fairness. arXiv:2202.09519, 2022.\n\n\nH. Wickham. ggplot2: Elegant graphics for data analysis. Springer-Verlag New York, 2016. URL https://ggplot2.tidyverse.org.\n\n\nJ. Wiśniewski and P. Biecek. Fairmodels: A flexible tool for bias detection, visualization, and mitigation in binary classification models. The R Journal, 14: 227–243, 2022. URL https://rj.urbanek.nz/articles/RJ-2022-019/.\n\n\nY. Xie, C. Dervieux and E. Riederer. R markdown cookbook. Boca Raton, Florida: Chapman; Hall/CRC, 2020. URL https://bookdown.org/yihui/rmarkdown-cookbook.\n\n\nM. B. Zafar, I. Valera, M. Gomez Rodriguez and K. P. Gummadi. Fairness beyond Disparate treatment & Disparate Impact. In Proceedings of the 26th international conference on world wide web, pages. 1171–1180 2017. Geneva, Switzerland: International World Wide Web Conferences Steering Committee. DOI 10.1145/3038912.3052660.\n\n\n\n\n",
    "preview": "articles/RJ-2023-034/RJ-2023-034_files/figure-html5/bmrbox-1.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 672,
    "preview_height": 672
  },
  {
    "path": "articles/RJ-2023-035/",
    "title": "ClusROC: An R Package for ROC Analysis in Three-Class Classification Problems for Clustered Data",
    "description": "This paper introduces an R package for ROC analysis in three-class classification problems, for clustered data in the presence of covariates, named ClusROC. The clustered data that we address have some hierarchical structure, i.e., dependent data deriving, for example, from longitudinal studies or repeated measurements. This package implements point and interval covariate-specific estimation of the true class fractions at a fixed pair of thresholds, the ROC surface, the volume under the ROC surface, and the optimal pairs of thresholds. We illustrate the usage of the implemented functions through two practical examples from different fields of research.",
    "author": [
      {
        "name": "Duc-Khanh To",
        "url": {}
      },
      {
        "name": "Gianfranco Adimari",
        "url": {}
      },
      {
        "name": "Monica Chiogna",
        "url": {}
      }
    ],
    "date": "2023-08-26",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-036/",
    "title": "Resampling Fuzzy Numbers with Statistical Applications: FuzzyResampling Package",
    "description": "The classical bootstrap has proven its usefulness in many areas of statistical inference. However, some shortcomings of this method are also known. Therefore, various bootstrap modifications and other resampling algorithms have been introduced, especially for real-valued data. Recently, bootstrap methods have become popular in statistical reasoning based on imprecise data often modeled by fuzzy numbers. One of the challenges faced there is to create bootstrap samples of fuzzy numbers which are similar to initial fuzzy samples but different in some way at the same time. These methods are implemented in [FuzzyResampling](https://CRAN.R-project.org/package=FuzzyResampling) package and applied in different statistical functions like single-sample or two-sample tests for the mean. Besides describing the aforementioned functions, some examples of their applications as well as numerical comparisons of the classical bootstrap with the new resampling algorithms are provided in this contribution.",
    "author": [
      {
        "name": "Maciej Romaniuk",
        "url": {}
      },
      {
        "name": "Przemysław Grzegorzewski",
        "url": {}
      }
    ],
    "date": "2023-08-26",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-037/",
    "title": "combinIT: An R Package for Combining Interaction Tests for Unreplicated Two-Way Tables",
    "description": "Several new tests have been proposed for testing interaction in unreplicated two-way analysis of variance models. Unfortunately, each test is powerful for detecting a pattern of interaction. Therefore, it is reasonable to combine multiple interaction tests to increase the power of detection for significant interactions. We introduce the package [combinIT](https://CRAN.R-project.org/package=combinIT) that provides researchers the results of six existing recommended interaction tests, including: the value of test statistics, exact Monte Carlo p-values, approximated or adjusted p-values, the results of four combined tests and explanations of interaction types if the discussed tests are significant. The software combinIT is a more comprehensive R package in comparison with the two existing packages. In addition, the software is executed quickly to obtain the exact Monte Carlo p-values, even for large Monte Carlo runs, in contrast to existing packages.",
    "author": [
      {
        "name": "Mahmood Kharrati-Kopaei",
        "url": {}
      },
      {
        "name": "Zahra Shenavari",
        "url": {}
      },
      {
        "name": "Hossein Haghbin",
        "url": {}
      }
    ],
    "date": "2023-08-26",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-037.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-038/",
    "title": "Estimating Causal Effects using Bayesian Methods with the R Package BayesCACE",
    "description": "Noncompliance, a common problem in randomized clinical trials (RCTs), complicates the analysis of the causal treatment effect, especially in meta-analysis of RCTs. The complier average causal effect (CACE) measures the effect of an intervention in the latent subgroup of the population that complies with its assigned treatment (the compliers). Recently, Bayesian hierarchical approaches have been proposed to estimate the CACE in a single RCT and a meta-analysis of RCTs. We develop an R package, BayesCACE, to provide user-friendly functions for implementing CACE analysis for binary outcomes based on the flexible Bayesian hierarchical framework. This package includes functions for analyzing data from a single study and for performing a meta-analysis with either complete or incomplete compliance data. The package also provides various functions for generating forest, trace, posterior density, and auto-correlation plots, which can be useful to review noncompliance rates, visually assess the model, and obtain study-specific and overall CACEs.",
    "author": [
      {
        "name": "Jincheng Zhou",
        "url": {}
      },
      {
        "name": "Jinhui Yang",
        "url": {}
      },
      {
        "name": "James S. Hodges",
        "url": {}
      },
      {
        "name": "Lifeng Lin",
        "url": {}
      },
      {
        "name": "Haitao Chu",
        "url": {}
      }
    ],
    "date": "2023-08-26",
    "categories": [],
    "contents": "\n1 Introduction\nNoncompliance in randomized clinical trials and causal effect\nRandomized clinical trials (RCTs) are often used to evaluate healthcare-related interventions. An RCT typically compares an experimental treatment to a standard treatment or to a placebo. A common problem in RCTs is that not all patients fully comply with the allocated treatments. Although RCT investigators control the randomization process, the actual treatments received by study participants may not follow the randomization allocation; this is called noncompliance. For example, in trials of a therapist-led intervention, noncompliance occurs when individuals randomized to the intervention fail to take the intervention (e.g., due to severe adverse events), or when some patients assigned to the control figure out a way to take the intervention. In some cases, investigators can collect outcome data on all of these patients, regardless of whether they followed interventions. When compliance status is incompletely observed, it is more complicated to evaluate the causal treatment effect.\nConventionally, researchers use the intention-to-treat (ITT) analysis, in which data are analyzed based on treatments originally allocated rather than treatments actually received. The ITT method estimates the effect of being offered the intervention, namely, the overall effect in the real world in which the intervention is made available. However, our interest may lie in a different question, namely the causal effect of actually receiving the treatment. When using ITT, the treatment effect tends to be diluted by including people who do not receive the treatment to which they were randomly allocated (Freedman 1990).\nTo identify a treatment’s causal effect, the principal stratification framework (Frangakis and Rubin 2002) is proposed, which stratifies subjects on the joint potential post-randomization variables. This causal inference method is widely used in handling various intercurrent events (also called an intermediate variable) in areas like vaccine effect (Hudgens and Halloran 2006; Zhou et al. 2016), pain relief use (Baccini et al. 2017), surrogate endpoint evaluation (Gilbert et al. 2015), noncompliance (Zhou et al. 2019), etc. An estimator called the “complier average causal effect” (CACE) has been proposed, in which patients are classified into different principal strata (compliers, never-takers, always-takers, and defiers) based on their potential behavior after assignment to both the treatment and control arms. Compliers are patients who receive the treatment as assigned; never-takers are those who do not receive treatment, regardless of treatment assignment; always-takers are those who receive treatment regardless of treatment assignment; and patients who always do the opposite of their treatment assignment are called defiers. The CACE is the causal effect of the intervention estimated from compliers. Because patients are assumed to be compliers (or not) before the randomization, the CACE retains the benefit of the randomization. Specifically, CACE is an unbiased estimate of the difference in outcomes for compliers in the intervention group compared to those in the control group, who would have engaged with treatment had they been randomized to the intervention group.\nThe biggest challenge in estimating the CACE is that we cannot actually identify which participants are compliers. Some of those receiving the treatment in the intervention group are compliers, but the rest are always-takers. Similarly, some of those not receiving the treatment in the control arm are compliers, but others are never-takers.\nSeveral R packages are available to perform CACE analysis in a single study. For example, the noncomplyR package (Coggeshall 2017) provides convenient functions for using Bayesian methods to perform inferences on the CACE. The package eefAnalytics (Kasim et al. 2017) provides tools for exploratory CACE analysis of simple randomized trials, cluster randomized trials, and multi-site trials with a focus on education trials. Besides the CACE analysis, another method commonly used to account for noncompliance is the instrumental variable (IV) method estimating the treatment effect with two-staged least squares (2SLS) regression (White 1982); the archived R package ivpack (Jiang and Small 2014) performs this type of analysis.\nCACE in meta-analysis\nAll of the above methods are framed in a single study setting. However, for analyzing multiple trials in the presence of noncompliance, no software is available for causal effect analysis, specifically for meta-analysis. When noncompliance data are reported in each trial, one could intuitively implement a two-step approach by first estimating CACE for each study and then combining the study-specific estimates using a fixed-effect or random-effects model to estimate the population-averaged CACE. Recently, Zhou et al. (2019) proposed a Bayesian hierarchical model to estimate the CACE in a meta-analysis of randomized trials where compliance may be heterogeneous between studies. It is also common that noncompliance data are not available for some trials. Simply excluding trials with incomplete noncompliance data from a meta-analysis can be inefficient and potentially biased. Zhou et al. (2021a) proposed an improved flexible Bayesian hierarchical CACE framework to account simultaneously for heterogeneous noncompliance and incomplete noncompliance data. More recently, Zhou et al. (2021b) used a generalized linear latent and mixed model to estimate CACE, which accounts for between-study heterogeneity with random effects.\nThe package BayesCACE focuses on providing user-friendly functions to estimate CACE in either a single study or meta-analysis using models based on Zhou et al. (2019), Baker (2020), Zhou et al. (2020) and Zhou et al. (2021a).\nThis article introduces the R package BayesCACE, which performs CACE analysis for binary outcomes in a single study, and meta-analysis with either complete or incomplete noncompliance information.\nThe package BayesCACE is available from the Comprehensive R Archive Network (CRAN).\nIt uses Markov chain Monte Carlo (MCMC) methods on the R platform through . is a program for analyzing Bayesian hierarchical models using MCMC simulation, which is available for diverse computer platforms including Windows and Mac OS X. Convergence of the MCMC routine can be assessed by the function outputs. The package also provides functions to generate posterior trace plots, density plots, and auto-correlation plots. For meta-analysis, the package provides a forest plot of study-specific CACE estimates with 95% credible intervals as well as the overall CACE estimate, to visually display the causal treatment effect comparisons.\nThis article is organized as follows. The next section defines CACE in mathematical notation that will be used throughout the paper. We also describe the assumptions needed to make the CACE a valid causal effect estimator. Following that, we present an overview of the Bayesian hierarchical models for CACE implemented in the BayesCACE package. Then, we illustrate use of the package with a case study example and discuss the output structures. Finally, we provide a brief discussion with potential future improvements.\nAssumptions and definition of CACE\nThe CACE is a measure of the causal effect of a treatment or intervention on patients who received it as intended by the original group allocation. It is an unbiased causal effect estimate based on five standard assumptions commonly used in causal inference research. First, it assumes that potential outcomes for each participant are independent of the potential outcomes for other participants, known as the Stable Unit Treatment Value Assumption (SUTVA). Second, it assumes that assignment to treatment is random, so that the proportion of compliers should be the same in the intervention and control groups, thus allowing the estimation of one of the core unobserved parameters needed to derive a CACE estimate. Third, it assumes that treatment assignment has an effect on the outcome only if it changes the actual treatment taken,\nan assumption known as exclusion restriction. For never-takers, for instance, it assumes that simply being assigned to treatment does not affect their outcomes, as they do not actually receive the treatment assigned to them. Fourth, it assumes that assigning the study treatment to participants in the intervention group induces at least some participants to receive the treatment, so the compliance rate is not zero. Finally, it assumes that there is a monotonic relationship between treatment assignment and treatment receipt, which implies that there are no individuals for whom assignment to treatment actually reduces the likelihood of receiving treatment (i.e., no defiers). This assumption reduces the number of compliance types for which estimates are derived, permitting a properly identified model.\nWe follow Zhou et al. (2019) and introduce notation both on the individual level and on the study level. Suppose a meta-analysis reviews \\(I\\) two-armed RCTs, and \\(N_i\\) is the number of subjects in the \\(i\\)-th trial for \\(i \\in \\{1, \\dots, I\\}\\). If the data include a single study only, then \\(I=1\\) and we can remove the subscript \\(i\\) from all notation.\nOn the individual level, notation is defined as follows for subject \\(j\\) in trial \\(i\\).\nLet \\(R_{ij}=r\\) index the randomization assignment with \\(r=0\\) for those randomized to control and \\(r=1\\) for those randomized to the intervention.\nLet \\(T^{r}_{ij}=t \\in \\{0, 1\\}\\) be the indicator of whether the individual received the intervention. This is a potential outcome under the randomization assignment \\(r\\in \\{0, 1\\}\\), i.e., what the value of treatment \\(t\\) would be for individual \\((i,j)\\) if \\(r=0\\) or \\(r=1\\), respectively.\nLet \\(Y^{r, t}_{ij}=o \\in \\{0, 1\\}\\) be the potential binary outcome under randomization assignment \\(r\\) and treatment received \\(t\\). Note that the assumption allows us to define \\(Y^{t}_{ij}\\equiv Y^{r, t}_{ij}\\).\nThe sets of \\(\\{Y^{r, t}_{ij}\\}\\) and \\(\\{T^{r}_{ij}\\}\\) are the potential outcome and treatment-received status respectively under possible \\(r\\) and \\(t\\), but for each subject in a trial, only one of the possible values of each set can be observed. Therefore, we denote the observed response and received treatment variables as \\(Y_{ij}\\) and \\(T_{ij}\\).\nWe allow \\(T_{ij}=*\\) if the actual received treatment is not recorded. Then let \\(M_{ij}=m\\) be the missing indicator corresponding to whether subject \\(j\\) has actual treatment received status on record (\\(m=0\\)) or missing (\\(m=1\\)).\nUsing these potential outcomes, we can define the compliers and the CACE. Let \\(C_{ij}\\) be the latent compliance class of individual \\(j\\) in trial \\(i\\), defined as follows:\n\\[\n   C_{ij}=\n   \\begin{cases}\n     0, & \\text{for never-taker with }\\ (T^0_{ij}, T^1_{ij})=(0, 0) \\\\\n     1, & \\text{for complier with }\\ (T^0_{ij}, T^1_{ij})=(0, 1) \\\\\n     2, & \\text{for always-taker with }\\ (T^0_{ij}, T^1_{ij})=(1, 1) \\\\\n     3, & \\text{for defier with }\\ (T^0_{ij}, T^1_{ij})=(1, 0)\n   \\end{cases}.\n\\]\nA subject’s compliance status \\(C_{ij}\\) is not observable because in a two-arm trial, only one of \\(T^1_{ij}\\) and \\(T^0_{ij}\\) can be observed. Based on the observed randomization group and actual treatment received, the compliance classes can be only partially identified.\nNow, the complier average causal effect of the \\(i\\)-th trial is the average difference between potential outcomes for compliers. In this case, the CACE in study \\(i\\) is \\(\\theta^\\text{CACE}_i=E(Y^1_{ij}-Y^0_{ij}|C_{ij}=1)\\), where the patients for whom \\(C_{ij}=1\\) are the compliers.\nOn the study level, \\(n_{irto}\\) denotes the observed number of individuals in study \\(i\\), randomization group \\(r\\), actual received treatment group \\(t\\), and outcome \\(o\\). If the compliance status of individual \\(j\\) in trial \\(i\\) is not on record, \\(T_{ij}=t=*\\) so the corresponding count is \\(n_{ir*o}\\), which is the sum of the two unobserved counts \\(n_{ir0o}\\) and \\(n_{ir1o}\\).\n2 Estimating CACE\nThis section briefly describes the Bayesian hierarchical models used to estimate CACE. These models form the basis of the framework proposed by Zhou et al. (2019) and underlie the BayesCACE package. In addition to the notation defined in the previous section, we define the following parameters for study \\(i\\).\nLet \\(\\pi_{ia}\\) and \\(\\pi_{in}\\) be the probabilities of being an always-taker and a never-taker, respectively. Because defiers are ruled out by the monotonicity assumption, each trial has at most only three compliance classes. Thus the probability of being a complier in study \\(i\\) is \\(\\pi_{ic}=1-\\pi_{ia}-\\pi_{in}\\).\nDefine these response probabilities: \\(u_{i1}\\) for a complier randomized to the treatment group; \\(v_{i1}\\) for a complier randomized to the control/placebo group; \\(s_{i1}\\) for a never-taker; and \\(b_{i1}\\) for an always-taker. Thus for study \\(i\\), the parameters included in the model are \\(\\boldsymbol{\\beta}_i=(\\pi_{ia}\\), \\(\\pi_{in}\\), \\(u_{i1}\\), \\(v_{i1}\\), \\(s_{i1}\\), \\(b_{i1})\\).\nAs the outcome is binary, the expected difference between outcomes from the two treatment groups among compliers is just the risk difference between \\(u_{i1}\\) and \\(v_{i1}\\). Therefore, the CACE can be written as \\(\\theta^\\text{CACE}_i=E(Y^1_{ij}-Y^0_{ij}|C_{ij}=1)=u_{i1}-v_{i1}\\).\nCACE for a single trial with noncompliance\nConsider first a single trial with noncompliance, i.e., \\(I = 1\\), so all notation and parameters defined earlier are reduced to the version without subscript \\(i\\). According to Zhou et al. (2019), each observed \\(n_{rto}\\) has a corresponding probability that can be written in terms of parameters defined in \\(\\boldsymbol{\\beta}=(\\pi_{a}\\), \\(\\pi_{n}\\), \\(u_{1}\\), \\(v_{1}\\), \\(s_{1}\\), \\(b_{1})\\), thus the vector \\((n_{000}, n_{001}, n_{010}, n_{011}, n_{100}, n_{101}, n_{110}, n_{111})\\) follows a multinomial distribution. The likelihood is available in the Supplemental Materials.\nThe CACE for a single study is \\(u_{1}-v_{1}\\), so the posterior of \\(\\theta^\\text{CACE}\\) is the posterior of \\(u_{1}-v_{1}\\).\nCACE for a meta-analysis with complete compliance information\nThis section introduces two methods for performing a meta-analysis of the CACE when noncompliance data are reported in each trial.\nThe two-step approach\nAs described in the previous section, using the observed data \\(n_{irto}\\), \\(\\theta^\\text{CACE}_i\\) is identified for study~\\(i\\). Therefore, to estimate the population-average CACE in a meta-analysis, we propose combining the study-specific estimates and standard errors using a standard meta-analysis method such as the fixed-effect (Laird and Mosteller 1990) or random-effects model (Hedges and Olkin 1985; Hedges and Vevea 1998). We call this a “two-step” approach. As the CACE measure is a risk difference, a transformation may be necessary to ensure that the normal distribution assumption is approximately true.\nBuilding upon the well-developed R package metafor, various estimators suggested in the literature can be estimated to account for potential between-study heterogeneity in the CACE, e.g., the Hunter–Schmidt estimator, the Hedges estimator, the DerSimonian–Laird estimator, the maximum-likelihood or restricted maximum-likelihood estimator, or the empirical Bayes estimator (Viechtbauer 2010).\nThe Bayesian hierarchical model\nIn a meta-analysis, the CACE can also be estimated using the joint likelihood from the Bayesian hierarchical model. This method is systematically introduced in Zhou et al. (2019). The log likelihood contribution of trial \\(i\\) is denoted\nby adding a subscript \\(i\\) to each parameter. Then the log likelihood for all trials in the meta-analysis is \\(\\log\\mathcal{L}(\\boldsymbol{\\beta})=\\sum_i {\\log L_i({\\boldsymbol{\\beta}}_{i})}\\).\nBecause the studies are probably not exactly identical in their eligibility criteria, measurement techniques, study quality, etc., differences in methods and sample characteristics may introduce heterogeneity to the meta-analysis. One way to model the heterogeneity is to use a random-effects model.\nTo guarantee the desired properties of study \\(i\\)’s latent compliance classes and to account for possible between-study heterogeneity in the compliance class and response probabilities, we use these transformations:\n\\(\\pi_{in}=\\frac{\\exp(n_i)}{1+\\exp(n_i)+\\exp(a_i)}, \\pi_{ia}=\\frac{\\exp(a_i)}{1+\\exp(n_i)+\\exp(a_i)}\\), where \\(n_i=\\alpha_{n}+\\delta_{in}, a_i=\\alpha_{a}+\\delta_{ia}\\), and \\(\\\\ {(\\delta_{in}, \\delta_{ia})}^\\top \\sim N(0, \\ {\\mathbf{\\Sigma}}_{ps})\\), \\(\\mathbf{\\Sigma}_{ps}=\\bigl(\\begin{smallmatrix} {\\sigma}^2_{n} & \\rho {\\sigma}_{n}{\\sigma}_{a} \\\\ \\rho {\\sigma}_{n}{\\sigma}_{a} & {\\sigma}^2_{a} \\end{smallmatrix} \\bigr)\\).\nWe also define random effect models on the transformed scale of each response probability \\(s_{i1}, b_{i1}, u_{i1}, v_{i1}\\):\n\\(g(s_{i1})=\\alpha_s+\\delta_{is}, \\ g(b_{i1})=\\alpha_b+\\delta_{ib}, \\ g(u_{i1})=\\alpha_u+\\delta_{iu}, \\ g(v_{i1})=\\alpha_v+\\delta_{iv}\\), where \\(g(\\cdot)\\) is a link function such as the logit or probit, \\(\\delta_{is} \\sim N(0,{\\sigma}_{s}^2)\\), \\(\\delta_{ib} \\sim N(0,{\\sigma}_{b}^2)\\), \\(\\delta_{iu} \\sim N(0,{\\sigma}^2_{u})\\), \\(\\delta_{iv} \\sim N(0,{\\sigma}^2_{v})\\).\nHere we allow correlation between \\(n_i\\) and \\(a_i\\), and assign random effect variables to all parameters. However, if a parameter does not vary between trials, it can be modeled as a fixed effect.\nLet \\(f(\\boldsymbol{\\beta}_i | \\boldsymbol{\\beta}_0, \\mathbf{\\Sigma}_0)\\) be the distributions described above of all parameters \\(\\boldsymbol{\\beta}_i=(\\pi_{ia}\\), \\(\\pi_{in}\\), \\(s_{i1}\\), \\(b_{i1}\\), \\(u_{i1}\\), \\(v_{i1})\\), where \\(\\boldsymbol{\\beta}_0\\) is the vector of mean hyper-parameters \\((\\alpha_{n}\\), \\(\\alpha_{a}\\), \\(\\alpha_s\\), \\(\\alpha_b\\), \\(\\alpha_u\\), \\(\\alpha_v)\\), and \\(\\mathbf{\\Sigma}_0\\) is the diagonal covariance matrix containing \\({\\mathbf{\\Sigma}}_{ps}\\), \\({\\sigma}^{2}_s\\), \\({\\sigma}^{2}_b\\), \\({\\sigma}^{2}_u\\) and \\({\\sigma}^{2}_v\\).\nIf we specify \\(f({\\boldsymbol{\\beta}_0})\\) and \\(f({\\mathbf{\\Sigma}_0})\\) as the prior distributions for the hyper-parameters, then the joint posterior distribution is proportional to the likelihood multiplied by the priors, i.e., \\(\\prod_i {L_i({\\boldsymbol{\\beta}}_{i})} f({\\boldsymbol{\\beta}}_{i} | \\boldsymbol{\\beta}_0, {\\mathbf{\\Sigma}}_0) f({\\boldsymbol{\\beta}}_0 ) f({\\mathbf{\\Sigma}}_0)\\).\nAs stated earlier, \\(\\theta^\\text{CACE}_i=u_{i1}-v_{i1}\\) for study \\(i\\), so for the meta-analysis, the overall CACE is \\(\\theta^\\text{CACE}=E(\\theta^\\text{CACE}_i)=E(u_{i1})-E(v_{i1})\\). When a random effect \\(\\delta_{iu}\\) or \\(\\delta_{iv}\\) is not assigned in the model, \\(E(u_{i1})=g^{-1}(\\alpha_u)\\) and \\(E(v_{i1})=g^{-1}(\\alpha_v)\\). Otherwise, \\(E(u_{i1})\\) and \\(E(v_{i1})\\) can be estimated by integrating out the random effects, e.g., \\(E(u_{i1})=\\int^{+\\infty }_{-\\infty }{g^{-1}(\\alpha_u+t)}\\sigma^{-1}_u \\phi (\\frac{t}{\\sigma_u})dt\\), where \\(\\phi(\\cdot)\\) is the standard Gaussian density. If the function \\(g(\\cdot)\\) is the probit link, this expectation has a closed form: \\(E(u_{i1})= \\Phi(\\frac{\\alpha_u}{\\sqrt{1+{\\sigma}^2_u}})\\). If the link function \\(g(\\cdot)\\) is logit, a well-established approximation \\(E(u_{i1}) \\approx \\text{logit}^{-1}(\\frac{\\alpha_u}{\\sqrt{1+{C^2\\sigma}^2_u}})\\) can be used, where \\(C=\\frac{16\\sqrt{3}}{15\\pi}\\) (Zeger et al. 1988). The above formulas also apply to \\(E(v_{i1})\\), the expected response rate of a complier in the control group.\nThe two-step approach, stated by Lin and Zeng (2010), can be viewed as asymptotically equivalent to the model using the joint likelihood. However, as the two-step approach requires the whole set of parameters to be estimated independently for each study, the total number of effective parameters tends to be larger than the Bayesian hierarchical model, so estimates using our method are likely to be more efficient.\nCACE for meta-analysis with incomplete compliance information\nAnother advantage of the Bayesian hierarchical model is that it can include trials with incomplete compliance data.\nCommonly, some trials do not report noncompliance data because study investigators do not collect actual received treatment status for some subjects or simply do not report compliance.\nThe two-step approach needs counts for all of the groups defined by randomized assignment, treatment received, and outcome in order to estimate the study-specific \\(\\theta^\\text{CACE}_i\\). Thus, by using this method, trials with incomplete compliance data are simply excluded, making estimation less efficient and potentially biased.\nZhou et al. (2021a) proposed a comprehensive framework to incorporate both heterogeneous and incomplete noncompliance data for estimating the CACE in a meta-analysis of RCTs. Here we present the data structure needed for binary outcomes.\nFor study \\(i\\), randomization group \\(r \\in \\{0, 1\\}\\) and output \\(o \\in \\{0, 1\\}\\), if the compliance information is reported, then values of \\(n_{ir0o}\\) and \\(n_{ir1o}\\) are reported, so we assign the marginal count \\(n_{ir*o}=0\\). Otherwise, we do not have data on outcomes for groups defined by actually received treatment, so only the marginal \\(n_{ir*o}\\) is observed, where \\(n_{ir*o}\\) is the number of patients randomized to treatment arm \\(r\\) who had outcome \\(o\\). In this situation, the two unobserved counts \\(n_{ir0o}\\) and \\(n_{ir1o}\\) are assigned as 0.\nIn the Supplemental Materials, a table for the observed counts data with corresponding probabilities is presented. The log likelihood is also obtained from the multinomial distribution.\nThe CACE for this meta-analysis incorporating incomplete compliance data is \\(\\theta^\\text{CACE} = E(\\theta^\\text{CACE}_i) = E(u_{i1}) - E(v_{i1}) = \\Phi(\\frac{\\alpha_u}{\\sqrt{1+{\\sigma}^2_u}}) - \\Phi(\\frac{\\alpha_v}{\\sqrt{1+{\\sigma}^2_v}})\\) if the probit link function is used for \\(u_{i1}\\) and \\(v_{i1}\\).\n3 Using the R package BayesCACE\nThe primary objective of the BayesCACE package is to provide a user-friendly implementation of the Bayesian method for estimating the CACE. The package is now available to download and install via CRAN at . It can be installed within R using the command install.packages(\"BayesCACE\"). The latest version of the package is 1.2.3.\nThe BayesCACE package depends on the R packages rjags (Plummer 2018), coda (Plummer et al. 2006), and forestplot (Gordon and Lumley 2017). Users need to install separately from its homepage as the BayesCACE package does not include a copy of the library. The current version of is 4.3.0, which is the version of the package that BayesCACE requires; earlier versions of may not guarantee exactly reproducible results.\nData structure for estimating the CACE\nWe introduce the data structures through the illustrative example included in the package BayesCACE: epidural_c and epidural_ic. These two data sets were obtained from Bannister-Tyrrell et al. (2015), who conducted an exploratory meta-analysis of the association between using epidural analgesia in labor and the risk of cesarean section. The dataset epidural_c contains 10 trials with full compliance information; each trial has 8 observed counts, denoted by \\(n_{irto}\\) and presented in columns nirto for \\(i=1, \\dots, 10\\) and \\(r, t, o \\in \\{0, 1\\}\\). These data were re-analyzed by Zhou et al. (2019) in a meta-analysis using their proposed Bayesian hierarchical model to estimate the CACE. The function cace.meta.c() performs this analysis. The column study.id contains IDs for the 10 studies, and study.name labels each study by its first author’s surname and its publication year.\nThe data can be loaded and printed using these commands:\n\n\nlibrary(\"BayesCACE\")\ndata(\"epidural_c\", package = \"BayesCACE\")\nepidural_c\n\n   study.id     study.name n000 n001 n010 n011 n100 n101 n110 n111\n1         1   Bofill, 1997   37    2   11    1    2    0   42    5\n2         2    Clark, 1998   72    6   68   16    7    2  134   13\n3         3  Halpern, 2004   62    5   44    7    0    0  112   12\n4         4     Head, 2002   51    7    2    0    3    0   43   10\n5         5     Jain, 2003   72   11    0    0    0    2   36    7\n6         6   Nafisi, 2006  179   19    0    0    0    0  173   24\n7         7  Nikkola, 1997    6    0    4    0    0    0   10    0\n8         8    Ramin, 1995  546   17   95    8  230    2  393   39\n9         9   Sharma, 1997  336   16    5    0  114    1  231   12\n10       10 Volmanen, 2008   23    1    3    0    1    0   23    1\n\nThe other dataset epidural_ic represents the situation in which not all trials report complete compliance data. It contains 27 studies, only 10 of which have full compliance information and are included in epidural_c. This dataset is also drawn from Bannister-Tyrrell et al. (2015), and represents studies with incomplete compliance information when estimating the CACE. The function cace.meta.ic() performs this analysis.\nEach study is represented by one row in the dataset; the columns study.id and study.name have the same meanings as in the dataset epidural_c. Each study’s data are summarized in 12 numbers (columns) denoted by \\(n_{irto}\\) and \\(n_{ir*o}\\). For a particular randomization group \\(r \\in \\{0, 1\\}\\), the observed counts are presented either as \\(n_{irto}\\) or \\(n_{ir*o}\\) depending on whether the compliance information is available; values for other columns are denoted by 0. The corresponding column names in the dataset are nirto and nirso, respectively.\nThe first 6 rows of the dataset epidural_ic are printed below.\n\n\ndata(\"epidural_ic\", package = \"BayesCACE\")\nhead(epidural_ic)\n\n  study.id       study.name n000 n001 n010 n011 n0s0 n0s1 n100 n101\n1        1     Bofill, 1997   37    2   11    1    0    0    2    0\n2        2      Clark, 1998   72    6   68   16    0    0    7    2\n3        3  Dickinson, 2002    0    0    0    0  428   71    0    0\n4        4      Evron, 2008   40    4    0    0    0    0    0    0\n5        5 El Kerdawy, 2010    0    0    0    0   12    3    0    0\n6        6   Gambling, 1998    0    0    0    0  573   34  206   10\n  n110 n111 n1s0 n1s1\n1   42    5    0    0\n2  134   13    0    0\n3    0    0  408   85\n4    0    0  129   19\n5    0    0   11    4\n6  371   29    0    0\n\nNote that NA is not allowed in a dataset for the package BayesCACE, but some trials may have 0 events or 0 noncompliance rates.\nPlotting noncompliance rates\nBefore performing the CACE analysis, one might want a visual overview of study-specific noncompliance rates in both randomization arms. The function plt.noncomp provides a forest plot of noncompliance rates in an R plot window. The function can be simply called as\n\n\nplt.noncomp(data, overall = TRUE)\n\n\nwhere data is a dataset with structure like epidural_c or epidural_ic. Specifically, the dataset should contain the following columns: , , and 8 or 12 columns of data represented by \\(n_{irto}\\), or \\(n_{irto}\\) and \\(n_{ir*o}\\) (see previous section for more details). Each row corresponds to one study. Only studies with full compliance information are included in this plot because noncompliance rates cannot be calculated without compliance data.\nFigure \\(\\ref{fig:noncomp}\\) shows the resulting plot, where the red dot with its horizontal line shows the study-specific noncompliance rate with its 95% exact confidence interval for the patients randomized to the treatment arm, and the blue square with its horizontal line represents that rate and interval for those in the control arm.\nThe confidence intervals are calculated by the Clopper–Pearson exact method (Clopper and Pearson 1934), which is based on the cumulative distribution function of the binomial distribution. Using the default overall = TRUE, the figure also gives a summary estimate of the compliance rates per randomization group. This overall rate is estimated using a logit generalized linear mixed model. Otherwise, if the argument overall is FALSE, the plot shows only study-specific noncompliance rates. Any additional parameters passed to the function will be automatically used in the forestplot function from the forestplot package.\n\n\n\nFigure 1: Noncompliance rates plot generated by the function plt.noncomp(). The red dots and lines show the study-specific noncompliance rate with its 95% confidence interval randomized to the treatment arm, and the blue squares and lines refer to those in the control arm.\n\n\n\nCACE analysis for a single study or in a meta-analysis\nThe major functions in BayesCACE\nare cace.study(), cace.meta.c(), and cace.meta.ic(), which implement the models introduced earlier to perform Bayesian CACE analysis for different data structures. In particular, cace.study() performs CACE analysis for a single study. The function cace.meta.c() performs CACE analysis for a meta-analysis when each trial reports noncompliance information. Users can choose to do the analysis either by the two-step approach or using the Bayesian hierarchical model. When some trials do not report noncompliance data, the function cace.meta.ic() can be applied to perform a CACE meta-analysis using the likelihood provided in the Supplemental Materials. Each function may take 1–15 minutes to run. Generally the two-step approach using the function cace.meta.c() takes longer because MCMC chains are run on the studies one by one. The actual run time depends on the amount of data and the user’s processor.\nFunction cace.study() for a study-specific analysis or a two-step meta-analysis\nFor the default interface, the arguments of the function cace.study() are\n\n\ncace.study(data, param = c(\"CACE\", \"u1\", \"v1\", \"s1\", \"b1\", \"pi.c\", \"pi.n\", \n  \"pi.a\"), re.values = list(), model.code = '', digits = 3, n.adapt = 1000, \n  n.iter = 100000, n.burnin = floor(n.iter/2), n.chains = 3, n.thin =  \n  max(1,floor((n.iter-n.burnin)/1e+05)), conv.diag = FALSE, mcmc.samples =\n  FALSE, two.step = FALSE, method = \"REML\")\n\n\nwhere users need to input data with the same structure as epidural_c, containing either one row of observations for a single study, or multiple rows referring to multiple studies in a meta-analysis. This function fits a model for a single study. If the data includes more than one study, the study-specific CACEs will be estimated by retrieving data row by row.\nThe argument param is a character string vector indicating the parameters to be tracked and estimated. By default all parameters are included: \\(\\theta^\\text{CACE}\\) (CACE), \\(u_1\\) (u1), \\(v_1\\) (v1), \\(s_1\\) (s1), \\(b_1\\) (b1), \\(\\pi_a\\) (pi.a), \\(\\pi_n\\) (pi.n), and \\(\\pi_c=1-\\pi_a-\\pi_n\\) (pi.c). Users can modify the string vector to only include parameters of interest besides \\(\\theta^\\text{CACE}\\).\nUsers can specify the prior distributions (mean and standard deviation) of \\(n, a, \\alpha_s, \\alpha_b, \\alpha_u, \\alpha_v\\) with the re.values parameter. By default, the re.values list is empty, and they are assigned to the transformed scale of the following parameters:\n\\(\\pi_{n}=\\frac{\\exp(n)}{1+\\exp(n)+\\exp(a)}\\), \\(\\pi_{a}=\\frac{\\exp(a)}{1+\\exp(n)+\\exp(a)}\\), \\(\\text{logit}(s_{1})=\\alpha_s\\), \\(\\text{logit}(b_{1})=\\alpha_b\\), \\(\\text{probit}(u_{1})=\\alpha_u\\), and \\(\\text{probit}(v_{1})=\\alpha_v\\), where \\(n, a \\sim N(0, 2.5^2)\\) and \\(\\alpha_s, \\alpha_b, \\alpha_u, \\alpha_v \\sim N(0, 2^2)\\). With these settings, a 95% prior probability interval for any of the probabilities \\(\\pi_{in}, \\pi_{ia}\\), and \\(\\pi_{ic}\\) ranges from about \\(0.001\\) to \\(0.91\\), and a 95% prior interval for the probabilities \\(s_1\\), \\(b_1\\), \\(u_1\\), and \\(v_1\\) ranges approximately from \\(0.01\\) to \\(0.98\\).\nThe prior parameters are passed into the model.study function to get the model code, which first calls the prior.study to get the custom prior distribution.\nHere we give an example output of prior.study when assigning \\(N(0, 10^{-2})\\) to every parameter:\n\n\n  out.string <-   \n    \"# priors\n    n ~ dnorm(0, 0.01)\n    a ~ dnorm(0, 0.01)\n    alpha.s ~ dnorm(0, 0.01)\n    alpha.b ~ dnorm(0, 0.01)\n    alpha.u ~ dnorm(0, 0.01)\n    alpha.v ~ dnorm(0, 0.01)\n    \"\n\n\nTo customize the model fully, the user can pass their complete model string to the cace.study() function with the parameter model.code. The arguments n.adapt, n.iter, n.burnin, n.chains, and n.thin control the MCMC algorithm run by the R package rjags (Plummer 2018). The argument n.adapt is the number of iterations for adaptation; it is used to maximize the sampling efficiency, and the default is set as 1,000. The argument n.chains determines the number of MCMC chains (the default is 3); n.iter is the number of iterations of each MCMC chain; n.burnin is the number of burn-in iterations to be discarded at the beginning of each chain; n.thin is the thinning rate for MCMC chains, which is used to avoid potential high auto-correlation and to save computer memory when n.iter is large. The default of n.thin is set as 1 or the largest integer not greater than ((n.iter - n.burnin)/1e+05)), whichever is larger.\nThe argument conv.diag specifies whether to compute the Gelman and Rubin convergence statistic (\\(\\hat{R}\\)) of each parameter as a convergence diagnostic (Gelman and Rubin 1992; Brooks and Gelman 1998). The chains are considered well-mixed and converged to the target distribution if \\(\\hat{R}\\ \\mathrm{\\le}\\ \\mathrm{1.1}\\). If the argument mcmc.samples = TRUE, the function saves each chain’s MCMC samples for all parameters, which can be used to produce trace, posterior density, and auto-correlation plots by calling the functions plt.trace, plt.density, and plt.acf.\nBy default, the function cace.study() returns a list including posterior estimates (posterior mean, standard deviation, median, and a 95% credible interval with 2.5% and 97.5% quantiles as the lower and upper bounds), and the deviance information criterion (DIC) statistic (Spiegelhalter et al. 2002) for each study.\nThe argument two.step is a logical value indicating whether to conduct a two-step meta-analysis. If two.step = TRUE, the posterior mean and standard deviation of study-specific \\(\\theta^\\text{CACE}_i\\) are used to perform a standard meta-analysis, using the R package metafor. The default estimation method is the REML (restricted maximum-likelihood estimator) method for the random-effects model (Harville 1977). Users can change the argument method to obtain different meta-analysis estimators from either a random-effects model or a fixed-effect model, e.g.,\nmethod = \"DL\" refers to the DerSimonian–Laird estimator, method = \"HE\" returns the Hedges estimator, and method = \"HS\" gives the Hunter–Schmidt estimator. More details are available from the documentation of the function metafor::rma (Viechtbauer 2010). If the input data include only one study, the meta-analysis result is the same as the result from the single study.\nHere is an example to demonstrate the function’s usage. We call the function cace.study() on the dataset epidural_c as follows:\n\n\ndata(\"epidural_c\", package = \"BayesCACE\")\nset.seed(123)\nout.study <- cace.study(data = epidural_c, conv.diag = TRUE, \n                        mcmc.samples = TRUE, two.step = TRUE)\n\n\nThe following messages are output as the code runs:\n\n% NA is not allowed in the input data set;\n% the rows containing NA are removed.\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 2\n   Unobserved stochastic nodes: 6\n   Total graph size: 44\n\nInitializing model\n\n  |++++++++++++++++++++++++++++++++++++++++++++++++++| 100%\n  |**************************************************| 100%\n  |**************************************************| 100%\nMCMC convergence diagnostic statistics are calculated and saved in conv.out\n\nIf the dataset contains more than one study, e.g., the epidural_c dataset has 10 trials, then once the model compiles for the first study, it automatically continues to run on the next study’s data. The results are saved in the object out.study, a list containing the model name, posterior information for each monitored parameter, and DIC of each study.\nWe can use parameter names to display the corresponding estimates.\nThe argument digits in the function cace.study() can be used to change the number of significant digits to the right of the decimal point. Here, we used the default setting digits = 3.\nFor example, the estimates of \\(\\theta^\\text{CACE}\\) for each single study (posterior mean and standard deviation, posterior median, 95% credible interval, and time-series standard error) can be displayed as:\n\n\nout.study$CACE\n\n          Mean     SD     2.5%       50%  97.5% Naive SE\n [1,]  0.04980 0.0797 -0.09510  4.46e-02 0.2180 1.45e-04\n [2,] -0.02490 0.0489 -0.12200 -2.23e-02 0.0785 8.94e-05\n [3,] -0.02210 0.0606 -0.12700 -2.90e-02 0.1120 1.11e-04\n [4,]  0.07180 0.0758 -0.07550  7.10e-02 0.2230 1.38e-04\n [5,]  0.08250 0.0768 -0.06260  8.11e-02 0.2370 1.40e-04\n [6,]  0.02600 0.0319 -0.03650  2.59e-02 0.0891 5.83e-05\n [7,]  0.01430 0.1580 -0.28200  2.11e-04 0.4050 2.89e-04\n [8,]  0.05030 0.0248  0.00176  5.02e-02 0.0993 4.54e-05\n [9,] -0.01100 0.0234 -0.05740 -1.09e-02 0.0350 4.27e-05\n[10,]  0.00145 0.0655 -0.13400 -4.36e-06 0.1460 1.20e-04\n      Time-series SE\n [1,]       2.51e-04\n [2,]       1.48e-04\n [3,]       1.94e-04\n [4,]       2.01e-04\n [5,]       2.51e-04\n [6,]       7.55e-05\n [7,]       4.21e-04\n [8,]       7.34e-05\n [9,]       6.22e-05\n[10,]       1.56e-04\n\nIf the argument conv.diag is specified as TRUE, the output list contains a sub-list conv.out, which outputs the point estimates of the “potential scale reduction factor” (the Gelman and Rubin convergence statistic, labeled Point est.) calculated for each parameter from each single study, and their upper confidence limits (labeled Upper C.I.).\nApproximate convergence is diagnosed when the upper limit is close to 1 (Gelman and Rubin 1992; Brooks and Gelman 1998).\nFor example, the first sub-list from conv.out is:\n\n\nout.study$conv.out[[1]]\n\n     Point est. Upper C.I.\nCACE   1.000025   1.000046\nb1     1.000041   1.000129\npi.a   1.000025   1.000094\npi.c   1.000036   1.000134\npi.n   1.000029   1.000067\ns1     1.000014   1.000018\nu1     1.000016   1.000033\nv1     1.000077   1.000185\n\nIn this example, we included mcmc.samples = TRUE in the argument, so the output list out.study includes each chain’s MCMC samples for all parameters. They can be used with our plotting functions to generate the trace, posterior density, and auto-correlation plots for further model diagnostics.\nIf the dataset used by the function cace.study() has more than one study, specifying the argument two.step = TRUE causes the two-step meta-analysis for \\(\\theta^\\text{CACE}\\) to be done. The outcomes are saved as a sub-list object meta. Note that users can obtain different meta-analysis estimators by changing the method argument as described earlier.\n\n\nout.study$meta\n\n\nRandom-Effects Model (k = 10; tau^2 estimator: REML)\n\ntau^2 (estimated amount of total heterogeneity): 0.0002 (SE = 0.0008)\ntau (square root of estimated tau^2 value):      0.0131\nI^2 (total heterogeneity / total variability):   8.10%\nH^2 (total variability / sampling variability):  1.09\n\nTest for Heterogeneity:\nQ(df = 9) = 5.9353, p-val = 0.7464\n\nModel Results:\n\nestimate      se    zval    pval    ci.lb   ci.ub    \n  0.0182  0.0143  1.2758  0.2020  -0.0098  0.0462    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nFunction cace.meta.c() for meta-analysis with complete compliance data\nThe function cace.meta.c() performs the Bayesian hierarchical model method for meta-analysis when the dataset has complete compliance information for all studies. The function’s default arguments are as shown:\n\n\ncace.meta.c(data, param = c(\"CACE\", \"u1out\", \"v1out\", \"s1out\", \"b1out\", \n  \"pic\", \"pin\", \"pia\"), random.effects = list(), re.values = list(), \n  model.code = '', digits = 3, n.adapt = 1000, n.iter = 100000,\n  n.burnin = floor(n.iter/2), n.chains = 3, n.thin =\n  max(1,floor((n.iter-n.burnin)/100000)), conv.diag = FALSE, \n  mcmc.samples = FALSE, study.specific = FALSE)\n\n\nThe arguments controlling the MCMC algorithm are mostly similar to those of cace.study(). One major difference is that users need to specify parameters that are modeled as random effects. Earlier, we showed how to specify random effects for each parameter on the transformed scales, namely \\(\\delta_{in}\\), \\(\\delta_{ia}\\), \\(\\delta_{iu}\\), \\(\\delta_{iv}\\), \\(\\delta_{is}\\), and \\(\\delta_{ib}\\), and allowed a non-zero correlation \\(\\rho\\) between \\(\\delta_{in}\\) and \\(\\delta_{ia}\\). The model with all of these random effects as well as the correlation \\(\\rho\\) is considered the full model. However, this function is flexible, allowing users to choose which random effects to include by specifying the random.effects argument. By default, the list is empty and all of the list values are set to TRUE. Users can customize that by setting delta.n, delta.a, delta.u, delta.v, delta.s, delta.b, and/or cor to FALSE.\nNote that \\(\\rho\\) (cor) can only be included when both \\(\\delta_{in}\\) (delta.n) and \\(\\delta_{ia}\\) (delta.a) are set to TRUE. Otherwise, a warning is shown and the model continues running by forcing delta.n = TRUE and delta.a = TRUE.\nThe default parameters to be monitored depend on which parameters are modeled as random effects. For example, u1out refers to \\(E(u_{i1})\\), where for the probit link, \\(E(u_{i1})=\\Phi({\\alpha_u})\\) if \\(\\delta_u\\) is not specified in the model, and \\(E(u_{i1})=\\Phi(\\frac{\\alpha_u}{\\sqrt{1+{\\sigma}^2_u}})\\) when the random effect \\(\\delta_u\\) is included.\nUsers can use the re.values parameter to customize the prior distribution. Like the function cace.study(), by default, weakly informative priors \\({\\alpha}_{n}, {\\alpha}_{a} \\sim N(0, 2.5^2)\\) and \\(\\alpha_s\\), \\(\\alpha_b\\), \\(\\alpha_u\\), \\(\\alpha_v \\sim N(0, 2^2)\\) are assigned to the means of these transformed parameters: \\(\\pi_{in}=\\frac{\\exp(n_i)}{1+\\exp(n_i)+\\exp(a_i)}\\), \\(\\pi_{ia}=\\frac{\\exp(a_i)}{1+\\exp(n_i)+\\exp(a_i)}\\), where \\(n_i={\\alpha}_{n}+{\\delta}_{in}\\), \\(a_i={\\alpha}_{a}+{\\delta}_{ia}\\), \\(\\text{logit}(s_{i1})=\\alpha_s + \\delta_{is}\\),\n\\(\\text{logit}(b_{i1})=\\alpha_b + \\delta_{ib}\\), \\(\\text{probit}(u_{i1})=\\alpha_u + \\delta_{iu}\\), and \\(\\text{probit}(v_{i1})=\\alpha_v + \\delta_{iv}\\).\nFor the random effects, we have \\({\\delta}_{is} \\sim N(0,{\\sigma}^2_{s})\\),\n\\({\\delta}_{ib} \\sim N(0,{\\sigma}^2_{b})\\),\n\\({\\delta}_{iu} \\sim N(0,{\\sigma}^2_{u})\\), and\n\\({\\delta}_{iv} \\sim N(0,{\\sigma}^2_{v})\\),\nas response rates are assumed to be independent between latent classes.\nA \\(Gamma(2, 2)\\) hyper-prior distribution is assigned to the precision parameters \\({\\sigma}^{-2}_s\\), \\({\\sigma}^{-2}_b\\), \\({\\sigma}^{-2}_u\\) and \\({\\sigma}^{-2}_v\\), which corresponds to a 95% interval of \\((0.6, 2.9)\\) for the corresponding standard deviations, allowing moderate heterogeneity in the response rates. In a reduced model with one of \\(\\delta_{in}\\) or \\(\\delta_{ia}\\) set to 0, the prior of the other precision parameter is also assumed to be \\(Gamma\\mathrm(2, 2)\\), which gives moderate heterogeneity for latent compliance class probabilities, whereas for the full model, \\({(\\delta_{in}, \\delta_{ia})}^\\top \\sim N(0, \\ {\\mathbf{\\Sigma}}_{ps})\\),\nthe prior for the variance-covariance matrix \\({\\mathbf{\\Sigma}}_{ps}\\) is \\(InvWishart(\\mathbf{I}, 3)\\), where \\(\\mathbf{I}\\) is the identity matrix.\nSimilar to cace.study(), to customize the model fully, the user can pass their complete model string with the parameter model.code. Because the function cace.meta.c() is more complicated depending on the choice of random effects, we show an example of the customized prior distributions when assigning delta.n = TRUE, delta.a = TRUE, delta.u = TRUE, delta.v = FALSE, delta.s = TRUE, delta.b = TRUE, and cor = TRUE while keeping default values for re.values.\n\n\nstring <-   \n\"# priors\nalpha.n ~  dnorm(0, 0.16)\nalpha.a ~ dnorm(0, 0.16)    \nalpha.s ~  dnorm(0, 0.25)\nalpha.b ~  dnorm(0, 0.25)\nalpha.u ~  dnorm(0, 0.25)\nalpha.v ~  dnorm(0, 0.25) \n\nII[1,1] <- 1\nII[2,2] <- 1\nII[1,2] <- 0\nII[2,1] <- 0\n\nOmega.rho ~  dwish (II[,], 3)\nSigma.rho <- inverse(Omega.rho)\nsigma.n <- Sigma.rho[1, 1]\nsigma.a <- Sigma.rho[2, 2]\nrho <- Sigma.rho[1, 2]\nu1out <- phi(alpha.u/sqrt(1+sigma.u^2))\ntau.u ~ dgamma(2, 2)\nsigma.u <- 1/sqrt(tau.u)\nv1out <- phi(alpha.v)\nCACE <- u1out-v1out\ns1out <- ilogit(alpha.s/sqrt(1 + (16^2*3/(15^2*pi^2))*sigma.s^2))\ntau.s ~ dgamma(2, 2)\nsigma.s <- 1/sqrt(tau.s)\nb1out <- ilogit(alpha.b/sqrt(1 + (16^2*3/(15^2*pi^2))*sigma.b^2))\ntau.b ~ dgamma(2, 2)\nsigma.b <- 1/sqrt(tau.b)\n\"\n\n\nThe epidural_c dataset is used as a real-study example:\n\n\ndata(\"epidural_c\", package = \"BayesCACE\")\nset.seed(123)\nout.meta.c <- cace.meta.c(data = epidural_c, conv.diag = TRUE, \n                          mcmc.samples = TRUE, study.specific = TRUE)\n\n\nThe usage of arguments conv.diag and mcmc.samples is the same as for the function cace.study.\nWhen the argument study.specific is specified as TRUE, the model will first check the logical status of arguments delta.u and delta.v. If both are FALSE, meaning that neither response rate \\(u_{i1}\\) or \\(v_{i1}\\) is modeled with a random effect, then the study-specific \\(\\theta^\\text{CACE}_i\\) is the same across studies. The function gives a warning and continues by making study.specific = FALSE. Otherwise, the study-specific \\(\\theta^\\text{CACE}_i\\) are estimated and saved as the parameter cacei.\nIn this example, by calling the object smry from the output list out.meta.c, posterior estimates (posterior mean, standard deviation, posterior median, 95% credible interval, and time-series standard error) are displayed.\n\n\nout.meta.c$smry\n\n               Mean     SD     2.5%       50%  97.5% Naive SE\nCACE       0.020200 0.0627 -0.10200  0.018900 0.1490 1.14e-04\nb1out      0.128000 0.0459  0.05970  0.121000 0.2370 8.39e-05\ncacei[1]   0.043900 0.0679 -0.08130  0.040700 0.1870 1.24e-04\ncacei[2]  -0.023100 0.0489 -0.11500 -0.025000 0.0822 8.94e-05\ncacei[3]  -0.007630 0.0569 -0.11000 -0.011800 0.1130 1.04e-04\ncacei[4]   0.065000 0.0678 -0.06620  0.064300 0.2010 1.24e-04\ncacei[5]   0.054000 0.0686 -0.07380  0.051500 0.1960 1.25e-04\ncacei[6]   0.026300 0.0309 -0.03390  0.026200 0.0875 5.64e-05\ncacei[7]   0.002770 0.0931 -0.18900 -0.000142 0.2100 1.70e-04\ncacei[8]   0.048300 0.0239  0.00171  0.048200 0.0956 4.36e-05\ncacei[9]  -0.010600 0.0224 -0.05520 -0.010500 0.0331 4.09e-05\ncacei[10]  0.000228 0.0600 -0.12000 -0.001200 0.1280 1.10e-04\npia        0.114000 0.0742  0.02460  0.098200 0.3010 1.35e-04\npic        0.821000 0.0842  0.60500  0.838000 0.9350 1.54e-04\npin        0.064200 0.0397  0.01540  0.056700 0.1590 7.25e-05\ns1out      0.184000 0.1040  0.04560  0.161000 0.4450 1.91e-04\nu1out      0.127000 0.0473  0.05520  0.120000 0.2390 8.64e-05\nv1out      0.107000 0.0406  0.04700  0.100000 0.2040 7.41e-05\n          Time-series SE\nCACE            7.69e-04\nb1out           4.07e-04\ncacei[1]        2.35e-04\ncacei[2]        1.89e-04\ncacei[3]        2.16e-04\ncacei[4]        1.62e-04\ncacei[5]        2.45e-04\ncacei[6]        6.87e-05\ncacei[7]        3.71e-04\ncacei[8]        6.38e-05\ncacei[9]        5.49e-05\ncacei[10]       2.15e-04\npia             3.92e-03\npic             4.50e-03\npin             2.11e-03\ns1out           9.03e-04\nu1out           6.10e-04\nv1out           4.55e-04\n\nThe posterior estimates of \\(\\theta^\\text{CACE}_i\\) can be used to make a forest plot by calling the function plt.forest.\nUsers can manually do model selection procedures by including different random effects and comparing DIC from the outputs. DIC and its two components are saved as an object DIC in the output list.\n\n\nout.meta.c$DIC\n\n               \nD.bar 204.40801\npD     44.74788\nDIC   249.15590\n\nDIC is the penalized deviance, calculated as the sum of D.bar and pD, where D.bar is the posterior expectation of the deviance, reflecting the model fit, and pD reflects the effective number of parameters in the model.\nD.bar is usually lower when more parameters are included in the model, but complex models may lead to overfitting. Thus DIC balances the model’s fit against the effective number of parameters.\nGenerally a model with smaller DIC is preferred. However, it is difficult to conclude what constitutes an important improvement in DIC. Following Lunn et al. (2012), we suggest that a reduction of less than 5 is not a substantial improvement.\nWhen fitting models to a particular dataset, it is usually uncertain which random effect variables should be included in the model. The function cace.meta.c() allows users to specify candidate models with different random effects, and thus to conduct a forward/backward/stepwise model selection procedure to choose the best fitting model.\nFunction cace.meta.ic() for meta-analysis with incomplete compliance information\nAnother major function in the package BayesCACE is cace.meta.ic(). It also estimates \\(\\theta^\\text{CACE}\\) using the Bayesian hierarchical model but can accommodate studies with incomplete compliance data. The arguments of this function are:\n\n\ncace.meta.ic(data, param = c(\"CACE\", \"u1out\", \"v1out\", \"s1out\", \"b1out\", \n  \"pic\", \"pin\", \"pia\"), random.effects = list(), re.values = list(), \n  model.code = '', digits = 3, n.adapt = 1000, n.iter = 100000,\n  n.burnin = floor(n.iter/2), n.chains = 3, n.thin = \n  max(1,floor((n.iter-n.burnin)/100000)), conv.diag = FALSE, \n  mcmc.samples = FALSE, study.specific = FALSE)\n\n\nThe arguments of cace.meta.ic() are mostly similar to those of cace.meta.c(), although the function cace.meta.ic() calls a different built-in model file from the package BayesCACE. The major difference in using this function is that users need to create a dataset with the same structure as epidural_ic. As for cace.meta.c(), users can set their customized prior distributions.\nHere we use the epidural_ic dataset as an example:\n\n\ndata(\"epidural_ic\", package = \"BayesCACE\")\nset.seed(123)\nout.meta.ic <- cace.meta.ic(data = epidural_ic, conv.diag = TRUE, \n                            mcmc.samples = TRUE, study.specific = TRUE)\n\n\nThe results are saved in the object out.meta.ic, a list containing posterior estimates for monitored parameters, DIC, convergence diagnostic statistics, and MCMC samples.\nIn this example, the argument study.specific is TRUE, so the summary for each study-specific \\(\\theta^\\text{CACE}_i\\) is displayed in the object out.meta.ic$smry\ntogether with other parameters.\nNote that when compiling the model, the warning “adaptation incomplete” may occasionally occur, indicating that the number of iterations for the adaptation process is not sufficient. The default value of n.adapt (the number of iterations for adaptation) is 1,000. This is an initial sampling phase during which the samplers adapt their behavior to maximize their efficiency (e.g., a Metropolis–Hastings random walk algorithm may change its step size) (Plummer 2018). The “adaptation incomplete” warning indicates that the MCMC algorithm may not achieve maximum efficiency, but it generally has little impact on the posterior estimates of the treatment effects. To avoid this warning, users may increase n.adapt.\nPlotting the trace plot, posterior density, and auto-correlation\nWhen compiling the models, it is helpful to assess the performance of the MCMC algorithm. The functions plt.trace, plt.density, and plt.acf provide diagnostic plots for the MCMC, namely trace plots, kernel density estimation plots, and auto-correlation plots. Both trace plots and auto-correlation plots can be used to examine whether the MCMC chains appear to be drawn from stationary distributions. A posterior density plot for a parameter visually shows the posterior distribution.\nUsers can simply call this function on objects produced by cace.study(), cace.meta.c(), or cace.meta.ic().\nThe arguments of this plot function are:\n\n\nplt.trace(obj, param = c(\"CACE\"), trialnumber = 1, ...)\nplt.density(obj, param = c(\"CACE\"), trialnumber = 1, ...)\nplt.acf(obj, param = c(\"CACE\"), trialnumber = 1, ...)\n\n\nWe use the objects list obtained from fitting the Bayesian hierarchical model cace.meta.ic() as an example to generate the three plots. To avoid lengthy output we just illustrate how these plots are produced for \\(\\theta^\\text{CACE}\\). The relevant code is:\n\n\nplt.trace(obj = out.meta.ic)\nplt.density(obj = out.meta.ic)\nplt.acf(obj = out.meta.ic)\n\n\nThe produced plots are shown in Figures \\(\\ref{fig:trace}\\)–\\(\\ref{fig:autocorr}\\).\nThe trace plots in Figure \\(\\ref{fig:trace}\\) show the parameter values sampled at each iteration versus the iteration number. Each chain is drawn as a separate trace plot to avoid overlay.\nHere we used the default n.chains = 3, so three trace plots are drawn. These plots show evidence that the posterior samples of \\(\\theta^\\text{CACE}\\) are drawn from the stationary distribution.\n\n\n\nFigure 2: Trace plots for \\(\\theta^\\text{CACE}\\) from the epidural_ic dataset fit using cace.meta.ic() for a sample of 3 chains. Because there are no strong patterns and the variability is relatively constant, we can conclude that the posterior means are drawn from a stationary distribution.\n\n\n\n\n\n\nFigure 3: The kernel smoothed density for \\(\\theta^{\\text{CACE}}\\) from the function cace.meta.ic() applied to the epidural analgesia in labor meta-analysis. The posterior mean is close to 0, indicating that the complier average causal effect may not be significant in this case.\n\n\n\nThe density plot in Figure \\(\\ref{fig:density}\\) is smoothed using the R function density(). It shows that the kernel-smoothed posterior of \\(\\theta^\\text{CACE}\\) is almost symmetric. The posterior mean is not far from 0, indicating that the complier average causal effect of using epidural analgesia in labor on cesarean section is likely not significant.\n\n\n\nFigure 4: Auto-correlation plot of \\(\\theta^{\\text{CACE}}\\) from the model cace.meta.ic() fit to the epidural_ic dataset. As the lag increases, the values become less correlated. Users can choose to address high auto-correlation with a longer chain or a larger n.thin.\n\n\n\nThe auto-correlation plot in Figure \\(\\ref{fig:autocorr}\\) is a bar plot displaying the auto-correlation for different lags.\nAt lag 0, the value of the chain has perfect auto-correlation with itself. As the lag becomes greater, the values become less correlated. After a lag of about 50, the auto-correlation drops below 0.1. If the plot shows high auto-correlation, users can run the chain longer or can choose a larger n.thin, e.g., n.thin = 10 would keep only 1 out of every 10 iterations, so that the thinned out chain is expected to have the auto-correlation drop quickly. Any additional parameters passed to the 3 plotting function will be automatically used in the plot function for plt.trace and plt.density, and in the acf function for plt.acf.\nPlotting the study-specific CACE in a forest plot\nA graphical overview of the results can be obtained by creating a forest plot (Lewis and Clarke 2001). The function plt.forest() draws a forest plot for \\(\\theta^{\\text{CACE}}\\) estimated from the meta-analysis.\nUsers can call this function for the objects from cace.meta.c() or cace.meta.ic().\nHere is an example using the object out.meta.ic:\n\n\nplt.forest(data = epidural_ic, obj = out.meta.ic)\n\n\nNote that in addition to the object out.meta.ic, users also need to specify the dataset used to compute that object, from which the plt.forest() function extracts the study names and publication years for the figure.\n\n\n\nFigure 5: Forest plot of study-specific \\(\\theta^{\\text{CACE}}\\) from the model cace.meta.ic() with full random effects fit to the epidural_ic dataset. The summary estimate and confidence interval limits based on the model cace.meta.ic() are included in the figure, both in terms of written values and the squares and lines on the right. Overall, it shows that the study-specific \\(\\theta^{\\text{CACE}}_i\\) vary from negative to positive in individual studies, while most of the 95% credible intervals cover zero.\n\n\n\nFigure \\(\\ref{fig:forest}\\) is a forest plot of \\(\\theta^\\text{CACE}_i\\) for each study individually, using the Bayesian method with full random effects and default priors.\nThe summary estimate based on the model cace.meta.ic() is automatically added to the figure, with the outer edges of the polygon indicating the confidence interval limits.\nThe 95% credible interval of the summary \\(\\theta^{\\text{CACE}}\\) covers zero, indicating a non-significant complier average causal effect estimate for using epidural analgesia in labor on the risk of cesarean section for the meta-analysis with 27 trials.\nFor a study with incomplete data on compliance status, a dashed horizontal line in the forest plot is used to represent the posterior 95% credible interval of \\(\\theta^\\text{CACE}_i\\) from the Bayesian hierarchical model fit.\nThe study-specific \\(\\theta^{\\text{CACE}}_i\\) vary from negative to positive in individual studies, while most of the 95% credible intervals cover zero.\nAs the \\(\\theta^\\text{CACE}_i\\) for a trial without complete compliance data is not estimable using only data from that single trial, dashed lines tend to have longer credible intervals than those with complete data (solid lines).\n4 Discussion\nThis article provides an overview of the BayesCACE package for conducting CACE analysis with R. Bayesian hierarchical models estimating the CACE in individual studies and in meta-analysis are introduced to demonstrate the underlying methods of the functions. Practical usage of various functions is illustrated using real meta-analysis datasets epidural_c and epidural_ic. The package provides several plots for model outputs and model diagnosis.\nIt is important to note that the two-step approach for meta-analysis is included in the package BayesCACE because by using the full observed data from a single study \\(i\\), \\(\\theta^\\text{CACE}_i\\) is identifiable, making it possible to pool the estimated posterior means and standard deviations of the \\(\\theta^\\text{CACE}_i\\) in a meta-analysis. However, the Bayesian hierarchical-model meta-analysis method for estimating the overall CACE is preferred for two reasons: the conventional two-step approach requires the whole set of parameters to be estimated for each trial, giving a greater total number of parameters than the random effect model, so the estimate of the CACE can be less efficient. Also, when study \\(i\\) does not report complete compliance data, it must be excluded from the two-step approach because \\(\\theta^\\text{CACE}_i\\) is no longer directly estimable by simply using the incomplete data from this individual study, while the function cace.meta.ic() can use the incomplete information and thus help improve the efficacy in estimation.\nThe Gelman and Rubin convergence statistics, time-series standard errors, trace plots, and auto-correlation plots are provided by the package BayesCACE to examine whether the MCMC chains are drawn from stationary distributions. However, in practice, any sample is finite, thus there is no guaranteed way to prove that the sampler has converged (Cowles and Carlin 1996; Kass et al. 1998).\nAdditional techniques may be required to determine the effective sample size for adequate convergence (Robert and Casella 2004). For example, the well-developed R package mcmcse (Flegal et al. 2017) can be used to assess whether MCMC has been run for enough iterations (sufficient chain lengths). To call the functions in mcmcse, users can specify the argument mcmc.samples = TRUE in cace.study(), cace.meta.c(), and cace.meta.ic(), so the MCMC posterior samples of monitored parameters are saved in the output objects.\nThe current version of BayesCACE only applies to binary outcomes.\nHowever, the Bayesian hierarchical model can be extended to handle ordinal outcomes \\(o \\in \\{1, \\dots, O\\}\\).\nBy selecting weighting scores \\(\\{W_1, W_2, \\dots, W_O\\}\\) to reflect distances between outcome categories \\(\\{1, \\dots, O\\}\\), \\(\\theta^\\text{CACE}_i\\) is defined as \\(E(Y^1_{ij}-Y^0_{ij}|C_{ij}=1)=\\sum_o{(W_o\\times u_{io})}-\\sum_o{(W_o\\times v_{io})}\\) (Zhou et al. 2019, 2021a). Equally spaced scores \\(\\{1,2,...,O\\}\\), their linear transforms, and midranks are reasonable weight choices (Agresti 2013).\nFuture work will add CACE meta-analysis functions for ordinal outcomes, and allow users to choose their preferred weights \\(\\{W_1, W_2, \\dots, W_O\\}\\).\nNote that ordinal outcomes lead to more complex correlation structures in the parameters related to response rates, so multivariate prior distributions are necessary to analyze such outcomes.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-038.zip\nCRAN packages used\nnoncomplyR, eefAnalytics, BayesCACE, metafor, rjags, coda, forestplot, mcmcse\nCRAN Task Views implied by cited packages\nBayesian, CausalInference, ClinicalTrials, Cluster, GraphicalModels, MetaAnalysis, MixedModels, Phylogenetics\n\n\nA. Agresti. Categorical data analysis. Third Edition Hoboken, NJ: John Wiley & Sons, 2013.\n\n\nM. Baccini, A. Mattei and F. Mealli. Bayesian inference for causal mechanisms with application to a randomized study for postoperative pain control. Biostatistics, 18(4): 605–617, 2017.\n\n\nS. G. Baker. CACE and meta-analysis (letter to the editor). Biometrics, 76(4): 1383–1384, 2020.\n\n\nM. Bannister-Tyrrell, B. Miladinovic, C. L. Roberts and J. B. Ford. Adjustment for compliance behavior in trials of epidural analgesia in labor using instrumental variable meta-analysis. Journal of Clinical Epidemiology, 68(5): 525–533, 2015.\n\n\nS. P. Brooks and A. Gelman. General methods for monitoring convergence of iterative simulations. Journal of Computational and Graphical Statistics, 7(4): 434–455, 1998.\n\n\nC. J. Clopper and E. S. Pearson. The use of confidence or fiducial limits illustrated in the case of the binomial. Biometrika, 26(4): 404–413, 1934.\n\n\nS. Coggeshall. : Bayesian analysis of randomized experiments with non-compliance. 2017. URL https://CRAN.R-project.org/package=noncomplyR. R package version 1.0.\n\n\nM. K. Cowles and B. P. Carlin. Markov chain monte carlo convergence diagnostics: A comparative review. Journal of the American Statistical Association, 91(434): 883–904, 1996.\n\n\nJ. M. Flegal, J. Hughes, D. Vats and N. Dai. : Monte carlo standard errors for MCMC. 2017. URL https://CRAN.R-project.org/package=mcmcse. R package version 1.3-2.\n\n\nC. E. Frangakis and D. B. Rubin. Principal stratification in causal inference. Biometrics, 58(1): 21–29, 2002.\n\n\nL. S. Freedman. The effect of partial noncompliance on the power of a clinical trial. Controlled Clinical Trials, 11(3): 157–168, 1990.\n\n\nA. Gelman and D. B. Rubin. Inference from iterative simulation using multiple sequences. Statistical Science, 7(4): 457–472, 1992.\n\n\nP. B. Gilbert, E. E. Gabriel, Y. Huang and I. S. Chan. Surrogate endpoint evaluation: Principal stratification criteria and the prentice definition. Journal of causal inference, 3(2): 157–175, 2015.\n\n\nM. Gordon and T. Lumley. : Advanced forest plot using “grid” graphics. 2017. URL https://CRAN.R-project.org/package=forestplot. R package version 1.7.2.\n\n\nD. A. Harville. Maximum likelihood approaches to variance component estimation and to related problems. Journal of the American Statistical Association, 72(358): 320–338, 1977.\n\n\nL. V. Hedges and I. Olkin. Statistical methods for meta-analysis. Orlando, FL: Academic Press, 1985.\n\n\nL. V. Hedges and J. L. Vevea. Fixed- and random-effects models in meta-analysis. Psychological Methods, 3(4): 486–504, 1998.\n\n\nM. G. Hudgens and M. E. Halloran. Causal vaccine effects on binary postinfection outcomes. Journal of the American Statistical Association, 101(473): 51–64, 2006.\n\n\nY. Jiang and D. Small. : Instrumental variable estimation. 2014. URL https://CRAN.R-project.org/package=ivpack. R package version 1.2.\n\n\nA. Kasim, Z. Xiao, S. Higgings and E. De Troyer. : Analysing education trials. 2017. URL https://CRAN.R-project.org/package=eefAnalytics. R package version 1.0.6.\n\n\nR. E. Kass, B. P. Carlin, A. Gelman and R. M. Neal. Markov chain monte carlo in practice: A roundtable discussion. The American Statistician, 52(2): 93–100, 1998.\n\n\nN. M. Laird and F. Mosteller. Some statistical methods for combining experimental results. International Journal of Technology Assessment in Health Care, 6(1): 5–30, 1990.\n\n\nS. Lewis and M. Clarke. Forest plots: Trying to see the wood and the trees. BMJ, 322(7300): 1479–1480, 2001.\n\n\nD. Y. Lin and D. Zeng. On the relative efficiency of using summary statistics versus individual-level data in meta-analysis. Biometrika, 97(2): 321–332, 2010.\n\n\nD. Lunn, C. Jackson, N. Best, D. Spiegelhalter and A. Thomas. The BUGS book: A practical introduction to bayesian analysis. New York, NY: Chapman; Hall/CRC, 2012.\n\n\nM. Plummer. : Bayesian graphical models using MCMC. 2018. URL https://CRAN.R-project.org/package=rjags. R package version 4-8.\n\n\nM. Plummer, N. Best, K. Cowles and K. Vines. CODA: Convergence diagnosis and output analysis for MCMC. R News, 6(1): 7–11, 2006.\n\n\nC. Robert and G. Casella. Monte carlo statistical methods. New York, NY: Springer Science & Business Media, 2004.\n\n\nD. J. Spiegelhalter, N. G. Best, B. P. Carlin and A. Van Der Linde. Bayesian measures of model complexity and fit. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64(4): 583–639, 2002.\n\n\nW. Viechtbauer. Conducting meta-analyses in r with the package. Journal of Statistical Software, 36(3): 1–48, 2010.\n\n\nH. White. Instrumental variables regression with independent observations. Econometrica, 50(2): 483–499, 1982.\n\n\nS. L. Zeger, K.-Y. Liang and P. S. Albert. Models for longitudinal data: A generalized estimating equation approach. Biometrics, 44(4): 1049–1060, 1988.\n\n\nJ. Zhou, H. Chu, M. G. Hudgens and M. E. Halloran. A bayesian approach to estimating causal vaccine effects on binary post-infection outcomes. Statistics in medicine, 35(1): 53–64, 2016.\n\n\nJ. Zhou, J. S. Hodges and H. Chu. A bayesian hierarchical CACE model accounting for incomplete noncompliance with application to a meta-analysis of epidural analgesia on cesarean section. Journal of the American Statistical Association, 116(536): 1700–1712, 2021a.\n\n\nJ. Zhou, J. S. Hodges and H. Chu. Rejoinder to ‘CACE and meta-analysis (letter to the editor)’ by stuart baker. Biometrics, 76(4): 1385, 2020.\n\n\nJ. Zhou, J. S. Hodges, M. F. K. Suri and H. Chu. A bayesian hierarchical model estimating CACE in meta-analysis of randomized clinical trials with noncompliance. Biometrics, 75(3): 978–987, 2019.\n\n\nT. Zhou, J. Zhou, J. S. Hodges, L. Lin, Y. Chen, S. R. Cole and H. Chu. Estimating the complier average causal effect in a meta-analysis of randomized clinical trials with binary outcomes accounting for noncompliance: A generalized linear latent and mixed model approach. American Journal of Epidemiology, 191(1): 220–229, 2021b.\n\n\n\n\n",
    "preview": "articles/RJ-2023-038/distill-preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 7846,
    "preview_height": 3851
  },
  {
    "path": "articles/RJ-2023-012/",
    "title": "robslopes: Efficient Computation of the (Repeated) Median Slope",
    "description": "Modern use of slope estimation often involves the (repeated) estimation of a large number of slopes on a large number of data points. Some of the most popular non-parametric and robust alternatives to the least squares estimator are the Theil-Sen and Siegel's repeated median slope estimators. The [robslopes](https://CRAN.R-project.org/package=robslopes) package contains fast algorithms for these slope estimators. The implemented randomized algorithms run in $\\mathcal{O}(n\\log(n))$ and $\\mathcal{O}(n\\log^2(n))$ expected time respectively and use $\\mathcal{O}(n)$ space. They achieve speedups up to a factor $10^3$ compared with existing implementations for common sample sizes, as illustrated in a benchmark study, and they allow for the possibility of estimating the slopes on samples of size $10^5$ and larger thanks to the limited space usage. Finally, the original algorithms are adjusted in order to properly handle duplicate values in the data set.",
    "author": [
      {
        "name": "Jakob Raymaekers",
        "url": {}
      }
    ],
    "date": "2023-03-09",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-012.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:39+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-013/",
    "title": "Generalized Mosaic Plots in the ggplot2 Framework",
    "description": "Graphical methods for categorical variables are not well developed when compared with visualizations for numeric data. One method available for  multidimensional categorical data visualizations is mosaic plots. Mosaic plots are an easy and powerful option for identifying relationships between multiple categorical variables. Although various packages have implemented mosaic plots, no implementation within the grammar of graphics supports mosaic plots. We present a new implementation of mosaic plots in R, ggmosaic, that implements a custom ggplot2 geom designed for generalized mosaic plots. Equipped with the functionality and flexibility of ggplot2, ggmosaic introduces new features not previously available for mosaic plots, including a novel method of incorporating a rendering of the underlying density via jittering. This paper provides an overview of the implementation and examples that highlight the versatility and ease of use of ggmosaic while demonstrating the practicality of mosaic plots.",
    "author": [
      {
        "name": "Haley Jeppson",
        "url": {}
      },
      {
        "name": "Heike Hofmann",
        "url": {}
      }
    ],
    "date": "2023-03-09",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n1 Introduction\nGraphical methods for categorical variables are not as thoroughly developed when compared with what is available for numeric variables. Categorical variables in scientific publications often appear in the form of bar charts (first published in the Commercial and Political Atlas by William Playfair in 1789, re-edited by Playfair et al. (2005)) and spine plots (Hummel 1996).\nBeyond those visualizations, categorical variables with a low number of levels are often incorporated in visualizations in the form of aesthetics such as color and shape. However, these indirect methods of visualizing categorical information are better suited as supplemental information and not as the primary source of information due to the associated loss of accuracy in retrieving the corresponding information (Cleveland and McGill 1984).\nMosaic plots provide a direct method of visualizing multidimensional categorical data; they are similar to a stacked bar chart but include the marginal distribution in addition to the conditional distribution, can be extended beyond two variables, and provide additional flexibility. Modern mosaic plots are usually attributed to Hartigan and Kleiner (Hartigan and Kleiner 1981, 1984; Kleiner and Hartigan 1981), but historical versions of mosaic plots can be found as far back as the late 1800s. The first mosaic plot is often attributed to Georg von Mayr (Friendly 2002); however, the mosaic plots in the Statistical Atlas of the 1870 Decennial Census (United States Census office. 9th census, 1870 and Walker 1874) pre-dates Mayr’s by a few years. Mosaic plots may also be identified as Marimekko charts (mainly in the InfoVis world) or Eikosograms (Oldford et al. 2018).\nIn R, mosaic plots have been implemented in a variety of packages. Base R is equipped with the function mosaicplot() from the graphics package based on code by Emerson (1998) adapted by Kurt Hornik. The base R generic function plot() contains a method for table objects and produces a mosaic plot from a contingency table using the mosaicplot() function. Similarly, the yardstick package (Kuhn et al. 2022) provides a method for the ggplot2 (Wickham et al. 2020) function autoplot() to visualize confusion matrices, with one of the options being a mosaic plot.\nThe vcd package (Meyer et al. 2020), influenced by Michael Friendly’s “Visualizing Categorical Data,” provides the functions mosaic() and strucplot() which allow for expanded versions of the original mosaic plot (Hartigan and Kleiner 1981). The eikosogram package (Oldford et al. 2018) is another R package capable of producing mosaic plots. The function tileplot() in the latticeExtra package (Sarkar and Andrews 2016) provides functionality to create mosaic plots in the lattice framework (Sarkar 2020).\nThe productplots package (Wickham and Hofmann 2016) provides a wrapper of ggplot2 functionality but does not provide universal access to all aspects of the ggplot2 framework like a geom does. The left plot in Figure 1 is an example of a mosaic plot created with the productplots package. While the default plot can be annotated with additional aesthetics, labels, themes, and color schemes, we can not add layers to this plot, and faceting is unavailable. A preview of the relevant code follows the next paragraph.\nWe present a new implementation of mosaic plots in R, ggmosaic, that implements a ggplot2 geom for mosaic plots. While several other packages are available for constructing mosaic plots, ggmosaic allows users to fully leverage the widely used ggplot2 framework resulting in a flexible package for generalized mosaic plots. More noteworthy, ggmosaic introduces several new features not previously available for mosaic plots, including a novel method of incorporating a rendering of the underlying density via jittering. ggmosaic includes a Shiny app to preview plots interactively and allow for model exploration. Many of the features we describe are possible because ggmosaic is implemented within the architecture defined by ggplot2. A preview of the syntax used in ggmosaic is shown below compared to that of the productplots package. The resulting plot is the right plot of Figure 1.\n\n\n# productplots\nproductplots::prodplot(flights, ~do_you_recline + rude_to_recline, mosaic()) + \n  aes(fill = do_you_recline)\n\n## ggmosaic\nggplot(data = flights) +\n   geom_mosaic(aes(x = product(do_you_recline, rude_to_recline),\n                   fill = do_you_recline))\n\n\n\n\n\nFigure 1: Example mosaic plots made with the productplots package (left) and the ggmosaic package (right). While the differences are subtle in this basic example, the differences between the two packages become more apparent as the customization of the mosaic plot increases.\n\n\n\nIn this paper, we motivate the ggmosaic implementation and introduce it with examples demonstrating the versatility and ease of use of ggmosaic. Next, we describe new features available in the latest release of ggmosaic, version 0.3.3, and discuss how these features can enhance the data visualizations made with ggmosaic. Finally, we conclude with a preview of a shiny application (Chang et al. 2021) designed for an exploratory model framework of logistic regression and loglinear models. The Shiny application is included in the development version of ggmosaic available from .\n2 ggmosaic: A ggplot2 implementation of mosaic plots\nggplot2 implements an adaptation of the grammar of graphics (Wilkinson 1999), a layered grammar (Wickham 2010). Because of its flexibility and ease of use, ggplot2 has become one of the most popular plotting packages available in the R ecosystem. Version 2.0.0 of ggplot2 introduced a method for other R packages to implement custom geometries, or “geoms”, allowing for an expansion of the utility of ggplot2. In turn, the increasingly popular package has continued to appeal to a more extensive user base.\nIn creating a ggplot2 geom for mosaic plots, we seek to appeal to this user base and leverage the robust framework of the grammar of graphics. Having mosaic plots in the ggplot2 framework makes creating mosaic plots more accessible as it is more straightforward for the novice user to create mosaic plots for data exploration purposes. For many users, having mosaic plots function as a true ggplot2 geom reduces the amount of syntax required, eliminates the need for prior calculations, and provides complete access to additional benefits of ggplot2, allowing for highly customized generalized mosaic plots.\nWith the R package ggmosaic, a custom ggplot2 geom designed for generalized mosaic plots is implemented. ggmosaic makes mosaic plots compatible with ggplot2 and creates a flexible method to generate a wide variety of categorical data visualizations. The remainder of the paper presents a thorough description of the ggmosaic package featuring examples that go beyond how to use ggmosaic and demonstrate how to make more informed decisions about how to use mosaic plots to answer various questions.\nWe begin with an introduction to the package with examples of the flexible framework that ggmosaic offers. The following three sections illustrate how to use ggmosaic. First, we address the data structures required for constructing mosaic plots and which structures are compatible with ggmosaic. Next, we demonstrate how ggmosaic fits mosaic plots into the ggplot2 framework and how to define the aesthetic mappings to construct the desired plot from the variables in the data. Lastly, we provide examples of mosaic plots customized with parameters unique to ggmosaic.\nThe remaining half of the paper presents the new features included in version 0.3.3 of ggmosaic. Frist, we introduce geom_mosaic_text(), designed to place text, or labels, in the center of each tile, followed by geom_mosaic_jitter(), a geom for jittered points designed to be superimposed on the mosaic plot and with multiple proposed applications. Next, we present theme_mosaic(), a theme for mosaic plots to remove items from the background of the plot. We conclude with an overview of an interactive Shiny app for exploratory data analysis (EDA) of high-dimensional categorical data using mosaic plots.\nMany of the features we describe are possible because ggmosaic is implemented within the ggplot2 framework. While other packages are available for constructing mosaic plots, ggmosaic allows users to leverage the widely used ggplot2 infrastructure to create highly customized generalized mosaic plots. Most notably, ggmosaic introduces several new features not previously available for mosaic plots. In total, ggmosaic offers users a way to make mosaic plots with ggplot2 to visualize and explore categorical data more effectively.\n3 The ggmosaic package\nThe R package ggmosaic implements a custom ggplot2 geom, geom_mosaic(), designed to offer a flexible framework for visualizing categorical data. The geom for mosaic plots was created using the productplots package and ggplot2’s custom object-oriented system and extension mechanism, ggproto. A stable version of ggmosaic is available on CRAN, and a development version is available at https://github.com/hjeppson/ggmosaic. This section provides a brief description of the package, the methods used in its implementation, and an example that showcases the flexibility that ggmosaic offers.\n\nDesigned to create visualizations of categorical data, geom_mosaic() has the capability to produce bar charts, stacked bar charts, mosaic plots, and double-decker plots and therefore offers a wide range of potential plots. Figures 2, 3, and 4 highlight the package’s versatility. The code for these examples is provided below and can be summarized as consisting of the following components. First, the mosaic plot is created by adding a mosaic geom. The x aesthetic takes one or more variables wrapped in the product() function, and the fill aesthetic determines the fill color of the rectangles. An optional divider parameter determines the partitioning strategy for the rectangles. The code will be explained in greater detail in the subsequent sections.\n\n\n# one-dimensional examples:\n# spine plot\nggplot(data = flights) + \n  geom_mosaic(aes(x = product(do_you_recline), \n                  fill = do_you_recline))\n\n# bar chart\nggplot(data = flights) + \n  geom_mosaic(aes(x = product(do_you_recline),\n                  fill = do_you_recline), \n              divider = \"hbar\") \n\n\n\n\n\nFigure 2: Both plots represent the distribution of and allow the same comparisons, though the spine plot does so with proportions and the bar chart with frequencies. The relative group sizes are more difficult to compare in the spine plot, but the group is still discernable as the largest and the group as the smallest. The bar chart provides an easier comparisons of the relative group sizes.\n\n\n\n\n\n# two-dimensional examples:\n# mosaic plot (2 variables)\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(do_you_recline, rude_to_recline), \n                  fill = do_you_recline))\n\n# stacked bar chart\nggplot(data = flights) + \n  geom_mosaic(aes(x = product(do_you_recline, rude_to_recline), \n                  fill = do_you_recline),\n              divider = c(\"vspine\", \"hbar\"))\n\n\n\n\n\nFigure 3: The conditional distribution of given is represented by the heights of the bars in both the mosaic plot (left) and the stacked bar chart (right). The mosaic plot displays the joint and marginal distribution of with the area and width of the tile, respectively. The stacked bar chart reveals a nearly equal number of never reclining responses across the categories of . The conditional probabilities of given and given have a more significant impact in the mosaic plot, from which we understand that there is a positive correlation between a respondent considering reclining to be rude and electing never to recline their seat. There are, however, respondents that do not consider reclining to be rude and yet never recline, and perhaps more egregious, there are those that do regard reclining as rude and yet usually recline their seats, a group that is important in the mosaic plot but challenging to see in the stacked bar chart.\n\n\n\n\n\n# three-dimensional examples:\n# mosaic plot (3 variables)\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(eliminate_reclining, do_you_recline,\n                              rude_to_recline),\n                  fill = do_you_recline, \n                  alpha = eliminate_reclining))\n\n# double-decker plot\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(do_you_recline, eliminate_reclining,\n                              rude_to_recline), \n                  fill = do_you_recline, \n                  alpha = eliminate_reclining),\n              divider = ddecker())\n\n\n\n\n\nFigure 4: The mosaic plot (left) and double-decker plot (right) include the three variables (, , ), though the direction of the splits varies between the two plots. Both plots display a connection between a passenger’s opinions of reclining and their tendency to recline. Those that consider reclining to be rude are less likely to recline and more likely to wish for reclining to be eliminated. The mosaic plot reveals an increasing desire to eliminate reclining when the respondents believe reclining to be rude across all categories of . There appears to be a slight positive correlation between reclining and not electing to eliminate reclining across all levels of . The double-decker plot (a mosaic plot with a specific structure) best highlights the conditional distribution of and allows for easier comparisons across the categories of the other two variables. For example, the relationship between and seems consistent across categories of , a conclusion that is less readily available from the mosaic plot.\n\n\n\nFurthermore, ggmosaic allows various features to be customized:\nthe type of data structure,\nthe order of the variables (Figure 6),\nthe formula setup of the plot (Figure 7),\nfaceting (Figure 7),\nthe type of partition (Figure 10 and Figure 11), and\nthe space between the categories (Figure 12).\nThe following sections will discuss these features and provide examples of their use.\nData structure\nThe first step to creating a mosaic plot with ggmosaic is to ensure the data are in a supported format. There are three data structures to consider (Friendly 2016), and two of the three data structures are compatible with ggmosaic. The structure of the data impacts the use of geom_mosaic(), so the user must understand their data’s structure. This section provides an example of each of the three data structures and highlights the essential differences.\nThe data set used for these examples, and all examples in this paper, is from a SurveyMonkey Audience poll conducted by FiveThirtyEight for two days in August of 2014. The survey asked twenty-six questions ranging from background information regarding the respondent to feelings on potentially aggravating behavior one might encounter on an airplane (Hickey 2014). The survey had 1,040 respondents (874 of whom had flown) aged 18-60+ from across the United States. The data set is available from FiveThirtyEight’s data GitHub repository, and a cleaned version of it is one of the three data sets included in the ggmosaic package.\nHere, we will use three variables do_you_recline, rude_to_recline, and eliminate_reclining from the fly data. For this paper, we made the following adjustments: remove all non-responses and collapse the variable do_you_recline from a factor with five levels (always, usually, about half the time, once in a while, and never) to a factor with three levels (usually, sometimes, never). Incidentally, by removing all non-responses, we also remove responses from those that have never flown. The code below completes the necessary modifications to the data.\n\n\n# A few modifications to data\nflights <- fly %>% \n  select(do_you_recline, rude_to_recline, eliminate_reclining) %>% \n  filter(!is.na(do_you_recline), !is.na(rude_to_recline)) %>% \n  mutate(do_you_recline = do_you_recline %>% \n           forcats::fct_collapse(\n             usually = c(\"always\", \"usually\"),\n             sometimes = c(\"about half the time\", \"once in a while\"),\n             never = \"never\") %>% \n           forcats::fct_relevel(\"never\", \"sometimes\", \"usually\")\n         )\n\n# Summary of the modified data\nflights %>% summary()\n\n   do_you_recline rude_to_recline eliminate_reclining\n never    :170    no      :502    no :595            \n sometimes:373    somewhat:281    yes:259            \n usually  :311    yes     : 71                       \n\nTo simplify the examples of the three data structures, we will focus on the two variables do_you_recline and rude_to_recline from the flights data. The following code creates the desired subset:\n\n\nflights_examp <- flights %>% select(do_you_recline, rude_to_recline)\nnames(flights_examp)\n\n[1] \"do_you_recline\"  \"rude_to_recline\"\n\nThe example data, flights_examp, contains the individual observations from the survey. Each row accounts for one survey response to each of the questions, and there are two columns, one for do_you_recline and one for rude_to_recline. This is the first of the three types of data structure.\n\n\nglimpse(flights_examp)\n\nRows: 854\nColumns: 2\n$ do_you_recline  <fct> sometimes, usually, usually, sometimes, usua…\n$ rude_to_recline <fct> somewhat, no, no, no, no, somewhat, no, no, …\n\nThe second data structure is a summary of the first data structure. This format consists of a data frame where each row is one of the possible combinations of levels of the categorical variables. In this structure, there is an additional column for the variable freq that supplies the number of observations in the first data structure with the row’s particular combination of levels.\n\n\nflights_examp %>% \n  count(do_you_recline, rude_to_recline, name = \"freq\") %>% \n  glimpse()\n\nRows: 9\nColumns: 3\n$ do_you_recline  <fct> never, never, never, sometimes, sometimes, s…\n$ rude_to_recline <fct> no, somewhat, yes, no, somewhat, yes, no, so…\n$ freq            <int> 35, 81, 54, 198, 164, 11, 269, 36, 6\n\nThe second data structure also occurs with weighted data. In which case, rather than a variable representing the counts or frequencies, a variable represents the weights. A typical example of weighted data is census data, where a weighting variable is used to compensate for a representation differential.\nThe final data structure summarizes the second data structure: it is the contingency table format. This data structure is not supported by ggmosaic and needs to be transformed into data structure one or two. Both as.data.frame() and as_tibble() provide a method for tables that converts this format into the summary format of data structure two.\n\n\nflights_examp %>% table()\n\n              rude_to_recline\ndo_you_recline  no somewhat yes\n     never      35       81  54\n     sometimes 198      164  11\n     usually   269       36   6\n\nWhile the contingency table data structure is incompatible with ggmosaic, or ggplot2, either the first or second structure is acceptable, and neither is preferable to the other. The data structure will affect the set of mappings constructed from the variables to the aesthetics, the topic of the next section.\nAesthetics\nTo fit ggmosaic within the ggplot2 infrastructure, we must create the desired plot from the variables in the data. The first step, defining the aesthetic mappings, requires specifications of how variables in the data will be mapped to the visual properties of the plot. In ggmosaic, the aesthetic mappings are transformed into a model formula using the R formula notation with the ~ operator. The formula then determines what is represented in the mosaic plot, or how the joint distribution of the variables is broken down into the marginal distribution and conditional distribution(s). Understanding how the aesthetics translate into the model formula helps a user specify the correct aesthetics for the desired plot. This section describes each of the aesthetics used in geom_mosaic(), some of which are unique to ggmosaic, describes how the aesthetics are translated into the model formula, and provides examples of the impact changes in the aesthetic mappings have on the final plot.\nConflicting with the infrastructure provided by ggplot2, mosaic plots do not have a one-to-one mapping between a variable and the x or y axis. Instead, the grammar of graphics defines the coordinate system of mosaic plots as a system based on recursive partitioning that can integrate several variables (Wilkinson 1999). To accommodate the variable number of variables, the mapping to x is created by the product() function. For example, the variables var1 and var2 are read in as x = product(var1, var2). However, if only one variable is to be included, it does not need to be wrapped in product(), and can be read in simply as x = var1. The product() function alludes to ggmosaic’s predecessor productplots and to the joint distribution as the product of the conditional and marginal distributions. The product function creates a special type of list that is evaluated internally and is what allows us to integrate multiple variables into the mosaic plot.\nIn order to include a variable in the mosaic plot, it must be specified as an aesthetic. In geom_mosaic(), the following aesthetics can be specified:\nx: select variables to add to the formula\ndeclared as x = product(var1, var2, ...)\n\nalpha: add an alpha transparency to the rectangles of the selected variable\nunless the variable is already part of the formula, it is added explicitly in the formula in the first position.\n\nfill: select a variable to determine the fill color of the rectangles\nunless the variable is already part of the formula, it is added explicitly in the formula in the first position after the optional alpha variable.\n\nconds: select a variable to condition on\ndeclared as conds = product(cond1, cond2, ...)\n\nweight: select a weighting variable\nThe specified aesthetics are then translated into the formula, weight ~ alpha + fill + x | conds, which determines what is to be represented in the mosaic plot and how to calculate the size of the corresponding tiles and determine their placement. Understanding the ordering of the specified aesthetics in the translation helps a user specify the correct aesthetics for the desired plot.\nThe minimal required aesthetics to create a mosaic plot is one variable mapped to the x aesthetic, wrapped in the product() function. Without a defined weight aesthetic, all observations are assumed to have equal weights, a weight of 1. In this minimal scenario, the resulting formula is 1 ~ x.\nThe weight aesthetic\nA mosaic plot is constructed such that the area of each rectangle is proportional to the number of observations that the tile represents. Without weighting, each row of a data set represents one observation. Alternatively, the aesthetic weight is available to modify the interpretation of each row of the data. For example, the weight aesthetic will need to be used with data that contains a variable that supplies the number of observations for each row’s particular combination of levels. In this case, if the weight aesthetic is left unused, the resulting mosaic plot will resemble the case of equal probabilities, as can be seen in Figure 5.\n\n\n# unweighted plot\nflights_examp %>%\n  count(do_you_recline, rude_to_recline, name = \"freq\") %>%\n  ggplot() +\n  geom_mosaic(aes(x = product(rude_to_recline), \n                  fill = do_you_recline))\n\n# weighted plot\nflights_examp %>%\n  count(do_you_recline, rude_to_recline, name = \"freq\") %>%\n  ggplot() +\n  geom_mosaic(aes(weight = freq, \n                  x = product(rude_to_recline), \n                  fill = do_you_recline))\n\n\n\n\n\nFigure 5: When using summarized data, a weighting aesthetic must be defined for the resulting mosaic plot to remain a faithful representation of the data. The mosaic plot on the left contains nine equally sized tiles for the nine combinations that result from the three levels of crossed with the three levels of . The mosaic plot on the right considers the frequency of those nine combinations resulting in nine tiles proportional to the number of observations each tile represents.\n\n\n\nThe weight aesthetic is also necessary with weighted data, such as weighted surveys. Otherwise, the resulting mosaic plot may not be a faithful representation of the relationship in the data. Here, weights serve to correct imbalances between the sample and the population.\nThe ordering of the variables\nBecause of a mosaic plot’s hierarchical construction, the ordering of the variables in the formula is vital. While any order of the variables ultimately represents the same data, different variable orders may better support different comparisons. Ideally, the comparison of interest is positioned along a common scale, an easier perceptual task than comparing areas (Cleveland and McGill 1984). In the simple case of two variables, with one an explanatory variable and the other considered the response, the explanatory variable is best placed on the first split of the mosaic and the response variable as the last split, those that occur within each of the tiles created by the first split. The code below and corresponding mosaic plots in Figure 6 illustrate the change that occurs when the variables in the formula are swapped.\n\n\n# original order\nggplot(data = flights) +\ngeom_mosaic(aes(x = product(do_you_recline, rude_to_recline), \n                fill = do_you_recline))\n\n# order reversed\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(rude_to_recline, do_you_recline), \n                  fill = do_you_recline))\n\n\n\n\n\nFigure 6: While the mosaics represent the same data, each supports different comparisons. The mosaic plot on the left emphasizes the effect has on . The variable order was reversed in the construction of the mosaic plot on the right resulting in a plot that emphasizes the effect has on .\n\n\n\nThe mosaic plots in Figure 6 represent the same joint distribution but are composed differently. The original order, the plot on the left, shows that most of those who believe it is rude to recline never recline their own seat. With the reversed order, we can see that of those who never recline their seats, a roughly equal proportion consider reclining rude as those that do not. The difference highlights how mosaic plots are more than a visualization of the joint distribution; mosaic plots are a visualization of the product of the conditional distribution and marginal distribution, which results in the joint distribution.\nIn the original order, x = product(do_you_recline, rude_to_recline), the joint distribution is the product of the conditional distribution of do_you_recline given rude_to_recline and the marginal distribution of rude_to_recline. With this order, we can address questions regarding the marginal distribution of rude_to_recline, such as “what proportion of respondents believe it is rude to recline?”, and questions regarding the conditional distribution of do_you_recline given each response to rude_to_recline, such as “Is there an association between a passenger’s reclining tendencies and their opinion reclining.\nIn contrast, x = product(rude_to_recline, do_you_recline), while resulting in the same joint distribution, is the product of the conditional distribution of rude_to_recline given do_you_recline and the marginal distribution of do_you_recline. Here, we can compare the responses to rude_to_recline within each response to do_you_recline, which reveals an interesting relationship between a passenger’s perceived rudeness given their own inclination to recline. We can see that those that usually recline their seats are more likely to not find reclining to be a rude behavior. Those that never recline their seats, however, are evenly divided with their opinions on reclining. While it is easier to estimate the proportion that never reclines in the reversed order, it is more challenging to decipher what proportion of the respondents believe it is rude to recline, an artifact of the plot representing rude_to_recline only through its conditional distribution given the responses to do_you_recline.\nUltimately, the preferred order of the variables depends on the comparisons of interest. To (re)arrange the categories within a variable, the levels of the factor variable must be reordered before using ggmosaic.\nThe conds aesthetic\nThe formula setup of the plot can be further altered by electing to view a conditional distribution rather than the full joint distribution. Conditioning can be used to better focus on the comparison of interest either by removing relationships that are not of interest or positioning the comparison of interest along a common scale (Cleveland and McGill 1984; Wickham and Hofmann 2011). When a variable is mapped to the conds aesthetic, the mosaic plot is no longer represents the joint distribution, but instead maps the conditional distribution.\nFaceting splits the data into subsets and generates a plot for each subset. Faceting therefore provides another method to accomplish conditioning. The result is the same as with the conds aesthetic, but the formatting is altered. The formatting difference will be discussed in more detail later. Figure 7 contains an example of each method of conditioning and is based on the code below.\n\n\n# not conditioned\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(rude_to_recline), \n                  fill = do_you_recline))\n\n# conditioned with aesthetic mapping\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(do_you_recline), \n                  fill = do_you_recline, \n                  conds = product(rude_to_recline)))\n\n# conditioned with facets\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(do_you_recline), \n                  fill = do_you_recline)) +\n  facet_grid(cols = vars(rude_to_recline))\n\n\n\n\n\nFigure 7: Mosaic plots typically depict the joint distribution (top left mosaic plot) but can instead depict the conditional distribution using facetting (bottom left mosaic plot) or the conds aesthetic (top right mosaic plot). Conditioning on may provide a clearer view of the relationship the responses to has with . However, we lose information on the marginal distribution of , and the area of a tile is only comparable to the other tiles within that same level of .\n\n\n\nFor two variables, as in Figure 7, the difference between conditioning and not conditioning amounts to the difference between a mosaic plot and a stacked bar chart with a standardized height of 1. That is, when conditioning on rude_to_recline, the plot no longer represents the marginal distribution of rude_to_recline. Instead, as in a stacked bar chart, the plot only represents the conditional distribution of do_you_recline given the responses to rude_to_recline.\nIn the case of three variables, relationships can be removed via conditioning to focus on one relationship in particular and position that relationship along a common scale (Wickham and Hofmann 2011). Figure 8 provides an example of conditioning used to focus on the conditional distribution of eliminate_reclining by removing the joint distribution of do_you_recline and rude_to_recline. The example is based on the code below.\n\n\n# not conditioned\nggplot(flights) +\n  geom_mosaic(aes(x = product(eliminate_reclining, do_you_recline,\n                              rude_to_recline), \n                  fill = do_you_recline, \n                  alpha = eliminate_reclining))\n\n# conditioned \nggplot(data = flights) +\n  geom_mosaic(aes(x = product(eliminate_reclining), \n                  conds = product(do_you_recline, rude_to_recline), \n                  fill = do_you_recline, \n                  alpha = eliminate_reclining))\n\n\n\n\n\nFigure 8: An example of using conditioning to focus on one relationship. The left mosaic plot represents the joint distribution of reclining tendencies, the perceived rudeness of reclining, and the desire to eliminate reclining, providing an overall picture. In the mosaic plot on the right, conditioning on and removes the representation of the joint distribution of reclining tendencies and the perceived rudeness of reclining but provides a clearer view of the conditional distribution of .\n\n\n\nThe conditioning completed in Figure 8 provides a better view of how responses to eliminate_reclining relate to do_you_recline and rude_to_recline responses. We can see that desires to eliminate reclining are less likely when the respondent reclines their seat. eliminate_reclining may have a stronger association with responses to rude_to_recline, but that comparison is more difficult to make since it is comparing areas rather than positions along a common scale.\nWhen conditioning on multiple variables, it is again important to consider how the variables’ order impacts the plot. The two mosaic plots in Figure 9 display the same data, but the direction of the final split is different. The orientation of the final split determines which comparisons are positioned along a common scale.\n\n\n# conditioned order 1\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(eliminate_reclining), \n                  conds = product(do_you_recline, rude_to_recline), \n                  fill = do_you_recline, \n                  alpha = eliminate_reclining))\n\n# conditioned order 2\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(eliminate_reclining), \n                  conds = product(rude_to_recline, do_you_recline), \n                  fill = do_you_recline, \n                  alpha = eliminate_reclining)) + \n  coord_flip()\n\n\n\n\n\nFigure 9: The two mosaic plots both portray the distribution of conditioned on and , but the direction of the final split is different. The lefthand plot is better suited to compare with , while the righthand plot is better suited to compare with . The comparison of interest should influence the order of the conditioning variables.\n\n\n\nFigure 9 rearranges the conditioned variables’ order to gauge the strength of the relationship between eliminate_reclining and rude_to_recline. For a fixed level of do_you_recline, the desire to eliminate reclining correlates with an increasingly hostile stance on reclining. While this example flips the orders of the conditioned variables and then uses coord_flip(), ggmosaic provides additional methods of manually modifying the directions of the splits in the mosaic plot, which will be discussed in the next section.\nThe defined set of aesthetic mappings impacts more than the look of the final graphic; it impacts the analysis and the inquiries the plot can support. ggmosaic requires the x and cond aesthetics to be defined using the product() function to accommodate a variable number of variables. The defined set of aesthetic mappings will result in a model formula that will determine which of the potentially numerous ways the mosaic plot will represent the decomposition of the joint distribution.\nParameters\nEasy customization is necessary for mosaic plots to be effective. Additional aspects of the mosaic plot that can be modified include the strategy used to partition the area into the tiles and the width of the spacing between the tiles. ggmosaic provides the parameters divider and offset to facilitate adjustments to the partitioning strategy and the spacing. This section demonstrates how to create generalized mosaic plots modified by these parameters and how they can be used to highlight different aspects of the data.\nIn ggmosaic, the desired type of partition is specified with the divider parameter (by setting divider = \" \"). The area of a mosaic plot can be partitioned into bars or spines, and partitions can be added horizontally or vertically, as shown in Figure 10 and the code below. When the area is partitioned into bars, the height is proportional to value, and the width equally divides the space. Bars can be arranged horizontally (\"hbar\") or vertically (\"vbar\"). Alternatively, space can be partitioned into spines, where the section’s width is proportional to the value, and the height occupies full range. Spines are space-filling and can be arranged horizontally (\"hspine\") or vertically (\"vspine\"). The default divider for a single variable is \"hspine\".\n\n\n# default / hspine\nggplot(data = flights) + \n  geom_mosaic(aes(x = product(do_you_recline), \n                  fill = do_you_recline))\n\n# vspine\nggplot(data = flights) + \n  geom_mosaic(aes(x = product(do_you_recline), \n                  fill = do_you_recline), \n              divider = \"vspine\") \n\n# hbar\nggplot(data = flights) + \n  geom_mosaic(aes(x = product(do_you_recline), \n                  fill = do_you_recline), \n              divider = \"hbar\") \n\n# vbar\nggplot(data = flights) + \n  geom_mosaic(aes(x = product(do_you_recline), \n                  fill = do_you_recline), \n              divider = \"vbar\") \n\n\n\n\n\nFigure 10: Examples of the four ways each dimension in a mosaic plot can be partitioned. While the values are easier to compare with the bars in this one-dimensional example, the advantages of spines become more apparent in higher-dimensional examples.\n\n\n\nIn the case of multiple variables, a type of partition must be defined for each variable. The default divider (divider = mosaic()) and the double-decker divider (divider = ddecker()) automatically select a pre-defined pattern for the partitions. For example, if three variables are plotted, the default, divider = mosaic(), partitions the plot with spines in alternating directions, beginning with a horizontal spine, i.e. divider = c(\"hspine\", \"vspine\", \"hspine\"). Alternatively, we can declare the type of partition for each variable, e.g. divider = c(\"hbar\", \"vspine\", \"vspine\"). The first partition declared in the vector will be used last in the plot. As mentioned above, an unspecified divider leads to the default divider = mosaic(), and the partition begins with a horizontal spine and alternate directions for each subsequent variable. To begin with a vertical spine and alternate directions from there, use divider = mosaic(direction = \"v\"). A preview of these options is available in Figure 11, and the associated code is below.\n\n\n# default / mosaic(\"h\")\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(eliminate_reclining, do_you_recline,\n                              rude_to_recline), \n                  fill = do_you_recline, \n                  alpha = eliminate_reclining))\n\n# mosaic(\"v\")\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(eliminate_reclining, do_you_recline,\n                              rude_to_recline), \n                  fill = do_you_recline, \n                  alpha = eliminate_reclining),\n              divider = mosaic(direction = \"v\"))\n\n# ddecker\nggplot(data = flights) + \n  geom_mosaic(aes(x = product(do_you_recline, eliminate_reclining,\n                              rude_to_recline), \n                  fill = do_you_recline, \n                  alpha = eliminate_reclining),\n              divider = ddecker()) \n\n# c(\"hbar\", \"hbar\", \"vspine\")\nggplot(data = flights) + \n  geom_mosaic(aes(x = product(do_you_recline, rude_to_recline,\n                              eliminate_reclining), \n                  fill = do_you_recline, \n                  alpha = eliminate_reclining),\n              divider = c(\"hbar\", \"hbar\", \"vspine\"))\n\n\n\n\n\nFigure 11: When multiple variables are included, a partition type must be defined for each variable, and the different partitions emphasize different relationships. The and functions automatically define a partition for each variable, and are shown in the first and third mosaic plots, respectively. , shown in the second plot, is similar to , but the coordinates are flipped and inversed. Alternatively, a type of partition can be defined manually for each dimension. emphasizes the conditional distribution of the final variable. In the final plot, the divider creates a plot similar to a faceted bar chart that supports comparisons similar to those supported by the double-decker plot but for relative values instead of relative proportions.\n\n\n\nThe divider parameter provides an additional mechanism for modifying the plot’s arrangement to focus on a particular relationship by enabling a user to modify any dimension of the divider to position the comparison of interest along a common scale. For instance, the double-decker plot in Figure 11 helps compare the effects changes in eliminate_reclining and rude_to_recline have on the responses to do_you_recline and how the relationship between do_you_recline and eliminate_reclining differs for different levels of rude_to_recline.\nggmosaic adopts the procedure followed by Hartigan and Kleiner (1981), Friendly (2002), Theus and Urbanek (2009), and Hofmann (2003), where an amount of space is allocated for each of the splits, with subsequent divisions receiving a smaller amount of space. The splits between the categories of the first variable are the widest and the splits decrease in width with each additional variable. Decreasing the widths of the splits with each additional variable allows the categories to group together according to the recursive strategy (Theus and Urbanek 2009) and the created spaces preserve the impact of small counts (Friendly 2002). The effect becomes apparent when an empty group is included. In this case, the spaces between the empty categories create a gap equal to the amount of space that is between non-empty categories.\nFor variables with many categories, it may be of interest to decrease the size of the spacing between the spines. The parameter offset can widen or shrink the size of the spacing between the categories of the first variable and the subsequent splits then gradually decrease in width with each additional variable. The default setting of this parameter is offset = 0.01, equivalent to 1% of the width of the plotting area. The code below and the corresponding mosaic plots in Figure 12 demonstrate how to use the offset parameter.\n\n\n# increased spacing\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(rude_to_recline), \n                  fill = do_you_recline),\n              offset = 0.03) \n  \n# decreased spacing\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(rude_to_recline), \n                  fill = do_you_recline),\n              offset = 0) \n\n# default spacing\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(rude_to_recline), \n                  fill = do_you_recline)) \n\n\n\n\n\nFigure 12: Three examples of how the spacing of the splits in the mosaic plot can be increased or decreased with the offset parameter.\n\n\n\nWhile the examples so far include up to three variables, there is no technical limit to the number of variables in a mosaic plot. However, when many categorical variables are present, mosaic plots can quickly become cluttered and difficult to interpret, and a judgment call must be made on the practical limit to the number of variables included in the mosaic plot. This practical limit could depend on many factors, including the number of categories within each variable, the viewing size of the result mosaic plot, and the viewer’s familiarity with the data.\nThe customization offered via the ggmosaic parameters allows users to easily make complex generalized mosaic plots that are more readable and simpler to interpret. The parameters provide additional dimensions of freedom with the ability to switch between bars and spines and allow users to highlight different aspects of high-dimensional categorical data to identify and communicate interesting patterns.\nInteractivity\nHaving a geom designed for generalized mosaic plots allows for a ggplotly() hook to create interactive mosaic plots with the plotly package, version 4.9.3 (Sievert et al. 2016). The ggplotly() function translates most of the basic geoms bundled with the ggplot2 package. To expand the functionality to custom geoms, we make use of the infrastructure provided in the plotly package that allows for a translation. In ggplot2, many geoms are special cases of other geoms. For example, geom_line() is equivalent to geom_path() once the data is sorted by the x variable. Because GeomMosaic can be reduced to the lower-level geom GeomRect, we were able to write a method for the to_basic() generic function in plotly (Sievert 2020). Figure 13 features an example of an interactive mosaic plot created with ggplotly() and the corresponding code is below.\n\n\np1 <- ggplot(data = flights) +\n  geom_mosaic(aes(x = product(rude_to_recline), \n                  fill = do_you_recline)) \n\nplotly::ggplotly(p1)\n\n\n\n\n\n\n\n\n\nFigure 13: Users can create interactive mosaic plots using ggmosaic with plotly. With plotly, users gain the ability to hover over a tile to see the names of the combination of levels and the number of observations that the tile represents.\n\n\n\n4 New features in ggmosaic version 0.3.3\nThe layered grammar implemented by ggplot2 provides a means to add additional layers (typically resulting in additional geometric objects) to a graphic. The unique scale system in ggmosaic, however, makes a correct placement of items in additional layers tricky. The x and y scales in a mosaic plot are both numeric and categorical; the numeric scale determines the placement of additional objects on a range between 0 and 1. Thus, to place an object in the center of each tile, it is necessary to know the numeric values for the corners of each of the tiles. Version 0.3.3 of ggmosaic introduced two additional geoms designed to build on stat_mosaic() and geom_mosaic() that bypass these calculations.\nThe two geoms, geom_mosaic_text() and geom_mosaic_jitter(), are provided to further enhance the generalized mosaic plots created with ggmosaic. These features are designed to add labels to the mosaic plot tiles and to view the tiles’ density via jittered points, and are implemented as additional geom items that can layer on top of the original mosaic geom. Lastly, we also provide a new minimal theme, theme_mosaic(), designed for mosaic plots. The theme removes items from the plot background in order to reduce clutter and increase readability. Together, these features add new functionality to mosaic plots to increase their usability and facilitate more profound insights into high-dimensional categorical data.\nA labeling geom\nThe flexibility of generalized mosaic plots can lead to inadequate space around the perimeter of the plot to label each of the categories for the variables displayed, making labeling a challenge. To ease the burden on the axis labels, geom_mosaic_text() applies labels to the tiles. This section introduces geom_mosaic_text(), its parameters, and its customization options with sample code and graphics.\nOne aspect of mosaic plots is that while the text and tick marks on the axes may be aligned with the correct category levels on one side of the plot, the alignments may not be appropriate for category levels on the opposite side of the plot. Thus, it may be advantageous to label the tiles to ensure the group identities are apparent to a viewer. geom_mosaic_text() provides the means to place text, or labels, in each of the tiles. geom_mosaic_text() has its counterpart, stat_mosaic_text(), perform the necessary mapping calculations to place each tile’s label in the center of the tile. Figure 14 features an example created with geom_mosaic() with geom_mosaic_text() added to place text in each of the tiles corresponding to the tiles’ combination of categories. The associated code is below.\n\n\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(do_you_recline, rude_to_recline), \n                  fill = do_you_recline)) +\n  geom_mosaic_text(aes(x = product(do_you_recline, rude_to_recline)))\n\n\n\n\n\nFigure 14: In this example, a geom of labels is layered on top of the mosaic geom to produce a mosaic plot with labels centered in each tile. Directly labeling each tile can alleviate issues caused by inadequate space around the perimeter for labels.\n\n\n\nAs a default, the label text contains the combinations of category levels the tile represents. Alternative labels may be desired and are achievable through the label aesthetic. For example, to label the tiles of a mosaic with counts, that variable can be mapped to the label aesthetic. The counts, however, need not be contained in the underlying data before plotting; the function after_stat() supports aesthetic mappings of variables calculated by the stat, in this case, the variable .wt calculated in stat_mosaic_label() (see Figure 15 and the code below).\n\n\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(do_you_recline, rude_to_recline), \n                  fill = do_you_recline)) +\n  geom_mosaic_text(aes(x = product(do_you_recline, rude_to_recline), \n                       label = after_stat(.wt)))\n\n\n\n\n\nFigure 15: Variables calculated by the associated stat can be used for the label. In this example, is used to place the frequencies represented by the tile in the center of that tile, providing a glance at the size of the data and removing potential guessing work aimed at comparing areas of unaligned tiles.\n\n\n\nTo help label the areas of the plots where labels may be densely packed and overlapping, ggmosaic uses the ggplot2 extension package, ggrepel (Slowikowski 2021). The geoms provided by ggrepel help ensure the text is readable by repelling the labels away from each other, data points, and edges of the plot panel. In addition to the standard \"Text\" and \"Label\" geoms, ggmosaic’s use of ggrepel allows the use of the \"TextRepel\" and \"LabelRepel\" geoms within mosaic plots. Thus, geom_mosaic_text() provides access to the features of four geoms. To access these four geoms, the parameters as.label and repel are introduced; their use is demonstrated in Figure 16, and the code is below. Furthermore, the parameter repel_params is available to use with either of the ggrepel options, and the parameter check_overlap is available to use with the \"Text\" geom.\n\n\n# default / text\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(do_you_recline, rude_to_recline), \n                  fill = do_you_recline)) +\n  geom_mosaic_text(aes(x = product(do_you_recline, rude_to_recline)))\n\n# label\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(do_you_recline, rude_to_recline), \n                  fill = do_you_recline)) +\n  geom_mosaic_text(aes(x = product(do_you_recline, rude_to_recline)), \n                   as.label = TRUE)\n  \n# repel text\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(do_you_recline, rude_to_recline), \n                  fill = do_you_recline)) +\n  geom_mosaic_text(aes(x = product(do_you_recline, rude_to_recline)), \n                   repel = TRUE)\n  \n# repel label\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(do_you_recline, rude_to_recline),\n                  fill = do_you_recline)) +\n  geom_mosaic_text(aes(x = product(do_you_recline, rude_to_recline)), \n                   repel = TRUE, as.label = TRUE)\n\n\n\n\n\nFigure 16: Examples of the four geoms accessible to . The geoms, from left to right and top to bottom, are , , , and .\n\n\n\nMosaic plots with many categories can quickly become unreadable, geom_mosaic_text() helps alleviate congestion-related confusion by providing quick and effective labeling in various formats. The parameters repel and as.label provide access to three additional geoms. The geom geom_mosaic_text() can add the text geom, the label geom, the ggrepel text geom, or the ggrepel label geom. The parameters repel_params and check_overlap provide access to the parameters native to the ggrepel geoms and label geoms, respectively. Labels add value to the mosaic plot as they ensure easy and correct identification of the tiles, which, in turn, eases the visual analysis.\nA jittering geom\nIn constructing of a mosaic plot, the tile area is proportional to the number of observations that the tile represents. In other words, the density of each tile in a mosaic plot is constant throughout the plot. While the number of observations, or density, is typically masked in a mosaic plot, a user can visualize these individual data points with geom_mosaic_jitter(). This added layer of visualization unlocks a range of applications, from adding additional aesthetic mappings, both included in the formula and not, to model diagnosis. Research suggests visualizations that incorporate individuals in charts allow viewers to digest and understand probabilities and risks involved more easily (Ancker et al. 2006; Galesic et al. 2009). These features further extend the ability of generalized mosaic plots to communicate interesting features of high-dimensional categorical data.\nWhen used in conjunction with geom_mosaic(), geom_mosaic_jitter() adds a layer of jittered points superimposed on the mosaic plot. The number of points in each rectangle is equal to the number of observations that the rectangle represents. The result is an even dispersal of points throughout the one-by-one square mosaic plot.\nWhen conditioning on a variable, geom_mosaic_jitter() provides a visual representation of the differences between the conditional probability and the joint probability; the spread of the points throughout the mosaic plot can help decipher a conditioning variable’s effect on a mosaic plot’s construction. In Figure 17, conditioning on the variable rude_to_recline causes a change in the density of the jittered points. The visual difference serves as an effective tool for teaching the concept. The plots are based on the following lines of code:\n\n\n# not conditioned\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(rude_to_recline), \n                  fill = do_you_recline), \n              alpha = 0.3) +\n  geom_mosaic_jitter(aes(x = product(rude_to_recline), \n                         color = do_you_recline))\n  \n# conditioned\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(do_you_recline), \n                  conds = product(rude_to_recline), \n                  fill = do_you_recline), \n              alpha = 0.3)  +\n  geom_mosaic_jitter(aes(x = product(do_you_recline), \n                         conds = product(rude_to_recline), \n                         color = do_you_recline))\n\n\n\n\n\nFigure 17: This example highlights the differences between the conditional (right) and joint (left) probability with the density of the jittered dots. When conditioning, the columns are equally sized, and the jittered points condense into the leftmost column, previously the widest column, rather than being evenly dispersed.\n\n\n\nIncluding the jittered points generates an awareness of the size of the data, something customarily masked in a mosaic plot. For smaller data sets such as the titanic data set, using geom_mosaic_jitter() may encourage engagement as the points connect with the individuals in the data. This connection may provide a more compelling and profound visual impact.\ngeom_mosaic_jitter() is more effective when the alpha argument is used in both geom_mosaic_jitter() and geom_mosaic() to create semi-transparent jittered points and semi-transparent rectangles. An alpha value of 0.3 for geom_mosaic() and an alpha value of 0.7 for geom_mosaic_jitter() is aesthetically pleasing.\ngeom_mosaic_jitter() introduces an additional parameter, drop_level. The drop_level parameter controls which level defines the color of the generated points. In other words, if a color aesthetic is defined, should that variable be included in the formula? If the formula includes the color aesthetic, drop_level = FALSE, the colored points are at the top level. If the formula does not include the color aesthetic, drop_level = TRUE, color is added to the points one level down. Figure 18 provides an example of these two options.\n\n\n# drop level\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(rude_to_recline)), \n              alpha = 0.1) +\n  geom_mosaic_jitter(aes(x = product(rude_to_recline), \n                         color = do_you_recline),\n                     drop_level = TRUE)\n# do not drop level\nggplot(data = flights) +\n  geom_mosaic(aes(x = product(rude_to_recline)), \n              alpha = 0.1) +\n  geom_mosaic_jitter(aes(x = product(rude_to_recline), \n                         color = do_you_recline),\n                     drop_level = FALSE)\n\n\n\n\n\nFigure 18: The parameter controls which level defines the colors of the generated points. On the left, color is added to the points one level down, and a mixing of the colored points occurs. In the plot on the right, the heights of the sorted points’ spaces represent the conditional distribution of given .\n\n\n\nAn additional aesthetic, weight2, is implemented in geom_mosaic_jitter(). The weight2 aesthetic allows the number of points generated within each tile to be different from the number of points the cell represents.\nWhile mosaic plots are typically drawn according to the observed values, they can be drawn according to the model’s expected values.\n\n\n\n\n\nflights_model %>% glimpse()\n\nRows: 9\nColumns: 4\n$ do_you_recline  <fct> never, never, never, sometimes, sometimes, s…\n$ rude_to_recline <fct> no, somewhat, yes, no, somewhat, yes, no, so…\n$ Observed        <int> 35, 81, 54, 198, 164, 11, 269, 36, 6\n$ Expected        <dbl> 100, 56, 14, 219, 123, 31, 183, 102, 26\n\nFigure 19 displays the observed values on the left and the expected values from the independence model on the right. The difference between the two plots represents the lack of fit. The plot is based on the following lines of code:\n\n\nflights_model %>% \n  gather(\"wt_type\", \"wt\", Expected:Observed) %>% \n  ggplot() + \n  geom_mosaic(aes(weight = wt, \n                  x = product(rude_to_recline), \n                  fill = do_you_recline)) +\n  facet_wrap(vars(wt_type))\n\n\n\n\n\nFigure 19: While mosaic plots are typically drawn according to the observed values, they can be drawn according to the model’s expected values. The mosaic plot on the right displays the expected values from the independence model, recognizable by the lattice structure. The differences in the heights of the tiles in the observed data mosaic plot on the left reveal a strong association between the two variables, and the differences between the two plots highlight the lack of model fit.\n\n\n\nIn Figure 19, the mosaic plot drawn according to the observed values represents the data space, whereas the mosaic plot drawn according to the expected values represents the model space. Rather than requiring two plots, jittering connects in one plot both the data space with the model space highlighting the differences between the two.\nIn Figure 20, the jittered points represent the observed values, while the tiles’ size represents the expected values. We can evaluate the fit of the model according to how evenly the points spread throughout the plot. For example, the overcrowded points seen in the leftmost bottom tile communicate that the independence model underestimates the number of respondents that think it is rude to recline and never recline their seats.\n\n\nggplot(flights_model) + \n  geom_mosaic(aes(weight = Expected, \n                  x = product(do_you_recline, rude_to_recline)), \n              alpha = .2) +\n  geom_mosaic_jitter(aes(weight2 = Observed, \n                         weight = Expected, \n                         x = product(do_you_recline, rude_to_recline)))\n\n\n\n\n\nFigure 20: In this example, the tiles of the mosaic plot are drawn according to the expected values. A jitter geom representing the observed values is layered on top. The uneven dispersal of the points indicates a lack of fit.\n\n\n\nThe jitter geom, with its additional aesthetic mappings, provides a convenient visual aide that can help solidify what the mosaic plot represents and help communicate the differences between conditional and joint probabilities. The implementation of the jitter geom allows the user to extend the functionality of mosaic plots beyond what has previously been available.\nA custom theme\nMosaic plots have two main characteristics that set mosaic plots apart from other graphics. First, at its foundation, a mosaic plot is a one-by-one square, and the area of each tile in the mosaic plot is proportional to the frequency of the combination of categories that the tile represents. Second, mosaic plots do not have a standard coordinate system. Rather, mosaic plots have a coordinate system based on recursive partitioning that can integrate several variables. These two characteristics clash with the default ggplot2 theme. For this reason, version 0.3.3 of ggmosaic includes a custom theme for mosaic plots, theme_mosaic(), that can be added to the plot in the same manner as any other ggplot2 theme. (Figure 21)\nThe plot grid lines suffer from the same issue as the axis labels; while the grid lines may be aligned with the\ncorrect category levels on one side of the plot, the alignments may not be appropriate for category levels on the opposite side of the plot. theme_mosaic() removes all grid lines but does not remove the axis labels and ticks.\nSeeking a more faithful representation of a mosaic plot, theme_mosaic() enforces a fixed aspect ratio of 1. When faceting, the aspect ratio should be modified according to the number of panels and the direction of the faceting. For example, in Figure 22, the faceting represents the responses to do_you_recline conditioned on the responses to rude_to_recline. The conditioning variable, rude_to_recline, contains three categories, and the faceting represents a horizontal spine. Hence, the aspect ratio is modified from 1 (the default) to 3.\n\n\n\nFigure 21: This example highlights the differences between the custom theme (left) and the default theme (right). The custom theme for mosaic plots seeks to minimize clutter by removing the plot background and the grid lines.\n\n\n\n\n\n\n\n\n\nFigure 22: When using faceting, the aspect ratio might need to be adjusted according to the desired outcome. The default (left) represents each facet as a 1-by-1 mosaic plot, whereas an aspect ratio of 3 (right) represents each facet as one equal-sized column within a 1-by-1 mosaic plot.\n\n\n\nThe custom theme, theme_mosaic(), seeks to help a viewer extract the correct information from the plot by providing a more suitable default aspect ratio and removing the axes lines that are not always appropriate across the entire plot. As with any theme, however, theme_mosaic() is merely a suggestion of a starting-off point, and it can be modified as seen fit for each individual plot.\n5 Interactive exploratory mosaic plot building\nMosaic plots help identify interesting relationships in high-dimensional categorical data and are an influential tool for exploratory data analysis (EDA). Because of the complexities that arise from comparing many categories, it is often necessary to iterate through many of the potential mosaic plots and obtain many views on the data. The addition of interactivity to the generation of mosaic plots can ease this process and help mosaic plots become more valuable and insightful (Hofmann 2003).\nTo facilitate exploring data with mosaic plots, ggmosaic version 0.3.4 includes a Shiny application that can be launched with the function ggmosaic_app() (Figure 23). This app accommodates structural changes to the mosaic plot with the press of certain keystrokes or buttons provided in the side panel. The app enables quick iterations between visualizations, providing a mechanism for discoveries and achieving more profound insight.\n\n\n\nFigure 23: A snapshot of the Shiny application that can be launched with the function . The Shiny app facilitates learning how to create mosaic plots. The user can explore one of three data sets provided with the package with mosaic plots created and modified via keystrokes or buttons, and no coding is required.\n\n\n\nThe app is organized into two tabs (Figure 23), “MOSAIC PLOT” and “DATA”, setting the stage for data set and variable selection. Creating a mosaic plot consists of several steps. Using the drop-down menu provided on the left-hand side, the user first selects from one of the three data sets exported with ggmosaic, the titanic data set, the happy data set, or the fly data set (the default selection). After the data set is selected, the user can create mosaic plots by selecting various variables to include and different dividers to be used, all completed with keystrokes or a set of buttons in the side panel.\nThe arrow keys (up, down, left, right) add, remove, or switch variables. The ordering of the variables can be quickly modified, allowing the user to find a sensible order for the variables in a streamlined manner. Additionally, the ‘h’, ‘v’, ‘s’, and ‘b’ keys switch the type of divider. The ‘h’ and ‘v’ keys switch between horizontal and vertical spines or bars, switching between ‘hspine’ and ‘vpsine’ or ‘hbar’ and ‘vbar’. Similarly, the ‘s’ and ‘b’ keys can be used to select to split the categories into spines or bars, switching between ‘hspine’ and ‘hbar’ or ‘vspine’ and ‘vbar’. If the user does not press the ‘h’, ‘v’, ‘s’, or ‘b’ keys, the ‘mosaic()’ divider is the default, corresponding to the default divider in geom_mosaic(). The ‘h’, ‘v’, ‘s’, or ‘b’ keystrokes and buttons will only affect the top-level variable. To modify the divider used on a lower-level variable, the user must backtrack via the down arrow key.\nThe Shiny app accommodates a better understanding of the myriad of possible forms a mosaic plot can take by accommodating a thorough search through the variables and structural changes to the mosaic plot with the simple press of certain keystrokes. The app provides an exploratory setting for visualizing many mosaic plots, and provide the user with the code necessary to recreate the selected mosaic plot.\n6 Summary\nBy bringing mosaic plots into the ggplot2 infrastructure, ggmosaic provides a highly customizable framework for generalized mosaic plots with a familiar syntax. The latest release of ggmosaic introduces novel uses of mosaic plots and exemplifies the opportunity the methods of visualizing multidimensional categorical data have for growth.\nThis manuscript is based on version 0.3.3 of the ggmosaic package. It can be installed from CRAN. The development version is available from the GitHub repository.\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-013.zip\nCRAN packages used\nyardstick, ggplot2, vcd, latticeExtra, lattice, productplots, ggmosaic, shiny, plotly, ggrepel\nCRAN Task Views implied by cited packages\nPhylogenetics, Spatial, TeachingStatistics, WebTechnologies\n\n\nJ. S. Ancker, Y. Senathirajah, R. Kukafka and J. B. Starren. Design features of graphs in health risk communication: A systematic review. Journal of the American Medical Informatics Association, 13(6): 608–618, 2006. URL https://doi.org/10.1197/jamia.M2115.\n\n\nW. Chang, J. Cheng, J. Allaire, C. Sievert, B. Schloerke, Y. Xie, J. Allen, J. McPherson, A. Dipert and B. Borges. Shiny: Web application framework for r. 2021. URL https://CRAN.R-project.org/package=shiny. R package version 1.6.0.\n\n\nW. S. Cleveland and R. McGill. Graphical perception: Theory, experimentation and application to the development of graphical methods. Journal of the American Statistical Association, 79(387): 531–554, 1984. URL https://doi.org/10.1080/01621459.1984.10478080.\n\n\nJ. W. Emerson. Mosaic displays in S-PLUS: A general implementation and a case study. Statistical Computing and Graphics Newsletter, 9(1): 17–23, 1998.\n\n\nM. Friendly. A brief history of the mosaic display. Journal of Computational and Graphical Statistics, 11(1): 89–107, 2002. URL https://doi.org/10.1198/106186002317375631.\n\n\nM. Friendly. Working with categorical data with R and the vcd and vcdExtra packages. 2016. URL https://cran.r-project.org/web/packages/vcdExtra/vignettes/vcd-tutorial.pdf.\n\n\nM. Galesic, R. Garcia-Retamero and G. Gigerenzer. Using icon arrays to communicate medical risks: Overcoming low numeracy. Health psychology, 28(2): 210–216, 2009. URL https://doi.org/10.1037/a0014474.\n\n\nJ. A. Hartigan and B. Kleiner. A mosaic of television ratings. The American Statistician, 38: 32–35, 1984. URL https://doi.org/10.1080/00031305.1984.10482869.\n\n\nJ. A. Hartigan and B. Kleiner. Mosaics for contingency tables. In Computer science and statistics: Proceedings of the 13th symposium on the interface, pages. 268–273 1981. Fairfax Station, VA: Interface Foundation of North America, Inc. URL https://doi.org/10.1007/978-1-4613-9464-8_37.\n\n\nW. Hickey. 41 percent of fliers think you’re rude if you recline your seat. FiveThirtyEight, 2014. URL http://fivethirtyeight.com/datalab/airplane-etiquette-recline-seat/.\n\n\nH. Hofmann. Constructing and reading mosaicplots. Computational Statistics and Data Analysis, 43(4): 565–580, 2003. URL https://doi.org/10.1016/S0167-9473(02)00293-1.\n\n\nJ. Hummel. Linked bar charts: Analysing categorical data graphically. Computational Statistics, 11(1): 23–33, 1996.\n\n\nB. Kleiner and J. A. Hartigan. Representing points in many dimensions by trees and castles. Journal of the American Statistical Association, 76(374): 260–269, 1981. URL https://doi.org/10.1080/01621459.1981.10477638.\n\n\nM. Kuhn, D. Vaughan and E. Hvitfeldt. Yardstick: Tidy characterizations of model performance. 2022. URL https://CRAN.R-project.org/package=yardstick. R package version 1.1.0.\n\n\nD. Meyer, A. Zeileis and K. Hornik. Vcd: Visualizing categorical data. 2020. URL https://CRAN.R-project.org/package=vcd. R package version 1.4-8.\n\n\nW. Oldford, E. Holgersen, B. Lafreniere and T. Zhu. Eikosograms: The picture of probability. 2018. URL https://CRAN.R-project.org/package=eikosograms. R package version 0.1.1.\n\n\nW. Playfair, H. Wainer and I. Spence. Playfair’s commercial and political atlas and statistical breviary. Cambridge University Press, 2005.\n\n\nD. Sarkar. Lattice: Trellis graphics for r. 2020. URL https://CRAN.R-project.org/package=lattice. R package version 0.20-41.\n\n\nD. Sarkar and F. Andrews. latticeExtra: Extra Graphical Utilities Based on Lattice. 2016. URL https://CRAN.R-project.org/package=latticeExtra. R package version 0.6-28.\n\n\nC. Sievert. Interactive web-based data visualization with R, plotly, and shiny. Chapman; Hall/CRC, 2020. URL https://plotly-r.com.\n\n\nC. Sievert, C. Parmer, T. Hocking, S. Chamberlain, K. Ram, M. Corvellec and P. Despouy. Plotly: Create interactive web graphics via ’plotly.js’. 2016. URL https://CRAN.R-project.org/package=plotly. R package version 4.9.3.\n\n\nK. Slowikowski. Ggrepel: Automatically position non-overlapping text labels with ’ggplot2’. 2021. URL https://CRAN.R-project.org/package=ggrepel. R package version 0.9.1.\n\n\nM. Theus and S. Urbanek. Interactive graphics for data analysis: Principles and examples. CRC Press, 2009.\n\n\nUnited States Census office. 9th census, 1870 and F. A. Walker. Statistical atlas of the United States based on the results of the ninth census, 1870 with contributions from many eminent men of science and several departments of the government. 1874.\n\n\nH. Wickham. A layered grammar of graphics. Journal of Computational and Graphical Statistics, 19: 3–28, 2010. URL https://doi.org/10.1198/jcgs.2009.07098.\n\n\nH. Wickham, W. Chang, L. Henry, T. L. Pedersen, K. Takahashi, C. Wilke, K. Woo, H. Yutani and D. Dunnington. ggplot2: Create elegant data visualisations using the grammar of graphics. 2020. URL https://CRAN.R-project.org/package=ggplot2. R package version 3.3.3.\n\n\nH. Wickham and H. Hofmann. Product plots. IEEE Transactions on Visualization and Computer Graphics, 17(12): 2223–2230, 2011. URL https://doi.org/10.1109/tvcg.2011.227.\n\n\nH. Wickham and H. Hofmann. Productplots: Product plots for r. 2016. URL https://CRAN.R-project.org/package=productplots. R package version 0.1.1.\n\n\nL. Wilkinson. The grammar of graphics. Springer New York, 1999. URL https://doi.org/10.1007/978-1-4757-3100-2.\n\n\n\n\n",
    "preview": "articles/RJ-2023-013/distill-preview.png",
    "last_modified": "2023-11-07T21:31:40+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 576
  },
  {
    "path": "articles/RJ-2023-020/",
    "title": "The openVA Toolkit for Verbal Autopsies",
    "description": "Verbal autopsy (VA) is a survey-based tool widely used to infer cause of death (COD) in regions without complete-coverage civil registration and vital statistics systems. In such settings, many deaths happen outside of medical facilities and are not officially documented by a medical professional. VA surveys, consisting of signs and symptoms reported by a person close to the decedent, are used to infer the COD for an individual, and to estimate and monitor the COD distribution in the population. Several classification algorithms have been developed and widely used to assign causes of death using VA data. However, the incompatibility between different idiosyncratic model implementations and required data structure makes it difficult to systematically apply and compare different methods. The openVA package provides the first standardized framework for analyzing VA data that is compatible with all openly available methods and data structure. It provides an open-source, R implementation of several most widely used VA methods. It supports different data input and output formats, and customizable information about the associations between causes and symptoms. The paper discusses the relevant algorithms, their implementations in R packages under the openVA suite, and demonstrates the pipeline of model fitting, summary, comparison, and visualization in the R environment.",
    "author": [
      {
        "name": "Zehang Richard Li",
        "url": {}
      },
      {
        "name": "Jason Thomas",
        "url": {}
      },
      {
        "name": "Eungang Choi",
        "url": {}
      },
      {
        "name": "Tyler H. McCormick",
        "url": {}
      },
      {
        "name": "Samuel J Clark",
        "url": {}
      }
    ],
    "date": "2023-02-25",
    "categories": [],
    "contents": "\n\n\n\n\n\n1 Introduction\n\n\nVerbal autopsy (VA) is a well-established approach to ascertaining the cause of death (COD) when medical certification and full autopsies are not feasible or practical (Taylor et al. 1983; Garenne 2014). After a death is identified, a specially-trained fieldworker interviews the caregivers (usually family members) of the decedent. A typical VA interview includes a set of structured questions with categorical or quantitative responses and a narrative section that records the “story” of the death from the respondent’s point of view (World Health Organization 2012). Currently, there are multiple commonly used questionnaires with overlapping, but not identical questions. VAs are routinely used both by researchers in health and demographic surveillance systems (Maher et al. 2010; Sankoh and Byass 2012) and multi-country research projects (Nkengasong et al. 2020; Breiman et al. 2021), and in national-scale surveys in many low- and middle-income countries (LMICs). For a more comprehensive overview of the current use of VA, we refer readers to Chandramohan et al. (2021).\nThe process of inferring a cause from VA data consists of two components. First, there must be some external information about the relationship between causes and symptoms. In supervised learning problems, the external information is typically derived from training datasets with known labels. In the context of VA, a common practice is to obtain training data using clinically trained, experienced physicians who read a fraction of the interviews and determine causes. To address the fact that physicians frequently do not agree on causes, VA interviews are often read by two physicians, and sometimes three, and the final causes are determined through a consensus mechanism (e.g., Kahn et al. 2012). This process can be extremely time- and resource-intensive. Another means of obtaining this information is directly through expert opinion. For example, one can ask groups of physicians to rate the likelihood of a symptoms occurring, given a particular COD, which can be converted into a set of probabilities of observing a symptom given a particular cause. Such expert knowledge can be highly useful in analyzing VA data when training datasets do not exist.\nSecondly, an algorithmic or statistical model is needed to assign the causes of death by extrapolating the relationship between symptoms and causes to the target population. The cause-of-death assignment model is conceptually a separate construct from the symptom-cause information. The current state of the VA literature, however, often does not distinguish between the two. This is in part due to popular VA software that combines a source of symptom-cause information (e.g., a certain training data or a database of expert knowledge) with a specific algorithm, and requires a specific type of survey instrument to be used. This restriction prevents robust comparison between methods and contexts. A health agency in one region may, for example, want to analyze VA data using the same VA algorithm as a neighboring region to ensure estimates are comparable. Unless the agencies used the same survey format, however, this is not possible with existing tools.\nThe openVA package (Li et al. 2022b) addresses this issue through an open-source toolkit. The openVA suite comprises a collection of R packages for the analysis of verbal autopsy data. The goal of this package is to provide researchers with an open-source tool that supports flexible data input formats and allows easier data manipulation and model fitting. The openVA suite consists of four core packages that are on CRAN, InterVA4 (Li et al. 2019), InterVA5 (Thomas et al. 2021b), InSilicoVA (Li et al. 2022a), and Tariff (Li et al. 2018), and an optional package nbc4va (Wen et al. 2022). Each of these packages implements one coding algorithm. For three of these algorithms – namely, InterVA-4, InterVA-5 and Tariff – there are also compiled software programs distributed by the original authors.\nThe main focus of this paper is to provide a general introduction to the implementation details of the included algorithms both in terms of the underlying methodology, and through case studies. The openVA package has four major contributions:\nIt provides a standardized and user-friendly interface for analysts to fit and evaluate each method on different types of data input using standard syntax. Previously, most of the methods were designed to be used specifically with their own input formats and are usually incompatible with others. The openVA package closes this gap by allowing easier and fair model comparison of multiple algorithms on the same data. This significantly facilitates further research on VA algorithms.\nIt provides a series of functionalities to summarize and visualize results from multiple algorithms, which is helpful for analysts not familiar with data manipulation and visualization in R.\nIt does not directly implement any algorithms for coding VA data, so that it is possible for a research group to maintain their own algorithm implementations callable from the openVA package, while also making it available to general users as a standalone piece of software. For example, the nbc4va was developed and maintained independently by researchers at the Center for Global Health Research in Toronto, but is designed so that it can be seamlessly integrated into the openVA package.\nIt is fully open source, and can be run on multiple platforms. The open-source nature of openVA significantly expands its potential for methodological research and its suitability for integration within a larger data analysis pipeline. Compared to the alternative implementations, the InterVA-4 and InterVA-5 software are distributed as compiled software that can only be run on Windows system. They provide the source codes as an additional code script, which are difficult to modify and re-compile. Tariff, as implemented through the SmartVA-Analyze application (Serina et al. 2015), was also primarily distributed as a compiled application that can only be run on Windows system (Institute for Health Metrics and Evaluation 2021b). However, their source codes were recently made available under the open source MIT License on GitHub (Institute for Health Metrics and Evaluation 2021a).\nThe rest of this paper is organized as follows: We first briefly introduce the main component packages and the underlying algorithms. We then demonstrate standard data structures and model fitting steps with the openVA package and functionalities to summarize results. We then discuss how additional information can be incorporated into modeling VA data, and we briefly survey additional packages and software developments built around the openVA package. We end with a discussion of remaining issues and limitations of the existing automated VA algorithms and propose new functionalities to be included in openVA package.\n2 Structure of the openVA package\nThe openVA suite of packages currently consists of four standalone packages that are available on CRAN and one optional package hosted on GitHub. In this section, we first provide a brief introduction to these five packages, and we discuss the mathematical details behind each algorithm in the next subsection.\nInterVA4 (Li et al. 2014; Li et al. 2019) is an R package that implements the InterVA-4 model (Byass et al. 2012). It provides replication of the widely used InterVA software (Byass 2015). The standard input of InterVA4 is in the form of a pre-defined set of indicators, based on the 2012 World Health Organization (WHO) VA instrument (World Health Organization 2012). The default InterVA-4 algorithm cannot be applied to other data input formats because its internal built-in prior information is specific to a fixed set of indicators and causes. The same restriction is also maintained in the InterVA4 package. However, the mathematical formulation of the InterVA-4 model is completely generalizable to other binary input formats, as described in later sections, and is also implemented in the openVA package.\nInterVA5 (Thomas et al. 2021b) is an R package that implements the InterVA-5 model (Byass et al. 2019). The InterVA-5 model updates the previous version in several ways. First, the input data must adhere to the format of the 2016 WHO VA instrument (D’Ambruoso et al. 2017). Second, changes have been made to the data processing steps. It is also worth noting that the model outputs have been expanded by the inclusion of the most likely Circumstances Of Mortality CATegory, or COMCAT, among the results – the categories include: culture, emergency, health systems, inevitable, knowledge, resources, or an indeterminant combination of multiple factors (for more details, see D’Ambruoso et al. 2017). Despite these changes, the mathematical formulation of InterVA-5 is identical to that of InterVA-4.\nInSilicoVA (Li et al. 2022a) is an R package that implements the InSilicoVA algorithm, a Bayesian hierarchical framework for cause-of-death assignment and cause-specific mortality fraction estimation proposed in McCormick et al. (2016). It is originally designed to work with the WHO VA instrument, i.e., the same input data used by the InterVA software, but is also generalizable to other data input formats. It is a fully probabilistic algorithm and can incorporate multiple sources of information, such as known sub-populations in the dataset, and physician coding when such data are available. The Markov Chain Monte Carlo (MCMC) sampler is implemented in Java for improved speed.\nTariff (Li et al. 2018) is an R package that implements the Tariff algorithm (James et al. 2011). It most closely reflects the description of the Tariff 2.0 method (Serina et al. 2015). The Tariff algorithm is developed by the Institute for Health Metrics and Evaluation (IHME) and is officially implemented in the SmartVA-Analyze software (Institute for Health Metrics and Evaluation 2021b). However, as the developers of this R package are not affiliated with the authors of the original algorithm, there are some discrepancies in implementation. The source code of the two versions of Tariff was not publicly available at the time when the Tariff package was created, so the package was developed based solely on the descriptions in the published work. Despite the difference in implementation, Tariff is able to achieve comparable results as the published work as demonstrated in McCormick et al. (2016). More detailed descriptions of the Tariff implementations are also discussed in the supplement of McCormick et al. (2016). The later released Python source codes of SmartVA-Analyze have been incorporated in the web application extension of the openVA package.\nnbc4va (Wen et al. 2022) is an R package that implements the Naive Bayes Classifier for VA encoding (Miasnikof et al. 2015). It calculates the conditional probabilities of symptoms given causes of death from a training dataset, instead of using physician-provided values. nbc4va is developed and maintained by researchers at the Center for Global Health Research in Toronto, but is designed so that it can be seamlessly integrated into openVA. It is an optional package that users can choose to load separately.\nWe note that there are additional methods and implementations to assign causes of death using VA data that are not included in the openVA implementation (e.g., Flaxman et al. 2011; Jeblee et al. 2019). These methods are not widely adopted by VA practitioners and most of these implementations are either not publicly available or require data processing steps that are specific to certain datasets, making them impractical for routine use in general. In addition, while the cause-of-death assignment process is closely related to the generic multi-class classification problem, naive application of off-the-shelf classification algorithms has been shown to perform poorly in the context of VA (Murray et al. 2014). Therefore, we focus on the methods that are currently adopted by practitioners. We briefly survey some more recent developments and their implementations at the end of this paper.\nThe openVA package is hosted on CRAN and can be installed with the following commands. Since posterior inference is carried out using MCMC in the InSilicoVA algorithm, we set the seed for the random number generator to make the paper reproducible. For the analysis in this paper, we also install the nbc4va package separately from GitHub. The versions of the supporting packages can be checked in R using the openVA_status() function.\n\n\nset.seed(12345)\nlibrary(openVA)\nopenVA_status()\n\n\nOverview of VA cause-of-death assignment methods\nThe common modeling framework for VA data consists of first converting the survey responses into a series of binary variables, and then assigning a COD to each death based on the binary input variables. Typically, the target of inference consists of two parts: the individual cause-of-death assignment, and the population-level cause-specific mortality fractions (CSMF), i.e., the fraction of deaths due to each cause. In this section, we formally compare the modeling approaches utilized by each algorithm for these two targets. We adopt the following notations. Consider \\(N\\) deaths, each with \\(S\\) binary indicators of symptoms. Let \\(s_{ij}\\) denote the indicator for the presence of \\(j\\)-th symptom in the \\(i\\)-th death, which can take values 0, 1, or NA (for missing data). We consider a pre-defined set of causes of size \\(C\\). For the \\(i\\)-th death, denote the COD by \\(y_i \\in \\{1, ..., C\\}\\) and the probability of dying from cause \\(k\\) is denoted by \\(P_{ik}\\). For the population, the CSMF of cause \\(k\\) is denoted as \\(\\pi_k\\), with \\(\\sum_{k=1}^C \\pi_k = 1\\).\nInterVA4 (Byass et al. 2012) and InterVA5 (Byass et al. 2019) algorithms calculate the probability of each COD given the observed symptoms using the following formula,\n\\[\nP_{ik} = \\frac{\\pi_{k}^{(0)} \\prod_{j=1}^S P(s_{ij}=1|y_{i}=k) \\mathbf{1}_{s_{ij} = 1}}\n{\\sum_{k' = 1}^C \\pi_{k'}^{(0)} \\prod_{j=1}^S P(s_{ij}=1|y_{i}=k') \\mathbf{1}_{s_{ij} = 1}},\n\\]\nwhere both the prior distribution of each of the causes, \\(\\pi_{k}^{(0)}\\) and the conditional probabilities \\(P(s_{ij} = 1 | y_i = k)\\) are fixed values provided in the InterVA software. It is worth noting that the formula does not follow the standard Bayes’ rule as it omits the probability that any symptom is absent. A detailed discussion of this modeling choice can be found in McCormick et al. (2016). The conditional probabilities, \\(P(s_{ij}=1|y_{i}=k)\\), used in InterVA algorithms are represented as rankings with letter grades instead of numerical values (Byass et al. 2012). For example, \\(P(s_{ij}=1|y_{i}=k) = A+\\) is translated into \\(P(s_{ij}=1|y_{i}=k) = 0.8\\), etc. The standard InterVA software only supports the fixed set of symptoms and causes where such prior information is provided. For a different data input format, this formulation can be easily generalized if training data are available. We include in the openVA package an extension of the algorithm that calculates \\(\\hat P(s_{ij}=1|y_{i}=k)\\) from the empirical distribution in the training data and then maps to letter grades with different truncation rules. Details of the extended InterVA algorithm can be found in McCormick et al. (2016).\nAfter the individual COD distributions are calculated, InterVA-4 utilizes a series of pre-defined rules to identify up to the top three most likely COD assignments, and truncates the probabilities for the rest of the CODs to 0 and adds an ‘undetermined’ category so that the probabilities sum up to 1 (See the user guide of Byass (2015)). Then the population-level CSMFs are calculated as the aggregation of individual COD distributions, such that\n\\[\n\\pi_k = \\sum_{i=1}^N P^*_{ik},\n\\]\nwhere \\(P^*_{ik}\\) denotes the individual COD distribution after introducing the undetermined category.\nNaive Bayes Classifier (Miasnikof et al. 2015) is very similar to the InterVA algorithm with two major differences. First, instead of considering only symptoms that present, the NBC algorithm also considers symptoms that are absent. Second, the conditional probabilities of symptoms given causes are calculated from training data instead of given by physicians, which is similar to our extension of InterVA discussed above. Similar to InterVA, the NBC method can be written as\n\\[\nP_{ik} = \\frac{\\pi_{k}^{(0)} \\prod_{j=1}^S (P(s_{ij}=1|y_{i}=k) \\mathbf{1}_{s_{ij} = 1} + P(s_{ij} \\neq 1|y_{i}=k) \\mathbf{1}_{s_{ij} \\neq 1})}\n{\\sum_{k' = 1}^C \\pi_{k'}^{(0)} \\prod_{j=1}^S (P(s_{ij}=1|y_{i}=k') \\mathbf{1}_{s_{ij} = 1}+ P(s_{ij} \\neq 1|y_{i}=k') \\mathbf{1}_{s_{ij} \\neq 1})},\n\\]\nand the CSMFs are calculated by \\(\\pi_k = \\sum_{i=1}^N P_{ik}\\).\nInSilicoVA algorithm (McCormick et al. 2016) assumes a generative model that characterizes both the CSMF at the population level, and the COD distributions at the individual level. In short, the core generative process assumes\n\\[\\begin{eqnarray}\ns_{ij} | y_i = k &\\propto& \\mbox{Bernoulli}(P(s_{ij} | y_i = k)), \\\\\ny_i | \\pi_1, ..., \\pi_C &\\propto& \\mbox{Categorical}(\\pi_1, ..., \\pi_C), \\\\\n\\pi_k &=& \\exp \\theta_k / \\sum_{k=1}^C \\exp \\theta_k, \\\\\n\\theta_k &\\propto& \\mbox{Normal}(\\mu, \\sigma^2).\n\\end{eqnarray}\\]\nHyperpriors are also placed on \\(P(s_{ij} | y_i = k)\\), \\(\\mu\\), and \\(\\sigma^2\\). The priors for \\(P(s_{ij} | y_i = k)\\) are set by the rankings used in InterVA-4 if the data are prepared into InterVA format, otherwise they are learned from training data. The priors on \\(\\mu\\) and \\(\\sigma^2\\) are diffuse uniform priors. Parameter estimation is performed using MCMC, so that a sample of posterior distributions of \\(\\pi_k\\) can be obtained after the sampler converges.\n\nTariff algorithm (James et al. 2011) differs from the other three methods in that it does not calculate an explicit probability distribution of the COD for each death. Instead, for each death \\(i\\), a Tariff score is calculated for each COD \\(k\\) so that\n\\[\nScore_{ik} = \\sum_{j = 1}^{S} \\mbox{Tariff}_{kj}\\mathbf{1}_{s_{ij}=1},\n\\]\nwhere the symptom-specific Tariff score \\(\\mbox{Tariff}_{kj}\\) is defined as\n\\[\n\\mbox{Tariff}_{kj} = \\frac{n_{kj} - median(n_{1j}, n_{2j}, ..., n_{Cj})} {IQR(n_{1j}, n_{2j}, ..., n_{Cj})},\n\\]\nwhere \\(n_{kj}\\) is the count of how many deaths from cause \\(k\\) contain symptom \\(j\\) in the training data. The Tariff scores are then turned into rankings by comparing them to a reference distribution of scores calculated from re-sampling the training dataset to obtain a uniform COD distribution. It is worth noting that the Tariff algorithm produces the COD distribution for each death in terms of their rankings instead of the probability distributions. And thus the CSMF for each cause \\(k\\) is calculated by the fraction of deaths with cause \\(k\\) being the highest ranked cause, i.e.,\n\\[\n\\pi_k = \\frac{\\sum_{i=1}^N\\mathbf{1}_{y_i = k}}{N}.\n\\]\nIn addition to the different model specifications underlying each algorithm, there is also a major conceptual difference in handling missing symptoms across the algorithms. Missing symptoms could arise from different stages of the data collection process. For example, the respondent may not know whether certain symptoms existed or may refuse to answer a question. From a statistical point of view, knowing that a symptom does not exist provides some information to the possible cause assignment, while a missing symptom does not. Although in theory, most of the VA algorithms could benefit from distinguishing ‘missing’ from ‘absence’, InSilicoVA is the only algorithm that has been implemented to acknowledge missing data. Missing indicators are assumed to be equivalent to ‘absence’ in InterVA, NBC, and Tariff.\n\n3 Data preparation\nIn the openVA package, we consider two main types of standardized questionnaire: the WHO instrument and the IHME questionnaire. In this section, we focus on describing these two data formats and tools to clean and convert data. Pre-processing the raw data collected from the survey instrument (usually with Open Data Toolkit) is usually performed with additional packages and software outside of the analysis pipeline in R. We briefly mention software for data pre-processing towards the end of this paper.\nThe WHO standard format\nFor users familiar with InterVA software and the associated data processing steps, the standard input format from the WHO 2012 and 2016 instruments is usually well understood. For the 2012 instrument, the data expected by the InterVA-4 software are organized into a data frame where each row represents one death and the corresponding VA information is contained in \\(246\\) fields, starting from the first item being the ID of the death. The \\(245\\) items following the ID each represent one binary variable of symptom/indicator, where ‘presence’ is coded by ‘Y’, and ‘absence’ is coded by an empty cell.\nTo accommodate updates for the WHO 2016 instrument (D’Ambruoso et al. 2017), the InterVA-5 software accepts a data frame with \\(354\\) columns that include \\(353\\) columns of symptom/indicators followed by an additional column for the record ID. It should be noted that the R package InterVA5 retains the format with the record ID residing in the first column. Another important update with InterVA-5 is that it acknowledges the difference between “Yes” and “No” (or “Y/y” and “N/n”, which is different from the coding scheme in InterVA-4), both of which are processed as relevant responses, while all other responses are treated as missing values and ignored. With respect to the list of causes of death, InterVA-5 utilizes the WHO 2016 COD categories, which is nearly identical to the WHO 2012 COD categories (used by InterVA-4) except that hemorrhagic fever and dengue fever are two separate categories in the 2016 COD categories.\nThe same input format is inherited by the openVA package, except for one modification. We further distinguish ‘missing’ and ‘absence’ in the input data frame explicitly. We highly recommend that users pre-process all the raw data so that a ‘missing’ value in the data spreadsheet is coded as a ‘.’ (following the Stata practice familiar to many VA practitioners), and an ‘absence’ value is indicated by an empty cell, as in the standard InterVA-4 software. For WHO 2016 data, both ‘.’ and ‘-’ (following the default coding scheme of InterVA-5 software) are interpreted as missing values. For methods other than InSilicoVA, ‘missing’ and ‘absence’ will be considered the same internally and thus will not introduce a compatibility problem.\nThe PHMRC format\nThe Population Health Metrics Research Consortium (PHMRC) gold standard VA data (Murray et al. 2011) consist of three datasets corresponding to adult, child, and neonatal deaths, respectively. All deaths occurred in health facilities and gold-standard causes are determined based on laboratory, pathology and medical imaging findings.\nThese datasets can be downloaded directly using the link returned by the function getPHMRC_url(). For example, we can read the adult VA dataset using the following command.\n\n\nPHMRC_adult <- read.csv(getPHMRC_url(\"adult\"))\n\n\nAlthough the data are publicly accessible, a major practical challenge for studies involving the PHMRC gold standard dataset is that the pre-processing steps described from the relevant publications are not clear enough nor easy to implement. The openVA package internally cleans up the PHMRC gold standard data when calling the codeVA() function on the PHMRC data. The procedure follows the steps described in the supplement material of McCormick et al. (2016). Notice that the original PHMRC data are useful for comparing and validating new methods, as well as for using as training data, but the cleaning functions only require that the columns are exactly the same as the PHMRC gold standard datasets, so they could also be used for new data that are pre-processed into the same format.\nCustomized format\nIn addition to the two standard questionnaires discussed previously, researchers might also be interested in including customized dichotomous symptoms in their analysis. The openVA package also supports customized inputs as long as they are dichotomous. In such case, neither the built-in conditional probability matrix of InterVA nor the PHMRC gold standard dataset could be used to learn the relationship between training and testing data, thus different training data with known causes of death are necessary for all three algorithms. The ConvertData() function can be used to convert data with customized coding schemes into the format recognized by the openVA package.\nFinally, we note that the openVA package currently does not reformat data from one standardized questionnaire to another. This is because mapping the symptoms collected from one questionnaire to those collected by another questionnaire inevitably creates loss of information. Such mapping tasks can be useful for some applications. For example, a full mapping of a PHMRC dataset into the WHO format enables the use of physician provided conditional probabilities included in the InterVA software on data collected by PHMRC questionnaires. This remains as an important feature to be added to the package in the future.\n4 Fitting VA cause-of-death assignment models\nIn this section, we demonstrate the model fitting process in the openVA package using two datasets: (1) a random sample of \\(1,000\\) deaths from the ALPHA network without cause-of-death labels collected with the WHO 2012 instrument, and (2) the adult VA records in the PHMRC gold standard data, with the \\(1,554\\) records from Andhra Pradesh, India used as a testing set and the rest used as a training dataset. In the first case without gold standard training data, only InterVA and InSilicoVA can be fitted. All four methods can be fitted in the second case.\nModeling data collected with WHO 2012 questionnaire\nThe randomly sampled VA records from the ALPHA network sites are already included in the openVA package as a dataset RandomVA1 and can be loaded directly.\n\n\ndata(RandomVA1)\ndim(RandomVA1)\n\n[1] 1000  246\n\nhead(RandomVA1[, 1:10])\n\n  ID elder midage adult child under5 infant neonate male female\n1 d1     Y                                             Y       \n2 d2     Y                                                    Y\n3 d3            Y                                      Y       \n4 d4                  Y                                       Y\n5 d5                  Y                                Y       \n6 d6                  Y                                       Y\n\nThe codeVA() function provides a standardized syntax to fit different VA models. Internally, the codeVA() function organizes the input data according to the specified data type, checks for incompatibility of the data and specified model, and calls the corresponding model fitting functions. It returns a classed object of the specified model class. In this example, we use version 4.03 of the InterVA software, which is the latest release of the original software compatible with the WHO 2012 instrument. Any additional model-specific parameters can be passed through the arguments of codeVA(). Here we specify the HIV and malaria prevalence levels required by the InterVA model to be ‘high’. Guidelines on how to set these parameters can be found in Byass et al. (2012).\n\n\nfit_inter_who <- codeVA(data = RandomVA1, data.type = \"WHO2012\", \n                        model = \"InterVA\", version = \"4.03\", \n                        HIV = \"h\", Malaria = \"h\")\n\n\n\n\nsummary(fit_inter_who) \n\nInterVA-4 fitted on 1000 deaths\nCSMF calculated using reported causes by InterVA-4 only\nThe remaining probabilities are assigned to 'Undetermined'\n\nTop 5 CSMFs:\n cause                     likelihood\n Undetermined              0.154     \n HIV/AIDS related death    0.122     \n Stroke                    0.072     \n Reproductive neoplasms MF 0.058     \n Pulmonary tuberculosis    0.055     \n\nWe can implement InSilicoVA method with similar syntax. We use the default parameters and run the MCMC for \\(10,000\\) iterations. Setting the auto.length argument to FALSE specifies that the algorithm does not automatically increase the length of the chain when convergence failed. In practice, we recommend setting this argument to TRUE if the algorithm displays warnings concerning MCMC convergence. The InSilicoVA algorithm is implemented using a Metropolis-Hastings within Gibbs sampler. The acceptance rate is printed as part of the message as the model samples from the posterior distribution.\n\n\nfit_ins_who <- codeVA(RandomVA1, data.type = \"WHO2012\", model = \"InSilicoVA\",\n                    Nsim = 10000, auto.length = FALSE)\n\n\n\n\nsummary(fit_ins_who) \n\nInSilicoVA Call: \n1000 death processed\n10000 iterations performed, with first 5000 iterations discarded\n 250 iterations saved after thinning\nFitted with re-estimated conditional probability level table\nData consistency check performed as in InterVA4\n\nTop 10 CSMFs:\n                                   Mean Std.Error Lower Median Upper\nOther and unspecified infect dis  0.266    0.0168 0.235  0.265 0.301\nHIV/AIDS related death            0.102    0.0091 0.085  0.102 0.119\nRenal failure                     0.101    0.0108 0.084  0.101 0.123\nOther and unspecified neoplasms   0.062    0.0089 0.046  0.061 0.080\nOther and unspecified cardiac dis 0.058    0.0076 0.044  0.058 0.075\nDigestive neoplasms               0.050    0.0077 0.033  0.050 0.065\nAcute resp infect incl pneumonia  0.048    0.0073 0.034  0.049 0.063\nPulmonary tuberculosis            0.039    0.0068 0.025  0.039 0.054\nStroke                            0.038    0.0061 0.027  0.038 0.052\nOther and unspecified NCD         0.034    0.0089 0.018  0.034 0.052\n\nModeling the PHMRC data\nIn the second example, we consider a prediction task using the PHMRC adult dataset. We first load the complete PHMRC adult dataset from its on-line repository, and organize it into training and test datasets. We treat all deaths from Andhra Pradesh, India as the test dataset.\n\n\nPHMRC_adult <- read.csv(getPHMRC_url(\"adult\"))\nis.test <- which(PHMRC_adult$site == \"AP\")\ntest <- PHMRC_adult[is.test, ]\ntrain <- PHMRC_adult[-is.test, ]\ndim(test)\n\n[1] 1554  946\n\ndim(train)\n\n[1] 6287  946\n\nIn order to fit the models on the PHMRC data, we specify data.type = \"PHMRC\" and phmrc.type = \"adult\" to indicate the data input is collected using the PHMRC adult questionnaire. We also specify the column of the causes-of-death label in the training data. The rest of the syntax is similar to the previous example.\nWhen the input consists of both training and testing data, the InterVA and InSilicoVA algorithms estimate the conditional probabilities of symptoms using the training data, instead of using the built-in values. In such case, the version argument for the InterVA algorithm is suppressed. There are several ways to map the conditional probabilities of symptoms given causes in the training dataset to a letter grade system, specified by the convert.type argument. The convert.type = \"quantile\" performs the mapping so that the percentile of each rank stays the same as the original \\(P_{s|c}\\) matrix in InterVA software. Alternatively we can also use the original fixed values of translation, and assign letter grades closest to each entry in \\(\\hat{P}_{s|c}\\). This conversion is specified by convert.type = \"fixed\", and is more closely aligned to the original InterVA and InSilicoVA setting. Finally, we can also directly use the values in the \\(\\hat{P}_{s|c}\\) without converting them to ranks and re-estimating the values associated with each rank. This can be specified by convert.type = \"empirical\". In this demonstration, we assume the fixed value conversion.\n\n\nfit_inter <- codeVA(data = test, data.type = \"PHMRC\", model = \"InterVA\", \n                     data.train = train, causes.train = \"gs_text34\", \n                     phmrc.type = \"adult\", convert.type = \"fixed\")\n\n\n\n\nfit_ins <- codeVA(data = test, data.type = \"PHMRC\", model = \"InSilicoVA\",\n                    data.train = train, causes.train = \"gs_text34\", \n                    phmrc.type = \"adult\", convert.type = \"fixed\", \n                    Nsim=10000, auto.length = FALSE)\n\n\nThe NBC and Tariff method can be fit using similar syntax.\n\n\nfit_nbc <- codeVA(data = test, data.type = \"PHMRC\", model = \"NBC\", \n                   data.train = train, causes.train = \"gs_text34\", \n                   phmrc.type = \"adult\")\n\n\n\n\nfit_tariff <- codeVA(data = test, data.type = \"PHMRC\", model = \"Tariff\",\n                     data.train = train, causes.train = \"gs_text34\", \n                     phmrc.type = \"adult\")\n\n\nNotice that we do not need to transform the PHMRC data manually. Data transformations are performed automatically within the codeVA() function.\n5 Summarizing results\nIn this section we demonstrate how to summarize results, extract output, and visualize and compare fitted results. All the fitted object returned by codeVA() are S3 objects, for which a readable summary of model results can be obtained with the summary() function as shown in the previous section. In addition, several other metrics are commonly used to evaluate and compare VA algorithms at either the population or individual levels. In the rest of this section, we show how to easily calculate and visualize some of these metrics with the openVA package.\nCSMF accuracy\nWe can extract the CSMFs directly using the getCSMF() function. The function returns a vector of the point estimates of the CSMFs, or a matrix of posterior summaries of the CSMF for the InSilicoVA algorithm.\n\n\ncsmf_inter <- getCSMF(fit_inter)\ncsmf_ins <- getCSMF(fit_ins)\ncsmf_nbc <- getCSMF(fit_nbc)\ncsmf_tariff <- getCSMF(fit_tariff)\n\n\nOne commonly used metric to evaluate the CSMF estimates is the so-called CSMF accuracy, defined as\n\\[\nCSMF_{acc} = 1 - \\frac{\\sum_j^C CSMF_i - CSMF_j^{(true)}}{2(1 - \\min CSMF^{(true)})}\n\\]\nThe CSMF accuracy can be readily computed using functions in openVA as the codes below shows.\n\n\ncsmf_true <- table(c(test$gs_text34, unique(PHMRC_adult$gs_text34))) - 1\ncsmf_true <- csmf_true / sum(csmf_true)\nc(getCSMF_accuracy(csmf_inter, csmf_true, undet = \"Undetermined\"), \n  getCSMF_accuracy(csmf_ins[, \"Mean\"], csmf_true), \n  getCSMF_accuracy(csmf_nbc, csmf_true), \n  getCSMF_accuracy(csmf_tariff, csmf_true))\n\n[1] 0.53 0.74 0.77 0.68\n\nWe use the empirical distribution in the test data to calculate the true CSMF distribution, i.e., \\(CSMF_j^{(true)} = \\frac{1}{n}\\sum_{i=1}^n {1}_{y_i = j}\\). Then we evaluate the CSMF accuracy using the getCSMF_accuracy() function. As discussed previously, the default CSMF calculation is slightly different for diCfferent methods. For example, the InterVA algorithm creates the additional category of Undetermined by default, which is not in the true CSMF categories and needs to be specified. The creation of the undetermined category can also be suppressed by interVA.rule = FALSE in the getCSMF() function call. For the InSilicoVA algorithm, we use the posterior mean to calculate the point estimates of the CSMF accuracy.\nIndividual COD summary\nAt the individual level, we can extract the most likely cause-of-death assignment from the fitted object using the getTopCOD() function.\n\n\ncod_inter <- getTopCOD(fit_inter)\ncod_ins <- getTopCOD(fit_ins)\ncod_nbc <- getTopCOD(fit_nbc)\ncod_tariff <- getTopCOD(fit_tariff)\n\n\nWith the most likely COD assignment, other types of metrics based on individual COD assignment accuracy can be similarly constructed by users. The summary methods can also be called for each death ID. For example, using the Tariff method, we can extract the fitted rankings of causes for the death with ID 6288 by\n\n\nsummary(fit_inter, id = \"6288\")\n\nInterVA-4 fitted top 5 causes for death ID: 6288\n\n Cause                     Likelihood\n Stroke                    0.509     \n Pneumonia                 0.318     \n COPD                      0.081     \n Other Infectious Diseases 0.064     \n Renal Failure             0.013     \n\nThe summary() function for InSilcoVA does not provide uncertainty estimates for individual COD assignments by default. This is because, in practice, the calculation of individual posterior probabilities of COD distribution can be memory-intensive when the dataset is large. To obtain individual-level uncertainty measurements, we can either run the MCMC chain with the additional argument indiv.CI = 0.95 when calling codeVA(), or update the fitted object directly with the saved posterior draws.\n\n\nfit_ins <- updateIndiv(fit_ins, CI = 0.95)\nsummary(fit_ins, id = \"6288\")\n\nInSilicoVA fitted top  causes for death ID: 6288\nCredible intervals shown: 95%\n                              Mean  Lower Median  Upper\nStroke                      0.5043 0.3485 0.5083 0.6361\nPneumonia                   0.4116 0.2615 0.4083 0.5834\nOther Infectious Diseases   0.0660 0.0411 0.0642 0.0966\nEpilepsy                    0.0099 0.0064 0.0097 0.0142\nCOPD                        0.0053 0.0031 0.0052 0.0079\nMalaria                     0.0007 0.0005 0.0007 0.0011\nDiabetes                    0.0005 0.0003 0.0005 0.0009\nAcute Myocardial Infarction 0.0004 0.0003 0.0004 0.0006\nFalls                       0.0004 0.0001 0.0004 0.0013\nRenal Failure               0.0004 0.0002 0.0003 0.0005\n\nFor \\(N\\) deaths, \\(C\\) causes, the posterior mean of individual COD distributions returned by the InSilicoVA model, along with median and with credible intervals can be represented by a \\((N \\times C \\times 4)\\)-dimensional array. The function getIndivProb() extracts this summary in the form of a list of \\(4\\) matrices of dimension \\(N\\) by \\(C\\), which can then be saved to other formats to facilitate further analysis. For other methods, the point estimates of individual COD distribution are returned as the \\(N\\) by \\(C\\) matrix.\n\n\nfit_prob <- getIndivProb(fit_inter)\ndim(fit_prob)\n\n[1] 1554   34\n\nVisualization\nThe previous sections discuss how results could be extracted and examined in R. In this subsection, we show some visualization tools provided in the openVA package for presenting these results. The fitted CSMFs for the top causes can be easily visualized by the plotVA() function. The default graph components are specific to each algorithm and individual package implementations, with options for further customization. For example, Figure[1] shows the estimated CSMF from the InterVA algorithm in the PHMRC data example.\n\n\nplotVA(fit_inter, title = \"InterVA\")\n\n\n\nFigure 1: Bar plot of the top CSMFs estimated by InterVA. The five causes of death with the highes estimated mortality fraction predicted by InterVA are plotted. Colors of the bar indicate the corresponding fractions.\n\n\n\nThe CSMFs can also be aggregated for easier visualization of groups of causes. For the InterVA-4 cause list, we included an example grouping built into the package, so the aggregated CSMFs can be compared directly. In practice, the grouping of causes of deaths often needs to be determined according to context and the research question of interest. Changing the grouping can be easily achieved by modifying the grouping argument in the stackplotVA() function. For example, to facilitate the new category of Undetermined returned by InterVA, we first modify the grouping matrix to include it as a new cause and visualize the aggregated CSMF estimates in Figure [2].\n\n\ndata(SampleCategory)\ngrouping <- SampleCategory\ngrouping[,1] <- as.character(grouping[,1])\ngrouping <- rbind(grouping, c(\"Undetermined\", \"Undetermined\"))\ncompare <- list(InterVA4 = fit_inter_who,\n                InSilicoVA = fit_ins_who)\nstackplotVA(compare, xlab = \"\", angle = 0,  grouping = grouping)\n\n\n\nFigure 2: Estimated aggregated CSMFs for InterVA-4 and InSilicoVA, adding undetermined category. The causes of death are aggregated to seven broad categories. Color indicate the category and the height of bar indicate the fraction of deaths due to the cause category. Overall, InterVA-4 and InSilicoVA estimate similar aggregated CSMFs, except in the communicable disease (excluding TA/AIDS) category, InSilicoVA estimates a larger fraction.\n\n\n\nThe ordering of the stacked bars can also be changed to reflect the structures within the aggregated causes, as demonstrated in Figure [3].\n\n\ngroup_order <- c(\"TB/AIDS\",  \"Communicable\", \"NCD\", \"External\", \"Maternal\",\n            \"causes specific to infancy\", \"Undetermined\") \nstackplotVA(compare, xlab = \"\", angle = 0, grouping = grouping, \n            group_order = group_order)\n\n\n\nFigure 3: Estimated aggregated CSMFs for InterVA-4 and InSilicoVA, with the causes reordered. The causes of death are aggregated to seven broad categories. Color indicate the category and the height of bar indicate the fraction of deaths due to the cause category. The order of the bars are arranged according to user input.\n\n\n\n6 Incorporating additional information\nAmong the VA methods discussed in this paper, the InSilicoVA algorithm (McCormick et al. 2016) allows for more flexible modifications to the Bayesian hierarchical model structure when additional information is available. In this section, we illustrate two features unique to the InSilicoVA method: jointly estimating CSMFs from multiple populations, and incorporating partial and potentially noisy physician coding into the algorithm.\nSub-population specific CSMFs\nIn practice researchers may want to estimate and compare CSMFs for different regions, time periods, or demographic groups in the population. Running separate models on subsets of data can be inefficient and does not allow parameter estimation to borrow information across different groups. The generative framework adopted by InSilicoVA allows the specification of sub-populations in analyzing VA data. Consider an input dataset with \\(G\\) different sub-populations. We can estimate different CSMFs \\(\\pi^{(g)}\\) for \\(g = 1, ..., G\\) for each sub-population, while assuming the same conditional probability matrix, \\(P_{s|c}\\) and other hyperpriors. As an example, we show how to estimate different CSMFs for sub-populations specified by sex and age groups, using a randomly sampled ALPHA dataset with additional columns specifying the sub-population each death belongs to.\n\n\ndata(RandomVA2)\nhead(RandomVA2[, 244:248])\n\n  stradm smobph scosts   sex age\n1      .      .      .   Men 60+\n2      .      .      . Women 60-\n3      .      .      . Women 60-\n4      .      .      . Women 60+\n5      .      .      . Women 60-\n6      .      .      . Women 60-\n\nThen we can fit the model with one or multiple additional columns specifying sub-population membership for each observation.\n\n\nfit_sub <- codeVA(RandomVA2, model = \"InSilicoVA\", \n              subpop = list(\"sex\", \"age\"),  indiv.CI = 0.95,\n              Nsim = 10000, auto.length = FALSE)\n\n\nFunctions discussed in the previous sections work in the same way for the fitted object with multiple sub-populations. Additional visualization tools are also available. Figure [4] plots the CSMFs for two sub-populations on the same plot by specify type = \"compare\".\n\n\nplotVA(fit_sub, type = \"compare\", title = \"Comparing CSMFs\", top = 3)\n\n\n\nFigure 4: Estimated CSMFs for different sub-populations. The points indicate posterior means of the CSMF and the error bars indicate 95% credible intervals of the CSMF.\n\n\n\nBy default, the comparison plots will select all the CODs that are included in the top causes (specified by the top argument) for each of the sub-populations. We can also plot only subsets of them by specifying the causes of interest, as shown in Figure [5].\n\n\nplotVA(fit_sub, type = \"compare\", title = \"Comparing CSMFs\",\n                causelist = c(\"HIV/AIDS related death\", \n                              \"Pulmonary tuberculosis\", \n                              \"Other and unspecified infect dis\", \n                              \"Other and unspecified NCD\"))\n\n\n\nFigure 5:  Estimated fraction of deaths due to selected CODs for different sub-populations. The points indicate posterior means of the CSMF and the error bars indicate 95% credible intervals of the CSMF.\n\n\n\nFigure [6] shows the visualization of the top CSMFs for a chosen sub-population using the which.sub argument.\n\n\nplotVA(fit_sub, which.sub = \"Women 60-\", title = \"Women 60-\")\n\n\n\nFigure 6: Top 10 CSMFs for a specified sub-population (women under \\(60\\) years old).\n\n\n\nSimilar to before, the stackplotVA() function can also be used to compare different sub-populations in aggregated cause groups, as shown in Figure [7].\n\n\nstackplotVA(fit_sub)\nstackplotVA(fit_sub, type = \"dodge\")\n\n\n\nFigure 7: Aggregated CSMFs for four different sub-populations. Left: comparison using the stacked bar chart. Right: comparison using the bar chart arranged side to side. The height of the bars indicate posterior means of the CSMF and the error bars indicate 95% credible intervals of the CSMF. Sub-populations corresponding to 60+ age groups are estimated to have a larger fraction of deaths due to NCD compared to 60- age groups.\n\n\n\nPhysician coding\nWhen physician-coded causes of death are available for all or a subset of the deaths, we can incorporate such information in the InSilicoVA model. The physician-coded causes can be either the same as the CODs used for the final algorithm, or consist of causes at a higher level of aggregation. When there is more than one physician code for each death and the physician identity is known, we can first de-bias the multiple codes provided from different physicians using the process described in McCormick et al. (2016). For the purpose of implementation, we only need to specify which columns are physician IDs, and which are their coded causes, respectively.\n\n\ndata(SampleCategory)\ndata(RandomPhysician)\nhead(RandomPhysician[, 245:250])\n\n  smobph scosts        code1 rev1        code2 rev2\n1      .      .          NCD doc9          NCD doc6\n2      .      .          NCD doc4          NCD doc3\n3      .      .          NCD doc1          NCD doc5\n4      .      .      TB/AIDS doc4      TB/AIDS doc7\n5      .      .      TB/AIDS doc5      TB/AIDS doc9\n6      .      . Communicable doc9 Communicable <NA>\n\n\n\ndoctors <- paste0(\"doc\", c(1:15))\ncauselist <- c(\"Communicable\", \"TB/AIDS\", \"Maternal\",\n              \"NCD\", \"External\", \"Unknown\")\nphydebias <- physician_debias(RandomPhysician, \n          phy.id = c(\"rev1\", \"rev2\"), phy.code = c(\"code1\", \"code2\"), \n          phylist = doctors, causelist = causelist, tol = 0.0001, max.itr = 100)\n\n\nThe de-biased step essentially creates a prior probability distribution for each death over the broader categories of causes. Then to run InSilicoVA with the de-biased physician coding, we can simply pass the fitted object from the previous step to the model. Additional arguments are needed to specify both the external cause category, since external causes are handled by separate heuristics, and the unknown category, which is equivalent to a uniform probability distribution over all other categories, i.e., the same as the case where no physician coding exists.\n\n\nfit_ins_phy <- codeVA(RandomVA1, model = \"InSilicoVA\",\n             phy.debias = phydebias, phy.cat = SampleCategory, \n             phy.external = \"External\", phy.unknown = \"Unknown\",\n             Nsim = 10000, auto.length = FALSE) \n\n\nFigure [8] compares the previous results without including physicians codes.\n\n\nplotVA(fit_ins_who, title = \"Without physician coding\")\nplotVA(fit_ins_phy, title = \"With physician coding\")\n\n\n\nFigure 8: Comparing fitted CSMF with and without physicians. Top: top 10 CSMFs and the 95% credible intervals estimated by InSilicoVA without using physician coding. Bottom: top 10 CSMFs and the 95% credible intervals estimated by InSilicoVA taking physician coding into account. More deaths were assigned from ‘Other and unspecified infectious disease’ to ‘HIV/AIDS related death’ after accounting for physician coding.\n\n\n\nRemoval of physically impossible causes\nThe originally proposed InSilicoVA assumes all causes of death are possible for each observation. The impact from such an assumption is mild when data are abundant, but could be problematic when either the sample size is small or the proportion of missing data is high. In both cases, physically impossible causes might get assigned with non-ignorable posterior mass. Since version 1.1.5 of InSilicoVA, when the input is in the WHO format, the algorithm automatically checks and removes impossible causes before fitting the model. The \\(k\\)-th cause is defined as physically impossible for the \\(i\\)-th death if \\(P(s_{ij}=1 | y_{i}=k) = 0\\) where \\(s_{ij}\\) is an indicator that the decedent belongs to a particular sex or age group. We then consider a cause to be physically impossible for the underlying population if it is impossible for all the observations in the input data. For example, with the new implementation, CSMF for pregnancy-related causes will not be estimated if the input data consist of only male deaths.\n7 Other related software packages\n\nSince the release of the openVA package on CRAN, there have been several new developments in both methodology and software that build on the openVA suite of packages and further extend its functionalities. Here we briefly survey some of the related methods, software packages, and ongoing work in the VA community that are related to openVA.\nFirst, the ability to easily fit and compare existing methods using the openVA package facilitated the development of several new VA methods in the last several years. Most of the development focuses on building statistical models to relax the conditional independence assumption of symptoms given a cause of death (e.g., Li et al. 2020; Kunihama et al. 2020; Li et al. 2021; Moran et al. 2021; Wu et al. 2021). These methods tend to be computationally more demanding compared to the algorithms currently included in the openVA package, but usually provide improved inference. It is future work to include some of these latest developments in the openVA package for routine use. Most of these methods have publicly available implementations, such as the farva package (Moran 2020) on GitHub. In another direction of research, a class of transfer learning methods focuses on building models to correct for bias in existing methods when predicting out of domain. These methods take the predicted cause of death assignments and distributions obtained with the openVA package and learn an ensemble of the predictions calibrated to the target population (Datta et al. 2020; Fiksel et al. 2021). The calibratedVA package (Fiksel and Datta 2018) is available to implement these models.\nOutside of research community that develops new VA algorithms, openVA has also been used extensively by governments and health care organizations, particularly in locations that lack a strong vital registration system and use VA data to identify the leading causes of death. To facilitate the work of these users, openVA has been wrapped into a web application, the openVA App (Thomas 2021), using the shiny package (Chang et al. 2021). The open source openVA App is available on GitHub and provides an intuitive interface to openVA that does not require one to learn R, but provides visual and tabular output produced by the different VA algorithms. It also runs the official Tariff algorithm by calling the Python source code of SmartVA-Analyze (Institute for Health Metrics and Evaluation 2021a) and processing the output to be consistent with the other algorithms. The openVA R package has also been a key component in a larger data analysis pipeline that pulls VA data from an Open Data Kit (ODK) Collect server and deposits the assigned causes of death to another server running the District Health Information Software 2 (DHIS2), which is a common information management system used in low and middle-income countries. This open-source software is implemented as a Python package, openva-pipeline, and is available on GitHub and the Python Package Index (Thomas et al. 2021a). Finally, the R package CrossVA (Thomas et al. 2020) and the Python package pyCrossVA (Choi et al. 2021) provide additional toolkits to convert raw VA data from its original format from the ODK server to the standardized formats discussed before. Both packages are open source and available on GitHub. The pyCrossVA package is also available on the Python Package Index.\n8 Conclusion\nIn this paper, we introduce the openVA package. This is the first open-source software that implements and compares the major VA methods. The openVA package allows researchers to easily access the most recent tools that were previously difficult to work with or unavailable. It also enables the compatibility of multiple data input formats and significantly reduces the tedious work to pre-process different data formats specific to each algorithm. The software framework of the openVA package allows for the integration of new methods developed in the future. The openVA package makes all the steps involved in analyzing VA data – i.e., data processing, model tuning and fitting, summarizing results, and evaluation metrics – transparent and reproducible. This contributes significantly to the public health community using VA.\nFinally, we make note of several features that will be helpful for future development. First, many users of the openVA package may not be familiar with the command line tools or do not have access to R on their local machines. A well designed graphical user interface can be very useful in such settings. The work on the shiny web application, openVA app, is a first step towards making the package more accessible. The authors intend to extend it to a better front end hosted on secure centralized servers. Second, although we aim to provide users with all the available methods for assigning causes of death, there is a lack of tools for comparing the accuracy and robustness between algorithms. Thus, much future work is needed to systematically assess, compare, and combine these methods in a better analytic framework. Finally, the development of VA algorithms is still an active area of research and it would be possible to extend the openVA suite to incorporate better VA algorithms and new types of data such as free-text narratives.\n9 Acknowledgement\nThis work was supported by grants K01HD078452, R01HD086227, and R21HD095451 from the Eunice Kennedy Shriver National Institute of Child Health and Human Development (NICHD). Funding was also received from the Data for Health Initiative, a joint project of Vital Strategies, the CDC Foundation and Bloomberg Philanthropies, through Vital Strategies. The views expressed are not necessarily those of the initiative.\n\nCRAN packages used\nopenVA, InterVA4, InterVA5, InSilicoVA, Tariff, nbc4va, shiny, CrossVA\nCRAN Task Views implied by cited packages\nWebTechnologies\n\n\nR. F. Breiman, D. M. Blau, P. Mutevedzi, V. Akelo, I. Mandomando, I. U. Ogbuanu, S. O. Sow, L. Madrid, S. El Arifeen, M. Garel, et al. Postmortem investigations and identification of multiple causes of child deaths: An analysis of findings from the Child Health and Mortality Prevention Surveillance (CHAMPS) network. PLoS medicine, 18(9): e1003814, 2021.\n\n\nP. Byass. InterVA-4 Software [Windows Executable]. www.interva.net, 2015.\n\n\nP. Byass, D. Chandramohan, S. J. Clark, L. D’Ambruoso, E. Fottrell, W. J. Graham, A. J. Herbst, A. Hodgson, S. Hounton, K. Kahn, et al. Strengthening standardised interpretation of verbal autopsy data: The new InterVA-4 tool. Global Health Action, 5: 2012.\n\n\nP. Byass, L. Hussain-Alkhateeb, L. D’Ambruoso, S. Clark, J. Davies, E. Fottrell, J. Bird, C. Kabudula, S. Tollman, K. Kahn, et al. An integrated approach to processing WHO-2016 verbal autopsy data: The InterVA-5 model. BMC Medicine, 17(1): 1–12, 2019.\n\n\nD. Chandramohan, E. Fottrell, J. Leitao, E. Nichols, S. J. Clark, C. Alsokhn, D. C. Munoz, C. AbouZahr, A. D. Pasquale, R. Mswia, et al. Estimating causes of death where there is no medical certification: Evolution and state of the art of verbal autopsy. Global Health Action, 14(sup1): 1982486, 2021. URL\nhttps://doi.org/10.1080/16549716.2021.1982486\n. PMID: 35377290.\n\n\nW. Chang, J. Cheng, J. Allaire, C. Sievert, B. Schloerke, Y. Xie, J. Allen, J. McPherson, A. Dipert and B. Borges. Shiny: Web application framework for r. 2021. URL https://CRAN.R-project.org/package=shiny. R package version 1.7.1.\n\n\nE. Choi, J. Thomas and E. Karpinski. Prepare data from WHO and PHRMC instruments for verbal autopsy algorithms. 2021.\n\n\nL. D’Ambruoso, T. Boerma, P. Byass, E. Fottrell, K. Herbst, K. Kallander and Z. Mullan. The case for verbal autopsy in health systems strengthening. LANCET GLOBAL HEALTH, 5(1): E20–E21, 2017.\n\n\nA. Datta, J. Fiksel, A. Amouzou and S. L. Zeger. Regularized bayesian transfer learning for population-level etiological distributions. Biostatistics, ISSN 1465–4644, 2020.\n\n\nJ. Fiksel and A. Datta. calibratedVA: Locally calibrated cause specific mortality fractions using verbal autopsy data. 2018. URL https://github.com/jfiksel/CalibratedVA.\n\n\nJ. Fiksel, A. Datta, A. Amouzou and S. Zeger. Generalized Bayesian quantification learning. Journal of the American Statistical Association, 1–19, 2021.\n\n\nA. D. Flaxman, A. Vahdatpour, S. Green, S. L. James and C. J. Murray. Random forests for verbal autopsy analysis: Multisite validation study using clinical diagnostic gold standards. Population health metrics, 9(1): 1–11, 2011.\n\n\nM. Garenne. Prospects for automated diagnosis of verbal autopsies. BMC Medicine, 12(1): 18, 2014.\n\n\nInstitute for Health Metrics and Evaluation. SmartVA-Analyze desktop application. 2021a.\n\n\nInstitute for Health Metrics and Evaluation. Verbal autopsy tools. 2021b.\n\n\nS. L. James, A. D. Flaxman, C. J. Murray and Consortium Population Health Metrics Research. Performance of the tariff method: Validation of a simple additive algorithm for analysis of verbal autopsies. Population Health Metrics, 9(31): 2011.\n\n\nS. Jeblee, M. Gomes, P. Jha, F. Rudzicz and G. Hirst. Automatically determining cause of death from verbal autopsy narratives. BMC medical informatics and decision making, 19(1): 1–13, 2019.\n\n\nK. Kahn, M. A. Collinson, F. X. Gómez-Olivé, O. Mokoena, R. Twine, P. Mee, S. A. Afolabi, B. D. Clark, C. W. Kabudula, A. Khosa, et al. Profile: Agincourt health and socio-demographic surveillance system. International Journal of Epidemiology, 41(4): 988–1001, 2012.\n\n\nT. Kunihama, Z. R. Li, S. J. Clark and T. H. McCormick. Bayesian factor models for probabilistic cause of death assessment with verbal autopsies. The Annals of Applied Statistics, 14(1): 241–256, 2020.\n\n\nZ. R. Li, T. H. McCormick and S. J. Clark. : Probabilistic verbal autopsy coding with ’InSilicoVA’ algorithm. 2022a. URL https://CRAN.R-project.org/package=InSilicoVA. R package version 1.4.0.\n\n\nZ. R. Li, T. H. McCormick and S. J. Clark. : Replicate tariff method for verbal autopsy. 2018. URL https://CRAN.R-project.org/package=Tariff. R package version 1.0.5.\n\n\nZ. R. Li, T. H. McCormick and S. J. Clark. InterVA4: An R package to analyze verbal autopsy data. Center for Statistics and the Social Sciences Working Paper, No.146, 2014.\n\n\nZ. R. Li, T. H. McCormick and S. J. Clark. Using Bayesian latent Gaussian graphical models to infer symptom associations in verbal autopsies. Bayesian Analysis, 15(3): 781, 2020.\n\n\nZ. R. Li, T. H. McCormick, S. J. Clark and P. Byass. : Replicate and analyse ’InterVA4’. 2019. URL https://CRAN.R-project.org/package=InterVA4. R package version 1.7.6.\n\n\nZ. R. Li, J. Thomas, T. McCormick and S. Clark. : Automated method for verbal autopsy. 2022b. URL https://CRAN.R-project.org/package=openVA. R package version 1.1.0.\n\n\nZ. R. Li, Z. Wu, I. Chen and S. J. Clark. Bayesian nested latent class models for cause-of-death assignment using verbal autopsies across multiple domains. arXiv preprint arXiv:2112.12186, 2021.\n\n\nD. Maher, S. Biraro, V. Hosegood, R. Isingo, T. Lutalo, P. Mushati, B. Ngwira, M. Nyirenda, J. Todd and B. Zaba. Translating global health research aims into action: The example of the ALPHA network. Tropical Medicine & International Health, 15(3): 321–328, 2010.\n\n\nT. H. McCormick, Z. R. Li, C. Calvert, A. C. Crampin, K. Kahn and S. J. Clark. Probabilistic cause-of-death assignment using verbal autopsies. Journal of the American Statistical Association, 111(515): 1036–1049, 2016.\n\n\nP. Miasnikof, V. Giannakeas, M. Gomes, L. Aleksandrowicz, A. Y. Shestopaloff, D. Alam, S. Tollman, A. Samarikhalaj and P. Jha. Naive bayes classifiers for verbal autopsies: Comparison to physician-based classification for 21,000 child and adult deaths. BMC Medicine, 13(1): 1, 2015.\n\n\nK. R. Moran. FActor regression for verbal autopsy. GitHub repository, 2020.\n\n\nK. R. Moran, E. L. Turner, D. Dunson and A. H. Herring. Bayesian hierarchical factor regression models to infer cause of death from verbal autopsy data. Journal of the Royal Statistical Society: Series C (Applied Statistics), 2021.\n\n\nC. J. Murray, A. D. Lopez, R. Black, R. Ahuja, S. M. Ali, A. Baqui, L. Dandona, E. Dantzer, V. Das, U. Dhingra, et al. Population Health Metrics Research Consortium gold standard verbal autopsy validation study: Design, implementation, and development of analysis datasets. Population Health Metrics, 9(1): 27, 2011.\n\n\nC. J. Murray, R. Lozano, A. D. Flaxman, P. Serina, D. Phillips, A. Stewart, S. L. James, A. Vahdatpour, C. Atkinson, M. K. Freeman, et al. Using verbal autopsy to measure causes of death: The comparative performance of existing methods. BMC medicine, 12(1): 1–19, 2014.\n\n\nJ. Nkengasong, E. Gudo, I. Macicame, X. Maunze, A. Amouzou, K. Banke, S. Dowell and I. Jani. Improving birth and death data for African decision making. The Lancet Global Health, 8(1): e35–e36, 2020.\n\n\nO. Sankoh and P. Byass. The INDEPTH network: Filling vital gaps in global epidemiology. International Journal of Epidemiology, 41(3): 2012.\n\n\nP. Serina, I. Riley, A. Stewart, S. L. James, A. D. Flaxman, R. Lozano, B. Hernandez, M. D. Mooney, R. Luning, R. Black, et al. Improving performance of the tariff method for assigning causes of death to verbal autopsies. BMC Medicine, 13(1): 1, 2015.\n\n\nC. E. Taylor, R. Parker, W. Reinke and R. Faruquee. Child and maternal health services in rural india. The narangwal experiment. 2. Integrated family planning and health care. Johns Hopkins University Press, 1983.\n\n\nJ. Thomas. Software for automating the processing of verbal autopsy data. 2021.\n\n\nJ. Thomas, E. Choi, Z. Li, N. Maire, T. McCormick, P. Byass and S. Clark. : Verbal autopsy data transformation for InSilicoVA and InterVA5 algorithms. 2020. URL https://CRAN.R-project.org/package=CrossVA. R package version 1.0.0.\n\n\nJ. Thomas, S. J. Clark and M. W. Bratschi. Software for automating the processing of verbal autopsy data. 2021a.\n\n\nJ. Thomas, Z. Li, P. Byass, T. McCormick, M. Boyas and S. Clark. InterVA5: Replicate and analyse ’InterVA5’. 2021b. URL https://CRAN.R-project.org/package=InterVA5. R package version 1.1.3.\n\n\nR. Wen, P. Miasnikof, V. Giannakeas and M. Gomes. : Bayes classifier for verbal autopsy data. 2022. URL https://CRAN.R-project.org/package=nbc4va. R package version 1.2.\n\n\nWorld Health Organization. Verbal autopsy standards: The 2012 WHO verbal autopsy instrument release candidate 1. Geneva: World Health Organization (WHO). 2012.\n\n\nZ. Wu, Z. R. Li, I. Chen and M. Li. Tree-informed Bayesian multi-source domain adaptation: Cross-population probabilistic cause-of-death assignment using verbal autopsy. arXiv preprint arXiv:2112.10978, 2021.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:41+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-001/",
    "title": "knitrdata: A Tool for Creating Standalone Rmarkdown Source Documents",
    "description": "Though Rmarkdown is a powerful tool for integrating text with code for analyses in a single source document exportable to a variety of output formats, until now there has been no simple way to integrate the data behind analyses into Rmarkdown source documents. The `knitrdata` package makes it possible for arbitrary text and binary data to be integrated directly into Rmarkdown source documents via implementation of a new `data` chunk type. The package includes command-line and graphical tools that facilitate creating and inserting `data` chunks into Rmarkdown documents, and the treatment of `data` chunks is highly configurable via chunk options. These tools allow one to easily create fully standalone Rmarkdown source documents integrating data, ancillary formatting files, analysis code and text in a single file. Used properly, the package can facilitate open distribution of source documents that demonstrate computational reproducibility of scientific results.",
    "author": [
      {
        "name": "David M. Kaplan",
        "url": "https://www.davidmkaplan.fr"
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\n1 Introduction\nThe basic principles of open science are that the data, research methodologies and analysis tools (e.g., the specific computational tools) used for scientific research should be made publicly available so that others can confirm and validate scientific analyses. Open science is particularly important for studies and disciplines for which true experimental replication is often difficult or impossible due to spatial, temporal or individual specificity [e.g., we cannot replicate Earth; Powers and Hampton (2019)]. In these cases, computational reproducibility, i.e., the ability to reproduce analytic results given the original data and analysis code, can still be achieved and can provide significant credibility to results (Powers and Hampton 2019). Though scientists, governments and journals often place great emphasis on access to raw data (Cassey and Blackburn 2006; Lowndes et al. 2017), it is important to remember that computational reproducibility can only be assured if data, methods, computational tools and the relationships between these are all openly accessible. Even when data are made publicly available, there are often significant gaps between the Methods section of a publication and the raw data that complicate reproducibility without access to the detailed code used to generate results. It is, therefore, essential for computational reproducibility that the code used to generate results be distributed along with the data and the publication itself. Though there are a number of potential ways to distribute all these elements together, probably the most common current approach is to place the data in a publicly accessible data store (e.g., Dryad) and to associate the code with the publication via the supplementary material and/or by including it in the data store. Though this is a perfectly viable approach that can greatly enhance transparency of research, it physically separates data from analysis code and interpretation of results, potentially leading to confusion and/or loss of information regarding how these different element interrelate. At times, it would be more convenient, transparent and/or effective to join all the elements into a single document. The R package presented here, knitrdata (Kaplan 2020a), provides tools for doing just that - integrating data directly into Rmarkdown source documents so that data, code for analyses and text interpreting results are all available in a single file.\nRmarkdown (Allaire et al. 2022b) has become an increasingly popular tool for generating rich scientific documents while maintaining a source document that makes explicit the relationship between text and specific analyses used to produce results (Xie 2014; Lowndes et al. 2017). In a nutshell, Rmarkdown source documents are text documents comprised of two major elements: structured text that make up the headings and paragraphs of the document, and blocks of code (typically, but not exclusively, R code) for doing analyses and generating figures and tables. Rmarkdown source documents can be processed into a variety of final output formats, including PDF documents formatted for scientific publication. During this processing, the blocks of code in the source document are executed and used to augment the final output document with figures, tables and analytic results. In addition to providing a single source document that includes both written text and code for carrying out analyses, Rmarkdown has other benefits for open science, such as requiring the user to provide fully functioning code that runs from start to end without errors and facilitating reuse and updating of documents when new data arrives.\nUntil now, however, it has been difficult to integrate the raw data itself that are the bases for analyses directly into Rmarkdown source documents. Typically, data are placed in external files that are accessed via R code contained in the Rmarkdown source document that is executed during the knitting. As previously mentioned, this has the disadvantage of physically separating data from analysis code and text contained in the Rmarkdown source document, potentially leading to confusion and/or information loss. Furthermore, on a practical level, it often can be extremely convenient to merge all pertinent information into a single source document (e.g., to facilitate collaboration on an Rmarkdown source document). knitrdata provides a simple mechanism for integrating arbitrary text or binary data directly into Rmarkdown source documents, thereby allowing one to create standalone source documents that include all the elements necessary for conducting analyses. This integration is done with minimal additional formatting of the data (e.g., allowing one to insert comma-separated value (CSV) data without escaping quotation marks directly into Rmarkdown documents) and in a way that clearly visually separates data from R code, thereby facilitating comprehension of the different elements that contribute to analyses. knitrdata also facilitates encryption of data integrated into Rmarkdown source documents, thereby allowing one to merge data with analysis code and text even in cases where industrial or ethical privacy constraints restrict data access to a specific group of individuals.\nBelow, I briefly provide a conceptual overview of how knitrdata works, presenting some simple examples of its use and the tools available to facilitate integrating data into Rmarkdown source documents. I then discuss typical use cases and workflows for development of Rmarkdown source documents with knitrdata, as well as a number of potential caveats for its use. I conclude by reflecting on the value of knitrdata for achieving computational reproducibility and its place within the growing pantheon of tools that make Rmarkdown an increasingly essential tool for research.\n2 knitrdata installation and usage\nThe knitrdata package is available on CRAN, though the latest version can be installed from github using the remotes (Csárdi et al. 2021) package:\n\n\nremotes::install_github(\"dmkaplan2000/knitrdata\",\n                        build_vignettes=TRUE)\n\n\nOnce the package has been installed, all that is needed to use the functionality provided by the package in a Rmarkdown source document is to load the library at the start of the document, typically in the setup chunk:\n\n\nlibrary(knitrdata)\n\n\n3 Conceptual overview of knitrdata\nTo understand how knitrdata works and the functionality it provides, one must first understand some of the terminology and functioning of Rmarkdown itself. As previously mentioned, Rmarkdown documents are a combination of text written in markdown, a simple, structured text format that can be translated into a large number of final output formats, and code for doing analyses that can augment the final output document with analytic results, tables and figures. The code is contained in specially delimited blocks, referred to as chunks. For example, adding the following to an Rmarkdown document:\n```{r}\nplot(-5:5,(-5:5)^2,type=\"l\")\n```\nwould add a plot of a parabola to the final output document. The process of translating a Rmarkdown document into a final output document is known as knitting, and this process is carried out using (often implicitly via RStudio) the knitr package (Xie 2015).\nThough code chunks typically contain R code, knitr supports a large number of other language engines, allowing one to integrate analyses in a number of other computer languages, including C, Python and SQL. For example, one could use the SQL language engine to import the contents of a database table into the R environment by including the following chunk in a Rmarkdown source document:\n```{sql connection=\"dbcon\",output.var=\"d\"}\nSELECT * FROM \"MyTable\";\n```\nDuring knitting, this will create in the R environment a variable d containing the contents of the table MyTable accessible via the (previously created) active R database connection dbcon. Note that the name of the database connection and the name of the output variable are supplied in the chunk header via what are known as chunk options. Though this database table could be imported into the R environment without the SQL language engine using R code:\n```{r}\nd = dbGetQuery(dccon,\"SELECT * FROM \\\"MyTable\\\";\")\n```\nthe use of the SQL language engine has both practical and conceptual advantages. On the practical side, it avoids the need to escape quotation marks and allows text editors to recognize and highlight the code as SQL, both of which becoming increasingly valuable as the length and complexity of SQL queries increase. On the conceptual side, using the SQL engine visually separates database queries from R code and text, thereby better communicating the structure and functioning of analyses in Rmarkdown documents.\nThe knitrdata package works in many ways analogously to the SQL language engine, adding a new data language engine to the list of language engines known to knitr that is specifically designed to import raw “data” into the R environment and/or export it to external files. Here the term “data” is used in a very wide sense, including not only standard data tables (e.g., CSV text files) or binary data (e.g., RDS files, NetCDF files, images), but also text and formatted text (e.g., XML files, BibTeX files). For example, placing the following chunk in a Rmarkdown source document will, during the knitting process, create in the R environment a data frame d containing the contents of the comma-separated values (CSV) data in the chunk (provided that the knitrdata package has been previously loaded as described above):\n```{data output.var=\"d\",loader.function=read.csv}\nname,score\nDavid M. Kaplan,1.2\nThe man formerly known as \"Prince\",3.4\nPeter O'Toole,5.6\n```\n\n\n\nAs with the SQL language engine, the name of the output variable for the chunk is supplied with a chunk option and in this example a loader.function option instructs knitrdata how to translate the contents of the chunk into a usable R object (in this example the R function read.csv is used to translate the CSV data into a data frame).\nThere are of course a number of other ways that such a simple data table could be imported into the R environment, including via an external data file or directly in R code, one approach to which might be:\n```{r}\nd = read.csv(textConnection(\n\"name,score\nDavid M. Kaplan,1.2\nThe man formerly known as \\\"Prince\\\",3.4\nPeter O'Toole,5.6\n\"))\n```\nHowever, using the data language engine has much the same practical and conceptual advantages as the SQL data language engine, avoiding the need for escaping certain characters and visually separating data from code, both of which become increasingly valuable as dataset size increases.\nIncorporating binary data into Rmarkdown source documents is a bit more complicated as the data must first be encoded as ASCII text (see the Section below on Binary data chunks for details), but the basic principles are the same - encoded binary data is incorporated into a data chunk and chunk options are used to tell knitrdata how to decode the data and load it into the R environment during knitting (see Table 1 for a full list of data chunk options). There is also the possibility of saving data chunk contents out to external files using the output.file chunk option. This option is particularly useful for integrating into Rmarkdown source documents ancillary text files used in the final formatting of the output of the knitting process, such as BibTeX files with references, LaTeX style files for PDF output and CSS style files for HTML output. For example, the following chunk would export a BibTeX reference to a file named refs.bib, taking care not to overwrite an existing file with the same name [though note that similar functionality can also be achieved with the cat language engine; Xie et al. (2020)]:\n```{data output.file = \"refs.bib\", eval=!file.exists(\"refs.bib\")}\n\n@book{allaireRmarkdownDynamicDocuments2020,\n  title = {Rmarkdown: {{Dynamic}} Documents for r},\n  author = {Allaire, JJ and Xie, Yihui and McPherson, Jonathan and Luraschi, Javier and Ushey, Kevin and Atkins, Aron and Wickham, Hadley and Cheng, Joe and Chang, Winston and Iannone, Richard},\n  year = {2020}\n}\n```\nAs code chunks are processed during knitting before generating the final output document, these files can be generated at any point during the knitting process using data chunks (in particular, it is often most practical to place this information at the end of a Rmarkdown document).\n\n\n\n\nTable 1:  Full list of knitrdata chunk options.\nChunk option\nDescription\ndecoding.ops\nA list with additional arguments for\ndata_decode. Currently only useful for passing\nthe verify argument to\ngpg::gpg_decrypt (Ooms 2022) for gpg\nencrypted chunks.\necho\nA boolean indicating whether or not to include\nchunk contents in Rmarkdown output. Defaults to\nFALSE.\nencoding\nOne of 'asis', 'base64' or 'gpg'. Defaults\nto 'asis' for format='text' and 'base64' for\nformat='binary'.\neval\nA boolean indicating whether or not to process the\nchunk. Defaults to TRUE.\nexternal.file\nA character string with the name of a file whose\ntext contents will be used as if they were the\ncontents of the data chunk.\nformat\nOne of 'text' or 'binary'. Defaults to\n'text'.\nline.sep\nOnly used when encoding='asis'. In this cases,\nspecifies the character string that will be used\nto join the lines of the data chunk before export\nto an external file, further processing or\nreturning the data. Defaults to\nknitrdata::platform.newline().\nloader.function\nA function that will be passed (as the first\nargument) the name of a file containing the\n(potentially decoded) contents of the data chunk.\nloader.ops\nA list of additional arguments to be passed to\nloader.function.\nmax.echo\nAn integer specifying the maximum number of lines\nof data to echo in the final output document.\nDefaults to 20. If the data exceeds this length,\nonly the first 20 lines will be shown and a final\nline indicating the number of ommitted lines will\nbe added.\nmd5sum\nA character string giving the correct md5sum of\nthe decoded chunk data. If supplied, the md5sum\nof the decoded data will be calculated and\ncompared to the supplied value, returning an error\nif the two do not match.\noutput.file\nA character string with the filename to which the\nchunk output will be written. At least one of\noutput.var or output.file must always be\nsupplied.\noutput.var\nA character string with the variable name to which\nthe chunk output will be assigned. At least one of\noutput.var or output.file must always be\nsupplied.\n\n\n\n\nUsing data chunks, just about any data or information that would typically be stored in external files can be integrated directly into Rmarkdown source documents. In particular, this permits creating standalone Rmarkdown source documents that can be knitted without need for external data files, thereby uniting text, code and data in a single source document.\nNote that this is different from the self_contained YAML header option permitted by some Rmarkdown output formats, notably HTML output formats. This option attempts to create a single output file that contains everything needed to visualize the final output document (e.g., in the case of HTML documents, the output HTML file will contain any CSS styles, javascript libraries and/or images used by the document), but it says nothing about whether or not external files are needed to knit the Rmarkdown source document (i.e., it is relevant to the output side of knitting, not the input side). In fact, a source document can be standalone in that all data and formatting files needed for knitting are incorporated within it using data chunks, but the final output (HTML) document may not be self contained because it relies on external files or libraries for visualization, and vice-versa (i.e., standalone source documents and standalone output documents are two separate and independent concerns).\nUnder the hood, the way knitrdata works is by adding (using the knitr::knit_engines$set() function) to the list of language engines that knitr maintains internally a data entry that points to a function inside the knitrdata package that processes data chunks (specifically the eng_data function, though users would typically not interact directly with this function). When knitting a Rmarkdown document, knitr will call this function each time a data chunk is encountered, passing it both the textual contents of the chunk and any chunk options. The function then uses this information to process the chunk, decoding it if necessary (via the format and encoding chunk options) and returning it as either a variable in the R environment (output.var chunk option) and/or an external file (output.file chunk option) after any additional processing has been carried out (via, e.g., the loader.function chunk option).\nText data chunks\nThough a basic example of a data chunk containing CSV tabular data has been presented in the previous section, it is useful to develop that example a bit more to better understand the functioning of knitrdata. The simplest data chunks contain plain text that is read, but not processed by knitrdata. For example, omitting the loader.function chunk option from the previously presented data chunk with CSV data produces a different outcome:\n```{data output.var=\"txt\"}\nsite,density\na,1.2\nb,3.4\nc,5.6\n```\nDuring the knitting process, this will place the text contents of the chunk into a R variable named txt, but no further processing of the text will be carried out (i.e., the variable txt will contain the literal text contents of the chunk, excluding the header with the chunk options and the tail). One could later convert the text into a R data.frame using the read.csv command in a R chunk placed after the data chunk:\n```{r}\nd <- read.csv(text=txt)\n```\nThe loader.function chunk option used in the initial data chunk example above causes knitrdata to combine into one process the two steps of (1) reading in the chunk contents and (2) converting them into a usable R data object. Whereas the first of these steps, reading the chunk contents, is carried out for all data chunks, the second only occurs if the loader.function chunk option is given. loader.function should be a function that takes in the name of a file containing the chunk contents and returns the processed contents. Though read.csv is likely to be a common choice, there are many other possibilities including readRDS, read.csv2, scan, png::readPNG and custom, user-defined functions. One can also supply a list of additional input arguments to the loader function using the loader.ops chunk option (e.g., one could change the expected separator in CSV data using loader.ops=list(sep=\"|\")).\nBinary data chunks\nThough text data chunks can integrate into Rmarkdown source documents many small- to medium-sized tabular data sets, binary data formats, such as RDS files, are more convenient for more complicated and/or larger data sets. Incorporating binary data into Rmarkdown documents requires additional steps relative to text data: encoding the binary data as text and telling the data chunk how to decode the encoded data. knitrdata provides tools for simplifying these two steps that currently support encoding and decoding of two widely-used encodings: base64 and gpg. base64 is a standard format for encoding binary data as ASCII text based on translation of 6 bits of information into one of 64 alphanumeric and symbolic ASCII characters. Base64 encoded data looks like a somewhat intimidating jumble of characters, but the format is extremely widely used behind the scenes in many common web applications, such as email attachments and embedding images in HTML pages. In particular, base64 is widely supported by a number of software packages and programming languages, including R, Python, Matlab and Julia, so base64 encoded data is highly readable and likely to remain so for a very long time.\ngpg, standing for GNU Privacy Guard, is a standard protocol for encrypting information so that only those with specific decryption keys can have access. This format can be used to ensure that only specific individuals can actually read and utilize the data contained in a Rmarkdown source document, as might be necessary when dealing with confidential (e.g., medical or trade-secret) data. Here, I focus primarily on base64 encoding as this is the simplest and likely most common format for binary data chunks, and a full description of the configuration and use of GPG is beyond the scope of this document. The detailed use of gpg is, however, described in the package vignette.\nThough knitrdata users rarely need to encode data by hand as the package provides graphical tools for this, it is instructive to have a basic understanding of the underlying functions for encoding and decoding data: data_encode and data_decode. data_encode takes the name of the file containing the data and the name of the encoding format, and it returns the encoded data that one would incorporate into a data chunk, either to the R command line or to a file. For example, if one saves the data frame d created in the previous section to a binary RDS file:\n\n\nsaveRDS(d,\"data.RDS\")\n\n\nthen one can encode this data as base64 using:\n\n\nb64 = knitrdata::data_encode(\"data.RDS\",\"base64\")\ncat(b64)\n\nH4sIAAAAAAAAA12OvQ+CMBDFKx+DGNTExPk2J1lc3HQwLsbIgInrBUokQmsKkbj5\nPzsrXrEMekn72l/f693JY4zZzLEsZrt0ZO4x2s6XxCZ0sWiNvwbWJx1t8JYlsA9g\nh9cchcEQnTkUKCCVquAqv8NFyFoAlhCqTMTc+PyQV1zBYRZJmXMCQ/336rloaz0w\nOk3bYjQVvfdM2BVY8NIMZBnoaNgZylgq/p+Kcyy7VAe9BCsMUqWzv/a+knXQNfJ1\nowdtTdO8SN4fPb8RnS0BAAA=\n\nThis jumble of characters starting with \"H4sI\" is the base64 encoded contents of the binary file that one would place in a data chunk. For large files, it is often more practical to output the encoded data to a file by supplying the output argument:\n\n\nknitrdata::data_encode(\"data.RDS\",\"base64\",\n                       output=\"data.RDS.base64\")\n\n\nGPG encoding works similarly to base64 encoding, but one must change the format from \"base64\" to \"gpg\" and specify the encryption key (i.e., the receiver ID) to be used to encrypt the data.\nOnce one has the encoded data, one can use it in a data chunk by supplying the format=\"binary\" chunk option and, optionally, an appropriate loader.function to convert the data into a R object:\n```{data format=\"binary\",output.var=\"d\",loader.function=readRDS}\nH4sIAAAAAAAAA12OvQvCMBDFz48OKn6A4Hybk11c3HQQFxE7VHA9aopimkgiFjf/\nZ2etl5oOesO93I/3crdvA0ADmnXuAT8h2MWryYynIQ9MYfA1QIu1v6Tb6YCbENd0\nkaQ8xvgoMCOFqTaZMPKOZ6VzhWQxMieVCO/rRuIqDG7HsdZSMOi5v+fPaVmLjtdR\nWhaUV0HNhwNFmbD+oLqHTQcrg020Ef+pRJKtUhVsH+hKYWpc9tfeMjoPq0Vdt+jB\nrSiKF8v7A6bdy9EtAQAA\n```\n\n\n\nDuring knitting, this chunk will be processed, decoding the encoded binary RDS file and loading it into the variable d using the readRDS function. knitrdata will by default assume that the encoding is base64 when format=\"binary\", but one can also specify the chunk option encoding=\"base64\" for increased clarity. For GPG encoded data, one would use encoding=\"gpg\". As with text data chunks, one can alternatively output the decoded contents of the chunk to a file (output.file option) or return it to the R session as a raw binary vector (by not supplying a loader.function).\nRStudio add-ins for creating data chunks\n\n\n\n\n\n\n\n\n\nFigure 1: The (a) ‘Insert filled data chunk’ and (b) ‘Remove chunks’ RStudio add-ins included with knitrdata. The dialogues will open when selected from the ‘Addins’ menu of RStudio. They allow one to (a) insert a data chunk containing the contents of an existing external data file into an open Rmarkdown document, and (b) delete one or more chunks from an open Rmarkdown document.\n\n\n\nAs manually encoding data and creating data chunks can be complicated, particularly for large data files, knitrdata includes graphical RStudio add-ins that do all the hard work of incorporating data in Rmarkdown documents. The principal add-in is called Insert filled data chunk (Fig. 1a). Though its use is meant to be largely self-explanatory, an instructional video is available on YouTube (Kaplan 2020b). The basic idea is that one opens a Rmarkdown source document in the RStudio editor, places the cursor at the location one wants to insert a data chunk and then activates the add-in. The add-in prompts for the name of the data file to be incorporated, as well as values for various data chunk output and processing options. Based on the type of data file selected, the add-in will attempt to select or suggest various appropriate options. For example, if a RDS file is chosen, then format will be set to binary, encoding will be set to base64 and the loader function will be set to readRDS. These defaults can be manually modified if not appropriate. The add-in also greatly facilitates and encourages the use of MD5 sum data integrity checks. After all options have been set, one clicks on Create chunk and an appropriately-formatted data chunk will be inserted in the active Rmarkdown source document at the cursor location.\nknitrdata also provides a Remove chunks add-in that allows ones one to quickly delete unwanted (data and non-data) chunks (Fig. 1b), as well as a set of functions for command-line examination, creation and removal of chunks from Rmarkdown documents (e.g., create_chunk, insert_chunk, list_rmd_chunks, remove_chunks).\nIf one is not using RStudio to edit and knit Rmarkdown documents, then one can invoke the Skiny dialog to create data chunks directly from the command line using the create_data_chunk_dialog function contained in the knitrdata package. In this case, chunk contents will be (silently) returned on the command line for later insertion in a Rmarkdown document.\n4 Use cases\nThere are a number of use cases for the functionality provided by knitrdata, primary among them providing a single source for public diffusion of all information related to a publication or report, and/or making collaboration on Rmarkdown source documents simpler by eliminating or reducing the need for external files. A simple example of the prior is the Rmarkdown source document used to generate this publication, which includes text data chunks for the tabular data in Table 1, as well as the ancillary formatting files associated with the document (BibTeX and LaTeX style files), and encoded binary data chunks for the PNG images in Figs. 1 & 2.\nA more complicated example is the Rmarkdown source document for Wain et al. (2021), publicly available on github. In this case, we wished to provide a permanent public record of the methods used in the paper and ensure that results could be verified, while at the same time respecting confidentiality agreements with respect to fine scale fishing activity data used in the paper. To achieve this we integrated the fine scale data in the Rmarkdown source document as an encrypted GPG data chunk. This approach may have value for a wide number of other studies using sensitive economic, social or medical data. To provide a complete record of the paper in a single document, we also integrated the Rmarkdown source document for the online supplementary materials into a data chunk within the Rmarkdown source document for the paper itself. As this supplementary materials document contains Rmarkdown chunks that would otherwise confuse the knitting process if integrated as raw text inside a data chunk, we base64 encoded this source document before including it in a data chunk. The document also contains data chunks for small data tables and for exporting to external files the ancillary formatting files required for knitting the document (BibTeX references, the LaTeX style file, the CSL citation style file, etc.). Finally, during the knitting process, the document also generates a lightweight version of itself that does not include the main data chunk, using the functionality of the knitrdata package to remove large data chunks. Overall, knitrdata provided a convenient way of generating a single document that contained all the necessary information for generating the final publication, thereby demonstrating computational reproducibility for the publication.\nThe uses of data chunks tend to fall into one of four general, not mutually-exclusive use cases:\nIntegration of ancillary formatting files into the Rmarkdown source document, thereby reducing the number of external files needed to knit a document\nInclusion of small- to medium-sized tabular data used in analyses and/or for tables\nInclusion of larger data sets using encoded binary data\nInclusion of confidential data using GPG-encrypted data chunks\nThough the first of these use cases, integration of ancillary formatting files, can also be achieved with the cat language engine that is included with the knitr package (Xie et al. 2020), knitrdata provides functionality that make this task easier and more secure. First, knitrdata allows for integrity checks on chunk contents that can control for unintentional modification of chunk contents (see the section on data integrity below). Second, RStudio add-ins provided by the knitrdata package facilitate the integration of data into Rmarkdown source documents and the use of integrity checks. Finally, encoding of text documents permits integrating files that contain Rmarkdown chunks or other formatting that would otherwise be problematic within a cat chunk.\nThe second of these use cases, tabular data, can also in principle be achieved using other tools in R, such as a textConnection as shown above or via functionality in the tibble package (Müller and Wickham 2022). Nevertheless, the use of data chunks is generally more ergonomic and flexible for anything but the smallest data tables as it allows the user to format data exactly as it would be in an external CSV file, without additional markup or the need to escape quotation marks. As an example, the information contained in Table 1 was implemented in the source document for this paper as a data chunk as it contains lots of quotations and formatting that would have been tedious to include using other approaches.\nThe third and fourth use cases for data chunks, involving encoded binary data, are unique to knitrdata and allow for integration of complex data sets that would otherwise be very difficult to include in a Rmarkdown source document.\n5 Workflow\nWhen and in what ways to use the functionality provided by knitrdata during the development of a Rmarkdown source document requires some thought and depends to some degree on the project goals. If the goal is to create a final Rmarkdown source document that demonstrates computational reproducibility of a set of results, then it may not be necessary or practical to use data chunks during the development stages of the project as the use of data chunks necessarily weighs down a Rmarkdown source document with information (e.g., binary data) that may not be immediately useful to authors during development. In this case, it may be best to work initially as one has always done, relying on external files for data and formatting. External data and formatting files can be incorporated in data chunks at the end of development when it is time to generate a final archival/public version of a Rmarkdown source document.\nOn the other hand, if the objective of using knitrdata also includes reducing the complexity of collaborating on a Rmarkdown source document by reducing the number of external files necessary for knitting a document, then certain types of data chunks can be incorporated in a Rmarkdown source document during the initial phases of development with little impact on authors. Small- to medium-sized tabular data sets can be incorporated and this can have the benefit of making the tabular data visually available during the development process. Similarly, most ancillary formatting files can be placed at the very end of the Rmarkdown source document as these are only used after all chunks have been processed and, therefore, will not encumber the development process. Larger data sets, and in particular binary data sets, are a bit more problematic as they necessarily appear in the Rmarkdown source document before the data is used for analyses and will introduce significant amounts of text that are not human readable into the Rmarkdown source document. For this reason it may be best to leave incorporation of these data until the final stages of development, though see the sections below on file size and readability for workarounds to these issues.\nThis latter workflow involving incorporation of data chunks in two distinct stages of development is what was used when creating the source document for Wain et al. (2021). Small data tables and formatting files were incorporated directly into the document from the start, but the larger data set that was the basis for statistical analyses and the Rmarkdown source document for the supplementary materials were only incorporated at the end of development to provide an archival source document for the paper capable of demonstrating computational reproducibility.\n6 Caveats and concerns\nThere are a number caveats and concerns with respect to the use of knitrdata, all of which have some validity, but for which a number of simple approaches exist to limit their impact. Below, I discuss four of them: file size, document readability, data integrity and security.\nWon’t this create huge Rmarkdown files?\nIncorporation of large data sets into data chunks will significantly increase the file size of Rmarkdown source documents, potentially making them more difficult to work with. Though it is unlikely to be practical to place extremely large data sets in Rmarkdown source documents, there are many contexts where data sets are sufficiently small so as to be included directly in a Rmarkdown source document. For example, the 8 years of fine scale fishing data used in Wain et al. (2021) added about 2 MB to the size of the Rmarkdown document when incorporated as a (compressed) RDS file, a size that is manageable and well within the limits of typical email attachments. RStudio currently will not open Rmarkdown documents larger than 5 MB in size, effectively limiting the amount of data that can be placed in a document unless one is willing to forgo graphical editing tools (larger documents can be rendered from the command line using the rmarkdown::render function, but not the more convenient and common “Knit” button of RStudio). Despite being in the era of big data, many scientific studies use primary data sets that are smaller than this size limit. As for Wain et al. (2021), many experimental or field studies may rely on data sets that are relatively small, and building a standalone Rmarkdown source document for these studies is an effective approach to documenting all quantitative information needed to reproduce results.\nWon’t Rmarkdown source documents become unreadable?\n\n\n\n\n\n\nFigure 2: Images demonstrating before (top) and after (bottom) a data chunk has been hidden from view in the RStudio editor. The top image shows two base64-encoded data chunks, one of which is hidden, whereas the other is visible. In the bottom image, both chunks have been hidden. The control for hiding chunk contents (top) and an indicator of a hidden chunk (bottom) are highlighted with red boxes.\n\n\n\nLarge amounts of encoded binary data are undoubtedly not pretty to look at, but readability is not necessarily the primary benefit of using Rmarkdown. Rather, completeness and the articulation of text and analyses are the strengths of Rmarkdown, benefits that data chunks enhance as opposed to diminish. Many Rmarkdown documents are already a complex mix of text and code that is difficult to read and manage without the tools RStudio and other editors provide to navigate the document, such as the ability to jump between sections and chunks. data chunks are no different in this sense, and use of informative chunk labels can greatly facilitate document navigation. Furthermore, RStudio includes the possibility of hiding chunk contents with a single click (Fig. 2), which can be quite practical when dealing with large data chunks. Once hidden, data chunk contents can be ignored, allowing one to edit the document unhindered.\nWon’t a misplaced keystroke mess up my data?\nIt is possible to unintentionally corrupt a data chunk due to a misplaced keystroke, particularly if the data is encoded and not readily human readable. However, the use of navigation tools and hiding of data chunk contents as described above (Fig. 2) can drastically reduce interaction with chunk contents, thereby limiting the possibility for error. Furthermore, there are a number of methods to validate chunk contents, the simplest of which is to do a MD5 sum check using functionality included in knitrdata. A MD5 sum is a very large number (typically encoded in hexadecimal) derived from a file’s contents that has a vanishingly small probability of being equal to that of another file if the files are not identical. data chunks can include a md5sum chunk option that specifies a MD5 sum that will be checked against that of the decoded chunk contents, generating an error if the two do not match. In this way, data corruption can be swiftly identified and corrected. The Insert filled data chunk RStudio add-in will by default calculate and include a MD5 sum check when inserting binary data chunks (and such a check can be optionally requested for text data chunks) so that users can easily benefit from these checks without having to worry about the details.\nAre there security concerns when using knitrdata?\nAny time one runs code from a third party, there are security risks. Typically, code can write files to disk, potentially modifying essential files or installing malicious software. Rmarkdown source documents using the functionality of knitrdata are no different in this sense, though the practical risks may be more important as knitrdata may encourage users to knit entire documents to gain access to raw data and arbitrary data may be encoded in formats that are not human readable. Reducing these risks involves responsibilities for both the authors and the users of Rmarkdown source documents. For authors, the primary responsibilities are to assure that source documents cannot be modified by third parties between the author and the user, and to use best practices when carrying out file input and output during the knitting process. Integrity of source documents can be protected by using reputable websites with established security protocols for publishing Rmarkdown source documents, including, but not limited to, github, Zenodo and the Dryad. Authors can also publish MD5 sums for Rmarkdown source documents so users can verify the integrity of those documents, though the security of those MD5 sums is only as strong as the websites on which MD5 sums and Rmarkdown source documents are published. Best practices for file input and output include using temporary files and/or relative paths entirely within the base directory containing the Rmarkdown source document when writing files to disk, using file names that are unique (e.g., avoiding generic names like data.csv) and performing checks for the existence of files with the same name before writing information to disk. The Insert filled data chunk RStudio add-in provided by knitrdata encourages the use of file existence checks in the eval chunk option controlling whether or not to process data chunks that write data to disk using the output.file chunk option, thereby avoiding overwriting existing files.\nFor users of Rmarkdown source documents, there are a number of simple steps one can take to avoid the most severe security risks. Knitting Rmarkdown source documents from an unprivileged user account and placing Rmarkdown source documents in new, empty directories can reduce the risks of the most malicious attacks. Users should also familiarize themselves with the workings of Rmarkdown source documents before knitting them and check for potentially problematic actions, such as use of absolute file paths and/or communication with external internet resources. This includes examination of the chunk options associated with data chunks (in particular, the output.file and loader.function). If one is primarily interested in just the raw data contained in data chunks within a Rmarkdown source document, then RStudio permits manual execution of individual chunks. This includes execution of data chunks, which can be processed individually using the Run current chunk button of RStudio once the knitrdata library has been loaded.\n7 Conclusion\nknitrdata provides a simple, but effective, tool for integrating arbitrary data into Rmarkdown source documents. If used appropriately, this can help assure computational reproducibility of many scientific documents by allowing one to integrate all relevant external files and data directly into a single Rmarkdown source document. Anyone who has attempted to validate the results in a publication by requesting the associated data has potentially encountered, if they managed to get the data, a set of one or more data tables with limited metadata and only the publication itself as documentation of the methods. Validating publication results under these conditions is often difficult and time consuming. By encouraging the integration of data, code for carrying out analyses, and text interpreting results in standalone Rmarkdown source documents, Rmarkdown with knitrdata can make it much easier to understand, reproduce and validate the details of scientific analyses. This combination can be particularly powerful when combined with other enhancements to Rmarkdown that make it possible to produce a wide variety of sophisticated scientific documents entirely within the confines of Rmarkdown, such as bookdown (Xie 2022), rticles (Allaire et al. 2022a) and starticles (Kaplan 2022) for producing books and scientific publications with Rmarkdown, citr (Aust 2019) for bibliographic citations, and kableExtra (Zhu 2021) for producing sophisticated data tables.\n8 Online supporting information\nThe Rmarkdown source documents for this publication and Wain et al. (2021) are available online at https://github.com/dmkaplan2000/knitrdata_examples. Additional examples and the package vignette are available in the knitrdata package itself.\n9 Acknowledgements\nI would like to thank my colleagues at the MARBEC laboratory in Sète, France for numerous conversations that encouraged me to develop the knitrdata package. I would also like to thank Yihui Xie for advice and encouragement regarding the development of the package. The handling editor and an anonymous reviewer provided valuable feedback that significantly improved the manuscript.\n\n\n\n\n\n\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-001.zip\nCRAN packages used\nknitrdata, remotes, knitr, gpg, tibble, bookdown, rticles, starticles, citr, kableExtra\nCRAN Task Views implied by cited packages\nReproducibleResearch\n\n\nJ. Allaire, Y. Xie, C. Dervieux, R Foundation, H. Wickham, Journal of Statistical Software, R. Vaidyanathan, Association for Computing Machinery, C. Boettiger, Elsevier, et al. Rticles: Article formats for r markdown. 2022a. URL https://github.com/rstudio/rticles. R package version 0.24.\n\n\nJ. Allaire, Y. Xie, J. McPherson, J. Luraschi, K. Ushey, A. Atkins, H. Wickham, J. Cheng, W. Chang and R. Iannone. Rmarkdown: Dynamic documents for r. 2022b. URL https://CRAN.R-project.org/package=rmarkdown. R package version 2.16.\n\n\nF. Aust. Citr: RStudio add-in to insert markdown citations. 2019. URL https://github.com/crsh/citr. R package version 0.3.2.\n\n\nP. Cassey and T. M. Blackburn. Reproducibility and Repeatability in Ecology. BioScience, 56(12): 958–959, 2006. DOI 10.1641/0006-3568(2006)56[958:RARIE]2.0.CO;2.\n\n\nG. Csárdi, J. Hester, H. Wickham, W. Chang, M. Morgan and D. Tenenbaum. Remotes: R package installation from remote repositories, including GitHub. 2021. URL https://CRAN.R-project.org/package=remotes. R package version 2.4.2.\n\n\nD. M. Kaplan. Knitrdata: Data language engine for knitr / rmarkdown. 2020a. URL https://github.com/dmkaplan2000/knitrdata. R package version 0.6.1.\n\n\nD. M. Kaplan. Starticles: A generic, publisher-independent template for writing scientific documents in rmarkdown. 2022. URL https://github.com/dmkaplan2000/starticles. R package version 0.1.0.\n\n\nD. M. Kaplan. Using knitrdata to create standalone Rmarkdown documents in Rstudio. 2020b.\n\n\nJ. S. S. Lowndes, B. D. Best, C. Scarborough, J. C. Afflerbach, M. R. Frazier, C. C. O’Hara, N. Jiang and B. S. Halpern. Our path to better science in less time using open data science tools. Nature Ecology & Evolution, 1(6): 1–7, 2017. DOI 10.1038/s41559-017-0160.\n\n\nK. Müller and H. Wickham. Tibble: Simple data frames. 2022. URL https://CRAN.R-project.org/package=tibble. R package version 3.1.8.\n\n\nJ. Ooms. Gpg: GNU privacy guard for r. 2022. URL https://github.com/jeroen/gpg. R package version 1.2.7.\n\n\nS. M. Powers and S. E. Hampton. Open science, reproducibility, and transparency in ecology. Ecological Applications, 29(1): e01822, 2019. DOI 10.1002/eap.1822.\n\n\nG. Wain, L. Guéry, D. M. Kaplan and D. Gaertner. Quantifying the increase in fishing efficiency due to the use of drifting FADs equipped with echosounders in tropical tuna purse seine fisheries. ICES Journal of Marine Science, 78(1): 235–245, 2021. DOI 10.1093/icesjms/fsaa216.\n\n\nY. Xie. Bookdown: Authoring books and technical documents with r markdown. 2022. URL https://CRAN.R-project.org/package=bookdown. R package version 0.26.\n\n\nY. Xie. Dynamic documents with R and knitr. 2nd ed Boca Raton, Florida: Chapman; Hall/CRC, 2015. URL https://yihui.org/knitr/. ISBN 978-1498716963.\n\n\nY. Xie. Knitr: A comprehensive tool for reproducible research in R. In Implementing reproducible computational research, Eds V. Stodden, F. Leisch and R. D. Peng 2014. Chapman; Hall/CRC. URL http://www.crcpress.com/product/isbn/9781466561595. ISBN 978-1466561595.\n\n\nY. Xie, C. Dervieux and E. Riederer. R Markdown Cookbook. 1st edition Boca Raton, Florida: CRC Press, 2020.\n\n\nH. Zhu. kableExtra: Construct complex table with kable and pipe syntax. 2021. URL https://CRAN.R-project.org/package=kableExtra. R package version 1.3.4.\n\n\n\n\n",
    "preview": "articles/RJ-2023-001/distill-preview.png",
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "articles/RJ-2023-002/",
    "title": "DGLMExtPois: Advances in Dealing with Over and Under-dispersion in a Double GLM Framework",
    "description": "In recent years the use of regression models for under-dispersed count data, such as COM-Poisson or hyper-Poisson models, has increased. In this paper the DGLMExtPois package is presented. DGLMExtPois includes a new procedure to estimate the coefficients of a hyper-Poisson regression model within a GLM framework. The estimation process uses a gradient-based algorithm to solve a nonlinear constrained optimization problem. The package also provides an implementation of the COM-Poisson model, proposed by @huang, to make it easy to compare both models. The functionality of the package is illustrated by fitting a model to a real dataset. Furthermore, an experimental comparison is made with other related packages, although none of these packages allow you to fit a hyper-Poisson model.",
    "author": [
      {
        "name": "Antonio J. Sáez-Castillo",
        "url": {}
      },
      {
        "name": "Antonio Conde-Sánchez",
        "url": {}
      },
      {
        "name": "Francisco Martínez",
        "url": {}
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-002.zip\n\n\nA. Huang. Mean-parametrized Conway–Maxwell–Poisson regression models for dispersed counts. Statistical Modelling, 17(6): 359–380, 2017. URL https://doi.org/10.1177/1471082X17697749.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-003/",
    "title": "Making Provenance Work for You",
    "description": "To be useful, scientific results must be reproducible and trustworthy. Data provenance---the history of data and how it was computed---underlies reproducibility of, and trust in, data analyses. Our work focuses on collecting data provenance from R scripts and providing tools that use the provenance to increase the reproducibility of and trust in analyses done in R. Specifically, our \"End-to-end provenance tools\" (\"E2ETools\") use data provenance to: document the computing environment and inputs and outputs of a script's execution; support script debugging and exploration; and explain differences in behavior across repeated executions of the same script. Use of these tools can help both the original author and later users of a script reproduce and trust its results.",
    "author": [
      {
        "name": "Barbara Lerner",
        "url": {}
      },
      {
        "name": "Emery Boose",
        "url": {}
      },
      {
        "name": "Orenna Brand",
        "url": {}
      },
      {
        "name": "Aaron M. Ellison",
        "url": {}
      },
      {
        "name": "Elizabeth Fong",
        "url": {}
      },
      {
        "name": "Matthew Lau",
        "url": {}
      },
      {
        "name": "Khanh Ngo",
        "url": {}
      },
      {
        "name": "Thomas Pasquier",
        "url": {}
      },
      {
        "name": "Luis A. Perez",
        "url": {}
      },
      {
        "name": "Margo Seltzer",
        "url": {}
      },
      {
        "name": "Rose Sheehan",
        "url": {}
      },
      {
        "name": "Joseph Wonsil",
        "url": {}
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-003.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-004/",
    "title": "remap: Regionalized Models with Spatially Smooth Predictions",
    "description": "Traditional spatial modeling approaches assume that data are second-order stationary, which is rarely true over large geographical areas. A simple way to model nonstationary data is to partition the space and build models for each region in the partition. This has the side effect of creating discontinuities in the prediction surface at region borders. The regional border smoothing approach ensures continuous predictions by using a weighted average of predictions from regional models. The R package remap is an implementation of regional border smoothing that builds a collection of spatial models. Special consideration is given to distance calculations that make remap package scalable to large problems. Using the remap package, as opposed to global spatial models, results in improved prediction accuracy on test data. These accuracy improvements, coupled with their computational feasibility, illustrate the efficacy of the remap approach to modeling nonstationary data.",
    "author": [
      {
        "name": "Jadon Wagstaff",
        "url": {}
      },
      {
        "name": "Brennan Bean",
        "url": {}
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-004.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-005/",
    "title": "HostSwitch: An R Package to Simulate the Extent of Host-Switching by a Consumer",
    "description": "In biology a general definition for host switch is when an organism (consumer) uses a new host (which represents a resource). The host switch process by a consumer may happen through its pre-existing capability to use a sub-optimal resource. The [HostSwitch](https://CRAN.R-project.org/package=HostSwitch) R package provides functions to simulate the dynamics of host switching (extent and frequency) in the population of a consumer that interacts with current and potential hosts over the generations. The [HostSwitch](https://CRAN.R-project.org/package=HostSwitch) package is based on a Individual-Based mock-up model published in FORTRAN by @araujo_understanding_2015. The package largely improve the previous mock-up model, by implementing numerous new functionalities such as comparison and evaluation of simulations with several customizable parameters to accommodate several types of biological consumer-host associations, an interactive visualization of the model, an in-depth description of the parameters in a biological context. Finally, we provided three real world scenarios taken from the literature selected from ecology, agriculture and parasitology. This package is intended to reach researchers in the broad field of biology interested in simulating the process of host switch of different types of symbiotic biological associations.",
    "author": [
      {
        "name": "Valeria Trivellone",
        "url": {}
      },
      {
        "name": "Sabrina B. L. Araujo",
        "url": {}
      },
      {
        "name": "Bernd Panassiti",
        "url": {}
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-005.zip\n\n\nS. B. L. Araujo, M. P. Braga, D. R. Brooks, S. J. Agosta, E. P. Hoberg, F. W. von Hartenthal and W. A. Boeger. Understanding host-switching by ecological fitting. PLOS ONE, 10(10): e0139225, 2015. URL https://doi.org/10.1371/journal.pone.0139225 [online; last accessed August 30, 2019].\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:39+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-006/",
    "title": "OTrecod: An R Package for Data Fusion using Optimal Transportation Theory",
    "description": "The advances of information technologies often confront users with a large amount of data which is  essential to integrate easily. In this context, creating a single database from multiple separate data sources can appear as an attractive but complex issue when same information of interest is stored in at least two distinct encodings. In this situation, merging the data sources consists in finding a common recoding scale to fill the incomplete information in a synthetic database. The OTrecod package provides R-users two functions  dedicated to solve this recoding problem using optimal transportation theory. Specific arguments of these functions enrich the algorithms by relaxing distributional constraints or adding a regularization term to make the data fusion more flexible.\nThe OTrecod package also provides a set of support functions dedicated to the harmonization of separate data sources, the handling of incomplete information and the selection of matching variables. This paper gives all the keys to quickly understand and master  the original algorithms implemented in the OTrecod package, assisting step by step the user in its data fusion project.",
    "author": [
      {
        "name": "Gregory Guernec",
        "url": {}
      },
      {
        "name": "Valerie Gares",
        "url": "http://vgares.perso.math.cnrs.fr/contact.html"
      },
      {
        "name": "Jeremy Omer",
        "url": "https://jeremyomer.wixsite.com/recherche"
      },
      {
        "name": "Philippe Saint-Pierre",
        "url": "https://perso.math.univ-toulouse.fr/psaintpi/"
      },
      {
        "name": "Nicolas Savy",
        "url": "https://perso.math.univ-toulouse.fr/savy/"
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\n1 Introduction\nThe large amount of data produced by information technology requires flexible tools to facilitate its handling. Among them, the field of data fusion (Hall and Llinas 1997; Klein 2004; Castanedo 2013) also known as statistical matching (Adamek 1994; D’Orazio et al. 2006; Vantaggi 2008) aims to integrate the overall information from multiple data sources for a better understanding of the phenomena that interact in the population.\nAssuming that two heterogeneous databases A and B share a set of common variables X while an information of interest is encoded in two distinct scales respectively: Y in A and Z in B. If Y and Z are never jointly observed, a basic data fusion objective consists in the recoding of Y in the same scale of Z (or conversely), to allow the fusion between the databases as illustrated in Table 1.\n\n\nTable 1: This package provides algorithms for merging two databases A and B where two variables of interest, Y and Z, are never jointly observed: the final result is a unique and synthetic database where Y and Z are fully completed.\n\n\n\n\nProviding a solution to this recoding problem is often very attractive because it aims at giving access to more accurate and consistent information with no additional costs in a unique and bigger database. Despite this, if we exclude all R data integration packages applied in the context of genetic area like the MultiDataSet package (Hernandez-Ferrer et al. 2017), OMICsPCA package (Das and Tripathy 2022), or the mixOmics package (F et al. 2017) which are often only effective for quantitative data integration. To our knowledge, the StatMatch package (D’Orazio 2022) is actually the only one that provide a concrete solution to the problem using hot deck imputation procedures. The main reason for this relative deficiency is that this problem is, in fact, often assimilated and solved like a missing data imputation problem. According to this idea, a very large amount of works and reference books now exist about the handling of missing data (Little and Rubin 2019; Zhu et al. 2019). Moreover, we can enumerate several R packages that we can sort by types of imputation methods (Mayer et al. 2019): mice (van Buuren and Groothuis-Oudshoorn 2011) and missForest (Stekhoven and Bühlmann 2012; Stekhoven 2022) which use conditional models, softImpute (Hastie and Mazumder 2021) and missMDA (Josse and Husson 2016) which apply low-rank based models. For all these packages, imputation performances can sometimes fluctuate a lot according to the structure and the proportion of non-response encountered. Regressions and non parametric imputation approaches (like hot-deck methods from donor based family) seem to use partially, or not at all, the available information of the two databases to provide the individual predictions. Contrary to our approach, all these methods only use the set of shared variables X for the prediction of Z in A (or conversely Y in B) without really taking into account Y and its interrelations with X in their process of predictions.\nThe purpose of this paper is to present a new package to the R community called OTrecod which provides a simple and intuitive access to two original algorithms (Garès et al. 2020; Garès and Omer 2022) dedicated to solve these recoding problems in the data fusion context by considering them as applications of Optimal Transportation theory (OT). In fact, this theory was already applied in many areas: for example, the transport package (Schuhmacher et al. 2022) solves optimal transport problems in the field of image processing. A specific package called POT: Python Optimal Transport (Flamary et al. 2021) also exists in the Python software (Van Rossum and Drake Jr 1995) to solve optimization problems using optimal transportation theory in the fields of signal theory, image processing and domain adaptation. Nevertheless, all these available tools were still not really adapted to our recoding problem and the performances established by OT-based approaches to predict missing information compared to more standard processes (Garès et al. 2020; Muzellec et al. 2020; Garès and Omer 2022) have finished to confirm our decision.\nIn OTrecod the first provided algorithm, called OUTCOME and integrated in the OT_outcome function consists in finding a map that pushes the distribution of Y forward to the distribution of Z (Garès et al. 2020) while the second one, called JOINT and integrated in the OT_joint function, pushes the distribution of (Y,X) forward to the distribution of (Z,X). Consequently, by building, these two algorithms take advantage of all the potential relationships between Y, Z, and X for the prediction of the incomplete information of Y and/or Z in A and B. Enrichments related to these algorithms and described in (Cuturi 2013; Garès and Omer 2022) are also available via the optional arguments of these two functions. In its current version, these algorithms are accompanied by original preparation (merge_dbs, select_pred) and validation (verif_OT) functions which are key steps of any standard statistical matching project and can be used independently of the OUTCOME and JOINT algorithms.\n2 Solving recoding problems using optimal transportation theory\nThe optimal transportation problem\nThe optimal transportation (OT) problem was originally stated by Monge (1781) and consists in finding the cheapest way to transport a pile of sand to fill a hole. Formally the problem writes as follows.\nConsider two (Radon) spaces \\(\\mathbb{X}\\) and \\(\\mathbb{Y}\\), \\(\\mu^X\\) a probability measure on \\(\\mathbb{X}\\), and \\(\\mu^Y\\) a probability measure on \\(\\mathbb{Y}\\) and \\(c\\) a Borel-measurable function from \\(\\mathbb{X} \\times \\mathbb{Y}\\) to \\(\\left[0,\\infty\\right]\\). The Kantorovich’s formulation of the optimal transportation problem (Kantorovich 1942) consists in finding a measure \\(\\gamma \\in \\Gamma(\\mu^X,\\mu^Y)\\) that realizes the infimum:\n\\[\\begin{equation}\n  \\inf\\left\\{\\left. \\int_{{\\mathbb{X}} \\times {\\mathbb{Y}}} c(x, y) \\, \\mathrm{d} \\gamma (x,y) \\right| \\gamma \\in \\Gamma(\\mu^X,\\mu^Y) \\right\\},\n\\tag{1}\n\\end{equation}\\]\nwhere \\(\\Gamma(\\mu^X,\\mu^Y)\\) is the set of measures on \\(\\mathbb{X} \\times \\mathbb{Y}\\) with marginals \\(\\mu^X\\) on \\(\\mathbb{X}\\) and \\(\\mu^Y\\) on \\(\\mathbb{Y}\\).\nThis theory is applied here to solve a recoding problem of missing distributions in a data fusion area. To do so, we make use of Kantovorich’s formulation adapted to the discrete case, known as Hitchcock’s problem (Hitchcock 1941). Therefore, by construction, the proposed algorithms are usable for specific target variables only: categorical variables, ordinal or nominal, and discrete variables with finite number of values.\nOptimal transportation of outcomes applied to data recoding\nLet \\(A\\) and \\(B\\) be two databases corresponding to two independent sets of subjects.\nWe assume without loss of generality that the two databases have equal sizes, so that they can be written as \\(A=\\{i_1,\\dots,i_{n}\\}\\) and \\(B=\\{j_1,\\dots,j_{n}\\}\\). Let \\(\\big((X_i,Y_i,Z_i)\\big)_{i\\in A}\\) and \\(\\big((X_j,Y_j,Z_j)\\big)_{j\\in B}\\) be two sequences of i.i.d. discrete random variables with values in \\(\\mathcal{X} \\times \\mathcal{Y} \\times \\mathcal{Z}\\), where \\(\\mathcal{X}\\) is a finite subset of \\(\\mathbb{R}^P\\), and \\(\\mathcal{Y}\\) and \\(\\mathcal{Z}\\) are finite subsets of \\(\\mathbb{R}\\). Variables \\((X_i,Y_i,Z_i), i\\in A\\), are i.i.d copies of \\((X^A,Y^A,Z^A)\\) and \\((X_j,Y_j,Z_j), j\\in B\\), are i.i.d copies of \\((X^B,Y^B,Z^B)\\). Moreover assume that \\(\\big\\{(X_i,Y_i,Z_i),i\\in A\\big\\}\\) are independent of \\(\\big\\{(X_j,Y_j,Z_j),j\\in B\\big\\}\\).\nThe first version using the optimal transportation algorithm approach, described in (Garès et al. 2020), assumes that:\nAssumption 1. \\(Y^A\\) and \\(Z^A\\) respectively follow the same distribution as \\(Y^B\\) and \\(Z^B\\).\nAssumption 2. For all \\(x \\in \\mathcal{X}\\) the probability distributions of \\(Y^A\\) and \\(Z^A\\) given that \\(X^A= x\\) are respectively equal to those of \\(Y^B\\) and \\(Z^B\\) given that \\(X^B= x\\).\nIn this setting, the aim is to solve the recoding problem given by equation (1) that pushes \\(\\mu^{Y^A}\\) forward to \\(\\mu^{Z^A}\\).\nThe variable \\(\\gamma\\) of (1) is a discrete measure with marginals \\(\\mu^{Y^A}\\) and \\(\\mu^{Z^A}\\), represented by a \\(|\\mathcal{Y}| \\times |\\mathcal{Z}|\\) matrix.\nThe cost function denoted as \\(c\\) is a \\(|\\mathcal{Y}| \\times |\\mathcal{Z}|\\) matrix, \\((c_{y,z})_{y\\in \\mathcal{Y},z\\in \\mathcal{Z}}\\).\nThe goal is in the identification of:\n\\[\\begin{equation}\n\\gamma^*\\in argmin_{\\gamma \\in \\mathbb{R}_+^{|\\mathcal{Y}| \\times |\\mathcal{Z}|}}\\left\\{\\langle \\gamma  \\; ,\\; c \\rangle : \\gamma \\mathbf{1}_{|\\mathcal{Z}|} = \\mu^{Y^A},\\gamma^T \\mathbf{1}_{|\\mathcal{Y}|} = \\mu^{Z^A}\\right\\},\n\\tag{2}\n\\end{equation}\\]\nwhere \\(\\langle \\cdot \\;,\\; \\cdot \\rangle\\)\nis the dot product, \\(\\mathbf{1}\\) is a vector of ones with appropriate dimension and \\(M^T\\) is the transpose of matrix \\(M\\). The cost function considered by Garès et al. (2020), \\(c_{y,z}\\), measures the average distance between the profiles of shared variables of \\(A\\) satisfying \\(Y=y\\) and subjects of \\(B\\) satisfying \\(Z=z\\), that is:\n\\[\\begin{equation}\nc_{y,z} = \\mathbb{E} \\left[d(X^A,X^B) \\mid Y^A= y, Z^B= z\\right],\n\\tag{3}\n\\end{equation}\\]\nwhere \\(d\\) is a given distance function to choose on \\(\\mathcal{X} \\times \\mathcal{X}\\).\nIn fact, the above situation cannot be solved in reality, since the distributions of \\(X^A\\), \\(X^B\\), \\(Y^A\\) and \\(Z^A\\) are never jointly observed. As a consequence, the following unbiased empirical estimators are used: \\(\\hat{\\mu}^{X^A}_n\\) of \\({\\mu}^{X^A}\\) and \\(\\hat{\\mu}^{X^B}_n\\) of \\({\\mu}^{X^B}\\). Because \\(Y\\) and \\(Z\\) are only available in \\(A\\) and \\(B\\) respectively, two distinct empirical estimators have to be defined:\n\\[\\begin{equation}\n    \\begin{aligned}\n        \\hat{\\mu}^{Y^A}_{n,y} & = & \\frac{1}{n}\\sum_{i\\in A} ~\\mathbf{1}_{\\{Y_i = y\\}},\\: \\forall y\\in\\mathcal{Y},\\\\\n        \\hat{\\mu}^{Z^A}_{n,z} & = & \\frac{1}{n}\\sum_{j\\in B} ~\\mathbf{1}_{\\{Z_j = z\\}},\\: \\forall z\\in \\mathcal{Z},\n    \\end{aligned}\n    \\tag{4}\n\\end{equation}\\]\nwhere \\(\\mathbf{1}_{\\{Y = y\\}}=1\\) if \\(Y=y\\) and \\(0\\) otherwise. The assumption 1 gives: \\({\\mu}^{Z^A}={\\mu}^{Z^B}\\) from which we can conclude that \\(\\hat{\\mu}^{Z^B}_{n,z}=\\hat{\\mu}^{Z^A}_{n,z}\\).\nFinally, denoting: \\[\\kappa_{n,y,z}\\equiv \\sum_{i\\in A} \\sum_{j\\in B}~ \\mathbf{1}_{\\left\\{Y_i=y,Z_j=z\\right\\}},\\]\nthe number of pairs \\((i,j)\\in A\\times B\\) such that \\(Y_i=y\\) and \\(Z_j=z\\), the cost matrix \\(c\\) is estimated by:\n\\[\\begin{equation}\n\\hat{c}_{n,y,z}=\\left\\{ \\begin{array}{ll} \\frac{1}{\\kappa_{n,y,z}}\\sum_{i\\in A} \\sum_{j\\in B}~ \\mathbf{1}_{\\left\\{Y_i=y,Z_j=z\\right\\}} \\times d(X_i,X_j), & \\: \\forall y\\in \\mathcal{Y}, z\\in\\mathcal{Z}:\\kappa_{n,y,z}\\neq 0,\\\\\n0, & \\:\\forall y \\in \\mathcal{Y}, z \\in \\mathcal{Z}:\\kappa_{n,y,z} = 0.\n\\end{array}\\right.\n\\tag{5}\n\\end{equation}\\]\nPlugging the values observed for these estimators in (2) yields to a linear programming model denoted:\n\\[\\begin{equation}\n\\hat{\\mathcal{P}}^0_n: \\left\\{\n    \\begin{aligned}\n         \\min\\: & <\\widehat{c}_n,\\gamma>\\\\\n                \\text{s.t.}\\:& \\sum_{z\\in Z} \\gamma_{y,z} = \\mu^{Y^A}_{n,y}, \\:\\forall y\\in \\mathcal{Y}, \\\\\n                & \\sum_{y\\in Y} \\gamma_{y,z} = \\mu^{Z^A}_{n,z}, \\:\\forall z\\in \\mathcal{Z}, \\\\\n                & \\gamma_{y,z} \\geq 0, \\: \\forall y\\in \\mathcal{Y}, \\forall z\\in \\mathcal{Z}.\n    \\end{aligned}\\right.\n    \\tag{6}\n\\end{equation}\\]\nThe solution \\(\\hat{\\gamma}_n\\) can then be interpreted as an estimator \\(\\hat{\\mu}^{(Y^A,Z^A)}_n\\) of the joint distribution of \\(Y^A\\) and \\(Z^A\\), \\(\\mu^{(Y^A,Z^A)}\\). If this estimate is necessary, it is nevertheless insufficient here to provide the individual predictions on \\(Z\\) in \\(A\\). These predictions are done in a second step using a nearest neighbor algorithm from which we deduce an estimation of \\(\\mu^{Z^A\\mid X^A=x,Y^A=y}\\) (see Garès and Omer (2022) for details). In the remainder, the overall algorithm described in this section is referred to as OUTCOME. To improve the few drawbacks of this algorithm described in Garès et al. (2020), derived algorithms from OUTCOME have been developed (Garès and Omer 2022) and described in the following part.\nOptimal transportation of outcomes and covariates\nUsing the same notations, Garès et al. (2020) propose to search for an optimal transportation map between the two joint distributions of \\((X^A,Y^A)\\) and \\((X^A,Z^A)\\) with marginals \\(\\mu^{(X^A,Y^A)}\\) and \\(\\mu^{(X^A,Z^A)}\\) respectively. Under Kantorovich’s formulation in a discrete setting, they search for:\n\\[\\gamma^*\\in \\operatorname{argmin}_{\\gamma\\in \\mathcal{D}} <c,\\gamma>,\\]\nwhere \\(c\\) is a given cost matrix and \\(\\mathcal{D}\\) is the set of joint distributions with marginals \\(\\mu^{(X^A,Y^A)}\\) and \\(\\mu^{(X^A,Z^A)}\\). It is natural to see any element \\(\\gamma\\in \\mathcal{D}\\) as the vector of joint probabilities \\(\\mathbb{P}((X^A=x,Y^A=y),(X^A=x',Z^A=z))\\) for any \\(x,x'\\in\\mathcal{X}^2\\), \\(y\\in\\mathcal{Y}\\) and \\(z\\in\\mathcal{Z}\\). Since this probability nullifies for all \\(x\\neq x'\\), \\(\\gamma\\in \\mathcal{D}\\) is defined as a vector of \\(\\mathbb{R}^{\\left\\lvert X \\right\\rvert\\times \\left\\lvert \\mathcal{Y} \\right\\rvert \\times\\left\\lvert\\mathcal{Z} \\right\\rvert }\\), where \\(\\gamma_{x,y,z}\\) stands for an estimation of the joint probability \\(\\mathbb{P}(X^A=x,Y^A=y,Z^A=z)\\).\nThese notations lead to the more detailed model:\n\\[\\begin{equation}\n    \\mathcal{P}:\\left\\{\n    \\begin{aligned}\n        \\min\\: & <c,\\gamma> \\\\\n                \\text{s.t.}\\:& \\sum_{z\\in Z} \\gamma_{x,y,z} = \\mu^{(X^A,Y^A)}_{x,y}, \\:\\forall x\\in\\mathcal{X}, \\forall y\\in \\mathcal{Y},\\\\\n                & \\sum_{y\\in Y} \\gamma_{x,y,z} = \\mu^{(X^A,Z^A)}_{x,z}, \\:\\forall x\\in\\mathcal{X}, \\forall z\\in \\mathcal{Z},\\\\\n                & \\gamma_{x,y,z} \\geq 0,  \\:\\forall x\\in\\mathcal{X}, \\forall y\\in \\mathcal{Y}, \\forall z\\in \\mathcal{Z}.\n    \\end{aligned}\\right.\n\\tag{7}\n\\end{equation}\\]\nThe above algorithm can be solved only if the marginals \\(\\mu^{(X^A,Y^A)}\\) and \\(\\mu^{(X^A,Z^A)}\\) are known, but, based on assumption 2, unbiased estimators \\(\\hat{\\mu}^{X^A,Y^A}_n\\) and \\(\\hat{\\mu}^{X^A,Z^A}_n\\) can be built according to the previous subsection. For the first one it gives:\n\\[\\begin{equation}\n     \\begin{aligned}\n         \\hat{\\mu}^{X^A,Y^A}_{n} & = & \\frac{1}{n}\\sum_{i\\in A} ~\\mathbf{1}_{\\left\\{Y_i = y,X_i = x\\right\\}}, \\:\\forall x\\in\\mathcal{X}, \\: \\forall y\\in\\mathcal{Y}.\n     \\end{aligned}\n     \\tag{8}\n\\end{equation}\\]\nThe cost matrix introduced in the OUTCOME algorithm is used (3) and estimated by (5). Formally we can write:\n\\[\\begin{equation}\n  c_{x,y,z} = c_{y,z}, \\:\\forall x\\in\\mathcal{X},\\forall y\\in\\mathcal{Y},\\forall z\\in\\mathcal{Z},\n  \\tag{9}\n\\end{equation}\\]\nwhich does not depend on the value of \\(x\\).\nPlugging the values observed for these estimators in (7) yield a linear programming model denoted as \\(\\widehat{\\mathcal{P}}_n\\). In contrast to , the algorithm that consists in solving \\(\\widehat{\\mathcal{P}}_n\\) to solve the recoding problem is referred to as JOINT in what follows.\nAn estimation of the distribution of \\(Z^A\\) given the values of \\(X^A\\) and \\(Y^A\\) is then given by:\n\\[\\begin{equation}\n\\tilde{\\mu}^{Z^A\\mid X^A=x,Y^A=y}_{n,z}=\n\\left\\{\\begin{aligned}\n&\\frac{\\hat{\\gamma}_{n,x,y,z}}{\\hat{\\mu}^{(X^A,Y^A)}_{n,x,y}}, & \\:\\forall x\\in\\mathcal{X},y\\in\\mathcal{Y},z\\in\\mathcal{Z}: \\hat{\\mu}^{(X^A,Y^A)}_{n,x,y}\\neq 0,\\\\\n&0, & \\:\\forall x\\in\\mathcal{X},y\\in\\mathcal{Y},z\\in\\mathcal{Z}: \\hat{\\mu}^{(X^A,Y^A)}_{n,x,y}= 0.\n\\end{aligned}\\right.\n\\tag{10}\n\\end{equation}\\]\nand an individual prediction of \\(Z^A\\) is then deduced using the maximum a posterior rule:\n\\[\n\\widehat{z}_i^A= \\operatorname{argmax}_{z\\in\\mathcal{Z}} \\tilde{\\mu}^{Z^A\\mid X^A=x_i,Y^A=y_i}_{n,z}.\n\\]\nDue to potential errors in the estimations of \\(\\mathcal{P}\\), the constraints of \\(\\widehat{\\mathcal{P}}_n\\) may derive from the true values of the marginals of \\(\\mu^{(X^A,Y^A,Z^A)}\\). To deal with this situation, small violations of the constraints of \\(\\widehat{\\mathcal{P}}_n\\) are allowed by enriching the initial algorithm as described in Garès and Omer (2022).\nThe equality constraints of \\(\\widehat{\\mathcal{P}}_n\\) are then relaxed as follows:\n\\[\\begin{align}\n            & \\sum_{z\\in Z} \\gamma_{x,y,z} = \\hat{\\mu}^{(X^A,Y^A)}_{n,x,y} + e^{X,Y}_{x,y}, \\:\\forall x\\in\\mathcal{X}, \\forall y\\in \\mathcal{Y}    \\tag{11}\\\\\n            & \\sum_{y\\in Y} \\gamma_{x,y,z} = \\tilde{\\mu}^{(X^A,Z^A)}_{n,x,z} + e^{X,Z}_{x,z}, \\:\\forall x\\in\\mathcal{X}, \\forall z\\in \\mathcal{Z}    \\tag{12}\\\\\n            & \\sum_{x\\in \\mathcal{X},y\\in \\mathcal{Y}} e^{X,Y}_{x,y} = 0,\\: \\sum_{x\\in \\mathcal{X},z\\in \\mathcal{Z}} e^{X,Z}_{x,z} = 0     \\tag{13}\\\\\n            & -e^{X,Y,+}_{x,y}\\leq e^{X,Y}_{x,y} \\leq e^{X,Y,+}_{x,y}, \\:\\forall x\\in\\mathcal{X}, \\forall y\\in \\mathcal{Y}    \\tag{14}\\\\\n            & -e^{X,Z,+}_{x,z}\\leq e^{X,Z}_{x,z} \\leq e^{X,Z,+}_{x,z}, \\:\\forall x\\in\\mathcal{X}, \\forall z\\in \\mathcal{Z}    \\tag{15}\\\\\n            & \\sum_{x\\in \\mathcal{X},y\\in \\mathcal{Y}} e^{X,Y,+}_{x,y} \\leq \\alpha_n,\\: \\sum_{x\\in \\mathcal{X},z\\in \\mathcal{Z}} e^{X,Z,+}_{x,z} \\leq \\alpha_n.  \\tag{16}\n\\end{align}\\]\nThis relaxation is possible by introducing extra-variables \\(e^{X,Y,+}\\) and \\(e^{X,Z,+}\\) as additional constraints (14)–(15).\nGarès and Omer (2022) suggests to consider \\(\\alpha_n:=\\frac{\\alpha}{\\sqrt{n}}\\) from (16), with a parameter \\(\\alpha\\) to calibrate numerically but proposes also a default value fixed to \\(0.4\\).\nA regularization term \\(\\lambda\\) given by \\((\\frac{\\pi_{x,y,z}}{\\hat{\\mu}^{X^A}_{n,x}})_{x\\in\\mathcal{X},y\\in\\mathcal{Y},z\\in\\mathcal{Z}}\\) can also be added to improve regularity in the variations of the conditional distribution \\(\\mu^{Y^A,Z^A\\mid X^A=x}\\) with respect to \\(x\\). The corresponding regularized algorithm is:\n\\[\\begin{equation}\n\\widehat{\\mathcal{P}}^R_n:    \\left\\{\n     \\begin{aligned}\n          \\min\\: & <\\widehat{c}_n,\\gamma> + \\lambda \\sum_{(x_i,x_j)\\in E_\\mathcal{X}}      w_{i,j}\\sum_{y\\in\\mathcal{Y},z\\in\\mathcal{Z}} r^+_{i,j,y,z}\\\\\n            &\\text{s.t.}\\: \\text{constraints }  \\text{(11)--(16)} \\\\\n            & \\frac{\\gamma_{x_i,y,z}}{\\hat{\\mu}^{X^A}_{n,x_i}}-\\frac{\\gamma_{x_j,y,z}}{\\hat{\\mu}^{X^A}_{n,x_j}} \\leq r^+_{i,j,y,z}, \\:\\forall \\{x_i,x_j\\}\\in E_\\mathcal{X}, y\\in\\mathcal{Y}, z\\in \\mathcal{Z}\\\\\n            &  \\frac{\\gamma_{x_i,y,z}}{\\hat{\\mu}^{X^A}_{n,x_i}}-\\frac{\\gamma_{x_j,y,z}}{\\hat{\\mu}^{X^A}_{n,x_j}} \\geq -r^+_{i,j,y,z}, \\:\\forall \\{x_i,x_j\\}\\in E_\\mathcal{X}, y\\in\\mathcal{Y}, z\\in \\mathcal{Z}\\\\\n             & \\gamma_{x,y,z} \\geq 0,  \\:\\forall x\\in\\mathcal{X}, \\forall y\\in \\mathcal{Y}, \\forall z\\in \\mathcal{Z}.\n     \\end{aligned}\\right.\n\\tag{17}\n\\end{equation}\\]\nThe constant \\(\\lambda\\in\\mathbb{R}^+\\) is a regularization parameter to be calibrated numerically (\\(0.1\\) can be considered as default value) and \\(E_\\mathcal{X}\\subset \\mathcal{X}^2\\) includes the pairs of elements of X defined as neighbors: \\(\\left\\{x_i, x_j\\right\\}\\in E_\\mathcal{X}\\) if \\(x_j\\) is among the k nearest neighbors of \\(x_i\\) for some parameter \\(k \\geq 1\\). The method that computes a solution to the recoding problem with regularization and relaxation is called R-JOINT.\nIn a same way, a relaxation of the assumption 1 is also proposed and added to the OUTCOME algorithm: this resulting method is denoted R-OUTCOME and is related to the following program:\n\\[\\begin{equation}\n\\widehat{\\mathcal{P}}^{0-R}_n:    \\left\\{\n\\begin{aligned}\n      \\min\\: & <\\widehat{c}_n,\\gamma>\\\\\n         & \\sum_{z\\in \\mathcal{Z}} \\gamma_{y,z} = \\hat{\\mu}^{Y^A}_{n,y} + e^{Y}_{y}, \\:\\forall y\\in \\mathcal{Y}\\\\\n         & \\sum_{y\\in \\mathcal{Y}} \\gamma_{y,z} = \\tilde{\\mu}^{Z^A}_{n,z} + e^{Z}_{z}, \\:\\forall z\\in \\mathcal{Z}\\\\\n         & \\sum_{y\\in \\mathcal{Y}} e^{Y}_{y} = 0,\\: \\sum_{z\\in \\mathcal{Z}} e^{Z}_z = 0\\\\\n         & -e^{Y,+}_{y}\\leq e^{Y}_{y} \\leq e^{Y,+}_{y}, \\:\\forall y\\in \\mathcal{Y}\\\\\n         & -e^{Z,+}_{z}\\leq e^{Z}_{z} \\leq e^{Z,+}_{z}, \\:\\forall z\\in \\mathcal{Z}\\\\\n         & \\sum_{y\\in \\mathcal{Y}} e^{Y,+}_{y} \\leq \\alpha_n,\\: \\sum_{z\\in \\mathcal{Z}} e^{Z,+}_{z} \\leq \\alpha_n\\\\\n         & \\gamma_{y,z} \\geq 0,  \\:\\forall y\\in \\mathcal{Y}, \\forall z\\in \\mathcal{Z}.\n\\end{aligned}\\right.\n\\tag{18}\n\\end{equation}\\]\nNote that algorithms JOINT and R-JOINT do not require assumption 1. The relaxation in R-OUTCOME alleviates its dependence to the satisfaction of this assumption. However, algorithms JOINT and R-JOINT require that \\(X\\) is a set of discrete variables (factors ordered or not are obviously allowed) while the absence of \\(X\\) in the linear algorithms OUTCOME and R-OUTCOME allow \\(X\\) to be a set of discrete and/or continuous variables. In this case, the nature of the variables \\(X\\) need to be considered when choosing the distance \\(d\\).\n3 Package installation and description\nInstallation\nThe OTrecod package can be installed from the Comprehensive R Archive Network (CRAN) by using the following template:\n\n\ninstall.packages(\"OTrecod\")\n\n\nThe development version of OTrecod is also available and can be directly installed from GitHub by loading the devtools (Wickham et al. 2022) package (R> install_github(\"otrecoding/OTrecod\")).\nMain functions\nThe two types of optimal transportation algorithms previously introduced (OUTCOME and JOINT) and their respective enrichments (R-OUTCOME and R-JOINT) are available in the OTrecod package via two core functions denoted OT_outcome and OT_joint. Details about their implementations in R are described in the following section. In this package, these algorithms of recoding are seen as fundamental steps of data fusion projects that also require often adapted preparation and validation functions. In this way, the Table 2 introduces the main functions proposed by OTrecod to handle a data fusion project.\n\n\nTable 2: A brief description of the main functions of OTrecod\n\n\nR Function\n\n\nDescription\n\n\nPre-process functions\n\n\n\n\nmerge_dbs\n\n\nHarmonization of the data sources\n\n\nselect_pred\n\n\nSelection of matching variables\n\n\nFunctions of data fusion\n\n\n\n\nOT_outcome\n\n\nData fusion with OT theory using the OUTCOME or R-OUTCOME algorithms.\n\n\nOT_joint\n\n\nData fusion with OT theory using the JOINT or R-JOINT algorithms.\n\n\nPost-process function\n\n\n\n\nverif_OT\n\n\nQuality assessment of the data fusion\n\n\nAll the intermediate functions integrated in the OT_outcome and OT_joint functions (proxim_dist, avg_dist_closest, indiv_grp_closest, indiv_grp_optimal), and their related documentations, are all included and usable separately in the package. They have been kept available for users to ensure a great flexibility as other interesting functions like power_set that returns the power set of a set. This function did not exist on R until now and could be of interest for specialists of algebra. These functions are not described here but detailed in the related pdf manual of the package.\n4 Functionalities overview\nExpected structure of the input databases as arguments\nThe functions of recoding OT_outcome and OT_joint require a specific structure of data.frame as input arguments. Described in Table 3, it must be the result of two overlayed databases made of at least four variables:\nA first variable, discrete or categorical, corresponding to the database identifier, stored in factor or not, but with only two classes or levels (for example: A and B, 1 and 2 or otherwise).\nThe target variable of the first database (or top database) denoted Y for example, whose values related to the second database are missing. This variable can be discrete or categorical stored in factor, ordered factor or not.\nIn the same way, the target variable of the second database (or below database) denoted Z for example, whose values related to the first database are missing.\nAt least one shared variable (defined as a variable with the same label and the same encoding in the two distinct data sources). The type of shared variables can be continuous, categorical stored in factor or not, complete or not. Nevertheless, few constraints must be noticed related to this question. First, in a critical situation where only one shared variable exists, this latter cannot be incomplete. Second, continuous shared variables are actually not allowed in the current version of the function OT_joint, therefore, these variables must be transformed beforehand.\n\n\nTable 3: Example of expected structure for two databases 1 and 2 with three shared variables X1, X2, X3\n\nDB\n\n\nY\n\n\nZ\n\n\nX_1\n\n\nX_2\n\n\nX_3\n\n\n1\n\n\n(600-800]\n\n\nNA\n\n\nM\n\n\nYes\n\n\n50\n\n\n1\n\n\n(600-800]\n\n\nNA\n\n\nM\n\n\nNo\n\n\n32\n\n\n1\n\n\n[200-600]\n\n\nNA\n\n\nW\n\n\nNo\n\n\n31\n\n\n2\n\n\nNA\n\n\nG1\n\n\nM\n\n\nNo\n\n\n47\n\n\n2\n\n\nNA\n\n\nG3\n\n\nW\n\n\nYes\n\n\n43\n\n\n2\n\n\nNA\n\n\nG2\n\n\nW\n\n\nNo\n\n\n23\n\n\n2\n\n\nNA\n\n\nG4\n\n\nM\n\n\nYes\n\n\n22\n\n\n2\n\n\nNA\n\n\nG2\n\n\nW\n\n\nYes\n\n\n47\n\n\nAs additional examples, users can also refer to the databases simu_data and tab_test provided in the package with expected structures. Note that class objects are not expected here as input arguments of these functions to allow users to freely work with or without the use of the pre-process functions provided in the package.\nChoice of solver\nThe package OTrecod uses the ROI optimization infrastructure (Theußl et al. 2017) to solve the optimization problems related to the OUTCOME and JOINT algorithms. The solver GLPK (The GNU Linear Programming Kit (Makhorin 2011)) is the default solver actually integrated in the OT_outcome and OT_joint functions for handling linear problems with linear constraints. The ROI infrastructure makes easy for users to switch solvers for comparisons. In many situations, some of them can noticeably reduce the running time of the functions.\nFor example, the solver Clp (Forrest et al. 2004) for COINT-OR Linear Programming, known to be particularly convenient in linear and quadratic situations, can be easily installed by users via the related plug-in available in ROI (searchable with the instruction ROI_available_solvers()) and following the few instructions detailed in (Theußl et al. 2020) or via the dedicated website.\nAn illustrative example\nIn California (United States), from 1999 to 2018, the Public Schools Accountability Act (PSAA) imposed on its California Department of Education (CDE) to provide annually the results of an Academic Performance Index (API) which established a ranking of the best public schools of the state.\nThis numeric score, indicator of school’s performance levels, could vary from 200 to 1000 and the performance objective to reach for each school was 800. Information related to the 418 schools (identified by cds) of Nevada (County 29) and to the 362 schools of San Benito (County 35), was respectively collected in two databases, api29 and api35, available in the package. The distributions of all the variables in the two databases are provided by following the R commands:\n\n\nlibrary(OTrecod)\ndata(api29); data(api35)\nsummary(api29) #--------------------------------------------------\n\n     cds                 apicl_2000  stype    awards   \n Length:418         [200-600] : 93   E:300   No  : 98  \n Class :character   (600-800] :180   M: 68   Yes :303  \n Mode  :character   (800-1000]:145   H: 50   NA's: 17  \n                                                       \n                                                       \n                                                       \n    acs.core        api.stu         acs.k3.20   grad.sch  \n Min.   :16.00   Min.   : 108.0   <=20   :190   0   : 86  \n 1st Qu.:26.00   1st Qu.: 336.2   >20    :108   1-10:180  \n Median :30.00   Median : 447.5   unknown:120   >10 :152  \n Mean   :31.97   Mean   : 577.8                           \n 3rd Qu.:39.00   3rd Qu.: 641.5                           \n Max.   :50.00   Max.   :2352.0                           \n       ell          mobility        meals     full   \n [0-10]  :153   [0-20]  :362   [0-25]  :200   1: 85  \n (10-30] : 83   (20-100]: 56   (25-50] : 56   2:333  \n (30-50] : 60                  (50-75] :100          \n (50-100]: 93                  (75-100]: 62          \n NA's    : 29                                        \n                                                     \n\nsummary(api35) #--------------------------------------------------\n\n     cds            apicl_1999 stype    awards       acs.core    \n Length:362         G1:91      E:257   No  :111   Min.   :16.00  \n Class :character   G2:90      M: 67   Yes :237   1st Qu.:25.00  \n Mode  :character   G3:90      H: 38   NA's: 14   Median :30.00  \n                    G4:91                         Mean   :31.81  \n                                                  3rd Qu.:39.00  \n                                                  Max.   :50.00  \n    api.stu         acs.k3.20   grad.sch         ell      mobility\n Min.   : 102.0   <=20   :227   0   : 50   [0-10]  :164   1:213   \n 1st Qu.: 363.2   >20    : 30   1-10:241   (10-30] : 99   2:149   \n Median : 460.0   unknown:105   >10 : 71   (30-50] : 64           \n Mean   : 577.2                            (50-100]: 10           \n 3rd Qu.: 624.0                            NA's    : 25           \n Max.   :2460.0                                                   \n      meals     full   \n [0-25]  : 77   1:244  \n (25-50] : 92   2:118  \n (50-75] :122          \n (75-100]: 71          \n                       \n                       \n\nThe two databases seem to share a majority of variables (same labels, same encodings) of different types, therefore inferential statistics could be ideally considered by combining all the information of the two databases to study the effects of social factors on the results of the API score in 2000.\nNevertheless, while this target variable called apicl_2000 is correctly stored in the database api29 and encoded as a three levels ordered factors clearly defined: [200-600], (600-800] and (800-1000], the only information related to the API score in the database api35 is the variable apicl_1999 for the API score collected in 1999, encoded in four unknown balanced classes (G1, G2, G3 and G4). As no school is common to the two counties, we easily deduce that these two variables have never been jointly observed.\nBy choosing these two variables as outcomes (called also target variables), the objective of the following examples consists in creating a synthetic database where the missing information related to the API score in 2000 is fully completed in api35 by illustrating the use of the main functions of the package.\nHarmonization of two datasources using merge_dbs\nThe function merge_dbs is an optional pre-process function dedicated to data fusion projects that merges two\nraw databases by detecting possible discrepancies among the respective sets of variables from one database to\nanother. The current discrepancy situations detected by the function follow specific rules described below:\nany variable (other than a target one) whose label (or name) is not common to the two data sources is automatically excluded. By default, the remaining variables are denoted shared variables.\namong the subset of shared variables, any variable stored in a different format (or type) from one datasource to another will be automatically discarded from the subset previously generated and its label saved in an output object called REMOVE1.\namong the remaining subset of shared variables, a factor variable (ordered or not) stored with different levels or number of levels from one data source to another will be automatically discarded from the subset and its label saved in an output object called REMOVE2.\nThese situations sometimes require reconciliation actions which are not supported by the actual version of the merge_dbs function. Therefore, when reconciliation actions are required by the user, they will have to be treated a posteriori and outside the function.\nApplied to our example and according to the introduced rules, the first step of our data fusion project consists in studying the coherence between the variables of the databases api29 and api35 via the following R code:\n\n\nstep1 = merge_dbs(DB1 = api29, DB2 = api35,\n        NAME_Y = \"apicl_2000\", NAME_Z = \"apicl_1999\",\n        row_ID1 = 1, row_ID2 = 1,  \n        ordinal_DB1 = c(2:3, 8:12), \n        ordinal_DB2 = c(2:3, 8:12))\n\n\nAs entry, the raw databases must be declared separately in the DB1 and DB2 arguments and the name of\nthe related target variables of each database must be specified via the NAME_Y and NAME_Z for DB1 and DB2 respectively. In presence of row identifiers, the respective column indexes of each database must be set in the argument row_ID1 and row_ID2. The arguments ordinal_DB1 and ordinal_DB2 list the related\ncolumn indexes of all the variables defined as ordinal in the two databases (including also the indexes of the\ntarget variables if necessary). Here, apicl_2000 is clearly an ordinal variable, and, by default, we suppose\nthat the unknown encoding related to apicl_1999 is also ordinal: the corresponding indexes (2 and 3) are so\nadded in these two arguments.\nAfter running, the function informs users that no row was dropped from the databases during the merging\nbecause each target variable is fully completed in the two databases. Nine potential predictors are kept while\nonly one variable is discarded because of discrepancies between the databases: its identity is consequently stored in output and informs user about the nature of the problem: the mobility factor has different levels from one database to the other.\n\n\nsummary(step1)\n\n              Length Class      Mode     \nDB_READY      12     data.frame list     \nID1_drop       0     -none-     character\nID2_drop       0     -none-     character\nY_LEVELS       3     -none-     character\nZ_LEVELS       4     -none-     character\nREMOVE1        0     -none-     NULL     \nREMOVE2        1     -none-     character\nREMAINING_VAR  9     -none-     character\nIMPUTE_TYPE    1     -none-     character\nDB1_raw       12     data.frame list     \nDB2_raw       12     data.frame list     \nSEED           1     -none-     numeric  \n\nstep1$REMOVE1   # List of removed variables because of type's problem\n\nNULL\n\nstep1$REMOVE2   # List of removed factors because of levels' problem\n\n[1] \"mobility\"\n\nlevels(api29$mobility)  # Verification\n\n[1] \"[0-20]\"   \"(20-100]\"\n\nlevels(api29$mobility); levels(api35$mobility)\n\n[1] \"[0-20]\"   \"(20-100]\"\n[1] \"1\" \"2\"\n\nstep1$REMAINING_VAR\n\n[1] \"acs.core\"  \"acs.k3.20\" \"api.stu\"   \"awards\"    \"ell\"      \n[6] \"full\"      \"grad.sch\"  \"meals\"     \"stype\"    \n\nThe function returns a list object and notably DB_READY, a data.frame whose structure corresponds to the expected structure introduced in the previous subsection: a unique database, here result of the superimposition of api29 on api35 where the first column (DB) corresponds to the database identifier (1 for api29 and 2 for api35), the second and third columns (Y, Z respectively) corresponds to the target variables of the two databases. Missing values are automatically assigned by the function to the unknown part of each target variable: in Y when the identifier equals to 2, in Z when the identifier equals to 1. The subset of shared variables whose information is homogeneous between the databases are now stored from the fourth column to the last one. Their identities are available in the output object called REMAINING_VAR.\nThe merge_dbs function can handle incomplete shared variables by using the impute argument. This\noption allows to keep the missing information unchanged (the choice by default, and also the case in this\nexample), to work with complete cases only, to do fast multivariate imputations by chained equations approach\n(the function integrates the main functionalities of the mice function of the mice package), or to impute data using the dimensionality reduction method introduced in the missMDA package (see the pdf manual for details).\nSelection of matching variables using select_pred\nIn data fusion projects, a selection of shared variables (also called matching variables) appears as an essential step for two main reasons. First, the proportion of shared variables X between the two databases can be important (much higher than three variables) and keeping a pointless part of variability between variables could strongly reduce the quality of the fusion. Second, this selection greatly influences the quality of the predictions regardless of the matching technique which is chosen a posteriori (Adamek 1994). The specific context of data fusion is subject to the following constraints:\nY, Z and X are never jointly observed in database A or B, so the relationships between Y and X, and between Z and X must be investigated separately.\nmatching variables need to be good predictors of both target variables (outcomes) Y and Z (Cohen 1991), in the sense that they explain relevant parts of variability of the targets.\nThese particularities suppose that the investigations have to be done separately but in the same manner in\nboth databases. In this way, a predictor which appears at the same time as highly correlated to Y and Z will be automatically selected for the fusion. On the contrary, predictors whose effects on Y and Z seem not obvious will be discarded. Additional recommended rules also emerge from literature:\nconcerning the subset of variables that would predict only one of the two targets Y or Z, D’Orazio et al. (2006) suggests a moderate parsimonious selection remaining too many predictors could complicate the fusion procedure.\nCibella (2010) and Scanu (2010) suggest to select quality predictors with no error and just a small amount of missing data.\nThe function of the package dedicated to this task is select_pred. From the DB_READY database\ngenerated in the previous subsection, studying the outputs related to each database and produced by the following R commands assist users in selecting the best predictors:\n\n\n# For the dataset api29 --------------\nstep2a = select_pred(step1$DB_READY,\n                     Y = \"Y\", Z = \"Z\", ID = 1, OUT = \"Y\",\n                     quanti  = c(4,6), nominal = c(1,5,7), \n                     ordinal = c(2,3,8:12), thresh_cat = 0.50, \n                     thresh_num = 0.70, RF_SEED = 3011)\n\n# For the dataset api35 --------------\nstep2b = select_pred(step1$DB_READY,\n                     Y = \"Y\", Z = \"Z\", ID = 1, OUT = \"Z\",\n                     quanti  = c(4,6), nominal = c(1,5,7), \n                     ordinal = c(2,3,8:12), thresh_cat = 0.50, \n                     thresh_num = 0.70, RF_SEED = 3011)\n\n\nThe quanti, nominal, and ordinal arguments requires vectors of column indexes related to the type\nof each variable of the input database. The ID argument specifies the column index of the database identifier. Y and Z expected the respective names of the target variables while OUT provides the target variable to predict (Y or Z).\nTo detect the subset of best predictors, select_pred studies the standard pairwise associations between\neach shared variable and the outcomes Y and Z, taken separately (by only varying the argument OUT), according to its type: for numeric and/or ordered factor variables, select_pred calculates the associations using rank correlation coefficients (Spearman) while the Cramer’s V criterion (Bergsma 2013) is used for categorical variables and not ordered factors. The related ranking tables of top scoring predictors are available in two distinct output objects: cor_OUTC_num and cor_OUTC_cat. In our example, the corresponding results, for each database, are:\n\n\n### ASSOCIATIONS BETWEEN TARGET VARIABLES AND SHARED VARIABLES\n\n## Results for the api29 dataset -----\n\nstep2a$cor_OUTC_num   # Y versus numeric or ordinal predictors\n\n  name1    name2 RANK_COR pv_COR_test   N\n7     Y    meals  -0.8030      0.0000 418\n4     Y      ell  -0.7514      0.0000 389\n5     Y     full   0.3919      0.0000 418\n6     Y grad.sch   0.3346      0.0000 418\n3     Y  api.stu  -0.1520      0.0018 418\n8     Y    stype  -0.1520      0.0018 418\n2     Y acs.core  -0.0556      0.2566 418\n\nstep2a$vcrm_OUTC_cat  # Y versus nominal or ordinal predictors\n\n  name1     name2 V_Cramer CorrV_Cramer   N\n7     Y     meals   0.6871       0.6835 418\n4     Y       ell   0.6015       0.5966 389\n5     Y      full   0.4508       0.4459 418\n6     Y  grad.sch   0.3869       0.3816 418\n3     Y    awards   0.2188       0.2073 401\n2     Y acs.k3.20   0.1514       0.1349 418\n8     Y     stype   0.1370       0.1185 418\n\n## Results for the api35 dataset -----\n\nstep2b$cor_OUTC_num   # Z versus numeric or ordinal predictors\n\n  name1    name2 RANK_COR pv_COR_test   N\n4     Z      ell  -0.7511      0.0000 337\n7     Z    meals  -0.7291      0.0000 362\n6     Z grad.sch   0.6229      0.0000 362\n5     Z     full   0.3997      0.0000 362\n3     Z  api.stu  -0.0563      0.2851 362\n8     Z    stype   0.0359      0.4959 362\n2     Z acs.core   0.0119      0.8219 362\n\nstep2b$vcrm_OUTC_cat  # Z versus nominal or ordinal predictors\n\n  name1     name2 V_Cramer CorrV_Cramer   N\n7     Z     meals   0.5181       0.5121 362\n6     Z  grad.sch   0.5131       0.5063 362\n4     Z       ell   0.4775       0.4701 337\n5     Z      full   0.4033       0.3934 362\n3     Z    awards   0.2040       0.1818 348\n8     Z     stype   0.1320       0.0957 362\n2     Z acs.k3.20   0.1223       0.0817 362\n\nThe two first tables related to api29 highlights the strong associations between Y and the variables meals, ell, full and grad.sch in this order of importance, while the summary tables related to api35 highlights the strong associations between Z and the variables meals, grad.sch, ell and full.\nIt is often not unusual to observe that one or more predictors are in fact linear combinations of others. In\nsupervised learning areas, these risks of collinearity increase with the number of predictors, and must be detected beforehand to keep only the most parsimonious subset of predictors for fusion. To avoid collinearity situations, the result of a Farrar and Glauber (FG) test is provided (Farrar and Glauber 1967). This test is based on the determinant of the rank correlation matrix of the shared variables D (Spearman) and the corresponding test statistic is given by:\n\\[\nS_{FG} = - \\left(n-1-\\frac{1}{6} (2k+5)\\times \\ln(\\det(D))\\right)\n\\]\nwhere \\(n\\) is the number of rows and \\(k\\) is the number of covariates. The null hypothesis supposes that\n\\(S_{FG}\\) follows a chi square distribution with \\(k(k-1)/2\\) degrees of freedom, and its acceptation indicates an absence of collinearity. In presence of a large number of numeric covariates and/or ordered factors, the approximate Farrar-Glauber test, based on the normal approximation of the null distribution (Kotz et al. 2000) can be more adapted and the statistic test becomes:\n\\[\n\\sqrt{2S_{FG}} - \\sqrt{2k-1}\n\\]\nUsers will note that these two tests can often appear highly sensitive in the sense that they tend to easily conclude in favor of multicollinearity. Thus, it is suggested to consider these results as indicators of collinearity between predictors rather than an essential condition of acceptability. The results related to this test are stored in the FG_test object of the select_pred output.\nIn presence of collinearity, select_pred tests the pairwise associations between all the potential predictors according to their types (Spearman or Cramér’s V). The thres_num argument fixed the threshold beyond which two ranked predictors are considered as redundant while thres_cat fixed the threshold of acceptability for the Cramér’s V criterion in the subgroup of factors. In output, the results stored in the collinear_PB object permit to identify the corresponding variables. In our example, we observe:\n\n\n### DETECTION OF REDUNDANT PREDICTORS\n\n## Results for the api29 dataset -----\n\n## Results of the Farrar-Glauber test \nstep2a$FG_test       \n\n          DET_X      pv_FG_test pv_FG_test_appr \n     0.04913067      0.00000000      0.00000000 \n\n## Identity of the redundant predictors\n\nstep2a$collinear_PB  \n\n$VCRAM\n       name1 name2 V_Cramer CorrV_Cramer   N\n71 acs.k3.20 stype   0.6988       0.6971 418\n43       ell meals   0.6696       0.6664 389\n34      full meals   0.5215       0.5152 418\n\n$SPEARM\n     name1 name2 RANK_COR pv_COR_test   N\n43     ell meals   0.9047           0 389\n62 api.stu stype   0.7069           0 418\n\n#### Results for the api35 dataset -----\n\nstep2b$FG_test    # Significant result\n\n          DET_X      pv_FG_test pv_FG_test_appr \n      0.1479339       0.0000000       0.0000000 \n\nstep2b$collinear_PB\n\n$VCRAM\n       name1 name2 V_Cramer CorrV_Cramer   N\n71 acs.k3.20 stype   0.6977       0.6957 362\n\n$SPEARM\n[1] name1       name2       RANK_COR    pv_COR_test N          \n<0 rows> (or 0-length row.names)\n\nThe FG test warns the user against the risks of collinearity between predictors, and the function notably detects strong collinearities between the variables meals, ell, full in the api29 (less strong trends in api35): an information that have to be taken into account during the selection. The part of predictors finally kept for the data fusion must be small to improve its quality. When the initial number of shared variables is not too important as in this example, choosing the best candidates between groups of redundant predictors can be made manually by selecting highest ranked predictors in the summary tables previously described. In this way, the variable meals could be preferred to ell, and full, while the variable stype could be dropped. Consequently, a possible final list of predictors could be only composed of the variables meals and grad.sch.\nWhen the number of predictors is important, or when users prefer that an automatic process performs the\nvariable selection, a random forest procedure can also be used via the select_pred function. Random forest approaches (Breiman 2001) are here particularly convenient (Grajski et al. 1986) for multiple reasons: it works fine when the number of variables exceeds the number of rows, whatever the types of covariates, it allows to deal with non linearity, to consider correlated predictors, ordered or not ordered outcomes and to rank good candidate predictors through an inbuilt criterion: the variable importance measure.\nIn few words, random forest processes aggregates many CART models (Breiman et al. 1984) with\nRF_ntree bootstrap samples from the raw data source and averaging accuracies from each model permits to reduce the related variances and also the errors of prediction. A standard random forest process provides two distinct measures of importance of each variable for the prediction of the outcome, the Gini importance criterion and the permutation importance criterion, which depends on the appearance frequency of the predictor but also on its place taken up in each tree. For more details about random forest theory, user can consult Breiman (2001) and/or the pdf manual of the randomForest (Liaw and Wiener 2002) package.\nStrobl et al. (2009) suggests that the permutation importance criterion, which works with permuted samples (subsampling without replacements) instead of bootstrap ones, is particularly convenient with uncorrelated predictors, but must be replaced by a conditional permutation measurement in presence of strong correlations. select_pred provides these assessments by integrating the main functionalities of the cforest and varimp functions of the package party(Hothorn et al. 2005, 2006; Strobl et al. 2007, 2008; Zeileis and Hothorn 2008). However, these measurements must be used with caution, by accounting the following constraints:\nthe Gini importance criterion can produce bias in favor of continuous variables and variables with many categories. This criterion is thus not available in the function.\nthe permutation importance criterion can overestimate the importance of highly correlated predictors and therefore redundant predictors will be discarded beforehand using the first steps of the process integrated in the function.\n\n\nTable 4: Completing the RF_condi argument according to predictors\n\n\nPossible scenarios\n\n\nCorrelation between predictors\n\n\nState of the RF_condi argument\n\n\nIncomplete information\n\n\nSame type predictors\n\n\nNO\n\n\nFALSE\n\n\nAllowed\n\n\nSame type predictors\n\n\nYES\n\n\nTRUE\n\n\nNot allowed\n\n\nDifferent type predictors\n\n\nNO\n\n\nFALSE\n\n\nAllowed\n\n\nDifferent type predictors\n\n\nYES\n\n\nTRUE\n\n\nNot allowed\n\n\nThe select_pred function allows to proceed with different scenarios according to the type of predictors (Table 4 can help users to choose). The first one consists in boiling down to a set of categorical variables (ordered or not) by categorizing all the continuous predictors using the dedicated argument (convert_num and convert_clss) and to work with the conditional importance assessments that directly provide unbiased estimations (by setting the RF_condi argument to TRUE). Users can consult (Hothorn et al. 2006) for more details about the approach and consult the pdf manual of the package for details about the related arguments of the select_pred function.\nThis approach does not take into account incomplete information, so that the method will be applied to complete data only (incomplete rows will be temporarily removed from the study). It is nevertheless possible to impute missing data beforehand by using dedicated pre-existing R packages like mice (van Buuren and Groothuis-Oudshoorn 2011) or by using the imput_cov function provided in the OTrecod package.\nThe second possible scenario (always usable in presence of mixed type predictors), consists in the execution\nof a standard random forest procedure after taking care to rule out collinearity issues by first selecting unique candidates between potential redundant predictors (in this case, the discarded predictors are stored in the drop_var output object). This is the default approach used by select_pred as soon as the RF_condi\nargument is set to FALSE while RF is set to TRUE. This scenario can work in presence of incomplete predictors. By constructing, note that results from random forest procedures stay dependent on successive random draws carried out for the constitution of trees, and it is so suggested to check this stability by testing different random seeds (RF_SEED argument) before concluding.\nThe R command previously written provides automatically the results related to the second approach as\nadditional results. The results from the two datasets show here the permutation importance estimates of each\nvariable ranked in order of importance and expressed as percentage, after resolving collinearity problems:\n\n\nstep2a$RF_PRED   # For the api29 dataset\n\n   meals    stype grad.sch   awards acs.core \n 71.5529  11.6282  11.1181   5.4674   0.2334 \n\nstep2b$RF_PRED   # For the api35 dataset\n\n   meals      ell grad.sch     full    stype  api.stu acs.core \n 35.9974  28.3051  22.2094   5.2143   4.0192   1.8965   1.5643 \n  awards \n  0.7937 \n\nThe results confirm that the variable meals appeared as the best predictor of the target variables in api29 and api35 respectively. The variable ell is not present in the first list (see RF_PRED from step2a) because the variables meals and ell has been detected as strongly collinear (according to the initial chosen threshold) and so select_pred keep the best choice between the two predictors: meals (the reasoning is the same for full which disappeared from the list).\nThe ell variable has been kept in the second list (not found as collinear enough to be removed here)\nand appears moreover as a good predictor of apicl_1999 in api35. Nevertheless its potential collinearity\nproblem with meals encourages us not to keep it for the rest of the study. According to this discussion, we\nfinally keep meals, stype and grad.sch which combines the advantages of being good predictors for the\ntwo target variables while not presenting major problems of collinearity between them.\nThe following synthetic database (called here bdd_ex) is now ready for the prediction of the missing API\nscores in api29, api35 or both, using the function OT_outcome or OT_joint:\n\n\nbdd_ex = step1$DB_READY[, c(1:3,10:12)]; head(bdd_ex,3)\n\n     DB         Y    Z grad.sch    meals stype\n2850  1 (600-800] <NA>      >10   [0-25]     H\n2851  1 (600-800] <NA>      >10   [0-25]     H\n2852  1 [200-600] <NA>     1-10 (75-100]     H\n\nData fusion using OT_outcome\nThe OT_outcome function provides individual predictions of Z in A (and/or Y in B) by considering the recoding problem involving optimal transportation of outcomes. In a first step, the aim of the function is so to determine \\(\\gamma\\) from (2) while this estimate is used in a second step to provide the predictions. The main input arguments of the function and the output values are respectively described in Tables 5 and 6.\n\n\nTable 5: Main arguments of the OT_outcome and OT_joint functions. (*: NULL as default value)\n\n\nArgument\n\n\nOT_outcome\n\n\nOT_joint\n\n\nDescription (default value)\n\n\ndatab\n\n\nX\n\n\nX\n\n\nData.frame in the expected structure\n\n\nindex_DB_Y_Z\n\n\nX\n\n\nX\n\n\nIndexes of the ID, Y, and Z columns (1,2,3)\n\n\nnominal\n\n\nX\n\n\nX\n\n\nColumn indexes of nominal variables (*)\n\n\nordinal\n\n\nX\n\n\nX\n\n\nColumn indexes of ordinal variables (*)\n\n\nlogic\n\n\nX\n\n\nX\n\n\nColumn indexes of boolean variables (*)\n\n\nquanti\n\n\nX\n\n\nX\n\n\nColumn indexes of quantitative variables (*)\n\n\nconvert.num\n\n\nX\n\n\nX\n\n\nColumn indexes of the quantitative variables to convert (* or =quanti in OT_joint)\n\n\nconvert.clss\n\n\nX\n\n\nX\n\n\nCorresponding numbers of desired classes for conversion (*)\n\n\nwhich.DB\n\n\nX\n\n\nX\n\n\nSpecify the target variables to complete: both or only one (BOTH)\n\n\nsolvR\n\n\nX\n\n\nX\n\n\nChoice of the solver to solve the optimization problem (glpk)\n\n\ndist.choice\n\n\nX\n\n\nX\n\n\nDistance function (Euclidean). See Table 7\n\n\npercent.knn\n\n\nX\n\n\nX\n\n\nRatio of closest neighbors involved in the computations (1)\n\n\nindiv.method\n\n\nX\n\n\n\n\nType of individual predictions (sequential) for OUTCOME and R-OUTCOME algorithms\n\n\nmaxrelax\n\n\nX\n\n\nX\n\n\nAdding of a relaxation parameter (0)\n\n\nlambda.reg\n\n\n\n\nX\n\n\nAdding of regularization parameter (0)\n\n\nThe arguments datab, index_DB_Y_Z, quanti, nominal, ordinal, and logic are not optional and must be carefully completed before each execution. A unique synthetic data.frame made of two overlayed databases (called A and B for example) (see Table 3) is expected as datab argument. If this data.frame corresponds to the output objects DB_USED or DB_READY of the select_pred or merge_dbs functions respectively, then the expected object has the required structure. Otherwise users must be sure that their data.frames are made up of two overlayed databases with at least 4 variables as described in the subsection Optimal transportation of outcomes applied to data recoding.\nThe order of the variables have no importance in the raw database but will have to be specified a posteriori in the index_DB_Y_Z argument if necessary.\n\n\nTable 6: Values of the OT_outcome and OT_joint functions. (*: NULL if not required)\n\n\nValue\n\n\nOT_outcome\n\n\nOT_joint\n\n\nDescription\n\n\ntime_exe\n\n\nX\n\n\nX\n\n\nRunning time of the algorithm\n\n\ngamma_A\n\n\nX\n\n\nX\n\n\nEstimation of the joint distribution of (Y, Z) for the prediction of Z in A (*)\n\n\ngamma_B\n\n\nX\n\n\nX\n\n\nEstimation of the joint distribution of (Y, Z) for the prediction of Y in B (*)\n\n\nprofile\n\n\nX\n\n\nX\n\n\nThe list of detected profiles of covariates\n\n\nres.prox\n\n\nX\n\n\nX\n\n\nA list that provides all the information related to the estimated proximities between profiles and groups of profiles\n\n\nestimator_ZA\n\n\nX\n\n\nX\n\n\nEstimates of the probability distribution of Z conditional to X and Y in database A (*)\n\n\nestimator_YB\n\n\nX\n\n\nX\n\n\nEstimates of the probability distribution of Y conditional to X and Z in database B (*)\n\n\nDATA1_OT\n\n\nX\n\n\nX\n\n\nThe database A fully completed (if required in input by the which.DB argument)\n\n\nDATA2_OT\n\n\nX\n\n\nX\n\n\nThe database B fully completed (if required in input by the which.DB argument)\n\n\nThe subset of remaining predictors used for fusion may requires prior transformations according to the distance function chosen in input by the user. This process is fixed by setting the dist.choice argument. The distances actually implemented in the function are: the standard Euclidean (\"E\") and Manhattan (\"M\") distances, the Hamming distance (\"H\", for binary covariates), and the Gower distance (\"G\" sometimes preferred with mixed variables). Automatic transformations prior to the use of each distance function are summarized in Table 7.\n\n\nTable 7: Internal variable transformations related to the choice of each distance function in OT_outcome and OT_joint . (*) If the number of covariates exceeds 1. T for table\n\n\n\n\n\nDistance function\n\n\n\nVariable transformations\n\n\nEuclidean\n\n\nManhattan\n\n\nGower\n\n\nHamming\n\n\nContinuous\n\n\nStandardized\n\n\nStandardized\n\n\nNo\n\n\nNot allowed\n\n\nBoolean\n\n\nBinary\n\n\nBinary\n\n\nNo\n\n\nBinary\n\n\nNominal\n\n\nDisjunctive T\n\n\nDisjunctive T\n\n\nNo\n\n\nDisjunctive T\n\n\nOrdinal\n\n\nDiscrete\n\n\nDiscrete\n\n\nNo\n\n\nDisjunctive T\n\n\nIncomlete information\n\n\nAllowed*\n\n\nAllowed*\n\n\nAllowed*\n\n\nAllowed*\n\n\ndist.choice argument\n\n\nE\n\n\nM\n\n\nG\n\n\nH\n\n\nThe first version of the OT algorithm described in Garès et al. (2020) was tested on numeric coordinates\nextracted from a factor analysis of mixed data (FAMD) fitted on mixed raw covariates (Pagès 2002). This transformation is here available by setting the FAMD.coord argument to \"YES\". The minimal percentage\nof information explained by the FAMD is also fixable using the FAMD.perc argument. The OT_outcome\nfunctions proposes four types of models for the prediction of Y in B (and/or) Z in A, according to the values of the method and maxrelax arguments:\nWhen maxrelax = 0 and indiv.method = \"sequential\" (default options), the fitted model corresponds to the OUTCOME algorithm described by \\(\\hat{\\mathcal{P}}^0_n\\) in equation (6). Assuming that \\(Y\\) and \\(Z\\) in \\(A\\) follow the same distribution as \\(Y\\) and \\(Z\\) in \\(B\\) respectively (assumption 1), this related algorithm derives the joint distribution of \\(Y\\) and \\(Z\\) in \\(A\\) (respectively in \\(B\\)) in a first step, and uses in a second step, a nearest neighbor procedure to predict missing values of \\(Z\\) in \\(A\\) (resp. \\(Y\\) in \\(B\\)). This algorithm calculates averaged distances between each subject from \\(A\\) (resp. \\(B\\)) and subgroups of subjects from \\(B\\) (resp. \\(A\\)) having same levels of \\(Z\\) in \\(B\\) (resp. \\(Y\\) in \\(A\\)). These calculations can be done using all subjects of each subgroups (by default, percent.knn = 1) or only restricted parts of them (percent.knn < 1).\nWhen maxrelax > 0 and indiv.method = \"sequential\", assumption 1 is alleviated by relaxing the constraints on marginal distributions and an R-OUTCOME algorithm (with relaxation) is fitted. In this case, the maxrelax argument corresponds to the \\(\\alpha_n\\) parameter in (18).\nWhen maxrelax = 0 and indiv.method = \"optimal\", the second step of the original algorithm (nearest neighbor procedure) is replaced by a linear optimization problem: searching for the individual predictions of \\(Z\\) in \\(A\\) (resp. \\(Y\\) in \\(B\\)) by minimizing the total of the distances between each individual of \\(A\\) and individuals of each levels of \\(Z\\) in \\(B\\).\nWhen maxrelax > 0 and indiv.method = \"optimal\", the constraints on marginal distributions of the previous model are also relaxed and the function fits an R-OUTCOME algorithm with relaxation. For these three last situations, the corresponding R-OUTCOME algorithm is so described by \\(\\hat{\\mathcal{P}}^{0-R}_n\\) (18).\nWhen maxrelax > 0, it is recommended to calibrate the maxrelax parameter by testing different values according to the stability of individual predictions. In output, the gamma_A and gamma_B} matrices correspond to the estimates of the joint distributions \\(\\gamma\\) of \\((Y^A,Z^A)\\) and \\((Y^B,Z^B)\\) respectively (2). Moreover, a posteriori estimates of conditional distributions probabilities \\((Z^A|Y^A, X^A)\\) and \\((Y^B|Z^B,X^B)\\) (10) are also provided in two lists called estimatorZA, and estimatorYB respectively and the completed databases \\(A\\) and \\(B\\) are stored in the DATA1_OT and DATA2_OT objects. In particular, the individual predictions are stored in the OTpred column of these data.frames. Moreover, the profile object is a data.frame that stores the profile of covariates encountered in the two data sources while the res_prox object stored all the distance computations that will be used in the validation step (users can consult details of the proxim_dist function of the pdf manual.\nThe computation of conditional probabilities implies to define the part of individuals considered as neighbors of each encountered profile of covariates. The argument prox.dist fixes this threshold for each profile, following the decision rule for \\(A\\) (and respectively for \\(B\\)): a subject \\(i\\) of \\(A\\) (or a statistical unit corresponding to a row of \\(A\\)) will be considered as neighbor of a given profile of shared variables \\(C\\) as soon as:\n\\[\n\\text{dist}(subject_i,C) <  \\text{prox.dist} \\times \\text{max}_{k=1,\\dots , n_A}  \\text{dist}(subject_k,C)\n\\]\nWhen the number of shared variables is small and all of them are categorical (optimal situation), it is suggested to set the prox.dist argument to \\(0\\). Finally, using the which.DB argument, users can choose to estimate the individual predictions of \\(Y\\) and \\(Z\\) in the two databases (which.DB = \"BOTH\") or only one of the both (which.DB = \"A\" for the predictions of \\(Z\\) in \\(A\\) or which.DB = \"B\" for the predictions \\(Y\\) in \\(B\\)).\nFrom the data.frame bdd_ex built previously, we use the OT_outcome function to illustrate the prediction of the target variable apicl_2000 in the api35 dataset via an OUTCOME algorithm and using an Euclidean distance function:\n\n\noutc1 = OT_outcome(bdd_ex, quanti = 1, ordinal = 2:6, \n                   dist.choice = \"E\", indiv.method = \"sequential\", \n                   which.DB = \"B\")\n\n#---------------------------------------\n# OT PROCEDURE in progress ...\n#---------------------------------------\n# Type                     = OUTCOME\n# Distance                 = Euclidean\n# Percent closest knn      = 100%\n# Relaxation parameter     = NO\n# Relaxation value         = 0\n# Individual pred process  = Sequential\n# DB imputed               = B\n#---------------------------------------\n\n\nIn bdd_ex, all variables except the first one (DB: the database identifier) are or can be considered as ordinal factors, and the quanti and ordinal arguments are filled in accordingly. For the prediction of apicl_2000 in the api35 dataset (the steps would be the same for the prediction of apicl_1999 in api29 by setting which.DB = \"A\" or \"BOTH\"), the optimal transportation theory determines a map \\(\\gamma\\) that pushes, in the distribution of apicl_1999 forward to the distribution of apicl_2000 in the database api35. In this case, \\(\\gamma^B\\) is an estimator of the joint distribution of apicl_2000 and apicl_1999 in api35. The estimation of \\(\\gamma^B\\) is available in the gamma_B output object while all the profiles of predictors met in the two databases are listed in the profile object:\n\n\noutc1$gamma_B\n\n                   G1        G2         G3        G4\n[200-600]  0.22248804 0.0000000 0.00000000 0.0000000\n(600-800]  0.02889318 0.2486188 0.15311005 0.0000000\n(800-1000] 0.00000000 0.0000000 0.09550874 0.2513812\n\noutc1$profile[1,]         # the first profile\n\n      ID grad.sch meals stype\n2850 P_1        3     1     3\n\nThe output object estimatorYB is a list that corresponds to the estimations of the conditional probabilities of apicl_2000 in the api35 dataset for a given profile of predictors. For example, the conditional probabilities related to the first profile of predictors are:\n\n\noutc1$estimatorYB[1,,]    # conditional probabilities (1st profile)\n\n        [,1]      [,2]      [,3]\nG1 0.3333333 0.3333333 0.3333333\nG2 0.3333333 0.3333333 0.3333333\nG3 0.0000000 0.0000000 1.0000000\nG4 0.0000000 0.0000000 1.0000000\n\nAccording to these results, we can conclude that: for a subject from api35 with a related profile of predictors P_1 and whose levels of apicl_1999 is 'G1', the probability for apicl_2000 to be predicted '[200,600]' is about \\(0.63\\). Finally, the individual predictions of apicl_2000 in api35 are available in the DATA2_OT object corresponding to the OTpred column:\n\n\nhead(outc1$DATA2_OT,3)  # The 1st 3 rows only\n\n     DB    Y  Z grad.sch meals stype    OTpred\n3895  2 <NA> G1        2     4     1 [200-600]\n3896  2 <NA> G3        2     3     1 (600-800]\n3897  2 <NA> G2        2     3     2 (600-800]\n\nThe OUTCOME algorithm uses here a nearest neighbor procedure to assign the individual predictions from the estimation of \\(\\gamma\\) which can be a drawback as described in (Garès et al. 2020). The R-OUTCOME algorithm proposes an enrichment that directly assigns the individual predictions of apicl_2000 from the estimation of \\(\\gamma\\), without using the nearest neighbor approach. In this case, a linear optimization problem is solved to determine the individual predictions that minimize the sum of the individual distances in api35 having the modalities of the target apicl_2000 in api29. The corresponding R command is:\n\n\n### R-OUTCOME algorithm: optimal assignments + relaxation parameter = 0\nR_outc3 = OT_outcome(bdd_ex, quanti = 1, ordinal = 2:6, \n                     dist.choice = \"E\" , indiv.method = \"optimal\",\n                     which.DB = \"B\")\n\n\nMoreover, the OUTCOME algorithm assumes that the distribution of apicl_2000 is identically distributed in api29 and api35 (assumption 1 from the subsection Optimal transportation of outcomes applied to data recoding) which can appear as a strong hypothesis to hold in many situations. To overcome this constraint, the R-OUTCOME algorithm also allows to relax the assumption 1 by adding a relaxation parameter (18) in the optimization system. This relaxation can be done by varying the maxrelax argument of the function:\n\n\n### R-OUTCOME algorithm: optimal assignments + relaxation parameter = 0.4\nR_outc4 = OT_outcome(bdd_ex, quanti = 1, ordinal = 2:6, \n                     dist.choice = \"E\",\n                     indiv.method = \"optimal\", \n                     maxrelax = 0.4, which.DB = \"B\")\n\n\nThe running times of these two models can take few minutes. The quality of these models will be compared in Tables 8, 9 and 10 (subsection Validation of the data fusion using verif_OT).\nData fusion using OT_joint\nThe OT_joint function provides individual predictions of \\(Z\\) in \\(A\\) (and/or \\(Y\\) in \\(B\\)) by considering the recoding problem as an optimal transportation problem of covariates and outcomes, that pushes the conditional distribution of \\((Y^A|X^A)\\) forward to the conditional distribution of \\((Z^A|X^A)\\) (and conversely \\((Z^B|X^B)\\) forward to the conditional distribution of \\((Y|X)\\)). The aim is to determine \\(\\gamma\\) from (7). Because joint distributions of outcomes and covariates are now mapped together (it was not the case with the OUTCOME family of algorithms), it is not required for target variables to be equally distributed (assumption 1).\nThe call of OT_joint was thought to propose globally the same structure as those of the function OT_outcome as described in Tables 5 and 6 with few differences described below. JOINT and R-JOINT algorithms are usable via OT_joint for solving the recoding problem depending on the values related to the maxrelax and lambda_reg arguments:\nWhen maxrelax = 0 and lambda.reg = 0 (default values), the fitted model corresponds to the JOINT algorithm described by \\(\\hat{\\mathcal{P}}_n\\) in equation (7).\nWhen at least one of these two arguments differs from 0, the R-JOINT algorithm is called. Using R-JOINT, it is so possible to relax constraints on marginal distributions (maxrelax > 0) and/or add an eventual more or less strong L1 regularisation term among the constraints of the algorithm by filling the lambda_reg argument. maxrelax parameter correspond to parameter \\(\\alpha_n\\) in (16) and lambda.reg parameter correspond to parameter \\(\\lambda\\) in (17).\nWhen maxrelax > 0 and/or lambda.reg > 0, it is recommended to calibrate these two parameters by testing many different values and so studying the stability of the related individual predictions. Nevertheless, by default or as a starting point, (Garès et al. 2020) suggests the use of default values determined from simulated databases: \\(0.1\\) for the regularization parameter (lambda.reg) and \\(0.4\\) for the relaxation one (maxrelax). Finally, the output objects are similar to those previously described in the OT_outcome function.\nThe implementation of the function is very similar to that of OT_outcome, and applied to our example, the R code related to a JOINT algorithm is:\n\n\noutj1 = OT_joint(bdd_ex, nominal = 1, ordinal = c(2:6), \n                 dist.choice = \"E\"  , which.DB = \"B\")\n\n#---------------------------------------\n# OT JOINT PROCEDURE in progress ...\n#---------------------------------------\n# Type                  = JOINT\n# Distance              = Euclidean\n# Percent closest       = 100%\n# Relaxation term       = 0\n# Regularization term   = 0\n# Aggregation tol cov   = 0.3\n# DB imputed            = B\n#---------------------------------------\n\n\n### Extract individual predictions from the OTpred column\nhead(outj1$DATA2_OT,3)  # The 1st 3 rows only\n\n     DB    Y  Z grad.sch meals stype    OTpred\n3895  2 <NA> G1        2     4     1 [200-600]\n3896  2 <NA> G3        2     3     1 (600-800]\n3897  2 <NA> G2        2     3     2 (600-800]\n\nFor relaxing the constraints stated on marginal distributions, we use the maxrelax argument that corresponds to varying the \\(\\alpha\\) parameter of a R-JOINT algorithm (see (17)) and a parameter of regularization \\(\\lambda\\) can be simply added to the previous model as follows:\n\n\n### R-JOINT algorithm (relaxation parameter = 0.4)\nR_outj1 = OT_joint(bdd_ex, nominal = 1, ordinal = c(2:6), \n                   dist.choice = \"E\",  maxrelax = 0.4,\n                   which.DB = \"B\")\n\n### R-JOINT algorithm (relaxation parameter = 0.4,\n###                                   & regularization parameter = 0.1)\nR_outj4 = OT_joint(bdd_ex, nominal = 1, ordinal = c(2:6), \n                   dist.choice = \"E\",  maxrelax = 0.4,\n                   lambda.reg = 0.1,\n                   which.DB = \"B\")\n\n\nThese models are compared in the next subsection.\nValidation of the data fusion using verif_OT\nAssessing the quality of individual predictions obtained through statistical matching techniques can be a complex task notably because it is legitimate to wonder about the true reliability of a joint distribution estimated from two variables which are never jointly observed (Rodgers 1984; Kadane 2001). When the study aims to identify estimators of interests that improve the understanding of the relationships between variables in the different databases (called macro approach in D’Orazio et al. (2006)) without going until the creation of a complete and unique dataset, uncertainty analyses are usually proposed to estimate the sensitivity of the assessments (Rässler 2002). On the contrary, if the objective is to create a synthetic dataset where the information is available for every unit, the use of auxiliary information or proxy variables must be privileged to assess the quality of the results (Paass 1986). In the absence of complementary information, Rubin (1986) suggests to study the preservation of the relationships at best between the distributions of the target variables. In this way, the verif_OT function proposes tools to assess quality of the individual predictions provided by the algorithms previously introduced.\nThe first expected input argument of verif_OT is an 'otres' object from the OT_outcome or OT_joint functions. In our example, this function is firstly applied to the out_c1 model by following the R command:\n\n\n### Quality criteria for outc1 (OUTCOME model)\nverif_outc1   = verif_OT(outc1, group.class = TRUE, ordinal = FALSE, \n                         stab.prob = TRUE, min.neigb = 5)\n\n### First results related to outc1:\nverif_outc1$nb.profil\n\n[1] 27\n\nverif_outc1$res.prox\n\n       N   V_cram rank_cor \n 362.000    0.860    0.892 \n\nIn output, the nb.profil value gives the number of profiles of predictors globally detected in the two databases \\((nb = 27)\\). In the example, a profile will be characterized by a combination of values related to the three predictors kept: grad.sch, meals and stype.\nThe first step of the function is dedicated to the study of the proximity between the two target variables. Therefore, standard criteria (Cramer’s V and Spearman’s rank correlation coefficient) are used to evaluate the pairwise association between \\(Y\\) and \\(Z\\), globally or in one of the two databases only, according to the predictions provided by the output of OT_outcome or OT_joint. Only the database api35 was completed in the example (because which.DB = \"B\" as argument of OT_outcome) and stored in the DATA2_OT object, therefore, the criteria compare here the proximity distribution between the predicted values of apicl_2000 (Y) and the observed value of apicl_1999 (Z) in the database api35 (B). Regarding independence or a small correlation between \\(Y^A\\) and \\(Z^A\\) (or \\(Y^B\\) and \\(Z^B\\)) must question about the reliability of the predictions especially when \\(Y\\) and \\(Z\\) are supposed to summarize a same information. In the example, whatever the criteria used, we notice via the res.prox object, a strong association between the two target variables in api35 which reassures about the consistency of the predictions.\nThe related confusion matrix between the predicted values of apicl_2000 (Y) and the observed value of apicl_1999 (Z) is stored in the conf.mat object.\nSecond, the function proposes an optional tool (by setting group.clss= TRUE) which evaluates the impact of grouping levels of one factor on the association of \\(Y\\) and \\(Z\\). When users have initially no information about one of the two encodings of interest, this approach can be particularly useful to detect ordinal from nominal ones and its principle is as follow. Assuming that \\(Y \\in \\mathcal{Y}\\), and \\(Z \\in \\mathcal{Z}\\) (\\(\\mathcal{Y}\\) and \\(\\mathcal{Z}\\) are the respective levels) and that \\(|\\mathcal{Y}| \\geq |\\mathcal{Z}|\\). From \\(Y\\), successive new variables \\(Y’ \\in \\mathcal{Y}'\\) are built, as soon as \\(\\mathcal{Y}'\\) is a partition of \\(\\mathcal{Y}\\) such as \\(|\\mathcal{Y}'| = |\\mathcal{Z}|\\) (and inversely for \\(Z\\) if the levels of \\(Z\\) is the greatest). The related associations between \\(Z\\) and \\(Y’\\) (with now equal number of levels) are then studied using: Cramer’s V, rank correlation, Kappa coefficient and confusion matrix and the results are stored in a table called res.grp. The corresponding outputs of the example are:\n\n\nverif_outc1$conf.mat\n\n            Z\npredY         G1  G2  G3  G4 Sum\n  [200-600]   81   1   1   1  84\n  (600-800]   10  89  55   1 155\n  (800-1000]   0   0  34  89 123\n  Sum         91  90  90  91 362\n\nverif_outc1$res.grp\n\n  grp levels Z to Y error_rate Kappa Vcramer RankCor\n4       G1/G2 G3/G4       13.3 0.794    0.83   0.877\n6       G1/G2/G3 G4       19.1 0.714    0.78   0.839\n2       G1 G3/G2/G4       28.2 0.593    0.68   0.624\n1       G1 G2/G3/G4       37.6 0.457    0.64   0.813\n3       G1 G4/G2/G3       43.4 0.374    0.59   0.115\n5       G1/G2 G4/G3       43.4 0.326    0.64   0.574\n\nIt appears from these results that grouping the levels G2 and G3 of apicl_1999 (Z) strongly improves the association of this variable with apicl_2000 (Y) (the error rate of the confusion matrix varies from \\(43.4\\) to \\(13.3\\)). Moreover the structure of conf.mat confirms that the encoding of apicl_1999 seems to be ordinal.\nThe third step of the quality process integrated in the function corresponds to the study of the Hellinger distance (Liese and Miescke 2008). This distance function is used as a measure of the discrepancies between the observed and predicted distributions of Y (\\(\\mathcal{L}(Y^A)\\) versus \\(\\mathcal{L}(\\widehat{Y}^B)\\)) and/or (\\(\\mathcal{L}(\\widehat{Z}^A)\\) versus \\(\\mathcal{L}(Z^B)\\)). For \\(Y\\) and \\(Z\\), the definition of the distance is respectively:\n\\[\n\\text{dist}_{\\text{hell}} (Y^A,\\widehat{Y}^B)= \\sqrt{\\frac{1}{2} \\sum_{y\\in \\mathcal{Y}}\\left(\\sqrt{\\mu_{n,y}^{Y^A}}- \\sqrt{\\mu_{n,y}^{\\widehat{Y}^B}}\\right)^2 )}\n\\]\nand\n\\[\n\\text{dist}_{\\text{hell}} (Z^B,\\widehat{Z}^A)= \\sqrt{\\frac{1}{2} \\sum_{z\\in \\mathcal{Z}}\\left(\\sqrt{\\mu_{n,z}^{Z^B}}- \\sqrt{\\mu_{n,z}^{\\widehat{Z}^A}}\\right)^2 )},\n\\]\nwhere \\(\\mu_{n,y}^{\\widehat{Y}^B}\\) and \\(\\mu_{n,z}^{\\widehat{Z}^A}\\) correspond to the empirical estimators of \\(\\mu^{\\widehat{Y}^B}\\) and \\(\\mu^{\\widehat{Z}^A}\\) respectively.\nThe Hellinger distance varies between 0 (identical) and 1 (strong dissimilarities) while 0.05 can be used as an acceptable threshold below which two distributions can be considered as similar. When OUTCOME models are applied on datasets, this criterion shows that predictions respect assumption 1. It can also be used to determine the best relaxation parameter of an R-OUTCOME model. On the contrary, there is no need to interpret this criterion when an R-JOINT model is applied, because in this case, the assumption 1 is not required. Results related to this criterion are stored in the hell object:\n\n\nverif_outc1$hell\n\n                YA_YB ZA_ZB\nHellinger dist. 0.008    NA\n\nWith a p-value equals to \\(0.008\\), the assumption 1 hold here but it stays interesting to test other algorithms to eventually improve the current one by notably adding relaxation and/or regularization parameters.\nFinally, the verif_OT function uses the mean and standard deviance of the conditional probabilities \\(\\mathbb{P}(Z=\\hat{z}_i|Y=y_i,X=x_i)\\) estimated by each model, as indicators of the stability of the individual predictions (provided that stab.prob = TRUE). It is nevertheless possible that conditional probabilities are computed from too few individuals (according to the frequency of each profile of shared variables met), to be considered as a reliable estimate of the reality. To avoid this problem, trimmed means and standard deviances are suggested by removing these specific probabilities from the computation, using the min.neigb parameter. In output, the results related to this last study are stored in the res.stab object:\n\n\nverif_outc1$eff.neig\n\n   Nb.neighbor Nb.Prob\n1            1      14\n2            2      18\n3            3      18\n4            4      28\n5            5      20\n6            6       6\n7            7      14\n8            8      16\n9            9      27\n10          10      20\n\nverif_outc1$res.stab\n\n         N min.N  mean    sd\n2nd DB 284     5 0.968 0.122\n\nThe first result shows that \\(14\\) individual predictions among \\(362\\) (the total number of rows in api35) have been assigned to subjects that exhibits a unique combination of predictors and outcome. From these subjects, it would be obviously overkill to draw conclusions about the reliability of the predictions provided by the model. We therefore decide to fix here a threshold of \\(5\\) below which it is difficult to extract any information related to the prediction. From the remaining ones, we simply perform the average (\\(0.968\\)) which could be interpreted as follows: when the fitted model is confronted with a same profile of predictors and a same level of apicl_1999, more than \\(96\\) times of a hundred, it will return the same individual prediction for apicl_2000.\nWe run a total of \\(11\\) models to determine the optimal one for the prediction of apicl_2000 in api35.\nEvery arguments from the syntax of the OT_outcome and OT_joint functions stay unchanged with the exception of indiv.method, maxrelax, and lambda.reg which vary.\nThe values linked to each criterion of the verif_OT function are summarized in Tables 8, 9 and 10. The syntax related to verif_OT stay unchanged for each model (notably same min.neigb arguments). Note that comparisons of stability predictions between OUTCOME and JOINT models impose that the prox.dist argument of the OT_outcome function is fixed to \\(0\\).\n\n\nTable 8: V Cramer criterion (V_cram) and rank correlation (rank_cor) between apicl_1999 (the observed variable) and apicl_2000 (the predicted variable) in the api35 database (N=362). Predictions comes from 11 models with various algorithms, relaxation and regularization parameters. They are very stable and reproductible here when the relaxation parameter differs from 0.\n\n\nModel\n\n\nType\n\n\nMethod\n\n\nRelax\n\n\nRegul\n\n\nN\n\n\nV_cram\n\n\nrank_cor\n\n\noutc1\n\n\nOUTCOME\n\n\nSEQUENTIAL\n\n\n0.0\n\n\n-\n\n\n362\n\n\n0.86\n\n\n0.892\n\n\nR_outc1\n\n\nR-OUTCOME\n\n\nSEQUENTIAL\n\n\n0.4\n\n\n-\n\n\n362\n\n\n0.94\n\n\n0.923\n\n\nR_outc2\n\n\nR-OUTCOME\n\n\nSEQUENTIAL\n\n\n0.6\n\n\n-\n\n\n362\n\n\n0.91\n\n\n0.917\n\n\nR_outc3\n\n\nR-OUTCOME\n\n\nOPTIMAL\n\n\n0.0\n\n\n-\n\n\n362\n\n\n0.87\n\n\n0.911\n\n\nR_outc4\n\n\nR-OUTCOME\n\n\nOPTIMAL\n\n\n0.4\n\n\n-\n\n\n362\n\n\n0.95\n\n\n0.939\n\n\nR_outc5\n\n\nR-OUTCOME\n\n\nOPTIMAL\n\n\n0.6\n\n\n-\n\n\n362\n\n\n0.92\n\n\n0.932\n\n\noutj1\n\n\nJOINT\n\n\n-\n\n\n0.0\n\n\n0.0\n\n\n362\n\n\n0.74\n\n\n0.834\n\n\nR_outj1\n\n\nR-JOINT\n\n\n-\n\n\n0.4\n\n\n0.0\n\n\n362\n\n\n0.95\n\n\n0.935\n\n\nR_outj2\n\n\nR-JOINT\n\n\n-\n\n\n0.6\n\n\n0.0\n\n\n362\n\n\n0.91\n\n\n0.927\n\n\nR_outj3\n\n\nR-JOINT\n\n\n-\n\n\n0.8\n\n\n0.0\n\n\n362\n\n\n0.91\n\n\n0.927\n\n\nR_outj4\n\n\nR-JOINT\n\n\n-\n\n\n0.4\n\n\n0.1\n\n\n362\n\n\n0.95\n\n\n0.931\n\n\nFrom these results, we can conclude that:\nwhatever the algorithm used (OUTCOME or JOINT), adding a relaxation parameter improves here the association between the target variables (see Table 8).\naccording to the results of Table 9, the R_outc2 and R_outc5 models seem not optimal because they are those for which the Hellinger criterion reflects the most clear violation of assumption 1 (see Table 9). Therefore, it is here suggested to keep a relaxation parameter less than \\(0.6\\) in the final model.\nTable 8 confirms this trend because adding a too high relaxation parameter seems to potentially affect the quality of the association (the V Cramer and rank correlation decrease when the relaxation parameter increases from \\(0.4\\) to \\(0.6\\)). Consequently, in this example, fixing \\(0.4\\) seems to be an acceptable compromise for the relaxation parameter whatever the R-OUTCOME algorithm used (\\(0.3\\) could also have been tested here).\namong the remaining models (R_outc1, R_outc4, R_outj1, and R_outj4), R_outj1 and R_outj4 seem to be those with the most stable predictive potential (Table 10). Moreover, R_outc4 appears here as the best model from the tested R-OUTCOME algorithms.\nadding a regularization parameter to the R_outj1 model caused a decrease in the stability of the predictions (see the value of R_outj4 compared to R_outj1 in Table 10) and we thus conclude in favor of R_outj1 as best model among those tested in the JOINT family of algorithms.\nTable 11 shows that the three remaining models (R_outc4, R_outj1 and R_outj4) counted between \\(92\\) an \\(97\\%\\) of common predictions and these last result also reassures the user about the quality of the provided predictions.\n\n\nTable 9: Hellinger distances related to the 6 models that used OUTCOME and R-OUTCOME algorithms. The values are relatively homogeneous from one model to another and do not indicate strong ditributional divergences (all values are very far from 1). Nevertheless, choosing here relaxation parameters higher than 0.4 can increase the risk of distributional dissimilarities (indeed, the criterion moves away from 0.05 when the relaxation parameter increases).\n\n\nModel\n\n\nType\n\n\nMethod\n\n\nRelax\n\n\nHell(YA_YB)\n\n\noutc1\n\n\nOUTCOME\n\n\nSEQUENTIAL\n\n\n0.0\n\n\n0.008\n\n\nR_outc1\n\n\nR-OUTCOME\n\n\nSEQUENTIAL\n\n\n0.4\n\n\n0.085\n\n\nR_outc2\n\n\nR-OUTCOME\n\n\nSEQUENTIAL\n\n\n0.6\n\n\n0.107\n\n\nR_outc3\n\n\nR-OUTCOME\n\n\nOPTIMAL\n\n\n0.0\n\n\n0.002\n\n\nR_outc4\n\n\nR-OUTCOME\n\n\nOPTIMAL\n\n\n0.4\n\n\n0.080\n\n\nR_outc5\n\n\nR-OUTCOME\n\n\nOPTIMAL\n\n\n0.6\n\n\n0.102\n\n\n\n\nTable 10: Stability of the predictions (min.neigb = 5). When the R_outj1 model will be confronted with a same profile of predictors and a same level of apicl_1999, more than 94 times of a hundred (mean = 0.942), it will be able to return the same individual prediction for apicl_2000. Among the OUTCOME family of algorithms, R_outc4 provides here the best stability of prediction while R_outj2 is the optimal one in the JOINT family of algorithms.\n\n\nModel\n\n\nType\n\n\nMethod\n\n\nRelax\n\n\nRegul\n\n\nN\n\n\nmean\n\n\nsd\n\n\noutc1\n\n\nOUTCOME\n\n\nSEQUENTIAL\n\n\n0.0\n\n\n-\n\n\n284\n\n\n0.968\n\n\n0.122\n\n\nR_outc1\n\n\nR-OUTCOME\n\n\nSEQUENTIAL\n\n\n0.4\n\n\n-\n\n\n284\n\n\n0.950\n\n\n0.151\n\n\nR_outc2\n\n\nR-OUTCOME\n\n\nSEQUENTIAL\n\n\n0.6\n\n\n-\n\n\n284\n\n\n0.954\n\n\n0.145\n\n\nR_outc3\n\n\nR-OUTCOME\n\n\nOPTIMAL\n\n\n0.0\n\n\n-\n\n\n284\n\n\n0.979\n\n\n0.100\n\n\nR_outc4\n\n\nR-OUTCOME\n\n\nOPTIMAL\n\n\n0.4\n\n\n-\n\n\n284\n\n\n0.987\n\n\n0.080\n\n\nR_outc5\n\n\nR-OUTCOME\n\n\nOPTIMAL\n\n\n0.6\n\n\n-\n\n\n284\n\n\n0.983\n\n\n0.091\n\n\noutj1\n\n\nJOINT\n\n\n-\n\n\n0.0\n\n\n0.0\n\n\n284\n\n\n0.911\n\n\n0.116\n\n\nR_outj1\n\n\nR-JOINT\n\n\n-\n\n\n0.4\n\n\n0.0\n\n\n284\n\n\n0.942\n\n\n0.128\n\n\nR_outj2\n\n\nR-JOINT\n\n\n-\n\n\n0.6\n\n\n0.0\n\n\n284\n\n\n0.953\n\n\n0.171\n\n\nR_outj3\n\n\nR-JOINT\n\n\n-\n\n\n0.8\n\n\n0.0\n\n\n284\n\n\n0.934\n\n\n0.199\n\n\nR_outj4\n\n\nR-JOINT\n\n\n-\n\n\n0.4\n\n\n0.1\n\n\n284\n\n\n0.926\n\n\n0.097\n\n\n\n\nTable 11: Ratio of common predictions between two models\n\n\nModel\n\n\noutc1\n\n\nR_outc1\n\n\nR_outc4\n\n\noutj1\n\n\nR_outj1\n\n\nR_outc1\n\n\n0.84\n\n\n-\n\n\n-\n\n\n-\n\n\n-\n\n\nR_outc4\n\n\n0.83\n\n\n0.95\n\n\n-\n\n\n-\n\n\n-\n\n\noutj1\n\n\n0.72\n\n\n0.81\n\n\n0.83\n\n\n-\n\n\n-\n\n\nR_outj1\n\n\n0.83\n\n\n0.91\n\n\n0.94\n\n\n0.84\n\n\n-\n\n\nR_outj4\n\n\n0.84\n\n\n0.96\n\n\n0.97\n\n\n0.84\n\n\n0.92\n\n\n\n\nTable 12: Confusion matrices in the api35 dataset for the models (a) R_outc4 and (b) R_outj1\n\n\n\n(a)\n\n\n\n\napicl_1999\n\n\n\n\n\n\n(b)\n\n\n\n\napicl_1999\n\n\n\napicl_2000\n\n\nG1\n\n\nG2\n\n\nG3\n\n\nG4\n\n\n\n\napicl_2000\n\n\nG1\n\n\nG2\n\n\nG3\n\n\nG4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[200-600]\n\n\n91\n\n\n15\n\n\n0\n\n\n0\n\n\n\n\n[200-600]\n\n\n91\n\n\n14\n\n\n1\n\n\n0\n\n\n[600-800]\n\n\n0\n\n\n75\n\n\n90\n\n\n0\n\n\n\n\n[600-800]\n\n\n0\n\n\n76\n\n\n89\n\n\n0\n\n\n[800-1000]\n\n\n0\n\n\n0\n\n\n0\n\n\n91\n\n\n\n\n[800-1000]\n\n\n0\n\n\n0\n\n\n0\n\n\n91\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe confusion matrices related to R_outc4 and R_outj1 are described in Table 12 and seems to confirm that the encoding of apicl_2000 in three groups, could simply correspond to the grouping of levels \\(G2\\) and \\(G3\\) of the api_cl1999 variable.\nFinally, note that the running time of each model took less than 15 seconds with R version \\(4.0.3\\) for Windows (10 Pro-64bits/ Process Intel \\(2.66\\) GHz).\n5 Conclusion and perspectives\nTo our knowledge, OTrecod is the first R package that takes advantage of the optimal transportation theory (Monge 1781) in the context of data fusion and the comparative study of methods described in (Garès and Omer 2022) underlines the promising performances of these approaches.\nFor more details and examples about the functions, users are invited to consult the ARTICLES section of the dedicated website.\n6 Drawbacks\nThe functions of OTrecod only apply to a specific data fusion context where there is no overlapping part between the two raw data sources. This package does not deal with record linkage which is already the focus of extensive works (Sariyar and Borg 2010). This package is not adapted when the target variables (outcomes) of the two databases are continuous with an infinite number of values (like weights in grams with decimals for example). Moreover, if the data fusion requires only one shared variable to work, the quality of the fusion depends on the presence of a subset of good shared predictors in the raw databases. Finally, if the function OT_outcome allows all types of predictors, the current version of the function OT_joint imposes categorical matching variables only (scale variables are allowed): this restriction should be relaxed in a next version.\n7 Perspectives\nA number of more advanced research still need further investigation:\nthe possibility of extending the OT algorithm for recoding variables to multidimensional frameworks.\nthe stability of the algorithm when the matching variables are incomplete and the non-response processes\nare missing at random or not.\nthe contribution of calibration techniques in the quality process assessment (Deming and Stephan 1940)\nthe creation of a tuning function which defines a grid search to find the optimal combination of parameters\nrelated to the OT algorithms, could be added in the future versions of the package.\n8 Acknowledgments\nThe authors would especially thank Pierre Navaro (CNRS UMR 6625) for their advices during the implementation of the OTrecod package. This research has received the help from Region Occitanie Grant RBIO-2015-14054319 and Mastodons-CNRS Grant.\n\n9 Supplementary R code\n\n\n### BASIC R CODE FOR SUMMARY TABLES  8, 9, 10, and 11 ---------\n\n## Validation of each model: Repeat the following R command for each model by\n## changing outc1:\nverif_outc1   = verif_OT(outc1, group.class = TRUE, ordinal = FALSE, \n                         stab.prob = TRUE, min.neigb = 5)\n\n## Association between Y and Z: Summary Table 8\nres.prx = rbind(\n           outc1   = verif_outc1$res.prox    , R_outc1 = verif_R_outc1$res.prox, \n           R_outc2 = verif_R_outc2$res.prox  , R_outc3 = verif_R_outc3$res.prox,\n           R_outc4 = verif_R_outc4$res.prox  , R_outc5 = verif_R_outc5$res.prox,\n           outj1   = verif_outj1$res.prox    , R_outj1 = verif_R_outj1$res.prox,\n           R_outj2 = verif_R_outj2$res.prox  , R_outj3 = verif_R_outj3$res.prox,\n           R_outj4 = verif_R_outj4$res.prox )\n                                \nres.prx = data.frame(Model = c(\"outc1\",\"R_outc2\",\"R_outc3\",\"R_outc4\",\n                               \"R_outc5\",\"R_outc6\",\"outj1\", \"R_outj1\",\n                               \"R_outj2\",\"R_outj3\",\"R_outj4\"), \n                     Type  = c(\"OUTCOME\",rep(\"R-OUTCOME\",5),\"JOINT\",\n                               rep(\"R-JOINT\",4)),\n                     Relax = c(0,0.4,0.6,0,0.4,0.6,0,0.4,0.6,0.8,0.4), \n                     Regul = c(rep(0,10),0.1), res.prx)\n\nrow.names(res.prx) = NULL; head(res.prx,3)\n\n#    Name      Type Relax Regul   N V_cram rank_cor\n#   outc1   OUTCOME   0.0   0.0 362   0.86    0.892\n# R_outc1 R-OUTCOME   0.0   0.0 362   0.87    0.911\n# R_outc2 R_OUTCOME   0.4   0.0 362   0.93    0.933\n#-----\n\n## Hellinger distance: Summary Table 9\nres.helld = rbind(\n             outc1   = verif_outc1$hell  , R_outc1 = verif_R_outc1$hell,\n             R_outc2 = verif_R_outc2$hell, R_outc3 = verif_R_outc3$hell, \n             R_outc4 = verif_R_outc4$hell, R_outc5 = verif_R_outc5$hell, \n             outj1   = verif_outj1$hell  , R_outj1 = verif_R_outj1$hell,\n             R_outj2 = verif_R_outj2$hell, R_outj3 = verif_R_outj3$hell,\n             R_outj4 = verif_R_outj4$hell )\n\nres.helld = data.frame(res.prx[,1:4], res.helld)\nrow.names(res.helld) = NULL; res.helld\n#-----\n\n## Stability of the prediction: Summary Table 10 \n# same R code as for Summary Table 8, changing res.prox by res.stab\n#----\n\n## Ratio of common predictions: Table 11\n\nstoc = list(outc1$DATA2_OT$OTpred  , R_outc1$DATA2_OT$OTpred,\n            R_outc4$DATA2_OT$OTpred, outj1$DATA2_OT$OTpred  ,\n            R_outj1$DATA2_OT$OTpred, R_outj4$DATA2_OT$OTpred)\n\ncorpred = matrix(ncol = 6, nrow = 6)\nfor (i in 1:6){\n      for (j in 1:6){\n        corpred[i,j] = round(sum(diag(table(stoc[[i]],stoc[[j]])))/362,2)\n      }\n}; corpred\n#-----\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-006.zip\nCRAN packages used\nOTrecod, StatMatch, mice, missForest, softImpute, missMDA, transport, devtools, randomForest, party\nCRAN Task Views implied by cited packages\nEnvironmetrics, MachineLearning, MissingData, MixedModels, OfficialStatistics, Psychometrics, Survival\nBioconductor packages used\nMultiDataSet, OMICsPCA, mixOmics\n\n\nJ. C. Adamek. Fusion: Combining data from separate sources. Marketing Research, 6(3): 48, 1994.\n\n\nW. Bergsma. A bias-correction for cramér’s v and tschuprow’s t. Journal of the Korean Statistical Society, 42(3): 323–328, 2013. URL https://www.sciencedirect.com/science/article/pii/S1226319212001032.\n\n\nL. Breiman. Random forests. Machine Learning, 45(1): 5–32, 2001. URL https://doi.org/10.1023/A:1010933404324.\n\n\nL. Breiman, J. Friedman, C. J. Stone and R. A. Olshen. Classification and regression trees. Taylor & Francis, 1984. URL https://books.google.fr/books?id=JwQx-WOmSyQC.\n\n\nF. Castanedo. A review of data fusion techniques. TheScientificWorldJournal, 2013: 704504, 2013. DOI 10.1155/2013/704504.\n\n\nN. Cibella. How to choose the matching variables, report WP2, ESS-net. Statistical Methodology Project on Integration of Surveys and Administrative Data, EUROSTAT, 2010.\n\n\nM. Cohen. Statistical matching and microsimulation models, improving information for social policy decisions, the use of microsimulation modeling, technical papers, II. Washington, DC: National Academy, 1991.\n\n\nM. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in neural information processing systems, Eds C. J. Burges, L. Bottou, M. Welling, Z. Ghahramani and K. Q. Weinberger 2013. Curran Associates, Inc. URL https://proceedings.neurips.cc/paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf.\n\n\nM. D’Orazio. StatMatch: Statistical matching or data fusion. 2022. URL https://CRAN.R-project.org/package=StatMatch. R package version 1.4.1.\n\n\nM. D’Orazio, M. Di Zio and M. Scanu. Statistical matching: Theory and practice. John Wiley & Sons, 2006.\n\n\nS. Das and Dr. S. Tripathy. OMICsPCA: An r package for quantitative integration and analysis of multiple omics assays from heterogeneous samples. 2022. R package version 1.16.0.\n\n\nW. E. Deming and F. F. Stephan. On a least squares adjustment of a sampled frequency table when the expected marginal totals are known. The Annals of Mathematical Statistics, 11(4): 427–444, 1940.\n\n\nR. F, G. B, S. A and L. C. K-A. mixOmics: An r package for ’omics feature selection and multiple data integration. PLoS computational biology, 13(11): e1005752, 2017. URL http://www.mixOmics.org.\n\n\nD. E. Farrar and R. R. Glauber. Multicollinearity in regression analysis: The problem revisited. The Review of Economics and Statistics, 49(1): 92–107, 1967. URL http://www.jstor.org/stable/1937887 [online; last accessed November 28, 2022].\n\n\nR. Flamary, N. Courty, A. Gramfort, M. Z. Alaya, A. Boisbunon, S. Chambon, L. Chapel, A. Corenflos, K. Fatras, N. Fournier, et al. POT : Python Optimal Transport. Journal of Machine Learning Research, 2021. URL https://hal.archives-ouvertes.fr/hal-03264013.\n\n\nJ. Forrest, D. de la Nuez and R. Lougee-Heimer. CLP user guide. IBM Research, 2004.\n\n\nV. Garès, C. Dimeglio, G. Guernec, R. Fantin, B. Lepage, M. R. Kosorok and N. Savy. On the use of optimal transportation theory to recode variables and application to database merging. International Journal of Biostatistics, 16(1): article number : 20180106, 2020. URL https://hal.archives-ouvertes.fr/hal-01905857.\n\n\nV. Garès and J. Omer. Regularized optimal transport of covariates and outcomes in data recoding. Journal of the American Statistical Association, 117(537): 320–333, 2022. URL\nhttps://doi.org/10.1080/01621459.2020.1775615\n.\n\n\nK. A. Grajski, L. Breiman, G. V. Di Prisco and W. J. Freeman. Classification of EEG spatial patterns with a tree-structured methodology: CART. IEEE Transactions on Biomedical Engineering, BME-33(12): 1076–1086, 1986. DOI 10.1109/TBME.1986.325684.\n\n\nD. Hall and J. Llinas. An introduction to multisensor data fusion. Proceedings of the IEEE, 85: 6–23, 1997. DOI 10.1109/5.554205.\n\n\nT. Hastie and R. Mazumder. softImpute: Matrix completion via iterative soft-thresholded SVD. 2021. URL https://CRAN.R-project.org/package=softImpute. R package version 1.4-1.\n\n\nC. Hernandez-Ferrer, C. Ruiz-Arenas, A. Beltran-Gomila and J. R. González. MultiDataSet: An r package for encapsulating multiple data sets with application to omic data integration. BMC Bioinformatics, 18(1): 36, 2017. URL https://doi.org/10.1186/s12859-016-1455-1.\n\n\nF. L. Hitchcock. The distribution of a product from several sources to numerous localities. Journal of mathematics and physics / Massachusetts Institute of Technology., 20: 224–230, 1941. URL https://doi.org/10.1002/sapm1941201224.\n\n\nT. Hothorn, P. Bühlmann, S. Dudoit, A. Molinaro and M. J. Van Der Laan. Survival ensembles. Biostatistics, 7(3): 355–373, 2005. URL https://doi.org/10.1093/biostatistics/kxj011.\n\n\nT. Hothorn, K. Hornik and A. Zeileis. Unbiased recursive partitioning: A conditional inference framework. Journal of Computational and Graphical Statistics, 15(3): 651–674, 2006. URL\nhttps://doi.org/10.1198/106186006X133933\n.\n\n\nJ. Josse and F. Husson. missMDA: A package for handling missing values in multivariate data analysis. Journal of Statistical Software, 70(1): 1–31, 2016. DOI 10.18637/jss.v070.i01.\n\n\nJ. B. Kadane. Some statistical problems in merging data files. Journal of Official Statistics, 17: 423–433, 2001.\n\n\nL. Kantorovich. On the transfer of masses. Doklady Akademii Nauk SSSR, 37: 7–8, 1942.\n\n\nL. A. Klein. Sensor and data fusion: A tool for information assessment and decision making. SPIE press, 2004.\n\n\nS. Kotz, N. Balakrishnan and N. Johnson. Continuous multivariate distributions: Models and applications, volume 1, second edition. John Wiley & Sons, 2000. DOI 10.1002/0471722065.\n\n\nA. Liaw and M. Wiener. Classification and regression by randomForest. R News, 2(3): 18–22, 2002. URL https://CRAN.R-project.org/doc/Rnews/.\n\n\nF. Liese and K.-J. Miescke. Statistical decision theory. In Statistical decision theory: Estimation, testing, and selection, pages. 1–52 2008. New York, NY: Springer New York. ISBN 978-0-387-73194-0. URL https://doi.org/10.1007/978-0-387-73194-0_3.\n\n\nR. J. A. Little and D. B. Rubin. Statistical analysis with missing data, third edition. Wiley, 2019.\n\n\nA. Makhorin. GNU linear programming kit, reference manual. Free software foundation, 4: 2011.\n\n\nI. Mayer, A. Sportisse, J. Josse, N. Tierney and N. Vialaneix. R-miss-tastic: A unified platform for missing values methods and workflows. 2019. URL https://arxiv.org/abs/1908.04822.\n\n\nG. Monge. Mémoire sur la Théorie des Déblais et des Remblais. Histoire de l’Académie royale des sciences de Paris, 666–704, 1781.\n\n\nB. Muzellec, J. Josse, C. Boyer and M. Cuturi. Missing data imputation using optimal transport. In ICML, 2020.\n\n\nG. Paass. Statistical match: Evaluation of existing procedures and improvements by using additional information. Microanalytic Simulation Models to Support Social and Financial Policy, 401–420, 1986.\n\n\nJ. Pagès. Analyse factorielle multiple appliquée aux variables qualitatives et aux données mixtes. Revue de Statistique Appliquée, 50(4): 5–37, 2002. URL http://eudml.org/doc/106525.\n\n\nS. Rässler. Frequentist theory of statistical matching. In Statistical matching: A frequentist theory, practical applications, and alternative bayesian approaches, pages. 15–43 2002. New York, NY: Springer New York. ISBN 978-1-4613-0053-3. URL https://doi.org/10.1007/978-1-4613-0053-3_2.\n\n\nW. L. Rodgers. An evaluation of statistical matching. Journal of Business & Economic Statistics, 2(1): 91–102, 1984. URL http://www.jstor.org/stable/1391358 [online; last accessed November 29, 2022].\n\n\nD. B. Rubin. Statistical matching using file concatenation with adjusted weights and multiple imputations. Journal of Business & Economic Statistics, 4(1): 87–94, 1986. URL http://www.jstor.org/stable/1391390 [online; last accessed November 29, 2022].\n\n\nM. Sariyar and A. Borg. The RecordLinkage package: Detecting errors in data. The R Journal, 2(2): 61–67, 2010.\n\n\nM. Scanu. Recommendations on statistical matching, report WP2, ESS-net. Statistical Methodology Project on Integration of Surveys and Administrative Data, 2010.\n\n\nD. Schuhmacher, B. Bähre, C. Gottschlich, V. Hartmann, F. Heinemann and B. Schmitzer. transport: Computation of optimal transport plans and wasserstein distances. 2022. URL https://cran.r-project.org/package=transport. R package version 0.13-0.\n\n\nD. J. Stekhoven. missForest: Nonparametric missing value imputation using random forest. 2022. R package version 1.5.\n\n\nD. J. Stekhoven and P. Bühlmann. MissForest–non-parametric missing value imputation for mixed-type data. Bioinformatics (Oxford, England), 28: 112–8, 2012.\n\n\nC. Strobl, A.-L. Boulesteix, T. Kneib, T. Augustin and A. Zeileis. Conditional variable importance for random forests. BMC Bioinformatics, 9(1): 307, 2008. URL https://doi.org/10.1186/1471-2105-9-307.\n\n\nC. Strobl, A.-L. Boulesteix, A. Zeileis and T. Hothorn. Bias in random forest variable importance measures: Illustrations, sources and a solution. BMC Bioinformatics, 8(1): 25, 2007. URL https://doi.org/10.1186/1471-2105-8-25.\n\n\nC. Strobl, T. Hothorn and A. Zeileis. Party on! The R Journal, 1(2): 14–17, 2009. URL https://doi.org/10.32614/RJ-2009-013.\n\n\nS. Theußl, F. Schwendinger and K. Hornik. ROI: An extensible R optimization infrastructure. Journal of Statistical Software, 94(15): 1–64, 2020. DOI 10.18637/jss.v094.i15.\n\n\nS. Theußl, F. Schwendinger and K. Hornik. ROI: The R optimization infrastructure package. 133. Vienna: WU Vienna University of Economics; Business. 2017. URL http://epub.wu.ac.at/5858/.\n\n\nS. van Buuren and K. Groothuis-Oudshoorn. mice: Multivariate imputation by chained equations in R. Journal of Statistical Software, 45(3): 1–67, 2011. DOI 10.18637/jss.v045.i03.\n\n\nG. Van Rossum and F. L. Drake Jr. Python reference manual. Centrum voor Wiskunde en Informatica Amsterdam, 1995.\n\n\nB. Vantaggi. Statistical matching of multiple sources: A look through coherence. Int. J. Approx. Reasoning, 49: 701–711, 2008. DOI 10.1016/j.ijar.2008.07.005.\n\n\nH. Wickham, J. Hester, W. Chang and J. Bryan. Devtools: Tools to make developing R packages easier. 2022. URL https://CRAN.R-project.org/package=devtools. R package version 2.4.5.\n\n\nA. Zeileis and T. Hothorn. Model-based recursive partitioning. Journal of Computational and Graphical Statistics - J COMPUT GRAPH STAT, 17: 492–514, 2008. DOI 10.1198/106186008X319331.\n\n\nZ. Zhu, T. Wang and R. J. Samworth. High-dimensional principal component analysis with heterogeneous missingness. 2019. URL https://arxiv.org/abs/1906.12125.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:39+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-007/",
    "title": "populR: a Package for Population Downscaling in R",
    "description": "Population data provision is usually framed by regulations and restrictions and hence spatially aggregated in predefined enumeration units such as city blocks and census tracts. Many applications require population data at finer scale, and therefore, one may use downscaling methods to transform population counts from coarse spatial units into smaller ones. Although numerous methods for downscaling of population data have been reported in the scientific literature, only a limited number of implementation tools exist. In this study, we introduce populR, an R package that responds to this need. populR provides two downscaling methods, namely Areal Weighted Interpolation and Volume Weighted Interpolation, which are illustrated and compared to alternative implementations in the sf and areal packages using a case study from Mytilini, Greece. The results provide evidence that the vwi approach outperforms the others, and thus, we believe R users may gain significant advantage by using populR for population downscaling.",
    "author": [
      {
        "name": "Marios Batsaris",
        "url": "https://www.mbatsaris.gr"
      },
      {
        "name": "Dimitris Kavroudakis",
        "url": "https://www.britannica.com/animal/bilby"
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\n1 Introduction\nInformation about the spatial distribution of the population is crucial in addressing a wide range of geographical information science problems (Dobson et al. 2000; Ahola et al. 2007; Freire and Aubrecht 2012; Bian and Wilmot 2015; Calka et al. 2017; Batsaris et al. 2019; Han et al. 2020; Bahadori et al. 2021; Wang et al. 2021). One of the most common sources of population data is the Census of Population (Liu et al. 2008; Tenerelli et al. 2015). In the EU, censuses are carried out every ten years and are regulated by the EU 763/2008 law (European Union 2009). During the census, detailed information about the population is collected with a high level of granularity. This information is confidential, and due to personal data laws and privacy concerns, census data are de-identified via spatial aggregation into coarse administrative units.\nAggregated representations of census data may not reflect the spatial heterogeneity of the population within the reported administrative units, and therefore, one may need to use downscaling methods to transform population counts from the predefined coarse boundaries (source) into finer-scale sub-units (target). Areal interpolation is broadly used in population downscaling (Tenerelli et al. 2015).\nAreal interpolation methods can be categorized according to usage of additional information such as land use data and night lights to guide the interpolation process. Ancillary information independent methods are further distinguished in: a) point-based and b) area-based (Wu et al. 2005; Comber and Zeng 2019). Areal Weighted Interpolation (AWI) is one of the most common methods of areal interpolation of population without exploiting ancillary information [Comber and Zeng (2019); Kim and Yao (2010);] in a Geographic Information System (GIS). AWI guides the interpolation process on the basis of areal weights calculated by the area of intersection between the source and target features (Goodchild and Lam 1980). This method has four main advantages: a) It is easy and straightforward to implement; b) it does not require ancillary information; c) it preserves the initial volume (i.e., the total population within the source zone); and d) it may be implemented in both vector and raster environments (Lam 1983; Fisher and Langford 1995; Kim and Yao 2010; Qiu et al. 2012; Comber and Zeng 2019). On the other hand, one of the main disadvantages of AWI is its assumption of homogeneity within source zone features (Qiu et al. 2012; Comber and Zeng 2019).\nsf (Pebesma 2018) and areal (Prener and Revord 2019) provide AWI functionality in R. sf comes up with a formula either for extensive (population) or intensive (population density) interpolation that calculates the areal weights based on the total area of the source features (total weights), which makes it suitable for completely overlapping data. areal extends sf functionality by providing an additional formula for weight calculation for data without complete overlap. In this case, areal weights are calculated using the sum of the remaining source areas after the intersection (sum weights) (Prener and Revord 2019).\nWhen the case involves downscaling of urban population data (small scale applications) where the source features (such as city blocks or census tracts) are somehow larger than target features (such as buildings) in terms of footprint area, the sf functionality is unable to calculate the areal weights correctly. Additionally, areal may be confusing for novice R (or GIS) users as it is not obvious that the weight option should be set to sum to calculate areal weights properly.\nTo overcome such limitations, we introduce populR (Batsaris 2022), a package for downscaling of population data using areal interpolation methods. populR provides an AWI approach that matches areal functionality using sum weights and a VWI (Volume Weighted Interpolation) approach that uses the area of intersection between source and target features multiplied by the building height or number of floors (volume) to guide the interpolation process.\nThe remainder of the paper is structured as follows. The next section lists the methods included in the populR package, further explains and demonstrates using examples. The numerical experiments section illustrates and discusses the results obtained by populR and compares the results to other alternatives in R. Finally, the paper concludes with a summary, along with future improvements, in the conclusions section.\n2 The populR package\npopulR is available to R users through the CRAN and Github repositories and may be installed as shown in the code block below.\n\n\n# CRAN installation\ninstall.packages(\"populR\")\n\n# Github installation\ndevtools::install_github(\"mbatsaris/populR\")\n\n\nThe populR functionality focuses on three main pillars, namely downscaling, rounding, and comparing. Additionally, sample data (objects of class sf (Pebesma 2018)) representing city blocks, including population counts (src) and individual buildings attributed by the number of floors (trg) from a small part of the city of Mytilini, Greece, are also available for further experimentation with the package’s functionality.\nDownscaling\nPopulation downscaling is carried out by the primary pp_estimate function.\nSuppose a set of \\(j = 1, ..., J\\) city block polygons (source) accompanied by population counts and an incongruent yet superimposed set of \\(i = 1, ..., B_j\\) individual building polygons (target), along with their characteristics such as footprint area and/or number of floors (or height), the pp_estimate, aims to transform population counts from the source to the target using AWI and VWI approaches. The mathematics behind both approaches is presented in the next two subsections and graphically illustrated in figure 1.\n\n\n\nFigure 1: Illustration of the populR package’s functionality. populR uses two areal interpolation methods to convert population values from city block polygons (left) to a set of individual building polygons (center). AWI (top) solely relies on the area of intersection between blocks and buildings (top-right). VWI (bottom) uses the area of intersection between blocks and buildings multiplied by the height or number of floors to aid the transformation process (bottom-right).\n\n\n\nAWI\nAWI proportionately interpolates the population value of the source features based on areal weights calculated by the area of intersection between the source and target zones and algebraically explained by equations (1) and (2) (Goodchild and Lam 1980). In equation (1), areal weights \\(w^a_{ij}\\) are calculated by standardizing the measured footprint area of individual building \\(i\\) in block \\(j\\) (\\(a_{ij}\\)) over the sum of building footprint areas in the \\(j-th\\) city block. Finally, building population values for building \\(i\\) in block \\(j\\) (\\(p_{ij}\\)) may be calculated by multiplying the areal weights of building \\(i\\) in block \\(j\\) with the population value of \\(j-th\\) block (\\(P_j\\)).\n\\[\\begin{equation}\n  w_{ij}^a = \\frac{a_{ij}}{\\sum_{i=1}^{B_{j}} a_{ij}} \\text{ , }\n  \\tag{1}\n\\end{equation}\\]\n\\[\\begin{equation}\n    p_{ij}^a = w_{ij}^a \\times P_j \\text{ . }\n    \\tag{2}\n\\end{equation}\\]\nA demonstration of the AWI approach is presented in the code block below. In the first line of code,\npopulR is attached to the script. The next two lines load the package’s built-in data, and the last line\ndemonstrates the usage of the pp_estimate function. As a result, pp_estimate returns an object\nof class sf including individual buildings (target) with population estimates.\n\n\n# attach package\nlibrary(populR)\n\n# load data\ndata('src')\ndata('trg')\n\n# downscaling using awi\nawi <- pp_estimate(target = trg, source = src, spop = pop, sid = sid,\nmethod = awi)\n\n\nWhere:\ntarget: An object of class sf that is used to interpolate data to. Usually, target may include\npolygon features representing building units\nsource: An object of class sf including data to be interpolated. Source may be a set of coarse\npolygon features, such as city blocks or census tracts\nsid: Source identification number\nspop: Source population values to be interpolated\nmethod: Two methods provided: AWI (Areal Weighted Interpolation) and VWI (Volume Weighted\nInterpolation). AWI proportionately interpolates the population values based on areal weights\ncalculated by the area of intersection between the source and target zones. VWI proportionately\ninterpolates the population values based on volume weights calculated by the area of inter-\nsection between the source and target zones multiplied by the volume information (height or\nnumber of floors)\nVWI\nGiven the number of floors (or height) of individual buildings, VWI applies the same logic as AWI. Instead of areal weights, VWI proportionately dis-aggregates source population counts using volume weights measured by the area of intersection multiplied either by individual building height (if available) or number of floors. VWI may be mathematically expressed by equations (3) to (5). First, the volume of building \\(i\\) in block \\(j\\) (\\(v_{ij}\\)) is measured by multiplying the footprint area (\\(a_{ij}\\)) of building \\(i\\) in block \\(j\\) with the number of floors or height (\\(n_{ij}\\)) as shown in equation (3). Next, volume weights (\\(w^v_{ij}\\)) are calculated by standardizing the volume of building \\(i\\) in block \\(j\\) (\\(v_{ij}\\)) by the sum of the total volume of buildings in the \\(j-th\\) block (4). Then, building population values for building \\(i\\) in block \\(j\\) (\\(p^v_{ij}\\)) may be calculated by multiplying the volume weights (\\(w^v_{ij}\\)) of building \\(i\\) in block \\(j\\) with the population value of block \\(j\\) (\\(P_J\\)) as shown in (5).\n\\[\\begin{equation}\n    v_{ij} = a_{ij} \\times n_{ij} \\text{ , }\n    \\tag{3}\n\\end{equation}\\]\n\\[\\begin{equation}\n    w_{ij}^v = \\frac{v_{ij}}{\\sum_{i=1}^{B_{j}} v_{ij}} \\text{ , }\n    \\tag{4}\n\\end{equation}\\]\n\\[\\begin{equation}\n    p_{ij}^v = w_{ij}^v \\times P_j \\text{ . }\n    \\tag{5}\n\\end{equation}\\]\nThe code block below provides an example of the VWI approach.\n\n\n# downscaling using vwi\nvwi <- pp_estimate(target = trg, source = src, spop = pop, sid = sid,\nvolume = floors, method = vwi)\n\n\nWhere:\nvolume: Target feature volume information (height or number of floors). Required when method\nis set to vwi\nIt is important to mention that when volume information (number of floors or building height)\nis not available for the entire target dataset, users may replace missing values with \\(1\\) if they want to\ninclude them as buildings with 1 floor; otherwise, users may replace missing values with \\(0\\) if they\nwant to exclude them from the downscaling process.\nRounding\nAWI is volume-preserving (Comber and Zeng 2019) (as VWI) and returns an object of class sf with\ndecimal population counts for each target feature. To increase the readability of the results as well\nas to maintain the initial source population values, it is essential for many applications to provide\ninteger values by rounding off to the closest integer. This transformation may result in a shortage or\nsurplus in comparison to the initial values (source values), and therefore, to cope with this problem,\nthe pp_round function is proposed.\nFirst, estimated population values are converted into integer numbers. Next, the pp_round function calculates the differences between the initial population counts and the sum of the integer values for each city block; it is activated only if the quantified difference is either positive or negative. In either case, during this process, target-based differences are also calculated and used to refine the integer counts by adding or removing one population unit in order to preserve the initial source counts when summed up.\nAn example of the pp_round function is provided below.\n\n\n# downscaling using awi\nawi <- pp_round(x = awi, tpop = pp_est, spop = pop, sid = sid)\n\n# downscaling using awi\nvwi <- pp_round(x = vwi, tpop = pp_est, spop = pop, sid = sid)\n\n\nWhere:\nx: An object of class sf obtained by the pp_estimate function\ntpop: Target population estimates obtained by the pp_estimate function\nspop: Initial source population values (included after the implementation of the pp_estimate\nfunction)\nsid: Source identification number\nAs a result, pp_round returns an object of class sf with integer estimates of population values\nwith respect to the initial source population values.\nComparing\nVolume-preserving means that estimated values should sum up to the initial population counts for each source unit, and therefore, one may need to compare target estimates to the initial source values. For this purpose, a function is introduced under the alias pp_compare. The pp_compare function measures the root mean squared error (RMSE), the mean absolute error (MAE), and finally, the statistical relationship between the initial source values and the estimated ones, which is calculated and depicted on a 2D scatter diagram along with the value of the correlation coefficient \\(R^2\\).\nA short example of the pp_compare function is presented in the code block below, where src_awi (and src_vwi) is a data.frame created by grouping the sum of the estimated values along with the initial population values for each source feature.\n\n\n# attach library\nlibrary(dplyr)\n\n# group estimated and actual values by source identification number - awi\nsrc_awi <- awi %>%\n  group_by(sid) %>%\n  summarise(est = sum(pp_est), act = unique(pop))\n\n# awi approach compare to source values\npp_compare(x = src_awi, estimated = est, actual = act, title = \"awi vs source\")\n\n# group estimated and actual values by source identification number - vwi\nsrc_vwi <- vwi %>%\n  group_by(sid) %>%\n  summarise(est = sum(pp_est), act = unique(pop))\n\n# vwi approach compare to source values\npp_compare(x = src_vwi, estimated = est, actual = act, title = \"vwi vs source\")\n\n\nWhere:\nx: An object of class sf or a data.frame including estimated and actual values\nestimated: Target population estimates obtained by the pp_estimate function\nactual: Actual population values\ntitle: Scatterplot title (quoted)\npp_compare returns a list of elements with RMSE, MAE, linear regression, and \\(R^2\\) coefficient\ninformation.\n3 Numerical Experimentation\nIn this section, the results of the populR package are illustrated and further compared to sf and areal. The analysis focus on implementation, comparison to a reference data set, and performance.\nCase study\nTo examine the efficacy and efficiency of the populR extension based on actual data, a small part of\nthe city of Mytilini was chosen as the subject for a case study (Figure 2). The study area consists of 9\ncity blocks (src), 179 buildings (trg), and 911 residents.\n\n\n\nFigure 2: Part of the city of Mytilini used as the case study for numerical experiments. The study area is represented by 9 city blocks and 179 buildings (in orange). The study area counts 911 residents.\n\n\n\n\n\nTable 1: Data used for numerical experiments\n\n\nTitle\n\n\nFormat\n\n\nSource\n\n\nYear\n\n\nCity block population counts\n\n\nTabular\n\n\nHellenic Statistical Authority\n\n\n2011\n\n\nCity blocks\n\n\nSpatial\n\n\nHellenic Statistical Authority\n\n\n2011\n\n\nBuildings\n\n\nSpatial\n\n\nHellenic Statistical Authority\n\n\n2011\n\n\n\n\n\nTable 1 presents the data used in this case study. City block population counts were retrieved in tabular format while city blocks (source - src) and buildings (target - trg) were provided in spatial format. City block population counts and city blocks and buildings correspond to the 2011 Census of Housing and Population (Hellenic Statistical Authority 2014).\nImplementation\nIn this section, a demonstration of the sf, areal and populR packages takes place. First, the packages are attached to the script and next, populR built-in data are loaded. Then, areal interpolation\nimplementation follows for each one of the aforementioned packages.\nFor the reader’s convenience, names were shortened as follows: a) awi: populR AWI approach, b)\nVWI: populR vwi approach, c) aws: areal using extensive interpolation and sum weights, d) awt: areal using extensive interpolation and total weights, and e) sf: sf using extensive interpolation and total weights.\n\n\n# attach libraries\nlibrary(populR)\nlibrary(areal)\nlibrary(sf)\n\n# load data\ndata(\"trg\", package = \"populR\")\ndata(\"src\", package = \"populR\")\n\n# populR - awi\nawi <- pp_estimate(target = trg, source = src, spop = pop, sid = sid,\nmethod = awi)\n\n# populR - vwi\nvwi <- pp_estimate(target = trg, source = src, spop = pop, sid = sid,\nvolume = floors, method = vwi)\n\n# areal - sum weights\naws <- aw_interpolate(trg, tid = tid, source = src, sid = 'sid',\nweight = 'sum', output = 'sf', extensive = 'pop')\n\n# areal - total weights\nawt <- aw_interpolate(trg, tid = tid, source = src, sid = 'sid',\nweight = 'total', output = 'sf', extensive = 'pop')\n\n# sf - total weights\nsf <- st_interpolate_aw(src['pop'], trg, extensive = TRUE)\n\n\nEvidently, sf requires less arguments than populR and areal, which makes it very easy to implement. populR requires at least 5 arguments, and areal at least 7, which may increase the implementation complexity.\nThe study area counts 911 residents, as already mentioned in previous section. awi, vwi and aws\ncorrectly estimated population values as they sum to 911, while awt and sf underestimated values.\nThis is expected as both methods use the total area of the source features during the interpolation\nprocess and are useful when source and target features completely overlap.\nMoreover, identical results were obtained by the awi and aws approaches, but somehow different\nresults were obtained by the vwi, as shown in Table 2.\n\n\nTable 2: Implementation results obtained by awi, vwi, awt, aws and sf for 10 buildings of the study area.\n\n\nnr\n\n\nawi\n\n\nvwi\n\n\naws\n\n\nawt\n\n\nsf\n\n\n1\n\n\n1.36\n\n\n0.37\n\n\n1.36\n\n\n0.58\n\n\n0.58\n\n\n2\n\n\n1.42\n\n\n0.39\n\n\n1.42\n\n\n0.60\n\n\n0.60\n\n\n3\n\n\n16.12\n\n\n15.47\n\n\n16.12\n\n\n5.99\n\n\n5.99\n\n\n4\n\n\n7.47\n\n\n4.30\n\n\n7.47\n\n\n2.77\n\n\n2.77\n\n\n5\n\n\n2.22\n\n\n0.61\n\n\n2.22\n\n\n0.94\n\n\n0.94\n\n\n6\n\n\n21.08\n\n\n28.32\n\n\n21.08\n\n\n7.83\n\n\n7.83\n\n\n7\n\n\n1.34\n\n\n0.44\n\n\n1.34\n\n\n0.62\n\n\n0.62\n\n\n8\n\n\n2.57\n\n\n1.45\n\n\n2.57\n\n\n1.36\n\n\n1.36\n\n\n9\n\n\n6.54\n\n\n5.03\n\n\n6.54\n\n\n2.75\n\n\n2.75\n\n\n10\n\n\n6.68\n\n\n3.84\n\n\n6.68\n\n\n2.48\n\n\n2.48\n\n\n\n\n\nFinally, visual representations of the results are shown in Figure 3.\n\n\n\nFigure 3: Cartographic representation of the downscaling results acquired by a) awt (top-left), b) sf (top-right), c) awi (center-left), d) aws (center-right), and e) vwi (bottom). awt and sf use the same formula therefore, they provide similar results. Additionally, the same results obtained by aws and awi as they are based on the same formula. vwi provide somehow different distribution because of the influence of the volume weights during the interpolation process.\n\n\n\nComparison to reference data\nDue to confidentiality concerns, population data at the granularity of building are not publicly available in Greece. Therefore, an alternate population distribution previously published in Batsaris et al. (2019) was used as reference data (rf). rf population values are also included in the build-in trg data set.\nUsing the pp_compare function as shown in the example below, we investigated the statistical relationship between rf and the results obtained by populR, areal, and sf packages.\n\n\n# compare awi to rf\npp_compare(data, estimated = awi, actual = rf, title = \"awi vs rf\")\n\n# compare vwi to rf\npp_compare(data, estimated = vwi, actual = rf, title = \"vwi vs rf\")\n\n# compare sf to rf\npp_compare(data, estimated = sf, actual = rf, title = \"sf vs rf\")\n\n# compare awt to rf\npp_compare(data, estimated = awt, actual = rf, title = \"awt vs rf\")\n\n# compare aws to rf\npp_compare(data, estimated = aws, actual = rf, title = \"aws vs rf\")\n\n\nTable 3 presents the results obtained for the first 10 individual buildings for each implementation in comparison to rf values. Additionally, \\(RMSE\\), \\(MAE\\), and \\(R^2\\) values are measured and depicted in\nTable 4 and finally, scatter diagrams provided in Figure 4.\nGenerally the scatter diagrams (Figure 4)) suggest strong and positive relationships in all cases. However, the vwi approach obtained the best results and achieved the smallest \\(RMSE\\) (1.44824) and \\(MAE\\) (0.9358159) values and the largest \\(R^2\\) (0.9878) value as shown in Table and Figure. Moreover, aws and awi provided the same error and \\(R^2\\) measurements (\\(RMSE\\): 5.325914, \\(MAE\\): 2.748126 and \\(R^2\\): 0.8215). sf and awt provided the same results and performed poorly in comparison to vwi by obtaining the largest error measurements and the smallest \\(R^2\\) (\\(RMSE\\): 7.416329, \\(MAE\\): 3.664695, and \\(R^2\\): 0.80367).\n\n\nTable 3: Implementation results obtained by awi, vwi, awt, aws and sf for 10 buildings of the study area in comparison to reference data (rf).\n\n\nnr\n\n\nrf\n\n\nawi\n\n\nvwi\n\n\naws\n\n\nawt\n\n\nsf\n\n\n1\n\n\n0.46\n\n\n1.36\n\n\n0.37\n\n\n1.36\n\n\n0.58\n\n\n0.58\n\n\n2\n\n\n0.49\n\n\n1.42\n\n\n0.39\n\n\n1.42\n\n\n0.60\n\n\n0.60\n\n\n3\n\n\n16.20\n\n\n16.12\n\n\n15.47\n\n\n16.12\n\n\n5.99\n\n\n5.99\n\n\n4\n\n\n5.63\n\n\n7.47\n\n\n4.30\n\n\n7.47\n\n\n2.77\n\n\n2.77\n\n\n5\n\n\n0.76\n\n\n2.22\n\n\n0.61\n\n\n2.22\n\n\n0.94\n\n\n0.94\n\n\n6\n\n\n31.77\n\n\n21.08\n\n\n28.32\n\n\n21.08\n\n\n7.83\n\n\n7.83\n\n\n7\n\n\n0.49\n\n\n1.34\n\n\n0.44\n\n\n1.34\n\n\n0.62\n\n\n0.62\n\n\n8\n\n\n1.57\n\n\n2.57\n\n\n1.45\n\n\n2.57\n\n\n1.36\n\n\n1.36\n\n\n9\n\n\n3.75\n\n\n6.54\n\n\n5.03\n\n\n6.54\n\n\n2.75\n\n\n2.75\n\n\n10\n\n\n5.03\n\n\n6.68\n\n\n3.84\n\n\n6.68\n\n\n2.48\n\n\n2.48\n\n\n\n\n\n\n\nTable 4: RMSE, MAE and R2 values were calculated to assess the estimation accuracy using rf as the control variable. vwi provide the best measurements.\n\n\nTitle\n\n\nRMSE\n\n\nMAE\n\n\nR2\n\nrf vs vwi\n\n\n1.45\n\n\n0.94\n\n\n0.988\n\n\nrf vs awi\n\n\n5.33\n\n\n2.75\n\n\n0.822\n\n\nrf vs aws\n\n\n5.33\n\n\n2.75\n\n\n0.822\n\n\nrf vs sf\n\n\n7.42\n\n\n3.66\n\n\n0.804\n\n\nrf vs awt\n\n\n7.42\n\n\n3.66\n\n\n0.804\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Investigation of the relationship of the outcomes and reference data (rf) as the control variable. a) rf vs awt, R2 = 0.804 (top-left), b) rf vs sf, R2 = 0.804 (top-right), c) rf vs aws, R2 = 0.822 (center-left), d) rf vs awi, R2 = 0.822 (center-right) and e) rf vs vwi, R2 = 0.988.\n\n\n\nPerformance\nIn this section, a performance comparison (execution times) takes place using the microbenchmark\npackage. Performance tests are carried out using the sample data provided by populR as well as\nexternal data sets from a significantly larger study area (Chios, Greece) with 845 city blocks and 15,946\nbuildings.\nExecution time measurements for both cases are shown in Tables 5 and 6 accordingly. In both\ncases, execution time measurements suggest that populR performs faster than areal and sf. Using the built-in data, awi and vwi scored the best mean execution time (about 76 milliseconds), which is about\n54 millisecond faster than aws, 61 milliseconds faster than sf, and almost 70 milliseconds faster than\nawt.\n\n\nTable 5: Execution time measurement comparisons (in milliseconds) using microbenchmark and sample data\n\n\nName\n\n\nMin\n\n\nMean\n\n\nMax\n\n\nvwi\n\n\n74.06\n\n\n76.23\n\n\n84.39\n\n\nawi\n\n\n74.31\n\n\n75.91\n\n\n99.70\n\n\naws\n\n\n128.48\n\n\n132.56\n\n\n143.97\n\n\nawt\n\n\n172.08\n\n\n176.27\n\n\n185.86\n\n\nsf\n\n\n136.48\n\n\n142.33\n\n\n289.38\n\n\n\n\n\n\n\nTable 6: Execution time measurement comparisons (in seconds) using microbenchmark and external\ndata\n\n\nName\n\n\nMin\n\n\nMean\n\n\nMax\n\n\nvwi\n\n\n1.55\n\n\n1.67\n\n\n1.94\n\n\nawi\n\n\n1.55\n\n\n1.68\n\n\n1.95\n\n\naws\n\n\n2.26\n\n\n2.39\n\n\n2.64\n\n\nawt\n\n\n2.27\n\n\n2.42\n\n\n2.62\n\n\nsf\n\n\n2.63\n\n\n2.79\n\n\n3.17\n\n\n\n\n\n4 Conclusions\nThis study is an attempt to contribute to the continuously growing scientific literature of population\ndownscaling using areal interpolation methods. Despite the fact that there are so many downscaling\nmethods developed, only a few implementation tools are available to the GIS and R community, and\ntherefore, it may be challenging for non-expert GIS users to take advantage of these methods (Langford 2007; Mennis 2009). Due to this lack of implementation tools, this study attempts to fill this gap\nby introducing populR, an R package for population downscaling using areal interpolation methods.\nThis package implements two areal interpolation methods, namely awi and vwi. The awi approach\nguides the interpolation process using the area of intersection between source and target zones while\nvwi uses the number of floors as additional information to influence the downscaling process. The\npackage is available to the R community though the CRAN and Github repositories.\nIn order to show the efficacy and efficiency of the introduced package on actual data, a subset\narea of the city of Mytilini, Greece, was used in the case study. Moreover, a comparative analysis\nbetween populR, areal, and sf was carried out, and the results showed that vwi outperformed others by achieving the smallest error measurements, easy implementation and faster execution times. Thus,\nwe strongly believe that R users will gain significant advantage by using populR for population\ndownscaling.\nThe evaluation of the results indicates the potential of populR in providing better and faster results\nin comparison to existing R packages. However, we believe that the results may be further improved\nby incorporating new forms of data, such as Volunteered Geographic Information (VGI), acquired\nfrom either social media or open spatial databases such as Open Street Map (Bakillah et al. 2014; Guo et al. 2017; Comber and Zeng 2019; Yang et al. 2019). By incorporating VGI as ancillary information, we can identify different types of buildings and therefore adjust the weight calculation accordingly. This will be an important improvement that would be helpful for R users interested in population\ndownscaling.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-007.zip\nCRAN packages used\nsf, areal, populR, microbenchmark\nCRAN Task Views implied by cited packages\nMissingData, Spatial, SpatioTemporal\n\n\nT. Ahola, K. Virrantaus, J. M. Krisp and G. J. Hunter. A spatio-temporal population model to support risk assessment and damage analysis for decision-making. International Journal of Geographical Information Science, 21(8): 935–953, 2007. URL https://doi.org/10.1080/13658810701349078.\n\n\nM. S. Bahadori, A. B. Gonçalves and F. Moura. A systematic review of station location techniques for bicycle-sharing systems planning and operation. ISPRS International Journal of Geo-Information, 10(8): 554, 2021. URL https://doi.org/10.3390/ijgi10080554.\n\n\nM. Bakillah, S. Liang, A. Mobasheri, J. Jokar Arsanjani and A. Zipf. Fine-resolution population mapping using OpenStreetMap points-of-interest. International Journal of Geographical Information Science, 28(9): 1940–1963, 2014. URL https://doi.org/10.1080/13658816.2014.909045.\n\n\nM. Batsaris. populR: Population downscaling. 2022. URL https://CRAN.R-project.org/package=populR. R package version 0.1.6.\n\n\nM. Batsaris, D. Kavroudakis, N. A. Soulakellis and T. Kontos. Location-allocation modeling for emergency evacuation planning in a smart city context. International Journal of Applied Geospatial Research, 10(4): 28–43, 2019. URL https://doi.org/10.4018/ijagr.2019100103.\n\n\nR. Bian and C. G. Wilmot. Spatiotemporal population distribution method for emergency evacuation. Transportation Research Record: Journal of the Transportation Research Board, 2532(1): 99–106, 2015. URL https://doi.org/10.3141/2532-12.\n\n\nB. Calka, J. Nowak Da Costa and E. Bielecka. Fine scale population density data and its application in risk assessment. Geomatics, Natural Hazards and Risk, 8(2): 1440–1455, 2017. URL https://doi.org/10.1080/19475705.2017.1345792.\n\n\nA. Comber and W. Zeng. Spatial interpolation using areal features: A review of methods and opportunities using new forms of data with coded illustrations. Geography Compass, 13(10): 1–23, 2019. URL https://doi.org/10.1111/gec3.12465.\n\n\nJ. E. Dobson, E. A. Bright, R. G. Durfee and B. A. Worley. LandScan: A global population database for estimating population at risk. Photogrammetric Engineering and Remote Sensing, 66(7): 849–857, 2000.\n\n\nE. European Union. European parliament and council regulation (EC) no 763/2008 on population and housing censuses. 2009. URL https://ec.europa.eu/eurostat/web/population-demography/population-housing-censuses/legislation [online; last accessed September 8, 2021].\n\n\nF. P. Fisher and M. Langford. Modelling the errors in areal interpolation between zonal systems by monte carlo simulation. Environment and Planning A, 27: 211–224, 1995. URL https://doi.org/10.1068/a270211.\n\n\nS. Freire and C. Aubrecht. Integrating population dynamics into mapping human exposure to seismic hazard. Natural Hazards and Earth System Science, 12(11): 3533–3543, 2012. URL https://doi.org/10.5194/nhess-12-3533-2012.\n\n\nM. F. Goodchild and S.-N. N. Lam. Areal interpolation: A variant of the traditional spatial problem. Geo-processing, 1(3): 297–312, 1980.\n\n\nH. Guo, K. Cao and P. Wang. Population estimation in singapore based on remote sensing and open data. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives, 42: 1181–1187, 2017. URL https://doi.org/10.5194/isprs-archives-XLII-2-W7-1181-2017.\n\n\nB. Han, M. Hu and J. Wang. Site selection for pre-hospital emergency stations based on the actual spatiotemporal demand: A case study of nanjing city, china. ISPRS International Journal of Geo-Information, 9(10): 542, 2020. URL https://doi.org/10.3390/ijgi9100559.\n\n\nH. Hellenic Statistical Authority. 2011 population and housing census of greece. Hellenic Statistical Authority. 2014.\n\n\nH. Kim and X. Yao. Pycnophylactic interpolation revisited: Integration with the dasymetric-mapping method. International Journal of Remote Sensing, 31(21): 5657–5671, 2010. URL https://doi.org/10.1080/01431161.2010.496805.\n\n\nN. S.-N. Lam. Spatial interpolation methods: A review. The American Cartographer, 10(2): 129–150, 1983. URL https://doi.org/10.1559/152304083783914958.\n\n\nM. Langford. Rapid facilitation of dasymetric-based population interpolation by means of raster pixel maps. Computers, Environment and Urban Systems, 31(1): 19–32, 2007. URL https://doi.org/10.1016/j.compenvurbsys.2005.07.005.\n\n\nX. H. Liu, P. C. Kyriakidis and M. F. Goodchild. Population-density estimation using regression and area-to-point residual kriging. International Journal of Geographical Information Science, 22(4): 431–447, 2008. URL https://doi.org/10.1080/13658810701492225.\n\n\nJ. Mennis. Dasymetric mapping for estimating population in small areas. Geography Compass, 3(2): 727–745, 2009. URL https://doi.org/10.1111/j.1749-8198.2009.00220.x.\n\n\nE. Pebesma. Simple features for R: Standardized support for spatial vector data. R Journal, 10(1): 439–446, 2018. URL https://doi.org/10.32614/rj-2018-009.\n\n\nC. Prener and C. Revord. areal: An R package for areal weighted interpolation. Journal of Open Source Software, 4(37): 1221, 2019. URL https://doi.org/10.21105/joss.01221.\n\n\nF. Qiu, C. Zhang and Y. Zhou. The development of an areal interpolation ArcGIS extension and a comparative study. GIScience and Remote Sensing, 49(5): 644–663, 2012. URL https://doi.org/10.2747/1548-1603.49.5.644.\n\n\nP. Tenerelli, J. F. Gallego and D. Ehrlich. Population density modelling in support of disaster risk assessment. International Journal of Disaster Risk Reduction, 13: 334–341, 2015. URL https://doi.org/10.1016/j.ijdrr.2015.07.015.\n\n\nW. Wang, Z. Xu, D. Sun and T. Lan. Spatial optimization of mega-city fire stations based on multi-source geospatial data: A case study in beijing. ISPRS International Journal of Geo-Information, 10(5): 282, 2021. URL https://doi.org/10.3390/ijgi10050282.\n\n\nS.-S. Wu, X. Qiu and L. Wang. Population estimation methods in GIS and remote sensing: A review. GIScience and Remote Sensing, 42(1): 80–96, 2005. URL https://doi.org/10.2747/1548-1603.42.1.80.\n\n\nX. Yang, T. Ye, N. Zhao, Q. Chen, W. Yue, J. Qi, B. Zeng and P. Jia. Population mapping with multisensor remote sensing images and point-of-interest data. Remote Sensing, 11(5): 574, 2019. URL https://doi.org/10.3390/rs11050574.\n\n\n\n\n",
    "preview": "articles/RJ-2023-007/distill-preview.png",
    "last_modified": "2023-11-07T21:31:39+00:00",
    "input_file": {},
    "preview_width": 4090,
    "preview_height": 2650
  },
  {
    "path": "articles/RJ-2023-008/",
    "title": "dycdtools: an R Package for Assisting Calibration and Visualising Outputs of an Aquatic Ecosystem Model",
    "description": "The high complexity of aquatic ecosystem models (AEMs) necessitates a large number of parameters that need calibration, and visualisation of their multifaceted and multi-layered simulation results is necessary for effective communication. Here we present an R package \"dycdtools\" that contains calibration and post-processing tools for a widely applied aquatic ecosystem model (DYRESM-CAEDYM). The calibration assistant function within the package automatically tests a large number of combinations of parameter values and returns corresponding values for goodness-of-fit, allowing users to narrow parameter ranges or optimise parameter values. The post-processing functions enable users to visualise modelling outputs in four ways: as contours, profiles, time series, and scatterplots. The \"dycdtools\" package is the first open-source calibration and post-processing tool for DYRESM-CAEDYM, and can also be adjusted for other AEMs with a similar structure. This package is useful to reduce the calibration burden for users and to effectively communicate model results with a broader community.",
    "author": [
      {
        "name": "Songyan Yu",
        "url": {}
      },
      {
        "name": "Christopher G. McBride",
        "url": {}
      },
      {
        "name": "Marieke A. Frassl",
        "url": {}
      },
      {
        "name": "Matthew R. Hipsey",
        "url": {}
      },
      {
        "name": "David P. Hamilton",
        "url": {}
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-008.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:39+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-009/",
    "title": "SurvMetrics: An R package for Predictive Evaluation Metrics in Survival Analysis",
    "description": "Recently, survival models have found vast applications in biostatistics, bioinformatics, reliability engineering, finance and related fields. But there are few R packages focusing on evaluating the predictive power of survival models. This lack of handy software on evaluating survival predictions hinders further applications of survival analysis for practitioners. In this research, we want to fill this gap by providing an \\\"all-in-one\\\" R package which implements most predictive evaluation metrics in survival analysis. In the proposed SurvMetrics R package, we implement concordance index for both untied and tied survival data; we give a new calculation process of Brier score and integrated Brier score; we also extend the applicability of integrated absolute error and integrated square error for real data. For models that can output survival time predictions, a simplified metric called mean absolute error is also implemented. In addition, we test the effectiveness of all these metrics on simulated and real survival data sets. The newly developed SurvMetrics R package is available on CRAN at <https://CRAN.R-project.org/package=SurvMetrics> and GitHub at <https://github.com/skyee1/SurvMetrics>.",
    "author": [
      {
        "name": "Hanpu Zhou",
        "url": {}
      },
      {
        "name": "Hong Wang\\*",
        "url": {}
      },
      {
        "name": "Sizheng Wang",
        "url": {}
      },
      {
        "name": "Yi Zou",
        "url": {}
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-009.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:39+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-010/",
    "title": "Limitations in Detecting Multicollinearity due to Scaling Issues in the mcvis Package",
    "description": "Transformation of the observed data is a very common practice when a troubling degree of near multicollinearity is detected in a linear regression model. However, it is important to take into account that these transformations may affect the detection of this problem, so they should not be performed systematically. In this paper we analyze the transformation of the data when applying the R package mcvis, showing that it only detects essential near multicollinearity when the *studentise* transformation is performed.",
    "author": [
      {
        "name": "Roman Salmeron Gomez",
        "url": "http://metodoscuantitativos.ugr.es/pages/web/romansg"
      },
      {
        "name": "Catalina B. Garcia Garcia",
        "url": "http://metodoscuantitativos.ugr.es/pages/web/cbgarcia"
      },
      {
        "name": "Ainara Rodriguez Sanchez",
        "url": "http://metodoscuantitativos.ugr.es/pages/web/cbgarcia"
      },
      {
        "name": "Claudia Garcia Garcia",
        "url": "http://metodoscuantitativos.ugr.es/pages/web/cbgarcia"
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\n1 Introduction\nGiven the model \\(\\mathbf{Y} = \\mathbf{X} \\cdot \\boldsymbol{\\beta} + \\mathbf{u}\\) for \\(n\\) observations and \\(p\\) independent variables where \\(\\mathbf{Y}\\) is a vector that contains the observations of the dependent variables, \\(\\mathbf{X} = [\\mathbf{1} \\ \\mathbf{X}_{2} \\dots \\mathbf{X}_{p}]\\) is a matrix whose columns contain the observations of the independent variables (where the first column is a vector of ones representing the intercept) and \\(\\mathbf{u}\\) represents the spherical random disturbance, the existence of linear relationships between the independent variables of a model is known as multicollinearity. It is well-known that a high degree of multicollinearity can affect the analysis of a linear regression model. In this case, it is said that the multicollinearity is troubling (Novales 1988; Ramanathan 2002; Wooldridge 2008; Gujarati 2010). It is also interesting to note the distinction made, for example, by Marquardt (1980) or Snee and Marquardt (1984), between essential (near-linear relationship between at least two independent variables excluding the intercept) and non-essential multicollinearity (near-linear relationship between the intercept and at least one of the remaining independent variables).\nNote that the detection process is key to determining which tool is best suited to mitigation of the problem: for example, ridge regression of Hoerl and Kennard (1970) and Hoerl and Kennard (1970); LASSO of Tibshirani (1996) or elastic net of Zou and Hastie (2005), among others.\nThe most commonly applied measures to detect whether the degree of multicollinearity is troubling are the following:\nThe Variation Inflation Factor (VIF) that is obtained with the following expression \\(VIF(j) = \\frac{1}{1-R_{j}^{2}}\\), \\(j=2,\\dots,p\\), where \\(R_{j}^{2}\\) is the coefficient of determination of the auxiliary regression \\(\\mathbf{X}_{j} = \\mathbf{X}_{-j} \\cdot \\boldsymbol{\\alpha} + \\mathbf{w}\\) with \\(\\mathbf{X}_{-j}\\) being the matrix obtained when the independent variable \\(\\mathbf{X}_{j}\\) is eliminated from matrix \\(\\mathbf{X}\\). It is considered that values of VIF higher than 10 indicate that the degree of multicollinearity is troubling (see, for example, Marquardt (1970) or O’Brien (2007)). Salmerón et al. (2018a) and Salmerón et al. (2020b) showed that the VIF is not an appropriate measure to detect linear relations between the intercept and other explanatory variables (non-essential collinearity).\nThe Condition Number (CN) that is obtained with the following expression \\(CN(\\mathbf{X}) = \\sqrt{\\frac{\\mu_{max}}{\\mu_{min}}}\\) where \\(\\mu_{max}\\) and \\(\\mu_{min}\\) are the maximum and minimum eigenvalue of matrix \\(\\mathbf{X}^{t} \\mathbf{X}\\). To obtain the eigenvalues, the matrix \\(\\mathbf{X}\\) has to be previously transformed in order to ensure that all its columns present unit length (see Salmerón et al. (2018a) for more details about this transformation). Values lower than 20 imply light collinearity, between 20 and 30 moderate collinearity, while values higher than 30 imply strong collinearity (see, for example, Belsley et al. (1980) and Belsley (1991)).\nAnother alternative is to calculate the CN with and without the intercept with the goal of analyzing the contribution of the intercept.\nAnother set of measures to detect the existence of troubling multicollinearity are the matrix of simple linear correlations between the independent variables, \\(\\mathbf{R} = \\left( cor(X_{l}, X_{m}) \\right)_{l,m=2,\\dots,p}\\) and its determinant, \\(| \\mathbf{R} |\\).\nGarcía et al. (2019) show that values for the coefficient of simple correlation between the independent variables higher than \\(\\sqrt{0.9}\\) and determinant lower than \\(0.1013 + 0.00008626 \\cdot n - 0.01384 \\cdot p\\) indicate a troubling degree of multicollinearity (see Salmerón et al. (2021a) or Salmerón et al. (2021b) for more details).\nThe first value differs strongly from the threshold normally proposed equal to 0.7 to indicate a problem of near collinearity (see, for example, Halkos and Tsilika (2018)).\nAlso useful to use the coefficient of variation (CV), values less than 0.1002506 indicate the existence of troubling multicollinearity (see Salmerón et al. (2020b) for more details).\nGarcía et al. (2016) and Salmerón et al. (2020a) showed that the VIF is invariant to origin and scale changes, which is the same as saying that model \\(\\mathbf{Y} = \\mathbf{X} \\cdot \\boldsymbol{\\beta} + \\mathbf{u}\\) and the model \\(\\mathbf{Y} = \\mathbf{x} \\cdot \\boldsymbol{\\beta} + \\mathbf{u}\\) present the same VIF, where \\(\\mathbf{x} = [\\mathbf{x}_{1} \\ \\mathbf{x}_{2} \\dots \\mathbf{x}_{p}]\\) with \\(\\mathbf{x}_{i} = \\frac{\\mathbf{X}_{i}-a_{i}}{b_{i}}\\) for \\(a_{i} \\in \\mathbb{R}\\), \\(b_{i}>0\\) and \\(i=1,\\dots,p\\). Note that if \\(a_{i} = \\overline{\\mathbf{X}}_{i}\\), \\(\\mathbf{x}_{1}\\) is a vector of zeros, i.e. the intercept disappears from the model. Instead, Salmerón et al. (2018b) showed that the CN is not invariant to origin and scale changes, meaning that the two previous models present different CNs. This fact implies that models \\(\\mathbf{Y} = \\mathbf{X} \\cdot \\boldsymbol{\\beta} + \\mathbf{u}\\) and \\(\\mathbf{Y} = \\mathbf{x} \\cdot \\boldsymbol{\\beta} + \\mathbf{u}\\) present different eigenvalues.\nConsequently, transforming the data in a linear regression model may affect the detection of the multicollinearity problem depending on the diagnostic measure. Furthermore, note that this sensitivity to scaling is due to the fact that there are certain transformations (such as data centering) that mitigate the multicollinearity problem, so that the dependence or otherwise on scaling simply highlights the capacity/incapacity of each measure to detect this reduction of the degree of near multicollinearity.\nTherefore, when transforming the data in a linear regression model and analyzing whether the degree of multicollinearity is of concern or not, it is necessary to be clear whether the measure used to detect it is affected by the transformation and whether it is capable of detecting the two types of near multicollinearity mentioned (essential and non-essential). Thus, in this paper we analyze the MC index recently presented in Lin et al. (2020).\nFirst, this paper will briefly review the MC index. In order to show that the MC index depends on the transformation of the data and its inability to detect non-essential multicollinearity, we present two simulations with a troubling degree of essential and non-essential multicollinearity, respectively, and a third simulation where the degree of multicollinearity is not troubling. For all these cases, we calculate the different measures to detect multicollinearity commented on the introduction together with the MC index. Two empirical applications recently applied in the scientific literature are also presented. After a discussion of the results we propose a scatter plot between the VIF and CV to detect which variables are the cause of the troubling degree of multicollinearity and the kind of multicollinearity (essential or non-essential) existing in the model. Finally, the main conclusions of the paper are summarized.\n2 Background: MC index\nThe MC index presented in Lin et al. (2020) is based on the existing relation between the VIFs ant the inverse of the eigenvalues of the matrix \\(\\mathbf{Z}^{t} \\mathbf{Z}\\) where \\(\\mathbf{Z}\\) represents the standardized matrix of \\(\\mathbf{X}\\). This is to say, is the matrix \\(\\mathbf{x}\\) mentioned in the introduction when for all \\(i\\) it is obtained that \\(a_{i} = \\overline{\\mathbf{X}}_{i}\\) and \\(b_{i} = \\sqrt{n \\cdot var \\left( \\mathbf{X}_{i} \\right)}\\) where \\(var \\left( \\mathbf{X}_{i} \\right)\\) represents the variance of \\(\\mathbf{X}_{i}\\).\nMore precisely, taking into account the fact that in the main diagonal of \\(\\left( \\mathbf{Z}^{t} \\mathbf{Z} \\right)^{-1}\\) we find the VIFs (for standardized data), it is possible to establish the following relation:\n\\[\\begin{equation}\n\\left(\n\\begin{array}{c}\nVIF(2) \\\\\n\\vdots \\\\\nVIF(p)\n\\end{array}\n\\right) = \\mathbf{A} \\cdot \\left(\n\\begin{array}{c}\n\\frac{1}{\\mu_{2}} \\\\\n\\vdots \\\\\n\\frac{1}{\\mu_{p}}\n\\end{array}\n\\right),\n\\end{equation}\\]\nwhere \\(\\mathbf{A}\\) is a matrix that depends on the eigenvalues of \\(\\mathbf{Z}^{t} \\mathbf{Z}\\) and \\(\\mu_{p}\\) is the maximum eigenvalue of this matrix.\nFrom this relationship, resampling and obtaining the regression of \\(1/\\mu_{p}\\) as a function of the VIFs, Lin et al. (2020) proposed the use of the t-statistics to conclude which variable contributes the most to this relationship, thus identifying which variables are responsible for the degree of approximate multicollinearity in the model. These authors defined the MC index as an index from zero to one, larger values indicating greater contribution of the variable i in explaining the observed severity of multicollinearity.\nTaking into account that the calculation of the MC index is based on the relation established between the VIFs and the inverse of the smallest eigenvalue, it seems logical to consider that the transformation of the data may affect the calculation of this measure. Thus, it is possible to conclude:\nIf the MC index is calculated with a transformation of the data that leads to the elimination of the intercept, the non-essential multicollinearity will be ignored.\nDue to the fact that MC index is based on the VIF, it should inherit its inability to detect the non-essential multicollinearity.\nIn conclusion, regardless of whether the data is transformed or not, the MC index is not capable of detecting non-essential multicollinearity. It is expected that it will show its usefulness in the case of essential multicollinearity.\nTherefore, when Lin et al. (2020) commented that there are different views on what centering technique is most appropriate in regression […] To facilitate this diversity of opinion, in the software implementation of mcvis, we allow the option of passing matrices with different centering techniques as input. The role of scaling is not the focus of our work as our framework does not rely on any specific scaling method, far from facilitating the use of their proposal, they consider scenarios for which the MC index is not designed, since in their theoretical development and step 2 of their method, the standardization of the data is performed. However, as shown in this paper, the MC index is capable of detecting multicollinearity of the essential type only when used with its default option studentise.\n3 Simulations\nIn this section, different versions of the matrix \\(\\mathbf{X} = [\\mathbf{1} \\ \\mathbf{X}_{2} \\ \\mathbf{X}_{3} \\ \\mathbf{X}_{4}]\\) will be simulated. The results on the correlation matrix, its determinant, condition number (with and without intercept), variance inflation factor and coefficient of variation are obtained using the multiColl package (see Salmerón et al. (2021a) and Salmerón et al. (2021b) for more details).\nIn all cases, are calculated the values for the MC index for each set of simulated data considering the two alternative transformation for the data: Euclidean (centered by mean and divided by Euclidean length) and studentise (centered by mean and divided by standard deviation). In each case (simulation and kind of transformation), the calculation of the MC index was performed 100 times.\nSimulation 1\nIn this case, 100 observations are generated according to \\(\\mathbf{X}_{i} \\sim N(10, 100)\\), \\(i=2,3\\), and \\(\\mathbf{X}_{4} = \\mathbf{X}_{3} - \\mathbf{p}\\) where \\(\\mathbf{p} \\sim N(1, 0.5)\\). The goal of this simulation is to ensure that the variables \\(\\mathbf{X}_{3}\\) and \\(\\mathbf{X}_{4}\\) will be highly correlated (essential multicollinearity). This fact is confirmed when taking into account the following results in relation to the correlation matrix, correlation matrix’s determinant, the CN with and without the intercept (with its corresponding increasing), the VIFs and the coefficient of variation of the different variables.\n\n\n\n\n\n  RdetR(X_S1)\n\n$`Correlation matrix`\n            X2_S1      X3_S1       X4_S1\nX2_S1  1.00000000 -0.0875466 -0.09110579\nX3_S1 -0.08754660  1.0000000  0.99881845\nX4_S1 -0.09110579  0.9988184  1.00000000\n\n$`Correlation matrix's determinant`\n[1] 0.002330192\n\n  CNs(X_S1)\n\n$`Condition Number without intercept`\n[1] 38.67204\n\n$`Condition Number with intercept`\n[1] 66.94135\n\n$`Increase (in percentage)`\n[1] 42.22997\n\n  VIF(X_S1)\n\n     X2_S1      X3_S1      X4_S1 \n  1.013525 425.587184 425.860062 \n\n  CVs(X_S1)\n\n[1] 1.239795 1.052896 1.166335\n\nTable 1 shows three random iterations, the average value of the 100 times and the standard deviation. As expected in the case of essential multicollinearity, from the average values of Simulation 1 it is noted that (specially with the transformation studentise) the MC index correctly identified that the variables \\(\\mathbf{X}_{3}\\) and \\(\\mathbf{X}_{4}\\) are causing the troubling degree of essential multicollinearity. However, it is noted that in some cases the intercept or the variable \\(\\mathbf{X}_{2}\\) are identified as relevant in the existing linear relations when the Euclidean transformation is performed. This behavior is not observed when the studentise transformation is performed. This fact seems to indicate that the MC index depends on the transformation with the studentise transformation being the most appropriate.\n\n\n\n\n\nTable 1: MC index for the independent variables in Simulation 1. Three random iterations, the average value of the 100 times and the standard deviation for Euclidean and studentise transformations. The relationship between the second and third variables is detected when studentise transformation is performed.\n\n\n\n\nX2\n\n\nX3\n\n\nX4\n\n\nEuclidean - Random 1\n\n\n0.2846628\n\n\n0.3670736\n\n\n0.3482636\n\n\nEuclidean - Random 2\n\n\n0.1466484\n\n\n0.4444270\n\n\n0.4089246\n\n\nEuclidean - Random 3\n\n\n0.4026253\n\n\n0.3140131\n\n\n0.2833616\n\n\nEuclidean - Average\n\n\n0.2505292\n\n\n0.3899482\n\n\n0.3595226\n\n\nEuclidean - Standard Deviation\n\n\n0.1075164\n\n\n0.0550333\n\n\n0.0526817\n\n\nStudentise - Random 1\n\n\n0.0000338\n\n\n0.4942761\n\n\n0.5056901\n\n\nStudentise - Random 2\n\n\n0.0001294\n\n\n0.4901233\n\n\n0.5097473\n\n\nStudentise - Random 3\n\n\n0.0000307\n\n\n0.4950536\n\n\n0.5049157\n\n\nStudentise - Average\n\n\n0.0000519\n\n\n0.4944290\n\n\n0.5055191\n\n\nStudentise - Standard Deviation\n\n\n0.0000264\n\n\n0.0023510\n\n\n0.0023528\n\n\nSimulation 2\nIn this case, 100 observations are generated according to \\(\\mathbf{X}_{i} \\sim N(10, 100)\\), \\(i=2,3\\), and \\(\\mathbf{X}_{4} \\sim N(10, 0.0001)\\). The goal of this simulation is to ensure that the variable \\(\\mathbf{X}_{4}\\) will be highly correlated to the intercept (non-essential multicollinearity). This fact is confirmed from the following results taking into account that Salmerón et al. (2020b) showed that a value of the CV lower than 0.1002506 indicates a troubling degree of non-essential multicollinearity.\n\n\n\n\n\nRdetR(X_S2)\n\n$`Correlation matrix`\n           X2_S2       X3_S2      X4_S2\nX2_S2  1.0000000 -0.08754660 0.06676070\nX3_S2 -0.0875466  1.00000000 0.09445547\nX4_S2  0.0667607  0.09445547 1.00000000\n\n$`Correlation matrix's determinant`\n[1] 0.9778526\n\nCNs(X_S2)\n\n$`Condition Number without intercept`\n[1] 2.999836\n\n$`Condition Number with intercept`\n[1] 2430.189\n\n$`Increase (in percentage)`\n[1] 99.87656\n\nVIF(X_S2)\n\n   X2_S2    X3_S2    X4_S2 \n1.013525 1.018091 1.014811 \n\nCVs(X_S2)\n\n[1] 1.239794695 1.052896496 0.001022819\n\nTable 2 presents three random iterations, the average value of the 100 times and the standard deviation for Simulation 2 for Euclidean and studentise transformations. Before commenting the results of Simulation 2, it is important to take into account the fact that with transformations that imply the elimination of the intercept it will not be possible to detect the non-essential multicollinearity.\nNote that in some occasions, when Euclidean transformation is performed, it is concluded that \\(\\mathbf{X}_{3}\\) and \\(\\mathbf{X}_{4}\\) are the most relevant while, when studentise transformation is performed, all variables seem to present the same relevance. In the first case, a higher stability is observed by considering the average values, although the conclusion is that there is a relation between \\(\\mathbf{X}_{3}\\) and \\(\\mathbf{X}_{4}\\) when the relationship is between the intercept and \\(\\mathbf{X}_{4}\\).\n\n\n\n\n\nTable 2: MC index for the independent variables in Simulation 2. Three random iterations, the average value of the 100 times and the standard deviation for Euclidean and studentise transformations. The relationship between the four variable and the intercept is not detected.\n\n\n\n\nX2\n\n\nX3\n\n\nX4\n\n\nEuclidean - Random 1\n\n\n0.2671991\n\n\n0.2097102\n\n\n0.5230907\n\n\nEuclidean - Random 2\n\n\n0.1649092\n\n\n0.5119198\n\n\n0.3231711\n\n\nEuclidean - Random 3\n\n\n0.2126011\n\n\n0.2678652\n\n\n0.5195337\n\n\nEuclidean - Average\n\n\n0.1678676\n\n\n0.3335266\n\n\n0.4986058\n\n\nEuclidean - Standard Deviation\n\n\n0.0725673\n\n\n0.0845660\n\n\n0.1029678\n\n\nStudentise - Random 1\n\n\n0.3307490\n\n\n0.3342851\n\n\n0.3349658\n\n\nStudentise - Random 2\n\n\n0.3646487\n\n\n0.2995420\n\n\n0.3358093\n\n\nStudentise - Random 3\n\n\n0.3107573\n\n\n0.3481032\n\n\n0.3411395\n\n\nStudentise - Average\n\n\n0.3541923\n\n\n0.3150319\n\n\n0.3307758\n\n\nStudentise - Standard Deviation\n\n\n0.0201173\n\n\n0.0203717\n\n\n0.0187872\n\n\nSimulation 3\nFinally, in this case 100 observations are generated according to \\(\\mathbf{X}_{i} \\sim N(10, 100)\\), \\(i=2,3,4\\). The goal of this simulation is to ensure that the degree of multicollinearity (essential and non-essential) will be not troubling. This fact is confirmed when taking into account the following results.\n\n\n\n\n\nRdetR(X_S3)\n\n$`Correlation matrix`\n           X2_S3       X3_S3      X4_S3\nX2_S3  1.0000000 -0.08754660 0.06676070\nX3_S3 -0.0875466  1.00000000 0.09445547\nX4_S3  0.0667607  0.09445547 1.00000000\n\n$`Correlation matrix's determinant`\n[1] 0.9778526\n\nCNs(X_S3)\n\n$`Condition Number without intercept`\n[1] 2.07584\n\n$`Condition Number with intercept`\n[1] 3.60862\n\n$`Increase (in percentage)`\n[1] 42.47552\n\nVIF(X_S3)\n\n   X2_S3    X3_S3    X4_S3 \n1.013525 1.018091 1.014811 \n\nCVs(X_S3)\n\n[1] 1.239795 1.052896 1.019006\n\nTable 3 presents three random iterations, the average value of the 100 times and the standard deviation for Simulation 3 for Euclidean and studentise transformations. Simulation 3 shows different situations depending on the transformation: when the Euclidean transformation is performed, the variable \\(\\mathbf{X}_{3}\\) is also identified apart from variable \\(\\mathbf{X}_{4}\\); with the studentise transformation, all the variables seem to be relevant.\n\n\nTable 3: MC index for the independent variables in Simulation 3. Three random iterations, the average value of the 100 times and the standard deviation for Euclidean and studentise transformations. The not troubling relationship between the variables is not detected.\n\n\n\n\nX2\n\n\nX3\n\n\nX4\n\n\nEuclidean - Random 1\n\n\n0.1318422\n\n\n0.3832372\n\n\n0.4849206\n\n\nEuclidean - Random 2\n\n\n0.1679453\n\n\n0.4315253\n\n\n0.4005294\n\n\nEuclidean - Random 3\n\n\n0.0728076\n\n\n0.5693939\n\n\n0.3577986\n\n\nEuclidean - Average\n\n\n0.1083812\n\n\n0.4107279\n\n\n0.4808910\n\n\nEuclidean - Standard Deviation\n\n\n0.0410296\n\n\n0.0661189\n\n\n0.0628212\n\n\nStudentise - Random 1\n\n\n0.3586837\n\n\n0.3154482\n\n\n0.3258681\n\n\nStudentise - Random 2\n\n\n0.3805084\n\n\n0.3205751\n\n\n0.2989166\n\n\nStudentise - Random 3\n\n\n0.3651891\n\n\n0.3069920\n\n\n0.3278189\n\n\nStudentise - Average\n\n\n0.3541923\n\n\n0.3150319\n\n\n0.3307758\n\n\nStudentise - Standard Deviation\n\n\n0.0201173\n\n\n0.0203717\n\n\n0.0187872\n\n\n\n\n\nInterpretation of the obtained results\nFrom the above results, it is concluded that the MC index applied individually is not able to detect if the degree of multicollinearity is troubling. This conclusion is in line with the comment presented by Lin et al. (2020) where it is stated that those classical collinearity measures are used together with mcvis for the better learning of how one or more variables display dominant behavior in explaining multicollinearity. That is to say, it is recommended to use measures such as the VIF and the CN to detect whether the degree of multicollinearity is troubling and, if it is, then use the MC index to detect which variables are more relevant.\nWe should reiterate the fact that the results of the MC index depend on the transformation performed with the data.\nFinally, it is worthy of note that the lowest dispersion is obtained when the studentise transformation is performed, which indicate that with this transformation a higher stability exists in the results obtained with the 100 iterations performed.\n4 Examples\nIn this section we will analyze two examples applied recently to illustrate the multicollinearity problem. The first one focuses on the existence of non-essential approximate multicollinearity while the second one is focused on essential multicollinearity.\nExample 1\nSalmerón et al. (2020b) analyzed the Euribor as a function of the harmonized index of consumer prices (HICP), the balance of payments to net current account (BC) and the government deficit to net non-financial accounts (GD).\nThe following determinant of the matrix of correlations of the independent variables, the VIFs, condition number without intercept and with intercept and the coefficients of variation indicate that the degree of essential near multicollinearity is not troubling while the non-essential type (due to the variable HIPC) is.\nFigure 1 shows a tour displayed with a scatterplot by using the tourr package. This package allows tours of multivariate data, see Wickham et al. (2011) for more details. From the tour on all the explanatory variables (it runs for 3.47 minutes in the html version), no linear relation is observed between the explanatory variables. Note that this package does not allow us to work with the intercept.\n\n\n\n\n\n\nFigure 1: An interactive 2-D tour for example 1 by using the R tourr package. From the 3.47 minutes tour on all the explanatory variables no linear relation is observed between the explanatory variables. Note that the intercept is not considered\n\n\n\nIn relation to the application of the mcvis package in this example, it can be observed that if the Euclidean transformation is performed the method concludes that all the variables seem to be relevant:\n\n     HIPC   BC   GD\ntau3 0.29 0.39 0.32\n\nFor studentise transformation it concludes the establishing of a relationship between HIPC and GD:\n\n     HIPC   BC   GD\ntau3 0.49 0.13 0.38\n\nClearly, these conclusions are not consistent. Moreover, by eliminating the intercept when transforming the data, it is not feasible to detect the non-essential multicollinearity.\nExample 2\nJames et al. (2013) illustrated multicollinearity with a data set which contains information about the dependent variable balance (average credit card debt for a number of individuals) and, among others, the following quantitative independent variables: income, limit (credit limit), rating (credit rating), cards (number of credit cards), age, education (years of education). Data is available in the ISLR package (see James et al. (2021) for more details).\nThe following determinant of the matrix of correlations of the independent variables, the simple correlation between limit and rating (0.99687974), the VIFs, condition number without intercept and with intercept and CVs indicate that the degree of approximate multicollinearity of the non-essential type is not troubling while that of the essential type (due to the relationship between variables limit and rating) is troubling.\nFigure 2 displays its tour by using again tourr package. Multicollinearity was checked using a tour on all the explanatory variables (it runs for 3.47 minutes in html version). In this case a certain linear relationship is observed, although it is difficult to determine which variables are related.\n\n\n\n\n\n\nFigure 2: An interactive 2-D tour for example 2 by using the R tourr package. From the 3.47 minutes tour on all the explanatory variables a certain linear relationship bewteen independent variables is observed between the explanatory variables. Note that the intercept is not considered\n\n\n\nIn relation to the application of the mcvis package in this example, it can be observed that if the Euclidean transformation is performed the variable with the highest value is income, with the variable rating being the second-lowest in value.\n\n     Income Limit Rating Cards  Age Education\ntau6   0.62  0.14   0.05  0.05 0.11      0.03\n\nFinally, when the studentise transformation is applied, the method clearly indicates that variables limit and rating are the ones responsible for the multicollinearity problem.\n\n     Income Limit Rating Cards Age Education\ntau6      0  0.49   0.51     0   0         0\n\n5 Discussion\nThe results shown in the previous sections indicate that, on the one hand, the calculation of the MC index depends on the transformation performed and, on the other hand, that to apply this index with guarantees, the studentise transformation is the most appropriate.\nIt was also showed that the MC index ``inherits’’ the same limitations indicated by Lin et al. (2020) when they state that collinearity indices such as the variance inflation factor and the condition number have limitations and may not be effective in some applications. In the case of VIF, these limitations are well summarized by Lin et al. (2020): The VIF can show how variables are correlated to each other, but as shown, low correlation does not guarantee low level of collinearity.\nNote that the example applied by the authors in Section 1 (similar to the one presented in Salmerón et al. (2018a) and a reduced version of the one presented by Belsley (1984)) is a clear case in which the multicollinearity is non-essential. As was commented earlier, the VIF ((Salmerón et al. 2018a) and (Salmerón et al. 2020b)) and the MC index are not able to detect this kind of multicollinearity, for this reason they are only recommended for detecting essential multicollinearity.\nAnother limitation of the VIF (and also of the MC index) worthy of note is that it is not adequate for calculating with dummy variables. Using these kinds of variables, the VIF is obtained from a coefficient of determination of an auxiliary regression whose dependent variable is a dummy variable. Although it is possible to apply ordinary least squares in this kind of regression, it is well known that it can present some problems: for example, the coefficient of determination is not representative since it measures the linear relation but the relation between the dummy variable and the rest of independent variables is not linear. For this reason, these kinds of regressions are estimated with non-linear models such as the logit/probit. Thus, we consider that if it is not adequate for calculating the VIF associated to a dummy variable, these kinds of variables should be avoided in the calculation of the MC index.\nFinally, Simulation 3 shows that MC index is not able to detect whether the degree of essential multicollinearity is troubling. For this reason, it should only be used once this situation is determined by other measures (as set out in the introduction) and in order to determine which variables cause it.\n6 Solution\nThe distinction between essential and non-essential multicollinearity and the limitations of each measure for detecting the different kinds of multicollinearity, can be very useful for detecting whether there is a troubling degree of multicollinearity, what kind of multicollinearity it is and which variables are causing the multicollinearity. Thus, taking into account the fact that the VIF is useful for detecting essential multicollinearity and the CV is useful for detecting non-essential multicollinearity, the scatter plot of both measures can provide interesting information in a joint way.\nWe present the scatter plot for the values of the VIF and the CV for the three simulations previously performed. Note that the figures include the lines corresponding to the established thresholds for each measure: red dashed vertical line for 0.1002506 (CV) and red dotted horizontal line for 10 (VIF). These lines determine four regions that can be interpreted as follows: A, existence of troubling non-essential and non-troubling essential multicollinearity; B, existence of troubling essential and non-essential multicollinearity; C, existence of non-troubling non-essential and troubling essential multicollinearity; D: non-troubling degree of existing multicollinearity (essential and non-essential).\nConsidering this classification, in Simulation 1 (Figure 3) it is noted that there is a troubling degree of essential multicollinearity due to the variables \\(\\mathbf{X}_{3}\\) and \\(\\mathbf{X}_{4}\\), while in Simulation 2 (Figure 4) it is noted that there is a troubling degree of non-essential multicollinearity due to the variable \\(\\mathbf{X}_{4}\\) and in Simulation 3 (Figure 5) it is noted that the degree of multicollinearity is not troubling.\n\n\n\nFigure 3: Representation of the VIFs and CVs of explanatory variables in Simulation 1. A troubling degree of essential multicollinearity due to variables third and four is detected by considering thresholds of 0.1002506 (CV) and 10 (VIF) highlighted with red lines. Note that VIFs of variables three and four are greater than 10 and the CV of variable 2 is lower than 0.1002506.\n\n\n\n\n\n\nFigure 4: Representation of the VIFs and CVs of explanatory variables in Simulation 2. A troubling degree of non-essential multicollinearity due to variable four is detected by considering thresholds of 0.1002506 (CV) and 10 (VIF) highlighted with red lines. Note that the VIFs of variables three and four are less than 10 and the CV greater than 0.1002506; while the CV of second variable is less than 0.1002506.\n\n\n\n\n\n\nFigure 5: Representation of the VIFs and CVs of explanatory variables in Simulation 3. The degree of multicollinearity is not troubling by considering thresholds of 0.1002506 (CV) and 10 (VIF) highlighted with red lines. Note that the VIFs of all variables are lower than 10 and the CVs greater than 0.1002506.\n\n\n\nAnalogously, we present Figures 6 and 7 for Examples 1 and 2, respectively. For Example 1, it is concluded that the only type of troubling multicollinearity is non-essential due to the second variable (HIPC). On the other hand, for Example 2 it is concluded that the only type of troubling multicollinearity is that essentially due to the relationship existing between the third and fourth variables (limit and rating).\n\n\n\nFigure 6: Representation of the VIFs and CVs of the variables of example 1. A troubling degree of non-essential multicollinearity generated by variable 2 (HIPC) is detected by considering thresholds of 0.1002506 (CV) and 10 (VIF) highlighted with red lines. Note that the VIFs of variables three and four are less than 10 and the CV greater than 0.1002506; while the CV of second variable is less than 0.1002506.\n\n\n\n\n\n\nFigure 7: Representation of the VIFs and CVs of the variables of example 2. A troubling degree of essential multicollinearity generated by variables three and four (limit and rating) is detected by considering thresholds of 0.1002506 (CV) and 10 (VIF) highlighted with red lines. Note that the VIFs of variables three and four are greater than 10 while the VIFs and CVs of the rest of variables are, respectively, lower than 10 and greater than 0.1002506.\n\n\n\n7 Summary\nThis paper analyses the limitations that may arise in detecting the problem of troubling multicollinearity in a linear regression model due to transformations performed on the data. The discussion is used to illustrate that the MC index presented by Lin et al. (2020) depends on the way that the data are transformed.\nIt was shown that when the studentise transformation is performed, the measure is stable for measuring what variable contributes the most to the linear relationship, and thus identifying what variables best explain the collinearity in the original data (Lin et al. 2020), as long as the multicollinearity is essential. This kind of transformation was taken as default by the authors and this paper contributes with a formal justification. Note that the MC index only provides interesting information if the troubling multicollinearity is previously detected using other measures (such as the VIF or the CN).\nSummarizing, the achievement of the goal intended by the mcvis package is conditioned by the following limitations:\nIt is not able to detect non-essential multicollinearity.\nOther measures (as the VIF or CN) should be applied previously to determine whether the degree of essential multicollinearity is troubling.\nThe studentise transformation should be applied.\nIt is not applicable for dummy variables.\nFinally, it is proposed to use a scatter plot between the VIFs and the CVs, on the one hand, to detect whether the degree of multicollinearity (essential or not essential) is troubling and, on the other hand, to detect which variables are causing the multicollinearity. This would overcome the limitations of the mcvis package discussed above while achieving its overall purpose.\n8 Acknowledgments\nThis work has been supported by project PP2019-EI-02 of the University of Granada (Spain), by project A-SEJ-496-UGR20 of the Andalusian Government’s Counseling of Economic Transformation, Industry, Knowledge and Universities (Spain) and by project I+D+i PID2019-107767GA-I0 financed by MCIN/AEI/10.13039/501100011033.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-010.zip\nCRAN packages used\nmultiColl, tourr, mcvis, ISLR\nCRAN Task Views implied by cited packages\n\n\nD. Belsley. A guide to using the collinearity diagnostics. Computational Science in Economics and Manegement, 4: 33–50, 1991.\n\n\nD. A. Belsley. Demeaning conditioning diagnostics through centering. The American Statistician, 38(2): 73–77, 1984.\n\n\nD. A. Belsley, E. Kuh and R. E. Welsch. Regression diagnostics: Identifying influential data and sources of collinearity. New York: John Wiley & Sons, 1980.\n\n\nC. García, R. Salmerón and C. B. García. Choice of the ridge factor from the correlation matrix determinant. Journal of Statistical Computation and Simulation, 89(2): 211–231, 2019.\n\n\nJ. García, R. Salmerón, C. García and M. López. Standardization of variables and collinearity diagnostic in ridge regression. International Statistical Review, 84: 245–266, 2016.\n\n\nD. Gujarati. Basic Econometrics. 8th ed McGraw Hill, 2010.\n\n\nG. Halkos and K. Tsilika. Programming correlation criteria with free cas software. Computational Economics, 52(1): 299–311, 2018.\n\n\nA. E. Hoerl and R. W. Kennard. Ridge regression: Biased estimation for nonorthogonal problems. Technometrics, 12(1): 55–67, 1970.\n\n\nG. James, D. Witten, T. Hastie and R. Tibshirani. An introduction to statistical learning with application in r. Springer, 2013.\n\n\nG. James, D. Witten, T. Hastie and R. Tibshirani. ISLR: Data for an introduction to statistical learning with applications in r. 2021. URL https://CRAN.R-project.org/package=ISLR. R package version 1.4.\n\n\nC. Lin, K. Wang and S. Mueller. MCVIS: A new framework for collinearity discovery, diagnostic and visualization. Journal of Computational and Graphical Statistics, 2020. DOI 10.1080/10618600.2020.1779729.\n\n\nD. W. Marquardt. Generalized inverses, ridge regression, biased linear estimation and nonlinear estimation. Technometrics, 12(3): 591–612, 1970.\n\n\nD. W. Marquardt. You should standardize the predictor variables in your regression models. J. Amer. Statist. Assoc., 75(369): 87–91, 1980.\n\n\nA. Novales. Econometría. Madrid: McGraw-Hill, 1988.\n\n\nR. M. O’Brien. A caution regarding rules of thumb for variance inflation factors. Quality & Quantity, 41: 673–690, 2007.\n\n\nR. Ramanathan. Introductory econometrics with applications. 2002.\n\n\nR. Salmerón, C. García and J. García. A guide to using the r package multiColl for detecting multicollinearity. Computational Economics, 57: 529–536, 2021a. DOI 10.1007/s10614-019-09967-y.\n\n\nR. Salmerón, C. García and J. García. Comment on “a note on collinearity diagnostics and centering” by velilla (2018). The American Statistician, 74(1): 68–71, 2020a.\n\n\nR. Salmerón, C. García and J. García. The multiColl package versus other existing packages in r to detect multicollinearity. Computational Economics, 2021b. DOI 10.1007/s10614-021-10154-1.\n\n\nR. Salmerón, C. García and J. García. Variance inflation factor and condition number in multiple linear regression. Journal of Statistical Computation and Simulation, 88: 2365–2384, 2018a.\n\n\nR. Salmerón, J. García, C. García and M. López. Transformation of variables and the condition number in ridge estimation. Computational Statistics, 33: 1497–1524, 2018b.\n\n\nR. Salmerón, A. Rodríguez and C. García. Diagnosis and quantification of the non-essential collinearity. Computational Statistics, 35: 647–666, 2020b.\n\n\nR. D. Snee and D. W. Marquardt. Comment: Collinearity diagnostics depend on the domain of prediction, the model, and the data. The American Statistician, 38(2): 83–87, 1984.\n\n\nR. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 267–288, 1996.\n\n\nH. Wickham, D. Cook, H. Hofmann and A. Buja. Tourr: A r package for exploring multivariate data with projections. Journal of Statistical Software, 40(2): 1–18, 2011. DOI 10.18637/jss.v040.i02.\n\n\nJ. M. Wooldridge. Introducción a la econometría. Un enfoque moderno. Second Madrid: Thomson Paraninfo, 2008.\n\n\nH. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67: 301–320, 2005.\n\n\n\n\n",
    "preview": "articles/RJ-2023-010/distill-preview.png",
    "last_modified": "2023-11-07T21:31:39+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "articles/RJ-2023-011/",
    "title": "netgwas: An R Package for Network-Based Genome Wide Association Studies",
    "description": "Graphical models are a powerful tool in modelling and analysing complex biological associations in high-dimensional data. The R-package netgwas implements the recent methodological development on copula graphical models to (i) construct linkage maps, (ii) infer linkage disequilibrium networks from genotype data, and (iii) detect high-dimensional genotype-phenotype networks. The netgwas learns the structure of networks from ordinal data and mixed ordinal-and-continuous data. Here, we apply the functionality in netgwas to various multivariate example datasets taken from the literature to demonstrate the kind of insight that can be obtained from the package. We show that our package offers a more realistic association analysis than the classical approaches, as it discriminates between direct and induced correlations by adjusting for the effect of all other variables while performing pairwise associations. This feature controls for spurious interactions between variables that can arise from conventional approaches in a biological sense. The netgwas package uses a parallelization strategy on multi-core processors to speed-up computations.",
    "author": [
      {
        "name": "Pariya Behrouzi",
        "url": {}
      },
      {
        "name": "Danny Arends",
        "url": {}
      },
      {
        "name": "Ernst C. Wit",
        "url": {}
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-011.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:39+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-014/",
    "title": "A Study in Reproducibility: The Congruent Matching Cells Algorithm and cmcR Package",
    "description": "Scientific research is driven by our ability to use methods, procedures, and materials from previous studies and further research by adding to it. As the need for computationally-intensive methods to analyze large amounts of data grows, the criteria needed to achieve reproducibility, specifically computational reproducibility, have become more sophisticated. In general, prosaic descriptions of algorithms are not detailed or precise enough to ensure complete reproducibility of a method. Results may be sensitive to conditions not commonly specified in written-word descriptions such as implicit parameter settings or the programming language used. To achieve true computational reproducibility, it is necessary to provide all intermediate data and code used to produce published results. In this paper, we consider a class of algorithms developed to perform firearm evidence identification on cartridge case evidence known as the *Congruent Matching Cells* (CMC) methods. To date, these algorithms have been published as textual descriptions only. We introduce the first open-source implementation of the Congruent Matching Cells methods in the R package cmcR. We have structured the cmcR package as a set of sequential, modularized functions intended to ease the process of parameter experimentation. We use cmcR and a novel variance ratio statistic to explore the CMC methodology and demonstrate how to fill in the gaps when provided with computationally ambiguous descriptions of algorithms.",
    "author": [
      {
        "name": "Joseph Zemmels",
        "url": {}
      },
      {
        "name": "Susan VanderPlas",
        "url": {}
      },
      {
        "name": "Heike Hofmann",
        "url": {}
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\n\n\n\n1 Introduction\nForensic examinations are intended to provide an objective assessment of the probative value of a piece of evidence.\nTypically, this assessment of probative value is performed by a forensic examiner who visually inspects the evidence to determine whether it matches evidence found on a suspect.\nThe process by which an examiner arrives at their evidentiary conclusion is largely opaque and has been criticized (President’s Council of Advisors on Sci. & Tech. 2016) because its subjectivity does not allow for an estimation of error rates.\nIn response, National Research Council (2009) pushed to augment subjective decisions made by forensic examiners with automatic algorithms that objectively assess evidence and can be explained during court testimony.\nIn addition to the objectivity of these algorithms, there is an additional benefit: we expect that an algorithm with the same random seed run on the same data multiple times will produce the same answer; that is, that the results are repeatable.\nThis is extremely beneficial because it allows the prosecution and defense to come to the same conclusion given objective evidence or data.\nRepeatability and reproducibility\nRepeatability in forensic labs is enforced primarily using standard operating procedures (SOPs), which specify the steps taken for any given evaluation, along with the concentrations of any chemicals used, the range of acceptable machine settings, and any calibration procedures required to be completed before the evidence is evaluated.\nWhen labs use computational procedures, this SOP is augmented with specific algorithms, which are themselves SOPs intended for use by man and machine.\nAlgorithms are generally described on two levels: we need both the conceptual description (intended for the human using the algorithm) and the procedural definition (which provides the computer hardware with a precise set of instructions).\nFor scientific and forensic repeatability and reproducibility, it is essential to have both pieces: the algorithm description is critical for establishing human understanding and justifying the method’s use in court, but no less important is the computer code which provides the higher degree of precision necessary to ensure the results obtained are similar no matter who evaluates the evidence.\nAs with SOPs in lab settings, the code parameters function like specific chemical concentrations; without those details, the SOP would be incomplete and the results produced would be too variable to be accepted in court.\nThe National Academy of Sciences, Engineering, and Medicine (2019) defines reproducibility as “obtaining consistent computational results using the same input data, computational steps, methods, code, and conditions of analysis.” This form of reproducibility requires that the input data, code, method, and computational environment are all described and made available to the community.\nIn many situations, this level of reproducibility is not provided – not just in forensics but in many other applied disciplines.\nIn forensics in particular, it is easier to list the exceptions: reproducible algorithms have been proposed in sub-disciplines including DNA (Tyner et al. 2019; Tvedebrink et al. 2020; Goor et al. 2020), glass (Curran et al. 2000; Park and Tyner 2019), handwriting (Crawford 2020), shoe prints (Park and Carriquiry 2020), and ballistic evidence (Hare et al. 2017; Tai and Eddy 2018).\nWe find it useful to instead consider a more inclusive hierarchy of reproducibility.\nAlgorithms at higher tiers of the hierarchy are more easily reproducible in the sense that fewer resources are required to (re)-implement the algorithm.\nDefinition 1 Hierarchy of Reproducibility\n\nConceptual description The algorithm is described and demonstrated in a scientific publication.\n\n\nPseudocode The algorithm is described at a high level of detail with pseudocode implementation provided, and results are demonstrated in a scientific publication.\n\n\nReproducible data The algorithm is described and demonstrated in a scientific publication, and input data are available in supplementary material.\n\n\nComparable results The algorithm is described and demonstrated in a scientific publication, and input data and numerical results are provided in supplementary material.\n\n\nFull reproducibility The algorithm is described and demonstrated in a scientific publication, and the input data, source code, parameter settings, and numerical results are provided in supplementary material.\n\nTo aid in comprehension of an algorithm, it is useful to supplement conceptual descriptions with pseudocode.\nHowever, a conceptual description and pseudocode alone do not contain sufficient detail (e.g., parameter settings) to ensure computational reproducibility.\nImplementing algorithms based on conceptual descriptions or pseudocode requires enumerating and testing possible parameter choices which, depending on their complexity, can be a lengthy and expensive process.\nIn contrast, implementing fully reproducible algorithms requires only as much time as it takes to emulate the original development environment.\nCommonly identified reasons for unreproducible results include (1) ambiguity in how procedures were implemented, (2) missing or incomplete data, and (3) missing or incomplete computer code to replicate all statistical analyses (Leek and Jager 2017).\nIn particular, for statistical algorithms which depend on input data, we find that full reproducibility depends on the provision of both original data and any manual pre-processing applied to said data, as this manual process is not reproducible by itself.\nIn combination with the code, the algorithm description, and the numerical results presented in the paper, it should be possible to fully reproduce the results of a paper.\nIn this paper, we demonstrate the importance of higher levels of reproducibility by examining the Congruent Matching Cells (CMC) algorithm for cartridge case comparisons and developing an open-source, fully reproducible version for general use in the forensics community.\nThe Congruent Matching Cells algorithm\nA cartridge case is the portion of firearm ammunition that encases a projectile (e.g., bullet, shots, or slug) along with the explosive used to propel the projectile through the firearm.\nWhen a firearm is discharged, the projectile is propelled down the barrel of the firearm, while the cartridge case is forced towards the back of the barrel.\nIt strikes the back wall, known as the breech face, of the barrel with considerable force, thereby imprinting any markings on the breech face onto the cartridge case and creating the so-called breech face impressions.\nThese markings are used in forensic examinations to determine whether two cartridge cases have been fired by the same firearm.\nDuring a forensic examination, two pieces of ballistic evidence are placed under a comparison microscope.\nComparison microscopes allow for a side-by-side comparison of two objects within the same viewfinder, as seen in Figure 1.\nA pair of breech face images is aligned along the thin black line in the middle of the images.\nThe degree to which these breech face markings can be aligned is used to determine whether the two cartridge cases came from the same source; i.e., were fired from the same firearm.\nThese breech face impressions are considered analogous to a firearm’s “fingerprint” left on a cartridge case (Thompson 2017).\n\n\n\nFigure 1: A cartridge case pair with visible breech face impressions under a microscope. A thin line can be seen separating the two views. The degree to which the markings coincide is used to conclude whether the pair comes from the same source.\n\n\n\nThe Congruent Matching Cells (CMC) pipeline is a collection of algorithms to process and compare cartridge case evidence (Song 2013).\nSince its introduction, the pipeline and its extensions (Tong et al. 2015; Chen et al. 2017; Song et al. 2018) have shown promise in being able to differentiate between matching and non-matching cartridge cases.\nHowever, so far the CMC pipelines have only been introduced in the form of conceptual descriptions.\nFurther, the cartridge case scans used to validate the pipelines are only available in their raw, unprocessed forms on the NIST Ballistics Toolmark Research Database (Zheng et al. 2016).\nWhile it is clear that the creators of the CMC pipeline have a working implementation, the wider forensic science community only has access to conceptual descriptions of the pipeline and summary statistics describing its performance.\nIn our hierarchy of reproducibility, this puts the CMC algorithm somewhere between the conceptual description and reproducible data stage: the steps are described but no code is available, and the raw data are available but manual pre-processing steps make this raw data insufficient to replicate the pipeline even with newly written code.\nThe development of the CMC algorithm seems to be representative of how many forensic algorithms are developed: after an algorithm is introduced, researchers build upon the foundation laid by the original algorithm in subsequent papers.\nThese changes are often incremental in nature and reflect a growing understanding of the algorithm’s behavior.\nWhile this cycle of scientific progress certainly is not unique to forensic algorithms, given the gravity of the application it is imperative that these incremental improvements not be unnecessarily delayed.\nAs such, we believe that the forensic community at-large would benefit greatly by establishing an open-source foundation for their algorithms upon which additional improvements can be developed.\nUsing open-source algorithms are cheaper to use than writing one’s own code, enables the process of peer review by providing an accessible benchmark, and helps other research groups or companies stay on the leading edge of technology development (The Linux Foundation 2017).\nHere, we describe the process of implementing the CMC pipeline for the comparison of marks on spent cartridge cases, using the descriptions from two published papers, Song et al. (2014) and Tong et al. (2015).\nOur R package, cmcR, provides an open-source implementation of the CMC pipeline.\nWe use cmcR to illustrate how ambiguities in the textual description of an algorithm can lead to highly divergent results.\nIn particular, our implementation highlights an extreme sensitivity to processing and parameter decisions that has not been discussed previously.\nAdditionally, we argue that our implementation can be used as a template for future implementations of forensic pattern-matching algorithms to not only ensure transparency and auditability, but also to facilitate incremental improvements in forensic algorithms.\nIn the remainder of this paper, we describe a general, reproducible, and open-source CMC pipeline which encompasses those discussed in Song (2013), Song et al. (2014), and Tong et al. (2015).\nSong (2013) lays out the conceptual framework for the original CMC pipeline later implemented in Song et al. (2014) and Tong et al. (2014).\nAn improvement of the pipeline presented in Tong et al. (2015) and used in subsequent papers is referred to as the “High CMC” method (Chen et al. 2017). However, it should be noted that what the authors refer to as the original and High CMC decision rules are variations of one step of a larger CMC pipeline.\nThe cmcR package contains implementations designed for use with 3D topographical scans of the original decision rule described in Song (2013) and Song et al. (2014) and the High CMC decision rule described in Tong et al. (2015).\nThe source code to the full cmcR package is accessible at https://github.com/CSAFE-ISU/cmcR.\n2 The CMC pipeline\nIn this section, we examine the process of implementing the CMC pipeline for automatic comparisons of 3D cartridge case scans.\nAt each step, we will discuss how we filled in the gaps of the original description during the creation of cmcR.\nAll of the CMC pipelines can be broken down into three broad stages: (1) pre-processing, (2) cell-based similarity feature extraction, and (3) application of a decision rule as illustrated in .\nIn the following sections we break each of these stages further into a set of modular steps.\nOne advantage of modularizing these algorithms is that we can implement an algorithm as a set of sequential procedures.\nThis allows us to test new variations against the old implementation in a coherent, unified framework.\n\n\n\nFigure 2: The stages of CMC pipelines. In the pre-processing stage, each scan is prepared for analysis, removing extraneous information and noise. Then, each scan is broken up into cells, which are numerically compared to cells in the other scan to determine an optimal alignment. Finally, each of the scores arising from the cells in the second stage are compared to a reference distribution to determine whether the scans originate from the same source or from different sources.\n\n\n\nThe primary difference between the two pipelines presented here, using the original and High CMC decision rules, lies in how the decision rules are utilized to separate matching vs. non-matching cartridge case pairs.\nIn addition, there are also several small differences in the parameters used in the pre-processing and comparison procedures.\nInitial data\nDigital microscopy is capable of precision measurements of surface topology at high resolutions.\nUsing a 3D microscope, we can obtain scans of breech face impressions at the micron level (\\(1 \\mu m = 10^{-3} mm = 10^{-6} m\\)).\nThese 3D topological scans are used as input to automated comparison algorithms, such as the CMC pipeline originally proposed in Song (2013).\nWe will use the same data set referenced in Song et al. (2014) and Tong et al. (2015) to illustrate usage of the cmcR package.\nThese 3D scans of cartridge cases are available from the NIST Ballistics Toolmark Research Database (Zheng et al. 2016).\nThe strings defined below refer to three cartridge case scans available on the NBTRD from Fadul et al. (2011) and will be used throughout the remainder of this paper.\n\n\n\n\n\nlibrary(cmcR)\n\nnbtrd_url <- \"https://tsapps.nist.gov/NRBTD/Studies/CartridgeMeasurement\"\n\nx3p_ids <- c(\"DownloadMeasurement/2d9cc51f-6f66-40a0-973a-a9292dbee36d\",\n             \"DownloadMeasurement/cb296c98-39f5-46eb-abff-320a2f5568e8\",\n             \"DownloadMeasurement/8ae0b86d-210a-41fd-ad75-8212f9522f96\")\n\nfile_names <- c(\"fadul1-1.x3p\",\"fadul1-2.x3p\",\"fadul2-1.x3p\")\n\npurrr::walk2(.x = x3p_ids,\n             .y = file_names,\n             .f = function(x3p_id,file_name){\n               download.file(url = file.path(nbtrd_url, x3p_id),\n                             destfile = paste0(\"data/\",file_name),mode = \"wb\")\n             })\n\n\nCartridge case scans are commonly stored in the ISO standard x3p file format (Geometrical product specifications (GPS) — Surface texture: Areal — Part 72: XML file format x3p 2017).\nx3p is a container format which consists of a single surface matrix representing the height value of the breech face surface and metadata concerning the parameters under which the scan was taken (size, resolution, creator, microscope, microscopy software versions, etc.).\nThe x3ptools package provides functionality to work with the format in R (Hofmann et al. 2020).\nFigure 3 shows the surface matrices of a known match (KM) pair of cartridge cases from a study by Fadul et al. (2011).\nIn this study, a total of 40 cartridge cases were scanned with a lateral resolution of 6.25 microns (micrometers) per pixel.\nThe surface matrices are approximately \\(1200 \\times 1200\\) pixels in size corresponding to an area of about \\(3.8 \\times 3.8\\) mm\\(^2\\).\n\n\n\nFigure 3: Unprocessed surface matrices of the known-match Fadul 1-1 and Fadul 1-2 Fadul et al. \\(2011\\). The observations in the corners of these surface matrices are artifacts of the staging area in which these scans were taken. The holes on the interior of the primer surfaces are caused by the firing pin striking the primer during the firing process. The region of the primer around this hole does not come into uniform contact with the breech face of the firearm.\n\n\n\n\n\n\nOnly certain regions of a cartridge case contain identifying breech face impression markings.\nSong (2013) defines “valid correlation regions” as regions where “the individual characteristics of the ballistics signature are found that can be used effectively for ballistics identification.” Prior to applying the CMC comparison procedure, cartridge scans must undergo some pre-processing to isolate the valid correlation regions.\nPre-processing procedures\nDuring the pre-processing stage, we apply sequential steps to prepare each cartridge case for analysis.\nThe goal of this process is to remove the edges and center of the scan which did not come into contact with the breech face, as well as any artifacts of the scan and microscope staging which do not accurately represent the breech face surface.\nThe various iterations of the CMC algorithm describe different variations of these steps.\nA summary of these steps is shown in Figure 4.\n\n\n\nFigure 4: Overview of the set of pre-processing steps used in the CMC algorithms. Where a procedure step is not discussed or explicitly not applied in the paper, the path traverses empty space.\n\n\n\nTranslating the pre-processing steps in Figure 4 into an implementation requires the implementer to decide between potentially many implicit parameter choices.\nFor example, Table 1 compares the pre-processing procedures as described in Song et al. (2014) to considerations that need to be made when implementing the procedures.\nDepending on one’s interpretation of the description, there are many possible implementations that satisfy the described procedure - in contrast, there was only one implementation that led to the original results.\nWhile not explicitly mentioned in Song et al. (2014), Song et al. (2018) indicates that the “trimming” of the unwanted regions of the scan is performed manually.\nIt is difficult to replicate manual steps as part of a reproducible pipeline; the best solution is for the authors to provide intermediate data after the manual steps have been completed.\n\n\n\n\n\nTable 1: Description of pre-processing procedures from Song et al. \\(2014\\) vs. considerations that need to be made when implementing these procedures. Each of these considerations requires the implementer to decide between potentially many choices.\n\n\nDescription from Song et al. (2014)\n\n\nImplementation Considerations\n\n\n“Trim off the inside firing pin surface and other areas outside the breech face mark, so that only breech face impression data remain for correlation.”\n\n\nRemoval of firing pin hole and primer exteriorRemoval of global trendRemoval of primer roll-off\n\n\n“Identify and remove dropouts or outliers.”\n\n\nDefinition of outliersWhat “removal” of dropouts or outliers means\n\n\n“Apply a band-pass Gaussian regression filter with 40 \\(\\mu\\)m short cut-off length and 400 \\(\\mu\\)m long cut-off length to remove low frequency components, including surface curvature, form error, waviness and high frequency components which mainly arise from the instrument noise.”\n\n\nWavelength cut-off parametersSpecific implementation of the filter\n\n\nThe pre-processing procedures are implemented via modularized functions of the form preProcess_*.\nModularizing the steps of the pre-processing procedures makes the overall process easier to understand and allows for experimentation.\nFigure 5 shows an overview of the pre-processing framework for the Fadul 1-1 breech face from reading the scan (left) to an analysis-ready region (right).\nFor each scan in Figure 5, eleven height value percentiles: the Minimum (0th), 1st, 2.5th, 10th, 25th, Median (50th), 75th, 90th, 97.5th, 99th, and Maximum (100th) are mapped to a purple-to-orange color gradient.\nThis mapping is chosen to highlight the extreme values in each scan.\n\n\n\nFigure 5: Illustration of the sequential application of pre-processing steps implemented in cmcR. We map the cartridge case surface height values to a divergent purple-white-orange color scale to emphasize deviations from the median height value (represented here as 0 micrometers). At each stage, the variability in height across the scan decreases as we emphasize the regions containing breech face impressions.\n\n\n\nWe demonstrate usage of the preProcess_* functions on the Fadul 1-1 scan.\nEach code chunk is followed up with an explanation of the functions used.\n\n\n# Step (1)\nfadul1.1 <- x3ptools::x3p_read(\"data/fadul1-1.x3p\")\n\n\nWe begin with a 3D scan.\nTypically, we downsample scans to about 25% of their size by only retaining every other row and column in the surface matrix.\nThe breech faces in Fadul et al. (2011) were initially scanned at a resolution of 3.125 \\(\\mu\\)m per pixel.\nDownsampling reduces the resolution to 6.25 \\(\\mu\\)m per pixel.\nStep (1) in Figure 5 shows an unprocessed breech face scan.\n\n\n# Step (2)\nfadul1.1_cropped <- fadul1.1 %>%\n  cmcR::preProcess_crop(region = \"exterior\") %>%\n  cmcR::preProcess_crop(region = \"interior\")\n\n\nWe then use a labeling algorithm to identify three major regions of the scan: the exterior of the cartridge case primer, the breech face impression region of interest, and the firing pin impression region in the center of the scan (Hesselink et al. 2001; Barthelme 2019).\nWe remove observations outside of the breech face impression region (i.e., replaced with NA).\nThe resulting breech face scan, like the one shown in step (2) of Figure 5, is reproducible assuming the same parameters are used.\nThe preProcess_crop function removes the exterior and firing pin impression region on the interior based on the region argument.\n\n\n# Step (3)\nfadul1.1_deTrended <- fadul1.1_cropped %>%\n  preProcess_removeTrend(statistic = \"quantile\", tau = .5, method = \"fn\")\n\n\nIn step (3), we remove the southwest-to-northeast trend observable in steps (1) and (2) of Figure 5 by subtracting the estimated conditional median height value.\nThe result of the preProcess_removeTrend function the median-leveled breech face scan in step (3) of Figure 5.\n\n\n# Step (4)\nfadul1.1_processed <- fadul1.1_deTrended %>%\n  preProcess_gaussFilter(filtertype = \"bp\", wavelength = c(16,500)) %>%\n  x3ptools::x3p_sample(m = 2)\n\n\nFinally, we apply a band-pass Gaussian filter to the surface values to attenuate noise and unwanted large-scale structure.\nStep (4) of Figure 5 shows the effect of the preProcess_gaussFilter function.\nThere is currently no determination or removal of outliers in the cmcR package’s pre-processing procedures.\nInstead, we rely on the low-pass portion of the Gaussian filter to reduce the effects of any high-frequency noise.\nFigure 6 displays the processed Fadul 1-1 and Fadul 1-2 scans; the second matrix is processed using the same parameters.\nNext, similarity features are extracted from a processed cartridge case pair in the cell-based comparison procedure.\n\n\n\nFigure 6: Fadul 1-1 and Fadul 1-2 after pre-processing. Similar striated markings are now easier to visually identify on both surfaces. It is now clearer that one of the scans needs to be rotated to align better with the other.\n\n\n\n“Correlation cell” comparison procedure\nAs described in Song (2013), breech face markings are not uniformly impressed upon a cartridge case during the firing process.\nAs such, only certain sections of the cartridge case are used in a comparison.\nIn the CMC pipeline as proposed by Song (2013) two scans are compared by partitioning one breech face scan into a grid of so-called “correlation cells”.\nThese cells are compared individually to their best-matching counterpart on the other scan.\nIf a large proportion of these correlation cells are highly similar to their counterparts on the other breech face scan, this is considered as evidence that the markings on the two cartridge cases were made by the same source.\nThe number of highly similar cells is defined as the CMC count \\(C\\) (Song 2013) of the breech-face comparison.\nThe CMC count is considered to be a more robust measure of similarity than the correlation calculated between two full scans.\n\n\n\nFigure 7: Illustration of comparing a cell in the reference cartridge case scan (left) to a larger region in a questioned cartridge case scan (right). Every one of the cells in the reference cartridge case is similarly paired with a region in the questioned cartridge case. To determine the rotation at which the two cartridge cases align, the cell-region pairs are compared for various rotations of the questioned cartridge case.\n\n\n\nFigure 7 illustrates the cell-based comparison procedure between two cartridge case scans.\nThe scan on the left serves as the reference; it is divided into a grid of \\(8 \\times 8\\) cells.\n\n\n\nFigure 8: Each CMC implementation uses a slightly different procedure to obtain a similarity score between two cartridge cases. Steps which are implemented with additional user-specified parameters are shaded purple; steps which are described but without sufficient detail are shaded grey.\n\n\n\nFigure 8 shows the steps of the correlation cell comparison process in each of the papers as well as the cmcR implementation.\nEach cell is paired with an associated larger region in the other scan.\nThe absolute location of each cell and region in their respective surface matrices remain constant.\nHowever, the scan on the right is rotated to determine the rotation at which the two scans are the most “similar,” as quantified by the cross-correlation function (CCF).\nFor real-valued matrices \\(A\\) and \\(B\\) of dimension \\(M \\times N\\) and \\(P \\times Q\\), respectively, the cross-correlation function, denoted \\((A \\star B)\\) is defined as\n\\[\n(A \\star B)[m,n] = \\sum_{i=1}^M \\sum_{j=1}^N A[i,j] B[(i + m), (j + n)],\n\\] where \\(1 \\leq m \\leq M + P - 1\\) and \\(1 \\leq n \\leq N + Q - 1\\).\nBy this definition, the \\([m,n]\\)th element of the resulting \\(M + P - 1 \\times N + Q - 1\\) CCF matrix quantifies the similarity between matrices \\(A\\) and \\(B\\) for a translation of matrix \\(B\\) by \\(m\\) pixels horizontally and \\(n\\) pixel vertically.\nThe index at which the CCF attains a maximum represents the optimal translation needed to align \\(B\\) with \\(A\\).\nThe CCF as defined need not be bounded between \\(-1\\) and \\(1\\).\nHowever, it is common to normalize the CCF for interpretability, and this is the convention adopted in the cmcR package.\nPrior to calculating the CCF, the matrices \\(A\\) and \\(B\\) are standardized through subtraction of their respective means and division by their respective standard deviations.\nThis is referred to as the Areal Cross-Correlation Function (ACCF) in some CMC papers (Ott et al. 2017).\nA direct calculation of the CCF for breech face scans based on the definition above is prohibitively slow.\nWhile computationally feasible alternatives exist, Song (2013) and other CMC papers do not specify the algorithm used to calculate the CCF.\nPublished descriptions of the CMC algorithm do not detail how the CCF is calculated.\nIn image processing, it is common to use an implementation based on the Fast Fourier Transform (Brown 1992).\nThis implementation leverages the Cross-Correlation Theorem, which states that for matrices \\(A\\) and \\(B\\), the CCF can be expressed in terms of a frequency-domain pointwise product:\n\\[\n(A \\star B )[m,n]= \\mathcal{F}^{-1}\\left(\\overline{\\mathcal{F}(A)} \\odot \\mathcal{F}(B)\\right)[m,n],\n\\]\nwhere \\(\\mathcal{F}\\) and \\(\\mathcal{F}^{-1}\\) denote the discrete Fourier and inverse discrete Fourier transforms, respectively, and \\(\\overline{\\mathcal{F}(A)}\\) denotes the complex conjugate (Brigham 1988).\nBecause the product on the right-hand side is calculated pointwise, we trade the moving sum computations from the definition of the CCF for two forward Fourier transformations, a pointwise product, and an inverse Fourier transformation.\nThe Fast Fourier Transform (FFT) algorithm can be used to reduce the computational load considerably.\nOur implementation of this FFT-based CCF calculation is adapted from the cartridges3D package (Tai 2021).\nNo computational shortcut comes without some trade-offs, though, and this FFT-based CCF calculation is no different.\nThe FFT does not tolerate missing values, and breech faces are not continuous surfaces – all of the white regions in Figure 7 correspond to missing values.\nWhile it is unclear how the CCF is implemented in the CMC papers, the cmcR package adopts the following conventions:\nOnly cells with a minimum proportion of non-missing pixels are assessed. This minimum threshold differs across CMC papers (15% in Chen et al. (2017) vs. 10% in Song et al. (2018), as shown in Figure 8), and is referenced but not specified in several other papers (Chu et al. 2013; Song et al. 2014; Tong et al. 2014). The comparison_calcPropMissing function computes the proportion of a matrix that is missing (NA-valued).\nMissing values are replaced with the overall mean value when the FFT-based CCF is computed (using function comparison_replaceMissing).\nThe optimal translation is determined using the FFT-based CCF (using comparison_fft_ccf).\nBased on the optimal translation determined from the FFT-based CCF, we compute the pairwise complete CCF directly, avoiding any distortion of the CCF computation based on compensation for missing values (using function comparison_cor).\nAll of the steps dealing with cell-based comparisons are implemented as functions of the form comparison_*.\nSimilar to the preProcess_* functions, the comparison_* functions can be chained together through a sequence of pipes.\nBelow, we use the comparison_allTogether function to perform the entire cell-based comparison procedure in one call.\nThe comparison procedure is performed twice: once with Fadul 1-1 considered the “reference” scan divided into cells that are compared to the “target” scan Fadul 1-2 and again with the roles reversed.\n\n\n# Fill in most of the arguments first\ncomp_w_pars <- purrr::partial(.f = comparison_allTogether,\n                              numCells = c(8,8), maxMissingProp = .85)\n\n# Then, map the remaining values to theta\nkmComparisonFeatures <- purrr::map_dfr(\n  seq(-30,30,by = 3),\n  ~comp_w_pars(reference = fadul1.1, target = fadul1.2, theta = .))\n\nkmComparisonFeatures_rev <- purrr::map_dfr(\n  seq(-30,30,by = 3),\n  ~comp_w_pars(reference = fadul1.2, target = fadul1.1, theta = .))\n\n\nThe comparison_allTogether function consists of the following steps wrapped into a single convenience function:\ncomparison_cellDivision: Divide the reference scan into cells\ncomparison_getTargetRegions: Extract regions associated with each reference cell from the target scan\ncomparison_calcPropMissing: Compute missing proportions and filter out cells with a proportion of missing values above the threshold.\ncomparison_standardizeHeights: Standardize height values\ncomparison_replaceMissing: Replace missing values\ncomparison_fft_ccf: Compute CCF and estimated translations using FFT\ncomparison_alignedTargetCell: Extract a matrix from the target scan corresponding to the region of the target scan to which the reference cell aligns\ncor: Calculate the pairwise-complete correlation between each cell pair\nThe comparison_allTogether is called repeatedly while rotating the target scan by a set of rotation angles.\nWhen implementing the High CMC decision rule (Tong et al. 2015), both combinations of reference and target scan are examined (e.g. A-B and B-A).\nTable 2 shows several rows of the data frame output of the comparison_allTogether function for the comparison of Fadul 1-1 vs. Fadul 1-2 considering Fadul 1-1 as the reference scan.\nAlthough we used a grid of \\(8 \\times 8\\) cells, there were only 26 cell-region pairs that contained a sufficient proportion of non-missing values (15% in this example).\nThe features derived from the correlation cell procedure (CCF\\(_{max}\\), \\(\\Delta x\\), \\(\\Delta y\\), \\(\\theta\\)) are then used to measure the similarity between scans.\n\n\n\n\n\nTable 2: Example of output from correlation cell comparison procedure between Fadul 1-1 and Fadul 1-2 rotated by -24 degrees. Due to the large proportion of missing values that are replaced to compute the FFT-based correlation, the pairwise-complete correlation is most often greater than the FFT-based correlation.\n\n\nCell Index\n\n\nPairwise-comp. corr.\n\n\nFFT-based corr.\n\n\n\\(\\Delta\\)x\n\n\n\\(\\Delta\\)y\n\n\n\\(\\theta\\)\n\n\n2, 7\n\n\n0.432\n\n\n0.228\n\n\n-14\n\n\n-33\n\n\n-24\n\n\n2, 8\n\n\n0.464\n\n\n0.176\n\n\n9\n\n\n-44\n\n\n-24\n\n\n3, 1\n\n\n0.841\n\n\n0.478\n\n\n-7\n\n\n15\n\n\n-24\n\n\n3, 8\n\n\n0.699\n\n\n0.277\n\n\n-7\n\n\n5\n\n\n-24\n\n\n4, 1\n\n\n0.850\n\n\n0.375\n\n\n-4\n\n\n11\n\n\n-24\n\n\nDecision rule\nFor each cell on the reference scan, we calculate the translation \\((\\Delta x, \\Delta y)\\) and cross-correlation across rotations by a set of angles \\(\\theta\\) of the target scan.\nThe task is to determine whether multiple cells come to a “consensus” on a particular translation and rotation.\nIf such a consensus is reached, then there is evidence that a true aligning translation and rotation exists and the cartridge cases match.\nThe CMC decision rules principally differ in how they identify consensus among the \\(\\Delta x, \\Delta y, \\theta\\) values.\nHere, we describe the two pipelines implemented in the cmcR package: using the original decision rule described in Song et al. (2014) and the High CMC decision rule proposed in Tong et al. (2015).\nThe Original CMC decision rule\nThis section briefly describes the decision rule used in the first CMC paper (Song 2013).\nFor a thorough explanation of the procedure, refer to the CMC Decision Rule Description vignette of the cmcR package.\nLet \\(x_i, y_i, \\theta_i\\) denote the translation and rotation parameters which produce the highest CCF for the alignment of cell-region pair \\(i\\), \\(i = 1,...,n\\) where \\(n\\) is the total number of cell-region pairs containing a sufficient proportion of non-missing values.\nSong (2013) propose the median as a consensus \\((x_{\\text{ref}}, y_{\\text{ref}}, \\theta_{\\text{ref}})\\) across the cell-region pairs.\nThen, the distance between each \\((x_i, y_i, \\theta_i)\\) and \\((x_{\\text{ref}}, y_{\\text{ref}}, \\theta_{\\text{ref}})\\) is compared to thresholds \\(T_{x}, T_{y}, T_\\theta, T_{\\text{CCF}}\\).\nA cell-region pair \\(i\\) is declared a “match” if all of the following conditions hold:\n\\[\\begin{eqnarray}\n|x_i - x_{\\text{ref}}| &\\leq& T_{x}, \\\\ \\nonumber\n|y_i - y_{\\text{ref}}| &\\leq& T_{y}, \\\\ \\nonumber\n|\\theta_i - \\theta_{\\text{ref}}| &\\leq& T_{\\theta}, \\\\ \\nonumber\n\\text{CCF}_{\\max,i} &\\geq& T_{\\text{CCF}}.\n\\tag{1}\n\\end{eqnarray}\\]\nThe number of matching cell-region pairs, the “CMC count,” is used as a measure of similarity between the two cartridge cases.\nSong et al. (2014) indicate that the thresholds \\(T_{x}, T_{y}, T_\\theta, T_{\\text{CCF}}\\) need to be determined experimentally.\nTable 3 summarizes the thresholds used in various CMC papers.\n\n\n\n\n\nTable 3: Different thresholds for translation, rotation, and CCF\\(_{\\max}\\) are used across different papers. The range in CCF\\(_{\\max}\\) is particularly notable.\n\n\nPaper\n\n\nTranslation \\(T_x, T_y\\) (in pixels)\n\n\nRotation \\(\\theta\\) (in degrees)\n\n\n\\(CCF_{\\max}\\)\n\n\nSong et al. (2014)\n\n\n20\n\n\n6\n\n\n0.60\n\n\nTong et al. (2014)\n\n\n30\n\n\n3\n\n\n0.25\n\n\nTong et al. (2015)\n\n\n15\n\n\n3\n\n\n0.55\n\n\nChen et al. (2017)\n\n\n20\n\n\n3\n\n\n0.40\n\n\nSong et al. (2018)\n\n\n20\n\n\n6\n\n\n0.50\n\n\nUnlike the original CMC pipeline, the High CMC decision rule considers multiple rotations for each cell-region pair.\nThe High CMC decision rule\nFor the High CMC decision rule, two scans are compared in both directions - i.e., each scan takes on the role of the reference scan that is partitioned into a grid of cells.\nTong et al. (2015) claim that some matching cell-region pairs “may be mistakenly excluded from the CMC count” under the original decision rule because they attain the largest CCF at a rotation outside the range allowed by \\(T_\\theta\\) “by chance.”\nTong et al. (2015) introduce consensus values across all cell-region pairs for each rotation angle \\(\\theta\\) and calculate a \\(\\theta\\)-dependent CMC count as the sum of matches observed.\nUnder the High CMC rule, a cell-region pair \\(i\\) is defined as a match conditional on a particular rotation \\(\\theta\\) if it satisfies the following three conditions:\n\\[\\begin{eqnarray}\n|x_{i,\\theta} - x_{ref,\\theta}| &\\leq& T_x \\\\ \\nonumber\n|y_{i,\\theta} - y_{ref,\\theta}| &\\leq& T_y \\\\ \\nonumber\n\\text{CCF}_{i,\\theta} &\\geq& T_{\\text{CCF}}.\n\\tag{2}\n\\end{eqnarray}\\]\nThe \\(\\theta\\)-dependent CMC count, CMC\\(_\\theta\\), is defined as the sum of matching cell-region pairs.\nTong et al. (2015) assert that for a truly matching cartridge case pair, the relationship between \\(\\theta\\) and CMC\\(_\\theta\\) should exhibit a “prominent peak” near the true rotation value.\nThat is, CMC\\(_{\\theta}\\) should be largest when the scans are close to being correctly aligned.\nFurther, non-matching pairs should exhibit a “relatively flat and random […] pattern” across the CMC\\(_{\\theta}\\) values.\nTo determine whether a “prominent peak” exists in the relationship between \\(\\theta\\) and CMC\\(_\\theta\\), Tong et al. (2015) consider an interval of rotation angles with large associated CMC\\(_\\theta\\) values.\nLet \\(CMC_{\\text{max}} = \\max_{\\theta} CMC_{\\theta}\\) be the maximum CMC\\(_\\theta\\) count across all rotation angles.\nFor \\(\\tau > 0\\), define \\(S(\\tau) = \\{\\theta : CMC_\\theta > (CMC_{\\max} - \\tau)\\}\\) as the set of rotations with “large” CMC\\(_\\theta\\) values.\nTong et al. (2015) consider the “angular range” as \\(R(\\tau) = |\\max_{\\theta} S(\\tau) - \\min_\\theta S(\\tau)|\\).\nIf \\(R(\\tau)\\) is small, then there is evidence that many cells agree on a single rotation and that the scans match.\nTo arrive at a CMC count similarity score, Tong et al. (2015) suggest a value for \\(\\tau\\) of 1 and determine:\n\nIf the angular range of the “high CMCs” is within the range \\(T_\\theta\\), identify the CMCs for each rotation angle in this range and combine them to give the number of CMCs for this comparison in place of the original CMC number.\n\nIf the angular range is larger than \\(T_\\theta\\), we say that the cartridge case pair “fails” the High CMC criteria and the original CMC number is used.\nThe High CMC decision rule returns a CMC count at least as large as the original decision rule.\nImplementation of decision rules\nIn this section, we implement the decision rules in cmcR for both the original and High CMC decision rules.\nFor illustrative purposes, we consider a set of thresholds: \\(T_x = T_y = 20\\), \\(T_{\\theta} = 6\\), and \\(T_{\\text{CCF}} = 0.5\\).\nDecision rules in cmcR are implemented as functions of the form decision_*.\nIn particular, the decision_CMC function applies both the original and High CMC decision rules depending on if the parameter \\(\\tau\\) is set.\nThe code below demonstrates the use of decision_CMC on the features kmComparisonFeatures, extracted from the comparison of scans Fadul 1-1 vs. Fadul 1-2.\nConvserely, kmComparisonFeatures_rev contains the features from a comparison of Fadul 1-2 vs. Fadul 1-1.\nFor comparison, we also compute the CMCs under both decision rules for the comparison between the non-match pair Fadul 1-1 and Fadul 2-1 (not shown to avoid redundancy).\n\n\n\n\n\nkmComparison_cmcs <- kmComparisonFeatures %>% mutate(\n  originalMethodClassif =\n    decision_CMC(cellIndex = cellIndex, x = x, y = y, theta = theta,\n                 corr = pairwiseCompCor, xThresh = 20, thetaThresh = 6,\n                 corrThresh = .5),\n  highCMCClassif =\n    decision_CMC(cellIndex = cellIndex, x = x, y = y, theta = theta,\n                 corr = pairwiseCompCor, xThresh = 20, thetaThresh = 6,\n                 corrThresh = .5, tau = 1))\n\n\nWe use the cmcPlot function to visualize congruent matching cells (CMCs) and non-congruent matching cells (non-CMCs).\nFigure 9 shows the CMCs and non-CMCs in blue and red, respectively, based on the original decision rule.\nThe (red) non-CMC patches are shown in the position where the maximum CCF value in the target scan is attained.\nThe top row shows 18 CMCs in blue and 8 non-CMCs in red when Fadul 1-1 is treated as the reference and Fadul 1-2 the target.\nThe bottom row shows the 17 CMCs and 13 non-CMCs when the roles are reversed.\nThere is no discussion in Song (2013) about combining the results from these two comparison directions, but Tong et al. (2015) propose using the minimum of the two CMC counts (17 in this example).\n\n\n\n\n\n\nFigure 9: CMC results for the comparison between Fadul 1-1 and Fadul 1-2 using the original decision rule. The two plots in the top row show the 18 CMCs when Fadul 1-1 is treated as the “reference” cartridge case to which Fadul 1-2 (the “target”) is compared. The second row shows the 17 CMCs when the roles are reversed. Red cells indicate where cells not identified as congruent achieve the maximum pairwise-complete correlation across all rotations of the target scan.\n\n\n\nSimilarly, CMCs and non-CMCs determined under the High CMC decision rule are shown in Figure 10.\nTreating Fadul 1-1 and Fadul 1-2 as the reference scan yields 20 and 18 CMCs, respectively.\nCombining the results as described above, the final High CMC count is 24.\n\n\n\nFigure 10: Applying the High CMC decision rule to the comparison of Fadul 1-1 and Fadul 1-2 results in 20 CMCs when Fadul 1-1 is treated as the reference (top) and 18 CMCs when Fadul 1-2 is treated as the reference (bottom). Although the individual comparisons do not yield considerably more CMCs than under the original CMC pipeline, Tong et al. (2015) indicate that the High CMCs from both comparisons are combined as the final High CMC count (each cell is counted at most once). Combining the results means that the High CMC decision rule tends to produce higher CMC counts than the original CMC pipeline. In this example, the combined High CMC count is 24 CMCs.\n\n\n\nIn contrast, Figure 11 shows the CMC results for a comparison between Fadul 1-1 and a known non-match scan, Fadul 2-1, under the exact same processing conditions.\nOnly two cells are classified as congruent matching cells under the original decision rule when Fadul 1-1 is the reference scan.\nNo cells are classified as CMCs in the other direction.\nWhile not shown, this pair fails the High CMC criteria and thus was assigned 0 CMCs under the High CMC decision rule.\n\n\n\nFigure 11: Applying both decision rules to the comparison between the non-match pair Fadul 1-1 and Fadul 2-1 results in 2 CMCs under the original decision rule (shown above) and 0 CMCs under the High CMC decision rule (not shown). The seemingly random behavior of the red cells exemplifies the assumption that cells in a non-match comparison do not exhibit an observable pattern. Random chance should be the prevailing factor in classifying non-match cells as CMCs.\n\n\n\n3 Discussion\nAmbiguity in algorithmic descriptions\nDuring the implementation process we encountered ambiguous descriptions of the various CMC pipelines.\nWe include the pre-processing and cell-based comparison procedures in the description of CMC methodology to emphasize how sensitive the final results are to decisions made in these first two steps.\nThe pre-processing and cell-based comparison procedures are discussed only briefly, if at all, in Song et al. (2014), Tong et al. (2014), Tong et al. (2015), or Chen et al. (2017).\nHowever, the results reported often indicate a sensitivity to these procedures.\nAmbiguities range from minor implicit parameter choices (e.g., the convergence criteria for the robust Gaussian regression filter (Brinkman and Bodschwinna 2003)) to procedures that are fundamental to feature calculation (e.g., how the cross-correlation is calculated).\nWe bring up these ambiguities to demonstrate the difficulties that we faced when translating the conceptual description of the CMC pipeline into an actual pipeline.\nWhile many of these choices are unlikely to affect the results dramatically, we believe that any amount of variability that exists solely because of uncertainty in how the method was intended to be implemented is both unnecessary and dangerous in this application.\nThe only solution to such ambiguity is to enumerate, implement, and pare-down the possible choices that could have been made to arrive to published results.\nUnsurprisingly, this process takes a considerable amount of time and resources that would be better spent furthering the state of the field.\nDuring the creation of the cmcR package, the process of re-implementing the comparison and decision steps of the pipeline was fairly straightforward.\nEmulating the pre-processing procedures used, on the other hand, took months of trial and error.\nEven after this effort, we still have no assurances that our implementation would match the results of the original implementation if applied to other data sets.\nIn the next section, we describe the process of resolving these ambiguities in the CMC pipeline descriptions.\nIn doing so, we abstract a set of principles by which pipelines and results can be rendered both computationally reproducible and more thoroughly understood.\nCMC pattern matching pipeline\nAs described in the initial data section, the set of cartridge case scans from Fadul et al. (2011) is commonly used to compare the performance of various classification methods (Song et al. 2014; Tong et al. 2015; Chen et al. 2017).\nThis set consists of 40 cartridge cases and 780 total comparisons: 63 known match comparisons and 717 known non-match comparisons.\nScans of each breech face impression were taken with a Nanofocus Confocal Light Microscope at 10 fold magnification for a nominal lateral resolution of 3.125 microns per pixel and published to the NBTRD (Zheng et al. 2016).\nWe also use the Weller et al. (2012) data set of 95 cartridge cases for comparison.\nFor the Weller et al. (2012) dataset, we manually isolated the breech face impression regions using the FiX3P software (accessible here: https://github.com/talenfisher/fix3p).\nWe compare results from the cmcR package to published results using processed scans available through the Iowa State University DataShare repository (Zemmels et al. 2022).\nOur goal is to show that results obtained from cmcR are similar, at least qualitatively, to previously published results.\nHowever, justification for any differences will ultimately involve educated guesses due to the closed-source nature of the original implementations.\nFor each cartridge case pair, we calculate CMC counts under both the original and High CMC decision rules.\nIn practice, we classify a cartridge case pair as “matching” if its CMC count surpasses some threshold; 6 CMCs being the generally accepted threshold in many papers (Song 2013; Tong et al. 2015; Song et al. 2018).\nHowever, this threshold has been shown to not generalize well to all proposed methods and cartridge case data sets (Chen et al. 2017).\nWe instead use an optimization criterion to select parameters.\nIn doing so, we will demonstrate the sensitivity of the pipeline to parameter choice.\nAdditionally, we introduce a set of principles designed to reduce the need for brute-force searches across parameter settings when re-implementing algorithms without accompanying code.\nAdherence to these principles yields not only computationally reproducible results, but also improves a reader’s understanding of a proposed pipeline.\nProcessing condition sensitivity\nChoosing threshold values \\(T_x, T_y, T_\\theta, T_{\\text{CCF}}\\) for translation, rotation, and maximum cross-correlation is crucial in declaring a particular cell-region pair “congruent.”\nHowever, many combinations of these thresholds yield perfect separation between the matching and non-matching CMC count distributions.\nTherefore, choosing parameters based on maximizing classification accuracy does not lead to an obvious, single set of parameters.\nWe instead consider the ratio of between- and within-group variability to measure separation between match and non-match CMC counts.\nLet C\\(_{ij}\\) denote the CMC count assigned to the \\(j\\)th cartridge case pair, \\(j = 1,...,n_i\\) from the \\(i\\)th group, \\(i = 1,2\\) representing matches and non-matches, respectively.\nFor each set of thresholds we calculate the Variance Ratio \\(r\\) as:\n\\[\nr = r\\left(T_x, T_y, T_\\theta, T_{\\text{CCF}}\\right) = \\frac{\\sum_{i=1}^2 \\left(\\overline{C}_{i.} - \\overline{C}_{..}\\right)^2}{\\sum_{i=1}^2 \\frac{1}{n_i - 1}\\sum_{j=1}^{n_i} \\left(C_{ij} - \\overline{C}_{i.}\\right)^2},\n\\]\nwhere \\(\\overline{C}_{i.}\\) denotes the within-group CMC count average and \\(\\overline{C}_{..}\\) denotes the grand CMC count average.\nGreater separation between and less variability within the match and non-match CMC count distributions yields larger \\(r\\) values.\nFor example, Figure 12 shows results for the original decision rule and the High CMC decision rule for parameters \\(T_x = 20 = T_y\\) pixels, \\(T_{\\text{CCF}} = 0.5\\), and \\(T_{\\theta} = 6\\).\nDespite both decision rules resulting in separation between the matching and non-matching CMC count distributions, the High CMC decision rule yields greater separation as evidenced by the larger \\(r\\) value.\n\n\n\nFigure 12: CMC count relative frequencies under the original decision rule and the High CMC decision rule for \\(T_{\\Delta x} = 20 = T_{\\Delta y}\\) pixels, \\(T_{\\text{CCF}} = 0.5\\), and \\(T_{\\theta} = 6\\) degrees. An AUC \\(= 1\\) corresponds to perfect separation of the match and non-match CMC count distributions. We can see that, for this set of processing parameters, the High CMC decision rule yields higher CMC counts for known matches that the original decision rule while known non-matches have the same distribution under both methods.\n\n\n\nTo explore the pipeline’s sensitivity, we consider five dimensions that have a demonstrable impact on CMC counts:\nthe decision rule (original or High CMC) used,\nwhether the global trend is removed during pre-processing, and\nchoice of congruency thresholds: translation \\(T_x, T_y\\), rotation \\(T_\\theta\\), and cross-correlation \\(T_{\\text{CCF}}\\).\nChoosing a single parameter setting that results in perfect identification is not enough to generally understand the algorithm.\nInstead, we use the variance ratio \\(r\\) to identify promising ranges of parameters.\nFigure 13 shows the value of the variance ratio under different parameter settings.\nWe see that the High CMC decision rule yields better separation than the original decision rule under any parameter setting.\nThe largest variance ratio values are achieved for thresholds \\(T_x, T_y \\in [10,20]\\), \\(T_\\theta = 6\\), and \\(T_{\\text{CCF}} \\in [0.4,0.5]\\).\nInterestingly, considering Table 3, only the parameters used in Song et al. (2018) fall into these ranges.\n\n\n\nFigure 13: Variance ratio values are plotted for different parameter settings. High variance ratios are indicative of a a good separation between CMC counts for known matching pairs and known-non matching pairs. The High CMC decision rule generally performs better than the original decision rule. Removing the trend during pre-processing has a major impact on the effectiveness of the CMC pipeline. In this setting, translation thresholds \\(T_x, T_y \\in [15,20]\\), a rotation threshold \\(T_\\theta = 6\\), and a CCF threshold \\(T_{\\text{CCF}} \\in [0.4,0.5]\\) lead to a separation of results.\n\n\n\nAs shown in Figure 13, de-trending breech-scans in the pre-processing stage emerges as a critical step to achieve good algorithmic results.\nThis step is not explicitly mentioned in the written-word descriptions of the algorithm in Song (2013), Tong et al. (2014), Tong et al. (2015), Chen et al. (2017), or Song et al. (2018), though it appears from their examples that it was used in the process.\nFigure 13 also illustrates how breaking a pipeline up into modularized steps eases experimentation.\nWe will expand upon this idea in the next section.\nWe compare the best results from cmcR to results presented in previous papers.\nIn particular, we have calculated variance ratio statistics shown in Figure 14 based on CMC counts reported in Song (2013), Tong et al. (2014), Tong et al. (2015), Chen et al. (2017), and Song et al. (2018).\nThe last row in each facet shows the variance ratio values obtained from cmcR.\nWe see that the implementation provided in cmcR yields comparable results to previous CMC papers.\n\n\n\nFigure 14: Variance ratios based on results reported in various CMC papers. The High CMC decision rule tends to outperform the original decision rule. However, it should be emphasized that each paper uses very different processing and parameter settings meaning the results are difficult to compare. The values labeled “cmcR” show the largest variance ratio values for the original and High CMC decision rules based on a limited grid search. These results indicate that the CMC pipeline implementation provided in cmcR yields comparable results to previous CMC papers.\n\n\n\n4 Conclusion\nReproducibility is an indispensable component of scientific validity (Goodman et al. 2016).\nIn this paper, we demonstrate at least three ways reproducibility can go awry: ambiguity in procedural implementation, missing or incomplete data, and missing or incomplete code.\nIn forensics, many matching algorithms are commonly presented in the form of conceptual descriptions with accompanying results.\nThere is sound reasoning to this; conceptual descriptions are more easily understood by humans compared to computer code.\nHowever, using the CMC pipelines as an example we have observed the gaps that can exist when translating a conceptual description of an algorithm to a genuine implementation.\nThis is largely due to the fact that conceptual descriptions rarely detail implicit parameter choices required to run an algorithm.\nConsequently, there are multiple choices that are compatible with the description of an algorithm in a publication.\nThis is dangerous in a forensics context because if many parameter settings are valid but only a narrow range lead to the same conclusion, it is entirely possible that witnesses for the prosecution and defense comef to different conclusions.\nIn order to prevent such misunderstandings, it is not enough to have guidelines for parameter settings and/or a sensitivity study – it is also necessary to standardize the specific computer code.\nThe parameter values are only useful within the context of a single software package or pipeline.\nThese principles of open, accessible, interoperable code are also critical for a fair (in the legal sense) justice system: the defense has access to the code to understand the evidence against them, lawyers and examiners can assess the utility of the analysis method, and judges can determine whether a method is admissible in court.\nTransparent and intuitive open-source algorithms, such as cmcR, should be considered the gold standard in allowing the forensic science community to validate a pipeline.\nOur contribution to the CMC literature is the open-source implementation, which fills the gaps in the human-friendly descriptions in the original papers.\nIn addition, because we have structured the cmcR implementation as a modular pipeline, it is easier to improve upon the CMC method and document the effects of specific changes to the algorithm compared to previous versions.\nThe modularization creates an explicit framework to assess the utility and effectiveness of each piece of the algorithm, and allows us to independently manipulate each step while monitoring the downstream impact on the results.\nAdditionally, it allows future collaborators to improve on pieces of the pipeline, adding new options and improving the method without having to re-invent the wheel.\nIndeed, re-implementing steps of the pipeline is at best a useful academic exercise and at worst a waste of time and resources that could be spent actually improving the pipeline.\nEven after many months of trial and error, although we have succeeded in obtaining qualitatively similar results on two data sets, it is difficult to know whether our implementation will behave the same as previous implementations on external data sets.\nGeneralizability is an important assessment for any computational algorithm (Vanderplas et al. 2020).\nOur application is far from unique: some journals have adopted policies encouraging or requiring that authors provide code and data sufficient to reproduce the statistical analyses, with the goal of building a “culture of reproducibility” in their respective fields (Peng 2009, 2011; Stodden et al. 2013).\nPeer-review and scientific progress in the truest sense requires that pre-processed data, code, and results be made openly available (Desai and Kroll 2017; Kwong 2017).\nOur experience with the CMC algorithm suggests that these standards should be adopted by the forensic science community, leveraging open-source ecosystems like R and software sharing platforms such as Github.\nWe firmly believe that the forensic community should not go only halfway, trading a subjective, human black box for objective, proprietary algorithms that are similarly opaque and unauditable.\nOpen, fully reproducible packages like cmcR allow research groups to make incremental changes, compare different approaches, and accelerate the pace of research and development.\n5 Acknowledgement\nThis work was partially funded by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreement 70NANB20H019 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, Duke University, University of California Irvine, University of Virginia, West Virginia University, University of Pennsylvania, Swarthmore College and University of Nebraska, Lincoln.\nWe greatly appreciate the constructive feedback from the two anonymous reviewers.\nSpecial thanks also to all the developers and open-source contributors of R, knitr (Xie 2014, 2015), rticles (Allaire et al. 2021), and the tidyverse (Wickham et al. 2019), without whom this project would not have been possible.\n6 Computational details\n\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] patchwork_1.1.2 rgl_1.0.1       x3ptools_0.0.3  forcats_1.0.0  \n [5] stringr_1.5.0   dplyr_1.1.0     purrr_1.0.1     readr_2.1.3    \n [9] tidyr_1.3.0     tibble_3.1.8    ggplot2_3.4.0   tidyverse_1.3.2\n[13] cmcR_0.1.11    \n\nloaded via a namespace (and not attached):\n  [1] fs_1.6.1            lubridate_1.9.1     httr_1.4.4         \n  [4] hunspell_3.0.2      tools_4.2.2         backports_1.4.1    \n  [7] bslib_0.4.2         utf8_1.2.3          R6_2.5.1           \n [10] DBI_1.1.3           colorspace_2.1-0    readbitmap_0.1.5   \n [13] withr_2.5.0         gridExtra_2.3       tidyselect_1.2.0   \n [16] downlit_0.4.2       compiler_4.2.2      extrafontdb_1.0    \n [19] textshaping_0.3.6   quantreg_5.94       cli_3.6.0          \n [22] rvest_1.0.3         SparseM_1.81        xml2_1.3.3         \n [25] labeling_0.4.2      sass_0.4.5          scales_1.2.1       \n [28] systemfonts_1.0.4   tiff_0.1-11         digest_0.6.31      \n [31] yulab.utils_0.0.6   rmarkdown_2.20      jpeg_0.1-10        \n [34] base64enc_0.1-3     pkgconfig_2.0.3     htmltools_0.5.4    \n [37] extrafont_0.19      dbplyr_2.3.0        fastmap_1.1.0      \n [40] highr_0.10          htmlwidgets_1.6.1   rlang_1.0.6        \n [43] readxl_1.4.2        rstudioapi_0.14     farver_2.1.1       \n [46] gridGraphics_0.5-1  jquerylib_0.1.4     generics_0.1.3     \n [49] zoo_1.8-11          jsonlite_1.8.4      distill_1.5.2      \n [52] googlesheets4_1.0.1 magrittr_2.0.3      ggplotify_0.1.0    \n [55] Matrix_1.5-3        Rcpp_1.0.10         munsell_0.5.0      \n [58] fansi_1.0.4         ggnewscale_0.4.8    lifecycle_1.0.3    \n [61] stringi_1.7.12      yaml_2.3.7          MASS_7.3-58.2      \n [64] grid_4.2.2          crayon_1.5.2        lattice_0.20-45    \n [67] cowplot_1.1.1       splines_4.2.2       haven_2.5.1        \n [70] hms_1.1.2           magick_2.7.3        knitr_1.42         \n [73] pillar_1.8.1        igraph_1.3.5        imager_0.42.18     \n [76] codetools_0.2-19    reprex_2.0.2        glue_1.6.2         \n [79] evaluate_0.20       bmp_0.3             rjtools_1.0.9.9001 \n [82] BiocManager_1.30.19 modelr_0.1.10       png_0.1-8          \n [85] vctrs_0.5.2         tzdb_0.3.0          yesno_0.1.2        \n [88] MatrixModels_0.5-1  Rttf2pt1_1.3.12     cellranger_1.1.0   \n [91] gtable_0.3.1        assertthat_0.2.1    cachem_1.0.6       \n [94] xfun_0.37           cranlogs_2.1.1      broom_1.0.3        \n [97] pracma_2.4.2        ragg_1.2.5          survival_3.5-0     \n[100] googledrive_2.0.0   gargle_1.3.0        memoise_2.0.1      \n[103] timechange_0.2.0    ellipsis_0.3.2     \n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-014.zip\nCRAN packages used\ncmcR, x3ptools\nCRAN Task Views implied by cited packages\n\n\nJ. Allaire, Y. Xie, R Foundation, H. Wickham, Journal of Statistical Software, R. Vaidyanathan, Association for Computing Machinery, C. Boettiger, Elsevier, K. Broman, et al. Rticles: Article formats for r markdown. 2021. URL https://CRAN.R-project.org/package=rticles. R package version 0.18.\n\n\nS. Barthelme. Imager: Image processing library based on ’CImg’. 2019. URL https://CRAN.R-project.org/package=imager. R package version 0.41.2.\n\n\nE. O. Brigham. The fast fourier transform and its applications. USA: Prentice-Hall, Inc., 1988.\n\n\nS. Brinkman and H. Bodschwinna. Advanced Gaussian filters. In Advanced techniques for assessment surface topography: Development of a basis for 3D surface texture standards \"SURFSTAND\", Eds L. Blunt and X. Jiang 2003. United States: Elsevier Inc. URL https://doi.org/10.1016/B978-1-903996-11-9.X5000-2.\n\n\nL. G. Brown. A Survey of Image Registration Techniques. ACM Computing Surveys, 24: 325–376, 1992. URL https://dl.acm.org/doi/10.1145/146370.146374.\n\n\nZ. Chen, J. Song, W. Chu, J. A. Soons and X. Zhao. A convergence algorithm for correlation of breech face images based on the congruent matching cells (CMC) method. Forensic Science International, 280: 213–223, 2017. URL https://doi.org/10.1016/j.forsciint.2017.08.033.\n\n\nW. Chu, M. Tong and J. Song. Validation Tests for the Congruent Matching Cells (CMC) Method Using Cartridge Cases Fired with Consecutively Manufactured Pistol Slides. Journal of the Association of Firearms and Toolmarks Examiners, 45(4): 6, 2013. URL https://www.nist.gov/publications/validation-tests-congruent-matching-cells-cmc-method-using-cartridge-cases-fired.\n\n\nA. Crawford. Bayesian hierarchical modeling for the forensic evaluation of handwritten documents. 2020. URL https://doi.org/10.31274/etd-20200624-257.\n\n\nJ. M. Curran, T. N. H. Champod and J. S. Buckleton, eds. Forensic interpretation of glass evidence. Boca Raton, FL: CRC Press, 2000.\n\n\nD. R. Desai and J. A. Kroll. Trust but Verify: A Guide to Algorithms and the Law. Harvard Journal of Law & Technology (Harvard JOLT), 31(1): 1–64, 2017. URL https://ssrn.com/abstract=2959472.\n\n\nT. Fadul, G. Hernandez, S. Stoiloff and G. Sneh. An Empirical Study to Improve the Scientific Foundation of Forensic Firearm and Tool Mark Identification Utilizing 10 Consecutively Manufactured Slides. 50, 2011. URL https://www.ojp.gov/ncjrs/virtual-library/abstracts/empirical-study-improve-scientific-foundation-forensic-firearm-and.\n\n\nGeometrical product specifications (GPS) — Surface texture: Areal — Part 72: XML file format x3p. Geneva, CH: International Organization for Standardization. 2017. URL https://www.iso.org/standard/62310.html.\n\n\nS. N. Goodman, D. Fanelli and J. P. A. Ioannidis. What does research reproducibility mean? Science Translational Medicine, 8(341): 341ps12–341ps12, 2016. URL https://doi.org/10.1126/scitranslmed.aaf5027.\n\n\nR. Goor, D. Hoffman and G. Riley. Novel method for accurately assessing pull-up artifacts in STR analysis. Forensic Science International: Genetics, 51: 102410, 2020. DOI 10.1016/j.fsigen.2020.102410.\n\n\nE. Hare, H. Hofmann and A. Carriquiry. Automatic Matching of Bullet Land Impressions. The Annals of Applied Statistics, 11(4): 2332–2356, 2017. URL http://arxiv.org/abs/1601.05788.\n\n\nW. H. Hesselink, A. Meijster and C. Bron. Concurrent determination of connected components. Science of Computer Programming, 41(2): 173–194, 2001. URL https://doi.org/10.1016/S0167-6423(01)00007-7.\n\n\nH. Hofmann, S. Vanderplas, G. Krishnan and E. Hare. x3ptools: Tools for Working with 3D Surface Measurements. 2020. URL https://cran.r-project.org/web/packages/x3ptools/index.html. R package version 0.0.3.\n\n\nK. Kwong. The Algorithm Says You Did It: The Use of Black Box Algorithms to Analyze Complex DNA Evidence Notes. Harvard Journal of Law & Technology (Harvard JOLT), 31(1): 275–302, 2017. URL https://jolt.law.harvard.edu/assets/articlePDFs/v31/31HarvJLTech275.pdf.\n\n\nJ. T. Leek and L. R. Jager. Is Most Published Research Really False? Annual Review of Statistics and Its Application, 4(1): 109–122, 2017. URL https://doi.org/10.1146/annurev-statistics-060116-054104.\n\n\nNational Academy of Sciences, Engineering, and Medicine. Reproducibility and replicability in science. National Academies Press, 2019. URL https://doi.org/10.17226/25303.\n\n\nNational Research Council. Strengthening Forensic Science in the United States: A Path Forward. Washington, DC: The National Academies Press, 2009. URL https://doi.org/10.17226/12589.\n\n\nD. Ott, R. Thompson and J. Song. Applying 3D measurements and computer matching algorithms to two firearm examination proficiency tests. Forensic Science International, 271: 98–106, 2017. URL https://doi.org/10.1016/j.forsciint.2016.12.014.\n\n\nS. Park and A. Carriquiry. An algorithm to compare two-dimensional footwear outsole images using maximum cliques and speeded-up robust feature. Statistical Analysis and Data Mining: The ASA Data Science Journal, 13(2): 188–199, 2020. URL https://doi.org/10.1002/sam.11449.\n\n\nS. Park and S. Tyner. Evaluation and comparison of methods for forensic glass source conclusions. Forensic Science International, 305: 110003, 2019. URL https://doi.org/10.1016/j.forsciint.2019.110003.\n\n\nR. D. Peng. Reproducible research and Biostatistics. Biostatistics, 10(3): 405–408, 2009. URL https://doi.org/10.1093/biostatistics/kxp014.\n\n\nR. D. Peng. Reproducible Research in Computational Science. Science, 334(6060): 1226–1227, 2011. URL https://www.jstor.org/stable/41352177. Publisher: American Association for the Advancement of Science.\n\n\nPresident’s Council of Advisors on Sci. & Tech. Forensic science in criminal courts: Ensuring scientific validity of feature-comparison methods. 2016. URL https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/PCAST/pcast_forensic_science_report_final.pdf.\n\n\nJ. Song. Proposed “NIST Ballistics Identification System (NBIS)” Based on 3D Topography Measurements on Correlation Cells. American Firearm and Tool Mark Examiners Journal, 45(2): 11, 2013. URL https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=910868.\n\n\nJ. Song, W. Chu, M. Tong and J. Soons. 3D topography measurements on correlation cells—a new approach to forensic ballistics identifications. Measurement Science and Technology, 25(6): 064005, 2014. URL https://doi.org/10.1088/0957-0233/25/6/064005.\n\n\nJ. Song, T. V. Vorburger, W. Chu, J. Yen, J. A. Soons, D. B. Ott and N. F. Zhang. Estimating error rates for firearm evidence identifications in forensic science. Forensic Science International, 284: 15–32, 2018. URL https://doi.org/10.1016/j.forsciint.2017.12.013.\n\n\nV. Stodden, P. Guo and Z. Ma. Toward Reproducible Computational Research: An Empirical Analysis of Data and Code Policy Adoption by Journals. PLoS ONE, 8(6): e67111, 2013. URL https://doi.org/10.1371/journal.pone.0067111.\n\n\nX. H. Tai. cartridges3D: Algorithm to compare cartridge case images. 2021. URL https://github.com/xhtai/cartridges3D. R package version 0.0.0.9000.\n\n\nX. H. Tai and W. F. Eddy. A Fully Automatic Method for Comparing Cartridge Case Images,. Journal of Forensic Sciences, 63(2): 440–448, 2018. URL http://doi.wiley.com/10.1111/1556-4029.13577.\n\n\nThe Linux Foundation. Using open source software to speed up development and gain business advantage. 2017. URL https://www.linuxfoundation.org/blog/using-open-source-software-to-speed-development-and-gain-business-advantage/.\n\n\nR. Thompson. Firearm identification in the forensic science laboratory. National District Attorneys Association, 2017. URL https://doi.org/10.13140/RG.2.2.16250.59846.\n\n\nM. Tong, J. Song and W. Chu. An Improved Algorithm of Congruent Matching Cells (CMC) Method for Firearm Evidence Identifications. Journal of Research of the National Institute of Standards and Technology, 120: 102, 2015. URL https://doi.org/10.6028/jres.120.008.\n\n\nM. Tong, J. Song, W. Chu and R. M. Thompson. Fired Cartridge Case Identification Using Optical Images and the Congruent Matching Cells (CMC) Method. Journal of Research of the National Institute of Standards and Technology, 119: 575, 2014. URL https://doi.org/10.6028/jres.119.023.\n\n\nT. Tvedebrink, M. M. Andersen and J. M. Curran. DNAtools: Tools for analysing forensic genetic DNA data. Journal of Open Source Software, 5(45): 1981, 2020. URL https://doi.org/10.21105/joss.01981.\n\n\nS. Tyner, Soyoung Park, G. Krishnan, K. Pan, E. Hare, A. Luby, X. H. Tai, H. Hofmann and G. Basulto-Elias. Sctyner/OpenForSciR: Create DOI for open forensic science in r. 2019. URL https://zenodo.org/record/3418141.\n\n\nS. Vanderplas, M. Nally, T. Klep, C. Cadevall and H. Hofmann. Comparison of three similarity scores for bullet LEA matching. Forensic Science International, 2020. URL http://www.sciencedirect.com/science/article/pii/S0379073820300293 [online; last accessed February 10, 2020].\n\n\nT. J. Weller, A. Zheng, R. Thompson and F. Tulleners. Confocal microscopy analysis of breech face marks on fired cartridge cases from 10 consecutively manufactured pistol slides. 57(4): 2012. URL https://doi.org/10.1111/j.1556-4029.2012.02072.x.\n\n\nH. Wickham, M. Averick, J. Bryan, W. Chang, L. D. McGowan, R. François, G. Grolemund, A. Hayes, L. Henry, J. Hester, et al. Welcome to the tidyverse. Journal of Open Source Software, 4(43): 1686, 2019. DOI 10.21105/joss.01686.\n\n\nY. Xie. Dynamic documents with R and knitr. 2nd ed Boca Raton, Florida: Chapman; Hall/CRC, 2015. URL https://yihui.org/knitr/. ISBN 978-1498716963.\n\n\nY. Xie. Knitr: A comprehensive tool for reproducible research in R. In Implementing reproducible computational research, Eds V. Stodden, F. Leisch and R. D. Peng 2014. Chapman; Hall/CRC. URL http://www.crcpress.com/product/isbn/9781466561595. ISBN 978-1466561595.\n\n\nJ. Zemmels, H. Hofmann and S. Vanderplas. Zemmels et al. (2022) Cartridge Case Scans. 2022. URL https://iastate.figshare.com/articles/dataset/Zemmels_et_al_2022_Cartridge_Case_Scans/19686297/1.\n\n\nX. A. Zheng, J. A. Soons and R. M. Thompson. NIST Ballistics Toolmark Research Database. 2016. URL https://tsapps.nist.gov/NRBTD/.\n\n\n\n\n",
    "preview": "articles/RJ-2023-014/distill-preview.png",
    "last_modified": "2023-11-07T21:31:40+00:00",
    "input_file": {},
    "preview_width": 896,
    "preview_height": 496
  },
  {
    "path": "articles/RJ-2023-015/",
    "title": "Bootstrapping Clustered Data in R using lmeresampler",
    "description": "Linear mixed-effects models are commonly used to analyze clustered data structures. There are numerous packages to fit these models in R and conduct likelihood-based inference. The implementation of resampling-based procedures for inference are more  limited. In this paper, we introduce the lmeresampler package for bootstrapping nested linear mixed-effects models fit via lme4 or nlme. Bootstrap  estimation allows for bias correction, adjusted standard errors and confidence  intervals for small samples sizes and when distributional assumptions break down.  We will also illustrate how bootstrap resampling can be used to diagnose this model  class. In addition, lmeresampler makes it easy to construct interval estimates  of functions of model parameters.",
    "author": [
      {
        "name": "Adam Loy",
        "url": "https://aloy.rbind.io/"
      },
      {
        "name": "Jenna Korobova",
        "url": {}
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\nIntroduction\nClustered data structures occur in a wide range of studies. For example, students are organized within classrooms, schools, and districts, imposing a correlation structure that must be accounted for in the modeling process. Similarly, cholesterol measurements could be tracked across time for a number of subjects, resulting in measurements being grouped by subject. Other names for clustered data structures include grouped, nested, multilevel, hierarchical, longitudinal, repeated measurements, and blocked data. The covariance structure imposed by clustered data is commonly modeled using linear mixed-effects (LME) models, also referred to as hierarchical linear or multilevel linear models (Pinheiro and Bates 2000; Raudenbush and Bryk 2002; Goldstein 2011).\nIn this paper, we restrict attention to the Gaussian response LME model for clustered data structures. For cluster\n\\(i=1, \\ldots, g\\), this model is expressed as\n\\[\\begin{equation}\n    \\underset{(n_i \\times 1)}{\\boldsymbol{y}_i} = \\underset{(n_i \\times p)}{\\boldsymbol{X}_i} \\ \\underset{(p \\times 1)}{\\boldsymbol{\\beta}} + \\underset{(n_i \\times q)}{\\boldsymbol{Z}_i} \\ \\underset{(q \\times 1)}{\\boldsymbol{b}_i} + \\underset{(n_i \\times 1)}{\\boldsymbol{\\varepsilon}_i},\n    \\tag{1}\n\\end{equation}\\]\nwhere\n\\[\\begin{equation}\n\\boldsymbol{\\varepsilon}_i \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^2 \\boldsymbol{I}_{n_i}),   \\boldsymbol{b}_i \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{D}),\n\\tag{2}\n\\end{equation}\\]\nwhere \\(\\boldsymbol{\\varepsilon}_i\\) and \\(\\boldsymbol{b}_i\\) are independent for all \\(i\\), \\(\\boldsymbol{\\varepsilon}_i\\) is independent of \\(\\boldsymbol{\\varepsilon}_j\\) for \\(i\\ne j\\), and \\(\\boldsymbol{b}_i\\) is independent of \\(\\boldsymbol{b}_j\\) for \\(i\\ne j\\). Here, \\(\\boldsymbol{0}\\) denotes a vector of zeros of length \\(n_i\\) (the number of observations in group \\(i\\)), \\(\\boldsymbol{I}_{n_i}\\) denotes the \\(n_i\\)-dimensional identity matrix,\nand \\(\\boldsymbol{D}\\) is a \\(q \\times q\\) covariance matrix.\nIn R, the two most popular packages to fit LME models are nlme (Pinheiro et al. 2017) and lme4 (Bates et al. 2015). Both packages fit LME models using either maximum likelihood or restricted maximum likelihood methods. These methods rely on the distributional assumptions placed on the residual quantities, \\(\\boldsymbol{\\varepsilon}_i\\) and \\(\\boldsymbol{b}_i\\), as well as large enough sample sizes. While some aspects of LME models are quite robust to model misspecification, others are more sensitive, leading to biased estimates and/or incorrect standard errors. Jacqmin-Gadda et al. (2007) found that inference for the fixed effects is robust if the distribution of the error terms, \\(\\boldsymbol{\\varepsilon}_i\\), is non-normal or heteroscedastic, but that variance components and random effects are biased if the covariance structure for the error terms is misspecified. The fixed intercept is not robust to misspecification of the random effects (Hui et al. 2021). In addition, misspecification of the random effects distribution can lead to biased estimates of the variance components and undercoverage of the confidence intervals (Hui et al. 2021). In cases where distributional assumptions are suspect, bootstrapping provides an alternative inferential approach that leads to consistent, bias-corrected parameter estimates, standard errors, and confidence intervals. Standard errors and confidence intervals for functions of model parameters are also easily calculated using a bootstrap procedure, and are available even in situations where closed-form solutions are not.\nA variety of bootstrap procedures for clustered data and the LME model have been proposed and investigated, including the cases (non-parametric) bootstrap, the residual bootstrap, the parametric bootstrap, the random effect block (REB) bootstrap, and the wild bootstrap (Morris 2002; Carpenter et al. 2003; Field and Welsh 2007; Chambers and Chandra 2013; Modugno and Giannerini 2015). Van der Leeden et al. (2008) provide a thorough overview of bootstrapping LME models. Sánchez-Espigares and Ocaña (2009) developed a full-featured bootstrapping framework for LME models in R; however, this package is not available and there appears to be no active development. Consequently, R users must pick and choose packages based on what bootstrap procedure they wish to implement. The parametric bootstrap is available in lme4 via the bootMer() function, as is the semi-parametric (residual) bootstrap proposed by Morris (2002). The simulateY() function in nlmeU (Galecki and Burzykowski 2013) makes it easy to simulate values of the response variable for lme model objects; however, the user is required to implement the remainder of the parametric bootstrap. Chambers and Chandra (2013) made R code available to implement their REB bootstrap as well as the parametric bootstrap and the residual bootstrap proposed by Carpenter et al. (2003) for LME models fix via nlme::lme(). Unfortunately, these functions were not published on CRAN or extended to models fit via lme4::lmer(). Bootstrap procedures for specific inferential tasks have also been implemented. The parametric bootstrap has been implemented to carry out likelihood ratio tests for the presence of variance components in RLRsim (Scheipl et al. 2008). pbkrtest (Halekoh and Højsgaard 2014) provides a Kenward-Roger approximation for performing F-tests using a parametric bootstrap approach for LME models and generalized LME models. rptR uses the parametric bootstrap to estimate repeatabilities for LME models (Stoffel et al. 2017). Finally, constrained inference for LMEs using the residual bootstrap is implemented in CLME (Farnan et al. 2014).\n\nIn this paper, we introduce the lmeresampler package which implements a suite of bootstrap methods for LME models fit using either nlme or lme4 in a single package. lmeresampler provides a unified bootstrap() command to reduce cognitive load on the user and also provides access to procedures that were not previously available on CRAN. In the next section, we will clarify some notation and terminology for LME models. In Bootstrap procedures for clustered data, we provide an overview of the bootstrap methods implemented in lmeresampler. We then give an Overview of lmeresampler, discuss a variety of Example applications for LME models, and show how to Bootstrapping in parallel works in lmeresampler. We conclude with a Summary and highlight areas for future development.\nBootstrap procedures for clustered data\nIn lmeresampler, we implement five bootstrap procedures for clustered data: a cases\n(i.e., nonparamteric) bootstrap, a residual bootstrap (i.e., semiparametric), a parametric\nbootstrap, a wild bootstrap, and a block bootstrap. In this section,\nwe provide an overview of these bootstrap approaches. Our discussion\nfocuses on two-level models, but procedures generalize to\nhigher-level models unless otherwise noted.\nThe cases bootstrap\nThe cases bootstrap is a fully non-parametric bootstrap that resamples\nthe clusters in the data set to generate bootstrap\nresamples. Depending on the nature of the data, this resampling can be\ndone only for the top-level cluster, only at the observation-level\nwithin a cluster, or at both levels. The choice of the exact resampling\nscheme should be dictated by the way the data were generated, since the\ncases bootstrap generates new data by mimicking this process.\nVan der Leeden et al. (2008) provide a cogent explanation of how to select a\nresampling scheme. To help ground the idea of resampling, consider a two-level\nhierarchical data set where students are organized into schools.\nOne version of the cases bootstrap is implemented by only\nresampling the clusters. This version of the bootstrap is what Field and Welsh (2007) term the\ncluster bootstrap and Goldstein (2011) term the non-parametric bootstrap. We would choose this resampling scheme, for example, if schools were chosen at random and then all students within each school were observed. In this case, the bootstrap proceeds as follows:\nDraw a random sample, with replacement, of size \\(g\\) from the\nclusters.\nFor each selected cluster, \\(k\\), extract all of the cases to form the\nbootstrap sample \\(\\left(\\boldsymbol{y}^*_k, \\boldsymbol{X}^*_k, \\boldsymbol{Z}^*_k \\right)\\).\nSince entire clusters are sampled, the total sample size may differ from the that of the original data set.\nRefit the model to the bootstrap sample and extract the parameter\nestimates of interest.\nRepeat steps 1–3 \\(B\\) times.\nAn alternative version of the cases bootstrap only resamples the\nobservations within clusters, which makes sense in our example\nif the schools were fixed and students were randomly sampled within schools.\nFor each cluster \\(i=1,\\ldots,g\\), draw a random sample of the rows for cluster \\(i\\), with\nreplacement, to form the bootstrap sample\n\\(\\left(\\boldsymbol{y}^*_i, \\boldsymbol{X}^*_i, \\boldsymbol{Z}^*_i \\right)\\).\nRefit the model to the bootstrap sample and extract the parameter\nestimates of interest.\nRepeat steps 1–2 \\(B\\) times.\nA third version of the cases bootstrap resamples both clusters and cases\nwithin clusters. This is what Field and Welsh (2007) term the two-state bootstrap.\nWe would choose this resampling scheme if both schools and\nstudents were sampled during the data collection process.\nAll three versions of the case bootstrap are implemented in lmeresampler. We explain how the resampling is specified in our Overview of lmeresampler. Regardless of which version of the cases bootstrap you choose, it requires the weakest conditions: it only requires that the hierarchical structure in the data set is correctly specified.\nThe parametric bootstrap\nThe parametric bootstrap simulates random effects and error terms from the fitted\ndistributions to form bootstrap resamples. Consequently, it requires the strongest conditions—that is, the parametric bootstrap assumes that the model, as specified by Equations (1) and (2), is correctly specified.\nLet \\(\\widehat{\\boldsymbol \\beta}\\), \\(\\widehat{\\boldsymbol D}\\), and \\(\\widehat{\\sigma}^2\\) be maximum likelihood or restricted maximum likelihood estimates of the fixed effects and variance components from the fitted model. The parametric bootstrap is then implemented through the following steps:\nSimulate \\(g\\) error term vectors, \\(\\boldsymbol{e}_i^*\\), of length \\(n_i\\) from \\(\\mathcal{N}\\left(\\boldsymbol{0},\\widehat{\\sigma}^2 \\boldsymbol{I}_{n_i} \\right)\\).\nSimulate \\(g\\) random effects vectors, \\(\\boldsymbol{b}_i^*\\), from \\(\\mathcal{N}\\left(\\boldsymbol{0},\\widehat{\\boldsymbol{D}} \\right)\\).\nGenerate bootstrap responses \\(\\boldsymbol{y}^*_i = \\boldsymbol{X}_i \\widehat{\\boldsymbol{\\beta}} + \\boldsymbol{Z}_i \\boldsymbol{b}_i^* + \\boldsymbol{e}_i^*\\).\nRefit the model to the bootstrap responses and extract the parameter estimates of interest.\nRepeat steps 2–4 \\(B\\) times.\nThe residual bootstrap\nThe residual bootstrap resamples the residual quantities from the fitted\nLME model in order to generate bootstrap resamples.\nThere are three general types of residuals for LME models (Haslett and Haslett 2007; Singer et al. 2017). correspond to the errors made using the marginal predictions, \\(\\widehat{\\boldsymbol r}_i = \\boldsymbol{y_i} - \\widehat{\\boldsymbol{y}}_i = \\boldsymbol{y}_i - \\boldsymbol{X}_i \\widehat{\\boldsymbol{\\beta}}\\). correspond to the errors made using predictions conditioned on the clusters, \\(\\widehat{\\boldsymbol{e}}_i = \\boldsymbol{y}_i - \\widehat{\\boldsymbol{y}}_i| \\boldsymbol{b}_i = \\boldsymbol{y}_i - \\boldsymbol{X}_i \\widehat{\\boldsymbol{\\beta}} - \\boldsymbol{Z}_i \\widehat{\\boldsymbol{b}}_i\\). The are the last type of residual quantity and are defined as \\(\\widehat{\\boldsymbol{b}}_i = \\widehat{\\boldsymbol{D}} \\boldsymbol{Z}_i^\\prime \\widehat{\\boldsymbol{V}}_i^{-1}\\left(\\boldsymbol{y}_i - \\boldsymbol{X}_i \\widehat{\\boldsymbol{\\beta}} \\right)\\) where \\(\\widehat{\\boldsymbol{V}}_i = \\boldsymbol{Z}_i \\widehat{\\boldsymbol{D}}\\boldsymbol{Z}_i^\\prime + \\widehat{\\sigma}^2 \\boldsymbol{I}_{n_i}\\).\nA naive implementation of the residual bootstrap would draw random samples, with\nreplacement, from the estimated conditional residuals\nand the best linear unbiased predictors (BLUPS);\nhowever, this will consistently underestimate the variability in the\ndata because the residuals are shrunken toward zero\n(Morris 2002; Carpenter et al. 2003). Carpenter et al. (2003) solve\nthis problem by “reflating” the residual quantities so that the empirical\ncovariance matrices match the estimated covariance matrices prior to resampling:\nFit the model and calculate the empirical BLUPs, \\(\\widehat{\\boldsymbol{b}}_i\\), and the\npredicted conditional residuals, \\(\\widehat{\\boldsymbol{e}}_i\\).\nMean center each residual quantity and reflate the centered\nresiduals. Only the process to reflate the predicted random effects is discussed below, but the process is analogous for the conditional residuals.\nArrange the random effects into a \\(g \\times q\\) matrix, where each row contains the predicted random effects from a single group. Denote this matrix as \\(\\widehat{\\boldsymbol{U}}\\). Define \\(\\boldsymbol{\\widehat{\\Gamma}} = \\boldsymbol{I}_g \\otimes \\widehat{\\boldsymbol{D}}\\), the block diagonal covariance matrix of \\(\\widehat{\\boldsymbol{U}}\\).\nCalculate the empirical covariance matrix as \\(\\boldsymbol{S} = \\widehat{\\boldsymbol{U}}^\\prime \\widehat{\\boldsymbol{U}} / g\\). We follow the approach of Carpenter et al. (2003), dividing by the number of groups, \\(g\\), rather than \\(g-1\\).\nFind a transformation of \\(\\widehat{\\boldsymbol{U}}\\), \\(\\widehat{\\boldsymbol{U}}^* = \\widehat{\\boldsymbol{U}} \\boldsymbol{A}\\), such that \\(\\widehat{\\boldsymbol{U}}^{*\\prime} \\widehat{\\boldsymbol{U}}^* / g = \\boldsymbol{\\widehat{\\Gamma}}\\). Specifically, we will find \\(\\boldsymbol{A}\\) such that \\(\\boldsymbol{A}^\\prime \\widehat{\\boldsymbol{U}}^\\prime \\widehat{\\boldsymbol{U}} \\boldsymbol{A} / g = \\boldsymbol{A}^\\prime \\boldsymbol{SA} = \\boldsymbol{\\widehat{\\Gamma}}\\). The choice of \\(\\boldsymbol{A}\\) is not unique, so we use the recommendation given by Carpenter et al. (2003): \\(\\boldsymbol{A} = \\left(\\boldsymbol{L}_D \\boldsymbol{L}_S^{-1} \\right)^\\prime\\) where \\(\\boldsymbol{L}_D\\) and \\(\\boldsymbol{L}_S\\) are the Cholesky factors of \\(\\boldsymbol{\\widehat{\\Gamma}}\\) and \\(\\boldsymbol{S}\\), respectively.\n\nDraw a random sample, with replacement, from the set \\(\\lbrace \\boldsymbol{u}_i^* \\rbrace\\) of size \\(g\\), where \\(\\boldsymbol{u}_i^*\\) is the \\(i\\)th row of the centered and reflated random effects matrix, \\(\\widehat{\\boldsymbol{U}}^*\\).\nDraw \\(g\\) random samples, with replacement, of sizes \\(n_i\\) from the set of the centered and reflated conditional residuals, \\(\\lbrace \\boldsymbol{e}_i^* \\rbrace\\).\nGenerate the bootstrap responses, \\(\\boldsymbol{y}^*_i\\), using the fitted model equation: \\(\\boldsymbol{y}^*_i = \\boldsymbol{X}_i \\widehat{\\boldsymbol{\\beta}} + \\boldsymbol{Z}_i \\widehat{\\boldsymbol{u}}^*_i + \\boldsymbol{e}_i^*\\).\nRefit the model to the bootstrap responses and extract the parameter\nestimates of interest.\nRepeat steps 3–6 B times.\nNotice that the residual bootstrap is a semiparametric bootstrap, since it depends on the model structure (both the mean function and the covariance structure) but not the distributional conditions (Morris 2002).\nThe random-effects block bootstrap\nAnother semiparametric bootstrap is the random effect block (REB) bootstrap proposed by Chambers and Chandra (2013). The REB bootstrap can be viewed as a version of the residual bootstrap where conditional residuals are resampled from within clusters (i.e., blocks) to allow for weaker assumptions on the covariance structure of the residuals. The residual bootstrap requires that the conditional residuals are independent and identically distributed, whereas the REB bootstrap relaxes this to only require that the covariance structure of the error terms is similar across clusters. In addition, the REB bootstrap utilizes the marginal residuals to calculate non-parametric predicted random effects rather than relying on the model-based empirical best linear unbiased predictors (EBLUPS). Chambers and Chandra (2013) developed three versions of the REB bootstrap, all of which have been implemented in lmeresampler. We refer the reader to Chambers and Chandra (2013) for a discussion of when each should be used. It’s important to note that at the time of this writing, that the REB bootstrap has only been explored and implemented for use with two-level models.\nREB/0\nThe base algorithm for the REB bootstrap (also known as REB/0) is as follows:\nCalculate non-parametric residual quantities for the model.\nCalculate the marginal residuals for each group, \\(\\widehat{\\boldsymbol{r}}_i = \\boldsymbol{y}_i - \\boldsymbol{X}_i \\widehat{\\boldsymbol{\\beta}}\\).\nCalculate the non-parametric predicted random effects, \\(\\tilde{\\boldsymbol{b}}_i = \\left( \\boldsymbol{Z}_i^\\prime \\boldsymbol{Z}_i \\right)^{-1} \\boldsymbol{Z}^\\prime_i \\boldsymbol{r}_i\\).\nCalculate the non-parametric conditional residuals using the residuals quantities obtained in the previous two steps, \\(\\boldsymbol{\\tilde{e}}_i = \\widehat{\\boldsymbol{r}}_i - \\boldsymbol{Z}_i \\boldsymbol{\\tilde{b}}_i\\).\n\nTake a random sample, with replacement, of size \\(g\\) from the set \\(\\lbrace \\tilde{\\boldsymbol{b}}_i \\rbrace\\). Denote these resampled random effects as \\(\\boldsymbol{\\tilde{b}}^*_i\\).\nTake a random sample, with replacement, of size \\(g\\) from the cluster ids. For each sampled cluster, draw a random sample, with replacement, of size \\(n_i\\) from that cluster’s vector of error terms, \\(\\boldsymbol{\\tilde{e}}_i\\).\nGenerate bootstrap responses, \\(\\boldsymbol{y}^*_i\\), using the fitted model equation: \\(\\boldsymbol{y}^*_i = \\boldsymbol{X}_i \\widehat{\\beta} + \\boldsymbol{Z}_i \\boldsymbol{\\tilde{b}}^*_i + \\boldsymbol{\\tilde{e}}^*_i\\).\nRefit the model to the bootstrap sample and extract the parameter estimates of interest.\nRepeat steps 2–5 \\(B\\) times.\nREB/1\nThe first variation of the REB bootstrap (REB/1) zero centers and reflates the residual quantities prior to resampling in order to satisfy the conditions for consistency (Shao and Tu 1995). This is the same process outlined in Step 2 of the residual bootstrap outlined above.\nREB/2\nThe second variation of the REB bootstrap (REB/2 or postscaled REB) addresses two issues: potential non-zero covariances in the joint bootstrap distribution of the variance components and potential bias in the parameter estimates. After the REB/0 algorithm is run, the following post processing is performed:\nUncorrelate the variance components.\nTo uncorrelate the bootstrap estimates of the variance components produced by REB/0, Chambers and Chandra (2013) propose the following procedure:\nApply natural logarithms to the bootstrap distribution of each variance component and form the following matrices. Note that we use \\(\\nu\\) to denote the total number of variance components.\n\\(\\boldsymbol{S}^*\\): a \\(B \\times \\nu\\) matrix formed by binding the columns of these distributions together\n\\(\\boldsymbol{M}^*\\): a \\(B \\times \\nu\\) matrix where each column contains the column mean from the corresponding column in \\(\\boldsymbol{S}^*\\)\n\\(\\boldsymbol{D}^*\\): a \\(B \\times \\nu\\) matrix where each column contains the column standard deviation from the corresponding column in \\(\\boldsymbol{S}^*\\)\n\nCalculate \\(\\boldsymbol{C}^*=\\mathrm{cov} \\left( \\boldsymbol{S}^* \\right)\\).\nCalculate \\(\\boldsymbol{L}^* = \\boldsymbol{M}^* + \\lbrace \\left(\\boldsymbol{S}^* - \\boldsymbol{M}^*\\right)\\boldsymbol{C}^{*-1/2} \\rbrace \\circ \\boldsymbol{D}^*\\), where \\(\\circ\\) denotes the Hadamard (elementwise) product.\nExponentiate the elements of \\(\\boldsymbol{L}^*\\). The columns of \\(\\boldsymbol{L}^*\\) are then uncorrelated versions of the bootstrap variance components.\nCenter the bootstrap estimates at the original estimate values.\nTo correct bias in the estimation of the fixed effects, apply a mean correction. For each parameter estimate, \\(\\widehat{\\beta}_k\\), adjust the bootstrapped estimates, \\(\\widehat{\\boldsymbol{\\beta}}_k^*\\) as follows: \\(\\widetilde{\\boldsymbol{\\beta}}_k^{*} = \\widehat{\\beta}_k \\boldsymbol{1}_{B} + \\widehat{\\boldsymbol{\\beta}}_k^* -{\\rm avg} \\left( \\widehat{ \\boldsymbol{\\beta} }_k^* \\right)\\).\nTo correct bias in the estimation of the variance components, apply a ratio correction. For each estimated variance component , \\(\\widehat{\\sigma}^2_v\\), adjust the uncorrelated bootstrapped estimates, \\(\\widehat{\\boldsymbol{\\sigma}}_v^{2*}\\) as follows: \\(\\widetilde{\\boldsymbol{\\sigma}}_{v}^{2*} = \\widehat{\\boldsymbol{\\sigma}}_v^{2*} \\circ \\lbrace \\widehat{\\sigma}^2_v / {\\rm avg} \\left( \\widehat{\\boldsymbol{\\sigma}}_v^{2*} \\right) \\rbrace\\)\nThe wild bootstrap\nThe wild bootstrap also relaxes the assumptions made on the error terms of the model, allowing heteroscedasticity both within and across groups. The wild bootstrap is well developed for the ordinary regression model (Liu 1988; Flachaire 2005; Davidson and Flachaire 2008) and Modugno and Giannerini (2015) adapt it for the nested LME model.\nTo begin, we can re-express model (1) as\n\\[\\begin{equation}\n\\boldsymbol{y}_i = \\boldsymbol{X}_i \\boldsymbol{\\beta} + \\boldsymbol{v}_i, \\text{ where } \\boldsymbol{v}_i = \\boldsymbol{Z}_i \\boldsymbol{b}_i + \\boldsymbol{\\varepsilon}_i.\n\\tag{3}\n\\end{equation}\\]\nThe wild bootstrap proceeds as follows:\nDraw a random sample, \\(w_1, w_2, \\ldots, w_g\\), from an auxiliary distribution with mean zero and unit variance.\nGenerate bootstrap responses using the re-expressed model equation (3): \\(\\boldsymbol{y}^*_i = \\boldsymbol{X}_i \\widehat{\\boldsymbol{\\beta}} + \\tilde{\\boldsymbol{v}}_i w_j\\), where \\(\\tilde{\\boldsymbol{v}}_i\\) is a heteroscedasticity consistent covariance matrix estimator. Modugno and Giannerini (2015) suggest using what Flachaire (2005) calls \\({{\\rm HC}_2}\\) or \\({{\\rm HC}_3}\\) in the regression context:\n\\[\\begin{align}\n{{\\rm HC}_2}&: \\tilde{\\boldsymbol{v}}_i = {\\rm diag} \\left( \\boldsymbol{I}_{n_i} - \\boldsymbol{H}_i \\right)^{-1/2} \\circ \\boldsymbol{r}_i\\\\\n{{\\rm HC}_3}&: \\tilde{\\boldsymbol{v}}_i = {\\rm diag} \\left( \\boldsymbol{I}_{n_i} - \\boldsymbol{H}_i \\right) \\circ \\boldsymbol{r}_i,\n\\end{align}\\]\nwhere \\(\\boldsymbol{H}_i = \\boldsymbol{X}_i \\left(\\boldsymbol{X}_i^\\prime \\boldsymbol{X}_i \\right)^\\prime \\boldsymbol{X}_i^\\prime\\), the \\(i\\)th diagonal block of the orthogonal projection matrix, and \\(\\boldsymbol{r}_i\\) is the vector of marginal residuals for group \\(i\\).\nRefit the model to the bootstrap sample and extract the parameter estimates of interest.\nRepeat steps 1–3 \\(B\\) times.\nFive options for the auxiliary distribution are implemented in lmeresampler:\nThe two-point distribution proposed by Mammen (1993) takes value \\(w_i=-(\\sqrt{5} - 1)/2\\) with probability \\(p=(\\sqrt{5}+1) / (2 \\sqrt{5})\\) and \\(w_i = (\\sqrt{5} + 1)/2\\) with probability \\(1-p\\).\nThe two-point Rademacher distribution places equal probability on \\(w_i = \\pm 1\\).\nThe six-point distribution proposed by Webb (2013) places equal probability on \\(w_i = \\pm \\sqrt{1/2}, \\pm1, \\pm \\sqrt{3/2}\\).\nThe standard normal distribution.\nThe gamma distribution with shape parameter 4 and scale parameter \\(1/2\\) (Liu 1988) that is centered to have mean zero. \\(w_i^*\\) are randomly sampled from the gamma distribution, and then centered via \\(w_i = w_i^* - 2\\).\nModugno and Giannerini (2015) found the Mammen distribution to be preferred based on a Monte Carlo study, but only compared it to the Rademacher distribution.\nOverview of lmeresampler\nThe lmeresampler package implements the five bootstrap procedures outlined in the previous section for Gaussian response\nLME models for clustered data structures fit using either nlme (Pinheiro et al. 2017) or lme4. The workhorse function in lmeresampler is\n\n\nbootstrap(model, .f, type, B, resample = NULL, reb_type = NULL, hccme, aux.dist,\n          orig_data, .refit)\n\n\nThe four required parameters to are:\nmodel, an lme or merMod fitted model object.\n.f, a function defining the parameter(s) of interest that should be extracted/calculated for each bootstrap iteration.\ntype, a character string specifying the type of bootstrap to run. Possible values include: \"parametric\", \"residual\", \"reb\", \"wild\", and \"case\".\nB, the number of bootstrap resamples to generate.\n.refit, whether the model should be refit to the bootstrap sample. This defaults to TRUE.\nThere are also five optional parameters: resample, reb_type, hccme, aux.dist, and orig_data.\nIf the user sets type = \"case\", then they must also set resample to specify what cluster resampling scheme to use. resample requires a logical vector of length equal to the number of levels in the model. A value of TRUE in the \\(i\\)th position indicates that cases/clusters at that level should be resampled. For example, to only resample the clusters (i.e., level 2 units) in a two-level model, the user would specify resample = c(FALSE, TRUE).\nIf the user sets type = \"reb\", then they must also set reb_type to indicate which version of the REB bootstrap to run. reb_type accepts the integers 0, 1, and 2 to indicate REB/0, REB/1, and REB/2, respectively.\nIf the user sets type = \"wild\", then they must specify both hccme and aux.dist. Currently, hccme can be set to \"hc2\" or \"hc3\" and aux.dist can be set to \"mammen\", \"rademacher\", \"norm\", \"webb\", or \"gamma\".\nIf variables are transformed within the model formula and lmer() is used to fit the LME model, then orig_data should be set to the original data frame, since this cannot be recovered from the fitted model object.\nThe bootstrap() function is a generic function that calls functions for each type of bootstrap. The user can call the specific bootstrap function directly if preferred. An overview of the specific bootstrap functions is given in Table 1.\n\n\nTable 1: Summary of the specific bootstrap functions called by bootstrap() and their required arguments.\n\n\nBootstrap\n\n\nFunction name\n\n\nRequired arguments\n\n\nCases\n\n\ncase_bootstrap\n\n\nmodel, .f, type, B, resample, orig_data, .refit\n\n\nResidual\n\n\nresid_bootstrap\n\n\nmodel, .f, type, B, orig_data, .refit\n\n\nREB\n\n\nreb_bootstrap\n\n\nmodel, .f, type, B, reb_type, orig_data, .refit\n\n\nWild\n\n\nwild_bootstrap\n\n\nmodel, .f, type, B, hccme, aux.dist, orig_data, .refit\n\n\nParametric\n\n\nparametric_bootstrap\n\n\nmodel, .f, type, B, orig_data, .refit\n\n\nEach of the specific bootstrap functions performs four general steps:\nSetup. Key information (parameter estimates, design matrices, etc.) is extracted from the fitted model to eliminate repetitive actions during the resampling process.\nResampling. The setup information is passed to an internal resampler function to generate the B bootstrap samples.\nRefitting. The model is refit for each of the bootstrap samples and the specified parameters are extracted/calculated.\nClean up. An internal completion function formats the original and bootstrapped quantities to return a list to the user.\nEach function returns an object of class lmeresamp, which is a list with elements outlined in Table 2. print(), summary(), plot(), and confint() methods are available for lmeresamp objects.\n\n\nTable 2: Summary of the specific bootstrap functions called by bootstrap() and their required arguments.\n\n\nElement\n\n\nDescription\n\n\nobserved\n\n\nvalues for the original model parameter estimates.\n\n\nmodel\n\n\nthe original fitted model object.\n\n\n.f\n\n\nthe function call defining the parameters of interest.\n\n\nreplicates\n\n\na \\(B \\times p\\) tibble containing the bootstrapped quantities. Each column contains a single bootstrap distribution.\n\n\nstats\n\n\na tibble containing the observed, rep.mean (bootstrap mean), se (bootstrap standard error), and bias values for each parameter.\n\n\nB\n\n\nthe number of bootstrap resamples performed.\n\n\ndata\n\n\nthe original data set.\n\n\nseed\n\n\na vector of randomly generated seeds that are used by the bootstrap.\n\n\ntype\n\n\na character string specifying the type of bootstrap performed.\n\n\ncall\n\n\nthe user’s call to the bootstrap function.\n\n\nmessage\n\n\na list of length B giving any messages generated during refitting. An entry will be NULL if no message was generated.\n\n\nwarning\n\n\na list of length B giving any warnings generated during refitting. An entry will be NULL if no warning was generated.\n\n\nerror\n\n\na list of length B giving any errors encountered during refitting. An entry will be NULL if no error was encountered.\n\n\nlmeresampler also provides the extract_parameters() helper function to extract the fixed effects and variance components from merMod and lme objects as a named vector.\nExample applications\nA two-level example: JSP data\nAs a first application of the bootstrap for nested LME models, consider the junior school project (JSP) data (Mortimore et al. 1988; Goldstein 2011) that is stored as jsp728 in lmeresampler. The data set is comprised of measurements taken on 728 elementary school students across 48 schools in London.\n\n\nlibrary(lmeresampler)\ntibble::as_tibble(jsp728)\n\n# A tibble: 728 × 9\n   mathA…¹ mathA…² gender class school normA…³ normA…⁴ schoo…⁵ mathA…⁶\n     <dbl>   <dbl> <fct>  <fct> <fct>    <dbl>   <dbl>   <dbl>   <dbl>\n 1      39      36 M      nonm… 1       1.80    1.55      22.4  13.6  \n 2      11      19 F      manu… 1      -2.29   -0.980     22.4  -3.42 \n 3      32      31 F      manu… 1      -0.0413  0.638     22.4   8.58 \n 4      27      23 F      nonm… 1      -0.750  -0.460     22.4   0.579\n 5      36      39 F      nonm… 1       0.743   2.15      22.4  16.6  \n 6      33      25 M      manu… 1       0.163  -0.182     22.4   2.58 \n 7      30      27 M      manu… 1      -0.372   0.0724    22.4   4.58 \n 8      17      14 M      manu… 1      -1.63   -1.52      22.4  -8.42 \n 9      33      30 M      manu… 1       0.163   0.454     22.4   7.58 \n10      20      19 M      manu… 1      -1.40   -0.980     22.4  -3.42 \n# … with 718 more rows, and abbreviated variable names ¹​mathAge11,\n#   ²​mathAge8, ³​normAge11, ⁴​normAge8, ⁵​schoolMathAge8, ⁶​mathAge8c\n\nSuppose we wish to fit a model using the math score at age 8, gender, and the father’s social class to describe math scores at age 11, including a random intercept for school (Goldstein 2011). This LME model can be fit using the lmer() function in lme4.\n\n\nlibrary(lme4)\njsp_mod <- lmer(mathAge11 ~ mathAge8 + gender + class + (1 | school), data = jsp728)\n\n\nTo implement the residual bootstrap to estimate the fixed effects, we can use the bootstrap() function and set type = \"residual\".\n\n\n(jsp_boot <- bootstrap(jsp_mod, .f = fixef, type = \"residual\", B = 2000))\n\nBootstrap type: residual \n\nNumber of resamples: 2000 \n\n            term   observed   rep.mean        se          bias\n1    (Intercept) 14.1577509 14.1697871 0.6702835  0.0120361440\n2       mathAge8  0.6388895  0.6391441 0.0248334  0.0002546297\n3        genderM -0.3571922 -0.3654058 0.3391162 -0.0082135863\n4 classnonmanual  0.7200815  0.7154916 0.3783149 -0.0045898872\n\nThere were 0 messages, 0 warnings, and 0 errors.\n\nWe can then calculate normal, percentile, and basic bootstrap confidence intervals via confint().\n\n\nconfint(jsp_boot)\n\n# A tibble: 12 × 6\n   term           estimate     lower  upper type  level\n   <chr>             <dbl>     <dbl>  <dbl> <chr> <dbl>\n 1 (Intercept)      14.2   12.8      15.5   norm   0.95\n 2 mathAge8          0.639  0.590     0.687 norm   0.95\n 3 genderM          -0.357 -1.01      0.316 norm   0.95\n 4 classnonmanual    0.720 -0.0168    1.47  norm   0.95\n 5 (Intercept)      14.2   12.8      15.5   basic  0.95\n 6 mathAge8          0.639  0.588     0.687 basic  0.95\n 7 genderM          -0.357 -1.00      0.323 basic  0.95\n 8 classnonmanual    0.720 -0.000162  1.46  basic  0.95\n 9 (Intercept)      14.2   12.9      15.5   perc   0.95\n10 mathAge8          0.639  0.590     0.690 perc   0.95\n11 genderM          -0.357 -1.04      0.287 perc   0.95\n12 classnonmanual    0.720 -0.0187    1.44  perc   0.95\n\nThe default setting is to calculate all three intervals, but this can be restricted by setting the type parameter to \"norm\", \"basic\", or \"perc\".\nUser-specified statistics: Estimating repeatability/intraclass correlation\nThe beauty of the bootstrap is its flexibility. Interval estimates can be constructed for functions of model parameters that would otherwise require more complex derivations. For example, the bootstrap can be used to estimate the intraclass correlation. The intraclass correlation measures the proportion of the total variance in the response accounted for by groups, and is an important measure of repeatability in ecology and evolutionary biology (Nakagawa and Schielzeth 2010). As a simple example, we’ll consider the BeetlesBody data set in rptR (Stoffel et al. 2017). This simulated data set contains information on body length (BodyL) and the Population from which the beetles were sampled. A simple Guassian-response LME model of the form\n\\[\ny_{ij} = \\beta_0 + b_i + \\varepsilon_{ij}, \\qquad b_i \\sim \\mathcal{N}(0, \\sigma^2_b), \\qquad \\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma^2),\n\\]\ncan be used to describe the body length of beetle \\(j\\) from population \\(i\\). The repeatability is then calculated as \\(R = \\sigma^2_b / (\\sigma^2_b + \\sigma^2)\\). Below we fit this model using lmer():\n\n\ndata(\"BeetlesBody\", package = \"rptR\")\n(beetle_mod <- lmer(BodyL ~ (1 | Population), data = BeetlesBody))\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: BodyL ~ (1 | Population)\n   Data: BeetlesBody\nREML criterion at convergence: 3893.268\nRandom effects:\n Groups     Name        Std.Dev.\n Population (Intercept) 1.173   \n Residual               1.798   \nNumber of obs: 960, groups:  Population, 12\nFixed Effects:\n(Intercept)  \n      14.08  \n\nTo construct a bootstrap confidence interval for the repeatability, we first must write a function to calculate it from the fitted model. Below we write a one-off function for this model to demonstrate a “typical” workflow rather than trying to be overly general.\n\n\nrepeatability <- function(object) {\n  vc <- as.data.frame(VarCorr(object))\n  vc$vcov[1] / (sum(vc$vcov))\n}\n\n\nThe original estimate of repeatability can then be quickly calculated:\n\n\nrepeatability(beetle_mod)\n\n[1] 0.2985548\n\nTo construct a bootstrap confidence interval for the repeatability, we run the desired bootstrap procedure, specifying .f = repeatability and then pass the results to confint().\n\n\n(beetle_boot <- bootstrap(beetle_mod, .f = repeatability, type = \"parametric\", B = 2000))\n\nBootstrap type: parametric \n\nNumber of resamples: 2000 \n\n   observed  rep.mean         se        bias\n1 0.2985548 0.2874126 0.08787679 -0.01114219\n\nThere were 0 messages, 0 warnings, and 0 errors.\n\n(beetle_ci <- confint(beetle_boot, type = \"basic\"))\n\n# A tibble: 1 × 6\n  term  estimate lower upper type  level\n  <chr>    <dbl> <dbl> <dbl> <chr> <dbl>\n1 \"\"       0.299 0.139 0.471 basic  0.95\n\nNotice that the column of beetle_ci is an empty character string since we did not have return a named vector.\nAlternatively, we can plot() the results, as shown in Figure 1. The plot method for lmeresamp objects uses stat_halfeye() from ggdist (Kay 2021) to render a density plot with associated 66% and 95% percentile intervals.\n\n\nplot(beetle_boot, .width = c(.5, .9)) + \n  ggplot2::labs(\n    title = \"Bootstrap repeatabilities\",\n    y = \"density\",\n    x = \"repeatability\"\n  )\n\n\n\nFigure 1: Density plot of the repeatabilities from the beetle model. The median bootstrap repeatability is approximatley .28 and denoted by a point under the density. 66% and 95% confidence intervals for the bootstrap repeatability are (0.217, 0.394) and (0.118, 0.475), respectively, and are displayed as line segments below the density.\n\n\n\nBootstrap tests for a single parameter\nWhile lmeresampler was designed with a focus on estimation, the\nbootstrap functions can be used to conduct bootstrap tests on individual parameters. For example, returning to the JSP example, we might be interested in generating approximate \\(p\\)-values for the fixed effects:\n\n\nsummary(jsp_mod)$coefficients\n\n                 Estimate Std. Error   t value\n(Intercept)    14.1577509 0.73344165 19.303173\nmathAge8        0.6388895 0.02544478 25.108865\ngenderM        -0.3571922 0.34009284 -1.050279\nclassnonmanual  0.7200815 0.38696812  1.860829\n\nTo generate a bootstrap \\(p\\)-value for a fixed effect, we must generate \\(B\\) bootstrap resamples under the reduced model (i.e., the null hypothesis), refit the full model, and then calculate the \\(t\\)-statistic for each resample, denoted \\(t^*_i\\). The bootstrap \\(p\\)-value is then calculated as \\((n_{\\rm extreme} + 1) / (B + 1)\\) (Davison and Hinkley 1997; Halekoh and Højsgaard 2014).\nUsing the bootstrap() function, you can implement this procedure for a single parameter. For example, if we wish to calculate the bootstrap \\(p\\)-value for the class fixed effect we first generate \\(B=1000\\) bootstrap samples from the reduced model without class. In this example, we use the Wild bootstrap to illustrate that we are not restricted to a parametric bootstrap.\n\n\nreduced_model <- update(jsp_mod, . ~ . - class)\nreduced_boot <- bootstrap(reduced_model, type = \"wild\", B = 1000, hccme = \"hc2\", \n                          aux.dist = \"mammen\", .refit = FALSE)\n\n\nNext, we refit the full model, jsp_mod, to each simulation and extract the \\(t\\)-statistic for the class variable. Note that the function extract_t() extracts the specified \\(t\\)-statistic from the coefficient table from model summary.\n\n\nextract_t <- function(model, term) {\n  coef(summary(model))[term, \"t value\"]\n}\n\ntstats <- purrr::map_dbl(\n  reduced_boot, \n  ~refit(jsp_mod, .x) %>% extract_t(., term = \"classnonmanual\")\n)\n\n\nWith the bootstrap \\(t\\)-statistics in hand, we can approximate the \\(p\\)-value using basic logical and arithmetic operators\n\n\n(sum(abs(tstats) >= extract_t(jsp_mod)) + 1) / (1000 + 1)\n\n[1] 0.2687313\n\nWhile the above process is not particularly difficult to implement using the tools provided by lmeresampler, things get tedious if multiple parameters are of interest in this summary table. To help reduce this burden on the user, we have provided the bootstrap_pvals() function that will add bootstrap \\(p\\)-values for each term in the coefficient summary table. For jsp_mod, this is achieved in the below code chunk:\n\n\nbootstrap_pvals(jsp_mod, type = \"wild\", B = 1000, hccme = \"hc2\", aux.dist = \"mammen\")\n\nBootstrap type: wild \n\nNumber of resamples: 1000 \n\n# A tibble: 4 × 5\n  term           Estimate `Std. Error` `t value`  p.value\n  <chr>             <dbl>        <dbl>     <dbl>    <dbl>\n1 (Intercept)      14.2         0.733      19.3  0.000999\n2 mathAge8          0.639       0.0254     25.1  0.000999\n3 genderM          -0.357       0.340      -1.05 0.273   \n4 classnonmanual    0.720       0.387       1.86 0.0659  \n\nIt’s important to note that running bootstraps for each term in the model is computationally demanding. To speed up the computation, you can run the command in parallel, as we discuss below in Bootstrapping in parallel.\nModel comparison\nThe bootstrap can be useful during model selection.\nFor example, if you are comparing a full and reduced model where the reduced model has fewer random effects, a 50:50 mixture of \\(\\chi^2\\) distributions is often used (Stram and Lee 1994); however, Pinheiro and Bates (2000) point out that this approximation is not always optimal. In this example, we explore the Machine data set discussed by Pinheiro and Bates (2000), which consists of productivity scores for six workers on three brands of machine. This data set can be loaded from nlme:\n\n\ndata(\"Machines\", package = \"nlme\")\n\n\nPinheiro and Bates (2000) consider two LME models for these data. The first model has a fixed effect for the machine and a random intercept for the worker.\n\n\nreduced_mod <- lmer(score ~ Machine + (1 | Worker), data = Machines, REML = FALSE)\n\n\nThe second model has the same fixed effects structure, but adds an additional random effect for the machine within the worker.\n\n\nfull_mod <- lmer(score ~ Machine + (1 | Worker/Machine), data = Machines, REML = FALSE)\n\n\nPinheiro and Bates (2000) note that the approximate null distribution given by \\(0.5\\chi^2_0 + 0.5 \\chi^2_1\\) is not successful when the models are fit via maximum likelihood, and that the mixture is closer to \\(0.65\\chi^2_0 + 0.35 \\chi^2_1\\). Instead of relying on the conventional approximation, a bootstrap test can be conducted using bootstrap() to simulate the responses from the reduced model.\nTo conduct this bootstrap test, we first extract the observed statistic obtained via anova() and then generate \\(B=1000\\) bootstrap responses from the reduced model, fm1_machine. Recall that specifying .refit = FALSE returns a data frame of the simulated responses. Here, we use a residual bootstrap for illustration.\n\n\nobserved <- anova(full_mod, reduced_mod)$Chisq[2]\n\nreduced_boot <- bootstrap(reduced_mod, type = \"residual\", B = 1000, .refit = FALSE)\n\n\nNext, we must fit both the full and reduced models to the bootstrap responses and calculate the test statistic. The user-written compare_models() function performs this refit and calculation for given models and bootstrap responses. The control argument for the full model was set to reduce the number of convergence warnings, since the null model had a variance component of 0 for machines within workers, so we expect warnings as we fit an expanded model.\n\n\ncompare_models <- function(full, reduced, newdata) {\n  full_mod <- refit(full, newdata, \n                    control = lmerControl(check.conv.singular = \"ignore\", \n                                          check.conv.grad = \"ignore\"))\n  reduced_mod <- refit(reduced, newdata)\n  anova(full_mod, reduced_mod)$Chisq[2]\n}\n \nchisq_stats <- purrr::map_dbl(reduced_boot, ~compare_models(full_mod, reduced_mod, newdata = .x))\n\n\nWith the test statistics in hand, we can quickly calculate the \\(p\\)-value\n\n\n(sum(chisq_stats >= observed) + 1) / (1000 + 1)\n\n[1] 0.000999001\n\nSimulation-based model diagnostics\nOur final example illustrates how the bootstrap() function can be used for model diagnosis. Loy et al. (2017) propose using the lineup protocol to diagnose LME models, since artificial structures often appear in conventional residual plots for this model class that are not indicative of a model deficiency.\nIn this example, we consider the Dialyzer data set provided by nlme. The data arise from a study characterizing the water transportation characteristics of 20 high flux membrane dialyzers, which were introduced to reduce the time a patient spends on hemodialysis (Vonesh and Carter 1992). The dialyzers were studied in vitro using bovine blood at flow rates of either 200 or 300 ml/min. The study measured the the ultrafiltration rate (ml/hr) at even transmembrane pressures (in mmHg). Pinheiro and Bates (2000) discuss modeling these data. Here, we explore how to create a lineup of residual plots to investigate the adequacy of the initial homoscedastic LME model fit by Pinheiro and Bates (2000).\n\n\nlibrary(nlme)\ndialyzer_mod <- lme(\n  rate ~ (pressure + I(pressure^2) + I(pressure^3) + I(pressure^4)) * QB, \n  data = Dialyzer, \n  random = ~ pressure + I(pressure^2)\n)\n\n\nPinheiro and Bates (2000) construct a residual plot of the conditional residuals plotted against the transmembrane pressure to explore the adequacy of the fitted model (Figure 2). There appears to be increasing spread of the conditional residuals, which would indicate that the homoscedastic model is not sufficient.\n\n\n\nFigure 2: A plot of the conditional residuals against the transmembrane pressure for the dialyzer model. It appears that the variability of the conditional residuals increases with transmembrance pressure, but does this indicate a model condition is violated?\n\n\n\nTo check if this pattern is actually indicative of a problem, we construct a lineup of residual plots. To do this, we must generate data from a number of properly specified models, say 19, to serve as decoy residual plots. Then, we create a faceted set of residual plots where the observed residual plot (Figure 2) is randomly assigned to a facet. To generate the residuals from properly specified models, we use the parametric bootstrap via bootstrap() and extract a data frame containing the residuals from each bootstrap sample using hlm_resid() from HLMdiag (Loy and Hofmann 2014).\n\n\nset.seed(1234)\nlibrary(HLMdiag)\nsim_resids <- bootstrap(dialyzer_mod, .f = hlm_resid, type = \"parametric\", B = 19)\n\n\nThe simulated residuals are stored in the replicates element of the sim_resids list. sim_resids$replicates is a tibble containing the 19 bootstrap samples, with the the replicate number stored in the .n column.\n\n\ndplyr::glimpse(sim_resids$replicates)\n\nRows: 2,660\nColumns: 15\n$ id              <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1…\n$ rate            <dbl> -0.8579226, 17.0489029, 34.3658738, 44.84955…\n$ pressure        <dbl> 0.240, 0.505, 0.995, 1.485, 2.020, 2.495, 2.…\n$ `I(pressure^2)` <I<dbl>>   0.0576, 0.255025, 0.990025, 2.205225,  …\n$ `I(pressure^3)` <I<dbl>>     0.013824,  0.128787625,  0.985074875,…\n$ `I(pressure^4)` <I<dbl>>   0.00331776, 0.065037...., 0.980149....,…\n$ QB              <fct> 200, 200, 200, 200, 200, 200, 200, 200, 200,…\n$ Subject         <ord> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3,…\n$ .resid          <dbl> -1.19758632, 0.65600425, -0.90104247, 1.3913…\n$ .fitted         <dbl> 0.3396637, 16.3928987, 35.2669162, 43.458244…\n$ .ls.resid       <dbl> -0.51757746, 1.23968783, -1.32231163, 0.5907…\n$ .ls.fitted      <dbl> -0.3403452, 15.8092151, 35.6881854, 44.25884…\n$ .mar.resid      <dbl> -2.1236410, -0.3100360, -2.1298323, -0.34531…\n$ .mar.fitted     <dbl> 1.265718, 17.358939, 36.495706, 45.194867, 4…\n$ .n              <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\nNow, we use the lineup() function from nullabor (Buja et al. 2009) to generate the lineup data. lineup() will randomly insert the observed (true) data into the samples data, “encrypt” the position of the observed data, and print a message that you can later decrypt in the console.\n\n\nlibrary(nullabor)\nlineup_data <- lineup(true = hlm_resid(dialyzer_mod), n = 19, samples = sim_resids$replicates)\ndplyr::glimpse(lineup_data)\n\nRows: 2,800\nColumns: 15\n$ id              <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1…\n$ rate            <dbl> -0.8579226, 17.0489029, 34.3658738, 44.84955…\n$ pressure        <dbl> 0.240, 0.505, 0.995, 1.485, 2.020, 2.495, 2.…\n$ `I(pressure^2)` <I<dbl>>   0.0576, 0.255025, 0.990025, 2.205225,  …\n$ `I(pressure^3)` <I<dbl>>     0.013824,  0.128787625,  0.985074875,…\n$ `I(pressure^4)` <I<dbl>>   0.00331776, 0.065037...., 0.980149....,…\n$ QB              <fct> 200, 200, 200, 200, 200, 200, 200, 200, 200,…\n$ Subject         <ord> 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3,…\n$ .resid          <dbl> -1.19758632, 0.65600425, -0.90104247, 1.3913…\n$ .fitted         <dbl> 0.3396637, 16.3928987, 35.2669162, 43.458244…\n$ .ls.resid       <dbl> -0.51757746, 1.23968783, -1.32231163, 0.5907…\n$ .ls.fitted      <dbl> -0.3403452, 15.8092151, 35.6881854, 44.25884…\n$ .mar.resid      <dbl> -2.1236410, -0.3100360, -2.1298323, -0.34531…\n$ .mar.fitted     <dbl> 1.265718, 17.358939, 36.495706, 45.194867, 4…\n$ .sample         <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\nWith the lineup data in hand, we can create a lineup of residual plots using facet_wrap():\n\n\nggplot(lineup_data, aes(x = pressure, y = .resid)) +\n  geom_hline(yintercept = 0, color = \"gray60\") +\n  geom_point(shape = 1) +\n  facet_wrap(~.sample) +\n  theme_bw() +\n  labs(x = \"Transmembrane pressure (dmHg)\", y = \"Residuals (ml/hr)\")\n\n\n\nFigure 3: A lineup of the conditional residuals against the transmembrane pressure for the dialyzer model. One of the facets contains the true residual plot generated from the fitted model, the others are decoys generated using a parametric bootstrap. Facet 13 contains the observed residuals and is discernibly different from decoy plots, providing evidence of that a model condition has been violated.\n\n\n\nIn Figure 3, the observed residual plot is in position 13. If you can discern this plot from the field of decoys, then there is evidence that the fitted homogeneous LME model is deficient. In this case, we believe 13 is discernibly different, as expected based on the discussion in Pinheiro and Bates (2000).\nSince we have discovered signs of within-group heteroscedasticity, we could reformulate our model via nlme::lme() adding a weights argument using varPower, or we could utilize the Wild bootstrap, as illustrated below.\n\n\nwild_dialyzer <- bootstrap(dialyzer_mod, .f = fixef, type = \"wild\", B = 1000, \n                           hccme = \"hc2\", aux.dist = \"webb\")\n\nconfint(wild_dialyzer, type = \"perc\")\n\n# A tibble: 10 × 6\n   term                estimate  lower   upper type  level\n   <chr>                  <dbl>  <dbl>   <dbl> <chr> <dbl>\n 1 (Intercept)          -16.0   -18.0  -13.9   perc   0.95\n 2 pressure              88.4    77.4   99.1   perc   0.95\n 3 I(pressure^2)        -44.3   -57.1  -30.8   perc   0.95\n 4 I(pressure^3)          9.17    2.45  15.5   perc   0.95\n 5 I(pressure^4)         -0.690  -1.70   0.425 perc   0.95\n 6 QB300                 -1.26   -4.55   2.06  perc   0.95\n 7 pressure:QB300         0.621 -16.0   16.9   perc   0.95\n 8 I(pressure^2):QB300    3.15  -18.5   27.6   perc   0.95\n 9 I(pressure^3):QB300    0.102 -12.0   11.1   perc   0.95\n10 I(pressure^4):QB300   -0.172  -1.98   1.84  perc   0.95\n\n\n\nBootstrapping in parallel\nBootstrapping is a computationally demanding task, but bootstrap iterations do not rely on each other so they are easy to implement in parallel. Rather than building parallel processing into lmeresampler, we created a utility function, combine_lmeresamp(), that allows the user to easily implement parallel processing using doParallel (Microsoft and Weston 2020a) and foreach (Microsoft and Weston 2020b). The code is thus concise and simple enough for users without much experience with parallelization, while also providing flexibility to the user.\ndoParallel and foreach default to multicore (i.e., forking) functionality when using parallel computing on UNIX operating systems and snow-like (i.e., clustering) functionality on Windows systems. In this section, we will use clustering in our example. For more information on forking, we refer the reader to the vignette in Microsoft and Weston (2020a).\nThe basic idea behind clustering is to execute tasks as a “cluster” of computers. Each cluster needs to be fed in information separately, and as a consequence clustering has more overhead than forking. Clusters also need to be made and stopped with each call to foreach() to explicitly tell the CPU when to begin and end the parallelization.\nBelow, we revisit the JSP example and distribute 2000 bootstrap iterations equally over two cores:\n\n\nlibrary(foreach)\nlibrary(doParallel)\n\nset.seed(5678)\n\n# Starting a cluster with 2 cores\nno_cores <- 2\ncl <- makeCluster(no_cores)\nregisterDoParallel(cores = no_cores)\n\n# Run 1000 bootstrap iterations on each core\nboot_parallel <- foreach(\n  B = rep(1000, 2), \n  .combine = combine_lmeresamp,\n  .packages = c(\"lmeresampler\", \"lme4\")\n) %dopar% {\n  bootstrap(jsp_mod, .f = fixef, type = \"parametric\", B = B)\n}\n\n# Stop the cluster\nstopCluster(cl)\n\n\nThe combine_lmeresamp() function combines the two lmeresamp objects that are returned from the two bootstrap() calls into a single lmeresamp object. Consequently, working with the returned object proceeds as previously discussed.\nIt’s important to note that running a process on two cores does not yield a runtime that is twice as fast as running the same process on one core. This is because parallelization takes some overhead to split the processes, so while runtime will substantially improve, it will not correspond exactly to the number of cores being used. For example, the runtime for the JSP example run on a single core was\n\n   user  system elapsed \n 23.084   1.212  24.376 \n\nand the runtime for the JSP run on two cores was\n\n   user  system elapsed \n 23.550   1.800  12.747 \n\nThese timings were generated using system.time() on a MacBook Pro with a 2.9 GHz Quad-Core Intel Core i7 processor. In this set up, running the 2000 bootstrap iterations over two cores reduced the runtime by a factor of about 1.91, but this will vary based on the hardware and setting used.\nSummary\nIn this paper, we discussed our implementation of five bootstrap procedures for nested, Gaussian-response LME models fit via the nlme or lme4 packages. The function in lmeresampler provides a unified interface to these procedures, allowing users to easily bootstrap their fitted LME models. In our examples, we illustrated the basic usage of the bootstrap, how it can be used to estimate functions of parameters, how it can be used for testing, and how it can be used to create simulation-based visual diagnostics. The bootstrap approach to inference is computationally intensive, so we have also demonstrated how users can bootstrap in parallel.\nWhile this paper focused solely on the nested, Gaussian-response LME model, lmeresampler implements bootstrap procedures for a wide class of models. Specifically, the cases, residual, and parametric bootstraps can be used to bootstrap generalized LME models fit via . Additionally, the parametric bootstrap works with LME models with crossed random effects, though the results may not be optimal (Mccullagh 2000). Future development of lmeresampler will focus on implementing additional extensions, especially for crossed data structures.\nAcknowledgements\nWe thank Spenser Steele for his contributions to the original code base of lmeresampler. We also thank the reviewers and associate editor whose comments improved the quality of this paper.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-015.zip\nCRAN packages used\nnlme, lme4, RLRsim, pbkrtest, rptR, CLME, lmeresampler, ggdist, HLMdiag, nullabor, doParallel, foreach\nCRAN Task Views implied by cited packages\nChemPhys, Econometrics, Environmetrics, Finance, HighPerformanceComputing, MixedModels, OfficialStatistics, Psychometrics, Spatial, SpatioTemporal\n\n\nD. Bates, M. Mächler, B. Bolker and S. Walker. Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67(1): 1–48, 2015. URL https://doi.org/10.18637/jss.v067.i01.\n\n\nA. Buja, D. Cook, H. Hofmann, M. Lawrence, E. Lee, D. F. Swayne and H. Wickham. Statistical inference for exploratory data analysis and model diagnostics. Royal Society Philosophical Transactions A, 367(1906): 4361–4383, 2009. URL https://doi.org/10.1098/rsta.2009.0120.\n\n\nJ. R. Carpenter, H. Goldstein and J. Rasbash. A novel bootstrap procedure for assessing the relationship between class size and achievement. Journal of the Royal Statistical Society, Series C, 52(4): 431–443, 2003. URL https://doi.org/10.1111/1467-9876.00415.\n\n\nR. Chambers and H. Chandra. A random effect block bootstrap for clustered data. Journal of Computational and Graphical Statistics, 22(2): 452–470, 2013. URL https://doi.org/10.1080/10618600.2012.681216.\n\n\nR. Davidson and E. Flachaire. The wild bootstrap, tamed at last. Journal of econometrics, 146(1): 162–169, 2008. URL https://doi.org/10.1016/j.jeconom.2008.08.003.\n\n\nA. C. Davison and D. V. Hinkley. Boostrap methods and their application. Cambridge University Press, 1997.\n\n\nL. Farnan, A. Ivanova and S. D. Peddada. Linear mixed efects models under inequality constraints with applications. PLOS ONE, 9(1): 2014. DOI 10.1371/journal.pone.0084778.\n\n\nC. A. Field and A. H. Welsh. Bootstrapping clustered data. Journal of the Royal Statistical Society, Series B, 69(3): 369–390, 2007. URL https://doi.org/10.1111/j.1467-9868.2007.00593.x.\n\n\nE. Flachaire. Bootstrapping heteroskedastic regression models: Wild bootstrap vs. Pairs bootstrap. Computational statistics & data analysis, 49(2): 361–376, 2005. URL https://doi.org/10.1016/j.csda.2004.05.018.\n\n\nA. Galecki and T. Burzykowski. Linear mixed-effects models using r: A step-by-step approach. New York: Springer, 2013. URL https://doi.org/10.1007/978-1-4614-3900-4.\n\n\nH. Goldstein. Multilevel statistical models. 4th ed West Sussex: John Wiley & Sons, Ltd., 2011. URL https://doi.org/10.1002/9780470973394.\n\n\nU. Halekoh and S. Højsgaard. A kenward-roger approximation and parametric bootstrap methods for tests in linear mixed models – the R package pbkrtest. Journal of Statistical Software, 59(9): 1–30, 2014. URL https://doi.org/10.18637/jss.v059.i09.\n\n\nJ. Haslett and S. J. Haslett. The three basic types of residuals for a linear model. International statistical review, 75(1): 1–24, 2007. URL https://doi.org/10.1111/j.1751-5823.2006.00001.x.\n\n\nF. K. C. Hui, S. Müller and A. H. Welsh. Random effects misspecification can have severe consequences for random effects inference in linear mixed models. International statistical review = Revue internationale de statistique, 89(1): 186–206, 2021. URL https://onlinelibrary.wiley.com/doi/10.1111/insr.12378.\n\n\nH. Jacqmin-Gadda, S. Sibillot, C. Proust, J.-M. Molina and R. Thiébaut. Robustness of the linear mixed model to misspecified error distribution. Computational statistics & data analysis, 51(10): 5142–5154, 2007. URL https://www.sciencedirect.com/science/article/pii/S016794730600185X.\n\n\nM. Kay. ggdist: Visualizations of distributions and uncertainty. 2021. URL https://doi.org/10.5281/zenodo.3879620. R package version 2.4.0.\n\n\nR. Y. Liu. Bootstrap procedures under some non-i.i.d. models. Annals of statistics, 16(4): 1696–1708, 1988. URL https://doi.org/10.1214/aos/1176351062.\n\n\nA. Loy and H. Hofmann. HLMdiag: A suite of diagnostics for hierarchical linear models in R. Journal of Statistical Software, 56(5): 1–28, 2014. URL https://doi.org/10.18637/jss.v056.i05.\n\n\nA. Loy, H. Hofmann and D. Cook. Model choice and diagnostics for linear Mixed-Effects models using statistics on street corners. Journal of Computational and Graphical Statistics, 26(3): 478–492, 2017. URL https://doi.org/10.1080/10618600.2017.1330207.\n\n\nE. Mammen. Bootstrap and wild bootstrap for high dimensional linear models. Annals of statistics, 21(1): 255–285, 1993. URL https://doi.org/10.1214/aos/1176349025.\n\n\nP. Mccullagh. Resampling and exchangeable arrays. Bernoulli, 6(2): 285–301, 2000. URL https://doi.org/10.2307/3318577.\n\n\nMicrosoft and S. Weston. doParallel: Foreach parallel adaptor for the ’parallel’ package. 2020a. URL https://CRAN.R-project.org/package=doParallel. R package version 1.0.16.\n\n\nMicrosoft and S. Weston. Foreach: Provides foreach looping construct. 2020b. URL https://CRAN.R-project.org/package=foreach. R package version 1.5.1.\n\n\nL. Modugno and S. Giannerini. The wild bootstrap for multilevel models. Communications in Statistics - Theory and Methods, 44(22): 4812–4825, 2015. URL https://doi.org/10.1080/03610926.2013.802807.\n\n\nJ. S. Morris. The BLUPs are not “best” when it comes to bootstrapping. Statistics and Probability Letters, 56(4): 425–430, 2002. URL https://doi.org/10.1016/S0167-7152(02)00041-X.\n\n\nP. Mortimore, P. Sammons, L. Stoll and R. Ecob. School matters. University of California Press, 1988.\n\n\nS. Nakagawa and H. Schielzeth. Repeatability for gaussian and non-gaussian data: A practical guide for biologists. Biological reviews of the Cambridge Philosophical Society, 85(4): 935–956, 2010. URL https://doi.org/10.1111/j.1469-185X.2010.00141.x.\n\n\nJ. C. Pinheiro and D. M. Bates. Mixed-effects models in s and s-PLUS. New York: Springer-Verlag, 2000. URL https://doi.org/10.1007/b98882.\n\n\nJ. Pinheiro, D. Bates, S. DebRoy, D. Sarkar and R Core Team. nlme: Linear and nonlinear mixed effects models. 2017. URL https://CRAN.R-project.org/package=nlme. R package version 3.1-131.\n\n\nS. W. Raudenbush and A. S. Bryk. Hierarchical linear models: Applications and data analysis methods. 2nd ed Thousand Oaks, CA: Sage Publications, Inc., 2002.\n\n\nJ. A. Sánchez-Espigares and J. Ocaña. An R implementation of bootstrap procedures for mixed models. 2009. URL https://www.r-project.org/conferences/useR-2009/slides/SanchezEspigares+Ocana.pdf.\n\n\nF. Scheipl, S. Greven and H. Kuechenhoff. Size and power of tests for a zero random effect variance or polynomial regression in additive and linear mixed models. Computational Statistics & Data Analysis, 52(7): 3283–3299, 2008. URL https://doi.org/10.1016/j.csda.2007.10.022.\n\n\nJ. Shao and D. Tu. The jackknife and bootstrap. Springer, 1995.\n\n\nJ. M. Singer, F. M. M. Rocha and J. S. Nobre. Graphical tools for detecting departures from linear mixed model assumptions and some remedial measures. International statistical review = Revue internationale de statistique, 85(2): 290–324, 2017. URL https://doi.org/10.1111/insr.12178.\n\n\nM. A. Stoffel, S. Nakagawa and H. Schielzeth. rptR: Repeatability estimation and variance decomposition by generalized linear mixed-effects models. Methods in Ecology and Evolution, 8: 1639–1644, 2017. URL https://doi.org/10.1111/2041-210X.12797.\n\n\nD. O. Stram and J. W. Lee. Variance components testing in the longitudinal mixed effects model. Biometrics, 50(4): 1171–1177, 1994.\n\n\nR. Van der Leeden, E. Meijer and F. M. T. A. Busing. Resampling multilevel models. In Handbook of multilevel analysis, Eds J. de Leeuw and E. Meijer pages. 401–433 2008. Springer New York. ISBN 978-0-387-73183-4. URL https://doi.org/10.1007/978-0-387-73186-5_11.\n\n\nE. F. Vonesh and R. L. Carter. Mixed-effects nonlinear regression for unbalanced repeated measures. Biometrics, 48(1): 1–17, 1992.\n\n\nM. D. Webb. Reworking wild bootstrap based inference for clustered errors. Queen’s Economics Department Working Paper. 2013. URL https://www.econstor.eu/handle/10419/97480.\n\n\n\n\n",
    "preview": "articles/RJ-2023-015/distill-preview.png",
    "last_modified": "2023-11-07T21:31:41+00:00",
    "input_file": {},
    "preview_width": 576,
    "preview_height": 432
  },
  {
    "path": "articles/RJ-2023-016/",
    "title": "BayesPPD: An R Package for Bayesian Sample Size Determination Using the Power and Normalized Power Prior for Generalized Linear Models",
    "description": "The R package BayesPPD (Bayesian Power Prior Design) supports Bayesian power and type I error calculation and model fitting after incorporating historical data with the power prior and the normalized power prior for generalized linear models (GLM). The package accommodates summary level data or subject level data with covariate information. It supports use of multiple historical datasets as well as design without historical data. Supported distributions for responses include normal, binary (Bernoulli/binomial), Poisson and exponential. The power parameter can be fixed or modeled as random using a normalized power prior for each of these distributions. In addition, the package supports the use of arbitrary sampling priors for computing Bayesian power and type I error rates. In addition to describing the statistical methodology and functions implemented in the package to enable sample size determination (SSD), we also demonstrate the use of BayesPPD in two comprehensive case studies.",
    "author": [
      {
        "name": "Yueqi Shen",
        "url": {}
      },
      {
        "name": "Matthew A. Psioda",
        "url": {}
      },
      {
        "name": "Joseph G. Ibrahim",
        "url": {}
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-016.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:41+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-017/",
    "title": "ppseq: An R Package for Sequential Predictive Probability Monitoring",
    "description": "Advances in drug discovery have produced numerous biomarker-guided therapeutic strategies for treating cancer. Yet the promise of precision medicine comes with the cost of increased complexity. Recent trials of targeted treatments have included expansion cohorts with sample sizes far exceeding those in traditional early phase trials of chemotherapeutic agents. The enlarged sample sizes raise ethical concerns for patients who enroll in clinical trials, and emphasize the need for rigorous statistical designs to ensure that trials can stop early for futility while maintaining traditional control of type I error and power. The R package ppseq provides a framework for designing early phase clinical trials of binary endpoints using sequential futility monitoring based on Bayesian predictive probability. Trial designs can be compared using interactive plots and selected based on measures of efficiency or accuracy.",
    "author": [
      {
        "name": "Emily C. Zabor",
        "url": "http://www.emilyzabor.com/"
      },
      {
        "name": "Brian P. Hobbs",
        "url": {}
      },
      {
        "name": "Michael J. Kane",
        "url": {}
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\n1 Introduction\nStatistical methods for early phase oncology trials were developed in the context of cytotoxic treatments. Most cytotoxic treatments exhibit increases in both efficacy and toxicity with increasing dose. As a result, phase I trials were centered on identifying the maximum tolerated dose (MTD), defined as the highest dose that did not exceed a pre-specified toxicity threshold. But advances in drug discovery have produced numerous biomarker-guided non-cytotoxic therapies such as small molecule inhibitors, antibody drug conjugates, immune checkpoint inhibitors, and monoclonal antibodies. These therapies typically do not exhibit the same pattern of increases in both efficacy and toxicity with increasing dose, so the MTD as traditionally defined may not exist. Instead, lower doses may have equivalent efficacy but lower toxicity as compared to higher doses. As a result, these therapies can be difficult to study with traditional dose-escalation designs such as the rule-based 3+3 and the model-based continual reassessment method, among others (Pestana et al. 2020). To address this issue, recent phase I trials have included large dose-expansion cohorts, in which additional patients are enrolled in phase I after the dose-escalation phase is complete. In this setup, the dose-escalation phase is considered phase Ia and used to assess the initial safety of multiple doses, then the dose-expansion phase is considered phase Ib and can have a variety of aims including to further refine the safety of one or more doses, to assess preliminary efficacy, to explore the treatment in various disease-specific subtypes, or to further characterize the pharmacokinetics and/or pharmacodynamics. The use of dose-expansion cohorts increased from 12% in 2006 to 38% in 2011 (Manji et al. 2013) and trials with dose-expansion cohorts led to higher response rates and more frequent success in phase II trials (Bugano et al. 2017).\nBut despite these successes, recent dose-expansion cohorts have not always been planned in advance, leading to uncertain statistical properties, and have at times included samples sizes that far exceed those typically seen in early phase trials. For example, the KEYNOTE-001 trial of pembrolizumab, initially designed as a 3+3 dose-escalation trial, included multiple protocol amendments and ultimately enrolled a total of 655 patients across five melanoma expansion cohorts and 550 patients across four non-small-cell lung cancer expansion cohorts (Khoja et al. 2015). In a basket trial of atezolizumab, an anti-PD-L1 treatment in patients with a variety of cancers and both with and without PD-L1 expression, an expansion cohort in metastatic urothelial carcinoma ultimately evaluated 95 patients, despite the fact that no expansion cohort in this disease subtype was originally planned in the trial protocol. The expansion cohort in metastatic urothelial carcinoma was added through a protocol amendment, and the sample size later was increased from what was initially planned (Powles et al. 2014; Petrylak et al. 2018). These enlarged sample sizes raise ethical concerns for patients who enroll in clinical trials, and emphasize the need for rigorous statistical designs to ensure that trials can stop early for futility while maintaining traditional control of type I error and power.\nBayesian predictive probability has been proposed as an approach for sequential monitoring in early phase oncology trials (Dmitrienko and Wang 2006; Lee and Liu 2008; Saville et al. 2014; Hobbs et al. 2018). However, in order to be useful to investigators designing such trials, software must be made available to calibrate the design for the desired statistical properties. To our knowledge no such software currently exists in the R programming language. This paper introduces the ppseq package for the R software language, which provides functions to design early phase clinical trials of binary endpoints using sequential predictive probability monitoring for futility. Static plots produced using the ggplot2 package (Wickham 2016) and interactive plots produced using the plotly package (Sievert 2020) compare design options based on frequentist type I error and power. Moreover, the ppseq package implements two criteria for selecting an ideal predictive probability monitoring design from among the options formed by combinations of posterior and predictive decision thresholds. While the ppseq package was developed with early phase oncology clinical trials in mind, the methodology is general and can be applied to any application of sequential futility monitoring in clinical trial design.\n2 Predictive probability monitoring\nConsider the setting of an expansion cohort with a binary outcome, such as tumor response as measured by the RECIST (Response Evaluation Criteria in Solid Tumors) criteria. Here we will focus on the one-sample setting, in which all patients in the trial are enrolled onto a single arm and given the experimental treatment of interest. Functionality is also available for the two-sample setting, in which patients are enrolled onto two different treatment arms, or a treatment and a control arm, for comparative purposes. Each patient, denoted by \\(i\\), enrolled in the trial either has a response such that \\(x_i = 1\\) or does not have a response such that \\(x_i = 0\\). Then \\(X = \\sum_{i=1}^n x_i\\) is the number of responses out of \\(n\\) currently observed patients up to a maximum of \\(N\\) total patients. Let \\(p\\) represent the true probability of response. Fix \\(p_0\\) as the threshold for unacceptable response rate and \\(p_1\\) as the threshold for acceptable response rate. Most dose-expansion studies with an efficacy aim will wish to test the null hypothesis \\(H_0: p \\leq p_0\\) versus the alternative hypothesis \\(H_1: p \\geq p_1\\).\nThe Bayesian paradigm of statistics is founded on Bayes’ theorem, which is a mathematical theory that specifies how to combine the prior distributions that define prior beliefs about parameters, such as the true response rate \\(p\\), with the observed data, such as the total number of responses \\(X\\), yielding a posterior distribution. Here the prior distribution of the response rate \\(\\pi(p)\\) has a beta distribution \\(\\mbox{Beta}(a_0, b_0)\\) and our data \\(X\\) have a binomial distribution \\(\\mbox{Bin}(n, p)\\). Combining the likelihood function for the observed data \\(L_x(p) \\propto p^x (1-p)^{n-x}\\) with the prior, we obtain the posterior distribution of the response rate, which follows the beta distribution \\(p|x \\sim \\mbox{Beta}(a_0 + x, b_0 + n - x)\\). A posterior probability threshold \\(\\theta\\) would be pre-specified during the trial design stage. At the end of the trial, if the posterior probability exceeded the pre-specified threshold, i.e. if \\(\\Pr(p>p_0 | X) > \\theta\\), the trial would be declared a success.\nThe posterior predictive distribution of the number of future responses \\(X^*\\) in the remaining \\(n^*=N-n\\) future patients follows a beta-binomial distribution \\(\\mbox{Beta-binomial}(n^*, a_0 + x, b_0 + n - x)\\). Then the posterior predictive probability (PPP), is calculated as \\(PPP = \\sum_{{x^*}=0}^{n^*} \\Pr(X^*=x^*|x) \\times I(\\Pr(p>p_0 | X, X^*=x^*) > \\theta)\\). The posterior predictive probability represents the probability that, at any given interim monitoring point, the treatment will be declared efficacious at the end of the trial when full enrollment is reached, conditional on the currently observed data and the specified priors. We would stop the trial early for futility if the posterior predictive probability dropped below a pre-specified threshold \\(\\theta^*\\), i.e. \\(PPP<\\theta^*\\). Predictive probability thresholds closer to 0 lead to less frequent stopping for futility whereas thresholds near 1 lead to frequent stopping unless there is almost certain probability of success. Predictive probability provides an intuitive interim monitoring strategy for clinical trials that tells the investigator what the chances are of declaring the treatment efficacious at the end of the trial if we were to continue enrolling to the maximum planned sample size, based on the data observed in the trial to date.\n3 Package overview\nThe ppseq package facilitates the design of clinical trials utilizing sequential predictive probability monitoring for futility. The goal is to establish a set of decision rules at the trial planning phase that would be used for interim monitoring during the course of the trial. The main computational challenge in designing such a trial is joint calibration of the posterior probability and posterior predictive probability thresholds to be used in the trial in order to achieve the desired levels of frequentist type I error and power. The main function for achieving this aim is the calibrate_thresholds() function, which will evaluate a grid of posterior thresholds \\(\\theta\\) and predictive thresholds \\(\\theta^*\\) provided by the user as vector inputs specified with the arguments pp_threshold and ppp_threshold, respectively. Other required arguments include the unacceptable response rate \\(p_0\\) specified by p_null, the acceptable response rate \\(p_1\\) specified by p_alt, a vector of sample sizes at which interim analyses are to be performed n, and the maximum total sample size N. The direction of the alternative hypothesis is specified with the argument direction and defaults to \"greater\", which corresponds to the alternative hypothesis \\(H_1: p \\geq p_1\\). The hyperparameters of the prior beta distribution are specified with the argument prior and default to c(0.5, 0.5), which denotes a \\(\\mbox{Beta}(0.5, 0.5)\\) distribution. The number of posterior samples is specified with the argument S, which defaults to 5000 samples, and the number of simulated trial datasets is specified with the argument nsim, which defaults to 1000. The additional argument delta, which defaults to NULL for the one-sample setting, can specify a clinically meaningful difference between groups \\(\\delta = p_1 - p_0\\) in the case of a two-sample trial design.\nThe calibrate_thresholds() function conducts the following algorithm, given the default arguments:\nGenerate nsim datasets, denoted by \\(j\\), containing the cumulative number of responses \\(x\\) at each interim sample size \\(n\\) from a binomial distribution under the unacceptable (i.e. null) response rate \\(p_0\\), specified by p_null, and under the acceptable (i.e. alternative) response rate \\(p_1\\), specified by p_alt, for each interim look, denoted by \\(l\\).\nFor dataset \\(j\\) and posterior threshold \\(\\theta_k\\), draw S samples, denoted by \\(s\\), from the posterior distribution \\(p|x_{ls} \\sim \\mbox{Beta}(a_0 + x_l, b_0 + n_l - x_l)\\) where \\(x_l\\) is the number of responses at interim look \\(l\\) and \\(n_l\\) is the number of enrolled patients at interim look \\(l\\). Use each \\(p|x_{ls}\\) as the response probability in a binomial distribution to generate the number of future responses \\(X^*_{ls}\\) in the remaining \\(n^*_l=N-n_l\\) future patients at interim look \\(l\\).\nThen for each \\(X^*_{ls}\\), generate S posterior probabilities, denoted by \\(s'\\), at the end of the trial: \\(PP^*_{lss'} \\sim \\mbox{Beta}(a_0 + X^*_{ls} + x_l, b_0 + n^*_l - (X^*_{ls} + x_l))\\) and calculate \\(PP^*_{ls} = \\Pr(p>p_0 | X^*_{ls}) = \\frac{\\sum_1^{S'} PP^*_{lss'} > p_0}{S'}\\).\nEstimate the predictive probability at posterior threshold \\(k\\) as \\(PPP_{lk} = \\frac{\\sum_1^S PP^*_{ls}>\\theta_k}{S}\\).\nStop the trial for dataset \\(j\\) at interim look \\(l\\) and predictive threshold \\(m\\) if \\(PPP_{lk} < \\theta^*_m\\). Otherwise continue enrolling.\n\nRepeat (2) over all combinations of datasets \\(j\\), posterior thresholds \\(k\\), and predictive thresholds \\(m\\).\nIf dataset \\(j\\) was stopped early for futility then we do not reject the null hypothesis. If dataset \\(j\\) reached full enrollment, we reject the null hypothesis \\(H_0: p \\leq p_0\\) at posterior threshold \\(k\\) if \\(PPP_{lk} > \\theta_k\\).\nThe function returns a list, the first element of which is a tibble containing the posterior threshold \\(\\theta\\), the predictive threshold \\(\\theta^*\\), the mean sample size under the null and the alternative, the proportion of positive trials under the null and alternative, and the proportion of trials stopped early under the null and alternative. The proportion of trials simulated under the null hypothesis for which the null hypothesis was rejected is an estimate of the type I error, and the proportion of trials simulated under the alternative hypothesis for which the null hypothesis was rejected is an estimate of the power. The print() option will print the results summary for each combination of thresholds, filtered by an acceptable range of frequentist type I error and minimum power, if desired. Note that the results will be sensitive to the choice of nsim and S. We have set what we believe are reasonable defaults and would caution users against reducing these values without careful consideration.\nDesign selection\nAfter obtaining results for all combinations of evaluated posterior and predictive thresholds, the next step is to select the ideal design from among the various options. The ppseq package introduces two criteria to assist users in making a selection. The first, called the “optimal accuracy” design, identifies the design that minimizes the Euclidean distance to 0 type I error probability and a power of 1. To accomplish this, the accuracy Euclidean distance (AED) for the design with posterior threshold \\(k\\) and predictive threshold \\(m\\) is calculated as \\(AED_{km} = w_\\alpha * (\\alpha_{km} - 0)^2 + w_{(1-\\beta)} * ((1-\\beta)_{km}-1)^2\\), where \\(w_\\alpha\\) and \\(w_{(1-\\beta)}\\) are optional weights on the type I error and power, respectively, and \\(\\alpha_{km}\\) denotes the estimated type I error and \\((1-\\beta)_{km}\\) denotes the estimated power. The design with the smallest value of \\(AED_{km}\\) is selected as optimal. The second criteria, called the “optimal efficiency” design, identifies the design that minimizes the Euclidean distance to minimal average sample size under the null and maximal average sample size under the alternative. To accomplish this, the efficiency Euclidean distant (EED) for the design with posterior threshold \\(k\\) and predictive threshold \\(m\\) is calculated as \\(EED_{km} = w_{\\bar{N}_{H_0}} * (\\bar{N}_{H_0km} - min(\\bar{N}_{H_0}))^2 + w_{\\bar{N}_{H_1}} * (\\bar{N}_{H_1km}-max(\\bar{N}_{H_1}))^2\\), where \\(w_{\\bar{N}_{H_0}}\\) and \\(w_{\\bar{N}_{H_1}}\\) are optional weights on the average sample size under the null and alternative, respectively, \\(\\bar{N}_{H_0km}\\) and \\(\\bar{N}_{H_1km}\\) denote the average sample sizes under the null and alternative, respectively, and \\(min(\\bar{N}_{H_0})\\) and \\(max(\\bar{N}_{H_1})\\) denote the minimum average sample size under the null and the maximum average sample size under the alternative alternative, respectively, across all combinations of \\(k\\) and \\(m\\). The design with the smallest value of \\(EED_{km}\\) is selected as optimal. The optimize_design() function returns a list that contains the details of each of the two optimal designs.\nDecision rules\nTo ease the implementation of clinical trials designed with sequential predictive probability monitoring, once a design has been selected, a table of decision rules can be produced using the calc_decision_rules() function. The function takes the sample sizes n at which interim analyses are to be performed as well as the maximum total sample size N, the null value to compare to in the one-sample case p0 (set to NULL in the two-sample case), the posterior threshold of the selected design theta, and the predictive threshold of the selected design ppp. Arguments direction, prior, S, delta are as described in the Package Overview section, with the same defaults. The function results in a tibble. The trial would stop at a given look if the number of observed responses is less than or equal to \\(r\\), otherwise the trial would continue enrolling if the number of observed responses is greater than \\(r\\). At the end of the trial when the maximum planned sample size is reached, the treatment would be considered promising if the number of observed responses is greater than \\(r\\). In the one-sample case, the resulting tibble includes a column for the sample size n at each interim look, \\(r\\) at each look, and a column for the associated posterior predictive probability ppp. In the two-sample case, the tibble includes columns for n0 and n1, the sample size at each interim analysis in the control and experimental arms, respectively. There are also columns for r0 and r1, the number of responses in the control arm and experimental arm, respectively, leading to the decision to stop or continue. Finally, there is a column for the posterior predictive probability associated with that decision ppp.\nVisualizations\nFinally, to assist users in comparing the results of the various design options, a plot() option is available for the results of calibrate_thresholds that allows creation of static plots using the ggplot2 package (Wickham 2016) or interactive plots using the plotly package (Sievert 2020). Two plots are produced, one plotting type I error by power and indicating the optimal accuracy design, and one plotting the average sample size under the null by the average sample size under the alternative and indicating the optimal efficiency design. The motivation for including an interactive graphics option was the utility of the additional information available when hovering over each point. Instead of simply eyeballing where points fall along the axes, users can see the specific type I error, power, average sample size under the null, average sample size under the alternative, the posterior and predictive thresholds associated with the design, as well as the distance to the upper left point on the plot. A plot() option is also available for the results of calc_decision_rules. In the one-sample case it produces a single plot showing the sample size at each interim analysis on the x-axis and the possible number of responses at each interim analysis on the y-axis. In the two-sample case a grid of plots is produced, with one plot for each interim analysis. The x-axis shows the number of possible responses in the control group and the y-axis shows the number of possible responses in the experimental group. In both cases, the boxes are colored green for a “proceed” decision and red for a “stop” decision for each combination and the hover box produced by plotly provides the details.\n4 Package demonstration\nIn this section I will present a basic example to demonstrate the functionality included in the ppseq. The following section will present a more realistic case study.\nFirst we install and load the ppseq package.\n\n\ninstall.packages(\"ppseq\")\n\n\n\n\nlibrary(ppseq)\n\n\nConsider the case where we are interested in designing a trial to investigate how a new treatment impacts tumor response measured as a binary outcome of response versus no response. We know the current standard of care treatment results in a tumor response rate of 10%, and we wish to improve this by 30%. So we wish to test \\(H_0: p \\leq 0.1\\) versus \\(H_1: p \\geq 0.4\\), so we set p_null = 0.1 and p_alt = 0.4. This is a rare disease so our maximum sample size is 15, so we set N = 15, and we will do interim analyses after every 5 patients, so we set n = seq(5, 15, 5). We wish to examine designs based on combinations of posterior thresholds \\(\\theta = {0.85, 0.90}\\) and predictive thresholds \\(\\theta^*={0.1, 0.2}\\), so we set pp_threshold = c(0.85, 0.9) and ppp_threshold = c(0.1, 0.2). Finally, for computational speed in this basic example, we set S=50 and nsim=50, but in practice we would want to use much larger values, in the thousands.\n\n\nset.seed(123)\n\ncal_thresh <-\n  calibrate_thresholds(\n    p_null = 0.1, \n    p_alt = 0.4,\n    n = seq(5, 15, 5), \n    N = 15,\n    pp_threshold = c(0.85, 0.9),\n    ppp_threshold = c(0.1, 0.2),\n    S = 50, \n    nsim = 50\n    )\n\n\nSince there are only four design options in this toy example, we print the entire results table using a call to print(). Each row represents a different combination of the considered posterior and predictive thresholds, denotes pp_threshold and ppp_threshold, respectively.\n\n\nprint(cal_thresh)\n\n# A tibble: 4 × 8\n  pp_threshold ppp_t…¹ mean_…² prop_…³ prop_…⁴ mean_…⁵ prop_…⁶ prop_…⁷\n         <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1         0.85     0.1    12.1    0.1     0.42    14.8    0.92    0.02\n2         0.85     0.2     9.5    0.06    0.64    14.8    0.92    0.02\n3         0.9      0.1    10.8    0.04    0.5     14.8    0.9     0.02\n4         0.9      0.2     8.9    0.04    0.74    14.7    0.88    0.04\n# … with abbreviated variable names ¹​ppp_threshold, ²​mean_n1_null,\n#   ³​prop_pos_null, ⁴​prop_stopped_null, ⁵​mean_n1_alt, ⁶​prop_pos_alt,\n#   ⁷​prop_stopped_alt\n\nWe use the optimize_design() function to identify the optimal accuracy and optimal efficiency designs. We constrain consideration to designs with type I error between 0.025 and 0.1, specified by type1_range = c(0.025, 0.1), and power of at least 0.75, specified by minimum_power = 0.75.\n\n\noptimize_design(cal_thresh, type1_range = c(0.025, 0.1), minimum_power = 0.75)\n\n$`Optimal accuracy design:`\n# A tibble: 1 × 6\n  pp_threshold ppp_threshold `Type I error` Power\n         <dbl>         <dbl>          <dbl> <dbl>\n1         0.85           0.2           0.06  0.92\n  `Average N under the null` `Average N under the alternative`\n                       <dbl>                             <dbl>\n1                        9.5                              14.8\n\n$`Optimal efficiency design:`\n# A tibble: 1 × 6\n  pp_threshold ppp_threshold `Type I error` Power\n         <dbl>         <dbl>          <dbl> <dbl>\n1          0.9           0.2           0.04  0.88\n  `Average N under the null` `Average N under the alternative`\n                       <dbl>                             <dbl>\n1                        8.9                              14.7\n\nTo ease interim analysis during the course of the trial, we obtain the decision table using the calc_decision_rules() function, with the thresholds of our selected optimal efficiency design passed as theta = 0.9 and ppp_threshold = 0.2. For computational speed in this basic example, we set S=50, but in practice we would want to draw many more posterior samples, and values of 5000 or more should be considered. If any NA value was returned in this table, it would indicate that at that interim look you would never stop, regardless of the number of responses.\n\n\ncalc_decision_rules(\n  n = seq(5, 15, 5), \n  N = 15,\n  theta = 0.9,\n  ppp = 0.2,\n  p0 = 0.1,\n  S = 50\n)\n\n# A tibble: 3 × 3\n      n     r   ppp\n  <dbl> <int> <dbl>\n1     5     0  0.06\n2    10     1  0.1 \n3    15     2  0   \n\nThis table tells us that at the first interim analysis after we have observed \\(n=5\\) patients for response, we would stop the trial if \\(\\leq r=0\\) responses had been observed, and would continue enrolling if \\(> r=0\\) responses had been observed. At the second interim analysis after we have observed \\(n=10\\) patients for response, we would stop the trial if \\(\\leq r=1\\) responses had been observed, and would continue enrolling if \\(> r=1\\) responses had been observed. At the end of the trial after all \\(n=N=15\\) patients have been observed for response, we would declare the new treatment promising if \\(> r=2\\) responses had been observed.\n5 Case study\nThe case study focuses on a re-design of a phase II study of atezolizumab in metastatic urothelial carcinoma patients (mUC) using sequential predictive probability monitoring. Atezolizumab is a programmed death-ligand 1 (PD-L1) blocking monoclonal antibody that was given accelerated approval by the U.S. Food and Drug Administration in May 2016 for the treatment of patients with locally advanced or metastatic urothelial carcinoma who had disease progression following platinum-containing chemotherapy. The approval was based on the results of a single-arm phase II study in 310 patients (Rosenberg et al. 2016). The phase II study used a hierarchical fixed-sequence testing procedure to test increasingly broad subgroups of patients based on PD-L1 status, and found overall response rates of 26% (95% CI: 18-36), 18% (95% CI: 13-24), and 15% (95% CI 11-19) in patients with \\(\\geq5\\%\\) PD-L1-positive immune cells (IC2/3 subgroup), in patients with \\(\\geq1\\%\\) PD-L1-positive immune cells (IC1/2/3 subgroup), and in all patients, respectively (Rosenberg et al. 2016). All three rates exceeded the historical control rate of 10%. Then, in March 2021, the approval in this indication was voluntarily withdrawn by the sponsor following negative results from a randomized phase III study (Powles et al. 2018). In the phase III study, 931 patients were randomly assigned to receive atezolizumab or chemotherapy in a 1:1 ratio, and the same hierarchical fixed-sequence testing procedure as in the phase II study was used. The phase III study found that overall survival did not differ significantly between the atezolizumab and chemotherapy groups of the IC2/3 subgroup (median survival 11.1 months [95% CI: 8.6-15.5] versus 10.6 months [95% CI: 8.4-12.2]), so no further testing was conducted for the primary endpoint (Powles et al. 2018). Further analyses revealed that while the response rates to atezolizumab were comparable to those seen in the phase II study, the response rates to chemotherapy were much higher than the historical control rate of 10%. The overall response rates to chemotherapy were 21.6% (95% CI: 14.5-30.2), 14.7% (95% CI: 10.9-19.2), and 13.4% (95% CI: 10.5-16.9) for the IC2/3 subgroup, IC1/2/3 subgroup, and all patients, respectively. The overall response rates to atezolizumab were 23% (95% CI: 15.6-31.9), 14.1% (95% CI: 10.4-18.5), and 13.4% (95% CI: 10.5-16.9) for the IC2/3 subgroup, IC1/2/3 subgroup, and all patients, respectively. These results indicate that PD-L1 status is a prognostic biomarker for both standard of care chemotherapies that comprised the control arm as well as atezolizumab in this patient population.\nWe wish to re-design the phase II trial of atezolizumab using a two-arm randomized design with sequential predictive probability monitoring. We will use the calibrate_thresholds() function to obtain the operating characteristics of designs based on on each combination of posterior and predictive thresholds. We focus here on the main biomarker subgroup of interest, the IC2/3 subgroup. We design the study with a null response rate of 0.1 in both arms, and an alternative response rate of 0.25 in the atezolizumab arm. So we set p_null = c(0.1, 0.1) and p_alt = c(0.1, 0.25). We plan the study with 100 participants, assuming that the total sample size available is similar to the 310 used in the actual single-arm phase II trial, and that a third of that patient population fall into our desired biomarker subgroup. So we set N = c(50, 50) to indicate equal allocation of the 100 patients to each of the two arms. We will check for futility after every 10 patients are enrolled on each arm, so we set n = cbind(seq(10, 50, 10), seq(10, 50, 10)), a matrix showing the interim analysis sample sizes in each of the two arms. To design the study, we need to calibrate the design over a range of posterior probability thresholds and predictive probability thresholds. In this example we will consider posterior thresholds of 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, and 0.99, and predictive thresholds of 0.05, 0.1, 0.15, and 0.2. So we set pp_threshold = seq(0.9, 0.99, 0.01) and ppp_threshold = seq(0.05, 0.2, 0.05). We selected these posterior thresholds because we want to have a posterior probability of at least 0.9 that our response rate exceeds the null. And we selected these predictive thresholds because we would not want to stop the trial early for futility if there was more than a 20% chance of success when full enrollment was reached. We use the defaults for all remaining arguments, including the defaults of S=5000 and nsim=1000, which we believe are sufficient to obtain stable results. Because of the inherent computational intensity in these calculations, this function relies on the future (Bengtsson 2021) and furrr (Vaughan and Dancho 2021) packages to parallelize computations. The user will be responsible for setting up a call to future::plan() that is appropriate to their operating environment and simulation setting. Because the code takes some time to run, the results of the below example code are available as a dataset called two_sample_cal_tbl included in the ppseq package.\n\n\nlibrary(future)\n\nset.seed(123)\n\nfuture::plan(future::multicore(workers = 40))\n\ntwo_sample_cal_tbl <- \n  calibrate_thresholds(\n    p_null = c(0.1, 0.1), \n    p_alt = c(0.1, 0.25),\n    n = cbind(seq(10, 50, 10), seq(10, 50, 10)),\n    N = c(50, 50), \n    pp_threshold = seq(0.9, 0.99, 0.01),\n    ppp_threshold = seq(0.05, 0.2, 0.05),\n    direction = \"greater\", \n    delta = 0, \n    prior = c(0.5, 0.5), \n    S = 5000, \n    nsim = 1000\n  )\n\n\nWe can compare the design options that meet our desired range of type I error and minimum power by passing the results of our call to the calibrate_thresholds() function to the plot() function. The plotly = TRUE option produces interactive visualizations or the plotly = FALSE option produces static visualizations.\n\n\nplot(\n  two_sample_cal_tbl, \n  type1_range = c(0.05, 0.1), \n  minimum_power = 0.7,\n  plotly = TRUE\n)\n\n\n\n\n\n\nFigure 1: Plot of efficiency design options made with the {plotly} package. The average sample size under the null is on the x-axis and the average sample size under the alternative is on the y-axis. The color represents the Euclidean distance to the top left point and the optimal design is indicated by a diamond. The optimal efficiency design is the one with posterior threshold 0.92 and predictive threshold 0.05. A similar plot is available for the accuracy design options.\n\n\n\nWe see that the optimal efficiency design is the one with posterior threshold 0.92 and predictive threshold 0.05. It has a type I error of 0.07, power of 0.7, average sample size under the null of 29 per arm, and average sample size under the alternative of 46 per arm. This design would allow us to stop early if the treatment were inefficacious, thus preserving valuable financial resources for use in studying more promising treatments and preventing our human subjects from continuing an ineffective treatment.\n\n\noptimize_design(\n  two_sample_cal_tbl, \n  type1_range = c(0.05, 0.1), \n  minimum_power = 0.7\n)\n\n$`Optimal accuracy design:`\n# A tibble: 1 × 6\n  pp_threshold ppp_threshold `Type I error` Power\n         <dbl>         <dbl>          <dbl> <dbl>\n1         0.86           0.1            0.1 0.751\n  `Average N under the null` `Average N under the alternative`\n                       <dbl>                             <dbl>\n1                       28.6                              45.1\n\n$`Optimal efficiency design:`\n# A tibble: 1 × 6\n  pp_threshold ppp_threshold `Type I error` Power\n         <dbl>         <dbl>          <dbl> <dbl>\n1         0.92          0.05           0.07 0.701\n  `Average N under the null` `Average N under the alternative`\n                       <dbl>                             <dbl>\n1                       28.6                              45.5\n\nFinally, we generate the decision table associated with the selected design for use in making decisions at each interim analysis during the conduct of the trial. Because of the computational time involved, the results of the below example code are available as a dataset called two_sample_decision_tbl included in the ppseq package. In the results table, we see that at the first interim futility look after just 5 patients, we would not stop the trial. After the first 10 patients we would stop the trial if there were 0 responses, and so on. At the end of the trial when all 95 patients have accrued, we would declare the treatment promising of further study if there were greater than or equal to 14 responses.\n\n\nset.seed(123)\n\ntwo_sample_decision_tbl <- \n  calc_decision_rules(\n    n = cbind(seq(10, 50, 10), seq(10, 50, 10)),\n    N = c(50, 50),\n    theta = 0.92, \n    ppp = 0.05, \n    p0 = NULL, \n    direction = \"greater\", \n    delta = 0, \n    prior = c(0.5, 0.5), \n    S = 5000\n    )\n\n\nThe resulting table contains all possible combinations of responses in the two arms, so is quite long. We can visualize the resulting decision rules by passing the results of the call to the calc_decision_rules() function to the plot() function, which returns a faceted plot according to the interim look. The color denotes whether the trial should proceed (green) or stop (red) for a given combination of number of responses in the control arm (x-axis) and number of responses on the experimental arm (y-axis). For example, we see that at the second interim look when there are 20 patients enrolled on each arm, if there were 10 responses in the control arm, we would stop the trial if there were 8 or fewer responses on the experimental arm; otherwise we would proceed with enrollment.\n\n\nggplotly(\n  plot(two_sample_decision_tbl, plotly = FALSE) + scale_fill_viridis_d()\n)\n\n\n\n\nFigure 2: Plot of decision rules made with {plotly}. Each facet is for the total sample size at a pre-specified interim analysis. The number of responses in the control arm is on the x-axis and the number of responses in the experimental arm is on the y-axis. The fill color indicates whether the trial should stop (purple) or proceed (yellow) for a given combination of number of responses in the two arms at each interim analysis. At a given interim analysis, one can look up the number of observed responses to make a decision about whether the trial should stop or proceed enrollment.\n\n\n\n6 Summary\nWith the focus of early stage clinical trial research in oncology shifting away from the study of cytotoxic treatments and toward immunotherapies and other non-cytotoxic treatments, new approaches to clinical trial design are needed that move beyond the traditional search for the maximum tolerated dose (Hobbs et al. 2019). Bayesian sequential predictive probability monitoring provides a natural and flexible way to expand the number of patients studied in phase 1 or to design phase 2 trials that allow for efficient early stopping for futility while maintaining control of type I error and power. The ppseq package implements functionality to evaluate a range of posterior and predictive thresholds for a given study design and identify the optimal design based on accuracy (i.e. type I error and power) or efficiency (i.e. average sample sizes under the null and alternative). Interactive visualization options are provided to ease comparison of the resulting design options. Once an ideal design is selected, a table of decision rules can be obtained to make trial conduct simple and straightforward.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-017.zip\nCRAN packages used\nppseq, ggplot2, plotly, future, furrr\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, Phylogenetics, Spatial, TeachingStatistics, WebTechnologies\n\n\nH. Bengtsson. A Unifying Framework for Parallel and Distributed Processing in R using Futures. The R Journal, 13(2): 273–291, 2021. URL https://doi.org/10.32614/RJ-2021-048.\n\n\nD. D. G. Bugano, K. Hess, D. L. F. Jardim, A. Zer, F. Meric-Bernstam, L. L. Siu, A. R. A. Razak and D. S. Hong. Use of expansion cohorts in phase I trials and probability of success in phase II for 381 anticancer drugs. Clin Cancer Res, 23(15): 4020–4026, 2017. DOI 10.1158/1078-0432.Ccr-16-2354.\n\n\nA. Dmitrienko and M. D. Wang. Bayesian predictive approach to interim monitoring in clinical trials. Stat Med, 25(13): 2178–95, 2006. DOI 10.1002/sim.2204.\n\n\nB. P. Hobbs, P. C. Barata, Y. Kanjanapan, C. J. Paller, J. Perlmutter, G. R. Pond, T. M. Prowell, E. H. Rubin, L. K. Seymour, N. A. Wages, et al. Seamless Designs: Current Practice and Considerations for Early-Phase Drug Development in Oncology. J Natl Cancer Inst, 111(2): 118–128, 2019.\n\n\nB. P. Hobbs, N. Chen and J. J. Lee. Controlled multi-arm platform design using predictive probability. Stat Methods Med Res, 27(1): 65–78, 2018.\n\n\nL. Khoja, M. O. Butler, S. P. Kang, S. Ebbinghaus and A. M. Joshua. Pembrolizumab. J Immunother Cancer, 3: 36, 2015.\n\n\nJ. J. Lee and D. D. Liu. A predictive probability design for phase II cancer clinical trials. Clin Trials, 5(2): 93–106, 2008. DOI 10.1177/1740774508089279.\n\n\nA. Manji, I. Brana, E. Amir, G. Tomlinson, I. F. Tannock, P. L. Bedard, A. Oza, L. L. Siu and A. R. Razak. Evolution of clinical trial design in early drug development: Systematic review of expansion cohort use in single-agent phase I cancer trials. J Clin Oncol, 31(33): 4260–7, 2013. DOI 10.1200/jco.2012.47.4957.\n\n\nR. C. Pestana, S. Sen, B. P. Hobbs and D. S. Hong. Histology-agnostic drug development - considering issues beyond the tissue. Nat Rev Clin Oncol, 17(9): 555–568, 2020.\n\n\nD. P. Petrylak, T. Powles, J. Bellmunt, F. Braiteh, Y. Loriot, R. Morales-Barrera, H. A. Burris, J. W. Kim, B. Ding, C. Kaiser, et al. Atezolizumab (MPDL3280A) monotherapy for patients with metastatic urothelial cancer: Long-term outcomes from a phase 1 study. JAMA Oncol, 4(4): 537–544, 2018. DOI 10.1001/jamaoncol.2017.5440.\n\n\nT. Powles, I. Durán, M. S. van der Heijden, Y. Loriot, N. J. Vogelzang, U. De Giorgi, S. Oudard, M. M. Retz, D. Castellano, A. Bamias, et al. Atezolizumab versus chemotherapy in patients with platinum-treated locally advanced or metastatic urothelial carcinoma (IMvigor211): A multicentre, open-label, phase 3 randomised controlled trial. Lancet, 391(10122): 748–757, 2018. DOI 10.1016/s0140-6736(17)33297-x.\n\n\nT. Powles, J. P. Eder, G. D. Fine, F. S. Braiteh, Y. Loriot, C. Cruz, J. Bellmunt, H. A. Burris, D. P. Petrylak, S. L. Teng, et al. MPDL3280A (anti-PD-L1) treatment leads to clinical activity in metastatic bladder cancer. Nature, 515(7528): 558–62, 2014. DOI 10.1038/nature13904.\n\n\nJ. E. Rosenberg, J. Hoffman-Censits, T. Powles, M. S. van der Heijden, A. V. Balar, A. Necchi, N. Dawson, P. H. O’Donnell, A. Balmanoukian, Y. Loriot, et al. Atezolizumab in patients with locally advanced and metastatic urothelial carcinoma who have progressed following treatment with platinum-based chemotherapy: A single-arm, multicentre, phase 2 trial. Lancet, 387(10031): 1909–20, 2016. DOI 10.1016/s0140-6736(16)00561-4.\n\n\nB. R. Saville, J. T. Connor, G. D. Ayers and J. Alvarez. The utility of bayesian predictive probabilities for interim monitoring of clinical trials. Clin Trials, 11(4): 485–493, 2014. DOI 10.1177/1740774514531352.\n\n\nC. Sievert. Interactive web-based data visualization with R, plotly, and shiny. Chapman; Hall/CRC, 2020. URL https://plotly-r.com.\n\n\nD. Vaughan and M. Dancho. furrr: Apply mapping functions in parallel using futures. 2021. URL https://CRAN.R-project.org/package=furrr. R package version 0.2.2.\n\n\nH. Wickham. ggplot2: Elegant graphics for data analysis. Springer-Verlag New York, 2016. URL https://ggplot2.tidyverse.org.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:41+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-018/",
    "title": "pCODE: Estimating Parameters of ODE Models",
    "description": "The ordinary differential equation (ODE) models are prominent to characterize the mechanism of dynamical systems with various applications in biology, engineering, and many other areas. While the form of ODE models is often proposed based on the understanding or assumption of the dynamical systems, the values of ODE model parameters are often unknown. Hence, it is of great interest to estimate the ODE parameters once the observations of dynamic systems become available. The parameter cascade method initially proposed by [@parcascade] is shown to provide an accurate estimation of ODE parameters from the noisy observations at a low computational cost. This method is further promoted with the implementation in the R package CollocInfer by [@CollocInfer]. However, one bottleneck in using CollocInfer to implement the parameter cascade method is the tedious derivations and coding of the Jacobian and Hessian matrices required by the objective functions for doing estimation. We develop an R package pCODE to implement the parameter cascade method, which has the advantage that the users are not required to provide any Jacobian or Hessian matrices. Functions in the pCODE package accommodate users for estimating ODE parameters along with their variances and tuning the smoothing parameters. The package is demonstrated and assessed with four simulation examples with various settings. We show that pCODE offers a derivative-free procedure to estimate any ODE models where its functions are easy to understand and apply. Furthermore, the package has an online Shiny app at <https://pcode.shinyapps.io/pcode/>.",
    "author": [
      {
        "name": "Haixu Wang",
        "url": {}
      },
      {
        "name": "Jiguo Cao",
        "url": {}
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-018.zip\n\n\nG. Hooker, J. Ramsay and L. Xiao. CollocInfer: Collocation inference in differential equation models. Journal of Statistical Software, Articles, 75(2): 1–52, 2016. URL https://www.jstatsoft.org/v075/i02.\n\n\nJ. Ramsay, G. Hooker, D. Campbell and J. Cao. Parameter estimation for differential equations: A generalized smoothing approach. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 69(5): 741–796, 2007.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:41+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2023-019/",
    "title": "TreeSearch: Morphological Phylogenetic Analysis in R",
    "description": "TreeSearch is an R package for phylogenetic analysis, optimized for discrete character data. Tree search may be conducted using equal or implied step weights with an explicit (albeit inexact) allowance for inapplicable character entries, avoiding some of the pitfalls inherent in standard parsimony methods. Profile parsimony and user-specified optimality criteria are supported.\nA graphical interface, which requires no familiarity with R, is designed to help a user to improve the quality of datasets through critical review of underpinning character codings; and to obtain additional information from results by identifying and summarizing clusters of similar trees, mapping the distribution of trees, and removing 'rogue' taxa that obscure underlying relationships.\nTaken together, the package aims to support methodological rigour at each step of data collection, analysis, and the exploration of phylogenetic results.",
    "author": [
      {
        "name": "Martin R. Smith",
        "url": "https://smithlabdurham.github.io/"
      }
    ],
    "date": "2023-02-10",
    "categories": [],
    "contents": "\n\n\n\n1 Introduction\nEven in the phylogenomic era, morphological data make an important contribution\nto phylogenetic questions. Discrete phenotypic data improve the accuracy and\nresolution of phylogenetic reconstruction even when outnumbered by molecular\ncharacters, and are the only way to incorporate the unique perspective on\nhistorical events that fossil taxa provide (Wiens 2004; Wortley and Scotland 2006; Koch and Parry 2020; Asher and Smith 2022).\nOne challenge with morphological analysis is the treatment of inapplicable\ncharacter states: for example, ‘tail colour’ cannot logically be ascribed either\nof the states ‘red’ or ‘blue’ in a taxon that lacks a tail (Maddison 1993). This\nsituation can profoundly mislead phylogenetic analysis, and is not handled\nappropriately by any standard Markov model or parsimony method.\nSolutions to this issue have recently been proposed (De Laet 2005; Brazeau et al. 2019; Tarasov 2019, 2022; Goloboff et al. 2021; Hopkins and St. John 2021). Where a single\n‘principal’ character (e.g. ‘tail’) exhibits \\(n\\) ‘contingent’ characters (e.g.\n‘tail colour’, ‘tail covering’), ‘exact’ solutions (Tarasov 2019, 2022; Goloboff et al. 2021) require the construction of multi-state hierarchies containing\n\\(O(2^n)\\) entries, meaning that analysis is only computationally tractable for\nsimple hierarchies with few contingent characters. Moreover, these approaches\ncannot accommodate characters that are contingent on more than one principal\ncharacter: for example, characters describing appendages on a differentiated\nhead may be contingent on the presence of the two characters ‘appendages’ and\n‘differentiated head’.\nSuch situations can be approached using the flexible parsimony approximation\nproposed by Brazeau et al. (2019). TreeSearch scores trees using the “Morphy” C\nimplementation of this algorithm (Brazeau et al. 2017). Morphy implements tree search\nunder equal step weights. TreeSearch additionally implements implied step\nweighting (Goloboff 1993), a method which consistently finds more accurate and\nprecise trees than equal weights parsimony (Goloboff et al. 2008, 2018a; Smith 2019a).\nThere has been lively discussion as to whether, with the rise of probabilistic\napproaches, parsimony remains a useful tool for morphological phylogenetics\n(e.g. O’Reilly et al. 2016; Brown et al. 2017; Puttick et al. 2017; Goloboff et al. 2018b; Sansom et al. 2018).\nNotwithstanding scenarios that go beyond the limits of parsimony, such as the\nsimultaneous incorporation of stratigraphic data and other prior knowledge (e.g. Guenser et al. 2021), neither parsimony nor probabilistic methods consistently recover\n‘better’ trees when gains in accuracy are balanced against losses in precision\n(Smith 2019a). Even if probabilistic methods may eventually be improved through\nthe creation of more sophisticated models that better reflect the nature of\nmorphological data (Goloboff et al. 2018a; Tarasov 2019, 2022), parsimony\nanalysis remains a useful tool – not only because treatments of inapplicable\ncharacter states are presently available, but also because it facilitates a\ndeeper understanding of the underpinning data by emphasizing the reciprocal\nrelationship between a tree and the synapomorphies that it implies.\nWhatever method is used to find phylogenetic trees, a single consensus tree may\nfail to convey all the signal in a set of phylogenetic results (Wilkinson 1994, 1996, 2003). A set of optimal trees can be better\ninterpreted by examining consensus trees generated from clusters of similar\ntrees (Stockham et al. 2002); by exploring tree space (Wright and Lloyd 2020; Smith 2022a) and by\nautomatically identifying, annotating and removing ‘wildcard’ taxa (Smith 2022b)\nwhose ‘rogue’ behaviour may reflect underlying character conflict or ambiguity\n(Kearney 2002). These methods are not always easy to integrate into phylogenetic\nworkflows, so are not routinely included in empirical studies.\n\n\n\nFigure 1: Flow charts summarizing key functions available in TreeSearch.\n\n\n\nTreeSearch provides functions that allow researchers to engage with the three\nmain aspects of morphological phylogenetic analysis: dataset construction and\nvalidation; phylogenetic search (including with inapplicable data); and the\ninterrogation of optimal tree sets (Fig. 1). These functions\ncan be accessed via the R command-line, as documented within the package and at\nms609.github.io/TreeSearch, or through a\ngraphical user interface (GUI). The GUI includes options to export a log of\nexecuted commands as a fully reproducible R script, and to save outputs in\ngraphical, Nexus or Newick formats.\n2 Implementation\nTree scoring\nTreeSearch can score trees using equal weights, implied weighting\n(Goloboff 1993), or profile parsimony (Faith and Trueman 2001). The function TreeLength()\ncalculates tree score using the “Morphy” phylogenetic library (Brazeau et al. 2017),\nwhich implements the Fitch (1971) and Brazeau et al. (2019) algorithms. Morphy returns the\nequal weights parsimony score of a tree against a given dataset. Implied weights\nand profile parsimony scores are computed by first making a separate call to\nMorphy for each character in turn, passed as a single-character dataset; then\npassing this value to the appropriate weighting formula and summing the total\nscore over all characters.\nImplied weighting (Goloboff 1993) is an approximate method that treats each\nadditional step (i.e. transition between tokens) in a character as less\nsurprising – and thus requiring less penalty – than the previous step. Each\nadditional step demonstrates that a character is less reliable for phylogenetic\ninference, and thus more likely to contain additional homoplasy. The score of a\ntree under implied weighting is \\(\\sum{\\frac{e_i}{e_i+k}}\\), where \\(e_i\\) denotes\nthe number of extra steps observed in character \\(i\\), and is derived by\nsubtracting the minimum score that the character can obtain on any tree from the\nscore observed on the tree in question (Goloboff 1993). The minimum length of a\ntree is one less than the number of unique tokens (excluding the inapplicable\ntoken ‘-’) that must be present.\nProfile parsimony (Faith and Trueman 2001) represents an alternative formulation of how\nsurprising each additional step in a character is (Arias and Miranda-Esquivel 2004): the penalty\nassociated with each additional step in a character is a function of the\nprobability that a character will fit at least as well as is observed on a\nuniformly selected tree. On this view, an additional step is less surprising if\nobserved in a character where there are more opportunities to observe homoplasy,\nwhether because a character contains fewer ambiguous codings (a motivation for\nthe ‘extended’ implied weighting of Goloboff (2014)) or because states are\ndistributed more evenly in a character, whose higher phylogenetic information\ncontent (Thorley et al. 1998) corresponds to a lower proportion of trees in which no\nadditional steps are observed.\nTreeSearch calculates the profile parsimony score by computing the logarithm of\nthe number of trees onto which a character can be mapped using \\(m\\) steps, using\ntheorem 1 of Carter et al. (1990). As computation for higher numbers of states\n(Maddison and Slatkin 1991) is more computationally complex, the present implementation is\nrestricted to characters that contain two informative applicable states, and\nuses the Fitch (1971) algorithm.\nTree search\nThe TreeSearch GUI uses the routine MaximizeParsimony() to search for optimal\ntrees using tree bisection and reconnection (TBR) searches and the parsimony\nratchet (Nixon 1999). This goes beyond the heuristic tree search implementation\nin the R package phangorn (Schliep 2011) by using compiled C++ code to\nrearrange trees, dramatically accelerating computation, and thus increasing the\nscale of dataset that can be analysed in reasonable time; and in supporting TBR\nrearrangements, which explore larger neighbourhoods of tree space: TBR evaluates\nmore trees than nearest-neighbour interchanges or subtree pruning and\nregrafting, leading to additional computational expense that is offset by a\ndecreased likelihood that search will become trapped in a local optimum\n(Goeffon et al. 2008; Whelan and Money 2010).\nBy default, search begins from a greedy addition tree generated by function\nAdditionTree(), which queues taxa in a random order, then attaches each taxon\nin turn to the growing tree at the most parsimonious location. Search may also\nbe started from neighbour-joining trees, or the results of a previous search.\nSearch commences by conducting TBR rearrangements – a hill-climbing approach\nthat locates a locally optimal tree from which no tree accessible by a single\nTBR rearrangement has a better score. A TBR iteration breaks a randomly selected\nedge in the focal tree, and reconnects each possible pair of edges in the\nresultant sub-trees to produce a list of candidate trees. Entries that are\ninconsistent with user-specified topological constraints are removed; remaining\ntrees are inserted into a queue and scored in a random sequence. If the score of\na candidate tree is at least as good as the best yet encountered (within the\nbounds of an optional tolerance parameter \\(\\epsilon\\), which allows the retention\nof almost-optimal trees in order to improve accuracy – see e.g. Smith (2019a)),\nthis tree is used as the starting point for a new TBR iteration. Otherwise, the\nnext tree in the list is considered. TBR search continues until the best score\nis found a specified number of times; a specified number of TBR break points\nhave been evaluated without any improvement to tree score; or a set amount of\ntime has passed.\nWhen TBR search is complete, iterations of the parsimony ratchet (Nixon 1999)\nare conducted in order to search areas of tree space that are separated from the\nbest tree yet found by ‘valleys’ that cannot be traversed by TBR rearrangements\nwithout passing through trees whose optimality score is below the threshold for\nacceptance. Each ratchet iteration begins by resampling the original matrix. A\nround of TBR search is conducted using this resampled matrix, and the tree thus\nproduced is used as a starting point for a new round of TBR search using the\noriginal data. After a specified number of ratchet iterations, an optional final\nround of TBR search allows a denser sampling of optimal trees from the final\nregion of tree space.\nA simple example search can be conducted using a morphological dataset included\nin the package, taken from Vinther et al. (2008):\n\n\nlibrary(\"TreeSearch\")\nvinther <- inapplicable.phyData[[\"Vinther2008\"]]\ntrees <- MaximizeParsimony(vinther, concavity = 10, tolerance = 0.05)\n\n\nThe MaximizeParsimony() command performs tree search under implied weights\nwith a concavity value of 10 (concavity = Inf would select equal weights),\nretaining any tree whose score is within 0.05 of the best score.\nThe resulting trees can be summarised according to their scores (optionally,\nagainst a different dataset or under a different weighting strategy, as\nspecified by concavity) and the iteration in which they were first hit:\n\n\nTreeLength(trees, dataset = vinther, concavity = 10) |> \n  signif() |>         # truncate non-significant digits\n  table()             # tabulate by score\n\n\n1.52814 1.54329  1.5641 \n      3      45       4 \n\nattr(trees, \"firstHit\")\n\n  seed  start ratch1 ratch2 ratch3 ratch4 ratch5 ratch6 ratch7  final \n     0     29      4      0     10      7      2      0      0      0 \n\nMore flexible, if less computationally efficient, tree searches can be conducted\nat the command line using the TreeSearch(), Ratchet() and Bootstrap()\ncommands, which support custom tree optimality criteria (e.g. Hopkins and St. John 2021).\nVisualization\nThe distribution of optimal trees, however obtained, can be visualized through\ninteractive mappings of tree space (Hillis et al. 2005; Smith 2022a).  The TreeSearch GUI supports the use of information theoretic distances\n(Smith 2020a); the quartet distance (Estabrook et al. 1985); or the Robinson–Foulds\ndistance (Robinson and Foulds 1981) to construct tree spaces, which are mapped into 2–12\ndimensions using principal coordinates analysis (Gower 1966). The degree to\nwhich a mapping faithfully depicts original tree-to-tree distances is measured\nusing the product of the trustworthiness and continuity metrics (Venna and Kaski 2001; Kaski et al. 2003; Smith 2022a), a composite score denoting the degree to which points\nthat are nearby when mapped are truly close neighbours (trustworthiness), and\nthe degree to which nearby points remain nearby when mapped (continuity).\nPlotting the minimum spanning tree – the shortest path that connects all trees\n(Gower and Ross 1969) – can highlight stress in a mapping (grey lines in Fig.\n2): the spatial relationships of trees are distorted in\nregions where the minimum spanning tree takes a circuitous route to connect\ntrees that are mapped close to one another (see fig. 1a–b in Smith 2022a).\n\n\n\nFigure 2: Three-dimensional map visualizing progress in a tree search in the TreeSearch GUI. Optimal trees belong to three statistically distinct clusters with good support (silhouette coefficient \\(>\\) 0.5), characterized by different relationships between certain taxa (plotting symbols). Although multiple ratchet iterations have visited each cluster, limited overlap between ratchet iterations suggests that a continuation of tree search may sample novel optimal trees. High trustworthiness and continuity values and a simple minimum spanning tree (grey) indicate that the mapping does not exhibit severe distortion. This figure depicts the tree space GUI display after loading the Wills et al. (2012) dataset; clearing previous trees from memory (sample n trees = 0); and starting a new search (Search→Configure) with equal step weighting and 101.5 max hits. 93 trees were sampled, coloured by “When first found”, with plotting symbols depicting “Relationships” between the specified taxa.\n\n\n\n\n\n\nTo relate the geometry of tree space to the underlying trees, each point in tree\nspace may be annotated according to the optimality score of its corresponding\ntree under a selected step weighting scheme; by the relationships between chosen\ntaxa that are inferred by that tree; and by the search iteration in which the\ntree was first found by tree search (Fig. 2).\nAnnotating trees by the iteration in which they were first found allows a user\nto evaluate whether a continuation of tree search is likely to yield more\noptimal trees. For example, if the retained trees were only recently found, the\nsearch may not yet have located a global optimum. Alternatively, if certain\nregions of tree space are visited only by a single ratchet iteration, it is\npossible that further isolated ‘islands’ (Bastert et al. 2002) remain to be found;\ncontinuing tree search until subsequent ratchet iterations no longer locate new\nclusters of trees will reduce the chance that optimal regions of tree space\nremain unvisited.\nAs the identification of clusters from mappings of tree space can be misleading\n(Smith 2022a), TreeSearch identifies clusters of trees from tree-to-tree\ndistances using K-means++ clustering, partitioning around medoids and\nhierarchical clustering with minimax linkage (Hartigan and Wong 1979; Murtagh 1983; Arthur and Vassilvitskii 2007; Bien and Tibshirani 2011; Maechler et al. 2019). Clusterings are evaluated using the\nsilhouette coefficient, a measure of the extent of overlap between clusters\n(Kaufman and Rousseeuw 1990). The clustering with the highest silhouette coefficient is\ndepicted if the silhouette coefficient exceeds a user-specified threshold; the\ninterpretation of the chosen threshold according to Kaufman and Rousseeuw (1990) is displayed to\nthe user. Plotting a separate consensus tree for each cluster often reveals\nphylogenetic information that is concealed by polytomies in the single ‘plenary’\nconsensus of all optimal trees (Stockham et al. 2002).\nPlenary consensus trees can also lack resolution because of wildcard or ‘rogue’\ntaxa, in which conflict or ambiguity in their character codings leads to an\nunsettled phylogenetic position (Wilkinson 1994, 2003; Kearney 2002).\nTreeSearch detects rogue taxa using a heuristic approach (Smith 2022b) that\nseeks to maximize the phylogenetic information content (sensu Thorley et al. 1998) of\na consensus tree created after removing rogue taxa from input trees. The\nposition of an excluded taxon is portrayed by shading each edge or node of the\nconsensus according to the number of times the specified taxon occurs at that\nposition on an underlying tree\n[Fig. 3; after\nKlopfstein and Spasojevic (2019)], equivalent to the ‘branch attachment frequency’ of Phyutility\n(Smith and Dunn 2008).\nIdentifying taxa with an unstable position, and splits with low support, can\nhelp an investigator to critically re-examine character codings; to this end,\neach edge of the resulting consensus can be annotated with the frequency of the\nsplit amongst the tree set, or with a concordance factor (Minh et al. 2020) denoting\nthe strength of support from the underlying dataset.\n\n\n\n\n\n\nFigure 3: Reduced consensus of 48 cladograms generated by analysis of data from Wills et al. (2012) under different parsimony methods by Brazeau et al. (2019), as displayed in the TreeSearch graphical user interface. Removal of taxa reveals strong support for relationships that would otherwise be masked by rogues such as Palaeoscolex, whose position in optimal trees is marked by the highlighted edges. The GUI state can be reproduced by selecting the options displayed in the figure.\n\n\n\nDataset review\nUltimately, the quality of a dataset plays a central role in determining the\nreliability of phylogenetic results, with changes to a relatively small number\nof character codings potentially exhibiting an outsized impact on reconstructed\ntopologies (Goloboff and Sereno 2021). Nevertheless, dataset quality does not always\nreceive commensurate attention (Simões et al. 2017). One step towards improving the\nrigour of morphological datasets is to annotate each cell in a dataset with an\nexplicit justification for each taxon’s coding (Sereno 2009), which can be\naccomplished in Nexus-formatted data files (Maddison et al. 1997) using software such\nas MorphoBank (O’Leary and Kaufman 2011).\nTreeSearch presents such annotations alongside a reconstruction of each\ncharacter’s states on a specified tree, with inapplicable states mapped\naccording to the algorithm of Brazeau et al. (2019). Neomorphic (presence/absence) and\ntransformational characters (Sereno 2007) are distinguished by reserving the\ntoken 0 to solely denote the absence of a neomorphic character, with tokens\n1 … n used to denote the \\(n\\) states of a transformational character\n(Brazeau et al. 2019). In order to identify character codings that contribute to taxon\ninstability, each leaf is coloured according to its mean contribution to tree\nlength for the visualized character (Pol and Escapa 2009).\nThis visualization of reconstructed character transitions can help to identify\ncases where the formulation of characters has unintended consequences\n(Wilkinson 1995; Brazeau 2011); where inapplicable states have been\ninconsistently applied (Brazeau et al. 2019); where taphonomic absence is wrongly coded\nas biological absence (Donoghue and Purnell 2009); where previous datasets are uncritically\nrecycled (Jenner 2001); or where taxa are coded with more confidence than a\ncritical evaluation of available evidence can truly support. Insofar as the\noptimal tree and the underlying characters are reciprocally illuminating\n(Mooi and Gill 2016), successive cycles of phylogenetic analysis and character\nre-formulation can improve the integrity of morphological datasets, and thus\nincrease their capacity to yield meaningful phylogenetic results (Hennig 1966).\n3 Availability\nTreeSearch can be installed through the Comprehensive R Archive\nNetwork (CRAN) using\ninstall.packages(\"TreeSearch\"); the graphical user\ninterface is launched with the command\nTreeSearch::EasyTrees().\nThe package has been tested on Windows 10, Mac OS X 10 and Ubuntu 20, and\nrequires only packages available from the CRAN repository.\nSource code is available at https://github.com/ms609/TreeSearch/,\nand is permanently archived at Zenodo\n(https://dx.doi.org/10.5281/zenodo.1042590). Online documentation is available\nat https://ms609.github.io/TreeSearch/.\n4 Acknowledgements\nI thank Alavya Dhungana and Joe Moysiuk for feedback on preliminary versions of\nthe software, and Martin Brazeau and anonymous referees for comments on the\nmanuscript. Functionality in TreeSearch employs the underlying R \npackages ape (Paradis and Schliep 2019), phangorn (Schliep 2011),\nQuartet (Sand et al. 2014; Smith 2019b), Rogue (Smith 2022b),\nshiny (Chang et al. 2021), shinyjs (Attali 2020), TreeDist\n(Smith 2020b), and TreeTools (Smith 2019c). Icons from R used under\nGPL-3; Font Awesome, CC-BY-4.0.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2023-019.zip\nCRAN packages used\nTreeSearch, phangorn, ape, Quartet, Rogue, shiny, shinyjs, TreeDist, TreeTools\nCRAN Task Views implied by cited packages\nEnvironmetrics, Phylogenetics, WebTechnologies\n\n\nJ. S. Arias and D. R. Miranda-Esquivel. Profile parsimony (PP): An analysis under implied weights (IW). Cladistics, 20(1): 56–63, 2004. DOI 10.1111/j.1096-0031.2003.00001.x.\n\n\nD. Arthur and S. Vassilvitskii. K-means++: The advantages of careful seeding. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages. 1027–1035 2007. USA: Society for Industrial and Applied Mathematics. URL https://dl.acm.org/doi/10.5555/1283383.1283494.\n\n\nR. J. Asher and M. R. Smith. Phylogenetic signal and bias in paleontology. Systematic Biology, 71(4): 986–1008, 2022. DOI 10.1093/sysbio/syab072.\n\n\nD. Attali. Shinyjs: Easily improve the user experience of your shiny apps in seconds. 2020. URL https://CRAN.R-project.org/package=shinyjs. R package version 2.0.0.\n\n\nO. Bastert, D. Rockmore, P. F. Stadler and G. Tinhofer. Landscapes on spaces of trees. Applied Mathematics and Computation, 131(2-3): 439–459, 2002. DOI 10.1016/S0096-3003(01)00164-3.\n\n\nJ. Bien and R. Tibshirani. Hierarchical clustering with prototypes via minimax linkage. Journal of the American Statistical Association, 106(495): 1075–1084, 2011. DOI 10.1198/jasa.2011.tm10183.\n\n\nM. D. Brazeau. Problematic character coding methods in morphology and their effects. Biological Journal of the Linnean Society, 104(3): 489–498, 2011. DOI 10.1111/j.1095-8312.2011.01755.x.\n\n\nM. D. Brazeau, T. Guillerme and M. R. Smith. An algorithm for morphological phylogenetic analysis with inapplicable data. Systematic Biology, 68(4): 619–631, 2019. DOI 10.1093/sysbio/syy083.\n\n\nM. D. Brazeau, M. R. Smith and T. Guillerme. MorphyLib: A library for phylogenetic analysis of categorical trait data with inapplicability. 2017. DOI 10.5281/zenodo.815372.\n\n\nJ. W. Brown, C. Parins-Fukuchi, G. W. Stull, O. M. Vargas and S. A. Smith. Bayesian and likelihood phylogenetic reconstructions of morphological traits are not discordant when taking uncertainty into consideration: A comment on Puttick et al. Proceedings of the Royal Society B: Biological Sciences, 284(1864): 20170986, 2017. DOI 10.1098/rspb.2017.0986.\n\n\nM. Carter, M. Hendy, D. Penny, L. A. Székely and N. C. Wormald. On the distribution of lengths of evolutionary trees. SIAM Journal on Discrete Mathematics, 3(1): 38–47, 1990. DOI 10.1137/0403005.\n\n\nW. Chang, J. Cheng, J. J. Allaire, C. Sievert, B. Schloerke, Y.-H. Xie, J. Allen, J. McPherson, A. Dipert and B. Borges. Shiny: Web application framework for R. 2021. URL https://CRAN.R-project.org/package=shiny. R package version 1.7.1.\n\n\nJ. E. De Laet. Parsimony and the problem of inapplicables in sequence data. Parsimony, Phylogeny, and Genomics, 81–116, 2005. DOI 10.1093/acprof:oso/9780199297306.003.0006.\n\n\nP. C. J. Donoghue and M. A. Purnell. Distinguishing heat from light in debate over controversial fossils. BioEssays, 31(2): 178–189, 2009. DOI 10.1002/bies.200800128.\n\n\nG. F. Estabrook, F. R. McMorris and C. A. Meacham. Comparison of undirected phylogenetic trees based on subtrees of four evolutionary units. Systematic Zoology, 34(2): 193–200, 1985. DOI 10.2307/sysbio/34.2.193.\n\n\nD. P. Faith and J. W. H. Trueman. Towards an inclusive philosophy for phylogenetic inference. Systematic Biology, 50(3): 331–350, 2001. DOI 10.1080/10635150118627.\n\n\nW. M. Fitch. Toward defining the course of evolution: Minimum change for a specific tree topology. Systematic Biology, 20(4): 406–416, 1971. DOI 10.1093/sysbio/20.4.406.\n\n\nA. Goeffon, J.-M. Richer and Jin-Kao Hao. Progressive Tree Neighborhood applied to the Maximum Parsimony problem. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 5(1): 136–145, 2008. DOI 10.1109/TCBB.2007.1065.\n\n\nP. A. Goloboff. Estimating character weights during tree search. Cladistics, 9(1): 83–91, 1993. DOI 10.1111/j.1096-0031.1993.tb00209.x.\n\n\nP. A. Goloboff. Extended implied weighting. Cladistics, 30(3): 260–272, 2014. DOI 10.1111/cla.12047.\n\n\nP. A. Goloboff, J. M. Carpenter, J. S. Arias and D. R. M. Esquivel. Weighting against homoplasy improves phylogenetic analysis of morphological data sets. Cladistics, 24(5): 758–773, 2008. DOI 10.1111/j.1096-0031.2008.00209.x.\n\n\nP. A. Goloboff, J. D. Laet, D. Ríos-Tamayo and C. A. Szumik. A reconsideration of inapplicable characters, and an approximation with step-matrix recoding. Cladistics, 37(5): 596–629, 2021. DOI 10.1111/cla.12456.\n\n\nP. A. Goloboff and P. C. Sereno. Comparative cladistics: Identifying the sources for differing phylogenetic results between competing morphology-based datasets. Journal of Systematic Palaeontology, 1–26, 2021. DOI 10.1080/14772019.2021.1970038.\n\n\nP. A. Goloboff, A. Torres and J. S. Arias. Weighted parsimony outperforms other methods of phylogenetic inference under models appropriate for morphology. Cladistics, 34(4): 407–437, 2018a. DOI 10.1111/cla.12205.\n\n\nP. A. Goloboff, A. Torres Galvis and J. S. Arias. Parsimony and model-based phylogenetic methods for morphological data: Comments on O’Reilly et al. Palaeontology, 61(4): 625–630, 2018b. DOI 10.1111/pala.12353.\n\n\nJ. C. Gower. Some distance properties of latent root and vector methods used in multivariate analysis. Biometrika, 53(3/4): 325–338, 1966. DOI 10.2307/2333639.\n\n\nJ. C. Gower and G. J. S. Ross. Minimum spanning trees and single linkage cluster analysis. Journal of the Royal Statistical Society. Series C (Applied Statistics), 18(1): 54–64, 1969. DOI 10.2307/2346439.\n\n\nP. Guenser, R. C. M. Warnock, W. Pett, P. C. J. Donoghue and E. Jarochowska. Does time matter in phylogeny? A perspective from the fossil record. bioR\\(\\chi\\)iv, 2021. DOI 10.1101/2021.06.11.445746.\n\n\nJ. A. Hartigan and M. A. Wong. Algorithm AS 136: A K-means clustering algorithm. Journal of the Royal Statistical Society. Series C (Applied Statistics), 28(1): 100–108, 1979. DOI 10.2307/2346830.\n\n\nW. Hennig. Phylogenetic systematics. Urbana: The University of Illinois Press, 1966.\n\n\nD. M. Hillis, T. A. Heath and K. St. John. Analysis and visualization of tree space. Systematic Biology, 54(3): 471–482, 2005. DOI 10.1080/10635150590946961.\n\n\nM. J. Hopkins and K. St. John. Incorporating hierarchical characters into phylogenetic analysis. Systematic Biology, 70(6): 1163–1180, 2021. DOI 10.1093/sysbio/syab005.\n\n\nR. A. Jenner. Bilaterian phylogeny and uncritical recycling of morphological data sets. Systematic Biology, 50(5): 730–742, 2001. DOI 10.1080/106351501753328857.\n\n\nS. Kaski, J. Nikkilä, M. Oja, J. Venna, P. Törönen and E. Castrén. Trustworthiness and metrics in visualizing similarity of gene expression. BMC Bioinformatics, 4: 48, 2003. DOI 10.1186/1471-2105-4-48.\n\n\nL. Kaufman and P. J. Rousseeuw. Partitioning around medoids (Program PAM). In Finding groups in data: An introduction to cluster analysis, pages. 68–125 1990. John Wiley & Sons, Ltd.\n\n\nM. Kearney. Fragmentary taxa, missing data, and ambiguity: Mistaken assumptions and conclusions. Systematic Biology, 51(2): 369–381, 2002. DOI 10.1080/10635150252899824.\n\n\nS. Klopfstein and T. Spasojevic. Illustrating phylogenetic placement of fossils using RoguePlots: An example from ichneumonid parasitoid wasps (Hymenoptera, Ichneumonidae) and an extensive morphological matrix. PLoS ONE, 14(4): e0212942, 2019. DOI 10.1371/journal.pone.0212942.\n\n\nN. M. Koch and L. A. Parry. Death is on our side: Paleontological data drastically modify phylogenetic hypotheses. Systematic Biology, 69(6): 1052–1067, 2020. DOI 10.1093/sysbio/syaa023.\n\n\nD. R. Maddison, D. L. Swofford and W. P. Maddison. NEXUS: An extensible file format for systematic information. Systematic Biology, 46(4): 590–621, 1997. DOI 10.1093/sysbio/46.4.590.\n\n\nW. P. Maddison. Missing data versus missing characters in phylogenetic analysis. Systematic Biology, 42(4): 576–581, 1993. DOI 10.1093/sysbio/42.4.576.\n\n\nW. P. Maddison and M. Slatkin. Null models for the number of evolutionary steps in a character on a phylogenetic tree. Evolution, 45(5): 1184–1197, 1991. DOI 10.1111/j.1558-5646.1991.tb04385.x.\n\n\nM. Maechler, P. Rousseeuw, A. Struyf, M. Hubert and K. Hornik. Cluster: Cluster Analysis Basics and Extensions. Comprehensive R Archive Network, 2.1.0: 2019.\n\n\nB. Q. Minh, M. W. Hahn and R. Lanfear. New methods to calculate concordance factors for phylogenomic datasets. Molecular Biology and Evolution, 37(9): 2727–2733, 2020. DOI 10.1093/molbev/msaa106.\n\n\nR. D. Mooi and A. C. Gill. Hennig’s auxiliary principle and reciprocal illumination revisited. In The Future of Phylogenetic Systematics, Eds D. Williams, M. Schmitt and Q. Wheeler pages. 258–285 2016. Cambridge: Cambridge University Press. DOI 10.1017/CBO9781316338797.013.\n\n\nF. Murtagh. A survey of recent advances in hierarchical clustering algorithms. The Computer Journal, 26(4): 354–359, 1983. DOI 10.1093/comjnl/26.4.354.\n\n\nK. C. Nixon. The Parsimony Ratchet, a new method for rapid parsimony analysis. Cladistics, 15(4): 407–414, 1999. DOI 10.1111/j.1096-0031.1999.tb00277.x.\n\n\nM. A. O’Leary and S. Kaufman. MorphoBank: Phylophenomics in the “cloud.” Cladistics, 27(5): 529–537, 2011. DOI 10.1111/j.1096-0031.2011.00355.x.\n\n\nJ. E. O’Reilly, M. N. Puttick, L. Parry, A. R. Tanner, J. E. Tarver, J. Fleming, D. Pisani and P. C. J. Donoghue. Bayesian methods outperform parsimony but at the expense of precision in the estimation of phylogeny from discrete morphological data. Biology Letters, 12(4): 20160081, 2016. DOI 10.1098/rsbl.2016.0081.\n\n\nE. Paradis and K. Schliep. Ape 5.0: An environment for modern phylogenetics and evolutionary analyses in R. Bioinformatics, 35(3): 526–528, 2019. DOI 10.1093/bioinformatics/bty633.\n\n\nD. Pol and I. H. Escapa. Unstable taxa in cladistic analysis: Identification and the assessment of relevant characters. Cladistics, 25(5): 515–527, 2009. DOI 10.1111/j.1096-0031.2009.00258.x.\n\n\nM. N. Puttick, J. E. O’Reilly, A. R. Tanner, J. F. Fleming, J. Clark, L. Holloway, J. Lozano-Fernandez, L. A. Parry, J. E. Tarver, D. Pisani, et al. Uncertain-tree: Discriminating among competing approaches to the phylogenetic analysis of phenotype data. Proceedings of the Royal Society B: Biological Sciences, 284(1846): 20162290, 2017. DOI 10.1098/rspb.2016.2290.\n\n\nD. F. Robinson and L. R. Foulds. Comparison of phylogenetic trees. Mathematical Biosciences, 53(1-2): 131–147, 1981. DOI 10.1016/0025-5564(81)90043-2.\n\n\nA. Sand, M. K. Holt, J. Johansen, G. S. Brodal, T. Mailund and C. N. S. Pedersen. tqDist: A library for computing the quartet and triplet distances between binary or general trees. Bioinformatics, 30(14): 2079–2080, 2014. DOI 10.1093/bioinformatics/btu157.\n\n\nR. S. Sansom, P. G. Choate, J. N. Keating and E. Randle. Parsimony, not Bayesian analysis, recovers more stratigraphically congruent phylogenetic trees. Biology Letters, 14(6): 20180263, 2018. DOI 10.1098/rsbl.2018.0263.\n\n\nK. P. Schliep. Phangorn: Phylogenetic analysis in R. Bioinformatics, 27(4): 592–593, 2011. DOI 10.1093/bioinformatics/btq706.\n\n\nP. C. Sereno. Comparative cladistics. Cladistics, 25(6): 624–659, 2009. DOI 10.1111/j.1096-0031.2009.00265.x.\n\n\nP. C. Sereno. Logical basis for morphological characters in phylogenetics. Cladistics, 23(6): 565–587, 2007. DOI 10.1111/j.1096-0031.2007.00161.x.\n\n\nT. R. Simões, M. W. Caldwell, A. Palci and R. L. Nydam. Giant taxon-character matrices: Quality of character constructions remains critical regardless of size. Cladistics, 33(2): 198–219, 2017. DOI 10.1111/cla.12163.\n\n\nM. R. Smith. Bayesian and parsimony approaches reconstruct informative trees from simulated morphological datasets. Biology Letters, 15(2): 20180632, 2019a. DOI 10.1098/rsbl.2018.0632.\n\n\nM. R. Smith. Information theoretic Generalized Robinson–Foulds metrics for comparing phylogenetic trees. Bioinformatics, 36(20): 5007–5013, 2020a. DOI 10.1093/bioinformatics/btaa614.\n\n\nM. R. Smith. Quartet: Comparison of phylogenetic trees using quartet and bipartition measures. Comprehensive R Archive Network, doi:10.5281/zenodo.2536318, 2019b. DOI 10.5281/zenodo.2536318.\n\n\nM. R. Smith. Robust analysis of phylogenetic tree space. Systematic Biology, 71(5): 1255–1270, 2022a. DOI 10.1093/sysbio/syab100.\n\n\nM. R. Smith. TreeDist: Calculate and map distances between phylogenetic trees. Comprehensive R Archive Network, doi:10.5281/zenodo.3528123, 2020b. DOI 10.5281/zenodo.3528123.\n\n\nM. R. Smith. TreeTools: Create, modify and analyse phylogenetic trees. Comprehensive R Archive Network, doi:10.5281/zenodo.3522725, 2019c. DOI 10.5281/zenodo.3522725.\n\n\nM. R. Smith. Using information theory to detect rogue taxa and improve consensus trees. Systematic Biology, 71(5): 1088–1094, 2022b. DOI 10.1093/sysbio/syab099.\n\n\nS. A. Smith and C. W. Dunn. Phyutility: A phyloinformatics tool for trees, alignments and molecular data. Bioinformatics, 24(5): 715–716, 2008. DOI 10.1093/bioinformatics/btm619.\n\n\nC. Stockham, L.-S. Wang and T. Warnow. Statistically based postprocessing of phylogenetic analysis by clustering. Bioinformatics, 18(Suppl 1): S285–S293, 2002. DOI 10.1093/bioinformatics/18.suppl_1.S285.\n\n\nS. Tarasov. Integration of anatomy ontologies and evo-devo using structured Markov models suggests a new framework for modeling discrete phenotypic traits. Systematic Biology, 68(5): 698–716, 2019. DOI 10.1093/sysbio/syz005.\n\n\nS. Tarasov. New phylogenetic Markov models for inapplicable morphological characters. bioR\\(\\chi\\)iv, 2021.04.26.441495, 2022. DOI 10.1101/2021.04.26.441495.\n\n\nJ. L. Thorley, M. Wilkinson and M. Charleston. The information content of consensus trees. In Advances in Data Science and Classification, Eds A. Rizzi, M. Vichi and H.-H. Bock pages. 91–98 1998. Berlin: Springer. DOI 10.1007/978-3-642-72253-0_12.\n\n\nJ. Venna and S. Kaski. Neighborhood preservation in nonlinear projection methods: An experimental study. In Artificial Neural Networks ICANN 2001, Eds G. Dorffner, H. Bischof and K. Hornik pages. 485–491 2001. Berlin, Heidelberg: Springer. DOI 10.1007/3-540-44668-0_68.\n\n\nJ. Vinther, P. Van Roy and D. E. G. Briggs. Machaeridians are Palaeozoic armoured annelids. Nature, 451(7175): 185–188, 2008. DOI 10.1038/nature06474.\n\n\nS. Whelan and D. Money. The prevalence of multifurcations in tree-space and their implications for tree-search. Molecular Biology and Evolution, 27(12): 2674–2677, 2010. DOI 10.1093/molbev/msq163.\n\n\nJ. J. Wiens. The role of morphological data in phylogeny reconstruction. Systematic Biology, 53(4): 653–661, 2004. DOI 10.1080/10635150490472959.\n\n\nM. Wilkinson. A comparison of two methods of character construction. Cladistics, 11(3): 297–308, 1995. DOI 10.1111/j.1096-0031.1995.tb00091.x.\n\n\nM. Wilkinson. Common cladistic information and its consensus representation: Reduced Adams and reduced cladistic consensus trees and profiles. Systematic Biology, 43(3): 343–368, 1994. DOI 10.2307/2413673.\n\n\nM. Wilkinson. Majority-rule reduced consensus trees and their use in bootstrapping. Molecular Biology and Evolution, 13(3): 437–444, 1996. DOI 10.1093/oxfordjournals.molbev.a025604.\n\n\nM. Wilkinson. Missing entries and multiple trees: Instability, relationships, and support in parsimony analysis. Journal of Vertebrate Paleontology, 23(2): 311–323, 2003. DOI 10.1671/0272-4634(2003)023[0311:MEAMTI]2.0.CO;2.\n\n\nM. A. Wills, S. Gerber, M. Ruta and M. Hughes. The disparity of priapulid, archaeopriapulid and palaeoscolecid worms in the light of new data. Journal of Evolutionary Biology, 25(10): 2056–2076, 2012. DOI 10.1111/j.1420-9101.2012.02586.x.\n\n\nA. H. Wortley and R. W. Scotland. The effect of combining molecular and morphological data in published phylogenetic analyses. Systematic Biology, 55(4): 677–685, 2006. DOI 10.1080/10635150600899798.\n\n\nA. M. Wright and G. T. Lloyd. Bayesian analyses in phylogenetic palaeontology: Interpreting the posterior sample. Palaeontology, 63(6): 997–1006, 2020. DOI 10.1111/pala.12500.\n\n\n\n\n",
    "preview": "articles/RJ-2023-019/Flow.svg",
    "last_modified": "2023-11-07T21:31:41+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-042/",
    "title": "WLinfer: Statistical Inference for Weighted Lindley Distribution",
    "description": "New distributions are still being suggested for better fitting of a distribution to data, as it is one of the most fundamental problems in terms of the parametric approach. One of such is weighted Lindley (WL) distribution [@ghitany:2011]. Even though WL distribution has become increasingly popular as a possible alternative to traditional distributions such as gamma and log normal distributions, fitting it to data has rarely been addressed in existing R packages. This is the reason we present the [WLinfer](https://CRAN.R-project.org/package=WLinfer) package that implements overall statistical inference for WL distribution. In particular, WLinfer enables one to conduct the goodness of fit test, point estimation, bias correction, interval estimation, and the likelihood ratio test simply with the `WL` function which is at the core of this package. To assist users who are unfamiliar with WL distribution, we present a brief review followed by an illustrative example with R codes.",
    "author": [
      {
        "name": "Yu-Hyeong Jang",
        "url": {}
      },
      {
        "name": "SungBum Kim",
        "url": {}
      },
      {
        "name": "Hyun-Ju Jung",
        "url": {}
      },
      {
        "name": "Hyoung-Moon Kim (Corresponding author)",
        "url": {}
      }
    ],
    "date": "2023-01-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-042.zip\n\n\nM. E. Ghitany, F. Alqallaf, D. K. Al-Mutairi and H. A. Husain. A two-parameter weighted lindley distribution and its applications to survival data. Mathematics and Computers in Simulation, 81(6): 1190–1201, 2011.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-051/",
    "title": "Log Likelihood Ratios for Common Statistical Tests Using the likelihoodR Package",
    "description": "The **likelihoodR** package has been developed to allow users to obtain statistics according to the likelihood approach to statistical inference. Commonly used tests are available in the package, such as: *t* tests, ANOVA, correlation, regression and a range of categorical analyses. In addition, there is a sample size calculator for *t* tests, based upon the concepts of strength of evidence, and the probabilities of misleading and weak evidence.",
    "author": [
      {
        "name": "Peter Cahusac",
        "url": {}
      }
    ],
    "date": "2023-01-13",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-051.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-043/",
    "title": "ICAOD: An R Package for Finding Optimal designs for Nonlinear Statistical Models by Imperialist Competitive Algorithm",
    "description": "Optimal design ideas are increasingly used in different disciplines to rein in experimental costs. Given a nonlinear statistical model and a design criterion, optimal designs determine the number of experimental points to observe the responses, the design points and the number of replications at each design point. Currently, there are very few free and effective computing tools for finding different types of optimal designs for a general nonlinear model, especially when the criterion is not differentiable. We introduce an R package [ICAOD](https://CRAN.R-project.org/package=ICAOD) to find various types of optimal designs and they include locally, minimax and Bayesian optimal designs for different nonlinear statistical models. Our main computational tool is a novel metaheuristic algorithm called imperialist competitive algorithm (ICA) and inspired by socio-political behavior of humans and colonialism. We demonstrate its capability and effectiveness using several applications. The package also includes several theory-based tools to assess optimality of a generated design when the criterion is a convex function of the design.",
    "author": [
      {
        "name": "Ehsan Masoudi",
        "url": {}
      },
      {
        "name": "Heinz Holling",
        "url": {}
      },
      {
        "name": "Weng Kee Wong",
        "url": {}
      },
      {
        "name": "Seongho Kim",
        "url": {}
      }
    ],
    "date": "2022-12-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-043.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-044/",
    "title": "dbcsp: User-friendly R package for Distance-Based Common Spatial Patterns",
    "description": "Common Spatial Patterns (CSP) is a widely used method to analyse electroencephalography (EEG) data, concerning the supervised classification of the activity of brain. More generally, it can be useful to distinguish between multivariate signals recorded during a time span for two different classes. CSP is based on the simultaneous diagonalization of the average covariance matrices of signals from both classes and it allows the data to be projected into a low-dimensional subspace. Once the data are represented in a low-dimensional subspace, a classification step must be carried out. The original CSP method is based on the Euclidean distance between signals, and here we extend it so that it can be applied on any appropriate distance for data at hand. Both the classical CSP and the new Distance-Based CSP (DB-CSP) are implemented in an R package, called dbcsp.",
    "author": [
      {
        "name": "Itsaso Rodríguez",
        "url": {}
      },
      {
        "name": "Itziar Irigoien",
        "url": {}
      },
      {
        "name": "Basilio Sierra",
        "url": {}
      },
      {
        "name": "Concepción Arenas",
        "url": {}
      }
    ],
    "date": "2022-12-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-044.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-045/",
    "title": "The R Package HDSpatialScan for the Detection of Clusters of Multivariate and Functional Data using Spatial Scan Statistics",
    "description": "This paper introduces the R package [HDSpatialScan](https://CRAN.R-project.org/package=HDSpatialScan). This package allows users to easily apply spatial scan statistics to real-valued multivariate data or both univariate and multivariate functional data. It also permits plotting the detected clusters and to summarize them. In this article the methods are presented and the use of the package is illustrated through examples on environmental data provided in the package.",
    "author": [
      {
        "name": "Camille Frévent",
        "url": {}
      },
      {
        "name": "Mohamed-Salem Ahmed",
        "url": {}
      },
      {
        "name": "Julien Soula",
        "url": {}
      },
      {
        "name": "Lionel Cucala",
        "url": {}
      },
      {
        "name": "Zaineb Smida",
        "url": {}
      },
      {
        "name": "Sophie Dabo-Niang",
        "url": {}
      },
      {
        "name": "Michaël Genin",
        "url": {}
      }
    ],
    "date": "2022-12-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-045.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-046/",
    "title": "HDiR: An R Package for Computation and Nonparametric Plug-in Estimation of Directional Highest Density Regions and General Level Sets",
    "description": "A deeper understanding of a distribution support, being able to determine regions of a certain (possibly high) probability content is an important task in several research fields. Package HDiR for R is designed for exact computation of directional (circular and spherical) highest density regions and density level sets when the density is fully known. Otherwise, HDiR implements nonparametric plug-in methods based on different kernel density estimates for reconstructing this kind of sets. Additionally, it also allows the computation and plug-in estimation of level sets for general functions (not necessarily a density). Some exploratory tools, such as suitably adapted distances and scatterplots, are also implemented. Two original datasets and spherical density models are used for illustrating HDiR functionalities.",
    "author": [
      {
        "name": "Paula Saavedra-Nieves",
        "url": {}
      },
      {
        "name": "Rosa M. Crujeiras",
        "url": {}
      }
    ],
    "date": "2022-12-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-046.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-047/",
    "title": "metapack: An R Package for Bayesian Meta-Analysis and Network Meta-Analysis with a Unified Formula Interface",
    "description": "Meta-analysis, a statistical procedure that compares, combines, and synthesizes research findings from multiple studies in a principled manner, has become popular in a variety of fields. Meta-analyses using study-level (or equivalently *aggregate*) data are of particular interest due to data availability and modeling flexibility. In this paper, we describe an R package metapack that introduces a unified formula interface for both meta-analysis and network meta-analysis. The user interface---and therefore the package---allows flexible variance-covariance modeling for multivariate meta-analysis models and univariate network meta-analysis models. Complicated computing for these models has prevented their widespread adoption. The package also provides functions to generate relevant plots and perform statistical inferences like model assessments. Use cases are demonstrated using two real data sets contained in metapack.",
    "author": [
      {
        "name": "Daeyoung Lim",
        "url": {}
      },
      {
        "name": "Ming-Hui Chen",
        "url": {}
      },
      {
        "name": "Joseph G. Ibrahim",
        "url": {}
      },
      {
        "name": "Sungduk Kim",
        "url": {}
      },
      {
        "name": "Arvind K. Shah",
        "url": {}
      },
      {
        "name": "Jianxin Lin",
        "url": {}
      }
    ],
    "date": "2022-12-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-047.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-048/",
    "title": "did2s: Two-Stage Difference-in-Differences",
    "description": "Recent work has highlighted the difficulties of estimating difference-in-differences models when the treatment is adopted at different times for different units. This article introduces the R package did2s which implements the estimator introduced in @Gardner_2021. The article provides an approachable review of the underlying econometric theory and introduces the syntax for the function `did2s`. Further, the package introduces functions, event_study and plot_event_study, which uses a common syntax to implement all of the modern event-study estimators.",
    "author": [
      {
        "name": "Kyle Butts",
        "url": "https://www.kylebutts.com/"
      },
      {
        "name": "John Gardner",
        "url": "https://jrgcmu.github.io/"
      }
    ],
    "date": "2022-12-20",
    "categories": [],
    "contents": "\nIntroduction\nA rapidly growing econometric literature has identified difficulties in traditional difference-in-differences estimation when treatment turns on at different times for different groups and when the effects of treatment vary across groups and over time (Goodman-Bacon 2018; Chaisemartin and D’Haultfoeuille 2019; Callaway and Sant’Anna 2020; Sun and Abraham 2020; Borusyak et al. 2021). Gardner (2022) proposes an estimator of the two-way fixed-effects model that is quick and intuitive. The estimator relies on the standard two-way fixed-effect model (see the following section) and forms an intuitive estimate: the average difference in outcomes between treated and untreated units after removing fixed unit- and time-invariant shocks.\nThis article first discusses the modern difference-in-differences theory in an approachable way and second discusses the software package, did2s, which implements the two-stage estimation approach proposed by Gardner (2022) to estimate robustly the two-way fixed-effects (TWFE) model. There are two notable technical features of this package. First, did2s utilizes the incredibly fast package, fixest (Bergé 2018), which can estimate regressions with a high number of fixed-effects very quickly. Second, since there are a few alternative TWFE event-study estimators implemented in R, each with their own syntax and data formatting requirements, the package also has a set of functions that allow quick estimation and plotting of every alternative event study estimator using a standardized syntax. This allows for easy comparison between the results of different methods.\nDifference-in-differences theory\nResearchers commonly use the difference-in-differences (DiD) methodology to estimate the effects of treatment in the case where treatment is non-randomly assigned. Instead of random assignment giving rise to identification, the DiD method relies on the so-called “parallel trends” assumption, which asserts that outcomes would evolve in parallel between the treated and untreated groups in a world where the treated were untreated. This is formalized with the two-way fixed-effects (TWFE) model. In a static setting where treatment effects are constant across treatment groups and over time, researchers estimate the static TWFE model:\n\\[\\begin{equation}\\label{eq:twfe}\n  y_{igt} = \\mu_g + \\eta_t + \\tau D_{gt} + \\varepsilon_{igt},\n\\end{equation}\\]\nwhere \\(y_{igt}\\) is the outcome variable of interest, \\(i\\) denotes the individual, \\(t\\) denotes time, and \\(g\\) denotes group membership where a “group” is defined as all units that start treatment at time \\(g\\).1 \\(\\mu_g\\) is a vector of time-invariant group fixed-effects, \\(\\eta_t\\) is a vector of shocks in a given time period that is experienced by all individuals equally, and \\(D_{gt}\\) is an indicator for whether initial-treatment group \\(g\\) is receiving treatment in period \\(t\\), i.e. \\(D_{gt} \\equiv \\mathbb{1}(g \\leq t)\\). The coefficient of interest is \\(\\tau\\), which is the (constant) average effect of the treatment on the treated (ATT). If it is indeed true that the treatment effect is constant across groups and over time, then the estimate formed by estimating the static TWFE model will be consistent for \\(\\tau\\) under a parallel trends assumption on the error term.\nHowever, treatment effects are not constant in most settings. The magnitude of a unit’s treatment effect can differ based on group status \\(g\\) (e.g. if groups that benefit more from a policy implement it earlier) and treatment duration (e.g. if treatment effects grow as the policy has been in place for longer periods). Therefore to enrich our model, we allow heterogeneity in treatment effects across \\(g\\) and \\(t\\) by introducing the group-time average treatment effect, \\(\\tau_{gt}\\). Correspondingly, we modify the TWFE model as follows:\n\\[\n  y_{igt} = \\mu_g + \\eta_t + \\tau_{gt} D_{gt} + \\varepsilon_{igt}.\n\\]\nThe key difference is that treatment effects are allowed to differ based on group status \\(g\\) and time period \\(t\\). Estimating any individual \\(\\tau_{gt}\\) may not be desirable since there would be too few observations. Instead, researchers aggregate group-time average treatment effects into the overall average treatment effect, \\(\\tau\\), which averages across \\(\\tau_{gt}\\):\n\\[\n  \\tau \\equiv \\sum_{g, t} \\frac{N_{gt}}{N_{post}} \\tau_{gt},\n\\]\nwhere \\(N_{gt}\\) denotes the number of observations in \\((g, t)\\) and \\(N_{post}\\) is the number of post-treatment observations (\\(t \\geq g\\)). The natural question is, “does the static TWFE model, (\\(\\ref{eq:twfe}\\)), produce a consistent estimate for the overall average treatment effect?” Except for a few specific scenarios, the answer is no (Goodman-Bacon 2018; Chaisemartin and D’Haultfoeuille 2019; Sun and Abraham 2020; Borusyak et al. 2021).\nOne way of thinking about this disappointing result is through the Frisch–Waugh–Lovell (FWL) theorem (Frisch and Waugh 1933). This theorem says that estimating the Static TWFE model is equivalent to estimating\n\\[\ny_{igt} - \\hat{\\mu}_g - \\hat{\\eta}_t = \\tau \\tilde{D}_{gt} + \\tilde{\\varepsilon}_{gt},\n\\]\nwhere \\(\\tilde{D}_{gt}\\) denotes the residuals from regressing \\(D_{gt}\\) on \\(\\mu_g\\) and \\(\\eta_t\\); \\(\\hat{\\mu}_g\\) and \\(\\hat{\\eta}_t\\) are estimates for the group and time fixed-effects, respectively. The left-hand side of this equation, under a parallel trends restriction on the error term \\(\\varepsilon_{it}\\), is our estimate for \\(\\tau_{gt}\\). Therefore, the FWL theorem tells us estimating the static TWFE model is equivalent to estimating2\n\\[\n\\hat{\\tau}_{gt} = \\tau \\tilde{D}_{gt} + \\tilde{\\varepsilon}_{gt}\n\\]\nThe resulting estimate for \\(\\tau\\) can be written as:\n\\[\n\\hat{\\tau} \\equiv \\sum_{g,t} w_{gt} \\hat{\\tau}_{gt},\n\\]\nwhere \\(w_{gt}\\) is the weight put on the corresponding \\(\\hat{\\tau}_{gt}\\). Results of Gardner (2022), Borusyak et al. (2021), and Chaisemartin and D’Haultfoeuille (2019) all characterize the weights \\(w_{gt}\\) from this regression. There are only two cases where the \\(\\hat{\\tau}\\) is a consistent estimate for the overall average treatment effect. First, when treatment occurs at the same time for all treated units, then \\(w_{gt}\\) is equal to \\(N_{gt}/N_{post}\\) for all \\(\\{g, t\\}\\) and therefore \\(\\hat{\\tau}\\) is a consistent estimate for the overall average treatment effect. The other scenario where \\(\\hat{\\tau}\\) estimates the overall average treatment effect is when \\(\\tau_{gt}\\) is constant across group and time, i.e. \\(\\tau_{gt} = \\tau\\). Since the weights, \\(w_{gt}\\), always sum to one, we have that \\(\\hat{\\tau} = \\sum w_{gt} \\hat{\\tau}_{gt} \\to \\sum w_{gt} \\tau = \\tau\\).\nThe above cases are not the norm in research. If there is heterogeneity in group-time treatment effects and units get treated at different times, then \\(\\hat{\\tau}\\) is not a consistent estimate for the average treatment effect \\(\\tau\\). Instead, \\(\\hat{\\tau}\\) will be a weighted average of group-time treatment effects with some weights, \\(w_{gt}\\), being potentially negative. This yields a treatment effect estimate that does not provide a good summary of the “average” treatment effect. It is even possible for the sign of \\(\\hat{\\tau}\\) to differ from that of the overall average treatment effect. This would occur, for example, if negative weights are placed primarily on the largest (in magnitude) group-time treatment effects.\nTo summarize the modern literature, the fundamental problem faced in estimating the TWFE model is the potential negative weighting. The proposed methodology in Gardner (2022) is based on the fact that if \\(\\hat{\\tau}_{gt}\\) is regressed on \\(D_{gt}\\), instead of \\(\\tilde{D}_{gt}\\), the resulting weights would be exactly equal to \\(N_{gt}/N_{post}\\) and the coefficient of \\(D_{gt}\\) would estimate the overall average treatment effect.\nEvent-study estimates\nResearchers have attempted to model treatment effect heterogeneity by allowing treatment effects to change over time. To do this, they introduce a (dynamic) event-study TWFE model:\n\\[\\begin{equation}\n  y_{igt} = \\mu_g + \\eta_t + \\sum_{k = -L}^{-2} \\tau^k D_{gt}^k + \\sum_{k = 0}^{K} \\tau^k D_{gt}^k + \\varepsilon_{igt},\n\\end{equation}\\]\nwhere \\(D_{gt}^k\\) are lags/leads of treatment (\\(k\\) periods from initial treatment date). The coefficients of interests are the \\(\\tau^k\\), which represent the average effect of being treated for \\(k\\) periods. For negative values of \\(k\\), \\(\\tau^k\\) are known as “pre-trends,” and represent the average deviation in outcomes for treated units \\(k\\) periods away from treatment, relative to their value in the reference period. These pre-trend estimates are commonly used as a test of the parallel counterfactual trends assumption.\nOur goal is to estimate the average treatment effect of being exposed for \\(k\\) periods, an average of \\(\\tau_{gt}\\) for only the set of \\(\\{g,t\\}\\) where \\(k\\) periods have elapsed since \\(g\\), i.e. \\(t - g = k\\):\n\\[\n  \\tau^k = \\sum_{g,t \\ : \\ t - g = k} \\frac{N_{gt}^k}{N^k} \\tau_{gt},\n\\]\nwhere the sum is over \\(\\{g,t\\}\\) with \\(t - g = k\\), \\(N_{gt}^k\\) is the number of observations in group \\(g\\) and \\(N^k\\) is the total number of observations with \\(t - g = k\\). The results of Sun and Abraham (2020) show that even though we allow for our average treatment effects to vary over time \\(\\tau^k\\), the negative weighting problems would arise if units are treated at different times and there is group-heterogeneity in treatment effects. Similar to the static TWFE model, the estimates of \\(\\tau^k\\) from running the event-study model form non-intuitively weighted averages of \\(\\tau_{gt}\\) with \\(w_{gt}^k \\neq N_{gt}^k/N^k\\). Even worse, the group-time treatment effects for \\(t-g \\neq k\\) will be included in the estimate of \\(\\hat{\\tau}^k\\). Hence, the need for a robust difference-in-differences estimator remains even in the event-study model.\nTwo-stage difference-in-differences estimator\nGardner (2022) proposes an estimator to resolve the problem with the two-way fixed-effects approaches. Rather than attempting to estimate the group and time effects simultaneously with the ATT (causing \\(D_{it}\\) to be residualized), Gardner’s approach proceeds from the observation that, under parallel trends, the group and time effects are identified from the subsample of untreated/not-yet-treated observations (\\(D_{gt} = 0\\)). This suggests a simple two-stage difference-in-differences estimator:\nEstimate the model\n\\[\ny_{igt} = \\mu_g + \\eta_t + \\varepsilon_{igt},\n\\]\nusing the subsample of untreated/not-yet-treated observations (i.e., all observations for which \\(D_{gt}=0\\)), retaining the estimated group and time effects to form the adjusted outcomes \\(\\tilde{y}_{igt} \\equiv y_{igt} - \\hat{\\mu}_g - \\hat{\\eta}_t\\).\nRegress adjusted outcomes \\(\\tilde{y}_{igt}\\) on treatment status \\(D_{gt}\\) or \\(D_{gt}^k\\) in the full sample to estimate treatment effects \\(\\tau\\) or \\(\\tau^k\\).\nTo see why this procedure works, note that parallel trends implies that outcomes can be expressed as\n\\[\\begin{align*}\n  y_{igt} &= \\mu_g + \\eta_t + \\tau_{gt} D_{gt} + \\varepsilon_{igt} \\\\\n  &= \\mu_g + \\eta_t + \\bar{\\tau} D_{gt} + (\\tau_{gt} - \\bar{\\tau}) D_{gt} + \\varepsilon_{igt},\n\\end{align*}\\]\nwhere \\(\\tau_{gt} = E(Y^1_{igt} - Y^0_{igt} \\ | \\ g, t)\\) is the average treatment effect for group \\(g\\) in period \\(t\\)3 and \\(\\bar{\\tau} = E(\\tau_{gt} | D_{gt}=1)\\) is the overall average treatment effect4. Note from parallel trends, \\(E(\\varepsilon_{igt} | D_{gt}, g, t) = 0\\). Rearranging, this gives\n\\[\n  y_{igt} - \\mu_g - \\eta_t = \\bar{\\tau} D_{gt} + (\\tau_{gt} - \\bar{\\tau}) D_{gt} + \\varepsilon_{igt}.\n\\]\nSuppose you knew the time and group fixed-effects and were able to directly observe the left-hand side (later we will estimate the left-hand side). Regressing the adjusted \\(y\\) variable, on \\(D_{gt}\\) will produce a consistent estimator for \\(\\bar{\\tau}\\). To see this, note that \\(E[(\\tau_{gt} - \\bar{\\tau}) D_{gt} \\ | \\ D_{gt}] = 0\\). Hence, the treatment dummy is uncorrelated with the omitted variable and the average treatment effect is identified in the second-stage. Since we are not able to directly observe \\(\\mu_g\\) and \\(\\eta_t\\), we estimate them using the untreated/not-yet-treated observations in the first-stage. However, standard errors need adjustment to account for the added uncertainty from the first-stage estimation.\nThis approach can be extended to dynamic models by replacing the second stage of the procedure with a regression of residualized outcomes onto the leads and lags of treatment status, \\(D_{gt}^k\\), \\(k \\in \\{-L, \\dots, K\\}\\). Under parallel trends, the second-stage coefficients on the lags identify the overall average effect of being treated for \\(k\\) periods (where the average is taken over all units treated for at least that many periods). The second-stage coefficients on the leads identify the average deviation from predicted counterfactual trends among units that are \\(k\\) periods away from treatment, which under parallel trends should be zero for any pre-treatment value of \\(k\\). Hence, the coefficients on the leads represent a test of the validity of the parallel trends assumption.\nInference\nThe standard variance-covariance matrix from the second-stage regression will be incorrect since it fails to account for the fact that the dependent variable is generated from the first-stage regression. However, this estimator takes the form of a joint generalized method of moments (GMM) estimator whose asymptotic variance is well understood (Newey and McFadden 1986).\nSpecifically, the estimator takes the form of a two-stage GMM estimator with the following two moment conditions:\n\\[\\begin{align}\n  m(\\theta) = (Y-X_{10}'\\gamma)X_{10}, \\\\\n  g(\\gamma, \\theta) = (Y - X_1'\\gamma - X_2'\\theta) X_2,\n\\end{align}\\]\nwhere \\(X_1\\) is the matrix of group and time fixed-effects, \\(X_{10}\\) corresponds to the matrix \\(X_1\\), but with rows corresponding to observations for which \\(D_{gt} = 1\\) replaced with zeros (as only observations with \\(D_{gt} = 0\\) are used in the first stage) and \\(X_2\\) is the matrix of treatment variable(s). The first equation corresponds with the first stage and the second equation corresponds with the second stage. From Theorem 6.1 of Newey and McFadden (1986), the asymptotic variance of the two-stage estimator is\n\\[\\begin{equation}\n  V = G_\\theta^{-1} E\\left[ (g + G_\\gamma \\psi)(g + G_\\gamma \\psi)' \\right] G_\\theta^{-1'},\n\\end{equation}\\]\nwhere from our moment conditions, we have:\n\\[\n  G_\\theta = - E\\left(X_2X_2' \\right),\n\\]\n\\[\n  G_\\gamma = - E\\left(X_2X_1'\\right),\n\\]\n\\[\n  \\psi = E(X_{10}X_{10}')^{-1} \\varepsilon_{10} X_{10}.\n\\]\nThis can be estimated using\n\\[\\begin{equation}\n  \\left(X_2'X_2\\right)^{-1} \\left(\\sum_{g=1}^G W_g' W_g\\right) \\left(X_2'X_2\\right)^{-1},\n\\end{equation}\\] where\n\\[\n  W_g = X_{2g}'\\hat{\\varepsilon}_{2g} - \\hat{\\varepsilon}_{10g}' X_{1g}\\left(X_{1g}'X_{1g}\\right)^{-1} \\left(X_{1g}'X_{2g}\\right),\n\\]\nand matrices indexed by \\(g\\) correspond to the \\(g\\)th cluster.\nThe did2s package\nThe did2s package introduces two sets of functions. The first is the did2s command which implements the two-stage difference-in-differences estimator as described above. The second is the event_study and plot_event_study commands that allow individuals to implement alternative ‘robust’ estimators using a singular common syntax.\nThe did2s command\nThe command did2s implements the two-stage difference-in-differences estimator following Gardner (2022). The general syntax is\n\n\ndid2s(data, yname, first_stage, second_stage, \n      treatment, cluster_var, weights = NULL, \n      bootstrap = FALSE, n_bootstraps = 250, \n      verbose = TRUE)\n\n\nand full details on the arguments is available in the help page, available by running ?did2s. There are a few arguments that are worth discussing in more detail.\nThe first_stage and second_stage arguments require formula arguments. These formulas are passed to the fixest::feols function from fixest and can therefore utilize two non-standard formula options that are worth mentioning (Bergé 2018). First, fixed-effects can be inserted after the covariates, e.g. ~ x1 | fe_1 + fe_2, which will make estimation much faster than using factor(fe_1). Second, the function fixest::i can be used for treatment indicators instead of factor. The advantage of this is that you can easily specify the reference values, e.g. for event-study indicators where researchers typically want to drop time \\(t = -1\\), ~ i(rel_year, ref = c(-1)) would be the correct second-stage formula. Additionally, fixest has a number of post-estimation exporting commands to make tables with fixest::etable and event-study plots with fixest::iplot/fixest::coefplot. The fixest::i function is better integrated with these functions as we will see below.\nThe option treatment is the variable name of a \\(0/1\\) variable that denotes when treatment is active for a given unit, \\(D_{gt}\\) in the above notation. Observations with \\(D_{gt} = 0\\) will be used to estimate the first stage, which removes the problem of treatment effects contaminating estimation of the unit and time fixed-effects. However, as an important note, if you suspect anticipation effects before treatment begins, the treatment variable should be shifted forward by \\(x\\) periods for observations to prevent the aforementioned contamination. For example, if you suspect that units could experience treatment effects 1 period ahead of treatment (a so-called anticipatory effect), then the treatment should begin one period ahead. These anticipation effects can be estimated, after adjusting the treatment variable, by using a reference year of say, \\(t = -2\\) and looking at the estimate for relative year \\(-1\\).\nExample usage of did2s\nFor basic usage, I will use the simulated dataset, df_het, that comes with the did2s package with the command\n\n\ndata(df_het, package = \"did2s\")\n\n\nThe data-generating process is displayed in Figure 1. The lines represent the mean outcome for each treatment group and the never-treated group. In the absence of treatment, each group is simulated to be on parallel trends. There is heterogeneity in treatment effects both within a treatment group over time and across treatment groups.\n\n\n\nFigure 1: This figure plots simulated data with two treated groups and a never-treated group. Each line represents the average outcome (y-value) in a given year (x-value) for each of the three groups. In the absence of treatment, all three groups would exhibit parallel trends (staying around a value of 4 in each period). Each of the treated groups are experience different treatment effect magnitudes that grow over time. This treatment effect heterogeneity creates problems for the classical two-way fixed effect OLS estimator.\n\n\n\nFirst, we will calculate a static difference-in-differences estimate using the did2s function.\n\n\nstatic = did2s(\n  data = df_het, \n  yname = \"dep_var\", \n  treatment = \"treat\",\n    first_stage = ~ 0 | unit + year, \n    second_stage = ~ i(treat, ref = FALSE),\n    cluster_var = \"unit\", \n  verbose = FALSE\n)\n\nsummary(static)\n\nOLS estimation, Dep. Var.: dep_var\nObservations: 46,500 \nStandard-errors: Custom \n            Estimate Std. Error t value  Pr(>|t|)    \ntreat::TRUE  2.23048   0.021408  104.19 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.0357   Adj. R2: 0.505683\n\nSince the returning object is a fixest object, all the accompanying output commands from fixest are available to use. For example, we can create regression tables:\n\n\nfixest::etable(static, fitstat = c(\"n\"),\n  title = \"Estimate of Static TWFE Model\", \n  notes = \"This table presents the estimated overall treatment effect. The effect is estimated using Two-Stage Difference-in-Differences proposed by Gardner (2021). The estimated effect is close to the true value.\")\n\n                           static\nDependent Var.:           dep_var\n                                 \ntreat = TRUE    2.230*** (0.0214)\n_______________ _________________\nS.E. type                  Custom\nObservations               46,500\n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nHowever, since there are dynamic treatment effects in this example, it is much better to estimate the dynamic effects themselves using an event-study specification. We will then plot the results using fixest::iplot, which plots coefficients corresponding to an i() variable. Note that rel_year is coded as Inf for never-treated units, so this has to be noted in the reference part of the formula.\n\n\nes = did2s(\n  data = df_het, \n  yname = \"dep_var\", \n  treatment = \"treat\",\n  first_stage = ~ 0 | unit + year, \n    second_stage = ~ i(rel_year, ref = c(-1, Inf)),\n    cluster_var = \"unit\", \n  verbose = FALSE\n)\n\nfixest::iplot(\n  es, \n  main = \"Event study: Staggered treatment\", \n  xlab = \"Relative time to treatment\", \n  col = \"steelblue\", ref.line = -0.5\n)\n\n# Add the (mean) true effects\ntrue_effects = tapply((df_het$te + df_het$te_dynamic), df_het$rel_year, mean)\ntrue_effects = head(true_effects, -1)\npoints(-20:20, true_effects, pch = 20, col = \"grey60\")\n\n# Legend\nlegend(x=-20, y=3, col = c(\"steelblue\", \"grey60\"), \n       pch = c(20, 20), \n       legend = c(\"Two-stage estimate\", \"True effect\"))\n\n\n\nFigure 2: This figure plots the true treatment effect and estimates using the Two-Stage Difference-in-Differences proposed by Gardner (2021). The x-axis of this figure is the relative time to treatment, i.e. how many years pre-/post- treatment that period is. The y-axis is estimated treatment effects. There are two sets of points. The first is for the true effect which is equal to 0 in all pre-periods and in the post-period starts at 1.5 and linearly grows to 3 by post-period 20. The second set of points is the estimates from the two-stage difference-in-difference estimates which follows closely the true effects but with additional noise from estimation error.\n\n\n\nThe event study estimates are found in Figure 2 and match closely to the true average treatment effects. For comparison to traditional OLS estimation of the event-study specification, Figure 3 plots point estimates from both methods. As pointed out by Sun and Abraham (2020), treatment effect heterogeneity between groups biases the estimated pre-trends. In the figure below, the OLS estimates appear to show violations of pre-trends even though the data was simulated under parallel pre-trends.\n\n\ntwfe = feols(dep_var ~ i(rel_year, ref=c(-1, Inf)) | unit + year, data = df_het) \n\nfixest::iplot(list(es, twfe), sep = 0.2, ref.line = -0.5,\n      col = c(\"steelblue\", \"#82b446\"), pt.pch = c(20, 18), \n      xlab = \"Relative time to treatment\", \n      main = \"Event study: Staggered treatment (comparison)\")\n\n# True Effects\npoints(-20:20, true_effects, pch = 20, col = \"grey60\")\n\n# Legend\nlegend(x=-20, y=3, col = c(\"steelblue\", \"#82b446\", \"grey60\"), pch = c(20, 18, 20), \n       legend = c(\"Two-stage estimate\", \"TWFE\", \"True Effect\"))\n\n\n\nFigure 3: This figure adds the standard ordinary-least squares estimates to the true effect and the did2s estimates present in Figure 2. The x-axis of this figure is the relative time to treatment, i.e. how many years pre-/post- treatment that period is. The y-axis is estimated treatment effects. There are three sets of points. The first two sets of points are the same as in Figure 2. The third set of points is the ordinary-least squares estimates. These points exhibit show evidence of parallel pre-trends failing.\n\n\n\nThe event_study and plot_event_study command\nThe command event_study presents a common syntax that estimates the event-study TWFE model for treatment-effect heterogeneity robust estimators recommended by the literature and returns all the estimates in a data.frame for easy plotting by the command plot_event_study. The general syntax is\n\n\nevent_study(\n  data, yname, idname, tname, gname, \n  estimator,\n  xformla = NULL, horizon = NULL, weights = NULL\n)\n\n\nThe option data specifies the data set that contains the variables for the analysis. The four other required options are all names of variables: yname corresponds with the outcome variable of interest; idname is the variable corresponding to the (unique) unit identifier, \\(i\\); tname is the variable corresponding to the time period, \\(t\\); and gname is a variable indicating the period when treatment first starts (group status).\n\n\nTable 1: Event Study Estimators\n  \n  Estimator and R package\n      Type\n      Comparison group\n      Main Assumptions\n    Gardner (2021) {did2s}\n\nImputes \\(Y(0)\\)\nNot-yet- and/or Never-treated\nParallel Trends for all unitsLimited anticipation*Correct specification of \\(Y(0)\\)Borusyak, Jaravel, and Spiess (2021) {didimputation}\n\nImputes \\(Y(0)\\)\nNot-yet- and/or Never-treated\nParallel Trends for all unitsLimited anticipation*Correct specification of \\(Y(0)\\)Callaway and Sant'Anna (2021) {did}\n\n2x2 Aggregation\nEither Not-yet- or Never-treated\nParallel Trends for Not-yet-treated or Never-treatedLimited anticipation*Sun and Abraham (2020) {fixest/sunab}\n\n2x2 Aggregation\nNot-yet- and/or Never-treated\nParallel Trends for all unitsLimited anticipation*Roth and Sant'Anna (2021) {staggered}\n\n2x2 Aggregation\nNot-yet-treated\nTreatment timing is randomLimited anticipation*This table summarizes the differences between various proposed event-study estimators in the econometric literature. It highlights the two different strategies that different estimators use, namely imputation and 2x2 aggregation. Importantly, it tries to show the differences in assumptions for each different estimator.* Anticipation can be accoufnted for by adjusting 'initial treatment day' back \\(x\\) periods, where \\(x\\) is the number of periods before treatment that anticipation can occur.\n    \n\nThere are five main estimators available and the choice is specified for the estimator argument and are described in Table 1.5 The following paragraphs will aim to highlight the differences and commonalities between estimators. These estimators fall into two broad categories. First, did2s and didimputation (Butts 2021) are imputation-based estimators as described above. Both rely on “residualizing” the outcome variable \\(\\tilde{Y} = Y_{it} - \\hat{\\mu}_g - \\hat{\\eta}_t\\) and then averaging those \\(\\tilde{Y}\\) to estimate the event-study average treatment effect \\(\\tau^k\\). These two estimators return identical point estimates for post-treatment effects, but differ in their asymptotic regime and hence their standard errors.\nThe second type of estimator, which we label 2x2 aggregation, takes a different approach for estimating event-study average treatment effects. The packages did (Callaway and Sant’Anna 2021), fixest and staggered (Roth and Sant’Anna 2021) first estimate \\(\\tau_{gt}\\) for all group-time pairs. To estimate a particular \\(\\tau_{gt}\\), they use a two-period (periods \\(t\\) and \\(g-1\\)) and two-group (group \\(g\\) and a “control group”) difference-in-differences estimator, known as a 2x2 difference-in-differences. The particular “control group” they use will differ based on estimator and is discussed in the next paragraph. Then, the estimator manually aggregate \\(\\tau_{gt}\\) across all groups that were treated for (at least) \\(k\\) periods to estimate the event-study average treatment effect \\(\\tau^k\\).\nThese estimators do not all rely on the same underlying assumptions, so the rest of the table summarizes the primary differences between estimators. The comparison group column describes which units are utilized as comparison groups in the estimator and hence will determine which units need to satisfy a parallel trends assumption. For example, in some circumstances, treated units will look very different from never-treated units. In this case, parallel trends may only hold between units that receieve treatment at some point and hence only these units should be used in estimation. In other cases, for example if treatment is assigned randomly, then it’s reasonable to assume that both not-yet- and never-treated units would all satisfy parallel trends.\nFor estimators labeled “Not-yet- and/or never-treated”, the default is to use both not-yet- and never-treated units in the estimator. However, if all never-treated units are dropped from the data set before using the estimator, then these estimators will use only not-yet-treated groups as the comparison group. did provides an option to use either the not-yet- treated or the never- treated group as a comparison group depending on which group a researcher thinks will make a better comparison group. staggered will automatically drop units that are never treated from the sample and hence only use not-yet-treated groups as a comparison group.\nThe next column, Main Assumptions, summarize concisely the main theoretical assumptions underlying each estimator. First, the assumptions about parallel trends match the previous discussion on the correct comparison group. The only estimator that doesn’t rely on a parallel trends assumption is staggered which relies on the assumption that when a unit receives treatment is random.\nThe next assumption, that is common across all estimators, is that there should be “limited anticipation” of treatment. In general, anticipatory effects are when units respond to treatment before it is actually implemented. For example, this can be common if the news of a policy triggers behavior responses before the treatment is put in place. “Limited anticipation” is when these anticipatory effects can only exist in a “few” pre-periods.6 In any of these cases, “treatment” should be manually moved back by the maximum number of periods where anticipation can occur. For example, if treatment starts in 2012 and anticipatory effects are reasonably only possible 2 years before, this units’ “group” should be labeled as 2010 in the data.\nThe imputation-based estimators require an additional assumption that the parametric model of \\(Y(0) = \\mu_i + \\eta_t + \\varepsilon_{it}\\) is correctly specified. This is because in the first stage, you have to accurately impute \\(Y(0)\\) when residualizing \\(Y\\) which relies on the correct specification of \\(Y(0)\\). The 2x2 aggregation models do not estimate a parametric form of \\(Y(0)\\) and hence only relies on a parallel trends assumption. While not in the table, it is worth noting that did allows for uniform inference of estimates. This addresses the problem that multiple hypotheses tests are being done by researchers (e.g. checking individually if all post period estimates are significant) by creating standard errors that adjust for multiple testing.\nExample usage of event_study\nThe result of event_study is a tibble in a tidy format (Robinson et al. 2021) that contains point estimates and standard errors for each relative time indicator for each estimator. The results of event_study are stored as a dataframe with event-study term, the estimate, standard error, and a column containing which estimator is used for that estimate. This output dataframe will in turn be passed to plot_event_study for easy comparison. plot_event_study will return a ggplot object (Wickham 2016). We return to the df_het dataset to see example usage of these functions.\n\n\ndata(df_het, package = \"did2s\")\nout = event_study(\n  data = df_het, yname = \"dep_var\", idname = \"unit\",\n  tname = \"year\", gname = \"g\", estimator = \"all\"\n)\n\n\n\n\nhead(out)\n\n   estimator term   estimate  std.error\n1:      TWFE  -20 0.04097725 0.07167704\n2:      TWFE  -19 0.13665695 0.07147683\n3:      TWFE  -18 0.14015820 0.07245520\n4:      TWFE  -17 0.15793252 0.07431871\n5:      TWFE  -16 0.09910002 0.07379570\n6:      TWFE  -15 0.20561127 0.07116478\n\n\n\nplot_event_study(out, horizon = c(-5, 10))\n\n\n\nFigure 4: This figure contains six plots displayed in a grid of different event study estimators. The estimators are labeled ‘TWFE’, ‘Borusyak, Jaravel, Spiess (2021)’, ‘Callaway and Sant’Anna (2020)’, ‘Gardner (2021)’, ‘Roth and Sant’Anna (2021)’, and ‘Sun and Abraham (2020)’. Each estimator’s necessary assumptions are described above. Each plot in the figure displays point estimates from pre-treatment year -5 through post-treatment year 10. Each estimator is approximately 0 for all pre-treatment periods. In post-periods, each figure follows the true treatment effect starting at 1.5 in post-period 1 and growing afterwards.\n\n\n\nConclusion\nThis article introduced the package did2s which provides a fast, memory-efficient, and treatment-effect heterogeneity robust way to estimate two-way fixed-effect models. The package also includes the event_study and plot_event_study functions to allow for a single syntax for the various estimators introduced in the literature. A companion package in Stata is also available with similar syntax for the did2s function.\nWhile this package includes an event_study function that aims to help individuals implement any of the proposed modern “solutions” to the difference-in-differences estimation, further research on this topic is needed to help practitioners be able to more precisely determine which estimators work best in their settings. Potentially, there could be data-driven methods to try to identify the plausibility of the different assumptions. Additionally, there is still more work to be done to formalize under what conditions covariates can flexibly be used in estimation. There is some initial work from Caetano et al. (2022), but there does not yet exist statistical software to perform their proposed estimator.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-048.zip\nCRAN packages used\ndid2s, fixest, didimputation, did, staggered\nCRAN Task Views implied by cited packages\nCausalInference, Econometrics\n\n\nL. Bergé. Efficient estimation of maximum likelihood models with multiple fixed-effects: The R package FENmlm. CREA Discussion Papers, (13): 2018.\n\n\nK. Borusyak, X. Jaravel and J. Spiess. Revisiting event study designs: Robust and efficient estimation. Working paper, 48 pages. 2021.\n\n\nK. Butts. Didimputation: Difference-in-differences estimator from borusyak, jaravel, and spiess (2021). 2021. URL https://www.github.com/kylebutts/didimputation.\n\n\nC. Caetano, B. Callaway, S. Payne and H. S. Rodrigues. Difference in differences with time-varying covariates. Working paper. 2022. URL http://arxiv.org/abs/2202.02903.\n\n\nB. Callaway and P. H. C. Sant’Anna. Did: Difference in differences. 2021. URL https://bcallaway11.github.io/did/.\n\n\nB. Callaway and P. H. C. Sant’Anna. Difference-in-differences with multiple time periods. Journal of Econometrics, 2020. URL https://www.sciencedirect.com/science/article/pii/S0304407620303948. Citation Key: CALLAWAY2020.\n\n\nC. de Chaisemartin and X. D’Haultfoeuille. Two-way fixed effects estimators with heterogeneous treatment effects. National Bureau of Economic Research, 2019. URL http://www.nber.org/papers/w25904.pdf.\n\n\nR. Frisch and F. V. Waugh. Partial time regressions as compared with individual trends. Econometrica, 1(4): 387–401, 1933. URL http://www.jstor.org/stable/1907330.\n\n\nJ. Gardner. Two-stage differences in differences. 2022. URL https://arxiv.org/abs/2207.05943.\n\n\nA. Goodman-Bacon. Difference-in-differences with variation in treatment timing. National Bureau of Economic Research, 2018. URL http://www.nber.org/papers/w25018.pdf.\n\n\nW. Newey and D. McFadden. Large sample estimation and hypothesis testing. In Handbook of econometrics, Eds R. F. Engle and D. McFadden 1st ed pages. 2111–2245 1986. Elsevier. URL https://EconPapers.repec.org/RePEc:eee:ecochp:4-36.\n\n\nD. Robinson, A. Hayes and S. Couch. Broom: Convert statistical objects into tidy tibbles. 2021. URL https://CRAN.R-project.org/package=broom. R package version 0.7.9.\n\n\nJ. Roth and P. H. C. Sant’Anna. Staggered: Efficient estimation under staggered treatment timing. 2021. URL https://github.com/jonathandroth/staggered.\n\n\nL. Sun and S. Abraham. Estimating dynamic treatment effects in event studies with heterogeneous treatment effects. Journal of Econometrics, 2020. URL https://www.sciencedirect.com/science/article/pii/S030440762030378X.\n\n\nH. Wickham. ggplot2: Elegant graphics for data analysis. Springer-Verlag New York, 2016. URL https://ggplot2.tidyverse.org.\n\n\nIn the literature, never treated units often are given a value of \\(g = \\infty\\).↩︎\nThis is a minor abuse of notation since \\(y_{igt} - \\hat{\\mu}_g - \\hat{\\eta}_t\\) is an estimate for \\(\\tau_{igt}\\) which can be different from \\(\\tau_{gt}\\) if there is within group-time heterogeneity.↩︎\ni.e., the average difference between treated and untreated potential outcomes \\(y^1_{igt}\\) and \\(y^0_{igt}\\), conditional on the observed treatment-adoption times.↩︎\ni.e., the population-weighted average of the group-time specific ATTs, \\(\\tau_{gt}\\).↩︎\nExcept for Sun and Abraham, the estimator option is the package name. For Sun and Abraham, the estimator option is sunab. A value of “all” will estimate all 5 estimators.↩︎\nThere should be more periods before treatment in the sample than whatever number a “few” is.↩︎\n",
    "preview": "articles/RJ-2022-048/distill-preview.png",
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 768
  },
  {
    "path": "articles/RJ-2022-049/",
    "title": "rbw: An R Package for Constructing Residual Balancing Weights",
    "description": "We describe the R package [rbw](https://CRAN.R-project.org/package=rbw), which implements the method of residual balancing weights (RBW) for estimating marginal structural models. In contrast to other methods such as inverse probability weighting (IPW) and covariate balancing propensity scores (CBPS), RBW involves modeling the conditional means of post-treatment confounders instead of the conditional distributions of the treatment to construct the weights. RBW is thus easier to use with continuous treatments, and the method is less susceptible to model misspecification issues that often arise when modeling the conditional distributions of treatments. RBW is also advantageous from a computational perspective. As its weighting procedure involves a convex optimization problem, RBW typically locates a solution considerably faster than other methods whose optimization relies on nonconvex loss functions --- such as the recently proposed nonparametric version of CBPS. We explain the rationale behind RBW, describe the functions in [rbw](https://CRAN.R-project.org/package=rbw), and then use real-world data to illustrate their applications in three scenarios: effect estimation for point treatments, causal mediation analysis, and effect estimation for time-varying treatments with time-varying confounders.",
    "author": [
      {
        "name": "Derick S. Baum",
        "url": {}
      },
      {
        "name": "Xiang Zhou",
        "url": {}
      }
    ],
    "date": "2022-12-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-049.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-050/",
    "title": "Tidy Data Neatly Resolves Mass-Spectrometry's Ragged Arrays",
    "description": "Mass spectrometry (MS) is a powerful tool for measuring biomolecules, but the data produced is often difficult to handle computationally because it is stored as a ragged array. In R, this format is typically encoded in complex S4 objects built around environments, requiring an extensive background in R to perform even simple tasks. However, the adoption of tidy data [@wickham2014] provides an alternate data structure that is highly intuitive and works neatly with base R functions and common packages, as well as other programming languages. Here, we discuss the current state of R-based MS data processing, the convenience and challenges of integrating tidy data techniques into MS data processing, and present [RaMS](https://CRAN.R-project.org/package=RaMS), a package that produces tidy representations of MS data.",
    "author": [
      {
        "name": "William Kumler",
        "url": {}
      },
      {
        "name": "Anitra E. Ingalls",
        "url": {}
      }
    ],
    "date": "2022-12-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-050.zip\n\n\nH. Wickham. Tidy data. Journal of Statistical Software, 59(10): 1–23, 2014. URL https://doi.org/10.18637/jss.v059.i10.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-052/",
    "title": "casebase: An Alternative Framework for Survival Analysis and Comparison of Event Rates",
    "description": "In clinical studies of time-to-event data, a quantity of interest to the clinician is their patient's risk of an event. However, methods relying on time matching or risk-set sampling (including Cox regression) eliminate the baseline hazard from the estimating function. As a consequence, the focus has been on reporting hazard ratios instead of survival or cumulative incidence curves. Indeed, reporting patient risk or cumulative incidence requires a separate estimation of the baseline hazard. Using case-base sampling, Hanley & Miettinen (2009) explained how parametric hazard functions can be estimated in continuous-time using logistic regression. Their approach naturally leads to estimates of the survival or risk function that are smooth-in-time. In this paper, we present the casebase R package, a comprehensive and flexible toolkit for parametric survival analysis. We describe how the case-base framework can also be used in more complex settings: non-linear functions of time and non-proportional hazards, competing risks, and variable selection. Our package also includes an extensive array of visualization tools to complement the analysis. We illustrate all these features through three different case studies. * SRB and MT contributed equally to this work.",
    "author": [
      {
        "name": "Sahir Rai Bhatnagar*",
        "url": "http://sahirbhatnagar.com/"
      },
      {
        "name": "Maxime Turgeon*",
        "url": "https://maxturgeon.ca/"
      },
      {
        "name": "Jesse Islam",
        "url": {}
      },
      {
        "name": "James A. Hanley",
        "url": "http://www.medicine.mcgill.ca/epidemiology/hanley/"
      },
      {
        "name": "Olli Saarela",
        "url": "http://individual.utoronto.ca/osaarela/"
      }
    ],
    "date": "2022-12-20",
    "categories": [],
    "contents": "\n\n\n\n1 Introduction\nThe semiparametric Cox model has become the default approach to survival analysis\neven though Cox himself later suggested he would prefer to model the hazard function\ndirectly. In a 1994 interview with Professor Nancy Reid, Sir David Cox was asked\nhow he would model a set of censored survival data, to which he responded: “I\nthink I would normally want to tackle problems parametrically … and if you want\nto do things like predict the outcome for a particular patient, it’s much more\nconvenient to do that parametrically” (Reid 1994). Indeed, the most\nrelevant quantity in a clinical setting is often the 5- or 10-year risk of\nexperiencing a certain event given the patient’s particular profile.\nHowever, the most reported metric from a Cox model is the (potentially\ntime-dependent) hazard ratio (HR). The covariate-conditional\nsurvival curve is arguably a more important\nsummary measure to report than the HR. While stepwise survival curves can be\ncomputed with the Cox model, they require a second step to separately estimate\nthe baseline hazard (Breslow 1972).\nSeveral authors have since pursued fully parametric approaches that made the\nfitting of smooth survival curves more transparent and intuitive through\ngeneralized linear models. The key feature of these procedures is splitting the\ntime axis into discrete intervals. Whitehead (1980) showed the\nequivalence between a Cox model and a Poisson regression with a parameter for\neach event time; Carstensen (2019) provides an exposition of this\nequivalence with a real data example and supporting R code for computing\nstandard errors. Arjas & Haara (1987) and Efron (1988) treated each\npatient-day as a Bernoulli random variable with probability equal to the\ndiscrete hazard rate. A potential issue with these approaches is that the\nnumber of time bins need to be chosen by the data analyst. On the one hand, a fine grouping\nof times may result in few (or none) events per interval, which then leads\nto instability in the Newton-Raphson procedure for estimation\n(Kalbfleisch and Prentice 2011 4.8), and large long-format datasets.\nOn the other hand, a coarse grouping could potentially mask\nnonlinear trends in the hazard function.\nRather than discretizing time, Hanley & Miettinen (2009)\nselected a discrete set of person-time coordinates (“person-moments”) in\ncontinuous time from all observed follow-up experience constituting the\nstudy base. By doing so, they obtained a likelihood expression for the hazard\nfunction that is equivalent to that of logistic regression with an offset term.\nMore specifically, all person-moments when the event of interest\noccurred are selected as the case series, complemented by a randomly sampled\nbase series of person-moments serving as controls. This approach allows\nflexible modeling of the hazard function by including functions of time as covariates\n(e.g. using splines or generalized additive models). Furthermore, time-dependent\ncovariates can be modeled through interactions with time. In short, Hanley &\nMiettinen (2009) use the well-understood logistic regression\nfor directly modeling the hazard function, without requiring a discrete-time model.\nIn this article, we present the casebase R package (Bhatnagar et al. 2021)\nwhich implements and extends the Hanley & Miettinen (2009) approach for\nfitting fully parametric hazard models and covariate-conditional survival curves\nusing the familiar interface of the glm function. Our implementation\nincludes extensions to other models such as penalized\nregression for variable selection and competing risk analysis. In addition,\nwe provide functions for exploratory data analysis and visualizing the estimated\nquantities such as the hazard function, survival curve, and their standard errors.\nThe ultimate goal of our package is to make fitting flexible hazards accessible\nto end users who favor reporting absolute risks and survival curves\nover hazard ratios.\nIn what follows, we first recap some theoretical details on case-base sampling\nand its use for estimating parametric hazard functions. We then give a short\nreview of existing R packages that implement comparable features to casebase.\nNext, we provide some details about the implementation of case-base sampling in\nour package, and we give a brief survey of its main functions. This is followed\nby three case studies that illustrate the flexibility and capabilities of\ncasebase. We show how the same framework can be used for non-linear\nfunctions of time and non-proportional hazards, competing risks, and variable\nselection via penalized regression. Finally, we end the article with a\ndiscussion of the results and of future directions.\n2 Theoretical details\nAs discussed in Hanley & Miettinen (2009), the key idea behind case-base sampling is to consider the entire study base as an infinite collection of person-moments. These person-moments are indexed by both an individual in the study and a time point, and therefore each person-moment has a covariate profile and an outcome status (i.e. whether the event happened) attached to it. By estimating the probability of an event occurring at a particular person-moment, we can extract information about hazards and survival.\nTherefore, we start by assembling all person-moments at which the event occurred; this collection of person-moments is what Hanley & Miettinen call the case series. The incidence of the case series is dictated by the hazard function of interest. Next, we sample a finite number of person-moments (blinded to case moments); this second collection of person-moments is what Hanley & Miettinen call the base series. The sampling mechanism for the base series is left at the discretion of the user, but in practice we find that sampling uniformly from the study base provides both simplicity and good performance. This is the default sampling mechanism in the package.\nLikelihood and estimating function\nTo describe the theoretical foundations of case-base sampling, we use the framework of counting processes. In what follows, we abuse notation slightly and omit any mention of \\(\\sigma\\)-algebras. Instead, following Aalen (2008), we use the placeholder “past” to denote the past history of the corresponding process. The reader interested in more details can refer to Saarela & Arjas (2015) and Saarela (2016). First, let \\(N_{i}(t) \\in \\{0, 1\\}\\) be counting processes corresponding to the event of interest for individual \\(i=1, \\ldots,n\\). For simplicity, we will consider Type I censoring due to the end of follow-up at time \\(\\tau\\) (the general case of independent censoring is treated in Saarela (2016)). We are interested in modeling the hazard functions \\(\\lambda_{i}(t)\\) of the processes \\(N_i(t)\\), and which satisfy\n\\[\\lambda_{i}(t)\\,\\textrm dt = E[\\,\\textrm dN_{i}(t)\\mid \\mathrm{past}].\\] The processes \\(N_i(t)\\) count the person-moments from the case series.\nTo complement the case series, we sample person-moments for the base series. To do so, we specify the base series sampling mechanism as non-homogeneous Poisson processes \\(R_i(t) \\in \\{0, 1, 2, \\ldots\\}\\); the person-moments where \\(\\,\\textrm dR_i(t) = 1\\) constitute the base series. We note that the same individual can contribute multiple person-moments to the base series. The process \\(Q_{i}(t) = R_i(t) + N_{i}(t)\\) then counts both the case and base series person-moments contributed by individual \\(i\\). As mentioned above, the processes \\(R_i(t)\\) are specified by the user via its intensity function \\(\\rho_i(t)\\). The process \\(Q_{i}(t)\\) is characterized by \\(E[\\,\\textrm dQ_{i}(t)\\mid\\mathrm{past}] = \\lambda_{i}(t)\\,\\textrm dt + \\rho_i(t)\\,\\textrm dt\\).\nIf the hazard function \\(\\lambda_{i}(t; \\theta)\\) is parametrized in terms of \\(\\theta\\), we can define an estimator \\(\\widehat{\\theta}\\) by maximization of the likelihood expression\n\\[L_0(\\theta) = \\prod_{i=1}^n \\exp\\left\\{ -\\int_0^{\\min(t_i,\\tau)} \\lambda_i(t; \\theta)\\,\\textrm dt \\right\\} \\prod_{i=1}^{n} \\prod_{t\\in[0,\\tau)} \\lambda_{i}(t;\\theta)^{\\,\\textrm dN_{i}(t)},\\]\nwhere \\(\\prod_{t\\in[0,u)}\\) represents a product integral from \\(0\\) to \\(u\\), and where \\(t_i\\) is the event time for individual \\(i\\). However, the integral over time makes the computation and maximization of \\(L_0(\\theta)\\) challenging using standard software.\nCase-base sampling allows us to avoid this integral. By conditioning on a sampled person-moment, we get individual partial likelihood contributions of the form\n\\[P(\\,\\textrm dN_{i}(t) \\mid \\,\\textrm dQ_{i}(t) = 1,\\mathrm{past}) \\stackrel{\\theta}{\\propto} \\frac{\\lambda_{i}(t; \\theta)^{\\,\\textrm dN_{i}(t)}}{\\rho_i(t) + \\lambda_{i}(t;\\theta)}.\\]\nBy using the symbol \\(\\stackrel{\\theta}{\\propto}\\), we mean that both sides of the expression are equal up to multiplicative factors that do not depend on \\(\\theta\\). Therefore, an estimating function for \\(\\theta\\) can be composed of these contributions as:\n\\[\\begin{equation}\nL(\\theta) = \\prod_{i=1}^{n} \\prod_{t\\in[0,\\tau)} \\left(\\frac{\\lambda_{i}(t; \\theta)^{\\,\\textrm dN_{i}(t)}}{\\rho_i(t) + \\lambda_{i}(t;\\theta)}\\right)^{\\,\\textrm dQ_i(t)}. \\tag{1}\n\\end{equation}\\]\nWhen a logarithmic link function is used for modeling the hazard function, the above expression is of a logistic regression form with an offset term \\(\\log(1/\\rho_i(t))\\). Note that the sampling units selected in the case-base sampling mechanism are person-moments, rather than individuals, and the parameters to be estimated are hazards or hazard ratios rather than odds or odds ratios. Generally, an individual can contribute more than one person-moment, and thus the terms in the product integral are not independent. Nonetheless, Saarela (2016) showed that the corresponding partial likelihood score function has mean zero at the true value \\(\\theta=\\theta_0\\), and that the resulting estimator \\(\\widehat{\\theta}\\) is asymptotically normally distributed.\nIn Hanley & Miettinen (2009), the authors suggest sampling the base series uniformly from the study base. In terms of Poisson processes, their sampling strategy corresponds to a time-homogeneous Poisson process with intensity equal to \\(\\rho_i(t) = b/B\\), where \\(b\\) is the number of sampled base series person-moments, and \\(B\\) is the total population-time for the study base (e.g. the sum of all individual follow-up times). More complex examples are also possible; see for example Saarela & Arjas (2015), where the intensity functions for the sampling mechanism are proportional to the cardiovascular disease event rate given by the Framingham score. Non-uniform sampling mechanisms can increase the efficiency of the resulting maximum partial likelihood estimators.\nVariance estimates\nThe asymptotic normality of \\(\\widehat{\\theta}\\) gives us an efficient way to estimate the variance of various estimators derived from the hazard function. For example, to construct confidence bands around risk functions and survival functions, we can use the following procedure:\nCompute \\(\\widehat{\\theta}\\) and its variance-covariance matrix \\(V(\\widehat \\theta)\\) which is obtained as part of the logistic regression output.\nSample \\(B\\) times from a multivariate normal \\(N\\left(\\widehat{\\theta}, V(\\widehat\\theta)\\right)\\).\nFor each sample, compute the survival function using Equation (6) below.\nUse the pointwise quantiles of these survival function estimates to construct a pointwise confidence band for the survival function of interest.\nThis procedure is similar to parametric bootstrap (Efron and Tibshirani 1994 6.5), but it can be more accurately described as a form of approximate Bayesian computation. As such, the validity of the confidence bands relies on the Bernstein-von Mises theorem (Van der Vaart 2000 10).\nCommon parametric models\nHere we show that certain named distributions such as exponential, Weibull or Gompertz can be fit using our framework, though we are not restricted to such models. Let \\(g(t; X)\\) be the linear predictor such that \\(\\log(\\lambda(t;X)) = g(t; X)\\). Different functions of \\(t\\) lead to different parametric hazard models. The simplest of these models is the one-parameter exponential distribution which is obtained by taking the hazard function to be constant over the range of \\(t\\):\n\\[\\begin{equation}\n\\log(\\lambda(t; X)) = \\beta_0 + \\beta_1 X. \\tag{2}\n\\end{equation}\\]\nIn this model, the instantaneous failure rate is independent of \\(t\\).1\nThe Gompertz hazard model is given by including a linear term for time:\n\\[\\begin{equation}\n\\log(\\lambda(t; X)) = \\beta_0 + \\beta_1 t + \\beta_2 X. \\tag{3}\n\\end{equation}\\]\nUse of \\(\\log(t)\\) yields the Weibull hazard which allows for a power dependence of the hazard on time (Kalbfleisch and Prentice 2011):\n\\[\\begin{equation}\n\\log(\\lambda(t; X)) = \\beta_0 + \\beta_1 \\log(t) + \\beta_2 X. \\tag{4}\n\\end{equation}\\]\nCompeting risk analysis\nCase-base sampling can also be used in the context of competing risk analysis. Assuming there are \\(J\\) competing events, we can show that each sampled person-moment’s contribution to the partial likelihood is of the form\n\\[\\frac{\\lambda_j(t)^{\\,\\textrm dN_j(t)}}{\\rho(t) + \\sum_{j=1}^J\\lambda_j(t)},\\]\nwhere \\(N_j(t)\\) is the counting process associated with the event of type \\(j\\) and \\(\\lambda_j(t)\\) is the corresponding cause-specific hazard function. As may be expected, this functional form is similar to the terms appearing in the likelihood function for multinomial regression with an offset term.2\nVariable selection\nTo perform variable selection on the regression parameters \\(\\theta \\in \\mathbb{R}^p\\) of the hazard function, we can add a penalty to the likelihood and optimise the following equation:\n\\[\\begin{equation}\n\\min _{\\theta \\in \\mathbb{R}^{p}}\\,\\,-\\ell\\left(\\theta\\right)+\\sum_{j=1}^p w_j p_{\\lambda,\\alpha}(\\theta_j) \\tag{5}\n\\end{equation}\\]\nwhere \\(\\ell\\left(\\theta\\right) = \\log L(\\theta)\\) is the log of the likelihood function given in (1), \\(p_{\\lambda,\\alpha}(\\theta_j)\\) is a penalty term controlled by the non-negative regularization parameters \\(\\lambda\\) and \\(\\alpha\\), and \\(w_j\\) is the penalty factor for the \\(j\\)th covariate. These penalty factors serve as a way of allowing parameters to be penalized differently. For example, we could set the penalty factor for time to be 0 to ensure it is always included in the selected model.\n3 Comparison with existing packages\nSurvival analysis is an important branch of applied statistics and epidemiology. Accordingly, there is already a vast ecosystem of R packages implementing different methodologies. In this section, we describe how the functionalities of casebase compare to these packages.\nAt the time of writing, a cursory examination of CRAN’s Survival Task View reveals that there are over 250 packages related to survival analysis (Allignol and Latouche 2019). For the purposes of this article, we restricted our review to packages that implement at least one of the following features: parametric modeling, non-proportional hazard models, competing risk analysis, penalized estimation, and Cumulative Incidence (CI) estimation. By searching for appropriate keywords in the DESCRIPTION file of these packages, we found 60 relevant packages. These 60 packages were then manually examined to determine which ones are comparable to casebase. In particular, we excluded packages that were focused on a different set of problems, such as frailty and multistate models. The remaining 14 packages appear in Table 1, along with some of the functionalities they offer.\nParametric survival models are implemented in several packages, each differing in the parametric distributions available: CFC (2019), flexsurv (2016), SmoothHazard (2017), rstpm2 (2019), mets (2014), and survival (2015). For example, SmoothHazard is limited to Weibull distributions (2017), whereas both flexsurv and survival allow users to supply any distribution of their choice. flexsurv, SmoothHazard, mets and rstpm2 can model the effect of time using splines, which allows flexible modeling of the hazard function. As discussed above, casebase can model any parametric family whose log-hazard can be expressed as a linear combination of covariates (including time). Therefore, our package is more general in that it allows the user to model any linear or non-linear transformation of time including splines and higher order polynomials. Also, by including interaction terms between covariates and time, it also allows users to fit (non-proportional) time-varying coefficient models. However, unlike flexsurv, we do not explicitly model any shape parameter.\nSeveral packages implement penalized estimation for the Cox model: glmnet (2011), glmpath (2018), penalized (2010), riskRegression (2020). Moreover, some packages also include penalized estimation in the context of Cox models with time-varying coefficients: elastic-net penalization with rstpm2 (2019), while survival (2015) has an implementation of ridge-penalized estimation. On the other hand, our package casebase provides penalized estimation of the hazard function. To our knowledge, casebase and rstpm2 are the only packages to offer this functionality.\nNext, several R packages implement methodologies for competing risk analysis; for a different perspective on this topic, see Mahani & Sharabiani (2019). The package survival provides functionality for competing risk analysis and multistate modelling. The package cmprsk provides methods for cause-specific subdistribution hazards, such as in the Fine-Gray model (1999). On the other hand, the package CFC estimates cause-specific CIs from unadjusted, non-parametric survival functions. Our package casebase also provides functionalities for competing risk analysis by estimating parametrically the cause-specific hazards. From these quantities, we can then estimate the cause-specific CIs.\nFinally, several packages include functions to estimate the survival function and the CI. The corresponding methods generally fall into two categories: transformation of the estimated hazard function, and semi-parametric estimation of the baseline hazard. The first category broadly corresponds to parametric survival models, where the full hazard is explicitly modeled. Using this estimate, the survival function and the CI can be obtained using their functional relationships (see Equations (6) and (7) below). Packages providing this functionality include CFC, flexsurv, mets, and survival. Our package casebase also follows this approach for both single-event and competing risk analyses. The second category outlined above broadly corresponds to semi-parametric models. These models do not model the full hazard function, and therefore the baseline hazard needs to be estimated separately in order to estimate the survival function. This is achieved using semi-parametric estimators (e.g. Breslow’s estimator) or parametric estimators (e.g. spline functions). Packages that implement this approach include riskRegression, rstpm2, survival, and glmnet. As mentioned in the introduction, a key distinguishing factor between these two approaches is that the first category leads to smooth estimates of the survival function, whereas the second category often produces estimates in the form of stepwise functions.\n\n\nTable 1: Comparison of various R packages for survival analysis. Competing Risks: whether an implementation for competing risks is present. Allows Non PH: includes models for non-proportional hazards. Penalized Regression: allows for a penalty term on the regression coefficients when estimating hazards (e.g. lasso or ridge). Splines: allows a flexible fit on time through the use of splines. Parametric: implementation for parametric models. Semi-parametric: implementation for semi-parametric models. Interval/left censoring: models for interval and left-censoring. If this is not selected, the package only handles right-censoring. Risk estimates: estimation of survival curve and cumulative incidence is available.\n\n\nPackage\n\n\nCompeting Risks\n\n\nAllows Non PH\n\n\nPenalized Regression\n\n\nSplines\n\n\nParametric\n\n\nSemi Parametric\n\n\nInterval/Left Censoring\n\n\nRisk Estimates\n\n\ncasebase\n\n\n✓\n\n\n✓\n\n\n✓\n\n\n✓\n\n\n✓\n\n\n\n\n\n\n✓\n\n\nCFC\n\n\n✓\n\n\n✓\n\n\n\n\n\n\n✓\n\n\n\n\n\n\n✓\n\n\ncmprsk\n\n\n✓\n\n\n\n\n\n\n\n\n\n\n✓\n\n\n\n\n✓\n\n\ncrrp\n\n\n✓\n\n\n\n\n✓\n\n\n\n\n\n\n✓\n\n\n\n\n\n\nfastcox\n\n\n\n\n\n\n✓\n\n\n\n\n\n\n✓\n\n\n\n\n\n\nflexrsurv\n\n\n\n\n✓\n\n\n\n\n✓\n\n\n✓\n\n\n\n\n\n\n✓\n\n\nflexsurv\n\n\n✓\n\n\n✓\n\n\n\n\n✓\n\n\n✓\n\n\n\n\n\n\n✓\n\n\nglmnet\n\n\n\n\n\n\n✓\n\n\n\n\n\n\n✓\n\n\n\n\n✓\n\n\nglmpath\n\n\n\n\n\n\n✓\n\n\n\n\n\n\n✓\n\n\n\n\n\n\nmets\n\n\n✓\n\n\n\n\n\n\n✓\n\n\n\n\n✓\n\n\n\n\n✓\n\n\npenalized\n\n\n\n\n\n\n✓\n\n\n\n\n\n\n✓\n\n\n\n\n\n\nriskRegression\n\n\n✓\n\n\n\n\n✓\n\n\n\n\n\n\n✓\n\n\n\n\n✓\n\n\nrstpm2\n\n\n\n\n✓\n\n\n\n\n✓\n\n\n✓\n\n\n✓\n\n\n✓\n\n\n✓\n\n\nSmoothHazard\n\n\n\n\n✓\n\n\n\n\n✓\n\n\n✓\n\n\n\n\n✓\n\n\n\n\nsurvival\n\n\n✓\n\n\n✓\n\n\n\n\n\n\n✓\n\n\n✓\n\n\n✓\n\n\n✓\n\n\n4 Implementation details\nThe functions in the casebase package can be divided into two categories: 1) exploratory data analysis, in the form of population-time plots; and 2) parametric modeling of the hazard function. We strove for compatibility with both data.frames and data.tables; this can be seen in the coding choices we made and the unit tests we wrote.\nPopulation-time plots\nPopulation-time plots are a descriptive visualization of incidence density, where the population time that constitutes the study base is represented by area, and events by points within the area. The case-base sampling approach described above can be visualized in the form of a population time plot. These plots are informative graphical displays of survival data and should be one of the first steps in an exploratory data analysis. The popTime function and plot method facilitate this task:\nThe casebase::popTime function takes as input the original dataset along with the column names corresponding to the timescale, the event status and an exposure group of interest (optional). This will create an object of class popTime.\nThe corresponding plot method for the object created in Step 1 can be called to create the population time plot with several options for customizing the aesthetics.\nBy splitting these tasks, we give flexibility to the user. While the method call in Step 2 allows further customization by using the ggplot2 (Wickham 2016) family of functions, users may choose the graphics system of their choice to create population-time plots from the object created in Step 1.\nTo illustrate these functions, we will use data from the European Randomized Study of Prostate Cancer Screening (ERSPC) (Schröder et al. 2009) which was extracted using the approach described in Liu et al. (2014). This dataset is available through the casebase package. It contains the recreated individual observations for 159,893 men from seven European countries, who were between the ages of 55 and 69 years when recruited for the trial.\nWe first create the necessary dataset for producing the population time plot using the popTime function. In this example, we stratify the plot by treatment group. The resulting object inherits from class popTime and stores the exposure variable as an attribute:\n\n\n\n\n\npt_object <- casebase::popTime(ERSPC, time = \"Follow.Up.Time\",\n                               event = \"DeadOfPrCa\", exposure = \"ScrArm\")\ninherits(pt_object, \"popTime\")\n\n#> [1] TRUE\n\nattr(pt_object, \"exposure\")\n\n#> [1] \"ScrArm\"\n\n\n\n\nFigure 1: Population time plot for the ERSPC dataset. A: The gray area can be thought of as N=159,893 infinitely thin horizontal rectangles ordered by length of follow-up. B: The red points correspond to when death has occurred for any one of those infinitely thin rectangles. C: To improve visibility, these red points are randomly redistributed along their respective x-coordinates, providing a visualization of incidence density. More events are observed at later follow-up times, motivating the use of non-constant hazard models. D: The base series, a representative sample of the entire grey area, is represented by the green points.\n\n\n\nWe then pass this object to the corresponding plot method:\n\n\nplot(pt_object, add.base.series = TRUE)\n\n\n\n\n\nFigure 1 depicts the process of creating a population-time plot. It is built sequentially by first adding a layer for the area representing the population time in gray (Figure 1A), with subjects having the least amount of observation time plotted at the top of the y-axis. We immediately notice a distinctive stepwise shape in the population time area. This is due to the randomization of the Finnish cohorts which were carried out on January 1 of each of year from 1996 to 1999. Coupled with the uniform December 31 2006 censoring date, this led to large numbers of men with exactly 11, 10, 9 or 8 years of follow-up. Tracked backwards in time (i.e. from right to left), the population-time plot shows the recruitment pattern from its beginning in 1991, and the January 1 entries in successive years. Tracked forwards in time (i.e. from left to right), the plot for the first three years shows attrition due entirely to death (mainly from other causes). Since the Swedish and Belgian centres were the last to complete recruitment in December 2003, the minimum potential follow-up is three years. Tracked further forwards in time (i.e. after year 3) the attrition is a combination of deaths and staggered entries. As we can see, population-time plots summarise a wealth of information about the study into a simple graph.\nNext, layers for the case series and base series are added. The y-axis location of each case moment is sampled at random vertically on the plot to avoid having all points along the upper edge of the gray area (Figure 1B). By randomly distributing the cases, we can get a sense of the incidence density. In Figure 1C, we see that more events are observed at later follow-up times. Therefore, a constant hazard model would not be appropriate in this instance as it would overestimate the incidence earlier on in time, and underestimate it later on. Finally, the base series is sampled uniformly from the study base (Figure 1D). The reader should refer to the package vignettes for more examples and a detailed description of how to modify the aesthetics of a population-time plot.\nParametric modeling\nThe parametric modeling step was separated into three parts:\ncase-base sampling;\nestimation of the smooth hazard function;\nestimation of the survival function.\nBy separating the sampling and estimation functions, we allow the possibility of users implementing more complex sampling scheme (as described in Saarela (2016)), or more complex study designs (e.g. time-varying exposure).\nThe sampling scheme selected for sampleCaseBase was described in Hanley & Miettinen (2009): we first sample along the “person” axis, proportional to each individual’s total follow-up time, and then we sample a moment uniformly over their follow-up time. This sampling scheme is equivalent to the following picture: imagine representing the total follow-up time of all individuals in the study along a single dimension, where the follow-up time of the next individual would start exactly when the follow-up time of the previous individual ends. Then the base series could be sampled uniformly from this one-dimensional representation of the overall follow-up time. In any case, the output is a dataset of the same class as the input, where each row corresponds to a person-moment. The covariate profile for each such person-moment is retained, and an offset term is added to the dataset. This output could then be used to fit a smooth hazard function, or for visualization of the base series.\nNext, the fitting function fitSmoothHazard starts by looking at the class of the dataset: if it was generated from sampleCaseBase, it automatically inherited the class cbData. If the dataset supplied to fitSmoothHazard does not inherit from cbData, then the fitting function starts by calling sampleCaseBase to generate the base series. In other words, users can bypass sampleCaseBase altogether and only worry about the fitting function fitSmoothHazard.\nThe fitting function retains the familiar formula interface of glm. The left-hand side of the formula should be the name of the column corresponding to the event type. The right-hand side can be any combination of the covariates, along with an explicit functional form for the time variable. Note that non-proportional hazard models can be achieved at this stage by adding an interaction term involving time (cf. Case Study 1 below). The offset term does not need to be specified by the user, as it is automatically added to the formula before calling glm.\nTo fit the hazard function, we provide several approaches that are available via the family parameter. These approaches are:\nglm: This is the familiar logistic regression.\nglmnet: This option allows for variable selection using the elastic-net (Zou and Hastie 2005) penalty (cf. Case Study 3). This functionality is provided through the glmnet package (Friedman et al. 2010).\ngam: This option provides support for Generalized Additive Models via the mgcv package (Hastie and Tibshirani 1987).\nIn the case of multiple competing events, the hazard is fitted via multinomial regression as performed by the VGAM package. We selected this package for its ability to fit multinomial regression models with an offset.\nOnce a model-fit object has been returned by fitSmoothHazard, all the familiar summary and diagnostic functions are available: print, summary, predict, plot, etc. Our package provides one more functionality: it computes risk functions from the model fit. For the case of a single event, it uses the familiar identity\n\\[\\begin{equation}\nS(t) = \\exp\\left(-\\int_0^t \\lambda(u;X) du\\right).\n\\tag{6}\n\\end{equation}\\]\nThe integral is computed using either the numerical or Monte-Carlo integration. The risk function (or cumulative distribution function) is then defined as\n\\[\\begin{equation}\nF(t) = 1 - S(t). \\tag{7}\n\\end{equation}\\]\nFor the case of a competing-event analysis, the event-specific risk is computed using the following procedure: first, we compute the overall survival function (i.e. for all event types):\n\\[ S(t) = \\exp\\left(-\\int_0^t \\lambda(u;X) du\\right),\\qquad \\lambda(t;X) = \\sum_{j=1}^J \\lambda_j(t;X).\\]\nFrom this, we can derive the cause-specific subdensities:\n\\[ f_j(t) = \\lambda_j(t)S(t).\\]\nBy integrating these subdensities, we obtain the cause-specific CI functions:\n\\[ CI_j(t) = \\int_0^t f_j(u)du.\\]\nAgain, the integrals are computed using either numerical integration (via the trapezoidal rule) or Monte Carlo integration. This option is controlled by the argument method of the absoluteRisk function.\nFinally, the output from absoluteRisk can be passed to a method confint to compute confidence bands around the survival function, as described in the Theoretical Details section. These bands are only valid when family = \"glm\" as it relies on the asymptotic normality of the estimator. Currently, this is only available for the single-event setting.\n5 Illustration of package\nIn this section, we illustrate the main functions of the casebase package through three case studies. Each one showcases a different type of analysis. First, we show how to model non-constant and non-proportional hazards through a flexible specification of time. Then we perform a competing risk analysis and compare our results with the Cox model and the Fine-Gray model. The third case study illustrates how to perform variable selection in high-dimensional datasets.\nCase study 1—flexible modeling of the hazard function\nFor our first case study, we return to the ERSPC study and investigate the\ndifferences in risk between the control and screening arms. Previous re-analyses\nof these data suggest that the 20% reduction in prostate cancer death due to\nscreening was an underestimate (Hanley 2010).\nThe estimated 20% (from a proportional hazards model) did not account for\nthe delay between screening and the time the effect is expected to be observed.\nAs a result, the null effects in years 1–7 masked the substantial reductions\nthat began to appear from year 8 onward. This motivates the use of a\ntime-dependent hazard ratio which can easily be fit with the casebase\npackage by including an interaction term with time in the model. We fit a\nflexible hazard by using a smooth function of time modeled with a penalized\ncubic spline basis with 2 degrees of freedom (implemented in the\nsurvival::pspline function). The model is fit using fitSmoothHazard\nwith the familiar formula interface:\n\n\nfit <- fitSmoothHazard(DeadOfPrCa ~ pspline(Follow.Up.Time, df = 2) * ScrArm, \n                       data = ERSPC, ratio = 10)\n\n\n\nThe output object from fitSmoothHazard inherits from the singleEventCB\nand glm classes. For this reason, we can leverage the summary\nmethod for glm objects to output a familiar summary of the results:\n\n\nsummary(fit) \n\n#> Fitting smooth hazards with case-base sampling\n#> \n#> Sample size: 159893 \n#> Number of events: 540 \n#> Number of base moments: 5400 \n#> ----\n#> \n#> Call:\n#> fitSmoothHazard(formula = DeadOfPrCa ~ pspline(Follow.Up.Time, \n#>     df = 2) * ScrArm, data = ERSPC, ratio = 10)\n#> \n#> Deviance Residuals: \n#>    Min      1Q  Median      3Q     Max  \n#> -1.168  -0.486  -0.414  -0.215   3.262  \n#> \n#> Coefficients:\n#>                                                        Estimate\n#> (Intercept)                                              -13.81\n#> pspline(Follow.Up.Time, df = 2)1                           2.66\n#> pspline(Follow.Up.Time, df = 2)2                           6.43\n#> pspline(Follow.Up.Time, df = 2)3                           5.57\n#> pspline(Follow.Up.Time, df = 2)4                           7.27\n#> pspline(Follow.Up.Time, df = 2)5                           6.54\n#> pspline(Follow.Up.Time, df = 2)6                          10.82\n#> pspline(Follow.Up.Time, df = 2)7                         -11.74\n#> ScrArmScreening group                                      9.22\n#> pspline(Follow.Up.Time, df = 2)1:ScrArmScreening group    -9.25\n#> pspline(Follow.Up.Time, df = 2)2:ScrArmScreening group    -9.80\n#> pspline(Follow.Up.Time, df = 2)3:ScrArmScreening group    -9.04\n#> pspline(Follow.Up.Time, df = 2)4:ScrArmScreening group    -9.39\n#> pspline(Follow.Up.Time, df = 2)5:ScrArmScreening group   -10.65\n#> pspline(Follow.Up.Time, df = 2)6:ScrArmScreening group    -8.86\n#> pspline(Follow.Up.Time, df = 2)7:ScrArmScreening group   -11.58\n#>                                                        Std. Error\n#> (Intercept)                                                  9.98\n#> pspline(Follow.Up.Time, df = 2)1                            10.96\n#> pspline(Follow.Up.Time, df = 2)2                             9.73\n#> pspline(Follow.Up.Time, df = 2)3                            10.10\n#> pspline(Follow.Up.Time, df = 2)4                             9.90\n#> pspline(Follow.Up.Time, df = 2)5                            10.10\n#> pspline(Follow.Up.Time, df = 2)6                            10.03\n#> pspline(Follow.Up.Time, df = 2)7                            30.13\n#> ScrArmScreening group                                       13.35\n#> pspline(Follow.Up.Time, df = 2)1:ScrArmScreening group      14.85\n#> pspline(Follow.Up.Time, df = 2)2:ScrArmScreening group      12.97\n#> pspline(Follow.Up.Time, df = 2)3:ScrArmScreening group      13.54\n#> pspline(Follow.Up.Time, df = 2)4:ScrArmScreening group      13.23\n#> pspline(Follow.Up.Time, df = 2)5:ScrArmScreening group      13.55\n#> pspline(Follow.Up.Time, df = 2)6:ScrArmScreening group      13.46\n#> pspline(Follow.Up.Time, df = 2)7:ScrArmScreening group      36.92\n#>                                                        z value\n#> (Intercept)                                              -1.38\n#> pspline(Follow.Up.Time, df = 2)1                          0.24\n#> pspline(Follow.Up.Time, df = 2)2                          0.66\n#> pspline(Follow.Up.Time, df = 2)3                          0.55\n#> pspline(Follow.Up.Time, df = 2)4                          0.73\n#> pspline(Follow.Up.Time, df = 2)5                          0.65\n#> pspline(Follow.Up.Time, df = 2)6                          1.08\n#> pspline(Follow.Up.Time, df = 2)7                         -0.39\n#> ScrArmScreening group                                     0.69\n#> pspline(Follow.Up.Time, df = 2)1:ScrArmScreening group   -0.62\n#> pspline(Follow.Up.Time, df = 2)2:ScrArmScreening group   -0.76\n#> pspline(Follow.Up.Time, df = 2)3:ScrArmScreening group   -0.67\n#> pspline(Follow.Up.Time, df = 2)4:ScrArmScreening group   -0.71\n#> pspline(Follow.Up.Time, df = 2)5:ScrArmScreening group   -0.79\n#> pspline(Follow.Up.Time, df = 2)6:ScrArmScreening group   -0.66\n#> pspline(Follow.Up.Time, df = 2)7:ScrArmScreening group   -0.31\n#>                                                        Pr(>|z|)\n#> (Intercept)                                                0.17\n#> pspline(Follow.Up.Time, df = 2)1                           0.81\n#> pspline(Follow.Up.Time, df = 2)2                           0.51\n#> pspline(Follow.Up.Time, df = 2)3                           0.58\n#> pspline(Follow.Up.Time, df = 2)4                           0.46\n#> pspline(Follow.Up.Time, df = 2)5                           0.52\n#> pspline(Follow.Up.Time, df = 2)6                           0.28\n#> pspline(Follow.Up.Time, df = 2)7                           0.70\n#> ScrArmScreening group                                      0.49\n#> pspline(Follow.Up.Time, df = 2)1:ScrArmScreening group     0.53\n#> pspline(Follow.Up.Time, df = 2)2:ScrArmScreening group     0.45\n#> pspline(Follow.Up.Time, df = 2)3:ScrArmScreening group     0.50\n#> pspline(Follow.Up.Time, df = 2)4:ScrArmScreening group     0.48\n#> pspline(Follow.Up.Time, df = 2)5:ScrArmScreening group     0.43\n#> pspline(Follow.Up.Time, df = 2)6:ScrArmScreening group     0.51\n#> pspline(Follow.Up.Time, df = 2)7:ScrArmScreening group     0.75\n#> \n#> (Dispersion parameter for binomial family taken to be 1)\n#> \n#>     Null deviance: 3619.1  on 5939  degrees of freedom\n#> Residual deviance: 3359.1  on 5924  degrees of freedom\n#> AIC: 3391\n#> \n#> Number of Fisher Scoring iterations: 7\n\nAs noted in the Theoretical Details section, the usual asymptotic results hold\nfor likelihood ratio tests built using case-base sampling models. Therefore, we\ncan easily test the significance of the spline term and its interaction with time:\n\n\nanova(fit, test = \"LRT\")\n\n#> Analysis of Deviance Table\n#> \n#> Model: binomial, link: logit\n#> \n#> Response: DeadOfPrCa\n#> \n#> Terms added sequentially (first to last)\n#> \n#> \n#>                                        Df Deviance Resid. Df\n#> NULL                                                    5939\n#> pspline(Follow.Up.Time, df = 2)         7    246.6      5932\n#> ScrArm                                  1      5.6      5931\n#> pspline(Follow.Up.Time, df = 2):ScrArm  7      7.9      5924\n#>                                        Resid. Dev Pr(>Chi)    \n#> NULL                                         3619             \n#> pspline(Follow.Up.Time, df = 2)              3373   <2e-16 ***\n#> ScrArm                                       3367    0.018 *  \n#> pspline(Follow.Up.Time, df = 2):ScrArm       3359    0.343    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSimilarly, to compare different models (e.g. time modeled linearly), we could\ncompute Akaike’s Information Criterion (AIC) for each model.\nTime-dependent hazard ratios\nIn what follows, the hazard ratio for a variable \\(X\\) is defined as\n\\[\n\\frac{\\lambda\\left(t | X=x_1, \\mathbf{Z}=\\mathbf{z} ; \\widehat{\\beta}\\right)}{\\lambda(t | X=x_0, \\mathbf{Z}=\\mathbf{z} ; \\widehat{\\beta})}\n\\]\nwhere \\(\\lambda(t|\\cdot;\\widehat{\\beta})\\) is the hazard rate as a function of the\nvariable \\(t\\) (which is usually time, but can be any other continuous variable),\n\\(x_1\\) is the value of \\(X\\) for the exposed group, \\(x_0\\) is the value of \\(X\\) for\nthe unexposed group, \\(\\mathbf{Z}\\) are other covariates in the model which are\nequal to \\(\\mathbf{z}\\) in both the exposed and unexposed group,\nand \\(\\widehat{\\beta}\\) are the estimated regression coefficients. As indicated by the\nformula above, it is most instructive to plot the hazard ratio as a function of\na variable \\(t\\) only if there is an interaction between \\(t\\) and \\(X\\). Otherwise,\nthe resulting plot will simply be a horizontal line across time.\nThe plot method with type=\"hr\" for objects of class\nsingleEventCB can be used to compute time-dependent hazard ratios and\nconfidence intervals. In Figure 2, we show the\nestimated hazard ratio and 95% confidence interval for screening vs. control\ngroup as a function of time. Note that we must specify the covariate profile for\nthe reference group and times for the predicted hazards.\n\n\nnew_time <- seq(1, 12, by  = 0.1)\nnew_data <- data.frame(ScrArm = factor(\"Control group\",\n                                         levels = c(\"Control group\",\"Screening group\")),\n                      Follow.Up.Time = new_time)\nplot(fit, type = \"hr\", newdata = new_data,\n     var = \"ScrArm\", xvar = \"Follow.Up.Time\", ci = TRUE)\n\n\n\n\n\nFigure 2: Estimated hazard ratio and 95% confidence interval for screening vs. control group as a function of time in the ERSPC dataset. Hazard ratios are estimated from fitting a parametric hazard model as a function of the interaction between a cubic pspline basis (df=2) of follow-up time and treatment arm. 95% confidence intervals are calculated using the delta method. The plot shows that the effect of screening only begins to become statistically apparent by year 7. The 25-60% reductions seen in years 8-12 of the study suggests a much higher reduction in prostate cancer due to screening than the single overall 20% reported in the original article.\n\n\n\nThe plot shows that the effect of screening only becomes statistically apparent by year 7 and later. The 25-60% reductions seen in years 8-12 of the study suggests a much higher reduction in prostate cancer due to screening than the single overall 20% reported in the original article.\nA more parsimonious model based on these results could be constructed as follows:\n\\[ \\log \\lambda\\left(t \\mid \\mathbf{Z}\\right) = \\begin{cases} \\alpha, &\\quad t < t_0\\\\\n\\alpha + \\beta Z(t - t_0), &\\quad t \\geq t_0\\end{cases},\\]\nwhere \\(t_0 \\approx 7\\) years. This model corresponds to a hazard ratio that is constant and equal to 1 until \\(t = t_0\\), after which it decreases exponentially. If we fix the value \\(t_0\\), we can easily implement this model using our package; for example:\n\n\nfitSmoothHazard(DeadOfPrCa ~ as.integer(Follow.Up.Time >= t0) :\n                    ScrArm : I(Follow.Up.Time - t0),\n                data = ERSPC)\n\n\nHere, we use the binary variable as.integer(Follow.Up.Time >= t0) in order to write the two cases of our formula above in a more compact way. We also use the function I(), which allows us to specify new variables within the formula using normal R code. This is required, as otherwise the symbol - would be interpreted as a formula operator. Finally, the term ScrArm : I(Follow.Up.Time - t0) represents the product \\(Z (t - t_0)\\) in the equation above. We use the operator : instead of * in order to only include the interaction term, not the main effects.\nAlternatively, the breakpoint \\(t_0\\) could also be estimated by combining case-base sampling with segmented regression. However, this extension is beyond the current scope of casebase.\nHazard functions\nModeling the hazard function directly allows us to easily visualize it with\nthe plot method and type=\"hazard\" for objects of class singleEventCB.\nWe plot the hazard functions for both treatment arms in Figure 3.\nThe pattern we see is consistent with the population-time plot shown in Figure 1C, where more events are observed at later follow-up times.\nThe drop at the end can be explained by the fact that very few observations were followed for the entire 15 year period.\n\n\nplot(fit, type = \"hazard\",\n     hazard.params = list(xvar = \"Follow.Up.Time\",\n                          by = \"ScrArm\"))\n\n#> Conditions used in construction of plot\n#> ScrArm: Control group / Screening group\n#> offset: 0\n\n\nFigure 3: Estimated hazard functions for control and screening groups in the ERSPC dataset. Hazards are estimated from fitting a parametric model with casebase sampling as a function of the interaction between a cubic pspline basis (df=2) of follow-up time and treatment arm. The package vignettes provide a detailed description of how to plot hazard functions for any combination of covariates along with confidence bands.\n\n\n\nAbsolute risk\nNext, the absoluteRisk function takes as input the singleEventCB object and returns a matrix where each column corresponds to the covariate profiles specified in the newdata argument, and each row corresponds to time points specified by the time argument:\n\n\nnew_data <- data.frame(ScrArm = c(\"Control group\", \"Screening group\"))\nnew_time <- seq(0,14,0.1)\nrisk <- absoluteRisk(fit, time = new_time, newdata = new_data)\n\n\n\n\n\n\n\nWe can subsequently compute confidence intervals for the risk function using\nthe method confint.absRiskCB:\n\n\nconf_ints <- confint(risk, fit)\nhead(conf_ints)\n\n#>   time estimate conf.low conf.high      cov_prof\n#> 1  0.0  0.0e+00  0.0e+00   0.0e+00 Control group\n#> 2  0.1  1.8e-06  1.9e-07   1.2e-05 Control group\n#> 3  0.2  3.9e-06  4.8e-07   2.2e-05 Control group\n#> 4  0.3  6.1e-06  9.0e-07   3.2e-05 Control group\n#> 5  0.4  8.7e-06  1.5e-06   4.1e-05 Control group\n#> 6  0.5  1.2e-05  2.3e-06   5.0e-05 Control group\n\nIn Figure 4, we see the 95% confidence bands around the estimates.\nWe also overlay the Kaplan-Meier curves as a reference.\n\n\n\nFigure 4: Risk function estimates for control and screening groups in the ERSPC data using case-base sampling and Kaplan-Meier, along with 95% confidence bands. The smooth curve (case-base sampling) vs. step function (Cox model) highlight one of the main differences between the two approaches. The larger confidence bands in the later years is due to the relatively few number of individuals who were followed for more than 12 years.\n\n\n\nCase study 2—competing risk analysis\nIn this case study, we show how case-base sampling can be used in the context of a competing risk analysis. For illustrative purposes, we use the same data that was used in Scrucca et al (2010). The data was downloaded from the first author’s website, and it is now available as part of the casebase package.\n\n\n\nThe data contains information on 177 patients who received a stem-cell transplant for acute leukemia. The event of interest is relapse, but other competing causes (e.g. death, progression, graft failure, graft-versus-host disease) were also recorded. Several covariates were captured at baseline: sex, disease type (acute lymphoblastic or myeloblastic leukemia, abbreviated as ALL and AML, respectively), disease phase at transplant (Relapse, CR1, CR2, CR3), source of stem cells (bone marrow and peripheral blood, coded as BM+PB, or only peripheral blood, coded as PB), and age.\nFirst, we look at a population-time plot to visualize the incidence density of both relapse and the competing events. In Figure 5, failure times are highlighted on the plot using red dots for the event of interest and blue dots for competing events. In this plot, we see evidence of a non-constant hazard function: the density of points is larger at the beginning of follow-up than at the end.\n\n\n\nFigure 5: Population-time plot for the stem-cell transplant study with both relapse and competing events. The area representing the population time is shown in gray, with subjects having the least amount of observation time plotted at the top of the y-axis. The y-axis location of each case series and competing event moment is sampled at random vertically on the plot to avoid having all points along the upper edge of the gray area. The density of points at the beginning of follow-up relative to the end indicates a non-constant hazard function.\n\n\n\nOur main objective is to compute the cumulative incidence of relapse for a given set of covariates. We start by fitting a smooth hazard to the data using a linear term for time:\n\n\nmodel_cb <- fitSmoothHazard(\n  Status ~ ftime + Sex + D + Phase + Source + Age,\n  data = bmtcrr,\n  ratio = 100,\n  time = \"ftime\"\n)\n\n\nWe want to compare our hazard ratio estimates to that obtained from a Cox regression (using the survival package version 3.2-13).\n\n\nlibrary(survival)\n# Prepare data for coxph\nbmtcrr_cox <- transform(bmtcrr, \n                        id = seq_len(nrow(bmtcrr)),\n                        Status = factor(Status))\n\nmodel_cox <- coxph(Surv(ftime, Status) ~ Sex + D + Phase + Source + Age,\n                   data = bmtcrr_cox, id = id)\n\n\n\n\nTable 2: Estimates and confidence intervals for the hazard ratios for each coefficient. Both estimates from case-base sampling and Cox regression are presented.\n\n\n\n\n\nCase-Base\n\n\n\n\nCox\n\n\n\nCovariates\n\n\nHR\n\n\n95% Conf.\n\n\nHR\n\n\n95% Conf.\n\n\nSex\n\n\n0.73\n\n\n(0.42, 1.27)\n\n\n0.68\n\n\n(0.39, 1.21)\n\n\nDisease\n\n\n0.54\n\n\n(0.3, 0.98)\n\n\n0.52\n\n\n(0.28, 0.94)\n\n\nPhase (CR2 vs. CR1)\n\n\n1.19\n\n\n(0.48, 2.97)\n\n\n1.21\n\n\n(0.47, 3.09)\n\n\nPhase (CR3 vs. CR1)\n\n\n1.44\n\n\n(0.37, 5.64)\n\n\n1.67\n\n\n(0.46, 6.08)\n\n\nPhase (Relapse vs. CR1)\n\n\n4.22\n\n\n(1.95, 9.12)\n\n\n4.55\n\n\n(1.98, 10.46)\n\n\nSource\n\n\n1.46\n\n\n(0.48, 4.46)\n\n\n1.46\n\n\n(0.47, 4.53)\n\n\nAge\n\n\n0.99\n\n\n(0.97, 1.02)\n\n\n0.99\n\n\n(0.97, 1.02)\n\n\nFrom the fit object, we can extract both the hazard ratios and their corresponding confidence intervals. These quantities appear in Table 2. As we can see, the type of disease corresponds to a significant hazard ratio: the hazard for AML is about half that for ALL. Moreover, being in relapse at transplant is associated with a hazard ratio of 4.22 when compared to CR1.\nGiven the estimate of the hazard functions obtained using case-base sampling, we can compute the cumulative incidence curve for a fixed covariate profile. We perform this computation for a 35 year old woman who received a stem-cell transplant from peripheral blood at relapse. We compare the absolute risk curve for such a woman with ALL with that for a similar woman with AML.\n\n\n\nNext, we compare our estimates to that obtained from a corresponding Fine-Gray model (1999). The Fine-Gray model is a semiparametric model for the cause-specific subdistribution hazard, i.e. the function \\(d_j(t)\\) such that\n\\[CI_j(t) = 1 - \\exp\\left( - \\int_0^t d_j(u) \\textrm{d}u \\right),\\]\nwhere \\(CI_j(t)\\) is the cause-specific CI. The Fine-Gray model allows to directly assess the effect of a covariate on the subdistribution hazard, as opposed to the cause-specific hazard. For the computation, we use the timereg package (Scheike and Zhang 2011):\n\n\nlibrary(timereg)\nmodel_fg <- comp.risk(Event(ftime, Status) ~ const(Sex) + const(D) +\n                        const(Phase) + const(Source) + const(Age),\n                      data = bmtcrr, cause = 1, model = \"fg\")\n\n# Estimate CI curve\nrisk_fg <- predict(model_fg, newdata, times = time_points)\n\n\nWe can also estimate the CI for relapse using the Cox model and the Aalen-Johansen estimator:\n\n\n# Estimate absolute risk curve\nrisk_cox <- survfit(model_cox, newdata = newdata)\n\n\n\n\n\nFigure 6: Cumulative Incidence curve for a fixed covariate profile and the two disease groups. The estimate obtained from case-base sampling is compared to the Fine-Gray and Aalen-Johansen estimates. In general, the three approaches agree quite well for AML, while there seems to be a difference of about 5% between the Fine-Gray curve and the curves estimated using case-base sampling and Cox regression for ALL. However, this difference does not appear to be significant as the curve from case-base sampling is contained within a 95% confidence band around the Fine-Gray absolute risk curve (figure not shown).\n\n\n\nFigure 6 shows the CI curves for all three models. As we can see, all three approaches agree quite well for AML; however, for ALL, there seems to be a difference of about 5% between the Fine-Gray curve and the curves estimated using case-base sampling and Cox regression. This difference does not appear to be significant: the curve from case-base sampling is contained within a 95% confidence band around the Fine-Gray absolute risk curve (figure not shown).\nCase study 3—variable selection\nFor the third case study, we show how casebase can also be used for variable selection through regularized estimation of the hazard function as given by Equation (5). We note that this is different than the semiparametric model Coxnet, which regularizes the Cox partial likelihood. To illustrate this functionality, we use the dataset from the Study to Understand Prognoses Preferences Outcomes and Risks of Treatment (SUPPORT) (Knaus et al. 1995).3 The SUPPORT dataset tracks death in five American hospitals within individuals who are considered seriously ill. The cleaned and imputed data consists of 9104 observations and 30 variables, and it is available as part of the casebase package. In the comparisons below, all covariates except sps and aps were used. These two variables correspond to scores for predicting the outcome that were developed as part of the original study. For more information about this dataset, the reader is encouraged to look at the documentation in our package.\nFor our penalized case-base model, we opt for the natural log of time which corresponds to a Weibull distribution. For fitting the penalized hazard, we use fitSmoothHazard.fit, which is a matrix interface to the fitSmoothHazard function. The fitSmoothHazard and fitSmoothHazard.fit functions sample the case and base series, calculate the required offset, and transform the data to match the expected input of the glmnet package. The penalized logistic regression is then fit for multiple values of the tuning parameter using the function glmnet::cv.glmnet and the binomial family. To fitSmoothHazard.fit, we supply both a matrix y containing the time and event variables, and a matrix x containing all other covariates. We apply the lasso penalty by setting alpha = 1 and assign a penalty.factor (\\(w_j\\); cf. Equation (5)) of 0 to the time variable to ensure it is in the selected model. We compare our approach to both Cox regression, and lasso penalized Cox regression (fitted via the glmnet package and using the Cox family).\nTo compare the performance of our models, we split the data into 95% training and 5% test sets. To assess both discrimination and calibration, we use a time-dependent version of the classical Brier score that is adjusted for censoring (Graf et al. 1999). The Brier score can be used with both parametric and semi-parametric models. We use the riskRegression package to compute these scores for all models.\n\n\n\n\n\n\n\n\n# Create matrices for inputs\nx <- model.matrix(death ~ . - d.time - aps - sps, \n                  data = train)[, -c(1)] # Remove intercept\ny <- data.matrix(subset(train, select = c(d.time, death)))\n\n# Regularized logistic regression to estimate hazard\npen_cb <- casebase::fitSmoothHazard.fit(x, y,\n  family = \"glmnet\",\n  time = \"d.time\", event = \"death\",\n  formula_time = ~ log(d.time), alpha = 1,\n  ratio = 10, standardize = TRUE,\n  penalty.factor = c(0, rep(1, ncol(x)))\n)\n\n\n\n\n\nIn Figure 7, we show the coefficient estimates for covariates that we selected by both penalized Cox and penalized case-base. We note that both penalized approaches produce similar results. We can also clearly see the shrinkage effect owing to the \\(\\ell_1\\) penalty.\n\n\n\nFigure 7: Coefficient estimates from the Cox model (Cox), penalized Cox model using the glmnet package (Pen. Cox), and our approach using penalized case-base sampling (Pen. CB). Only the covariates that were selected by both penalized approaches are shown. The shrinkage of the coefficient estimates for Pen. Cox and Pen. CB occurs due to the \\(\\ell_1\\) penalty.\n\n\n\nWe then compare the risk estimation over the test set. The predicted probabilities for each test set observation are averaged, resulting in the absolute risk curves shown in Figure 8A. The Kaplan-Meier curve is calculated on the test set only. We see minimal differences between the four approaches across follow-up-time. Note that the apparent smoothness of the Cox and penalized Cox curves is due to the large number of observations in the training set, which is used to derive the Breslow estimate of the baseline hazard. As described above, we compare the performance between the models by computing the Brier scores over time. In Figure 8B, we see that the adjusted models all perform similarly, outperforming the Kaplan-Meier estimate.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Comparison of Cox regression (Cox), penalized Cox regression (Pen. Cox), penalized case-base sampling estimation (Pen. CB), and Kaplan-Meier (K-M). (A) Probability of death as a function of follow-up time which is the average of the predicted probabilities for each test set observation. The Kaplan-Meier curve is calculated on the test set only. We see minimal differences between the four approaches across follow-up-time for the absolute risk curves. Note that the apparent smoothness of the Cox and penalized Cox curves is due to the large number of observations in the training set, which is used to derive the Breslow estimate of the baseline hazard. (B) Brier score as a function of follow-up time, where a lower score corresponds to better performance. We see that the adjusted models all perform similarly, outperforming the Kaplan-Meier estimate.\n\n\n\n6 Discussion\nIn this article, we presented the R package casebase, which provides functions for fitting smooth parametric hazards and estimating risk functions using case-base sampling. Our package also provides several functions to produce graphical summaries of the data and the results. We outlined the theoretical underpinnings of the approach, we provided details about our implementation, and we illustrated the merits of the case-base framework and the package through three case studies.\nAs a methodological framework, case-base sampling is very flexible. Some of this flexibility has been explored before in the literature: for example, Saarela and Hanley (2015) used case-base sampling to model a time-dependent exposure variable in a vaccine safety study. As another example, Saarela and Arjas (2015) combined case-base sampling and a Bayesian non-parametric framework to compute individualized risk assessments for chronic diseases. In the case studies above, we further explored this flexibility along two fronts. On the one hand, we showed how splines could be used as part of the linear predictor to model the effect of time on the hazard. This strategy yielded estimates of the survival function that were qualitatively similar to semiparametric estimates derived from Cox regression; however, case-base sampling led to estimates of the survival function that vary smoothly in time and are thus easier to interpret. On the other hand, we also displayed the flexibility of case-base sampling by showing how it could be combined with penalized logistic regression to perform variable selection. Furthermore, the second case study showed how case-base sampling can be applied to competing risks settings. It should be noted that the three case studies presented above only considered covariates that were fixed at baseline. In one of the package vignettes, we use the Stanford Heart Transplant data Crowley and Hu (1977) to show how case-base sampling can also be used in the context of time-dependent exposure. In this study, the exposure period was defined as the week following vaccination. Hence, the main covariate of interest, i.e. exposure to the vaccine, was changing over time. In this context, case-base sampling offers an efficient alternative to nested case-control designs or self-matching.\nEven though we did not illustrate it in this article, case-base sampling can also be combined with the framework of generalized additive models. This functionality has already been implemented in the package. Similarly, case-base sampling can be combined with quasi-likelihood estimation to fit survival models that can account for the presence of over-dispersion. All of these examples illustrate how the case-base sampling framework in general, and the package casebase in particular, allows the user to fit a broad and flexible family of survival functions.\nPoisson regression can also be used to estimate the full hazard by discretizing time. However, this method requires user input on the number of intervals, or equivalently, on the cut points. This choice made by the user can have a significant impact on the model. Small intervals may result in many empty, non-informative bins. This may cause convergence issues for the Newton-Raphson procedure (Kalbfleisch and Prentice 2011). If the intervals are too wide, the nonlinear trends that are present in the hazard may be masked. Rather than discretizing time like in Poisson regression, case-base sampling provides a continuous-time approach to using GLMs for estimating hazard functions.\nAs presented in Hanley & Miettinen (2009), case-base sampling is comprised of three steps: 1) sampling a case series and a base series from the study; 2) fit the log-hazard as a linear function of predictors (including time); and 3) use the fitted hazard to estimate the risk function. Accordingly, our package provides functions for each step. Moreover, the simple interface of the fitSmoothHazard function resembles the glm interface. This interface should look familiar to new users. Our modular approach also provides a convenient way to extend our package for new sampling or fitting strategies.\nIn the case studies above, we compared the performance of case-base sampling with that of Cox regression and Fine-Gray models. In terms of function interface, casebase uses a formula interface that is closer to that of glm, in that the event variable is the only variable appearing on the left-hand side of the formula. By contrast, both survival::coxph and timereg::comp.risk use arrays that capture both the event type and time. Both approaches to modeling yield user-friendly code. However, in terms of output, both approaches differ significantly. Case-base sampling produces smooth hazards and smooth survival curves, whereas Cox regression and Fine-Gray models produce stepwise CIs and never explicitly model the hazard function. Qualitatively, we showed that by using splines in the linear predictor, all three models yielded similar curves.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur choice of modeling the log-hazard as a linear function of covariates allows us to develop a simple computational scheme for estimation. However, as a downside, it does not allow us to model location and scale parameters separately like the package flexsurv. For example, if we look at the Weibull distribution as parametrised in stats::pweibull, the log-hazard function is given by\n\\[ \\log \\lambda(t; \\alpha, \\beta) = \\left[\\log(\\alpha/\\beta) - (\\alpha - 1)\\log(\\beta)\\right] + (\\alpha - 1)\\log t,\\]\nwhere \\(\\alpha,\\beta\\) are shape and scale parameters, respectively. Unlike casebase, the approach taken by flexsurv also allows the user to model the scale parameter as a function of covariates. Of course, this added flexibility comes at the cost of interpretability: by modeling the log-hazard directly, the parameter estimates from casebase can be interpreted as estimates of log-hazard ratios. To improve the flexibility of casebase at capturing the scale of a parametric family, we could replace the logistic regression with its quasi-likelihood counterpart and therefore model over- and under-dispersion with respect to the logistic likelihood. We defer the study of the properties and performance of such a model to a future article.\nFuture work will look at some of the methodological extensions of case-base sampling. First, to assess the quality of the model fit, we would like to study the properties of the residuals (e.g. Cox-Snell, martingale). More work needs to be done to understand these residuals in the context of the partial likelihood underlying case-base sampling. The resulting diagnostic tools could then be integrated in this package. Also, we are interested in extending case-base sampling to account for interval censoring. This type of censoring is very common in longitudinal studies, and many packages (e.g. SmoothHazard, survival and rstpm2) provide functions to account for it. Again, we hope to include any resulting methodology as part of this package.\nIn future versions of the package, we also want to increase the complement of diagnostic and inferential tools that are currently available. For example, we would like to include more functions to compute calibration and discrimination statistics (e.g. AUC) for our models. Saarela and Arjas (2015) also describe how to obtain a posterior distribution for the AUC from their model. Their approach could potentially be included in casebase. Finally, we want to provide more flexibility in how the case-base sampling is performed. This could be achieved by adding a hazard argument to the function sampleCaseBase. In this way, users could specify their own sampling mechanism. For example, they could provide a hazard that gives sampling probabilities that are proportional to the cardiovascular disease event rate given by the Framingham score (Saarela and Arjas 2015).\nIn conclusion, we presented the R package casebase which implements case-base sampling for fitting parametric survival models and for estimating smooth survival functions using the framework of generalized linear models. We strongly believe that its flexibility and its foundation on the familiar logistic regression model will make it appealing to new and established practitioners. The casebase package is freely available from the Comprehensive R Archive Network at https://cran.r-project.org/package=casebase. Interested users can visit http://sahirbhatnagar.com/casebase/ for detailed package documentation and vignettes.\n7 Acknowledgments\nWe would like to thank the anonymous reviewers for their insightful comments and criticisms. We would also like to thank Yi Yang for helpful discussions on penalized regression models. Bhatnagar (RGPIN-2020-05133) and Turgeon (RGPIN-2021-04073) both gratefully acknowledge funding via a Discovery Grant from the Natural Sciences and Engineering Research Council of Canada (NSERC), RGPIN.\n\n\n\n\n\n\n\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-052.zip\nCRAN packages used\ncasebase, CFC, flexsurv, SmoothHazard, rstpm2, mets, survival, glmnet, glmpath, penalized, riskRegression, cmprsk, ggplot2, mgcv, VGAM, timereg\nCRAN Task Views implied by cited packages\nBayesian, CausalInference, ClinicalTrials, Distributions, Econometrics, Environmetrics, Epidemiology, ExtremeValue, MachineLearning, MixedModels, Phylogenetics, Psychometrics, Spatial, Survival, TeachingStatistics\n\n\nO. Aalen, O. Borgan and H. Gjessing. Survival and event history analysis: A process point of view. Springer Science & Business Media, 2008.\n\n\nA. Allignol and A. Latouche. CRAN task view: Survival analysis. CRAN Task View: Survival Analysis, 2019. URL https://cran.r-project.org/web/views/Survival.html.\n\n\nE. Arjas and P. Haara. A logistic regression model for hazard: Asymptotic results. Scandinavian Journal of Statistics, 1–18, 1987.\n\n\nS. Bhatnagar, M. Turgeon, J. Islam, O. Saarela and J. Hanley. Casebase: Fitting flexible smooth-in-time hazards and risk functions via logistic and multinomial regression. 2021. URL https://CRAN.R-project.org/package=casebase. R package version 0.9.1.\n\n\nN. Breslow. Discussion of the paper by Dr Cox cited below. Journal of the Royal Statistical Society: Series B (Methodological), 34: 187–220, 1972.\n\n\nB. Carstensen. Who needs the cox model anyway. 2019. URL http://bendixcarstensen.com/WntCma.pdf.\n\n\nD. A. Clark, E. B. Stinson, R. B. Griepp, J. S. Schroeder, N. E. Shumway and D. Harrison. Cardiac transplantation in man. Annals of Internal Medicine, 75(1): 15–21, 1971.\n\n\nM. Clements and X.-R. Liu. rstpm2: Smooth survival models, including generalized survival models. 2019. URL https://CRAN.R-project.org/package=rstpm2. R package version 1.5.1.\n\n\nJ. Crowley and M. Hu. Covariance analysis of heart transplant survival data. Journal of the American Statistical Association, 72(357): 27–36, 1977.\n\n\nB. Efron. Logistic regression, survival analysis, and the kaplan-meier curve. Journal of the American statistical Association, 83(402): 414–425, 1988.\n\n\nB. Efron and R. J. Tibshirani. An introduction to the bootstrap. CRC press, 1994.\n\n\nJ. P. Fine and R. J. Gray. A proportional hazards model for the subdistribution of a competing risk. Journal of the American statistical association, 94(446): 496–509, 1999.\n\n\nJ. Friedman, T. Hastie and R. Tibshirani. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1): 2010. DOI 10.18637/jss.v033.i01.\n\n\nT. A. Gerds and B. Ozenne. riskRegression: Risk regression models and prediction scores for survival analysis with competing risks. 2020. URL https://CRAN.R-project.org/package=riskRegression. R package version 2020.02.05.\n\n\nJ. J. Goeman. L1 penalized estimation in the cox proportional hazards model. Biometrical Journal, (52): –14, 2010.\n\n\nE. Graf, C. Schmoor, W. Sauerbrei and M. Schumacher. Assessment and comparison of prognostic classification schemes for survival data. Statistics in medicine, 18(17-18): 2529–2545, 1999.\n\n\nJ. A. Hanley. Mortality reductions produced by sustained prostate cancer screening have been underestimated. Journal of Medical Screening, 17(3): 147–151, 2010.\n\n\nJ. A. Hanley and O. S. Miettinen. Fitting smooth-in-time prognostic risk functions via logistic regression. The International Journal of Biostatistics, 5(1): 2009.\n\n\nT. Hastie and R. Tibshirani. Generalized additive models: Some applications. Journal of the American Statistical Association, 82(398): 371–386, 1987.\n\n\nC. Jackson. flexsurv: A platform for parametric survival modeling in R. Journal of Statistical Software, 70(8): 1–33, 2016. DOI 10.18637/jss.v070.i08.\n\n\nJ. D. Kalbfleisch and R. L. Prentice. The statistical analysis of failure time data. John Wiley & Sons, 2011.\n\n\nW. A. Knaus, F. E. Harrell, J. Lynn, L. Goldman, R. S. Phillips, A. F. Connors, N. V. Dawson, W. J. Fulkerson, R. M. Califf, N. Desbiens, et al. The SUPPORT prognostic model: Objective estimates of survival for seriously ill hospitalized adults. Annals of internal medicine, 122(3): 191–203, 1995.\n\n\nZ. Liu, B. Rich and J. A. Hanley. Recovering the raw data behind a non-parametric survival curve. Systematic reviews, 3(1): 151, 2014.\n\n\nA. Mahani and M. Sharabiani. Bayesian, and non-Bayesian, cause-specific competing-risk analysis for parametric and nonparametric survival functions: The R package CFC. Journal of Statistical Software, 89(9): 1–29, 2019. DOI 10.18637/jss.v089.i09.\n\n\nM. Y. Park and T. Hastie. Glmpath: L1 regularization path for generalized linear models and cox proportional hazards model. 2018. URL https://CRAN.R-project.org/package=glmpath. R package version 0.98.\n\n\nN. Reid. A conversation with sir david cox. Statistical Science, 9(3): 439–455, 1994.\n\n\nO. Saarela. A case-base sampling method for estimating recurrent event intensities. Lifetime data analysis, 22(4): 589–605, 2016.\n\n\nO. Saarela and E. Arjas. Non-parametric Bayesian hazard regression for chronic disease risk assessment. Scandinavian Journal of Statistics, 42(2): 609–626, 2015.\n\n\nO. Saarela and J. A. Hanley. Case-base methods for studying vaccination safety. Biometrics, 71(1): 42–52, 2015.\n\n\nT. H. Scheike, K. K. Holst and J. B. Hjelmborg. Estimating twin concordance for bivariate competing risks twin data. Statistics in medicine, 33(7): 1193–1204, 2014.\n\n\nT. H. Scheike and M.-J. Zhang. Analyzing competing risk data using the R timereg package. Journal of Statistical Software, 38(2): 1–15, 2011.\n\n\nF. H. Schröder, J. Hugosson, M. J. Roobol, T. L. Tammela, S. Ciatto, V. Nelen, M. Kwiatkowski, M. Lujan, H. Lilja, M. Zappa, et al. Screening and prostate-cancer mortality in a randomized European study. New England Journal of Medicine, 360(13): 1320–1328, 2009.\n\n\nL. Scrucca, A. Santucci and F. Aversa. Regression modeling of competing risk using R: An in depth guide for clinicians. Bone marrow transplantation, 45(9): 1388, 2010.\n\n\nN. Simon, J. Friedman, T. Hastie and R. Tibshirani. Regularization paths for Cox’s proportional hazards model via coordinate descent. Journal of Statistical Software, 39(5): 1–13, 2011.\n\n\nT. M. Therneau. A package for survival analysis in s. 2015. URL https://CRAN.R-project.org/package=survival. version 2.38.\n\n\nC. Touraine, T. A. Gerds and P. Joly. SmoothHazard: An R package for fitting regression models to interval-censored observations of illness-death models. Journal of Statistical Software, 79(7): 1–22, 2017. DOI 10.18637/jss.v079.i07.\n\n\nA. W. Van der Vaart. Asymptotic statistics. Cambridge university press, 2000.\n\n\nJ. Whitehead. Fitting cox’s regression model to survival data using GLIM. Journal of the Royal Statistical Society: Series C (Applied Statistics), 29(3): 268–275, 1980.\n\n\nH. Wickham. ggplot2: Elegant graphics for data analysis. Springer-Verlag New York, 2016. URL https://ggplot2.tidyverse.org.\n\n\nH. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Methodological), 67(2): 301–320, 2005.\n\n\nThe conditional chance of failure in a time interval of specified length is the same regardless of how long the individual has been in the study. This is also known as the memoryless property (Kalbfleisch and Prentice 2011).↩︎\nSpecifically, it corresponds to the following parametrization for a multinomial random variable \\(Y\\): \\[\\begin{align*} \\log\\left(\\frac{P(Y=j \\mid X)}{P(Y = J \\mid X)}\\right) = X^T\\beta_j + log(1/\\rho), \\qquad j = 1,\\ldots, J-1.\\end{align*}\\]↩︎\nThe original data is available online from the Department of Biostatistics at Vanderbilt University: https://biostat.app.vumc.org/wiki/Main/SupportDesc↩︎\n",
    "preview": "articles/RJ-2022-052/plot-erspc-data-1.png",
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 1008
  },
  {
    "path": "articles/RJ-2022-053/",
    "title": "logitFD: an R package for functional principal component logit regression",
    "description": "The functional logit regression model was proposed by [@Escabias04] with the objective of modeling a scalar binary response variable from a functional predictor. The model estimation proposed in that case was performed in a subspace of $L^2(T)$ of squared integrable functions of finite dimension, generated by a finite set of basis functions. For that estimation it was assumed that the curves of the functional predictor and the functional parameter of the model belong to the same finite subspace. The estimation so obtained was affected by high multicollinearity problems and the solution given to these problems was based on different functional principal component analysis. The [logitFD](https://CRAN.R-project.org/package=logitFD) package introduced here provides a toolbox for the fit of these models by implementing the different proposed solutions and by generalizing the model proposed in 2004 to the case of several functional and non-functional predictors. The performance of the functions is illustrated by using data sets of functional data included in the [fda.usc](https://CRAN.R-project.org/package=fda.usc) package from R-CRAN.",
    "author": [
      {
        "name": "Manuel Escabias",
        "url": {}
      },
      {
        "name": "Ana M. Aguilera",
        "url": {}
      },
      {
        "name": "Christian Acal",
        "url": {}
      }
    ],
    "date": "2022-12-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-053.zip\n\n\nM. Escabias, A. M. Aguilera and M. J. Valderrama. Principal component estimation of functional logistic regression: Discussion of two different approaches. Journal of Nonparametric Statistics, 16(3-4): 365–384, 2004. URL https://doi.org/10.1080/10485250310001624738.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-054/",
    "title": "eat: An R Package for fitting Efficiency Analysis Trees",
    "description": "eat is a new package for R that includes functions to estimate production frontiers and technical efficiency measures through non-parametric techniques based upon regression trees. The package specifically implements the main algorithms associated with a recently introduced methodology for estimating the efficiency of a set of decision-making units in Economics and Engineering through Machine Learning techniques, called Efficiency Analysis Trees [@esteve2020]. The package includes code for estimating input- and output-oriented radial measures, input- and output-oriented Russell measures, the directional distance function and the weighted additive model, plotting graphical representations of the production frontier by tree structures, and determining rankings of importance of input variables in the analysis. Additionally, it includes the code to perform an adaptation of Random Forest in estimating technical efficiency. This paper describes the methodology and implementation of the functions, and reports numerical results using a real data base application.",
    "author": [
      {
        "name": "Miriam Esteve",
        "url": "https://cio.umh.es/"
      },
      {
        "name": "Victor España",
        "url": "https://cio.umh.es/"
      },
      {
        "name": "Juan Aparicio",
        "url": "https://cio.umh.es/"
      },
      {
        "name": "Xavier Barber",
        "url": "https://cio.umh.es/"
      }
    ],
    "date": "2022-12-20",
    "categories": [],
    "contents": "\n1 Introduction\nEfficiency analysis refers to the discipline of estimating production frontiers while measuring the efficiency of a set of observations, named Decision Making Units (DMUs), which use several inputs to produce several outputs. In the literature of Economics, Engineering and Operations Research, the estimation of production frontiers is a current topic of interest (see, for example, Arnaboldi et al. 2014; Aparicio et al. 2017; O’Donnell et al. 2018). In this line, many models for estimating production frontiers have been developed, resorting to parametric and non-parametric approaches. In the non-parametric approach, a functional form does not need to be specified (e.g. a Cobb-Douglas production function) through the specification of a set of parameters to be estimated, since they are usually data-driven. Additionally, non-parametric models innately cope with multiple-output scenarios. In contrast, the parametric approach aggregates the outputs into a single production index or attempts to model the technology using a dual cost function (Orea and Zof ’io 2019). These are some of the advantages that makes the non-parametric approaches for measuring technical efficiency more appealing than their parametric counterparts.\nContextualizing the non-parametric measurement of efficiency analysis requires outlining the following works. Farrel (1957) was a renowned opponent of estimating efficiency by determining average performance and, indeed, he was the first author in the literature to introduce a method for constructing production frontiers as the maximum producible output from an input bundle. Inspired by Koopmans (1951) and Debreu (1951), Farrell introduced a piece-wise linear upper enveloping surface of the data cloud as the specification of the production frontier, satisfying some microeconomics postulates: free disposability, convexity and minimal extrapolation. A DMU is considered technically inefficient if it is located below the frontier. Furthermore, Farrell’s measure of efficiency, inspired by Shephard (1953), is based on radial movements (equiproportional changes) from technically inefficient observations to their benchmarks located at the estimated production frontier. In the same context as Farrell, Afriat (1972) determined a production frontier under non-decreasing and concavity mathematical assumptions and, at the same time, as close as possible to the sample of observations. Finally, in the same line of research, Charnes et al. (1978) and Banker et al. (1984) proposed Data Envelopment Analysis (DEA), rooted in mathematical programming to provide a relative efficiency assessment of a set of DMUs by the construction of a piece-wise linear frontier. Along with DEA, Free Disposal Hull (FDH) is another of the most recognized non-parametric models for estimating production frontiers. FDH is a deterministic model introduced by Deprins and Simar (1984), which is only based on the free disposability and minimal extrapolation principles, as opposed to DEA, which also assumes convexity. In fact, FDH can be considered the skeleton of DEA, since the convex hull of the former coincides with DEA’s frontier (see Daraio and Simar 2005). In addition, other recent alternative non-parametric techniques for estimating production frontiers are: Banker and Maindiratta (1992) and Banker (1993), who showed that DEA can be interpreted as a Maximum Likelihood estimator; Simar and Wilson (1998, 2000b; 2000a), who introduced how to determine confidence intervals for the efficiency score of each DMU through adapting the bootstrapping methodology by Efron (1979) to the context of FDH and DEA; or Kuosmanen and Johnson (2010; 2017), who have recently shown that DEA may be interpreted as non-parametric least-squares regression, subject to shape constraints on the production frontier and sign constraints on residuals; to name a few.\nHowever, few of the above methodologies are based upon Machine Learning techniques, despite being a rising research field (see, for example, the recent papers by Khezrimotlagh et al. 2019; and Zhu et al. 2019; or the book by Charles et al. 2020). Recently, a bridge has been built between these literatures, Machine Learning and production theory, through a new technique proposed in Esteve et al. (2020), called Efficiency Analysis Trees. This new method shares some similarities with the standard FDH technique. In contrast to FDH, Efficiency Analysis Trees overcomes the well-known problem of overfitting linked to FDH and DEA, by using cross-validation to prune back the deep tree obtained in a first stage. Esteve et al. (2020) also showed that the performance of Efficiency Analysis Trees, checked through Monte Carlo simulations, clearly outperforms the FDH technique with respect to bias and mean squared error.\nMany of the standard models for estimating technical efficiency are nowadays available as R packages such as: Benchmarking (Bogetoft and Otto 2010), for estimating technologies and measuring efficiencies using Data Envelopment Analysis (DEA) and Stochastic Frontier Analysis (SFA); nonparaeff (Oh and Suh 2013), for measuring efficiency and productivity using DEA and its variations; npbr (Daouia et al. 2017), which covers data envelopment techniques based on piece-wise polynomials, splines, local linear fitting, extreme values and kernel smoothing; snfa (McKenzie 2018), which fits both a smooth analogue of DEA and a non-parametric analogue of SFA; or semsfa (Ferrara and Vidoli 2018), which, in a first stage, estimates Stochastic Frontier Models by semiparametric or non-parametric regression techniques to relax parametric restrictions and, in a second stage, applies a technique based on pseudolikelihood or the method of moments for estimating variance parameters. Additionally, there are other packages on efficiency measurement developed for alternative platforms. In MATLAB (The MathWorks Inc. 2021), we can find the Data Envelopment Analysis Toolbox (Álvarez et al. 2020), which implements the main DEA models and solves measures like the directional distance function (with desirable and undesirable outputs), the weighted additive model, and the Malmquist-Luenberger index; or the Total Factor Productivity Toolbox (Balk et al. 2018), which includes functions to calculate the main Total Factor Productivity indices and their decomposition by DEA models. In Stata (StataCorp 2021), it is possible to find a similar package in Ji and Lee (2010).\nIn this paper, we introduce a new package in R, called eat, for fitting regression trees to estimate production frontiers in microeconomics and engineering, by implementing the main features of Efficiency Analysis Trees (Esteve et al. 2020). In particular, eat includes a complete set of baseline functions, covering a wide range of efficiency models fitted by Efficiency Analysis Trees (Esteve et al. 2020) and Random Forest (Esteve et al. 2021), and reporting numerical and graphical results. eat is available as free software, under the GNU General Public License version 3, and can be downloaded from the Comprehensive R Archive Network (CRAN) at https://CRAN.R-project.org/package=eat, including supplementary material as datasets or vignettes to replicate all the results presented in this paper. In addition, eat is hosted on an open source repository on GitHub at https://github.com/MiriamEsteve/EAT. The main objective of this package is the estimation of a production frontier through regression trees satisfying the microeconomic principles of free disposability, convexity and deterministic data. Free disposability states that if a certain input and output bundle is producible, then any input and output bundle that presents a greater value for inputs and a lower value for outputs is also producible. In some sense, it means that doing it worse is always feasible. Convexity means that if two input-output bundles are assumed producible, then any convex combination of them are also feasible. Finally, the deterministic quality means that the observations that belong to the data sample have been observed without noise. In other words, the technology always contains all these observations and, graphically, the production frontier envelops all the data cloud from above.\nThe efficiency measurement field has witnessed the introduction of many different technical efficiency measures throughout the last decades. Regarding the technical efficiency measures implemented in the new eat package, it is worth mentioning that numerical scores and barplots are provided for the output-oriented and input-oriented BCC radial models (Banker et al. 1984), the directional distance function (Chambers et al. 1998), the weighted additive model (Lovell and Pastor 1995; and Cooper et al. 1999) and the output-oriented and input-oriented Russell measures (Färe and Lovell 1978). Additionally, the adaptation of Random Forest (Breiman 2001) for dealing with ensembles of Efficiency Analysis Trees, recently introduced in Esteve et al. (2021) and denoted as RF+EAT, has been also incorporated into the new eat package. The frontier estimator based on Random Forest, which is associated with more robust results, also allows to determine out-of-sample efficiency evaluation for the assessed DMUs. Another remarkable aspect of Efficiency Analysis Trees is the inherited ability to calculate feature importance as performed by other tree-based models of Machine Learning. Specifically, this fact allows the researchers to know which are the most relevant variables for obtaining efficiency and thus getting an explanation of the level of technical efficiency identified for each assessed unit. This ranking of importance variable has been implemented in the eat package. Finally, and from a data visualization point of view, the obtained frontier from Efficiency Analysis Trees can be represented by means of a tree structure, ideal for high-dimensional scenarios where the patterns between the inputs and the efficient levels of outputs are very complex. In addition, FDH and DEA standard models have been included in the new package in order to facilitate comparison with the efficiency scores determined by the Efficiency Analysis Trees technique. Also, the convexification of the estimation of the technology provided by Efficiency Analysis Trees, named Convexified Efficiency Analysis Trees (CEAT) by Aparicio et al. (2021), is implemented in the eat package, with the objective of determining estimations under the axiom of convexity.\nThe functions included in the eat package are summarized in Table 1. This table comprises two columns divided into four subsections for Efficiency Analysis Trees, Random Forest for Efficiency Analysis Trees, Convexified Efficiency Analysis Trees and functions for data simulation. The first column is the name of the main functions and the second one is the description of the functions and the reference of the paper in which we can find the most detailed theoretical explanation of the corresponding function.\nThe paper is organized as follows. The following section summarises the two methodologies implemented in the eat package in R: Efficiency Analysis Trees and Random Forest for Efficiency Analysis Trees. Section 3 describes the data structures that characterize the production possibility sets, the structure of the functions, the results, etc., and briefly explains which data are used to illustrate the package. Section 4 presents the basic methods. The next Section 5 deals with the measurement of economic efficiency of FDH, DEA, Efficiency Analysis Trees, Random Forest for Efficiency Analysis Trees and Convexified Efficiency Analysis Trees models. Advanced options, including displaying and exporting results can be found in Section 6. Section 7 concludes.\n\n\nTable 1: eat package functions\n\n\nFunctions\n\n\nDescription\n\n\nEAT\n\n\nFor EAT. It generates a pruned Efficiency Analysis Trees model and returns an EAT object.\n\n\nbestEAT\n\n\nFor EAT. It computes the root mean squared error (RMSE) for a set of Efficiency Analysis Trees models made up of a set of user-entered hyperparameters. These models are fitted with a training sample and evaluated with a test sample.\n\n\nefficiencyEAT\n\n\nFor EAT. It computes the efficiency scores of a set of DMUs through a Efficiency Analysis Trees model and returns a data.frame. The FDH scores can also be computed. Alternative mathematical programming models for calculating the efficiency scores are: \"BCC.OUT\" (the output-oriented BCC radial model), \"BCC.INP\" (the input-oriented BCC radial model), \"DDF\" (the directional distance function), \"RSL.OUT\" (the output-oriented Russell model), \"RSL.INP\" (the input-oriented Russell model), \"WAM.MIP\" (the weighted additive model with Measure of Inefficiency Proportion) and \"WAM.RAM\" (the weighted additive model with Range Adjusted Measure of Inefficiency).\n\n\nefficiencyJitter\n\n\nFor EAT. It returns a jitter plot (from ggplot2) that represents the dispersion of the efficiency scores of the set of DMUs in the leaf nodes of an Efficiency Analysis Trees model. Mean and standard deviation of scores are shown.\n\n\nefficiencyDensity\n\n\nIt returns a density plot (from ggplot2) to compare the distribution of efficiency scores between two given models (\"EAT\", \"FDH\", \"CEAT\", \"DEA\" and \"RFEAT\" are available).\n\n\nplotEAT\n\n\nFor EAT. It returns a plot of the tree-structure (from ggparty and partykit) of an Efficiency Analysis Trees model.\n\n\nfrontier\n\n\nFor EAT. It returns a plot (from ggplot2) of the estimated production function obtained by an Efficiency Analysis Trees model in a two-dimensional scenario (1 input and 1 output). Optionally, the FDH frontier can be plotted.\n\n\npredict\n\n\nFor EAT. Generic function to predict the expected output by an EAT object. The result is a data.frame with the the predicted values.\n\n\nrankingEAT\n\n\nFor EAT. It returns a data.frame with the scores of variable importance obtained by an Efficiency Analysis Trees model and optionally a barplot representing the variable importance.\n\n\nRFEAT\n\n\nFor RFEAT. It generates a Random Forest for Efficiency Analysis Trees model and returns an RFEAT object.\n\n\nbestRFEAT\n\n\nFor RFEAT. It computes the root mean squared error (RMSE) for a set of Random Forest for Efficiency Analysis Trees models made up of a set of user-entered hyperparameters. These models are fitted with a training sample and evaluated with a test sample.\n\n\nefficiencyRFEAT\n\n\nFor RFEAT. It computes the efficiency scores of a set of DMUs through a Random Forest for Efficiency Analysis Trees model and returns a data.frame. The FDH scores can also be computed. Only the output-oriented BCC radial model is available.\n\n\nplotRFEAT\n\n\nFor RFEAT. It returns a line plot (from ggplot2) with the Out-of-Bag (OOB) error for a random forest consisting of k trees.\n\n\npredict\n\n\nFor RFEAT. Generic function to predict the expected output by an RFEAT object. The result is a data.frame with the predicted values.\n\n\nrankingRFEAT\n\n\nFor RFEAT. It returns a data.frame with the scores of variable importance obtained by a Random Forest for Efficiency Analysis Trees model and optionally a barplot representing the variable importance.\n\n\nefficiencyCEAT\n\n\nFor CEAT. It computes the efficiency scores of a set of DMUs through a Convexified Efficiency Analysis Trees model and returns a data.frame. The DEA scores can also be computed. Alternative mathematical programming models for calculating the efficiency scores are: \"BCC.OUT\" (the output-oriented BCC radial model), \"BCC.INP\" (the input-oriented BCC radial model), \"DDF\" (the directional distance function), \"RSL.OUT\" (the output-oriented Russell model), \"RSL.INP\" (the input-oriented Russell model), \"WAM.MIP\" (the weighted additive model with Measure of Inefficiency Proportion) and \"WAM.RAM\" (the weighted additive model with Range Adjusted Measure of Inefficiency).\n\n\nY1.sim\n\n\nSimulation function. It returns a data.frame with simulated data in a single output scenario (1, 3, 6, 9, 12 and 15 inputs can be generated).\n\n\nX2Y2.sim\n\n\nSimulation function. It returns a data.frame with simulated data in a scenario with 2 inputs and 2 outputs.\n\n\n2 Background\nEfficiency Analysis Trees\nIn this section, we briefly introduce the main fundaments of Efficiency Analysis Trees. Nevertheless, we first need to introduce some notation related to the standard Free Disposal Hull (FDH) and Classification and Regression Trees (CART) techniques.\nWe consider the observation of \\(n\\) Decision Making Units (DMUs), which consumes \\({\\it \\textbf{x}}_{i} =(x_{1i} ,...,x_{mi})\\) \\(\\in R_{+}^{m}\\) quantity of inputs for the production of \\({\\it \\textbf{y}}_{i} =(y_{1i} ,...,y_{si} )\\in R_{+}^{s}\\) quantity of outputs. The dataset is denoted in a compact way as \\(\\aleph =\\left\\{\\left({\\it \\textbf{x},\\textbf{y}}\\right)\\right\\}_{i=1,...,n}^{}\\). The so-called production possibility set or technology, which is the set of technically feasible combinations of \\(({\\it \\textbf{x},\\textbf{y}})\\), is defined, in general terms, as:\n\\[\\begin{equation} \\label{(1)}\n\\psi =\\left\\{({\\it \\textbf{x},\\textbf{y}})\\in R_{+}^{m+s} :{\\it \\textbf{x}\\; }{\\it can\\; produce\\; \\textbf{y}}\\right\\}.\n\\end{equation}\\]\nOn this set, certain assumptions are usually made (see, Färe and Primont 1995), such as: monotonicity (free disposability) of inputs and outputs, which means that if \\(({\\it \\textbf{x},\\textbf{y}})\\in \\psi\\), then \\(({\\it \\textbf{x'},\\textbf{y'}})\\in \\psi\\), as long as \\({\\it \\textbf{x'}\\; }\\ge {\\it \\textbf{x}}\\) and \\({\\it \\textbf{y'}\\; }\\le {\\it \\textbf{y}}\\); and convexity, i.e., if \\(\\left({\\it \\textbf{x}},{\\it \\textbf{y}}\\right)\\in \\psi\\) and \\(\\left({\\it \\textbf{x'}},{\\it \\textbf{y'}}\\right)\\in \\psi\\), then \\(\\lambda \\left({\\it \\textbf{x}},{\\it \\textbf{y}}\\right)+\\left(1-\\lambda \\right)\\left({\\it \\textbf{x'}},{\\it \\textbf{y'}}\\right)\\in \\psi\\), \\(\\forall \\lambda \\in \\left[0,1\\right]\\). In the case of the FDH estimator, only free disposability is assumed. Additionally, FDH is assumed to be deterministic. In other words, the production possibility set determined by FDH always contains all the observations that belong to the data sample and, graphically, the production frontier envelops the data cloud from above. Also, FDH satisfies the minimal extrapolation postulate, which is associated with the typical problem-solving principle of Occam’s razor. That is, additional requirements are needed to select the right estimator because there are a lot of possible estimators that can meet free disposability and the deterministic quality. In this sense, according to Occam’s razor, the most conservative estimate of the production frontier would be that related to a surface that would envelop the data from above, satisfy free disposability and, at the same time, be as close as possible to the data cloud. In contrast, the DEA estimator requires stronger assumptions, such as convexity of the set \\(\\psi\\).\nWith regard to the measurement of technical efficiency, a certain part of the set \\(\\psi\\) is actually of interest. It is the efficient frontier or production frontier of \\(\\psi\\), which is defined as \\(\\partial ({\\it \\boldsymbol{\\psi}}):=\\left\\{({\\it \\textbf{x}},{\\it \\textbf{y}})\\in {\\it \\boldsymbol{\\psi}}:\\hat{{\\it \\textbf{x}}}<{\\it \\textbf{x}},\\hat{{\\it \\textbf{y}}}>{\\it \\textbf{y}}\\Rightarrow (\\hat{{\\it \\textbf{x}}},\\hat{{\\it \\textbf{y}}})\\notin {\\it \\boldsymbol{\\psi}}\\right\\}\\). Technical efficiency is understood as the distance from a point belonging to \\(\\psi\\) to the production frontier \\(\\partial ({\\it \\boldsymbol{\\psi}})\\). In particular, Deprins and Simar (1984) proposed the FDH estimator of the set \\(\\psi\\) from the dataset \\(\\aleph\\) as:\n\\[\\begin{equation} \\label{(2)}\n\\hat{{\\it \\boldsymbol{\\psi}}}_{FDH} =\\left\\{\\left({\\it \\textbf{x}},{\\it \\textbf{y}}\\right)\\in R_{+}^{m+s} :\\exists i=1,...,n\\, \\, {\\rm such\\; that}\\, {\\it \\textbf{y}}\\le {\\it \\textbf{y}}_{i} ,{\\it \\textbf{x}}\\ge {\\it \\textbf{x}}_{i} \\right\\}.\n\\end{equation}\\]\nThe FDH technique is very attractive because it is based on very few suppositions, but it suffers from overfitting due to its construction. This problem is shared by other well-known data-based approaches. For example, Classification and Regression Trees (CART), a technique that belongs to the field of machine learning, suffer problems of overfitting when a deep tree is developed. However, this problem can be fixed using a cross-validation process to prune the deep tree. The principle behind CART is relatively simple: a certain criterion is chosen to recursively generate binary partitions of the data until a meaningful division is no longer possible or a stopping rule is maintained. The graphic result of this approach is a tree that starts at the root node, develops through the intermediate nodes and ends at the terminal nodes, also known as leaves. The binary nature of CART is represented by each parent node, except for the leaves, giving rise to two child nodes.\nNext, we briefly introduce the recent technique named Efficiency Analysis Trees by Esteve et al. (2020). This technique allows the estimation of production frontiers, fulfilling the common axioms of microeconomics, through a data-based approach that is not founded on any particular distribution on the data noise and, in addition, creates a step function as a estimator. It shares these characteristics with the FDH technique, but the overfitting problem related to FDH can be solved through cross-validation based on pruning.\nWe now introduce the main steps of the algorithm linked to the Efficiency Analysis Trees technique. Let us assume that we have a node \\(t\\) in the tree structure to be split. This node contents a subset of the original sample \\(\\aleph\\). The algorithm has to select an input variable \\(j\\), \\(j=1,...,m\\), and a threshold \\(s_{j} \\in S_{j}\\), where \\(S_{j}\\) is the set of possible thresholds for variable \\(j\\), such that the sum of the mean squared error (MSE) calculated for the data that belong to the left child node \\(t_{L}\\) and the MSE corresponding to the data belonging to the right child node \\(t_{R}\\) is minimized. The data of the left child node \\(t_{L}\\) satisfies the condition \\(x_{j} <s_{j}\\), while the data of the right child node \\(t_{R}\\) satisfies the condition \\(x_{j} \\ge s_{j}\\). Additionally, in the algorithm, the set \\(S_{j}\\) is defined from the observed values of the input \\(j\\) in the data sample \\(\\aleph\\). Formally, the split consists in selecting the combination \\(\\left(x_{j} ,s_{j} \\right)\\) which minimizes \\(R\\left(t_{L} \\right)+R\\left(t_{R} \\right)=\\frac{1}{n} \\sum _{\\left({\\it x}_{i} ,{\\it y}_{i} \\right)\\in t_{L} }\\sum _{r=1}^{s}\\left(y_{ri} -y_{r} \\left(t_{L} \\right)\\right)^{2} +\\frac{1}{n} \\sum _{\\left({\\it x}_{i} ,{\\it y}_{i} \\right)\\in t_{R} }\\sum _{r=1}^{s}\\left(y_{ri} -y_{r} \\left(t_{R} \\right)\\right)^{2}\\), where \\(y_{r} \\left(t\\right)\\) denotes the estimation of the \\(r\\)-th output of the node \\(t\\). One of the most important aspects in the production context is how to define \\(y_{r} \\left(t\\right)\\) in each node for fulfilling the free disposability property during the growing process of the tree. In this sense, the notion of Pareto-dominance between nodes introduced in Esteve et al. (2020) is really relevant.\nAs described above, each node \\(t\\) is defined by a series of conditions in the input space as \\(\\left\\{x_{j} <s_{j} \\right\\}\\) or \\(\\left\\{x_{j} \\ge s_{j} \\right\\}\\). In this sense, after executing the split, a region in the input space is created. This region in the input space is called the “support” of node \\(t\\) and is defined as \\({\\rm supp}\\left(t\\right)=\\left\\{{\\it \\textbf{x}}\\in R_{+}^{m} :a_{j}^{t} \\le x_{j} <b_{j}^{t} ,j=1,...,m\\right\\}\\). The parameters \\(a_{j}^{t}\\) and \\(b_{j}^{t}\\) are originated from the several thresholds selected during the splitting process. Giving the notion of support of a node, it is possible to establish the concept of Pareto-dominance. Let \\(k=1,...,K\\) be the total number of splits executed. Let \\(T_{k} \\left(\\aleph \\right)\\) be the tree built after the th split. Let \\(\\tilde{T}_{k} \\left(\\aleph \\right)\\) be the set of leaves in the tree \\(T_{k} \\left(\\aleph \\right)\\). More notation: let \\(t^{*} \\in \\tilde{T}_{k} \\left(\\aleph \\right)\\) be the node to be split in a certain step of the algorithm, then \\(T\\left(k|t^{*} \\to t_{L} ,t_{R} \\right)\\) denotes the tree associated with this specific split. Let \\(k=1,...,K\\) and \\(t\\in \\tilde{T}_{k} \\left(\\aleph \\right)\\), then the set of Pareto-dominant nodes of node \\(t\\) is defined as \\(P_{T_{k} \\left(\\aleph \\right)} \\left(t\\right)=\\{t'\\in \\tilde{T}_{k} \\left(\\aleph \\right)-t:\\exists {\\it \\textbf{x}}\\in {\\rm supp}\\left(t\\right),\\exists {\\it \\textbf{x'}}\\in {\\rm supp}\\left(t'\\right)\\, \\, {\\rm such\\; that}\\,\\) \\({\\it \\textbf{x'}} \\le {\\it \\textbf{x}}\\}\\). \\(P_{T_{k} \\left(\\aleph \\right)} \\left(t\\right)\\) contains all the nodes such that at least one input vector in its corresponding support, non-necessarily observed, dominates at least one input vector belonging to the support of node \\(t\\) (in the Pareto sense). To do so, in practice, it is only necessary to compare the components of \\({\\it \\textbf{a}}^{t'}\\) and \\({\\it \\textbf{b}}^{t}\\). Specifically, \\({\\it \\textbf{a}}^{t'} <{\\it \\textbf{b}}^{t}\\) if and only if \\(t'\\in P_{T_{k} \\left(\\aleph \\right)} \\left(t\\right)\\).\nNow, we return to how to estimate the outputs in each child node with the aim of guaranteeing the satisfaction of free disposability. For any node \\(t^{*} \\in \\tilde{T}_{k} \\left(\\aleph \\right)\\), the way to estimate the value of the outputs for the right child node is through the estimation of the outputs of its parent node, i.e., \\(y_{r} \\left(t_{R} \\right)=y_{r} (t^{*} ),\\, \\, r=1,...,s,\\) while the estimation of outputs for the left child node is:\n\\[\\begin{equation} \\label{(3)}\ny_{r} \\left(t_{L} \\right)=\\max \\left\\{\\max \\left\\{y_{ri} :\\left({\\it \\textbf{x}}_{i} ,{\\it \\textbf{y}}_{i} \\right)\\in t_{L} \\right\\},y_{r} \\left(I_{T\\left(k|t^{*} \\to t_{L} ,t_{R} \\right)} \\left(t_{L} \\right)\\right)\\right\\},\\, \\, r=1,...,s,\n\\end{equation}\\]\nwhere \\(y_{r} \\left(I_{T\\left(k|t^{*} \\to t_{L} ,t_{R} \\right)} \\left(t_{L} \\right)\\right)= \\max \\left\\{y_{r} \\left(t'\\right):t'\\in I_{T\\left(k|t^{*} \\to t_{L} ,t_{R} \\right)} \\left(t_{L} \\right)\\right\\}\\) and \\(y_{r} \\left(t'\\right)\\) is the estimation of the output \\(y_{r}\\) at node \\(t'\\in \\tilde{T}\\left(k|t^{*} \\to t_{L} ,t_{R} \\right)\\), \\(r=1,...,s\\). This way of estimating the output values guarantees free disposability.\nAccordingly, the algorithm selects the best pair \\(\\left(x_{j^{*} } ,s_{j^{*} } \\right)\\) such that the sum of the MSE of the left and right child nodes is minimized. Once the split of node \\(t^{*}\\) is executed, the tree \\(T\\left(k|t^{*} \\to t_{L}^{*} ,t_{R}^{*} \\right)\\) is obtained. This process continues until bipartition is not possible because all the data in a node have the same input values or a certain stopping rule is satisfied. The usual stopping rule is \\(n\\left(t\\right)\\le n_{\\min } = 5\\), where \\(n(t)\\) is the sample size of node \\(t\\). The final tree built is denoted as \\(T_{\\max } \\left(\\aleph \\right)\\), which usually is a deep tree.\n\\(T_{\\max} \\left(\\aleph \\right)\\) suffers from the same problem as FDH, i.e., overfitting. Esteve et al. (2020) proposed to prune the tree exploiting the same technique as Breiman et al. (1984). This pruning process resorts to the notion of the error-complexity measure \\(R_{\\alpha} \\left(T\\left(\\aleph \\right)\\right)\\), which is a combination between a measure of the accuracy of the tree, defined as the sum of the MSE determined at each leaf node, and a measure of the number of leaf nodes. Also, \\(R_{\\alpha } \\left(T\\left(\\aleph \\right)\\right)\\) depends on a parameter \\(\\alpha\\), which compensates the values of the two components of the error: \\(R_{\\alpha } \\left(T\\left(\\aleph \\right)\\right)=R\\left(T\\left(\\aleph \\right)\\right)+\\alpha \\left|\\tilde{T}\\left(\\aleph \\right)\\right|\\). The idea behind the pruning of \\(T_{\\max } \\left(\\aleph \\right)\\) is to minimize \\(R_{\\alpha } \\left(T\\left(\\aleph \\right)\\right)\\). The pruning process is also based on cross-validation (see Breiman et al. (1984) for more details). The tree resulting from the pruned process is \\(T^{*} \\left(\\aleph \\right)\\). This tree doesn’t suffer from the overfitting problem. For this reason, the use of \\(T^{*} \\left(\\aleph \\right)\\) is recommended rather than \\(T_{\\max } \\left(\\aleph \\right)\\), unless a descriptive analysis of the sample is required.\nFinally, \\({\\it \\textbf{d}}_{T^{*} \\left(\\aleph \\right)} \\left({\\it \\textbf{x}}\\right)\\) will denote hereinafter the multi-dimensional estimator defined from \\(T^{*} \\left(\\aleph \\right)\\) and the sample \\(\\aleph\\), i.e., \\(d_{rT^{*} \\left(\\aleph \\right)} \\left({\\it \\textbf{x}}\\right)=\\sum _{t\\in T^{*} \\left(\\aleph \\right)}y_{r} \\left(t\\right)I\\left({\\it \\textbf{x}}\\in t\\right)\\), for all \\(r=1,...,s\\), with \\(I\\left(\\cdot \\right)\\) being the indication function. From this estimator, it is possible to define a production possibility set or technology estimated from the Efficiency Analysis Trees technique as:\n\\[\\begin{equation} \\label{(4)}\n\\hat{\\Psi}_{T^{*} (\\aleph)} =\\left\\{\\left({\\it \\textbf{x}},{\\it \\textbf{y}}\\right)\\in R_{+}^{m+s} :{\\it \\textbf{y}}\\le {\\it \\textbf{d}}_{T^{*} \\left(\\aleph \\right)} \\left({\\it \\textbf{x}}\\right)\\right\\}.\n\\end{equation}\\]\n\\(\\hat{\\Psi }_{T^{*} \\left(\\aleph \\right)}\\) satisfies free disposability and the deterministic quality.\nBy analogy with the existing relationship between FDH and DEA, it is possible to derive an estimation of \\(\\Psi\\) by the convexification of the set \\(\\hat{\\Psi }_{T^{*}}\\). In this sense, the convexification of the production possibility set derived from EAT would be as follows:\n\\[\\begin{equation} \\label{(5)}\nconv\\left(\\hat{\\Psi}_{T^{*} } \\right)=\\left\\{\\left({\\it \\textbf{x}},{\\it \\textbf{y}}\\right)\\in R_{+}^{m+s} :{\\it \\textbf{x}}\\ge \\sum _{t\\in \\tilde{T}^{*} }^{}\\lambda _{t} {\\it \\textbf{a}}^{t}  ,{\\it \\textbf{y}}\\le \\sum _{t\\in \\tilde{T}^{*} }^{}\\lambda _{t} {\\it \\textbf{d}}_{T^{*} } \\left({\\it \\textbf{a}}^{t} \\right) ,\\sum _{t\\in \\tilde{T}^{*} }^{}\\lambda_{t}  {\\rm =1}{\\it ,\\boldsymbol{\\lambda}}\\ge 0_{\\left|\\tilde{T}^{*} \\right|} \\right\\}.\n\\end{equation}\\]\nUnder the convexity assumption, the EAT methodology is known as the Convexified Efficiency Analysis Trees technique (hereinafter referred to as CEAT) (see Aparicio et al. 2021).\nRandom Forest for Efficiency Analysis Trees\nIn this section, we briefly describe the extension of the approach by Esteve et al. (2020) to the context of using ensembles of trees to provide estimates of production frontiers (see Esteve et al. 2021). Specifically, we briefly revise the way to adapt the standard Random Forest (Breiman 2001) for estimating production frontiers satisfying fundamental postulates of microeconomics, such as free disposability. The adaptation of Random Forest to the estimation of production frontiers by Esteve et al. (2021) is the first one that focuses on the introduction of a methodology for measuring technical efficiency that is robust to the resampling of data and, at the same time, to the specification of input variables.\nData robustness and resampling methods for input modeling are both topics of interest in the literature on technical efficiency measurement. Regarding robustness to data, Simar and Wilson (1998, 2000b; 2000a) were the first ones to adapt the bootstrapping methodology (Efron 1979) to the context of DEA and FDH. As regards the importance of the robustness of input and output variables in non-parametric efficiency analysis, since the beginning of DEA and FDH, researchers have always been aware that the selection of input and output variables to be considered in efficiency analysis is one of the crucial issues in the specification of the model. In practice, the researchers’ previous experience may lead to the selection of some inputs and outputs considered essential to represent the underlying technology. However, there may be other variables whose inclusion in the model the analyst is not always sure of (Pastor et al. 2002). Some approaches focus on balancing the experience of researchers with the information provided by observations (see, for example, Banker 1993, 1996; Pastor et al. 2002). Another recent approach is based, in contrast, on determining efficiency scores that are robust to variable selection by considering all the possible combinations of inputs and outputs and their aggregation (Landete et al. 2017).\nOn the whole, Random Forest (Breiman 2001) is an ensemble learning method that works by constructing a multitude of decision trees by CART (Breiman et al. 1984) at training time and aggregating the information of the individual trees in a final prediction value. In particular, when Random Forest is applied to regression problems, the final estimator corresponds to the mean of each individual prediction (Breiman 2001). Random Forest modifies the growing process of an individual tree as follows, by: (i) applying bootstrapping on the data training for each individual tree and (ii) selecting a random subset of the predictors in each iteration. In this way, given a learning sample \\(\\aleph\\) of size \\(n\\), Random Forest repeatedly selects random samples of size \\(n\\) with replacement of the set \\(\\aleph\\). Then, the method fits the trees to these samples but, to do this, it uses a modified tree learning algorithm that chooses, in each candidate division of the learning process, a random subset of predictors. The reason for doing this is due to the instability of the model. It is known that individual decision trees, such as CART, are very unstable (Berk 2016). This means that completely different tree structures are given when the training data is modified slightly. In this way, the result of applying Random Forest is an estimator that overcomes overfitting and instability problems in general, resulting in a substantial reduction in variance.\nThe algorithm associated with the adaptation of the Random Forest technique to the world of technical efficiency assessment, called RF+EAT, is introduced in Esteve et al. (2021). The steps that must be carried out in Random Forest for Efficiency Analysis Trees are shown in Algorithm Algorithm 1. This algorithm is based on the typical algorithm of Random Forest that can be found in Kuhn et al. (2013). In Algorithm Algorithm 1, the first step consists of selecting the number of trees that will make up the forest, that is, the hyperparameter \\(p\\). Then, \\(p\\) (bootstrap) random samples from the original data sample with replacement are generated. Next, the Efficiency Analysis Trees algorithm by Esteve et al. (2020) is applied to each subsample applying the stopping rule \\(n\\left(t\\right)\\le n_{\\min}\\), but without pruning. Also, in this algorithm, \\(n_{\\min}\\) is treated as an additional hyperparameter that could be tuned. During the execution of the Efficiency Analysis Trees algorithm, a subset of input variables (\\(mtry\\)) from the original set is randomly selected each time the splitting subroutine is applied. To do that, one of the following five thumb rules is used following the literature:\nBreiman’s Rule: \\(mtry=\\frac{m}{3}\\),\nRule DEA1: \\(mtry=\\frac{n\\left(t\\right)}{2} -s\\) (Golany and Roll 1989; Homburg 2001),\nRule DEA2: \\(mtry=\\frac{n\\left(t\\right)}{3} -s\\) (Nunamaker 1985; Banker et al. 1989; Friedman and Sinuany Stern 1998; Raab and Lichty 2002),\nRule DEA3: \\(mtry=\\frac{n\\left(t\\right)}{2s}\\) (Dyson et al. 2001),\nRule DEA4: \\(mtry=\\min \\left\\{\\frac{n\\left(t\\right)}{s}, \\frac{n\\left(t\\right)}{3} -s\\right\\}\\) (Cooper et al. 2007).\n\n\n\nOnce Algorithm Algorithm 1 has been applied, \\(p\\) fitted trees are determined with the aim of obtaining an output estimation giving an input vector \\({\\it \\textbf{x}}\\in R_{+}^{m}\\). In this regard, we have \\(T\\left(\\aleph _{1} \\right),...,T\\left(\\aleph _{p} \\right)\\) tree structures derived from the application of the Efficiency Analysis Trees algorithm on the \\(p\\) bootstrap subsamples \\(\\aleph _{1} ,...,\\aleph _{p}\\). Given an input vector \\({\\it \\textbf{x}}\\in R_{+}^{m}\\), an output estimator is determined by averaging the individual estimator corresponding to each tree:\n\\[\\begin{equation} \\label{(6)}\n{\\it \\textbf{y}}^{RF+EAT\\left(\\aleph \\right)} \\left({\\it \\textbf{x}}\\right):=\\frac{1}{p} \\sum_{q=1}^{p}{\\it \\textbf{d}}_{T\\left(\\aleph _{q} \\right)} \\left({\\it \\textbf{x}}\\right).  \n\\end{equation}\\]\nwhere \\({\\it \\textbf{d}}_{T\\left(\\aleph _{q} \\right)} \\left({\\it \\textbf{x}}\\right)\\) denotes the output estimator associated with each tree structure \\(T\\left(\\aleph _{q} \\right)\\), given an input vector \\({\\it \\textbf{x}}\\in R_{+}^{m}\\).\nIn addition, this estimator allows the technology or production possibility set to be defined as:\n\\[\\begin{equation} \\label{(7)}\n\\hat{\\Psi }_{RF+EAT} =\\left\\{\\left({\\it \\textbf{x}},{\\it \\textbf{y}}\\right)\\in R_{+}^{m+s} :{\\it \\textbf{y}}\\le {\\it \\textbf{y}}^{RF+EAT\\left(\\aleph \\right)} \\left({\\it \\textbf{x}}\\right)\\right\\}.\n\\end{equation}\\]\nAs happens with the standard Random Forest, Random Forest for Efficiency Analysis Trees also exploits the Out-Of-Bag (OOB) concept. The OOB estimate at observation \\(\\left({\\it \\textbf{x}}_{i} ,{\\it \\textbf{y}}_{i} \\right)\\) consists in evaluating the prediction of the ensemble just using the individual models \\(T\\left(\\aleph _{q} \\right)\\) whose corresponding bootstrap samples \\(\\aleph _{q}\\) are such that \\(\\left({\\it \\textbf{x}}_{i},{\\it \\textbf{y}}_{i} \\right)\\notin \\aleph _{q}\\). From this definition, the generalization error is defined as the average of the OOB estimates calculated over all the observations in the learning sample \\(\\aleph\\):\n\\[\\begin{equation} \\label{(8)}\nerr^{RF+EAT\\left(\\aleph \\right)} =\\frac{1}{n} \\sum_{\\left(x_{i} ,y_{i} \\right)\\in \\aleph }\\sum_{r=1}^{s}\\left(y_{ri} -y_{r}^{RF+EAT\\left(\\aleph \\right)} \\left({\\it \\textbf{x}}_{i} \\right)\\right)^{2}.    \n\\end{equation}\\]\nThe generalization error is useful for determining a measure of variable importance, which can be used for creating a sorted list of inputs \\(x_{1} ,...,x_{m}\\). The way to calculate the input importance of variable \\(x_{j}\\) is: firstly, generate a new database, \\(\\aleph ^{j}\\), identical to the original one \\(\\aleph\\), where specifically the values of variable \\(x_{j}\\) were randomly permuted; secondly, apply Algorithm Algorithm 1 on the new `virtual’ learning sample \\(\\aleph ^{j}\\); thirdly, determine the value of the generalization error, i.e., \\(err^{RF+EAT\\left(\\aleph ^{j} \\right)}\\); and, finally, calculate the percentage increase of the generalization error when variable \\(x_{j}\\) is shuffled as:\n\\[\\begin{equation} \\label{(9)}\n\\% Inc^{RF+EAT} \\left(x_{j} \\right)=100\\cdot \\left(\\frac{err^{RF+EAT\\left(\\aleph ^{j} \\right)} -err^{RF+EAT\\left(\\aleph \\right)} }{err^{RF+EAT\\left(\\aleph \\right)}} \\right).\n\\end{equation}\\]\n3 Data structure\nData are managed as a regular R data.frame in the eat functions (matrix is often accepted but will be converted to a data.frame in the functions pre-processing). The main functions of the eat package are EAT() and RFEAT(), which return structured objects named EAT and RFEAT, respectively. These objects contain fields with relevant information such as the estimation results or the arguments introduced by the user in the function call.\nThe fields of the EAT object are the following:\ndata: Contains the input and output variables.\ndf: Data introduced by the user in a data.frame structure after being preprocessed.\nx: Input indexes in df.\ny: Output indexes in df.\ninput_names: Name of the input variables in df.\noutput_names: Name of the output variables in df.\nrow_names: Name of the observations in df.\n\ncontrol: Contains the hyperparameters selected by the user.\nfold: Number of folds in which is divided df to apply cross-validation.\nnumStop: Minimum number of observations in a node.\nmax.leaves: Maximum number of leaf nodes.\nmax.depth: Maximum number of nodes between the root node (not included) and the furthest leaf node.\nna.rm: A logical variable that indicates if NA rows should be ignored.\n\ntree: list containing the nodes of the fitted Efficiency Analysis Trees model. Each node is made up of the following elements:\nid: Node index\nF: Father node index.\nSL: Left child node index.\nSR: Right child node index.\nindex: Set of indexes corresponding to the observations in a node.\nR: Error at the node.\nxi: Index of the variable that produces the split in a node.\ns: Threshold of the variable xi.\na: The components of the vector \\({\\it \\textbf{a}}^{t}\\).\nb: The components of the vector \\({\\it \\textbf{b}}^{t}\\).\n\nnodes_df: Contains the following information related to the nodes of the fitted Efficiency Analysis Trees model in a data.frame structure:\nid: Node index\nN: Number of observations in a node.\nProportion: Proportion of observations in a node.\ny: Fitted values.\nR: Error at the node.\nindex: Indexes of the observations in a node.\n\nmodel: Contains the following information related to the fitted Efficiency Analysis Trees model:\nnodes: Number of nodes in the tree.\nleaf_nodes: Number of leaf nodes in the tree.\na: The components of the vector \\({\\it \\textbf{a}}^{t}\\).\ny: Output estimation for each leaf node.\n\nRegarding the RFEAT object, it contains the following fields:\ndata: same fields as the EAT object.\ncontrol: Contains the hyperparameters selected by the user.\nnumStop: Minimum number of observations in a node.\nm: Number of trees that make up the random forest.\ns_mtry: Number of inputs that can be randomly selected in each split.\nna.rm: A logical variable that indicates if NA rows should be ignored.\n\nforest: A list containing the individual Efficiency Analysis Trees that make up the random forest.\nError: The Out-of-Bag error at the random forest.\nOOB: A list containing the observations used for training each Efficiency Analysis Tree that makes up the random forest.\nDataset and statistical sources\n\n\n# We load the library\nlibrary(\"eat\")\n\n# We load the data\ndata(\"PISAindex\")\n\n\nWe illustrate all the models presented in this paper resorting to a single dataset (PISAindex) available in the eat package. Our dataset consists of 72 countries with 3 outputs and 13 inputs. The output data have been collected by the PISA (Programme for International Student Assessment) 2018 survey (OECD 2018) and refers to the average score in mathematics, reading and science domains for schools in these countries. Regarding the input data, the variables have been collected from the Social Progress Index (2018) and are related to the socioeconomic environment of these countries. These inputs can be classified into four blocks as follows:\nBasic Human Needs:\nNutrition and Basic Medical Care (NBMC)\nWater and Sanitation (WS)\nShelter (S)\nPersonal Safety (PS).\n\nFoundations of Wellbeing:\nAccess to Basic Knowledge (ABK)\nAccess to Information and Communications (AIC)\nHealth and Wellness (HW)\nEnvironmental Quality (EQ).\n\nOpportunity:\nPersonal Rights (PR)\nPersonal Freedom and Choice (PFC)\nInclusiveness (I)\nAccess to Advanced Education (AAE).\n\nEconomy:\nGross Domestic Product based on Purchasing Power Parity (GDP_PPP).\n\nFinally, in order to simplify the examples and reduce computation time, a subset of variables is selected as follows:\n\n\n# Inputs (5): PR, PFC, I, AAE, GDP_PPP\n# Outputs (3): S_PISA, R_PISA, M_PISA\nPISAindex <- PISAindex[, c(3, 4, 5, 14, 15, 16, 17, 18)]\n\nhead(PISAindex)\n\n    S_PISA R_PISA M_PISA    PR   PFC     I   AAE GDP_PPP\nSGP    551    549    569 71.70 87.90 48.26 74.31  97.745\nJPN    529    504    527 94.07 82.40 62.32 81.29  41.074\nKOR    519    514    526 92.71 79.06 63.54 86.32  41.894\nEST    530    523    523 95.67 84.10 55.58 73.16  35.308\nNLD    503    485    519 96.34 89.04 75.82 82.99  56.455\nPOL    511    512    516 86.41 78.25 57.58 76.21  31.766\n\nTable 2 reports the descriptive statistics for these variables (outputs and inputs).\n\n\n\n\nTable 2: Descriptive statistics (averages, standard deviations, minimum, median and maximum) of input–output.\nvariable\ntype\nmean\nsd\nmin\nmedian\nmax\nS_PISA\noutput\n455.06\n48.32\n336.00\n466.00\n551.00\nR_PISA\noutput\n450.89\n50.52\n340.00\n466.00\n549.00\nM_PISA\noutput\n454.81\n52.17\n325.00\n463.50\n569.00\nPR\ninput\n81.62\n17.98\n21.14\n88.40\n98.07\nPFC\ninput\n75.42\n11.03\n47.25\n78.19\n91.65\nI\ninput\n54.17\n17.07\n12.37\n55.51\n81.91\nAAE\ninput\n69.87\n10.75\n48.37\n71.65\n90.43\nGDP_PPP\ninput\n36.04\n21.91\n7.44\n31.42\n114.11\n\n\n\n\n4 Basic functions of the library\nIn this section, we introduce the main functions of the library related to Efficiency Analysis Trees and Random Forest for Efficiency Analysis Trees. To execute the following examples, the package eat must be loaded and the seed 100 must be set for reproducibility.\n\n\n# We set the seed\nset.seed(100)\n\n\nThe EAT basic model\nThe basic model of Efficiency Analysis Trees that we explained in subsection Efficiency Analysis Trees can be implemented in R using the function EAT():\n\n\nEAT(\n  data, x, y,\n  numStop = 5,\n  fold = 5,\n  max.depth = NULL,\n  max.leaves = NULL,\n  na.rm = TRUE\n)\n\n\nThe EAT() function is the cornerstone of the eat library. The minimum arguments of this function are the data (data) containing the study variables, the indexes of the predictor variables or inputs (x) and the indexes of the predicted variables or outputs (y). Additionally, the numStop, fold, max.depth and max.leaves arguments are included for more experienced users in the fields of machine learning and tree-based models. Modifying these four hyperparameters allows obtaining different frontier estimates and therefore selecting the one that best suits the needs of the analysis. The description of these parameters is as follows:\nnumStop refers to the minimum number of observations in a node to be split and is directly related to the size of the tree. The higher the value of numStop, the smaller the size of the tree.\nfold refers to the number of parts in which the data is divided to apply the cross-validation technique. Variations in the fold argument are not directly related to the size of the tree.\nmax.depth limits the number of nodes between the root node (not included) and the furthest leaf node. When this argument is introduced, the typical process of growth-pruning is not carried out. In this case, the tree is allowed to grow to the required depth.\nmax.leaves determines the maximum number of leaf nodes. As in max.depth, the process of growth-pruning is not performed. In this respect, the tree grows until the required number of leaf nodes is reached, and then, the tree is returned.\nNotice that including the arguments max.depth or max.leaves reduces the computation time by eliminating the pruning procedure. However, the pruning process is preferred if the objective of the study is inferential instead of simply descriptive. If both are included at the same time, a warning message is displayed and only max.depth is used.\nAs an example, using data from subsection Dataset and statistical sources, we next create a multi response tree using the suitable code as follows. Results are returned as an EAT object, as explained in Section Data structure.\n\n\nmodelEAT <- EAT(data = PISAindex, x = 4:8, y = 1:3)\n\n\nThe RFEAT basic model\nThe basic model of Random Forest for Efficiency Analysis Trees that we explained in subsection Random Forest for Efficiency Analysis Trees can be implemented in R using the function RFEAT():\n\n\nRFEAT(\n  data, x, y,\n  numStop = 5,\n  m = 50,\n  s_mtry = \"BRM\",\n  na.rm = TRUE\n)\n\n\nThe RFEAT() function has also been developed with the aim of providing greater statistical robustness to the results obtained by the EAT() function. The RFEAT() function requires the data containing the variables for the analysis, x and y corresponding to the inputs and outputs indexes respectively, the minimum number of observations in a node for a split to be attempted (numStop) and na.rm to ignore observations with NA cells. All these arguments are used for the construction of the \\(p\\) (this is denoted with m in the RFEAT() function) individual Efficiency Analysis Trees that make up the random forest. Finally, the argument s_mtry indicates the number of inputs that can be randomly selected in each split. It can be set as any integer although there are also certain predefined values. Let \\(m\\) be the number of inputs, let \\(s\\) be the number of outputs and let \\(n(t)\\) be the number of observations in a node. Then, the predefined values for s_mtry are:\nBRM = \\(\\frac{m}{3}\\),\nDEA1 = \\(\\frac{n\\left(t\\right)}{2}-s\\),\nDEA2 = \\(\\frac{n\\left(t\\right)}{3}-s\\),\nDEA3 = \\(\\frac{n\\left(t\\right)}{2s}\\),\nDEA4 = \\(\\min \\left\\{\\frac{n\\left(t\\right)}{s}, \\frac{n\\left(t\\right)}{3}-s \\right\\}\\).\nAs an example, using data from subsection Dataset and statistical sources, we next create a forest with 30 trees. Results are returned as an RFEAT object, as explained in Section Data structure.\n\n\nmodelRFEAT <- RFEAT(data = PISAindex, x = 4:8, y = 1:3, m = 30)\n\n\nPredictions\nThe estimators of the Efficiency Analysis Trees and Random Forest for Efficiency Analysis Trees can be computed in R using the function predict():\n\n\npredict(\n  object,\n  newdata,\n  x, ...\n)\n\n\nRegarding the arguments of predict(), object can be an EAT or an RFEAT object, newdata refers to a data.frame and x to the set of inputs to be used. This function returns a data.frame with the expected output for a set of observations. For predictions using an EAT object, only one tree is used. However, for the RFEAT model, the output is predicted by each of the \\(p\\) (m in the RFEAT() function) individual trees trained and subsequently the mean value of all predictions is obtained.\nAs an example, we next evaluate the last 3 DMUs from the data of subsection Dataset and statistical sources and the corresponding EAT and RFEAT models. Results are returned in a data.frame structure with the output predictions:\n\n\npredict(object = modelEAT, newdata = tail(PISAindex, 3), x = 4:8)\n\n  S_PISA_pred R_PISA_pred M_PISA_pred\n1         428         424         437\n2         377         359         368\n3         377         359         368\n\n\n\npredict(object = modelRFEAT, newdata = tail(PISAindex, 3), x = 4:8)\n\n  S_PISA_pred R_PISA_pred M_PISA_pred\n1    439.9667    435.1333    441.2000\n2    402.0667    389.0667    403.9000\n3    399.0333    389.3333    399.6333\n\nIn the same way, the user can also create a new data.frame and calculate predictions for it as follows:\n\n\nnew <- data.frame(AAE = c(61, 72), PR = c(76, 81), I = c(41, 55), GDP_PPP = c(19, 31),\n                  PFC = c(67, 78))\n                  \npredict(object = modelEAT, newdata = new, x = 1:5)\n\n  S_PISA_pred R_PISA_pred M_PISA_pred\n1         428         424         421\n2         481         479         488\n\nImportance of predictor variables\nThe way to compute in R the predictor variables importance in the Efficiency Analysis Trees methodology is using the functions rankingEAT() or rankingRFEAT():\n\n\n# Through Efficiency Analysis Trees\nrankingEAT(\n  object,\n  barplot = TRUE,\n  threshold = 70,\n  digits = 2\n)\n\n# Through Random Forest for Efficiency Analysis Trees\nrankingRFEAT(\n  object,\n  barplot = TRUE,\n  digits = 2\n)\n\n\nThese functions allow a selection of variables by calculating a score of importance through Efficiency Analysis Trees or Random Forest for Efficiency Analysis Trees, respectively. These importance scores represent how influential each variable is in the model. Regarding the Efficiency Analysis Trees [RankingEAT()], the notion of surrogate splits by Breiman et al. (1984) was implemented. In this regard, the measure of importance of a variable \\(x_j\\) is defined as the sum over all nodes of the decrease in mean squared error produced by the best surrogate split on \\(x_j\\) at each node (see Definition 5.9 in Breiman et al. (1984)). Since only the relative magnitudes of these measures are interesting for researchers, the actual measures of importance that we report are normalized. In this way, the most important variable has always a value of 100, and the others are in the range 0 to 100. As for the Random Forest for Efficiency Analysis Trees [RankingRFEAT()], (9) was implemented for each input variable. Regarding the available arguments of the functions, the user can specify the number of decimal units (digits) and include a barplot (from ggplot2) with the scores of importance (barplot). Additionally, the rankingEAT() function allows to display a horizontal line in the graph to facilitate the cut-off point between important and irrelevant variables (threshold).\nAs an example, we next use the objects modelEAT (an EAT object from the EAT() function) and modelRFEAT (an RFEAT object from the RFEAT() function) created in the previous section to assess the predictors used. These functions return the name of the predictor variables, the scores of importance (in the range 0-100 for the rankingEAT() function) and a barplot (without horizontal line for the rankingRFEAT() function).\n\n\nrankingEAT(object = modelEAT)\n\n$scores\n        Importance\nAAE         100.00\nGDP_PPP      82.13\nI            72.58\nPR           72.45\nPFC          29.07\n\n$barplot\n\n\nFigure 1: Barplot generated by applying rankingEAT() to the PISAindex database to determine the ranking of variable importance.\n\n\n\n\n\nrankingRFEAT(object = modelRFEAT)\n\n$scores\n        Importance\nPR            1.75\nPFC          -1.64\nGDP_PPP      -2.16\nI            -2.87\nAAE          -3.67\n\n$barplot\n\n\nFigure 2: Barplot generated by applying rankingRFEAT() to the PISAindex database to determine the ranking of variable importance.\n\n\n\nNote that negative scores may appear when calculating the importance of variables using the rankingRFEAT() function. The appearance of this type of (negative) score can be understood as, if that variable were removed from the model, ceteris paribus, then an improvement in the predictive capacity of the model would be produced.\n5 Basic EAT and RFEAT models\nEfficiency scores are numerical values that indicate the degree of efficiency of a set of Decision Making Units (DMU). In the eat package, these scores can be calculated through an Efficiency Analysis Trees model, a Random Forest for Efficiency Analysis Trees model or a Convexified Efficiency Analysis Trees model. The code is as follows:\n\n\n# For Efficiency Analysis Trees\nefficiencyEAT(\n  data, x, y, object, scores_model, digits = 3,\n  FDH = TRUE, print.table = FALSE, na.rm = TRUE\n)\n\n# For Random Forest for Efficiency Analysis Trees\nefficiencyRFEAT(\n  data, x, y, object, digits = 3, \n  FDH = TRUE, print.table = FALSE, na.rm = TRUE\n)\n\n# For Convexified Efficiency Analysis Trees\nefficiencyCEAT(\n  data, x, y, object, scores_model, digits = 3,\n  DEA = TRUE, print.table = FALSE, na.rm = TRUE\n)\n\n\nA dataset (data) and the corresponding indexes of input(s) (x) and output(s) (y) must be entered. It is recommended that the data with the DMUs whose efficiency is to be calculated coincide with those used to estimate the frontier. However, it is also possible to calculate the efficiency scores for new data. The efficiency scores are calculated using the mathematical programming model included in the argument scores_model. The following models are available:\nBCC.OUT: The output-oriented radial model (Banker et al. 1984).\nBCC.INP: The input-oriented radial model (Banker et al. 1984).\nRSL.OUT: The output-oriented Russell model (Färe and Lovell 1978).\nRSL.INP: The input-oriented Russell model (Färe and Lovell 1978).\nDDF: The Directional Distance Function (Chambers et al. 1998).\nWAM.MIP: The Measure of Inefficiency Proportions as a type of Weighted Additive Model (Lovell and Pastor 1995).\nWAM.RAM: The Range-Adjusted Measure of Inefficiency as a type of Weighted Additive Model (Lovell and Pastor 1995; Cooper et al. 1999).\nFDH or DEA scores can optionally be computed by setting FDH = TRUE or DEA = TRUE, respectively. Finally, a summary descriptive table of the efficiency scores can be displayed with the argument print.table = TRUE.\nThe output-oriented radial model\nThe output-oriented radial model determines the efficiency score for \\(\\left({\\it \\textbf{x}}_{k} ,{\\it \\textbf{y}}_{k} \\right)\\in R_{+}^{m+s}\\) by equiproportionally increasing all its outputs while maintaining inputs constant: \\(\\phi \\left({\\it \\textbf{x}}_{k}, {\\it \\textbf{y}}_{k} \\right)=\\max \\{\\phi _{k} \\in R:\\) \\(\\left({\\it \\textbf{x}}_{k} ,\\phi_{k} {\\it \\textbf{y}}_{k} \\right)\\in \\Psi \\}\\).\nThe efficiency score \\(\\phi \\left({\\it \\textbf{x}}_{k}, {\\it \\textbf{y}}_{k} \\right)\\) can be estimated through FDH by plugging \\(\\hat{\\Psi }_{FDH}\\) from (2) into \\(\\max \\left\\{\\phi _{k} \\in R:\\left({\\it \\textbf{x}}_{k} ,\\phi _{k} {\\it \\textbf{y}}_{k} \\right)\\in \\Psi \\right\\}\\) in place of \\(\\Psi\\). In that case, the optimization problem can be rewritten as a mixed-integer linear optimization program, as follows:\n\\[\\begin{equation} \\label{(10)}\n\\begin{array}{lllll}\n{\\phi ^{FDH} \\left({\\it \\textbf{x}}_{k} ,{\\it \\textbf{y}}_{k} \\right)=} & {\\max } & {\\phi,} & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} x_{ji} \\le x_{jk} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} y_{ri} \\ge \\phi y_{rk} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} = 1,} & {} & {} \\\\\n{} & {} & {\\lambda _{i} \\in \\left\\{0,1\\right\\},} & {i=1,...,n} & {}\n\\end{array}\n\\end{equation}\\]\nThe Linear Programming model that should be solved under Data Envelopment Analysis would be:\n\\[\\begin{equation} \\label{(11)}\n\\begin{array}{lllll}\n{\\phi ^{DEA} \\left({\\it \\textbf{x}}_{k} ,{\\it \\textbf{y}}_{k} \\right)=} & {\\max } & {\\phi_{}, } & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} x_{ji}  \\le x_{jk} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} y_{ri}  \\ge \\phi_{} y_{rk} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i}  =1,} & {} & {} \\\\\n{} & {} & {\\lambda _{i} \\ge 0,} & {i=1,...,n} & {}\n\\end{array}\n\\end{equation}\\]\nThe following Mixed-Integer Linear Program should be solved for Efficiency Analysis Trees:\n\\[\\begin{equation} \\label{(12)}\n\\begin{array}{lllll}\n{\\phi ^{EAT} \\left({\\it \\textbf{x}}_{k} ,{\\it \\textbf{y}}_{k} \\right)=} & {\\max } & {\\phi, } & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*}} \\lambda_{t} {\\it a}_{j}^{t}  \\le x_{jk} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*}} \\lambda_{t} {\\it d}_{rT^{*}} \\left({\\it \\textbf{a}}^{t} \\right) \\ge \\phi y_{rk} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*}}^{}\\lambda_{t}  =1,} & {} & {} \\\\\n{} & {} & {\\lambda _{t} \\in \\left\\{0,1\\right\\},} & {t\\in \\tilde{T}^{*} } & {}\n\\end{array}\n\\end{equation}\\]\nIn R, this model can be computed by setting scores_model = \"BCC.OUT\" in efficiencyEAT():\n\n\nscores <- efficiencyEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, \n                        scores_model = \"BCC.OUT\", digits = 2, \n                        print.table = TRUE)\n\n Model Mean Std. Dev. Min Q1 Median   Q3  Max\n   EAT 1.03      0.04   1  1   1.01 1.01 1.16\n   FDH 1.01      0.02   1  1   1.00 1.00 1.12\n\nscores %>% sample_n(3)\n\n    EAT_BCC_OUT FDH_BCC_OUT\nCHL        1.09        1.05\nSAU        1.05        1.00\nCAN        1.01        1.01\n\nFinally, the optimization model that should be solved for Convexified Efficiency Analysis Trees is:\n\\[\\begin{equation} \\label{(13)}\n\\begin{array}{lllll}\n{\\phi ^{CEAT} \\left({\\it \\textbf{x}}_{k} ,{\\it \\textbf{y}}_{k} \\right)=} & {\\max } & {\\phi, } & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*}}^{}\\lambda_{t} {\\it a}_{j}^{t}  \\le x_{jk} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*}}^{}\\lambda_{t} {\\it d}_{rT^{*}} \\left({\\it \\textbf{a}}^{t} \\right) \\ge \\phi y_{rk} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*}}^{}\\lambda_{t}=1,} & {} & {} \\\\\n{} & {} & {\\lambda _{t} \\ge 0,} & {t\\in \\tilde{T}^{*}} & {}\n\\end{array}\n\\end{equation}\\]\nIn R, this model can be computed by setting scores_model = \"BCC.OUT\" in efficiencyCEAT():\n\n\nscores <- efficiencyCEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, \n                         scores_model = \"BCC.OUT\", digits = 2,\n                         print.table = TRUE)\n\n Model Mean Std. Dev. Min   Q1 Median   Q3  Max\n  CEAT 1.11      0.07   1 1.05   1.09 1.09 1.31\n   DEA 1.05      0.04   1 1.01   1.05 1.05 1.18\n\nscores %>% sample_n(3)\n\n    CEAT_BCC_OUT DEA_BCC_OUT\nBLR         1.07        1.00\nSVK         1.07        1.04\nRUS         1.04        1.00\n\nIn the case of the output-oriented radial model, Esteve et al. (2021) showed how this measure can be computed through Random Forest where \\(\\phi \\left({\\it \\textbf{x}}_{k} ,{\\it \\textbf{y}}_{k} \\right)=\\max \\left\\{\\phi _{k} \\in R:\\left({\\it \\textbf{x}}_{k} ,\\phi_{k} {\\it \\textbf{y}}_{k} \\right)\\in \\Psi \\right\\}\\) can be estimated by substituting the theoretical production possibility set \\(\\Psi\\) by its estimation \\(\\hat{\\Psi}_{RF+EAT}\\), i.e., \\(\\phi ^{RF+EAT} \\left({\\it \\textbf{x}}_{k} ,{\\it \\textbf{y}}_{k} \\right)= \\max \\left\\{\\phi _{k} \\in R:\\left({\\it \\textbf{x}}_{k} ,\\phi _{k} {\\it \\textbf{y}}_{k} \\right)\\in \\hat{\\Psi }_{RF+EAT} \\right\\}\\). In particular, \\(\\phi^{RF+EAT} \\left({\\it \\textbf{x}}_{k} ,{\\it \\textbf{y}}_{k} \\right)\\) may be calculated as:\n\\[\\begin{equation} \\label{(14)}\n\\phi^{RF+EAT} \\left({\\it \\textbf{x}}_{k} ,{\\it \\textbf{y}}_{k} \\right)={\\mathop{\\min }\\limits_{r=1,...,s}} \\left\\{\\frac{y_{r}^{RF+EAT\\left(\\aleph \\right)} \\left({\\it \\textbf{x}}_{k} \\right)}{y_{rk} } \\right\\},  \n\\end{equation}\\]\nwhere \\(y_{r}^{RF+EAT\\left(\\aleph \\right)} \\left({\\it \\textbf{x}}_{k} \\right)\\) is the estimation of the r-th output given the input bundle \\({\\it \\textbf{x}}_{k}\\).\nIn R, this model can be computed using efficiencyREAT():\n\n\nscores <- efficiencyRFEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelRFEAT, \n                          digits = 2, print.table = TRUE)\n\n Model Mean Std. Dev.  Min Q1 Median   Q3  Max\n RFEAT 1.03      0.04 0.94  1   1.02 1.02 1.15\n   FDH 1.01      0.02 1.00  1   1.00 1.00 1.12\n\nscores %>% sample_n(3)\n\n    RFEAT_BCC_OUT FDH_BCC_OUT\nSGP          0.94           1\nHUN          0.99           1\nMEX          1.00           1\n\nThe input-oriented radial model\nBy analogy with the previous section, where the output-oriented radial model was shown, it is possible to calculate the input-oriented radial technical efficiency of the input-output bundle \\((\\textbf{x}_k, \\textbf{y}_k)\\) by solving the following Mixed-Integer Linear Program, counterpart to (10):\n\\[\\begin{equation} \\label{(15)}\n\\begin{array}{lllll} {} & {\\min } & {\\theta, } & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} x_{ji}  \\le \\theta_{} x_{jk} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} y_{ri}  \\ge y_{rk} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i}  =1,} & {} & {} \\\\\n{} & {} & {\\lambda_{i} \\in \\left\\{0,1\\right\\},} & {i=1,...,n} & {}\n\\end{array}\n\\end{equation}\\]\nThe same type of technical measure can be estimated through DEA by convexification of the production frontier generated by FDH. Next, we show the Linear Programming model that should be solved in that case:\n\\[\\begin{equation} \\label{(16)}\n\\begin{array}{lllll}  \n{} & {\\min } & {\\theta, } & {} & {}  \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} x_{ji}  \\le \\theta_{} x_{jk} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} y_{ri}  \\ge y_{rk} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i}  =1,} & {} & {} \\\\\n{} & {} & {\\lambda _{i} \\ge 0,} & {i=1,...,n} & {}\n\\end{array}\n\\end{equation}\\]\nThe input-oriented radial model in the case of the Efficiency Analysis Trees technique can be determined through the following Mixed-Integer Linear Program:\n\\[\\begin{equation} \\label{(17)}\n\\begin{array}{lllll}\n{} & {\\min } & {\\theta,} & {} & {}  \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t} {\\it a}_{j}^{t}  \\le \\theta x_{jk} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t} {\\it d}_{rT^{*}} \\left({\\it \\textbf{a}}^{t} \\right) \\ge y_{rk} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda _{t}  =1,} & {} & {} \\\\\n{} & {} & {\\lambda _{t} \\in \\left\\{0,1\\right\\},} & {t\\in \\tilde{T}^{*} } & {}\n\\end{array}\n\\end{equation}\\]\nIn R, this model can be computed by setting scores_model = \"BCC.INP\" in efficiencyEAT():\n\n\nscores <- efficiencyEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, \n                        scores_model = \"BCC.INP\", digits = 2,\n                        print.table = TRUE)\n\n Model Mean Std. Dev.  Min   Q1 Median   Q3 Max\n   EAT 0.94      0.06 0.69 0.90   0.96 0.96   1\n   FDH 0.98      0.03 0.90 0.97   1.00 1.00   1\n\nscores %>% sample_n(3)\n\n    EAT_BCC_INP FDH_BCC_INP\nDEU        0.88        0.92\nKAZ        1.00        1.00\nSRB        0.99        1.00\n\nAdditionally, under the Convexified Efficiency Analysis Trees technique, the optimization model corresponding to the convexification of the production possibility set derived from \\(conv(\\hat{\\Psi }_{T^{*}})\\) from (5) should be solved in order to determine an estimation of the input-oriented radial measure as follows:\n\\[\\begin{equation} \\label{(18)}\n\\begin{array}{lllll}\n{} & {\\min } & {\\theta,} & {} & {}  \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t} {\\it a}_{j}^{t}  \\le \\theta_{} x_{jk} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t} {\\it d}_{rT^{*}} \\left({\\it \\textbf{a}}^{t} \\right) \\ge y_{rk} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t}  =1,} & {} & {} \\\\\n{} & {} & {\\lambda _{t} \\ge 0,} & {t\\in \\tilde{T}^{*} } & {}\n\\end{array}\n\\end{equation}\\]\nIn R, this model can be computed by setting scores_model = \"BCC.INP\" in efficiencyCEAT():\n\n\nscores <- efficiencyCEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, \n                         scores_model = \"BCC.INP\", digits = 2, \n                         print.table = TRUE)\n\n Model Mean Std. Dev.  Min   Q1 Median   Q3 Max\n  CEAT 0.82      0.08 0.69 0.76   0.81 0.81   1\n   DEA 0.92      0.07 0.72 0.87   0.91 0.91   1\n\nscores %>% sample_n(3)\n\n    CEAT_BCC_INP DEA_BCC_INP\nQAT         0.93        1.00\nCYP         0.69        0.78\nCHE         0.73        0.85\n\nThe output-oriented Russell measure\nThe output-oriented Russell measure under FDH must be calculated through the following optimization model:\n\\[\\begin{equation} \\label{(19)}\n\\begin{array}{lllll}\n{} & {\\max } & {\\frac{1}{s} \\sum_{r=1}^{s}\\phi_{r},} & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{t} x_{ji}  \\le x_{jk} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{t} y_{ri}  \\ge \\phi_{r} y_{rk} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{t}=1,} & {} & {} \\\\\n{} & {} & {\\lambda_{i} \\in \\left\\{0,1\\right\\},} & {i=1,...,n} & {} \\\\\n{} & {} & {{\\it \\boldsymbol{\\phi}}\\ge {\\bf 1}_{s}.}\n\\end{array}\n\\end{equation}\\]\nUnder DEA, the corresponding model would be:\n\\[\\begin{equation} \\label{(20)}\n\\begin{array}{lllll}\n{} & {\\max } & {\\frac{1}{s} \\sum_{r=1}^{s}\\phi_{r},} & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{t} x_{ji} \\le x_{jk} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{t} y_{ri} \\ge \\phi_{r} y_{rk} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{t}=1,} & {} & {} \\\\\n{} & {} & {\\lambda_{i} \\ge 0,} & {i=1,...,n} & {} \\\\\n{} & {} & {{\\it \\boldsymbol{\\phi}}\\ge {\\bf 1}_{s}.}\n\\end{array}\n\\end{equation}\\]\nIf we resort to the Efficiency Analysis Trees technique, then the model to be solved should be the following:\n\\[\\begin{equation} \\label{(21)}\n\\begin{array}{lllll}\n{} & {\\max } & {\\frac{1}{s} \\sum_{r=1}^{s}\\phi_{r},} & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*}}\\lambda_{t} {\\it a}_{j}^{t} \\le x_{jk} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*}}\\lambda_{t} {\\it d}_{rT^{*}} ({\\it \\textbf{a}}^{t}) \\ge \\phi_{r} y_{rk} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t}=1,} & {} & {} \\\\\n{} & {} & {\\lambda_{t} \\in \\left\\{0,1\\right\\},} & {i=1,...,n} & {} \\\\\n{} & {} & {{\\it \\boldsymbol{\\phi}}\\ge {\\bf 1}_{s}.}\n\\end{array}\n\\end{equation}\\]\nIn R, this model can be computed by setting scores_model = \"RSL.OUT\" in efficiencyEAT():\n\n\nscores <- efficiencyEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, \n                        scores_model = \"RSL.OUT\", digits = 2,\n                        print.table = TRUE)\n\nscores %>% sample_n(3)\n\n\nFinally, under the Convexified Efficiency Analysis Trees technique, the model would be:\n\\[\\begin{equation} \\label{(22)}\n\\begin{array}{lllll}\n{} & {\\max } & {\\frac{1}{s} \\sum_{r=1}^{s}\\phi_{r},  } & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t} {\\it a}_{j}^{t}  \\le x_{jk} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t} {\\it d}_{rT^{*}} \\left({\\it \\textbf{a}}^{t} \\right) \\ge \\phi_{r} y_{rk} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t}  =1,} & {} & {} \\\\\n{} & {} & {\\lambda_{t} \\ge 0,} & {i=1,...,n} & {} \\\\\n{} & {} & {{\\it \\boldsymbol{\\phi}}\\ge {\\bf 1}_{s}.}\n\\end{array}\n\\end{equation}\\]\nIn R, this model can be computed by setting scores_model = \"RSL.OUT\" in efficiencyCEAT():\n\n\nscores <- efficiencyCEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, \n                         scores_model = \"RSL.OUT\", digits = 2, \n                         print.table = TRUE)\n\n Model Mean Std. Dev. Min   Q1 Median   Q3  Max\n  CEAT 1.13      0.08   1 1.07   1.10 1.10 1.34\n   DEA 1.06      0.05   1 1.02   1.06 1.06 1.22\n\nscores %>% sample_n(3)\n\n    CEAT_RSL_OUT DEA_RSL_OUT\nLVA         1.07        1.05\nCHL         1.18        1.13\nMAR         1.14        1.00\n\nThe input-oriented Russell measure\nBy analogy with the output-oriented Russell measure, the input-oriented Russell measure should be calculated through the following optimization models, depending on the selected approach:\n\\[\\begin{equation} \\label{(23)}\n\\begin{array}{lllll}\n{} & {\\min } & {\\frac{1}{m} \\sum_{j=1}^{m}\\theta_{j},  } & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{t} x_{ji} \\le \\theta_{j} x_{jk} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{t} y_{ri} \\ge y_{rk} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{t}  =1,} & {} & {} \\\\\n{} & {} & {\\lambda_{i} \\in \\left\\{0,1\\right\\},} & {i=1,...n} & {} \\\\\n{} & {} & {{\\it \\boldsymbol{\\theta}}\\le {\\bf 1}_{m}.}\n\\end{array}\n\\end{equation}\\]\nUnder DEA, the corresponding model would be:\n\\[\\begin{equation} \\label{(24)}\n\\begin{array}{lllll}\n{} & {\\min } & {\\frac{1}{m} \\sum_{j=1}^{m}\\theta_{j},} & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{t} x_{ji}  \\le \\theta_{j} x_{jk} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{t} y_{ri}  \\ge y_{rk} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{t}=1,} & {} & {} \\\\\n{} & {} & {\\lambda _{i} \\ge 0,} & {i=1,...n} & {} \\\\\n{} & {} & {{\\it \\boldsymbol{\\theta}}\\le {\\bf 1}_{m}.}  \n\\end{array}\n\\end{equation}\\]\nIf we resort to the Efficiency Analysis Trees technique, then the model to be solved should be the following:\n\\[\\begin{equation} \\label{(25)}\n\\begin{array}{lllll}\n{} & {\\min } & {\\frac{1}{m} \\sum_{j=1}^{m}\\theta_{j},  } & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t} {\\it a}_{j}^{t}  \\le \\theta_{j} x_{jk} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t} {\\it d}_{rT^{*}} \\left({\\it \\textbf{a}}^{t} \\right) \\ge y_{rk} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t}=1,} & {} & {} \\\\\n{} & {} & {\\lambda _{t} \\in \\left\\{0,1\\right\\},} & {i=1,...,n} & {} \\\\\n{} & {} & {{\\it \\boldsymbol{\\theta}}\\le {\\bf 1}_{m}.}\n\\end{array}\n\\end{equation}\\]\nIn R, this model can be computed by setting scores_model = \"RSL.INP\" in efficiencyEAT():\n\n\nscores <- efficiencyEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, \n                        scores_model = \"RSL.INP\", digits = 2, \n                        print.table = TRUE)\n\n Model Mean Std. Dev.  Min   Q1 Median   Q3  Max\n   EAT 0.58      0.09 0.43 0.52   0.56 0.56 0.81\n   FDH 0.87      0.10 0.59 0.81   0.86 0.86 1.00\n\nscores %>% sample_n(3)\n\n    EAT_RSL_INP FDH_RSL_INP\nLBN        0.58        0.73\nMAR        0.69        0.97\nMKD        0.58        0.87\n\nFinally, under the Convexified Efficiency Analysis Trees technique, the model would be:\n\\[\\begin{equation} \\label{(26)}\n\\begin{array}{lllll}\n{} & {\\min } & {\\frac{1}{m} \\sum_{j=1}^{m}\\theta_{j},  } & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t} {\\it a}_{j}^{t}  \\le \\theta_{j} x_{jk} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t} {\\it d}_{rT^{*}} \\left({\\it \\textbf{a}}^{t} \\right) \\ge y_{rk} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t}  =1,} & {} & {} \\\\\n{} & {} & {\\lambda _{t} \\ge 0,} & {i=1,...,n} & {} \\\\\n{} & {} & {{\\it \\boldsymbol{\\theta}}\\le {\\bf 1}_{m}.}\n\\end{array}\n\\end{equation}\\]\nIn R, this model can be computed by setting scores_model = \"RSL.INP\" in efficiencyCEAT():\n\n\nscores <- efficiencyCEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, \n                         scores_model = \"RSL.INP\", digits = 2,\n                         print.table = TRUE)\n\n Model Mean Std. Dev.  Min   Q1 Median   Q3  Max\n  CEAT 0.54      0.08 0.43 0.49   0.53 0.53 0.79\n   DEA 0.80      0.11 0.59 0.74   0.78 0.78 1.00\n\nscores %>% sample_n(3)\n\n    CEAT_RSL_INP DEA_RSL_INP\nARG         0.44        0.59\nLUX         0.44        0.65\nCHE         0.49        0.74\n\nThe directional distance function\n\nChambers et al. (1998) introduced the directional distance function (DDF) as a technical efficiency measure that projects\n\\((\\textit{x}_k, \\textit{y}_k)\\)\nthrough a pre-assigned direction \\(\\textbf{g}=(-\\textbf{g}_{j}^{-},+\\textbf{g}_{r}^{+}) \\ne 0_{m+s}, \\textbf{g}_{j}^{-} \\in R^{m}, \\textbf{g}_{r}^{+} \\in R^{s}\\) to the efficiency frontier of the corresponding technology. Under FDH, the DDF is calculated as follows:\n\\[\\begin{equation} \\label{(27)}\n\\begin{array}{lllll}\n{} & {\\max } & {\\beta_{k}, } & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} x_{ji}  \\le x_{jk} -\\beta_{k} g_{j}^{-} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} y_{ri}  \\ge y_{rk} +\\beta_{k} g_{r}^{+} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda _{i}  =1,} & {} & {} \\\\\n{} & {} & {\\lambda _{i} \\in \\left\\{0,1\\right\\},} & {i=1,...,n} & {}\n\\end{array}\n\\end{equation}\\]\nThe corresponding linear program in DEA is as follows:\n\\[\\begin{equation} \\label{(28)}\n\\begin{array}{lllll}\n{} & {\\max } & {\\beta_{k}, } & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} x_{ji} \\le x_{jk} -\\beta_{k} g_{j}^{-} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} y_{ri}  \\ge y_{rk} +\\beta_{k} g_{r}^{+} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i}=1,} & {} & {} \\\\\n{} & {} & {\\lambda_{i} \\ge 0,} & {i=1,...,n} & {}\n\\end{array}\n\\end{equation}\\]\nIn the context of Efficiency Analysis Trees, the DDF is calculated through the following Mixed-Integer Linear Program:\n\\[\\begin{equation} \\label{(29)}\n\\begin{array}{lllll}\n{} & {\\max } & {\\beta_{k}, } & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*}}^{}\\lambda_{t}{\\it a}_{j}^{t}  \\le x_{jk} -\\beta_{k} g_{j}^{-} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*}}^{}\\lambda_{t}{\\it d}_{rT^{*}} \\left({\\it \\textbf{a}}^{t} \\right) \\ge y_{rk} +\\beta_{k} g_{r}^{+} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*}}^{}\\lambda_{t}=1,} & {} & {} \\\\\n{} & {} & {\\lambda _{t} \\in \\left\\{0,1\\right\\},} & {t\\in \\tilde{T}^{*}. } & {}\n\\end{array}\n\\end{equation}\\]\nIn R, this model can be computed by setting scores_model = \"DDF\" in efficiencyEAT():\n\n\nscores <- efficiencyEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, \n                        scores_model = \"DDF\", digits = 2, \n                        print.table = TRUE)\n\n Model Mean Std. Dev. Min Q1 Median   Q3  Max\n   EAT 0.02      0.02   0  0   0.01 0.01 0.13\n   FDH 0.01      0.01   0  0   0.00 0.00 0.05\n\nscores %>% sample_n(3)\n\n    EAT_DDF FDH_DDF\nMDA    0.00    0.00\nLVA    0.00    0.00\nBRA    0.03    0.01\n\nIn the case of Convexified Efficiency Analysis Trees, the optimization model is as follows.\n\\[\\begin{equation} \\label{(30)}\n\\begin{array}{lllll}\n{} & {\\max } & {\\beta _{k}, } & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t} {\\it a}_{j}^{t} \\le x_{jk} -\\beta_{k} g_{j}^{-} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t} {\\it d}_{rT^{*}} \\left({\\it \\textbf{a}}^{t} \\right) \\ge y_{rk} +\\beta_{k} g_{r}^{+} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*} }^{}\\lambda_{t}=1,} & {} & {} \\\\\n{} & {} & {\\lambda _{t} \\ge 0,} & {t\\in \\tilde{T}^{*}. } & {}\n\\end{array}\n\\end{equation}\\]\nIn R, this model can be computed by setting scores_model = \"DDF\" in efficiencyCEAT():\n\n\nscores <- efficiencyCEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, \n                         scores_model = \"DDF\", digits = 2, \n                         print.table = TRUE)\n\n Model Mean Std. Dev. Min   Q1 Median   Q3  Max\n  CEAT 0.07      0.04   0 0.05   0.06 0.06 0.18\n   DEA 0.03      0.03   0 0.00   0.03 0.03 0.12\n\nscores %>% sample_n(3)\n\n    CEAT_DDF DEA_DDF\nIRL     0.05    0.03\nLBN     0.15    0.10\nFRA     0.07    0.06\n\nThe weighted additive model\nThe additive model measures technical efficiency based on input excesses and output shortfalls. It characterizes efficiency in terms of the input and output slacks: \\(\\textbf{s}^{-} \\in R^{m}\\) and \\(\\textbf{s}^{+} \\in R^{s}\\), respectively. The eat package implements the weighted additive model formulation of Lovell and Pastor (1995), where \\(({\\it \\textbf{w}}^{-} ,{\\it \\textbf{w}}^{+})\\in R_{+}^{m} \\times R_{+}^{s}\\) are the input and output weights whose elements can vary across DMUs.\nIn the case of the FDH, the optimization program to be solved would be:\n\\[\\begin{equation} \\label{(31)}\n\\begin{array}{lllll}\n{} & {\\max } & {\\sum_{j=1}^{m}w_{j}^{-} {\\it s}_{jk}^{-} + \\sum_{r=1}^{s}w_{r}^{+} {\\it s}_{rk}^{+},} & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{t} x_{ji} \\le x_{jk} - {\\it s}_{jk}^{-} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{t} y_{ri} \\ge y_{rk} + {\\it s}_{rk}^{+} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} = 1,} & {} & {} \\\\\n{} & {} & {\\lambda _{i} \\in \\left\\{0,1\\right\\},} & {i=1,...,n} & {} \\\\\n{} & {} & {{\\it \\textbf{s}}_{k}^{-} \\ge \\textbf{0}_{m}, {\\it \\textbf{s}}_{k}^{+} \\ge \\textbf{0}_{s}.}\n\\end{array}\n\\end{equation}\\]\nUnder DEA, the model would be as follows:\n\\[\\begin{equation} \\label{(32)}\n\\begin{array}{lllll}\n{} & {\\max } & {\\sum_{j=1}^{m}w_{j}^{-} {\\it s}_{jk}^{-} + \\sum_{r=1}^{s}w_{r}^{+} {\\it s}_{rk}^{+},} & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{t} x_{ji} \\le x_{jk} - {\\it s}_{jk}^{-} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{t} y_{ri} \\ge y_{rk} + {\\it s}_{rk}^{+} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} = 1,} & {} & {} \\\\\n{} & {} & {\\lambda _{i} \\ge 0,} & {i=1,...,n} & {} \\\\\n{} & {} & {{\\it \\textbf{s}}_{k}^{-} \\ge \\textbf{0}_{m}, {\\it \\textbf{s}}_{k}^{+} \\ge \\textbf{0}_{s}.}\n\\end{array}\n\\end{equation}\\]\nWithin the framework of Efficiency Analysis Trees, the weighted additive model would be calculated as follows:\n\\[\\begin{equation} \\label{(33)}\n\\begin{array}{lllll}\n{} & {\\max } & {\\sum_{j=1}^{m}w_{j}^{-} {\\it s}_{jk}^{-} +\\sum_{r=1}^{s}w_{r}^{+} {\\it s}_{rk}^{+},} & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*}}^{}\\lambda_{t} {\\it a}_{j}^{t}  \\le x_{jk} - {\\it s}_{jk}^{-} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*}}^{}\\lambda_{t} {\\it d}_{rT^{*}} \\left({\\it \\textbf{a}}^{t} \\right) \\ge y_{rk} +{\\it s}_{rk}^{+} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} = 1,} & {} & {} \\\\\n{} & {} & {\\lambda _{i} \\in \\left\\{0,1\\right\\},} & {i=1,...,n} & {} \\\\\n{} & {} & {{\\it \\textbf{s}}_{k}^{-} \\ge \\textbf{0}_{m}, {\\it \\textbf{s}}_{k}^{+} \\ge \\textbf{0}_{s}.}\n\\end{array}\n\\end{equation}\\]\nIn R, this model can be computed by setting scores_model = \"WAM.MIP\" for the Measure of Inefficiency Proportions or \"WAM.RAM\" for the Range-Adjusted Measure of Inefficiency (Cooper et al. 1999) in efficiencyEAT():\n\n\nscores <- efficiencyEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT, \n                        scores_model = \"WAM.MIP\", digits = 2,\n                        print.table = TRUE)\n\n Model Mean Std. Dev.  Min   Q1 Median   Q3  Max\n   EAT 2.07      0.56 0.61 1.58   2.18 2.18 3.14\n   FDH 0.40      0.58 0.00 0.00   0.00 0.00 2.37\n\nscores %>% sample_n(3)\n\n    EAT_WAM_MIP FDH_WAM_MIP\nMLT        2.54           0\nSVN        1.86           0\nHRV        2.30           0\n\n\n\nscores <- efficiencyEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT,\n                        scores_model = \"WAM.RAM\", digits = 2,\n                        print.table = TRUE)\n\n Model    Mean Std. Dev.   Min      Q1  Median      Q3     Max\n   EAT 2704.65   1677.11 455.3 1467.78 2425.09 2425.09 8293.59\n   FDH  959.09   1388.56   0.0    0.00    0.00    0.00 5413.27\n\nscores %>% sample_n(3)\n\n    EAT_WAM_RAM FDH_WAM_RAM\nITA     2192.98        0.00\nLVA     1890.38        0.00\nCAN     1724.26     1182.66\n\nAnd, finally, the Convexified Efficiency Analysis Trees weighted additive model would be:\n\\[\\begin{equation} \\label{(34)}\n\\begin{array}{lllll}\n{} & {\\max } & {\\sum_{j=1}^{m}w_{j}^{-} {\\it s}_{jk}^{-} +\\sum_{r=1}^{s}w_{r}^{+} {\\it s}_{rk}^{+},} & {} & {} \\\\\n{} & {s.t.} & {} & {} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*}}^{}\\lambda_{t} {\\it a}_{j}^{t}  \\le x_{jk} - {\\it s}_{jk}^{-} ,} & {j=1,...,m} & {} \\\\\n{} & {} & {\\sum_{t\\in \\tilde{T}^{*}}^{}\\lambda_{t} {\\it d}_{rT^{*}} \\left({\\it \\textbf{a}}^{t} \\right) \\ge y_{rk} +{\\it s}_{rk}^{+} ,} & {r=1,...,s} & {} \\\\\n{} & {} & {\\sum_{i=1}^{n}\\lambda_{i} = 1,} & {} & {} \\\\\n{} & {} & {\\lambda _{i} \\ge 0,} & {i=1,...,n} & {} \\\\\n{} & {} & {{\\it \\textbf{s}}_{k}^{-} \\ge \\textbf{0}_{m}, {\\it \\textbf{s}}_{k}^{+} \\ge \\textbf{0}_{s}.}\n\\end{array}\n\\end{equation}\\]\nIn R, this model can be computed by setting scores_model = \"WAM.MIP\" for the Measure of Inefficiency Proportions or \"WAM.RAM\" for the Range-Adjusted Measure of Inefficiency in efficiencyCEAT():\n\n\nscores <- efficiencyCEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT,\n                         scores_model = \"WAM.MIP\", digits = 2,\n                         print.table = TRUE)\n\n Model Mean Std. Dev.  Min   Q1 Median   Q3  Max\n  CEAT 2.39      0.45 0.96 2.24   2.50 2.50 3.14\n   DEA 0.90      0.62 0.00 0.23   1.07 1.07 2.37\n\nscores %>% sample_n(3)\n\n    CEAT_WAM_MIP DEA_WAM_MIP\nISR         2.74        1.13\nITA         2.73        1.31\nSGP         1.91        0.00\n\n\n\nscores <- efficiencyCEAT(data = PISAindex, x = 4:8, y = 1:3, object = modelEAT,\n                         scores_model = \"WAM.RAM\", digits = 2,\n                         print.table = TRUE)\n\n Model    Mean Std. Dev.    Min      Q1  Median      Q3      Max\n  CEAT 5285.99   2573.13 612.54 3362.98 4836.79 4836.79 12088.64\n   DEA 2413.66   1821.33   0.00  746.22 2495.20 2495.20  7167.29\n\nscores %>% sample_n(3)\n\n    CEAT_WAM_RAM DEA_WAM_RAM\nKOR      1538.61        0.00\nROU      7239.16     4727.17\nEST       612.54        0.00\n\n6 Advanced options and displaying and exporting results\nAdvanced optimization options\nThe bestEAT() and bestRFEAT() functions are aimed at finding the value of the hyperparameters that minimize the root mean squared error (RMSE) calculated from a test sample through an Efficiency Analysis Trees or a Random Forest for Efficiency Analysis Trees model fitted using a training sample. The code of these functions is as follows:\n\n\n# Hyperparameter tuning for Efficiency Analysis Trees\nbestEAT(\n  training, test, x, y,\n  numStop = 5, fold = 5,\n  max.depth = NULL,\n  max.leaves = NULL,\n  na.rm = TRUE\n  )\n\n# Hyperparameter tuning for Random Forest for Efficiency Analysis Trees\nbestRFEAT(\n  training, test, x, y,\n  numStop = 5, m = 50,\n  s_mtry = c(\"5\", \"BRM\"), \n  na.rm = TRUE\n  )\n\n\nHere is an example of using the bestEAT() function. First, the PISAindex database explained in Section Data structure is divided into a training subset with 70% of the DMUs and a test subset with the remaining 30% (these values can be modified).\n\n\nn <- nrow(PISAindex)              # Observations in the dataset\nselected <- sample(1:n, n * 0.7)  # Training indexes\ntraining <- PISAindex[selected, ] # Training set\ntest <- PISAindex[- selected, ]   # Test set\n\n\nThen, we can apply the bestEAT() function. This function, and its equivalent bestRFEAT(), requires a training set (training) on which to fit an Efficiency Analysis Trees model (with cross-validation), a test set (test) on which to calculate the root mean squared error and the input and output indexes (x and y, respectively). The rest of the arguments (numStop, fold, max.depth and max.leaves in case of using the bestEAT() function) are used to create a grid of combinations that determines the number of models to fit. Notice that it is not possible to enter NULL and a certain value in max.depth or max.leaves arguments at the same time (i.e. max.depth = c(NULL, 5, 3)).\nIn the following example, the arguments numStop = (3, 5, 7) and fold = (5, 7) are entered and, consequently, six different models are constructed and fitted with {numStop = 3, fold = 5}, {numStop = 3, fold = 7}, {numStop = 5, fold = 5}, {numStop = 5, fold = 7}, {numStop = 7, fold = 5} and {numStop = 7, fold = 7}. Let us show a numerical example:\n\n\nbestEAT(training = training, test = test, x = 4:8, y = 1:3, \n        numStop = c(3, 5, 7), fold = c(5, 7))\n\n  numStop fold   RMSE leaves\n1       5    5  74.74     15\n2       7    7  83.61     13\n3       3    5  84.17     13\n4       3    7  84.17     13\n5       7    5  86.39      8\n6       5    7 104.93      8\n\nThe best model is given by the hyperparameters {numStop = 5, fold = 5} with RMSE = 74.74 and 15 leaf nodes. Note that sometimes it might be interesting to select a model with a higher RMSE but with a lower number of leaf nodes. With this result, we fit the final Efficiency Analysis Trees model using all the original data.\n\n\nbestEAT_model <- EAT(data = PISAindex, x = 4:8, y = 1:3, numStop = 5, fold = 5)\n\n\nDisplaying results\nGeneral functions for the EAT object\nThe simplest functions to use in order to explore the results of an EAT object are print() and summary(). The function print() returns the tree-structure of an Efficiency Analysis Trees model; while the function summary() returns general information about the fitted model. We show the results with an example:\n\n\nmodelEAT2 <- EAT(data = PISAindex, x = 7, y = 3)\n\n\n\n\nprint(modelEAT2) # [node] y: [prediction] || R: error n(t): nº of DMUs\n\n [1] y: [ 569 ] || R: 15724.19 n(t): 72 \n \n |  [2] AAE < 70.12 --> y: [ 486 ] || R: 3094.43 n(t): 34 \n \n |   |  [4] AAE < 60.75 --> y: [ 472 ] <*> || R: 1553.93 n(t): 17 \n \n |   |  [5] AAE >= 60.75 --> y: [ 486 ] <*> || R: 1006.94 n(t): 17 \n \n |  [3] AAE >= 70.12 --> y: [ 569 ] <*> || R: 3753.38 n(t): 38 \n \n<*> is a leaf node\n\n\n\n# Primary & surrogate splits: Node i --> {SL, SR} || var --> {R: error, s: threshold}\nsummary(modelEAT2)\n\n\n  Formula:  M_PISA ~ AAE \n\n # ========================== # \n #   Summary for leaf nodes   # \n # ========================== # \n \n id n(t)  % M_PISA    R(t)\n  3   38 53    569 3753.38\n  4   17 24    472 1553.93\n  5   17 24    486 1006.94\n\n # ========================== # \n #            Tree            # \n # ========================== # \n \n Interior nodes: 2 \n     Leaf nodes: 3 \n    Total nodes: 5 \n \n           R(T): 6314.25 \n        numStop: 5 \n           fold: 5 \n      max.depth: \n     max.leaves:\n \n # ========================== # \n # Primary & surrogate splits # \n # ========================== # \n \n Node 1 --> {2,3} || AAE --> {R: 6847.81, s: 70.12}\n\n Node 2 --> {4,5} || AAE --> {R: 2560.88, s: 60.75}\n\nRepresenting the efficiency scores\nefficiencyJitter() returns a jitter plot from ggplot2. This graphic shows how DMUs are grouped into leaf nodes in a model built using the EAT() function where each leaf node groups DMUs with the same level of resources. A black dot and a black line represent, respectively, the mean value and the standard deviation of the scores (df_scores from the efficiencyEAT() or the efficiencyCEAT() functions) of a given node. Additionally, efficient DMU labels are always displayed based on the model entered in the scores_model argument. Finally, the user can specify an upper bound (upb) and a lower bound (lwb) in order to show, in addition, the labels whose efficiency score lies between them. The code is as follows:\n\n\nefficiencyJitter(\n  object,\n  df_scores,\n  scores_model,\n  upb = NULL,\n  lwb = NULL\n)\n\n\nAs an example, using data from Section Data structure, we create a new Efficiency Analysis Trees model containing only the AAE and the M_PISA variables. Next, we evaluate the Efficiency Analysis Trees efficiency scores corresponding to the output-oriented radial model and plot them through efficiencyJitter().\n\n\nscores <- efficiencyEAT(data = PISAindex, x = 7, y = 3, object = modelEAT2, \n                        scores_model = \"BCC.OUT\", digits = 2, \n                        print.table = FALSE)\n\n\n\n\n\nFigure 3: Jitter plot generated by ‘efficiencyJitter()’ to show how the countries are grouped inside three particular leaf nodes.\n\n\n\nefficiencyDensity() returns a density plot from ggplot2. This graphic allows to verify the similarity between the scores obtained by the different available methodologies (EAT, FDH, CEAT, DEA and RFEAT) in the eat package.\n\n\nefficiencyDensity(\n  df_scores,\n  model = c(\"EAT\", \"FDH\")\n)\n\n\nIn this case, a comparison between the scores of the EAT and FDH models is shown, where it can be clearly seen how FDH is less restrictive when determining a unit as efficient:\n\n\n\nFigure 4: Density plot generated by ‘efficiencyDensity()’ to show the difference between the score obtained by EAT and FDH.\n\n\n\nOther graphics\nIn the limited case of using only one input for producing only one output, we can display the frontier (from ggplot2) estimated by the EAT() function through the frontier() function:\n\n\nfrontier(\n  object, FDH = TRUE,\n  observed.data = TRUE,\n  observed.color = \"black\",\n  pch = 19,vsize = 1,\n  rwn = FALSE,\n  max.overlaps = 10\n)\n\n\nOptionally, the frontier estimated by FDH can also be plotted if FDH = TRUE. Observed DMUs can be showed by a scatterplot if observed.data = TRUE and its color, shape and size can be modified with observed.color, pch and size respectively. Finally, row names can be included with rwn = TRUE.\nAs an example, we use data simulated from the eat package to generate a data.frame with 50 rows (N = DMUs) and 1 input (nX):\n\n\nsimulated <- Y1.sim(N = 50, nX = 1)\nmodelEAT3 <- EAT(data = simulated, x = 1, y = 2)\n\n\nThen, we apply the frontier() function, where it can be observed how the Efficiency Analysis Trees model generalizes the results obtained by the FDH model:\n\n\n\nFigure 5: Plot of productions functions corresponding to the EAT and the FDH estimator when ‘frontier()’ is applied.\n\n\n\nThe function frontier() shown above only works for the simple case of a low-dimensional scenario with one input and one output. For multiple input and/or output scenarios, the typical tree-structure showing the relationships between outputs and inputs is given by the function plotEAT().\n\n\nplotEAT(\n  object\n)\n\n\nThe nodes of the tree are colored according to the variable by which the split is performed or they are black, in the case of being a leaf node. For each node, we can obtain the following information:\nid: node index.\nR: error at the node.\nn(t): number of DMUs at the node.\ninput variable associated with the split.\ny: vector of output predictions.\nNext, we limit the growth of an Efficiency Analysis Trees model to a maximum size of 5 (max.depth = 4) and display the tree-structure using the plotEAT() function:\n\n\n\nFigure 6: Plot of the tree structure obtained through an EAT model with the parameter max.depth defined as 4.\n\n\n\nFinally, the function plotRFEAT() returns the Out-Of-Bag error for a random forest consisting of k trees. The code of the function and an example with the object modelRFEAT are shown above:\n\n\nplotRFEAT(\n  object\n)\n\n\n\n\n\nFigure 7: Plot of the OOB error corresponding to 30 different RFEAT where k represents the number of trees belonging to each RF.\n\n\n\nIn view of the results, it can be seen how the OOB error presents a great variability for a small number of trees, however, it usually levels off. In our case, it seems that the OOB error levels off from 20 trees onwards around an OOB error of 56, so it could be interesting not to include a greater number of trees in the random forest in order to reduce the computational cost.\n7 Conclusions\nThe eat package allows the estimation of production frontiers in microeconomics and engineering through suitable adaptations of Regression Trees and Random Forest. In the first case, the package implements in R the so-called Efficiency Analysis Trees (EAT) by Esteve et al. (2020), which is a non-parametric technique that competes against the more standard Free Disposal Hull (FDH) technique. In this regard, the EAT technique overcomes the overfitting problem suffered by the FDH technique. FDH is based on three microeconomic postulates. First, the technology determined by FDH satisfies free disposability in inputs and outputs. Second, it is assumed to be deterministic, that is, the production possibility set built by this technique always contains all the observations that belong to the data sample. Third, FDH meets the minimal extrapolation principle. This last postulate implies that FDH generates the smallest set that satisfies the first two postulates. Consequently, the derived efficient frontier is as close to the data as possible, generating overfitting problems. In contrast, the Efficiency Analysis Trees (EAT) technique meets the first two postulates but does not satisfy the minimal extrapolation principle. This fact avoids possible overfitting problems. The difficulty for non-overfitted models lies in where to locate the production possibility set in such a way that it is close to the (unknown) technology associated with the underlying Data Generating Process. In the case of EAT, it is achieved through cross-validation and pruning. A subsequent convexification of the EAT estimation of the technology, known as CEAT by its acronym, yields an alternative estimate of the production possibility set in contrast to the traditional Data Envelopment Analysis (DEA) technique. In the second case, an ensemble of tree models is fitted and aggregated with the objective of achieving robustness in the estimation of the production frontier (Esteve et al. 2021).\nSeveral functions have been implemented in the eat package for determining the best model, through a pruning process based on cross-validation, graphing the results, calculating a ranking of importance of inputs and comparing the efficiency scores estimated by EAT with respect to the standard approaches, i.e., FDH and DEA, through a list of standard technical efficiency measures. We refer to the input and output-oriented radial models, the input and output-oriented Russell measures, the Directional Distance Function and the Weighted Additive model.\nThroughout the paper, we have also shown how to organize the data, use the available functions, and interpret the results. In particular, to illustrate the different functions implemented in the package, we applied all of them on a common empirical example so that results can easily be compared. In this way, we believe that the eat package is a valid self-contained R package for the measurement of technical efficiency from the popular machine learning technique: Decision Trees. Finally, since the code is freely available in an open source repository, users will benefit from the collaboration and review of the community. Users may check and modify the code to adapt it to their own needs and extend it with new definitions.\n8 Acknowledgments\nM. Esteve, V. España and J. Aparicio thank the grant PID2019-105952GB-I00 funded by Ministerio de Ciencia e Innovación/ Agencia Estatal de Investigación /10.13039/501100011033. Additionally, M. Esteve gratefully acknowledges the financial support from the Spanish Ministry of Science, Innovation and Universities under Grant FPU17/05365. X. Barber gratefully acknowledges the financial support from the Spanish Ministry of Science and the State Research Agency under grant PID2019-106341GB-I00. X Barber and J. Aparicio gratefully acknowledge the financial support from the University Miguel Hernandez and the Vice-Rectorate for Research under grant AW1020IP-2020/NAC/00073. This work was also supported by the Generalitat Valenciana under Grant ACIF/2021 (V. España).\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-054.zip\nCRAN packages used\neat, Benchmarking, nonparaeff, npbr, snfa, semsfa\nCRAN Task Views implied by cited packages\nEconometrics\n\n\nS. N. Afriat. Efficiency estimation of production functions. International economic review, 568–598, 1972.\n\n\nI. Álvarez, J. Barbero and J. Zofio. A data envelopment analysis toolbox for MATLAB. Journal of Statistical Software (Online), 95(3): 2020.\n\n\nJ. Aparicio, M. Esteve, J. J. Rodriguez-Sala and J. Zofio. The estimation of productive efficiency through machine learning techniques: Efficiency analysis trees. In Data-enabled analytics: DEA for big data, Eds J. Zhu and V. Charles 2021. Springer. In press.\n\n\nJ. Aparicio, J. T. Pastor, F. Vidal and J. L. Zof ’io. Evaluating productive performance: A new approach based on the product-mix problem consistent with data envelopment analysis. Omega, 67: 134–144, 2017.\n\n\nM. Arnaboldi, G. Azzone and M. Giorgino. Performance measurement and management for engineers. Academic Press, 2014.\n\n\nB. M. Balk, J. Barbero and J. L. Zof ’Io. A total factor productivity toolbox for MATLAB. Available at SSRN 3178911, 2018.\n\n\nR. D. Banker. Hypothesis tests using data envelopment analysis. Journal of productivity analysis, 7(2): 139–159, 1996.\n\n\nR. D. Banker. Maximum likelihood, consistency and data envelopment analysis: A statistical foundation. Management science, 39(10): 1265–1273, 1993.\n\n\nR. D. Banker, A. Charnes and W. W. Cooper. Some models for estimating technical and scale inefficiencies in data envelopment analysis. Management science, 30(9): 1078–1092, 1984.\n\n\nR. D. Banker, A. Charnes, W. W. Cooper and R. Clarke. Constrained game formulations and interpretations for data envelopment analysis. European Journal of Operational Research, 40(3): 299–308, 1989.\n\n\nR. D. Banker and A. Maindiratta. Maximum likelihood estimation of monotone and concave production frontiers. Journal of Productivity Analysis, 3(4): 401–415, 1992.\n\n\nR. A. Berk. Statistical learning from a regression perspective. Springer, 2016.\n\n\nP. Bogetoft and L. Otto. Benchmarking with DEA, SFA, and r. Springer Science & Business Media, 2010.\n\n\nL. Breiman. Random forests. Machine learning, 45(1): 5–32, 2001.\n\n\nL. Breiman, J. Friedman, C. J. Stone and R. A. Olshen. Classification and regression trees. CRC press, 1984.\n\n\nR. G. Chambers, Y. Chung and R. Färe. Profit, directional distance functions, and nerlovian efficiency. Journal of optimization theory and applications, 98(2): 351–364, 1998.\n\n\nV. Charles, J. Aparicio and J. Zhu. Data science and productivity analytics. Springer, 2020.\n\n\nA. Charnes, W. W. Cooper and E. Rhodes. Measuring the efficiency of decision making units. European journal of operational research, 2(6): 429–444, 1978.\n\n\nW. W. Cooper, K. S. Park and J. T. Pastor. RAM: A range adjusted measure of inefficiency for use with additive models, and relations to other models and measures in DEA. Journal of Productivity analysis, 11(1): 5–42, 1999.\n\n\nW. Cooper, L. Seiford, K. Tone and J. Zhu. Some models and measures for evaluating performances with DEA: Past accomplishments and future prospects. Journal of Productivity Analysis, 28(3): 151–163, 2007.\n\n\nA. Daouia, T. Laurent and H. Noh. npbr: A package for nonparametric boundary regression in R. Journal of Statistical Software, 79(9): 1–43, 2017. DOI 10.18637/jss.v079.i09.\n\n\nC. Daraio and L. Simar. Introducing environmental variables in nonparametric frontier models: A probabilistic approach. Journal of productivity analysis, 24(1): 93–121, 2005.\n\n\nG. Debreu. The coefficient of resource utilization. Econometrica: Journal of the Econometric Society, 273–292, 1951.\n\n\nD. Deprins and L. Simar. Measuring labor efficiency in post offices, the performance of public enterprises: Concepts and measurements, M. Marchand, P. Pestieau and H. Tulkens. 1984.\n\n\nR. G. Dyson, R. Allen, A. S. Camanho, V. V. Podinovski, C. S. Sarrico and E. A. Shale. Pitfalls and protocols in DEA. European Journal of operational research, 132(2): 245–259, 2001.\n\n\nB. Efron. Bootstrap methods: Another look at the jackknife. Annals of statistics, 7(1): 1–26, 1979.\n\n\nM. Esteve, J. Aparicio, A. Rabasa and J. J. Rodriguez-Sala. Efficiency analysis trees: A new methodology for estimating production frontiers through decision trees. Expert Systems with Applications, 162: 113783, 2020.\n\n\nM. Esteve, J. Aparicio, J. J. Rodriguez-Sala and J. Zhu. Estimation of production frontiers through random forest: The treatment of lack of robustness, ranking of inputs and curse of dimensionality under free disposal hull. Working paper. 2021.\n\n\nR. Färe and C. K. Lovell. Measuring the technical efficiency of production. Journal of Economic theory, 19(1): 150–162, 1978.\n\n\nR. Färe and D. Primont. Multi-output production and duality: Theory and applications. Springer Science & Business Media, 1995.\n\n\nM. Farrel. The measure of productive efficiency. Journal of the Royal Statistical Society, 120: 1957.\n\n\nG. Ferrara and F. Vidoli. Semsfa: Semiparametric estimation of stochastic frontier models. 2018. URL https://CRAN.R-project.org/package=semsfa. R package version 1.1.\n\n\nL. Friedman and Z. Sinuany Stern. Combining ranking scales and selecting variables in the DEA context: The case of industrial branches. Computers & Operations Research, 25(9): 781–791, 1998.\n\n\nB. Golany and Y. Roll. An application procedure for DEA. Omega, 17(3): 237–250, 1989.\n\n\nC. Homburg. Using data envelopment analysis to benchmark activities. International journal of production economics, 73(1): 51–58, 2001.\n\n\nY. Ji and C. Lee. Data envelopment analysis. The Stata Journal, 10(2): 267–280, 2010.\n\n\nD. Khezrimotlagh, J. Zhu, W. D. Cook and M. Toloo. Data envelopment analysis and big data. European Journal of Operational Research, 274(3): 1047–1054, 2019.\n\n\nT. C. Koopmans. An analysis of production as an efficient combination of activities. Activity analysis of production and allocation, 1951.\n\n\nM. Kuhn, K. Johnson, et al. Applied predictive modeling. Springer, 2013.\n\n\nT. Kuosmanen and A. Johnson. Modeling joint production of multiple outputs in StoNED: Directional distance function approach. European Journal of Operational Research, 262(2): 792–801, 2017.\n\n\nT. Kuosmanen and A. L. Johnson. Data envelopment analysis as nonparametric least-squares regression. Operations Research, 58(1): 149–160, 2010.\n\n\nM. Landete, J. F. Monge and J. L. Ruiz. Robust DEA efficiency scores: A probabilistic/combinatorial approach. Expert Systems with Applications, 86: 145–154, 2017.\n\n\nC. K. Lovell and J. T. Pastor. Units invariant and translation invariant DEA models. Operations research letters, 18(3): 147–151, 1995.\n\n\nT. McKenzie. Snfa: Smooth non-parametric frontier analysis. 2018. URL https://CRAN.R-project.org/package=snfa. R package version 0.0.1.\n\n\nT. R. Nunamaker. Using data envelopment analysis to measure the efficiency of non-profit organizations: A critical evaluation. Managerial and decision Economics, 6(1): 50–58, 1985.\n\n\nC. J. O’Donnell et al. Productivity and efficiency analysis. Springer, 2018.\n\n\nP. OECD. Assessment and analytical framework, PISA. 2018.\n\n\nD. Oh and D. Suh. Nonparaeff: Nonparametric methods for measuring efficiency and productivity. 2013. URL https://CRAN.R-project.org/package=nonparaeff. R package version 0.5-8.\n\n\nL. Orea and J. L. Zof ’io. Common methodological choices in nonparametric and parametric analyses of firms’ performance. In The palgrave handbook of economic performance analysis, pages. 419–484 2019. Springer.\n\n\nJ. T. Pastor, J. L. Ruiz and I. Sirvent. A statistical test for nested radial DEA models. Operations Research, 50(4): 728–735, 2002.\n\n\nR. L. Raab and R. W. Lichty. Identifying subareas that comprise a greater metropolitan area: The criterion of county relative efficiency. Journal of Regional Science, 42(3): 579–594, 2002.\n\n\nR. W. Shephard. Theory of cost and production functions. Princeton University Press, 1953.\n\n\nL. Simar and P. W. Wilson. A general methodology for bootstrapping in non-parametric frontier models. Journal of applied statistics, 27(6): 779–802, 2000a.\n\n\nL. Simar and P. W. Wilson. Sensitivity analysis of efficiency scores: How to bootstrap in nonparametric frontier models. Management science, 44(1): 49–61, 1998.\n\n\nL. Simar and P. W. Wilson. Statistical inference in nonparametric frontier models: The state of the art. Journal of productivity analysis, 13(1): 49–78, 2000b.\n\n\nSocial Progress Index. Social progress index 2018. 2018.\n\n\nStataCorp. Stata statistical software: Release 14. 2021. URL http://www.stata.com/. StataCorp LP, College Station.\n\n\nThe MathWorks Inc. MATLAB – the language of technical computing. 2021. URL http://www.mathworks.com/products/matlab/. The MathWorks, Natick, MA, USA.\n\n\nJ. Zhu et al. DEA under big data: Data enabled analytics and network data envelopment analysis. Annals of Operations Research, 1–23, 2019.\n\n\n\n\n",
    "preview": "articles/RJ-2022-054/distill-preview.png",
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {},
    "preview_width": 707,
    "preview_height": 350
  },
  {
    "path": "articles/RJ-2022-055/",
    "title": "Will the Real Hopkins Statistic Please Stand Up?",
    "description": "Hopkins statistic [@hopkins1954new] can be used to test for spatial randomness of data and for detecting clusters in data. Although the method is nearly 70 years old, there is persistent confusion regarding the definition and calculation of the statistic. We investigate the confusion and its possible origin. Using the most general definition of Hopkins statistic, we perform a small simulation to verify its distributional properties, provide a visualization of how the statistic is calculated, and provide a fast R function to correctly calculate the statistic.  Finally, we propose a protocol of five questions to guide the use of Hopkins statistic.",
    "author": [
      {
        "name": "Kevin Wright",
        "url": {}
      }
    ],
    "date": "2022-12-20",
    "categories": [],
    "contents": "\n1 Introduction\nHopkins and Skellam (1954) introduced a statistic to test for spatial randomness of data. If the null hypothesis of spatial randomness is rejected, then one possible interpretation is that the data may be clustered into distinct groups. Since one of the problems with clustering methods is that they will always identify clusters, (even if there are no meaningful clusters in the data), Hopkins statistic can be used to determine if there are clusters in the data before applying clustering methods. In the description below on how to calculate Hopkins statistic, we follow the terminology of earlier authors and refer to an “event” as one of the existing data values in a matrix \\(X\\), and a “point” as a new, randomly chosen location. For clarity in the discussions below we make a distinction between \\(D\\), the dimension of the data, and \\(d\\), the exponent in the formula for Hopkins statistic.\nLet \\(X\\) be a matrix of \\(n\\) events (in rows) and \\(D\\) variables (in columns). Let \\(U\\) be the space defined by \\(X\\).\nHopkins statistic is calculated with the following algorithm:\nSample at random one of the existing events from the data \\(X\\). Let \\(w_i\\) be the Euclidean distance from this event to the nearest-neighbor event in \\(X\\).\nGenerate one new point uniformly distributed in \\(U\\). Let \\(u_i\\) be the Euclidean distance from this point to the nearest-neighbor event in \\(X\\).\nRepeat steps (1) and (2) \\(m\\) times, where \\(m\\) is a small fraction of \\(n\\), such as 10%.\nCalculate \\(H = \\sum_{i=1}^m u_i^d \\big/ \\sum_{i=1}^m (u_i^d + w_i^d)\\), where \\(d=D\\).\nBecause of sampling variability, it is common to calculate \\(H\\) multiple times and take the average. Under the null hypothesis of spatial randomness, this statistic has a Beta(\\(m\\),\\(m\\)) distribution and will always lie between 0 and 1. The interpretation of \\(H\\) follows these guidelines:\nLow values of \\(H\\) indicate repulsion of the events in \\(X\\) away from each other.\nValues of \\(H\\) near 0.5 indicate spatial randomness of the events in \\(X\\).\nHigh values of \\(H\\) indicate possible clustering of the events in \\(X\\). Values of \\(H > 0.75\\) indicate a clustering tendency at the 90% confidence level (Lawson and Jurs 1990).\n2 A short history of Hopkins statistic\nThere exists considerable confusion about the definition of Hopkins statistic in scientific publications. In particular, when calculating Hopkins statistic, there are 3 different values of the exponent \\(d\\) (in step 4 above) that have been used in statistical literature: \\(d=1\\), \\(d=2\\), and the generalized \\(d=D\\). Here is a brief timeline of how this exponent has been presented.\n1954: Hopkins and Skellam (1954) introduced Hopkins statistic in a two-dimensional setting. The formula they present is in a slightly different form, but is equivalent to \\(\\sum u_i^2 \\big/ \\sum (u_i^2 + w_i^2 )\\). The exponent here is \\(d=2\\).\n1976: Diggle et al. (1976) presented a formula for Hopkins statistic in a two-dimensional setting as \\(\\sum u_i \\big/ \\sum (u_i + w_i )\\). This formula has no exponents and therefore at first glance appears to use the exponent \\(d=1\\) in the equation for Hopkins statistic. However, a careful reading of their text shows that their \\(u_i\\) and \\(w_i\\) values were actually squared Euclidean distances. If their \\(u_i\\) and \\(w_i\\) had represented ordinary (non-squared) Euclidean distances, then their formula would have been \\(\\sum u_i^2 \\big/ \\sum (u_i^2 + w_i^2 )\\). We suspect this paper is the likely source of confusion by later authors.\n1982: Cross and Jain (1982) generalized Hopkins statistic for \\(X\\) of any dimension \\(d=D\\) as \\(\\sum u_i^d \\big/ \\sum (u_i^d + w_i^d )\\). This formula was also used by Zeng and Dubes (1985a), Dubes and Zeng (1987), and Banerjee and Dave (2004).\n1990: Lawson and Jurs (1990) and Jurs and Lawson (1990) give the formula for Hopkins statistic as \\(\\sum u_i \\big/ \\sum (u_i + w_i)\\), but used ordinary distances instead of squared distances. Perhaps this was a result of misunderstanding the formula in Diggle et al. (1976).\n2015: The R function hopkins() in the clustertend package (YiLan and RuTong 2015 version 1.4) cited Lawson and Jurs (1990) and used also used the exponent \\(d=1\\).\n2022: The new function hopkins() in the hopkins package (Wright 2022 version 1.0) uses the general exponent \\(d=D\\) as found in Cross and Jain (1982).\n3 Simulation study for the distribution of Hopkins statistic\nHaving identified the confusion in the statistical literature, we now ask the question, “Does it matter what value of \\(d\\) is used in the exponent?” In a word, “yes”.\nAccording to Cross and Jain (1982), under the null hypotheses of no structure in the data, the distribution of the Hopkins statistic is Beta(\\(m\\),\\(m\\)) where \\(m\\) is the number of rows sampled in \\(X\\). This distribution can be verified in a simple simulation study:\nGenerate a matrix \\(X\\) with 100 rows (events) and \\(D=3\\) columns, filled with random uniform numbers. (This is the assumption of no spatial structure in a 3D hypercube.)\nSample \\(m=10\\) events and also generate 10 new uniform points.\nCalculate Hopkins statistic with exponents \\(d=1\\) (incorrect value).\nCalculate Hopkins statistic with exponents \\(d=3\\) (correct value).\nRepeat 1000 times.\nCompare the empirical density curves of the two different methods to the Beta(\\(m\\),\\(m\\)) distribution.\n\n\n\nFigure 1: Results of a simulation study of the distribution of Hopkins statistic. The red and blue lines are the empirical density curves of 1000 Hopkins statistics calculated with exponents \\(d=1\\) (red) and \\(d=3\\) (blue). The black line is the theoretical distribution of the Hopkins statistic. The red line is very far away from the black line and shows that calculating Hopkins statistic with exponent \\(d=1\\) is incorrect.\n\n\n\nIn Figure 1:\nThe black curve is the density of Beta(10,10).\nThe red curve is the density of Hopkins statistic when \\(d=1\\) is used in the calculation (incorrect).\nThe blue curve is the density of Hopkins statistic when \\(d=3\\) (the number of columns in \\(X\\)) is used (correct).\nThe empirical density of the blue curve is similar to the theoretical distribution shown by the black line. The empirical density of the red curve is clearly dissimilar. The distribution of Hopkins statistic with \\(d=1\\) is clearly incorrect (except in trivial cases where \\(X\\) has only 1 column). One more thing to note about the graph is that the blue curve is slightly flatter than the theoretical distribution shown in black. This is not accidental, but is caused by edge effects of the sampling region and will be discussed in a later section.\n4 Examples\n\n\n\nThe first three examples in this section are adapted from Gastner (2005). The datasets are available in the spatstat.data package (Baddeley et al. 2021). A modified version of the hopkins() function was written for this paper to show how the Hopkins statistic is calculated (inspired by Figure 1 of Lawson and Jurs (1990)). In order to minimize the amount of over-plotting, only \\(m=3\\) sampling points are used for these examples. In each figure, 3 of the existing events in \\(X\\) are chosen at random and a light-blue arrow is drawn to the nearest neighbor in \\(X\\). In addition, 3 points are drawn uniformly in the plotting region and a light-red arrow is drawn to the nearest neighbor in \\(X\\). The colored numbers are the lengths of the arrows.\nExample 1: Systematically-spaced data\n\n\n\nFigure 2: An example of how Hopkins statistic is calculated with systematically-spaced data. The black circles are the events of the cells data. Each blue W represents a randomly-chosen event. Each blue arrow points from a W to the nearest-neighboring event. Each red U is a new, randomly-generated point. Each red arrow points from a U to the nearest-neighboring event. The numbers are the length of the arrows. In systematically-spaced data, red arrows tend to be shorter than blue arrows.\n\n\n\nThe cells data represent the centers of mass of 42 cells from insect tissue. The scatterplot of the data in Figure 2 shows that events are systematically spaced as nearly far apart as possible. Because the data is two-dimensional, Hopkins statistics is calculated as the sum of the squared distances \\(u_i^2\\) divided by the sum of the squared distances \\(u_i^2 + w_i^2\\):\n\n\n(.046^2 + .081^2 + .021^2) / \n  ( (.046^2 + .081^2 + .021^2) + (.152^2 + .14^2 + .139^2) )\n\n[1] 0.1281644\n\nThe hopkins() function returns the same value:\n\n\nset.seed(17)\nhopkins(cells, m=3)\n\n[1] 0.1285197\n\n\n\n\nThe value of the Hopkins statistic in this calculation is based on only \\(m=3\\) events and will have sizable sampling error. To reduce the sampling error, a larger sample size can be used up to approximately 10% of the number of events. To reduce sampling error further while maintaining the independence assumption of the sampling in calculating Hopkins statistic, repeated samples can be drawn. Here we use the idea of Gastner (2005) to calculate Hopkins statistic 100 times and then calculate the mean and standard deviation for the 100 values of Hopkins statistic, which in this case are 0.21 and 0.06. This value of the statistic is quite a bit lower than 0.5 and indicates the events are spaced more evenly than purely-random events (p-value 0.05).\nExample 2: Randomly-spaced data\nThe japanesepines data contains the locations of 65 Japanese black pine saplings in a square 5.7 meters on a side. The plot of the data in Figure 3 is an example of data in which the events are randomly spaced.\n\n\n\nFigure 3: An example of how Hopkins statistic is calculated with randomly-spaced data. The black circles are the events of the japanesepines data. Each blue W represents a randomly-chosen event. Each blue arrow points from a W to the nearest-neighboring event. Each red U is a new, randomly-generated point. Each red arrow points from a U to the nearest-neighboring event. The numbers are the length of the arrows. In randomly-spaced data, red arrows tend to be similar in length to blue arrows.\n\n\n\n\nThe value of Hopkins statistic using 3 events and points is:\n\n\n(.023^2+.076^2+.07^2) /\n  ((.023^2+.076^2+.07^2) + (.104^2+.1^2+.058^2))\n\n[1] 0.3166596\n\n\n\n\nThe mean and standard deviation of the 100 Hopkins statistics are 0.48 and 0.12. The value of the statistic is close to 0.5 and indicates no evidence against a random distribution of data (p-value 0.9).\nExample 3: Clustered data\n\n\n\nFigure 4: An example of how Hopkins statistic is calculated with clustered data. The black circles are the events of the redwood data. Each blue W represents a randomly-chosen event. Each blue arrow points from a W to the nearest-neighboring event. Each red U is a new, randomly-generated point. Each red arrow points from a U to the nearest-neighboring event. The numbers are the length of the arrows. In clustered data, red arrows tend to be longer in length than blue arrows.\n\n\n\n\nThe redwood data are the coordinates of 62 redwood seedlings in a square 23 meters on a side. The plot in Figure 4 shows events that exhibit clustering. The value of Hopkins statistic for the plot is:\n\n\n(.085^2+.078^2+.158^2) /\n  ((.085^2+.078^2+.158^2) + (.028^2+.028^2+.12^2))\n\n[1] 0.7056101\n\n\n\n\nThe mean and standard deviation of the 100 Hopkins statistics are 0.79 and 0.13. The value of the statistic is much higher than 0.5, which indicates that the data are somewhat clustered (p-value 0.03).\nExample 4\nAdolfsson et al. (2017) provide a review of various methods of detecting clusterability. One of the methods they considered was Hopkins statistic, which they calculated using 10% sampling. They evaluated the clusterability of nine R datasets by calculating Hopkins statistic 100 times and then reporting the proportion of time that Hopkins statistic exceeded the appropriate beta quantile. We can repeat their analysis and calculate Hopkins statistic for both \\(d=1\\) dimension and \\(d=D\\) dimensions, where \\(D\\) is the number of columns for each dataset.\n\n\n\n\n\nTable 1: In this table, dataset is the R dataset name, n is the number of rows in the data, D is the number of columns in the data, Adolfsson is the the proportion of 100 times that Hopkins statistic was significant as appearing in the paper by Adolfsson, Ackerman, and Brownsteain (2017), Hopkins1 is the proportion of 100 times that Hopkins statistic was significant when calculated with the exponent \\(d=1\\) (similar to the clustertend package), and HopkinsD is the proportion of 100 times that Hopkins statistic was significant when calculated with the exponent \\(d=D\\). Since the Adolfsson and Hopkins1 columns are similar (within samling variation), it appears that Adolfsson, Ackerman, and Brownstein (2017) used the clustertend package to calculate Hopkins statistic.\n\n\ndataset\n\n\nn\n\n\nD\n\n\nAdolfsson\n\n\nHopkins1\n\n\nHopkinsD\n\n\nfaithful\n\n\n272\n\n\n2\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\niris\n\n\n150\n\n\n5\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\nrivers\n\n\n141\n\n\n1\n\n\n0.92\n\n\n0.89\n\n\n0.90\n\n\nswiss\n\n\n47\n\n\n6\n\n\n0.41\n\n\n0.25\n\n\n0.94\n\n\nattitude\n\n\n30\n\n\n7\n\n\n0.00\n\n\n0.00\n\n\n0.59\n\n\ncars\n\n\n50\n\n\n2\n\n\n0.19\n\n\n0.23\n\n\n0.68\n\n\ntrees\n\n\n31\n\n\n3\n\n\n0.18\n\n\n0.22\n\n\n0.71\n\n\nUSJudgeRatings\n\n\n43\n\n\n12\n\n\n0.69\n\n\n0.53\n\n\n1.00\n\n\nUSArrests\n\n\n50\n\n\n4\n\n\n0.01\n\n\n0.00\n\n\n0.56\n\n\n\n\n\nIn Table 1:\nColumn 1 is the name of the R dataset.\nColumn 2 is the number of observations \\(n\\).\nColumn 3 is the number of dimensions \\(D\\).\nColumn 4 is the proportion of 100 times that Hopkins statistic is significant as reported by Adolfsson et al. (2017).\nColumns 5 and 6 use the hopkins package. Column 5 is the proportion of 100 times that Hopkins statistic with exponent \\(d=1\\) and column 6 is the proportion of 100 times that Hopkins statistic with exponent \\(d=D\\) is significant.\nSince the Adolfsson and Hopkins1 columns are similar (within sampling variability), it appears that Adolfsson et al. (2017) used Hopkins statistic with \\(d=1\\) as the exponent. This would be expected if they had used the clustertend package (YiLan and RuTong 2015 version 1.4) to calculate Hopkins statistic.\nFor a few of the datasets, there is substantial disagreement between the last two columns. For example, the swiss data is significantly clusterable 41% of the time according to Adolfsson et al. (2017), but 94% of the time when using Hopkins statistic with exponent \\(d=D\\). A scatterplot of the swiss data in Figure 5 shows that the data are strongly non-random, which agrees with the 94%.\n\n\n\nFigure 5: Pairwise scatterplots of the R dataset swiss. The meaning of the variables is not important here. Because some panels show a lack of spatial randomness of the data, we would expect Hopkins statistic to be significant.\n\n\n\nSimilarly, the trees data is significantly clusterable 18% of the time according to the Adolfsson column, but 71% of the time according to HopkinsD. The scatterplot in Figure 6 shows strong non-random patterns, which agrees with the 71%\n\n\n\nFigure 6: Pairwise scatterplots of the R dataset trees. The data are Girth, Height, and Volume of 31 black cherry trees. Because all panels show a lack of spatial randomness of the data, we would expect Hopkins statistic to be significant.\n\n\n\nScatterplot matrices of the swiss, attitude, cars, trees, and USArrests datasets can be found in Brownstein et al. (2019). Each scatterplot matrix shows at least one pair of the variables with notable correlation and therefore the data are not randomly-distributed, but rather are clustered. For each of these datasets, the proportion of times Hopkins1 is significant is less than 0.5, but the proportion of times HopkinsD is significant is greater than 0.5. The HopkinsD statistic is accurately detecting the presence of clusters in these datasets.\n5 Correcting for edge effects\nIn the cells, japanesepines and redwood examples above, it is possible or even probable that there are additional events outside of the sampling frame that contains the data. The sampling frame thus has the effect of cutting off potential nearest neighbors from consideration. If the distribution of the data can be assumed to extend beyond the sampling frame and if the shape of the sampling frame can be viewed as a hypercube, then edge effects due to the sampling frame can be corrected by using a torus geometry that wraps edges of the sampling frame around to the opposite side (Li and Zhang 2007). To see an illustration of this, look again at the plot of the japanesepines data in Figure 3. The randomly-generated event \\(U\\) in the upper left corner is a distance of \\(0.076\\) away from the nearest event. However, if the left edge of the plot is wrapped around an imaginary cylinder and connected to the right edge of the plot, then the nearest neighbor is the event in the upper-right corner at coordinates (0.97, 0.86).\nTo see what effect the torus geometry has on the distribution of the Hopkins statistic, consider the following simulation. We generate \\(n=100\\) events uniformly in a \\(D=5\\) dimension unit cube and sample \\(m=10\\) events to calculate the value of Hopkins statistic using both a simple geometry and a torus geometry. Repeat these steps \\(B=1000\\) times. The calculation of the nearest neighbor using a torus geometry is computationally more demanding than using a simple geometry, especially as the number of dimensions \\(D\\) increases, so the use of parallel computing can reduce the computing time linearly according to the number of processors used. As a point of reference, this small simulation study was performed in less than 1 minute on a reasonably-powerful laptop with 8 cores using the doParallel package (Microsoft Corporation and Weston 2020). We found that \\(B=1000\\) provided results that were stable, regardless of the seed value for the random number generation in the simulations.\n\n\n\n\n\n\nFigure 7: Results of a simulation study considering how the spatial geometry affects Hopkins statistic. The thin black line is the theoretical distribution of Hopkins statistic. The blue and green lines are the empirical density curves of 1000 Hopkins statistics calculated with simple geometry (blue) and torus geometry (green). Calculating Hopkins statistic with a torus geometry aligns closely to the theoretical distribution.\n\n\n\nIn Figure 7:\nThe black curve is the density of Beta(10,10).\nThe blue curve is the empirical density of the 1000 values of Hopkins statistic calculated using a simple geometry.\nThe green curve is the empirical density of the 1000 values of Hopkins statistic calculated using a torus geometry.\nWhen using a torus geometry to correct for edge effects in this example, the empirical distribution of Hopkins statistic is remarkably close to its theoretical distribution. In contrast, when a simple geometry is used, the empirical distribution of Hopkins statistic is somewhat flattened with heavier tails. The practical result is that when no edge correction is used, the Hopkins statistic is more likely to deviate from 0.5 and therefore more likely to suggest the data is not uniformly distributed. This erroneous interpretation is a greater risk as the number of dimensions \\(D\\) increases and edge effects become more pronounced\n6 Sampling frame effects\n\n\n\nFigure 8: The left figure shows 250 points simulated randomly in a unit square. As expected, the value of Hopkins statistic is close to 0.5. The right figure shows the same points, but only those inside a unit-diameter circle. The value of Hopkins statistic H is much larger than 0.5. Although both figures depict spatially-uniform points, the square shape of the sampling frame affects the value of Hopkins statistic.\n\n\n\nAnother practical problem affecting the correct use and interpretation of Hopkins statistic has to do with the shape of the sampling frame. Consider the example data in Figure 8. On the left side, there were 250 random events simulated in a 2-dimensional unit square. On the right side, the same data are used, but have been subset to keep only the events inside a unit-diameter circle. For both figures, Hopkins statistic was calculated 100 times with 10 events sampled each time.\nOn the left side, both the bounding box and the actual sampling frame are the unit square. The median of 100 Hopkins statistics is 0.51, providing no evidence against random distribution. On the right side, the actual sampling frame of the data is a unit-circle, but the Hopkins statistic still uses the unit square (for generating new points in \\(U\\)) and the median Hopkins statistic is 0.75, indicating clustering of the data within the sampling frame, even though the distribution of the data was generated uniformly. A few more examples of problems related to the sampling frame can be found in Smith and Jain (1984).\nTo consider the problem with the sampling frame on real data, refer again to the trees data in Figure 6. Because trees usually grow both in height and girth at the same time, it would be unexpected to find tall trees with narrow girth or short trees with large girth. Also, since the volume is a function of the girth and height, it is correlated with those two variables. In the scatterplot of girth versus volume, it would be nearly impossible to find points in the upper left or lower right corner of the square. From a biological point of view, the sampling frame cannot be shaped like a square and the null hypothesis of uniform distribution of data is violated a priori, which means the distribution of Hopkins statistic does not follow a Beta(\\(m\\),\\(m\\)) distribution.\n7 A protocol for using Hopkins statistic\nBecause Hopkins statistic is not hard to calculate and is easy to interpret, yet can be misused (as shown in the previous sections), we propose a protocol for using Hopkins statistic. The protocol simply asks the practitioner to consider the following five questions before calculating Hopkins statistic.\nIs the number of events \\(\\mathbf{n > 100}\\) and the number of randomly-sampled events at most 10% of \\(\\mathbf{n}\\)? This is recommended by Cross and Jain (1982).\nIs spatial randomness of the events even possible? If the events are known or suspected to be correlated, this violates the null hypothesis of spatial uniformity, and may also mean that the sampling frame is not shaped like a hypercube.\nCould nearest-neighbor events have occurred outside the boundary of the sampling frame? If yes, it may be appropriate to calculate nearest-neighbor distances using a torus geometry.\nIs the sampling frame non-rectangular? If yes, then be extremely careful with the use of Hopkins statistic in how points are samples from \\(U\\).\nIs the dimension of the data much greater than 2? Edge effects are more common in higher dimensions.\nThe important point of this protocol is to raise awareness of potential problems. We leave it to the practitioner to decide what do with the answers to these questions.\n8 Conclusion\nThe statistical literature regarding Hopkins statistic is filled with confusion about how to calculate the statistic. Some publications have erroneously used the exponent \\(d=1\\) in the formula for Hopkins statistic and this error has propagated into much statistical software and led to incorrect conclusions. To remedy this situation, the R package hopkins (Wright 2022) provides a function hopkins() that calculates Hopkins statistic using the general exponent \\(d=D\\) for D-dimensional data. The function can use simple geometry for fast calculations or torus geometry to correct for edge effects. Using this function, we show that the distribution of Hopkins statistic calculated with the general exponent \\(d=D\\) aligns closely with the theoretical distribution of the statistic. Because inference with Hopkins statistic can be trickier than expected, we introduce a protocol of five questions to consider when using Hopkins statistic.\nAlternative versions of Hopkins statistic have been examined by Zeng and Dubes (1985b), Rotondi (1993), Li and Zhang (2007). Other methods of examining multivariate uniformity of data have been considered by Smith and Jain (1984), Yang and Modarres (2017), and Petrie and Willemain (2013).\n9 Acknowledgements\nThanks to Deanne Wright for bringing the confusion about Hopkins statistic to our attention.\nThanks to Vanessa Windhausen and Deanne Wright for reading early drafts of this paper and to Dianne Cook for reviewing the final version.\nThanks to Wong (2013) for the pdist package for fast computation of nearest neighbors and thanks to Northrop (2021) for the donut package for nearest neighbor search on a torus.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-055.zip\nCRAN packages used\nclustertend, hopkins, doParallel, pdist, donut\nCRAN Task Views implied by cited packages\n\n\nA. Adolfsson, M. Ackerman and N. C. Brownstein. To cluster, or not to cluster: How to answer the question. Proceedings of Knowledge Discovery from Data, Halifax, Nova Scotia, Canada, August 13-17 (TKDD ’17), 2017. URL https://maya-ackerman.com/wp-content/uploads/2018/09/clusterability2017.pdf.\n\n\nA. Baddeley, R. Turner and E. Rubak. spatstat.data: Datasets for ’spatstat’ family. 2021. URL https://CRAN.R-project.org/package=spatstat.data. R package version 2.1-0.\n\n\nA. Banerjee and R. N. Dave. Validating clusters using the Hopkins statistic. In 2004 IEEE international conference on fuzzy systems (IEEE cat. No. 04CH37542), pages. 149–153 2004. IEEE. URL https://doi.org/10.1109/FUZZY.2004.1375706.\n\n\nN. C. Brownstein, A. Adolfsson and M. Ackerman. Descriptive statistics and visualization of data from the R datasets package with implications for clusterability. Data in Brief, 25: 104004, 2019. URL https://doi.org/10.1016/j.dib.2019.104004.\n\n\nG. R. Cross and A. K. Jain. Measurement of clustering tendency. In Theory and application of digital control, pages. 315–320 1982. URL https://doi.org/10.1016/B978-0-08-027618-2.50054-1.\n\n\nP. J. Diggle, J. Besag and J. T. Gleaves. Statistical analysis of spatial point patterns by means of distance methods. Biometrics, 659–667, 1976. URL https://doi.org/10.2307/2529754.\n\n\nR. C. Dubes and G. Zeng. A test for spatial homogeneity in cluster analysis. Journal of Classification, 4(1): 33–56, 1987. URL https://doi.org/10.1007/BF01890074.\n\n\nM. T. Gastner. Spatial distributions: Density-equalizing map projections, facility location, and two-dimensional networks. 2005. URL https://hdl.handle.net/2027.42/125368.\n\n\nB. Hopkins and J. G. Skellam. A new method for determining the type of distribution of plant individuals. Annals of Botany, 18(2): 213–227, 1954. URL https://doi.org/10.1093/oxfordjournals.aob.a083391.\n\n\nP. C. Jurs and R. G. Lawson. Clustering tendency applied to chemical feature selection. Drug Information Journal, 24(4): 691–704, 1990. URL https://doi.org/10.1177/216847909002400405.\n\n\nR. G. Lawson and P. C. Jurs. New index for clustering tendency and its application to chemical problems. Journal of Chemical Information and Computer Sciences, 30(1): 36–41, 1990. URL https://doi.org/10.1021/ci00065a010.\n\n\nF. Li and L. Zhang. Comparison of point pattern analysis methods for classifying the spatial distributions of spruce-fir stands in the north-east USA. Forestry, 80(3): 337–349, 2007. URL https://doi.org/10.1093/forestry/cpm010.\n\n\nMicrosoft Corporation and S. Weston. doParallel: Foreach parallel adaptor for the ’parallel’ package. 2020. URL https://CRAN.R-project.org/package=doParallel. R package version 1.0.16.\n\n\nP. J. Northrop. donut: Nearest neighbour search with variables on a torus. 2021. URL https://CRAN.R-project.org/package=donut. R package version 1.0.2.\n\n\nA. Petrie and T. R. Willemain. An empirical study of tests for uniformity in multidimensional data. Computational Statistics & Data Analysis, 64: 253–268, 2013. URL https://doi.org/10.1016/j.csda.2013.02.013.\n\n\nR. Rotondi. Tests of randomness based on the k-NN distances for data from a bounded region. Probability in the Engineering and Informational Sciences, 7: 557–569, 1993. URL https://doi.org/10.1017/S0269964800003132.\n\n\nS. P. Smith and A. K. Jain. Testing for uniformity in multidimensional data. IEEE transactions on pattern analysis and machine intelligence, (1): 73–81, 1984. URL https://doi.org/10.1109/TPAMI.1984.4767477.\n\n\nJ. Wong. pdist: Partitioned distance function. 2013. URL https://CRAN.R-project.org/package=pdist. R package version 1.2.\n\n\nK. Wright. hopkins: Calculate hopkins statistic for clustering. 2022. URL https://CRAN.R-project.org/package=hopkins. R package version 1.0.\n\n\nM. Yang and R. Modarres. Multivariate tests of uniformity. Statistical Papers, 58: 627–639, 2017. URL https://doi.org/10.1007/s00362-015-0715-x.\n\n\nL. YiLan and Z. RuTong. clustertend: Check the clustering tendency. 2015. URL https://CRAN.R-project.org/package=clustertend. R package version 1.4.\n\n\nG. Zeng and R. C. Dubes. A comparison of tests for randomness. Pattern Recognition, 18(2): 191–198, 1985a. URL https://doi.org/10.1016/0031-3203(85)90043-3.\n\n\nG. Zeng and R. C. Dubes. A test for spatial randomness based on k-NN distances. Pattern Recognition Letters, 3(2): 85–91, 1985b. URL https://doi.org/10.1016/0167-8655(85)90013-3.\n\n\n\n\n",
    "preview": "articles/RJ-2022-055/distill-preview.png",
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {},
    "preview_width": 2880,
    "preview_height": 1920
  },
  {
    "path": "articles/RJ-2022-056/",
    "title": "Multivariate Subgaussian Stable Distributions in R",
    "description": "We introduce and showcase [mvpd](https://CRAN.R-project.org/package=mvpd) (an acronym for *m*ulti*v*ariate *p*roduct *d*istributions), a package that uses a product distribution approach to calculating multivariate subgaussian stable distribution functions. The family of multivariate subgaussian stable distributions are elliptically contoured multivariate stable distributions that contain the multivariate Cauchy and the multivariate normal distribution. These distributions can be useful in modeling data and phenomena that have heavier tails than the normal distribution (more frequent occurrence of extreme values). Application areas include log returns for stocks, signal processing for radar and sonar data, astronomy, and hunting patterns of sharks.",
    "author": [
      {
        "name": "Bruce J. Swihart",
        "url": {}
      },
      {
        "name": "John P. Nolan",
        "url": {}
      }
    ],
    "date": "2022-12-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-056.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-057/",
    "title": "Analysis of the Results of Metadynamics Simulations by metadynminer and metadynminer3d",
    "description": "Molecular simulations solve the equation of motion of molecular systems, making the 3D shapes of molecules four-dimensional by adding the time coordinate. These methods have great potential in drug discovery because they can realistically model the structures of protein molecules targeted by drugs, as well as the process of binding of a potential drug to its molecular target. However, routine application of biomolecular simulations is hampered by the very high computational costs of this method. Several methods have been developed to address this problem. One of them, metadynamics, disfavors states of the simulated system that have been already visited and thus forces the system to explore new states. Here we present the package metadynminer and metadynminer3d to analyze and visualize results from metadynamics, in particular those produced by a popular metadynamics package Plumed.",
    "author": [
      {
        "name": "Dalibor Trapl",
        "url": {}
      },
      {
        "name": "Vojtech Spiwok",
        "url": {}
      }
    ],
    "date": "2022-12-20",
    "categories": [],
    "contents": "\n1 Introduction\nMolecular simulations and their pioneers Martin Karplus, Michael Levitt, and Arieh Warshel\nhave been awarded the Nobel Prize in 2013 (Karplus 2013). Their methods, in particular the method\nof molecular dynamics simulation, computationally simulate the motions of atoms in a molecular\nsystem. A simulation starts from a molecular system defined by positions (Cartesian\ncoordinates) of the individual atoms. The heart of the method is in a calculation of forces acting\non individual atoms and their numerical integration in the spirit of\nNewtonian dynamics, i.e., the conversion of a force vector to an acceleration vector, then\nvelocity vector and, finally, to a new position of an atom. By repeating these steps,\nit is possible to reconstruct a record of atomic motions known as a trajectory.\nMolecular simulations have great potential in drug discovery. A molecule of drug\ninfluences (enhances or blocks) the function of some biomolecule in the patient’s\nbody, typically a receptor, enzyme or other protein. These molecules are called\ndrug targets. The process of design for a new drug can be significantly accelerated with knowledge\nof the 3D structure (Cartesian coordinates of atoms) of the target.\nWith such knowledge, it is possible to find a “druggable” cavity in\nthe target and a molecule that fits and favorably binds\nto this cavity to influence its function.\nStrong binding implies that the drug influences the target even in low doses, hence\ndoes not cause side effects by interacting with unwanted targets.\nExperimental determination of the 3D structures of proteins and other biomolecules\nis a very expensive and laborious process. Molecular simulations can, at least\nin principle, replace such expensive and laborious experiments by computing.\nIn principle, a molecular simulation starting from virtually any 3D shape of\na molecule would end up in energetically the most favorable shape. This is analogous\nwith water flowing from mountains to valleys and not in the opposite way.\nUnfortunately, this approach is extremely computationally expensive.\nThe integration step of a simulation must be small enough to\ncomprise the fastest motions in the molecular system. In practical simulations, it is necessary\nto use femtosecond integration steps. This means that it is necessary to carry out\nthousands of steps to simulate picoseconds, millions of steps to simulate nanoseconds, and so\nforth. In each step, it is necessary to evaluate a substantial number of interactions between atoms.\nAs a result, it is possible to routinely simulate nano- to microseconds.\nLonger simulations require special high-performance computing resources.\nProtein folding, i.e., the transition from a quasi-random to the biologically relevant\n3D structure, takes place in microseconds for very small proteins and in much longer\ntime scales for pharmaceutically interesting proteins. For this reason, prediction\nof a 3D structure by molecular simulations is limited to few small and fast folding\nproteins. For large proteins, it is currently impossible or at least far from being routine.\nSeveral methods have been developed to address this problem. Metadynamics (Laio and Parrinello 2002)\nuses artificial forces to force the system to explore states that have not been previously\nexplored in the simulation. At the beginning of the simulation, it is necessary to\nchose some parameters of the system referred to as collective variables. For example,\nnumerically expressed compactness of the protein can be used as a collective variable\nto accelerate its folding from a noncompact to a compact 3D structure.\nMetadynamics starts as a usual simulation. After a certain number of steps\n(typically 500), the values of the collective variables are calculated and from this moment\nthis state becomes slightly energetically disfavored due to the addition of\nan artificial bias potential in the shape of a Gaussian hill.\nAfter another 500 steps, another\nhill is added to the bias potential and so forth. These Gaussian hills accumulate until\nthey “flood” some energy minimum and help the system to escape this minimum\nand explore various other states\n(Figure 1). In the analogy of water floating from mountains\nto valleys, metadynamics adds “sand” to fill valleys to make water flow from\nvalleys back to mountains. This makes the simulation significantly more efficient\ncompared to a conventional simulation because the “water” does not get stuck anywhere.\nUsing the application of metadynamics, it is possible to significantly accelerate the process\nof folding. Hopefully, by the end of metadynamics we can see folded, unfolded, and many\nother states of the protein. However, the interpretation of the trajectory is not\nstraightforward. In standard molecular dynamics simulation (without metadynamics),\nthe state which is the most populated is the most populated in reality. This is\nnot true anymore with metadynamics.\nPackages metadynminer and metadynminer3d use the results of metadynamics simulations\nto calculate the free energy surface of the molecular system.\nThe most favored states (states most populated in reality) correspond to minima\non the free energy surface. The state with the lowest free energy is the most\npopulated state in the reality, i.e., the folded 3D structure of the protein.\nAs an example to illustrate metadynamics and our package, we use\nan ultrasimple molecule of “alanine dipeptide”\n(Figure 1). This molecule can be viewed as a “protein” with\njust one amino acid residue (real proteins have hundreds or thousands of amino acid\nresidues). As a collective variable it is possible to use an angle \\(\\phi\\)\ndefined by four atoms. Biasing of this collective variable accelerates a slow\nrotation around the corresponding bond. Figure 1 shows the\nfree energy surface of alanine dipeptide as the black thick line. It is not known\nbefore the simulation. The simulation starts from the state B. After 500 simulation\nsteps, the hill is added (the hill is depicted as the red line, the flooding potential (“sand”)\nat the top, the free energy surface with added flooding potential at the bottom).\nThe sum of 1, 10, 100, 200, 500, and 700 hills are depicted as red to blue lines.\nAt the end of simulation the free energy surface is relatively well flattened\n(blue line in Fig. 1 bottom). Therefore, the free energy\nsurface can be estimated as a negative imprint of added “sand”:\n\\[\\begin{equation}\nG(s) = -kT \\log(P(s)) = -V(s) = \\sum_i w_i \\exp(-(s-S_i)^2/{2 \\sigma^2}),\n\\tag{1}\n\\end{equation}\\]\nwhere \\(G\\), \\(V\\), and \\(P\\) are free energy, metadynamics bias (flooding)\npotential, and probability,\nrespectively, of a state with a collective variable \\(s\\), \\(k\\) is Boltzmann constant,\n\\(T\\) is temperature in Kelvins, \\(w_i\\) is height, \\(S_i\\) is position and \\(\\sigma_i\\)\nis width of each hill. The equation can be easily generalized for two or more\ncollective variables.\n\n\n\nFigure 1: Metadynamics simulation of alanine dipeptide. Dihedral angle \\(\\phi\\) was used as the collective variable. The top part shows molecular structures of three free energy minima (stable structures) differing in the value of \\(\\phi\\). According to metadynamics prediction, A is the global minimum (free energy 0 kJ/mol) and B and C are local minima (1.5 and 6.3 kJ/mol, respectively). According to Equation 1, this corresponds to probabilities 0.61, 0.34, and 0.05 for A, B, and C, respectively. The middle part shows the bias potential (scaled by \\((T+\\Delta T)/\\Delta T\\)) after addition of 1, 10, 100, 200, 500, and 700 hills (colors from red to blue). The bottom part shows the accurate free energy surface calculated by metadynamics with 30,000 hills (black) flooded by 1, 10, 100, 200, 500, and 700 hills (colors from red to blue). The figure was generated by metadynminer except for molecular structures and final assembly.\n\n\n\nThe original version of metadynamics was developed with constant heights of Gaussian hills.\nLater, a so-called well-tempered metadynamics was developed (Barducci et al. 2007), which uses\ndecreasing hill heights to improve the accuracy of the results. This requires modification\nof the equation:\n\\[\\begin{equation}\nG(s) = -kT \\log(P(s)) = - \\frac{T + \\Delta T}{\\Delta T} V(s) =\n- \\frac{T + \\Delta T}{\\Delta T} \\sum_i w_i \\exp(-(s-S_i)^2/{2 \\sigma^2}),\n\\tag{2}\n\\end{equation}\\]\nwhere \\(\\Delta T\\) an input parameter with the dimension of temperature\n(zero for unbiased simulation and infinity for the original metadynamics with constant\nhill heights). Nowadays, the vast majority of metadynamics applications use the\nwell-tempered metadynamics algorithm for better convergence towards an accurate\nfree energy surface prediction.\nThere are numerous packages for molecular simulations such as Amber (Weiner and Kollman 1981),\nGromacs (Abraham et al. 2015), Gromos (Christen et al. 2005), NAMD (Phillips et al. 2020),\nCHARMM (Brooks et al. 2009), Acemd (Harvey et al. 2009), and others.\nThese packages are primarily developed for basic unbiased simulations with no or very\nlimited support of metadynamics. Plumed software (Tribello et al. 2014) has been developed to\nintroduce metadynamics into various simulation programs. Since its introduction,\nPlumed articles have been cited in more than thousand papers from drug design,\nmolecular biology, material sciences, and other fields. The R package metadynminer\nwas developed for analysis and visualization of the results from Plumed. With\na simple file conversion script, it can be used also with other simulation\nprograms that support metadynamics.\n2 Example of usage\nThe package metadynminer will be presented on a bias potential from a\n30 ns (30,000 hills) simulation of alanine dipeptide (Figure 1).\nTwo rotatable bonds of the molecule, referred to as \\(\\phi\\) and \\(\\psi\\),\nwere used as collective variables. This is basically an expansion of the free energy\nsurface in Figure 1 to two dimensions.\nHills from simulations with two collective variables (\\(\\phi\\) and \\(\\psi\\)) and with\none collective variable (\\(\\phi\\)) are provided in metadynminer as\nacealanme and acealanme1d, respectively. metadynminer3d was developed\nfor analysis of metadynamics with three collective variables. It contains a sample\ndata acealanmed3, with collective variables \\(\\phi\\), \\(\\psi\\) and \\(\\omega\\).\nWe decided to distribute metadynminer and metadynminer3d separately, because of\nthe use of different visualization tools and to keep the size of packages low.\nMetadynamics simulations with 1-3 collective variables comprise almost all metadynamics\napplications nowadays (not considering special metadynamics variants).\nHills file generated by Plumed package (filename HILLS) can be loaded to R by\nthe function read.hills:\n  hillsfile <- read.hills(\"HILLS.txt\", per=c(TRUE, TRUE))\nThe parameter per indicates periodicity of the collective variable (dihedral angles\nare periodic, i.e., \\(+\\pi \\simeq -\\pi\\)).\nFor the simulation described above, hillsfile is identical\nto acealanme already contained in metadynminer as\nan example.\nTyping the name hillsfile will return its dimensionality (the number of collective variables) and\nthe number of hills. A hills object can be plotted:\n  plot(hillsfile, xlab=\"phi\", ylab=\"psi\", pch=19, cex=0.5, col=gray(0, 0.1))\nFor metadynamics with one collective variable, it plots its evolution. For metadynamics\nwith two or three collective variables, it plots a scatter plot of collective variables\nnumber 1 vs. 2 or 1 vs. 2 vs. 3, respectively (Figure 2).\n\n\n\nFigure 2: Scatter plot of hills position. Each point in the plot represents a single hill in the space of collective variable coordinates. This helps to assess which states of the system were sampled.\n\n\n\nIn well-tempered metadynamics it may be interesting to see the evolution\nof hill heights (\\(w_i\\) in Equation (2)).\nThis can be plotted (Figure 3) by typing:\n  plotheights(hillsfile)\n\n\n\nFigure 3: Evolution of heights of hills in metadynamics plotted by function plotheights. In well-tempered metadynamics, heights of hills decrease with the progress of flooding of free energy minima. The evolution of heights of hills may help to assess the completeness of flooding.\n\n\n\nAddition operation is available for hillsfile object. For example, multiple hills\nfiles can be concatenated.\nNext, the user can sum negative values of all hills to make the free energy surface\nestimate by typing:\n  fesurface <- fes(hillsfile)\nHills files from well-tempered metadynamics are prescaled by \\((\\Delta T + T)/\\Delta T\\)\nwhen printed by Plumed, so no special action is required in metadynminer.\nThe function fes uses the Bias Sum algorithm (Hošek and Spiwok 2016). This function is\nfast because instead of evaluation of Gaussian function for every hill, it uses\na precomputed Gaussian hill that is relocated to hill centers.\nIt is also fast because it was implemented in C++ via Rcpp. Because of approximations\nused in the function fes, this function should be used for visualization purposes.\nFor detailed analysis of a free energy surface, we advise to use a slow but accurate\nfes2 function. This function explicitly evaluates Gaussian function for every\nhill. It can be also used for (rarely used) metadynamics with variable hill widths.\nTyping the name of the variable with a free energy surface returns its dimensionality,\nnumber of points, and free energy maximum and minimum. The same is returned by\nsummary function. It is possible\nto add and subtract two free energy surfaces with the same number of grid points.\nThe functions min and max can be used as well to calculate minimum or\nmaximum. It is also possible to multiply or divide the free\nenergy surface by a constant (for example, to convert kJ to kcal and vice versa).\nFree energy surface can be plotted (Figure 4) by typing:\n  plot(fesurface, xlab=\"phi\", ylab=\"psi\")\n\n\n\nFigure 4: Free energy surface. Minima (blue colors) represent stable states with high abundance, whereas regions with high free energy correspond to low abundance states.\n\n\n\nIn metadynamics simulation, it is important to find free energy minima. The global minimum\nrefers to the most favored state of the system (i.e., the state with the highest probability).\nOther local minima correspond to metastable states. The user can find free energy\nminima by typing:\n  minima <- fesminima(fesurface)\nThis function locates minima using a simple algorithm. The free energy surface is separated into\n8, 8x8, or 8x8x8 bins (for 1D, 2D, or 3D surface, respectively). The minimum in each bin is\nlocated. Next, the program tests whether the minimum is a local minimum of the whole free\nenergy surface. The number of grid points can be changed by ngrid parameter.\nTyping the name of the minima variable will return the table of minima (denoted as\nA, B, C, … in the order of their free energies), their collective variables, and free\nenergy values.\nIn addition, the function summary provides populations of each\nminimum calculated as:\n\\[\\begin{equation}\nP_{i,rel} = \\exp(-G_i/kT),\n\\end{equation}\\]\n\\[\\begin{equation}\nP_i = P_{i,rel} / \\sum (P_{j,rel}).\n\\end{equation}\\]\n\n  letter CV1bin CV2bin        CV1        CV2 free_energy relative_pop\n1      A     78    236 -1.2443171  2.6487938   -97.26095 8.614856e+16\n2      B     28    240 -2.4763142  2.7473536   -95.63038 4.480527e+16\n3      C     74    118 -1.3428769 -0.2587194   -94.73163 3.124915e+16\n4      D    166    151  0.9239978  0.5543987   -91.66626 9.143024e+15\n5      E    170    251  1.0225576  3.0183929   -84.37799 4.920882e+14\n         pop\n1 50.1335658\n2 26.0741201\n3 18.1852268\n4  5.3207200\n5  0.2863674\n\nUsing the plot function on a fesminima output provides the same plot as for fes output\nwith additional letters indicating minima (Figure 5).\n\n\n\nFigure 5: Free energy surface with indicated free energy minima A-E. The minimum A is the most abundant state, minima B-E are metastable states.\n\n\n\nIt is essential to evaluate the accuracy of metadynamics and to decide\nwhen the simulation is accurate enough so that it can be stopped. For this\npurpose, it is useful to look at the evolution of relative free energies.\nThe relative free energies (for example, the free energy difference of minima\nA and C) evolve rapidly at the beginning of the simulation, and with the progress\nof the simulation, their difference is converging towards the real free energy\ndifference. Function feprof calculates the evolution of free energy differences\nfrom the global minimum (global at the end of the simulation).\nIt can be used as:\n  prof <- feprof(minima)\nFunction summary provides minima and maxima of these free energy differences. The evolution\ncan be plotted (Figure 6) by typing:\n  plot(prof)\n\n\n\nFigure 6: Evolution of free energy differences. The free energy differences of minima B-E (relative to the global minimum A) converge to the exact free energy differences with the progress of the simulation. This plot helps to assess the accuracy of the predicted free energy differences.\n\n\n\nBeside minima, another important points on the free energy surface are transition\nstates. Change of the molecular structure from one minimum to\nanother takes place via a path with the lowest energy demand.\nThe state with the highest energy along this path is called\nthe transition state. Free energy difference between the initial\nand transition state can be used to predict kinetics (rates)\nof the studied molecular process. Furthermore, identification\nof transition states is important in drug design because\ncompounds designed to mimic the transition states of an enzymatic\nreaction are often potent enzymes inhibitors and thus good drug\ncandidates (Itzstein et al. 1993).\nIn metadynminer, such path can be identified by Nudged\nElastic Band method (Henkelman and Jónsson 2000). Briefly, this method plots\na line between selected minima as an initial approximation\nof the transition path. Next, this line is curved so that the\ncorresponding physical process becomes feasible. This function\ncan be applied on, for example, minima A and D as:\n  nebAD <- neb(minima, min1=\"A\", min2=\"D\")\nThe result can be analyzed by summary (to provide kinetics\nof the A to D and D to A change predicted by Eyring\nequation (Eyring 1935)), by plot (to plot the free energy\nprofile of the molecular process) and by pointsonfes\nor linesonfes (to plot the path on top of the free energy\nsurface). The last example can be invoked by:\n  plot(minima, xlab=\"phi\", ylab=\"psi\")\n  linesonfes(nebAD, lwd=4)\nThe resulting plot is depicted in Figure 7.\n\n\n\nFigure 7: Path of transition from A to D projected onto free energy surface. This represents the most favorable path between these minima. It can be used to identify the transition state (the point with the highest energy on the path) and the rate of the transition.\n\n\n\nLet us also briefly present metadynminer3d. This package uses\npackages rgl and misc3d to plot the free energy surface as\nan interactive (mouse rotatable) isosurface in the space of three collective variables (see\nFigure 8). metadynminer3d can produce interactive WebGL visualizations\nusing writeWebGL command from the rgl package.\n\n\n\nFigure 8: 3D free energy surface depicted as isosurface at -30 kJ/mol. It is analogous to the 2D plot in Figure 4, but with three collective variables.\n\n\n\nmetadynminer and metadynminer3d were developed to be highly flexible.\nThis flexibility can be demonstrated on two examples. First, it is\nuseful to visualize the progress of metadynamics as a video sequence showing\nthe evolution of the free energy surface. The code to generate\ncorresponding images can be written in metadynminer as:\ntfes <- fes(hillsfile, tmax=100)\npng(\"snap%04d.png\")\nplot(tfes, zlim=c(-200,0))\nfor(i in 1:299) {\n  tfes <- tfes+fes(acealanme, imin=100*i+1, imax=100*(i+1))\n  plot(tfes, zlim=c(-200,0), xlab=\"phi\", ylab=\"psi\")\n}\ndev.off()\nThis generates a series of images that can be concatenated by external\nsoftware to make a video file.\nThe second example demonstrates a more complicated analysis of the\nresults from metadynamics. Functions fes and fes2 use equations\n(1) and (2) to predict the free energy surface. A limitation\nof this approach is that the prediction of the free energy surface is\nbased only on the positions of hills. The evolution of collective variables between\nhills depositions is not used. As an alternative, it is possible to use\nreweighting (Torrie and Valleau 1977),(Tiwary and Parrinello 2015). This approach calculates the free\nenergy surface from hills positions as well as from evolution of collective\nvariables. Briefly, regions of the free energy surface that are sampled\ndespite being disfavored by high flooding potential have higher weights than those\ndisfavored by low flooding potential. This approach, in general, is more accurate.\nA file containing values of collective variables\nand the bias potential at different snapshots of the simulation\n(default filename COLVAR) is required.\nReweighting can be done using the code:\nbf <- 15\nkT <- 8.314*300/1000\nnpoints <- 50\nmaxfes <- 75\noutfes <- 0*fes(hillsfile, npoints=npoints)\nstep <- 1:50*length(hillsfile$time)/50\ns1 <- sapply(step, FUN=function(x) {\n          sum(exp(-fes(hillsfile, imax=x)$fes/kT))\n})\ns2 <- sapply(step, FUN=function(x) {\n          sum(exp(-fes(hillsfile, imax=x)$fes/kT/bf))\n})\nebetac <- s1/s2\ncvs <- read.table(\"COLVAR.txt\")\nnsamples <- nrow(cvs)\nxlim <- c(-pi,pi)\nylim <- c(-pi,pi)\nstep <- (1:nsamples-1)*50/nsamples+1\nix <- npoints*(cvs[,2]-xlim[1])/(xlim[2]-xlim[1])+1\niy <- npoints*(cvs[,3]-ylim[1])/(ylim[2]-ylim[1])+1\nfor(i in 1:nsamples) {\n  outfes$fes[ix[i],iy[i]] <- outfes$fes[ix[i],iy[i]] + exp(cvs[i,4]/kT)/\nebetac[step[i]]\n}\noutfes$fes <- -kT*log(outfes$fes)\noutfes <- outfes - min(outfes)\noutfes$fes[outfes$fes>maxfes] <- maxfes\nplot(outfes, xlab=\"phi\", ylab=\"psi\")\nwhere bf is the bias factor (\\((T+\\Delta T)/T\\) in Equation (2)),\nkT is temperature in Kelvins multiplied by Boltzmann constant,\nnpoints is the granularity of the resulting free energy surface\nand maxfes is the maximal possible free energy (to avoid problems\nwith infinite free energy in unsampled regions).\nFirst, outfes is introduced as a zero\nfree energy surface. First, the correction ebetac for the\nevolution of flooding potential developed by Tiwary and Parrinello\n(Tiwary and Parrinello 2015) is calculated.\nNext, a file with the evolution of collective variables\nCOLVAR (from the same simulation used to generate acealanme\ndataset, available at https://www.metadynamics.cz/metadynminer/data/)\nis read. The second loop evaluates the sampling weighted by the factor\n\\(\\exp (V(s)/kT)\\) divided by ebetac to correct for the evolution of\nthe bias potential (Tiwary and Parrinello 2015). Finally, probabilities are converted\nto the free energy surface and plotted (Figure 9).\n\n\n\nFigure 9: Free energy surface calculated by reweighting by Tiwary and Parrinello. This free energy surface was calculated by combining the information on time spent in different regions of the free energy surface and on the potential disfavoring these regions. This approach is in general more accurate than the summation of hills used to generate Figures 4-8.\n\n\n\n3 Simulation details\nAll simulations were done using Gromacs 2016.4 (Abraham et al. 2015) patched by\nPlumed 2.4b (Tribello et al. 2014). Alanine dipeptide was modeled using\nAmber99SB-ILDN force field (Lindorff-Larsen et al. 2010). The simulated system\ncontained alanine dipeptide and 874 TIP3P (Jorgensen et al. 1983) water\nmolecules. The temperature was kept constant at 300 K using Bussi\nthermostat (Bussi et al. 2007). Metadynamics hills of height\n1 kJ/mol (bias factor 10) and widths 0.3 rad were added\nevery 1 ps. Two simulations were done, one with\none dihedral angle \\(\\phi\\) (dataset acealanme1d),\ntwo dihedral angles \\(\\phi\\) and \\(\\psi\\) (dataset acealanme),\nor with three angle \\(\\phi\\), \\(\\phi\\) and \\(\\omega\\) (dataset\nacealanme3d in metadynminer3d).\nSupporting material is available at\nhttps://www.metadynamics.cz/metadynminer/data/\nor in Plumed nest (PLUMED consortium 2019) at\nhttps://www.plumed-nest.org/eggs/20/023/.\n4 Summary\nThe package metadynminer and metadynminer3d provides fast algorithm\nBias Sum (Hošek and Spiwok 2016) for calculation of free energy surfaces from\nmetadynamics. This algorithm is available in our on-line tool\nMetadynView (http://metadyn.vscht.cz), but this tools is\nintended for routine checking of the progress of metadynamics\nsimulations rather than for in-depth analysis and visualization.\nBesides this, users of metadynamics use built-in functions in\nPlumed or various in-lab scripts. Such scripts do not provide\nappropriate flexibility in analysis and visualization.\nThe biggest advantage we see is in the fact that\nmetadynminer can produce publication quality figures\nvia graphics output functions in R. As shown above,\nusing a simple for loop it is possible to plot\nindividual snapshots and concatenate them outside R\nto make a movie. metadynminer3d provides the possibility\nto produce interactive 3D web models by WebGL technology.\nWe also tested 3D printing of a free energy surface that\nis very easy using metadynminer and rayshader.\nVarious tips and tricks can be found\non the website of the project\n(https://www.metadynamics.cz/metadynminer/).\nAnother advantage we see is in the reporting of results. Reproducibility\nis a big issue in science, including molecular simulations.\nPackages like knitr or rmarkdown ca be\nused to record all steps of data analysis pipeline to compile\na report for routine and reproducible use of metadynamics.\n5 Acknowledgement\nThis project was supported by Ministry of Education, Youth and\nSports of the Czech Republic - COST action OpenMultiMed\n(CA15120, LTC18074) for development and Czech National\nInfrastructure for Biological data (ELIXIR CZ, LM2015047,\nLM2018131)\nfor future sustainability.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-057.zip\nCRAN packages used\nmetadynminer, metadynminer3d, Rcpp, rgl, misc3d, rayshader, knitr, rmarkdown\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NumericalMathematics, ReproducibleResearch, SpatioTemporal\n\n\nJ. M. Abraham, T. Murtola, R. Schulz, S. Páll, J. C. Smith, B. Hess and L. Erik. GROMACS: High performance molecular simulations through multi-level parallelism from laptops to supercomputers. SoftwareX, 1–2: 19–25, 2015. URL https://doi.org/10.1016/j.softx.2015.06.001.\n\n\nA. Barducci, G. Bussi and M. Parrinello. Well-tempered metadynamics: A smoothly converging and tunable free-energy method. Physical Review Letters, 100(2): 020603, 2007. URL https://doi.org/10.1103/PhysRevLett.100.020603.\n\n\nB. R. Brooks, C. L. Brooks, A. D. Mackerell, L. Nilsson, R. J. Petrella, B. Roux, Y. Won, G. Archontis, C. Bartels, S. Boresch, et al. CHARMM: The biomolecular simulation program. Journal of Computational Chemistry, 30(10): 1545–1614, 2009. URL https://doi.org/10.1002/jcc.21287.\n\n\nG. Bussi, D. Donadio and M. Parrinello. Canonical sampling through velocity rescaling. Journal of Chemical Physics, 1(126): 014101, 2007. URL https://doi.org/10.1063/1.2408420.\n\n\nM. Christen, P. H. Hünenberger, D. Bakowies, R. Baron, R. Bürgi, D. P. Geerke, T. N. Heinz, M. A. Kastenholz, V. Kräutler, C. Oostenbrink, et al. The GROMOS software for biomolecular simulation: GROMOS05. Journal of Computational Chemistry, 26(16): 1719–1751, 2005. URL https://doi.org/10.1002/jcc.20303.\n\n\nH. Eyring. The activated complex in chemical reactions. The Journal of Chemical Physics, 3(2): 1935. URL https://doi.org/10.1063/1.1749604.\n\n\nM. J. Harvey, G. Giupponi and G. D. Fabritiis. ACEMD: Accelerating biomolecular dynamics in the microsecond time scale. Journal of Chemical Theory and Computation, 5(6): 1632–1639, 2009. URL https://doi.org/10.1021/ct9000685.\n\n\nG. Henkelman and H. Jónsson. Improved tangent estimate in the nudged elastic band method for finding minimum energy paths and saddle points. The Journal of Chemical Physics, 113(22): 9978–9985, 2000. URL https://doi.org/10.1063/1.1323224.\n\n\nP. Hošek and V. Spiwok. Metadyn view: Fast web-based viewer of free energy surfaces calculated, by metadynamics. Computer Physics Communications, 198: 222–229, 2016. URL https://doi.org/10.1016/j.cpc.2015.08.037.\n\n\nM. von Itzstein, W.-Y. Wu, G. B. Kok, M. S. Pegg, J. C. Dyasson, B. Jin, T. V. Phan, M. L. Smythe, H. F. White, S. W. Oliver, et al. Rational design of potent sialidase-based inhibitors of influenza virus replication. Nature, 363: 418–423, 1993. URL https://doi.org/10.1038/363418a0.\n\n\nW. L. Jorgensen, J. Chandrasekhar, J. D. Madura, R. W. Impey and M. L. Klein. Comparison of simple potential functions for simulating liquid water. The Journal of Chemical Physics, 79(2): 926, 1983. URL https://doi.org/10.1063/1.445869.\n\n\nM. Karplus. Nobel lecture. 2013. URL https://www.nobelprize.org/prizes/chemistry/2013/karplus/lecture/.\n\n\nA. Laio and M. Parrinello. Escaping free-energy minima. Proceedings of the National Academy of Sciences of the United States of America, 20(99): 12562–12566, 2002. URL https://doi.org/10.1073/pnas.202427399.\n\n\nK. Lindorff-Larsen, S. Piana, K. Palmo, P. Maragakis, J. L. Klepeis, R. O. Dror and D. E. Shaw. Improved side-chain torsion potentials for the amber ff99SB protein force field. Proteins: Structure, Function, and Bioinformatics, 78(8): 1950–1958, 2010. URL https://doi.org/10.1002/prot.22711.\n\n\nJ. C. Phillips, D. J. Hardy, J. D. C. Maia, J. E. Stone, J. V. Ribeiro, R. C. Bernardi, R. Buch, G. Fiorin, J. Hénin, W. Jiang, et al. Scalable molecular dynamics on CPU and GPU architectures with NAMD. Journal of Chemical Physics, 153(4): 044130, 2020. URL https://doi.org/10.1063/5.0014475.\n\n\nPLUMED consortium. Promoting transparency and reproducibility in enhanced molecular simulations. Nature Methods, 16: 670–673, 2019. URL https://doi.org/10.1038/s41592-019-0506-8.\n\n\nP. Tiwary and M. Parrinello. A time-independent free energy estimator for metadynamics. The Journal of Physical Chemistry B, 119(3): 736–742, 2015. URL https://doi.org/10.1021/jp504920s.\n\n\nG. M. Torrie and J. P. Valleau. Nonphysical sampling distributions in monte carlo free-energy estimation: Umbrella sampling. Journal of Computational Physics, 23: 187–199, 1977. URL https://doi.org/10.1016/0021-9991(77)90121-8.\n\n\nG. A. Tribello, M. Bonomi, D. Branduardi, C. Camilloni and G. Bussi. PLUMED 2: New feathers for an old bird. Computer Physics Communications, 185(2): 604–613, 2014. URL https://doi.org/10.1016/j.cpc.2013.09.018.\n\n\nP. K. Weiner and P. A. Kollman. AMBER: Assisted model building with energy refinement. A general program for modeling molecules and their interactions. Journal of Computational Chemistry, 2(3): 287–303, 1981. URL https://doi.org/10.1002/jcc.540020311.\n\n\n\n\n",
    "preview": "articles/RJ-2022-057/distill-preview.png",
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {},
    "preview_width": 3780,
    "preview_height": 4016
  },
  {
    "path": "articles/RJ-2022-058/",
    "title": "CIMTx: An R Package for Causal Inference with Multiple Treatments using Observational Data",
    "description": "[CIMTx](https://CRAN.R-project.org/package=CIMTx) provides efficient and unified functions to implement modern methods for causal inferences with multiple treatments using observational data with a focus on binary outcomes. The methods include regression adjustment, inverse probability of treatment weighting, Bayesian additive regression trees, regression adjustment with multivariate spline of the generalized propensity score, vector matching and targeted maximum likelihood estimation. In addition, [CIMTx](https://CRAN.R-project.org/package=CIMTx) illustrates ways in which users can simulate data adhering to the complex data structures in the multiple treatment setting. Furthermore, the [CIMTx](https://CRAN.R-project.org/package=CIMTx) package offers a unique set of features to address the key causal assumptions: positivity and ignorability. For the positivity assumption, [CIMTx](https://CRAN.R-project.org/package=CIMTx) demonstrates techniques to identify the common support region for retaining inferential units using inverse probability of treatment weighting, Bayesian additive regression trees and vector matching. To handle the ignorability assumption, [CIMTx](https://CRAN.R-project.org/package=CIMTx) provides a flexible Monte Carlo sensitivity analysis approach to evaluate how causal conclusions would be altered in response to different magnitude of departure from ignorable treatment assignment.",
    "author": [
      {
        "name": "Liangyuan Hu",
        "url": {}
      },
      {
        "name": "Jiayi Ji",
        "url": {}
      }
    ],
    "date": "2022-12-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-058.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-041/",
    "title": "Introducing fastpos: A Fast R Implementation to Find the Critical Point of Stability for a Correlation",
    "description": "The R package fastpos provides a fast algorithm to estimate the required sample size for a Pearson correlation to *stabilize* [@schonbrodt2013]. The stability approach is an innovative alternative to other means of sample size planning, such as power analysis. Although the approach is young, it has already attracted much interest in the research community. Still, to date, there exists no easy way to use the stability approach because there is no analytical solution and a simulation approach is computationally expensive with a quadratic time complexity. The presented package overcomes this limitation by speeding up the calculation of correlations and achieving linear time complexity. For typical parameters, the theoretical speedup is around a factor of 250, which was empirically confirmed in a comparison with the original implementation `corEvol`. This speedup allows practitioners to use the stability approach to plan for sample size and theoreticians to further explore the method.",
    "author": [
      {
        "name": "Johannes Titz",
        "url": "https://johannestitz.com"
      }
    ],
    "date": "2022-12-12",
    "categories": [],
    "contents": "\n1 Sample size planning with the stability approach\nSample size planning is one of the most crucial steps before conducting an empirical study. The approach-avoidance conflict lies in the desire for reliable conclusions, but the unwillingness to spend resources for large samples. To balance benefit and cost there exist three more or less established paths: power analysis (e.g. Cohen 1988), accuracy in parameter estimation [AIPE; e.g. Maxwell et al. (2008)] and interval based accuracy methods (Algina and Olejnik 2003). Recently, a fourth way was introduced: stability (Schönbrodt and Perugini 2013). The general idea of this approach is to determine the sample size at which a certain percentage of studies will fall into an priori specified interval and stay in this interval if the sample size is increased further. For instance, if the population correlation is 0.5, one can define the limits to be 0.4 and 0.6. Given these constraints, what sample size is required to guarantee, with a certain probability (e.g. 90%), that the correlation coefficient will not drop below 0.4 or rise above 0.6 if more participants are added. This sample size is also referred to as the critical point of stability for the specific parameters. The stability approach is promising because it (1) focuses on the effect size instead of significance and (2) is fairly intuitive. Indeed, the interest in the method is growing, evident in more than 1500 citations of the original publication. But a proper software package for the stability approach is still missing.\nWhen the concept was introduced, the authors presented a collection of R scripts (corEvol, available at a github repository: https://github.com/nicebread/corEvol) to derive a sample size table for certain parameters. This implementation is too slow to plan the sample size for an individual study as it can take hours to get reliable results. In this article a faster implementation of the stability approach is introduced available in the R package fastpos with the function find_critical_pos.\n2 Model and implementations\nThe general model can be shortly described as follows: Define a population correlation \\(\\rho\\), the corridor of stability with lower limit \\(l\\) and upper limit \\(u\\) and a confidence \\(1-\\alpha\\). Now, pairs of values from a bivariate normal distribution with correlation \\(\\rho\\) are drawn. In a first step \\(n_\\mathrm{min}\\) pairs are drawn, to which, repeatedly, one more pair is added so that the sample size \\(n\\) is sequentially increased by 1. For every \\(n\\) the correlation \\(r_n\\) is calculated. The point of stability \\(n_\\mathrm{pos}\\) can be described as:\n\\[\\begin{equation}\nn_\\mathrm{pos}=\\min{\\left\\{ n\\in \\mathbb{N}|l\\leq r_{m}\\leq u,\\forall m\\geq n\\right\\}} \\tag{1}\n\\end{equation}\\]\nMeaning that the corridor of stability is not left again after the point of stability has been crossed and that the corridor of stability was just entered at the point of stability. Note that \\(n_\\mathrm{pos}\\) is a random variable that has to be evaluated with respect to the normal bivariate distribution. The critical point of stability is the quantile \\(1-\\alpha\\) of the probability density function of \\(n_\\mathrm{pos}\\). It is possible to calculate the transition probabilities of entering, leaving or staying in the corridor of stability for two neighboring sample sizes \\(n\\) and \\(n+1\\). But, so far, no analytical solution to calculate the critical point of stability has been proposed.\nInstead, Schönbrodt and Perugini (2013) set up a Monte Carlo simulation to produce a sample size table for some parameter combinations. In a simulation a maximum sample size \\(n_\\mathrm{max}\\) has to be chosen. Then, for every \\(n\\) from \\(n_\\mathrm{min}\\) to \\(n_\\mathrm{max}\\) the correlation can be calculated. The point of stability for one simulation study can again be described by the above condition. From many of such studies, the critical point of stability can be estimated for the desired confidence.\nIn the original implementation, the correlations were calculated from scratch for each \\(n\\), using the function cor from stats. This is slow as several millions of correlations have to be calculated for a reliable estimate. The correlations at \\(n\\) and \\(n+1\\) only differ by one pair of values, which can be exploited for speed. Take the sum formula for the correlation coefficient at a specific sample size \\(n\\):\n\\[\\begin{equation}\nr_{n} = \\frac{n\\sum_{i=1}^{n} x_i y_i - \\sum_{i=1}^{n} x_i \\sum_{i=1}^{n} y_i}\n{\\sqrt{n\\sum_{i=1}^{n} x_i^2-\\left(\\sum_{i=1}^{n} x_i\\right)^2}\n\\sqrt{n\\sum_{i=1}^{n} y_i^2-\\left(\\sum_{i=1}^{n} y_i\\right)^2}}\n  \\tag{2}\n\\end{equation}\\]\nSeveral sums are calculated, each consisting of adding up \\(n\\) terms. In corEvol this is done for every sample size from the minimum to the maximum one. Thus, the total number of added terms for one sum is:\n\\[\\begin{equation}\n  \\sum_{n=n_\\mathrm{min}}^{n_\\mathrm{max}}n = \\sum_{n=1}^{n_\\mathrm{max}}n - \\sum_{n=1}^{n_\\mathrm{min}-1}n = \\frac{n_\\mathrm{max}\\left( n_\\mathrm{max}+1\\right)}{2} - \\frac{\\left( n_\\mathrm{min}-1\\right) \\left( n_\\mathrm{min}-1+1\\right)}{2}\n  \\tag{3}\n\\end{equation}\\]\nThe variable \\(n_\\mathrm{min}\\) can be ignored as it is usually a small value and could even be set to 2. Furthermore, the number of sums in the correlation formula will be the same for every algorithm and is a constant. Dropping constant factors and lower order terms, the time complexity of the described algorithm is \\(\\mathcal{O}(n_\\mathrm{max}^2)\\).\nIn contrast, fastpos calculates the correlation for the maximum sample size first. This requires to add \\(n_\\mathrm{max}\\) numbers for one sum. Then it subtracts one value from this sum to find the correlation for the sample size \\(n_\\mathrm{max}-1\\), which happens repeatedly until the minimum sample size is reached (or the corridor is left). In the worst case, the total number of terms for one sum amounts to:\n\\[\\begin{equation}\nn_\\mathrm{max}+n_\\mathrm{max}-n_\\mathrm{min}\n  \\tag{4}\n\\end{equation}\\]\nAgain, dropping constant factors and lower order terms, the time complexity of this algorithm is \\(\\mathcal{O}(n_\\mathrm{max})\\). The ratio between the two approaches is:\n\\[\\begin{equation}\n  \\frac{n_\\mathrm{max}\\left(n_\\mathrm{max}+1\\right) -\\left(n_\\mathrm{min}-1\\right)n_\\mathrm{min}}{4n_\\mathrm{max}-2n_\\mathrm{min}}\n  \\tag{5}\n\\end{equation}\\]\nFor the typically used \\(n_\\mathrm{max}\\) of 1,000 and \\(n_\\mathrm{min}\\) of 20, a speedup of about 250 can be expected. From a theoretical perspective it is also interesting to study the stability approach with larger values of \\(n_\\mathrm{max}\\), for which the difference becomes even more pronounced.\nThe theoretical speedup is only an approximation for several reasons. First, one can stop the algorithm when the corridor is left the first time, which is done in fastpos but not in corEvol. Second, the main function of fastpos was written in C++ (via Rcpp, Eddelbuettel et al. 2022), which is much faster than normal R. At the same time, the algorithms contain many more steps than just calculating correlations. For instance, setting up the population with a specific \\(\\rho\\) takes some time since it usually consists of a million value pairs. The interface functions to setup the simulations also play a role, especially when the algorithm itself is very fast. Thus, it is necessary to study the speed benefit empirically. But before running a benchmark it will be useful to show (1) how to use fastpos in general and (2) that it produces the same estimates as corEvol.\n3 How to use fastpos\nFor a simple illustration, imagine you plan an empirical study and believe the population correlation is 0.6. You would be happy to find a stable correlation between 0.5 and 0.7 with a probability of 80%. What this means is that there is an 80% chance of finding a correlation between 0.5 and 0.7 and by adding more participants this corridor is not left again. In fastpos you can run:\n\n\nlibrary(fastpos)\nset.seed(20200219)\nfind_critical_pos(rho = 0.6, precision_absolute = 0.1, confidence_levels = .8,\n                  sample_size_min = 20, sample_size_max = 1e3, n_studies = 1e4)\n\n  rho_pop pos.80% sample_size_min sample_size_max lower_limit\n1     0.6     104              20            1000         0.5\n  upper_limit n_studies n_not_breached precision_absolute\n1         0.7     10000              0                0.1\n  precision_relative\n1                 NA\n\nThis loads the package, sets a seed for reproducibility, and runs the simulation with default parameters (except for the ones specifically set). A progress bar is displayed if run in interactive mode. The result is a critical point of stability of 104.\nThe main function of the package find_critical_pos will usually suffice for most use cases. Its parameters are documented in detail in the package. The population correlation (rho) and the number of simulation studies (n_studies) is self-explanatory. The chosen precision (precision_absolute) of 0.1 (i.e. the half-width) will result in the desired corridor between 0.5 and 0.7. There is also a convenience argument to set the precision as a relative value, precision_relative, which will override precision_absolute. For instance, precision_relative = 0.1 produces an interval of \\(\\rho \\pm \\rho \\cdot 0.1\\). Alternatively, one can also provide the lower and upper limit of the corridor directly via lower_limit and upper_limit. This is especially useful if the corridor is not symmetric. Notable, most parameters can also take vectors so it is possible to run multiple simulations for different rho values (and corresponding other parameters) at once.\nThe parameter confidence_levels defines the quantile corresponding to the critical point of stability. This parameter can be a single value or a vector, but is fixed for all rho values. If different confidence levels are of interest, providing them as a vector saves a lot of resources because one simulation can be used to calculate the critical points of stability for all confidence levels.\nThe parameters sample_size_min and sample_size_max set the minimum and maximum sample size of one simulation study. As in corEvol they default to 20 and 1,000. This means a sample of 20 observation pairs is drawn from the population and step by step one more observation is added until the sample size of 1,000 is reached.\nThe output summarizes the individually set (and default) parameters as well as the critical point of stability of about 104. The value will change slightly from run to run because only 10,000 simulations are done here. In practice one can make a quick estimate with the default parameters and then increase the number of simulation studies for a more robust estimate. Under GNU/Linux one can also take advantage of the multicore support (parameter n_cores). This functionality is currently implemented via the pbmcapply package (Kuang et al. 2022), which is based on parallel.1\nFor another illustration let us reproduce Schönbrodt and Perugini (2013)’s oft-cited table of the critical points of stability for an absolute precision of 0.1 (meaning that the corridor will be \\(\\rho\\pm.1\\)). We take advantage of the vectorized input option by providing several \\(\\rho\\) values at once. Furthermore, we increase the number of studies to 100,000 to get accurate estimates. To cache the simulation results we use simpleCache (Nagraj and Sheffield 2021):\n\n\nlibrary(simpleCache)\nsetCacheDir(\"titz_cache\")\nsimpleCache(\"sim2\", {find_critical_pos(rho = seq(.1, .7, .1), n_studies = 1e5)})\nsim2\n\n  rho_pop pos.80% pos.90% pos.95% sample_size_min sample_size_max\n1     0.1     253     363     478              20            1000\n2     0.2     237     339     448              20            1000\n3     0.3     212     305     404              20            1000\n4     0.4     181     262     343              20            1000\n5     0.5     143     208     277              20            1000\n6     0.6     103     150     200              20            1000\n7     0.7      65      96     129              20            1000\n  lower_limit upper_limit n_studies n_not_breached precision_absolute\n1         0.0         0.2     1e+05            139                0.1\n2         0.1         0.3     1e+05            102                0.1\n3         0.2         0.4     1e+05             43                0.1\n4         0.3         0.5     1e+05             15                0.1\n5         0.4         0.6     1e+05              5                0.1\n6         0.5         0.7     1e+05              0                0.1\n7         0.6         0.8     1e+05              0                0.1\n  precision_relative\n1                 NA\n2                 NA\n3                 NA\n4                 NA\n5                 NA\n6                 NA\n7                 NA\n\nThe results are very close to the original publication (Schönbrodt and Perugini 2013). Note that a warning is shown because in some simulations the point of stability was not found. This is not too surprising as one can easily imagine an extreme outlier study that, for instance, starts at a negative correlation with \\(n=20\\) and does not reach the specified corridor of stability at the maximum sample size of \\(n=1,000\\). There are different ways to handle these outliers, which will affect the estimate.\n4 Handling outliers\nWhen comparing the table from above with the one in Schönbrodt and Perugini (2013), one should notice that fastpos usually produces larger estimates. To illustrate this more reliably we need to increase the number of studies, so that random fluctuations are minimized. Here we will run 100 simulations with 1,000,000 studies each.2\n\n\nsimpleCache(\"sim3\", {find_critical_pos(rho = rep(0.1, 100),\n                                       sample_size_max = 1e3, n_studies = 1e6)})\n\n\nA good summary of the data is the mean and the standard error of the distribution. Before calculating these statistics, we select only the points of stability from the result:\n\n\nsim3 <- sim3[, c(\"pos.80%\", \"pos.90%\", \"pos.95%\")]\ncolMeans(sim3)\n\n pos.80%  pos.90%  pos.95% \n253.2020 363.2100 477.5905 \n\nround(apply(sim3, 2, sd), 3)\n\npos.80% pos.90% pos.95% \n  0.603   0.729   1.035 \n\nThe average estimates are 253, 363 and 478 (with reasonably small standard errors), while in Schönbrodt and Perugini (2013) they are 252, 362, 470 and in Schönbrodt and Perugini (2018) 252, 360 and 474. Note that in every case fastpos gives a slightly larger estimate, which is not just a random fluctuation but related to the warning. In corEvol, if the corridor of stability is not reached, the respective study is ignored when calculating the critical point of stability. This leads to a systematic underestimation of the critical point of stability.\nTo illustrate this, we can use the lower level functions create_pop and simulate_pos to create a distribution of points of stability. In the following, the first line creates a population with a specific correlation and the second line produces several points of stability by drawing from this population. In contrast to the main function of the package (find_critical_pos), the function simulate_pos does not calculate quantiles, but only generates points of stability.\n\n\npop <- create_pop(rho = 0.1, size = 1e6)\nsimpleCache(\"sim4\", {simulate_pos(x_pop = pop[, 1], y_pop = pop[, 2],\n                                  n_studies = 1e6, sample_size_min = 20,\n                                  sample_size_max = 1e3, replace = TRUE,\n                                  lower_limit = 0, upper_limit = 0.2,\n                                  progress = FALSE)})\n\n\nThere are two ways to calculate the quantiles of interest:\n\n\nquantile(sim4, c(.8, .9, .95), na.rm = TRUE)\n\n80% 90% 95% \n252 361 473 \n\nsim4b <- ifelse(is.na(sim4), 1e3, sim4)\nquantile(sim4b, c(.8, .9, .95))\n\n80% 90% 95% \n253 363 478 \n\nIn the first calculation, the studies that did not reach the corridor of stability are ignored (like in corEvol), while in the second calculation it is assumed that the point of stability was reached at the maximum sample size. When repeating this simulation, the values will vary slightly but the second method will never produce smaller estimates. That the second method is more accurate can be tested by increasing the maximum sample size (to avoid studies that do not reach the corridor of stability). Here, we will set the maximum sample size to 5,000:\n\n\nsimpleCache(\"sim5\", {find_critical_pos(rho = rep(0.1, 100),\n                                       sample_size_max = 5e3,\n                                       n_studies = 1e6)})\nsim5 <- sim5[, c(\"pos.80%\", \"pos.90%\", \"pos.95%\")]\ncolMeans(sim5)\n\n pos.80%  pos.90%  pos.95% \n253.3100 363.5100 478.4305 \n\nround(apply(sim5, 2, sd), 3)\n\npos.80% pos.90% pos.95% \n  0.631   0.870   1.266 \n\nIf every study reaches the point of stability, the estimates are 253, 364 and 478. When the maximum sample size is too small (as in the second to last simulation), fastpos is indeed closer to these estimates than corEvol. While the difference to corEvol might seem practically negligible, corEvol’s estimates are clearly biased. Furthermore, depending on the parameters, the problem can become more severe. A very narrow corridor will lead to many studies not reaching the corridor, which corEvol will not even notice. On the other hand, fastpos will throw a warning, which should be taken seriously.\nBut even fastpos might underestimate the critical point of stability if the maximum sample size is too small: All estimates with a maximum sample size of 5,000 are slightly larger than the ones with a maximum sample size of 1,000. With a larger maximum sample size, there are more opportunities to leave the corridor again. At some point the probability of this event is very low because the corridor limits are too far away, but the probability is not 0. Thus, increasing the maximum sample size even further (here to 10,000) should lead to slightly larger estimates:\n\n\nsimpleCache(\"sim6\", {find_critical_pos(rho = rep(0.1, 100),\n                                       sample_size_max = 1e4, \n                                       n_studies = 1e6)})\nsim6 <- sim6[, c(\"pos.80%\", \"pos.90%\", \"pos.95%\")]\ncolMeans(sim6)\n\n pos.80%  pos.90%  pos.95% \n253.4000 363.7000 478.7005 \n\nround(apply(sim6, 2, sd), 3)\n\npos.80% pos.90% pos.95% \n  0.667   0.937   1.234 \n\nIndeed, all estimates are slightly larger but after rounding to a whole number only for the confidence of 95% the critical point of stability changes from 478 to 479. Furthermore, the randomness of the simulations permits such fluctuations since the standard errors are about 1. But note that all estimates increase when the maximum sample size changes from 1,000 to 5,000 and then to 10,000, which is a clear hint for a bias. Nonetheless, it appears unlikely that the estimates would increase much, when the maximum sample size grows further. The remaining problem is that the theoretical idea of stability assumes an infinite maximum sample size or, at least, that the maximum sample size is equal to the population size. It is therefore of some technical and practical interest to investigate the relationship between the maximum sample size and the critical point of stability in a dedicated simulation study with fastpos. Such a study would not be easy to approach with corEvol because of the quadratic time complexity. In the next section the speed difference between both packages is demonstrated empirically in a benchmark.\n5 Benchmark\ncorEvol was written as a script for a simulation study and cannot be simply called via a function in a package. Thus, a helper function will be used that sources the script files. To make the benchmark reproducible, the original repository corEvol was forked and a benchmark branch created. With git and a shell installed, the following tries to update the repository in the corEvol folder. If this is unsuccessful (the folder does not exist), the repository is cloned.\n\ngit -C corEvol pull || git clone --single-branch --branch benchmark \\\n  https://github.com/johannes-titz/corEvol\n\nAlternatively, you can download the required files from the supplementary material of this article.\nFor corEvol, two files are sourced for the benchmark. The first file generates the simulations and the second calculates the critical point of stability. In corEvol a simulation run takes a lot of time and thus it is not practical to run it too many times. But since the expected speed difference between both implementations is substantial, this should not be a concern. Here, ten repetitions were done with the microbenchmark (Mersmann 2021) package. The code was run on a Dell Server r6515 with an AMD EPYC 7302P CPU. Only one core was used to not confound the result with the specific parallel implementation.\n\n\nlibrary(microbenchmark)\ncorevol <- function() {\n  setwd(\"corEvol\")\n  source(\"01-simdata.R\")\n  source(\"02-analyse.R\")\n  setwd(\"../\")\n}\nfastpos <- function() {\n  find_critical_pos(rho = .1, sample_size_max = 1e3, n_studies = 1e4,\n                    progress = FALSE)\n}\nsimpleCache(\"bm\", {microbenchmark(corevol = corevol(), fastpos = fastpos(),\n                                  times = 10, unit = \"s\")})\nsummary(bm)\n\n     expr         min         lq        mean     median          uq\n1 corevol 350.4551133 352.642384 355.7306834 355.221224 358.2854179\n2 fastpos   0.5692708   0.579922   0.6215005   0.596842   0.6066209\n          max neval cld\n1 365.8751542    10   b\n2   0.8496276    10  a \n\nFor the chosen parameters, fastpos is about 572 times faster than corEvol, for which there are two main reasons: (1) fastpos is built around a C++ function via Rcpp and (2) this function does not calculate every calculation from scratch, but only calculates the difference between the correlation at sample size \\(n\\) and \\(n-1\\) via the sum formula of the Pearson correlation (see Equation (2)). There are some other factors that might play a role, but they cannot account for the large difference found. For instance, setting up a population takes quite long in corEvol (about 17s), but compared to the 6 minutes required overall, this is only a small fraction. There are other parts of the corEvol code that are fated to be slow, but again, a speedup by a factor of 572 cannot be achieved by improving these parts. The presented benchmark is not comprehensive, but still demonstrates that fastpos can be used with no significant waiting time for a typical scenario, while for corEvol this is not the case.\nAnother benchmark on a local i5-3320 2.6 GHz CPU from 2012 resulted in means of 1.5s for fastpos and 603s for corEvol giving a speedup of around 400. Thus, even on older CPUs and single-cored fastpos delivers almost instantly for default parameters.\n6 Other effect sizes\nThe focus of fastpos is on the Pearson correlation as the effect size. In principle the stability approach can be extended to all sorts of effect sizes or even other statistical parameters. Since the original authors studied the Pearson correlation, it made sense to improve the algorithm for this specific use case. But mathematical shortcuts as in Equation (2) should also exist for other effect sizes and might be implemented in the future.\nA simple alternative for applying the method to other effect sizes is to convert these effects to the Pearson correlation. Such conversions are very common in meta-analyses, where a consistent effect size must be used across all studies to calculate a meaningful average effect. Standard approximate conversion formulas can be found in text books on research methods (Sedlmeier and Renkewitz 2018; Borenstein et al. 2021). Several packages in R also provide these conversions. For instance, effectsize (Ben-Shachar et al. 2020) includes the functions d_to_r and r_to_d. d_to_r is based on the approximation \\(r = \\frac{d}{d^2+4}\\), which should only be used for equal group sizes. As an example, consider \\(d=0.5\\) between two equally sized groups and a corridor with limits of 0.4 and 0.6.\n\n\nr <- effectsize::d_to_r(0.5)\nlower_limit <- effectsize::d_to_r(0.4)\nupper_limit <- effectsize::d_to_r(0.6)\nsimpleCache(\"sim7\", {find_critical_pos(rho = r, sample_size_max = 11e3,\n                                       n_studies = 1e5,\n                                       lower_limit = lower_limit,\n                                       upper_limit = upper_limit)})\nsim7\n\n    rho_pop pos.80% pos.90% pos.95% sample_size_min sample_size_max\n1 0.2425356    1119    1606    2108              20           11000\n  lower_limit upper_limit n_studies n_not_breached precision_absolute\n1   0.1961161   0.2873479     1e+05              0                 NA\n  precision_relative\n1                 NA\n\nThe corresponding Pearson correlation for \\(d=0.5\\pm0.1\\) is about 0.24, with very narrow and slightly asymmetric limits (0.20 to 0.29). The critical point of stability is 2108 for a confidence level of 95%.\n7 Summary\nIn this article, fastpos, a package for estimating the critical point of stability was introduced. The package is much faster than the original implementation and can be conveniently used for sample size planning as well as Monte Carlo simulation studies. While the original implementation ignores studies that do not reach the corridor of stability, fastpos takes them into account and gives a more conservative and more accurate estimate (i.e. a larger critical point of stability). From a practitioner’s perspective, this detail might be negligible for typical parameters and relatively wide corridors. But from a statistical perspective, this detail is of relevance and further simulation studies are required to better understand the stability approach in general. Finally, a comparison to other methods of sample size planning would be of much interest and could influence how empirical scientists plan for sample size in the future. fastpos can be a useful tool to achieve these goals.\n8 Acknowledgment\nI want to thank Matthias Hörr and Thomas Schäfer for insightful discussions about the stability approach. Furthermore, I want to thank an anonymous reviewer for many helpful suggestions on how to improve the article and the package.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-041.zip\nCRAN packages used\nfastpos, Rcpp, pbmcapply, simpleCache, microbenchmark, effectsize\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, MetaAnalysis, NumericalMathematics\n\n\nJ. Algina and S. Olejnik. Sample size tables for correlation analysis with applications in partial correlation and multiple regression analysis. Multivariate Behavioral Research, 38: 309–323, 2003. URL https://doi.org/10.1207/S15327906MBR3803_02.\n\n\nM. S. Ben-Shachar, D. Lüdecke and D. Makowski. effectsize: Estimation of effect size indices and standardized parameters. Journal of Open Source Software, 5(56): 2815, 2020. URL https://doi.org/10.21105/joss.02815.\n\n\nM. Borenstein, L. V. Hedges, J. P. Higgins and H. R. Rothstein. Introduction to meta-analysis. John Wiley & Sons, 2021.\n\n\nJ. Cohen. Statistical power analysis for the behavioral sciences. Hillsdale, NJ: Lawrence Erlbaum Associates, 1988.\n\n\nD. Eddelbuettel, R. Francois, J. Allaire, K. Ushey, Q. Kou, N. Russell, I. Ucar, D. Bates and J. Chambers. Rcpp: Seamless r and c++ integration. 2022. URL https://CRAN.R-project.org/package=Rcpp. R package version 1.0.9.\n\n\nK. Kuang, Q. Kong and F. Napolitano. pbmcapply: Tracking the progress of mc*pply with progress bar. 2022. URL https://CRAN.R-project.org/package=pbmcapply. R package version 1.5.1.\n\n\nS. E. Maxwell, K. Kelley and J. R. Rausch. Sample size planning for statistical power and accuracy in parameter estimation. Annual Review of Psychology, 59: 537–563, 2008. URL https://doi.org/10.1146/annurev.psych.59.103006.093735.\n\n\nO. Mersmann. microbenchmark: Accurate timing functions. 2021. URL https://CRAN.R-project.org/package=microbenchmark. R package version 1.4.9.\n\n\nV. Nagraj and N. Sheffield. simpleCache: Simply caching r objects. 2021. URL https://CRAN.R-project.org/package=simpleCache. R package version 0.4.2.\n\n\nF. D. Schönbrodt and M. Perugini. At what sample size do correlations stabilize? Journal of Research in Personality, 47: 609–612, 2013. URL https://doi.org/10.1016/j.jrp.2013.05.009.\n\n\nF. D. Schönbrodt and M. Perugini. Corrigendum to “At What Sample Size Do Correlations Stabilize?” [J. Res. Pers. 47 (2013) 609–612]. Journal of Research in Personality, 74: 194, 2018. URL https://doi.org/10.1016/j.jrp.2018.02.010.\n\n\nP. Sedlmeier and F. Renkewitz. Forschungsmethoden und Statistik für Psychologen und Sozialwissenschaftler. 3rd ed Hallbergmoos, Germany: Pearson Studium, 2018.\n\n\nThe multicore support will not be demonstrated here because it is difficult to create reproducible examples across different operating systems and number of cores.↩︎\nIt is worth noting that (with a single core) this simulation would take several weeks to complete with corEvol but only takes about 66 minutes with fastpos.↩︎\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:38+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-023/",
    "title": "brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R",
    "description": "Longitudinal (panel) data provide the opportunity to examine temporal patterns of individuals, because measurements are collected on the same person at different, and often irregular, time points. The data is typically visualised using a \"spaghetti plot\", where a line plot is drawn for each individual. When overlaid in one plot, it can have the appearance of a bowl of spaghetti. With even a small number of subjects, these plots are too overloaded to be read easily. The interesting aspects of individual differences are lost in the noise. Longitudinal data is often modelled with a hierarchical linear model to capture the overall trends, and variation among individuals, while accounting for various levels of dependence. However, these models can be difficult to fit, and can miss unusual individual patterns. Better visual tools can help to diagnose longitudinal models, and better capture the individual experiences. This paper introduces the R package, brolgar (BRowse over Longitudinal data Graphically and Analytically in R), which provides tools to identify and summarise interesting individual patterns in longitudinal data.",
    "author": [
      {
        "name": "Nicholas Tierney",
        "url": "https://njtierney.com"
      },
      {
        "name": "Dianne Cook",
        "url": "https://dicook.org"
      },
      {
        "name": "Tania Prvan",
        "url": {}
      }
    ],
    "date": "2022-11-11",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\n\n\n\n1 Introduction\nThis paper is about exploring longitudinal data effectively. By “longitudinal data” we specifically mean individuals repeatedly measured through time. This could include panel data, where possibly different samples from a key variable (e.g. country), are aggregated at each time collection. The important component is a key variable with repeated measurements regularly, or irregularly over time. The inherent structure allows us to examine temporal patterns of individuals, shown in Figure 1, of the average height of Australian males over years. The individual component is country, and the time component is year. The variable country along with other variables is measured repeatedly from 1900 to 1970, with irregular intervals between years.\n\n\n\nFigure 1: Example of longitudinal data: average height of men in Australia for 1900-1970. The height increase over time, and are measured at irregular intervals.\n\n\n\nThe full dataset of Figure 1 is shown in Figure 2, showing 144 countries from the year 1700. This plot is challenging to understand because there is overplotting, making it hard to see the individuals. Solutions to this are not always obvious. Showing separate individual plots of each country does not help, as 144 plots is too many to comprehend. Making the lines transparent or fitting a simple model to all the data Figure 2B, might be a common first step to see common trends. However, all this seems to clarify is: 1) There is a set of some countries that are similar, and they are distributed around the center of the countries, and 2) there is a general upward trend in heights over time. We learn about the collective, but lose sight of the individuals.\n\n\n\nFigure 2: The full dataset shown as a spaghetti plot (A), with transparency (B), and with a linear model overlayed (C). It is still hard to see the individuals.\n\n\n\nThis paper demonstrates how to effectively and efficiently explore longitudinal data, using the R package, brolgar. We examine four problems in exploring longitudinal data:\nHow to sample the data\nFinding interesting individuals\nFinding representative individuals\nUnderstanding a model\nThis paper proceeds in the following way: first, a brief review of existing approaches to longitudinal data, then the definition of longitudinal data, then approaches to these four problems are discussed, followed by a summary.\n2 Background\nR provides basic time series, ts, objects, which are vectors or matrices that represent data sampled at equally spaced points in time. These have been extended through packages such as xts, and zoo (Zeileis and Grothendieck 2005; Ryan and Ulrich 2020), which only consider data in a wide format with a regular implied time series. These are not appropriate for longitudinal data, which can have indexes that are not time unit oriented, such as “Wave 1…n”, or may contain irregular intervals.\nOther packages focus more directly on panel data in R, focussing on data operations and model interfaces. The pmdplyr package provides “Panel Manoeuvres” in dplyr(Huntington-Klein and Khor 2020). It defines the data structure in as a pibble object (panel tibble), requiring an id and group column being defined to identify the unique identifier and grouping. The pmdplyr package focuses on efficient and custom joins and functions, such as inexact_left_join(). It does not implement tidyverse equivalent tools, but instead extends their usecase with a new function, for example mutate_cascade and mutate_subset. The panelr package provides an interface for data reshaping on panel data, providing widening and lengthening functions (widen_panel() and long_panel() (Long 2020)). It also provides model facilitating functions by providing its own interface for mixed effects models. The plm package (Millo 2017) for panel data econometrics provides methods for estimating models such as GMM for panel data, and testing, for example for model specification or serial correlation. It also provides a data structure, the pdata.frame, which stores the index attribute of the individual and time dimensions, for use within the package’s functions.\nThese software generally re-implement their own custom panel data class object, as well as custom data cleaning tasks, such as reshaping into long and wide form. They all share similar features, providing some identifying or index variable, and some grouping or key.\n3 Longitudinal Data Structures\nLongitudinal data is a sibling of many other temporal data forms, including panel data, repeated measures, and time series. The differences are many, and can be in data collection, context and even the field of research. Time series are usually long and regularly spaced in time. Panel data may measure different units at each time point and aggregate these values by a categorical or key variable. Repeated measures typically measure before and after treatment effects. We like to think of longitudinal as measuring the same individual (e.g. wage earner) over time, but this definition is not universally agreed on. Despite the differences, they all share a fundamental similarity: they are measurements over a time period.\nThis time period has structure - the time component (dates, times, waves, seconds, etc), and the spacing between measurements - unequal or equal. This data structure needs to be respected during analysis to preserve the lowest level of granularity, to avoid for example, collapsing across month when the data is collected every second, or assuming measurements occur at fixed time intervals. These mistakes can be avoided by encoding the data structure into the data itself. This information can then be accessed by analysis tools, providing a consistent way to understand and summarise the data. This ensures the different types of longitudinal data previously mentioned can be handled in the same way.\nBuilding on a tsibble\nSince longitudinal data can be thought of as “individuals repeatedly measured through time”, they can be considered as a type of time series, as defined in Hyndman and Athanasopoulos (2018): “Anything that is observed sequentially over time is a time series”. This definition has been realised as a time series tsibble in (Wang et al. 2020). These objects are defined as data meeting these conditions:\nThe index: the time variable\nThe key: variable(s) defining individual groups (or series)\nThe index and key (1 + 2) together determine a distinct row\nIf the specified key and index pair do not define a distinct row - for example, if there are duplicates in the data, the tsibble will not be created. This helps ensure the data is properly understood and cleaned before analysis is conducted, removing avoidable errors that might have impacted downstream decisions.\nWe can formally define our heights data from Figure 1 as a tsibble using, as_tsibble:\n\n\nheights_brolgar <- as_tsibble(heights_brolgar,\n                      index = year,\n                      key = country,\n                      regular = FALSE)\n\n\nThe index is year, the key is country, and regular = FALSE since the intervals in the years measured are not regular. Using a tsibble means that the index and key time series information is recorded only once, and can be referred to many times in other parts of the data analysis by time-aware tools.\nIn addition to providing consistent ways to manipulate time series data, further benefits to building on tsibble are how it works within the tidyverse ecosystem, as well as the tidy time series packages called “tidyverts”, containing fable (O’Hara-Wild et al. 2020a), feasts, (O’Hara-Wild et al. 2020b). For example, tsibble provides modified tidyverse functions to explore implicit missing values in the index (e.g., has_gaps() and fill_gaps()), as well as grouping and partitioning based on the index with index_by(). For full details and examples of use with the tidyverts time series packages, see Wang et al. (2020).\nThe brolgar package uses tsibble so users can take advantage of these tools, learning one way of operating a data analysis that will work and have overlap with other contexts.\nCharacterising Individual Series\nCalculating a feature\nWe can summarise the individual series by collapsing their many measurements into a single statistic, such as the minimum, maximum, or median, with one row per key. We do this with the features function from the fabletools package, made available in brolgar. This provides a summary of a given variable, accounting for the time series structure, and returning one row per key specified. It can be thought of as a time-series aware variant of the summarise function from dplyr. The feature function works by specifying the data, the variable to summarise, and the feature to calculate. A template is shown below\nfeatures(<DATA>, <VARIABLE>, <FEATURE>)\nor, with the pipe:\n<DATA> %>% features(<VARIABLE>, <FEATURE>)\nFor example, to calculate the minimum height for each key (country), in heights, we specify the heights data, then the variable to calculate features on, height_cm, then the feature to calculate, min (we write c(min = min) so the column calculated gets the name “min”):\n\n\nheights_min <- features(.tbl = heights_brolgar, \n                        .var = height_cm, \n                        features = c(min = min))\n\nheights_min\n\n# A tibble: 119 × 2\n   country       min\n   <chr>       <dbl>\n 1 Afghanistan  161.\n 2 Algeria      166.\n 3 Angola       159.\n 4 Argentina    167.\n 5 Armenia      164.\n 6 Australia    170 \n 7 Austria      162.\n 8 Azerbaijan   170.\n 9 Bangladesh   160.\n10 Belgium      163.\n# … with 109 more rows\n\nWe call these summaries features of the data. We can use this information to summarise these features of the data, for example, visualising the distribution of minimum values (Figure 3A).\nWe are not limited to one feature at a time, many features can also be calculated, for example:\n\n\nheights_three <- heights_brolgar %>%\n  features(height_cm, c(\n    min = min,\n    median = median,\n    max = max\n  ))\n\nheights_three\n\n# A tibble: 119 × 4\n   country       min median   max\n   <chr>       <dbl>  <dbl> <dbl>\n 1 Afghanistan  161.   167.  168.\n 2 Algeria      166.   169   171.\n 3 Angola       159.   167.  169.\n 4 Argentina    167.   168.  174.\n 5 Armenia      164.   169.  172.\n 6 Australia    170    172.  178.\n 7 Austria      162.   167.  179.\n 8 Azerbaijan   170.   172.  172.\n 9 Bangladesh   160.   162.  164.\n10 Belgium      163.   166.  177.\n# … with 109 more rows\n\nThese can then be visualised together (Figure 3).\n\n\n\nFigure 3: Three plots showing the distribution of minimum, median, and maximum values of height in centimeters. Part A shows just the distribution of minimum, part B shows the distribution of minimum, median, and maximum, and part C shows these three values plotted together as a line graph. We see that there is overlap amongst all three statistics. That is, some countries minimum heights are taller than some countries maximum heights.\n\n\n\nThese sets of features can be pre-specified, for example, brolgar provides a five number summary (minimum, 25th quantile, median, mean, 75th quantile, and maximum) of the data with feat_five_num:\n\n\nheights_five <- heights_brolgar %>%\n  features(height_cm, feat_five_num)\n\nheights_five\n\n# A tibble: 119 × 6\n   country       min   q25   med   q75   max\n   <chr>       <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Afghanistan  161.  164.  167.  168.  168.\n 2 Algeria      166.  168.  169   170.  171.\n 3 Angola       159.  160.  167.  168.  169.\n 4 Argentina    167.  168.  168.  170.  174.\n 5 Armenia      164.  166.  169.  172.  172.\n 6 Australia    170   171.  172.  173.  178.\n 7 Austria      162.  164.  167.  169.  179.\n 8 Azerbaijan   170.  171.  172.  172.  172.\n 9 Bangladesh   160.  162.  162.  163.  164.\n10 Belgium      163.  164.  166.  168.  177.\n# … with 109 more rows\n\nThis takes the heights data, pipes it to features, and then instructs it to summarise the height_cm variable, using feat_five_num. There are several handy functions for calculating features of the data that\nbrolgar provides. These all start with feat_, and include:\nfeat_ranges(): min, max, range difference, interquartile range;\nfeat_spread(): variance, standard deviation, median absolute distance, and interquartile range;\nfeat_monotonic(): is it always increasing, decreasing, or unvarying?;\nfeat_diff_summary(): the summary statistics of the differences amongst a value, including the five number summary, as well as the standard deviation and variance;\nfeat_brolgar(), which will calculate all features available in the brolgar package.\nOther examples of features from the feasts package.\nFeature sets\nIf you want to run many or all features from a package on your data you can collect them all with feature_set. For example:\n\n\nlibrary(fabletools)\nfeat_set_brolgar <- feature_set(pkgs = \"brolgar\")\nlength(feat_set_brolgar)\n\n[1] 6\n\nYou could then run these like so:\n\n\nheights_brolgar %>%\n  features(height_cm, feat_set_brolgar)\n\n# A tibble: 119 × 46\n   country     min...1 med...2 max...3 min...4 q25...5 med...6 q75...7\n   <chr>         <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Afghanistan    161.    167.    168.    161.    164.    167.    168.\n 2 Algeria        166.    169     171.    166.    168.    169     170.\n 3 Angola         159.    167.    169.    159.    160.    167.    168.\n 4 Argentina      167.    168.    174.    167.    168.    168.    170.\n 5 Armenia        164.    169.    172.    164.    166.    169.    172.\n 6 Australia      170     172.    178.    170     171.    172.    173.\n 7 Austria        162.    167.    179.    162.    164.    167.    169.\n 8 Azerbaijan     170.    172.    172.    170.    171.    172.    172.\n 9 Bangladesh     160.    162.    164.    160.    162.    162.    163.\n10 Belgium        163.    166.    177.    163.    164.    166.    168.\n# … with 109 more rows, and 38 more variables: max...8 <dbl>,\n#   min...9 <dbl>, max...10 <dbl>, range_diff...11 <dbl>,\n#   iqr...12 <dbl>, var...13 <dbl>, sd...14 <dbl>, mad...15 <dbl>,\n#   iqr...16 <dbl>, min...17 <dbl>, max...18 <dbl>, median <dbl>,\n#   mean <dbl>, q25...21 <dbl>, q75...22 <dbl>, range1 <dbl>,\n#   range2 <dbl>, range_diff...25 <dbl>, sd...26 <dbl>,\n#   var...27 <dbl>, mad...28 <dbl>, iqr...29 <dbl>, …\n\nTo see other features available in the feasts R package run library(feasts) then ?fabletools::feature_set.\nCreating your own feature\nTo create your own features or summaries to pass to features, you provide a named vector of functions. These can include functions that you have written yourself. For example, returning the first three elements of a series, by writing our own second and third functions.\n\n\nsecond <- function(x) nth(x, n = 2)\nthird <- function(x) nth(x, n = 3)\n\nfeat_first_three <- c(first = first,\n                      second = second,\n                      third = third)\n\n\nThese are then passed to features like so:\n\n\nheights_brolgar %>%\n  features(height_cm, feat_first_three)\n\n# A tibble: 119 × 4\n   country     first second third\n   <chr>       <dbl>  <dbl> <dbl>\n 1 Afghanistan  168.   166.  167.\n 2 Algeria      169.   166.  169 \n 3 Angola       160.   159.  160.\n 4 Argentina    170.   168.  168 \n 5 Armenia      169.   168.  166.\n 6 Australia    170    171.  170.\n 7 Austria      165.   163.  162.\n 8 Azerbaijan   170.   171.  171.\n 9 Bangladesh   162.   162.  164.\n10 Belgium      163.   164.  164 \n# … with 109 more rows\n\nAs well, brolgar provides some useful additional features for the five number summary, feat_five_num, whether keys are monotonically increasing feat_monotonic, and measures of spread or variation, feat_spread. Inside brolgar, the features are created with the following syntax:\n\n\nfeat_five_num <- function(x, ...) {\n  c(\n    min = b_min(x, ...),\n    q25 = b_q25(x, ...),\n    med = b_median(x, ...),\n    q75 = b_q75(x, ...),\n    max = b_max(x, ...)\n  )\n}\n\n\nHere the functions b_ are functions with a default of na.rm = TRUE, and in\nthe cases of quantiles, they use type = 8, and names = FALSE. What is particularly useful is that these will work on any type of time series data, and you can use other more typical time series features from the feasts package, such as autocorrelation, feat_acf() and Seasonal and Trend decomposition using Loess feat_stl() (O’Hara-Wild et al. 2020b).\nThis demonstrates a workflow that can be used to understand and explore your longitudinal data. The brolgar package builds upon this workflow made available by feasts and fabletools. Users can also create their own features to summarise the data.\n4 Breaking up the Spaghetti\nPlots like Figure 2 are often called, “spaghetti plots”, and can be useful for a high level understanding as a whole. However, we cannot process and understand the individuals when the data is presented like this.\nSampling\nJust how spaghetti is portioned out for consumption, we can sample some of the data by randomly sampling the data into sub-plots with the facet_sample() function (Figure 4).\n\n\nggplot(heights_brolgar,\n       aes(x = year,\n           y = height_cm,\n           group = country)) + \n  geom_line() + \n  facet_sample() + \n  scale_x_continuous(breaks = c(1750, 1850, 1950))\n\n\n\nFigure 4: Twelve facets with three keys per facet shown. This allows us to quickly view a random sample of the data.\n\n\n\nThis defaults to 12 facets and 3 samples per facet, and provides options for the number of facets, and the number of samples per facet. This means the user only needs to consider the most relevant questions: “How many keys per facet?” and “How many facets to look at?”. The code to change the figure from Figure 2 into 4 requires only one line of code, shown below:\nggplot(heights_brolgar,\n       aes(x = year,\n           y = height_cm,\n           group = country)) + \n  geom_line() + \n  facet_sample()\nStratifying\nExtending this idea of samples, we can instead look at all of the data, spread out equally over facets, using facet_strata(). It uses 12 facets by default, controllable with n_strata. The code to do so is shown below, creating Figure 5.\n\n\nggplot(heights_brolgar,\n       aes(x = year,\n           y = height_cm,\n           group = country)) + \n  geom_line() + \n  facet_strata() +\n  scale_x_continuous(breaks = c(1750, 1850, 1950))\n\n\n\nFigure 5: All of the data is shown by spreading out each key across twelve facets. Each key is only shown once, and is randomly allocated to a facet.\n\n\n\nFeaturing\nFigure 4 and Figure 5 only show each key once, being randomly assigned to a facet. We can meaningfully place the keys into facets, by arranging the heights “along” a variable, like year, using the along argument in facet_strata to produce Figure 6:\n\n\nggplot(heights_brolgar,\n       aes(x = year,\n           y = height_cm,\n           group = country)) + \n  geom_line() + \n  facet_strata(along = -year) + \n  scale_x_continuous(breaks = c(1750, 1850, 1950))\n\n\n\nFigure 6: Displaying all the data across twelve facets. Instead of each key being randomly in a facet, each facet displays a specified range of values of year. In this case, the top left facet shows the keys with the earliest starting year, and the bottom right shows the facet with the latest starting year.\n\n\n\nWe have not lost any of the data, only the order in which they are presented has changed. We learn the distribution and changes in heights over time, and those measured from the earliest times appear to be more similar, but there is much wider variation in the middle years, and then for more recent heights measured from the early 1900s, the heights are more similar. The starting point of each of these years seems to increase at roughly the same interval. This informs us that the starting times of the years is approximately uniform.\nTogether facet_sample() and facet_strata() allow for rapid exploration, by focusing on relevant questions instead of the minutiae. This is achieved by appropriately randomly assigning while maintaining key structure, keeping the correct number of keys per plot, and so on. For example, facet_sample() the questions are: “How many lines per facet” and “How many facets?”, and for facet_strata() the questions are: “How many facets / strata?” and “What to arrange plots along?”.\nAnswering these questions keeps the analysis in line with the analytic goals of exploring the data, rather than distracting to minutiae. This is a key theme of improving tools for data analysis. Abstracting away the parts that are not needed, so the analyst can focus on the task at hand.\nUnder the hood, facet_sample() and facet_strata() are powered with sample_n_keys() and stratify_keys(). These can be used to create data structures used in facet_sample() and facet_strata(), and extend them for other purposes.\nUsing a tsibble stores important key and index components, in turn allowing for better ways to break up spaghetti plots so we can look at many and all sub-samples using facet_sample() and facet_strata().\n5 Book-keeping\nLongitudinal data is not always measured at the same time and at the same frequency. When exploring longitudinal data, a useful first step is to explore the frequency of measurements of the index. We can check if the index is regular using index_regular() and summarise the spacing of the index with index_summary(). These are S3 methods, so for data.frame objects, the index must be specified, however for the tsibble objects, the defined index is used.\n\n\nindex_summary(heights_brolgar)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1710    1782    1855    1855    1928    2000 \n\nindex_regular(heights_brolgar)\n\n[1] TRUE\n\nWe can explore how many observations per country by counting the number of observations with features, like so:\n\n\nheights_brolgar %>% features(year, n_obs)\n\n# A tibble: 119 × 2\n   country     n_obs\n   <chr>       <int>\n 1 Afghanistan     5\n 2 Algeria         5\n 3 Angola          9\n 4 Argentina      20\n 5 Armenia        11\n 6 Australia      10\n 7 Austria        18\n 8 Azerbaijan      7\n 9 Bangladesh      9\n10 Belgium        10\n# … with 109 more rows\n\nThis can be further summarised by counting the number of times there are a given number of observations:\n\n\nheights_brolgar %>% features(year, n_obs) %>% count(n_obs)\n\n# A tibble: 24 × 2\n   n_obs     n\n   <int> <int>\n 1     5    11\n 2     6    11\n 3     7    13\n 4     8     5\n 5     9    12\n 6    10    12\n 7    11     9\n 8    12     4\n 9    13     7\n10    14     6\n# … with 14 more rows\n\nBecause we are exploring the temporal patterns, we cannot reliably say anything about those individuals with few measurements. The data used, heights_brolgar has less than 5 measurements. This was done using add_n_obs(), which adds the number of observations to the existing data. Overall this drops 25 countries, leaves us with 119 out of the original 144 countries.\n\n\nheights_brolgar <- heights %>% \n  add_n_obs() %>% \n  filter(n_obs >= 5)\n\n\nWe can further explore when countries are first being measured using features to find the first year for each country number of starting years with the first function from dplyr, and explore this with a visualisation (Figure 7).\n\n\nheights_brolgar %>% \n  features(year, c(first = first))\n\n# A tibble: 119 × 2\n   country     first\n   <chr>       <dbl>\n 1 Afghanistan  1870\n 2 Algeria      1910\n 3 Angola       1790\n 4 Argentina    1770\n 5 Armenia      1850\n 6 Australia    1850\n 7 Austria      1750\n 8 Azerbaijan   1850\n 9 Bangladesh   1850\n10 Belgium      1810\n# … with 109 more rows\n\n\n\nheights_brolgar %>% \n  features(year, c(first = first)) %>% \n  ggplot(aes(x = first)) +\n  geom_bar()\n\n\n\nFigure 7: Distribution of starting years of measurement. The data is already binned into 10 year blocks. Most of the years start between 1840 and 1900.\n\n\n\nWe can explore the variation in first year using feat_diff_summary. This combines many summaries of the differences in year.\n\n\nheights_diffs <- heights_brolgar %>% \n  features(year, feat_diff_summary)\n\nheights_diffs\n\n# A tibble: 119 × 10\n   country     diff_…¹ diff_…² diff_…³ diff_…⁴ diff_…⁵ diff_…⁶ diff_…⁷\n   <chr>         <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Afghanistan      10      10      30    32.5    55.8      60   692. \n 2 Algeria          10      10      10    22.5    39.2      60   625  \n 3 Angola           10      10      10    17.5    10        70   450  \n 4 Argentina        10      10      10    11.6    10        40    47.4\n 5 Armenia          10      10      10    15      20.8      30    72.2\n 6 Australia        10      10      10    13.3    10        40   100  \n 7 Austria          10      10      10    13.5    10        40    74.3\n 8 Azerbaijan       10      10      10    25      25.8      90  1030  \n 9 Bangladesh       10      10      10    18.8    15.8      70   441. \n10 Belgium          10      10      10    16.7    23.3      40   125  \n# … with 109 more rows, 2 more variables: diff_sd <dbl>,\n#   diff_iqr <dbl>, and abbreviated variable names ¹​diff_min,\n#   ²​diff_q25, ³​diff_median, ⁴​diff_mean, ⁵​diff_q75, ⁶​diff_max,\n#   ⁷​diff_var\n\nThis is particularly useful as using diff on year would return a very wide dataset that is hard to explore:\n\n\nheights_brolgar %>% \n  features(year, diff)\n\n# A tibble: 119 × 30\n   country  ...1  ...2  ...3  ...4  ...5  ...6  ...7  ...8  ...9 ...10\n   <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Afghan…    10    50    60    10    NA    NA    NA    NA    NA    NA\n 2 Algeria    10    10    60    10    NA    NA    NA    NA    NA    NA\n 3 Angola     10    10    70    10    10    10    10    10    NA    NA\n 4 Argent…    10    10    10    10    10    10    10    10    10    10\n 5 Armenia    10    30    10    10    30    20    10    10    10    10\n 6 Austra…    10    10    10    10    10    10    10    40    10    NA\n 7 Austria    20    10    10    30    10    10    10    10    10    10\n 8 Azerba…    10    90    10    10    10    20    NA    NA    NA    NA\n 9 Bangla…    10    10    10    70    10    20    10    10    NA    NA\n10 Belgium    10    10    10    10    10    10    30    40    20    NA\n# … with 109 more rows, and 19 more variables: ...11 <dbl>,\n#   ...12 <dbl>, ...13 <dbl>, ...14 <dbl>, ...15 <dbl>, ...16 <dbl>,\n#   ...17 <dbl>, ...18 <dbl>, ...19 <dbl>, ...20 <dbl>, ...21 <dbl>,\n#   ...22 <dbl>, ...23 <dbl>, ...24 <dbl>, ...25 <dbl>, ...26 <dbl>,\n#   ...27 <dbl>, ...28 <dbl>, ...29 <dbl>\n\nWe can then look at the summaries of the differences in year by changing to long form and facetting (Figure 8), we learn about the range of intervals between measurements, the smallest being 10 years, the largest being 125, and that most of the data is measured between 10 and 30 years.\n\n\n\nFigure 8: Exploring the different summary statistics of the differences amongst the years. We learn that the smallest interval between measurements is 10 years, and the largest interval is between 10 and 125 years, and that most of the data is measured between 10 and 30 or so years.\n\n\n\n6 Finding Waldo\nLooking at a spaghetti plot, it can be hard to identify which lines are the most interesting, or unusual. A workflow to identify interesting individuals to start with is given below:\nDecide upon an interesting feature (e.g., maximum)\nThis feature produces one value per key\nExamine the distribution of the feature\nJoin this table back to the data to get all observations for those keys\nArrange the keys or filter, using the feature\nDisplay the data for selected keys\nThis workflow is now demonstrated. Firstly, we decide on an interesting feature, “maximum height”, and whether height is always increasing. We calculate our own “feature”, calculating maximum height, and whether a value is increasing (with brolgar’s increasing function) as follows:\n\n\nheights_max_in <- heights_brolgar %>% \n  features(height_cm, list(max = max,\n                           increase = increasing))\n\nheights_max_in\n\n# A tibble: 119 × 3\n   country       max increase\n   <chr>       <dbl> <lgl>   \n 1 Afghanistan  168. FALSE   \n 2 Algeria      171. FALSE   \n 3 Angola       169. FALSE   \n 4 Argentina    174. FALSE   \n 5 Armenia      172. FALSE   \n 6 Australia    178. FALSE   \n 7 Austria      179. FALSE   \n 8 Azerbaijan   172. FALSE   \n 9 Bangladesh   164. FALSE   \n10 Belgium      177. FALSE   \n# … with 109 more rows\n\nThis returns a dataset of one value per key. Figure 9 examines the distribution of the features, showing us the distribution of maximum height, and the number of countries that are always increasing.\n\n\n\nFigure 9: The different distributions of the features - A is depicting the distribution of maximum height, and B displays the number of countries that are always increasing (FALSE), and always increasing (TRUE). We note that the average maximum heights range from about 160cm to 185cm, with most being around 170cm. We also learn that the vast majority of countries are not always increasing in height through time.\n\n\n\nWe can now join this table back to the data to get all observations for those keys to move from one key per row to all many rows per key.\n\n\nheights_max_in_full <- heights_max_in %>% \n  left_join(heights_brolgar,\n            by = \"country\")\n\nheights_max_in_full\n\n# A tibble: 1,406 × 9\n   country       max incre…¹  year n_obs conti…² heigh…³ year0 count…⁴\n   <chr>       <dbl> <lgl>   <dbl> <int> <chr>     <dbl> <dbl> <fct>  \n 1 Afghanistan  168. FALSE    1870     5 Asia       168.   160 Afghan…\n 2 Afghanistan  168. FALSE    1880     5 Asia       166.   170 Afghan…\n 3 Afghanistan  168. FALSE    1930     5 Asia       167.   220 Afghan…\n 4 Afghanistan  168. FALSE    1990     5 Asia       167.   280 Afghan…\n 5 Afghanistan  168. FALSE    2000     5 Asia       161.   290 Afghan…\n 6 Algeria      171. FALSE    1910     5 Africa     169.   200 Algeria\n 7 Algeria      171. FALSE    1920     5 Africa     166.   210 Algeria\n 8 Algeria      171. FALSE    1930     5 Africa     169    220 Algeria\n 9 Algeria      171. FALSE    1990     5 Africa     171.   280 Algeria\n10 Algeria      171. FALSE    2000     5 Africa     170.   290 Algeria\n# … with 1,396 more rows, and abbreviated variable names ¹​increase,\n#   ²​continent, ³​height_cm, ⁴​country_fct\n\nWe can then arrange the keys or filter, using the feature, for example, filtering only those countries that are only increasing:\n\n\nheights_increase <- heights_max_in_full %>% filter(increase)\nheights_increase\n\n# A tibble: 22 × 9\n   country    max increase  year n_obs continent heigh…¹ year0 count…²\n   <chr>    <dbl> <lgl>    <dbl> <int> <chr>       <dbl> <dbl> <fct>  \n 1 Honduras  168. TRUE      1950     6 Americas     164.   240 Hondur…\n 2 Honduras  168. TRUE      1960     6 Americas     164.   250 Hondur…\n 3 Honduras  168. TRUE      1970     6 Americas     165.   260 Hondur…\n 4 Honduras  168. TRUE      1980     6 Americas     165.   270 Hondur…\n 5 Honduras  168. TRUE      1990     6 Americas     165.   280 Hondur…\n 6 Honduras  168. TRUE      2000     6 Americas     168.   290 Hondur…\n 7 Moldova   174. TRUE      1840     5 Europe       165.   130 Moldova\n 8 Moldova   174. TRUE      1950     5 Europe       172.   240 Moldova\n 9 Moldova   174. TRUE      1960     5 Europe       173.   250 Moldova\n10 Moldova   174. TRUE      1970     5 Europe       174.   260 Moldova\n# … with 12 more rows, and abbreviated variable names ¹​height_cm,\n#   ²​country_fct\n\nOr tallest country\n\n\nheights_top <- heights_max_in_full %>% top_n(n = 1, wt = max)\nheights_top\n\n# A tibble: 16 × 9\n   country   max increase  year n_obs continent height…¹ year0 count…²\n   <chr>   <dbl> <lgl>    <dbl> <int> <chr>        <dbl> <dbl> <fct>  \n 1 Denmark  183. FALSE     1820    16 Europe        167.   110 Denmark\n 2 Denmark  183. FALSE     1830    16 Europe        165.   120 Denmark\n 3 Denmark  183. FALSE     1850    16 Europe        167.   140 Denmark\n 4 Denmark  183. FALSE     1860    16 Europe        168.   150 Denmark\n 5 Denmark  183. FALSE     1870    16 Europe        168.   160 Denmark\n 6 Denmark  183. FALSE     1880    16 Europe        170.   170 Denmark\n 7 Denmark  183. FALSE     1890    16 Europe        169.   180 Denmark\n 8 Denmark  183. FALSE     1900    16 Europe        170.   190 Denmark\n 9 Denmark  183. FALSE     1910    16 Europe        170    200 Denmark\n10 Denmark  183. FALSE     1920    16 Europe        174.   210 Denmark\n11 Denmark  183. FALSE     1930    16 Europe        174.   220 Denmark\n12 Denmark  183. FALSE     1940    16 Europe        176.   230 Denmark\n13 Denmark  183. FALSE     1950    16 Europe        180.   240 Denmark\n14 Denmark  183. FALSE     1960    16 Europe        180.   250 Denmark\n15 Denmark  183. FALSE     1970    16 Europe        181.   260 Denmark\n16 Denmark  183. FALSE     1980    16 Europe        183.   270 Denmark\n# … with abbreviated variable names ¹​height_cm, ²​country_fct\n\nWe can then display the data by highlighting it in the background, first creating a background plot and overlaying the plots on top of this as an additional ggplot layer, in Figure\n10.\n\n\n\n\n\n\n\n\n\n\nFigure 10: Interactive plot to explore longnostics of maximum height and slope from a simple linear fit, relative to the profiles. Click on either plot to select countries. For example, the country with the most negative slope, that is people are getting shorter is Eritrea.\n\n\n\n7 Dancing with Models\nThese same workflows can be used to interpret and explore a model. As the data tends to follow a non linear trajectory, we use a general additive model (gam) with the mgcv R package (Wood 2017) using the code below:\n\n\nheights_gam <- gam(\n    height_cm ~ s(year0, by = country_fct) + country_fct,\n    data = heights_brolgar,\n    method = \"REML\"\n  )\n\n\nThis fits height in centimetres with a smooth effect for year for each country, with a different intercept for each country. It is roughly equivalent to a random intercept varying slope model. Note that this gam model took approximately 8074 seconds to fit. We add the predicted and residual values for the model below, as well as the residual sums of squares for each country.\n\n\n\n\n\nlibrary(mgcv)\nlibrary(modelr)\nheights_aug <- heights_brolgar %>%\n  add_predictions(heights_gam, var = \"pred\") %>%\n  add_residuals(heights_gam, var = \"res\") %>% \n  group_by_key() %>% \n  mutate(rss = sum(res^2)) %>% \n  ungroup()\n\n\nWe can use the previous approach to explore the model results. We can take a look at a sample of the predictions along with the data, by using sample_n_keys. This provides a useful way to explore some set of the model predictions. In order to find those predictions that best summarise the best, and worst, and in between, we need to use the methods in the next section, “Stereotyping”.\n\n\nheights_aug %>% \n  sample_n_keys(12) %>% \n  ggplot(aes(x = year,\n             y = pred,\n             group = country)) + \n  geom_line(colour = \"steelblue\") +\n  geom_point(aes(y = height_cm)) + \n  facet_wrap(~country)\n\n\n\nFigure 11: Exploration of a random sample of the data. This shows the data points of 12 countries, with the model fit in blue.\n\n\n\n8 Stereotyping\nTo help understand a population of measurements over time, it can be useful to understand which individual measurements are typical (or “stereotypical”) of a measurement. For example, to understand which individuals are stereotypical of a statistic such as the minimum, median, and maximum height. This section discusses how to find these stereotypes in the data.\nFigure 12 shows the residuals of the simple model fit to the data in the previous section. There is an overlaid five number summary, showing the minimum, 1st quantile, median, 3rd quantile, and maximum residual value residuals, as well as a rug plot to show the data. We can use these residuals to understand the stereotypes of the data - those individuals in the model that best match to this five number summary.\n\n\n\nFigure 12: Five number summary of residual values from the model fit. The residuals are centered around zero with some variation.\n\n\n\nWe can do this using keys_near() from brolgar. By default this uses the 5 number summary, but any function can be used. You specify the variable you want to find the keys nearest, in this case rss, residual sums of squares for each key:\n\n\nkeys_near(heights_aug, var = rss)\n\n# A tibble: 62 × 5\n   country   rss stat  stat_value stat_diff\n   <chr>   <dbl> <fct>      <dbl>     <dbl>\n 1 Denmark  9.54 med         9.54         0\n 2 Denmark  9.54 med         9.54         0\n 3 Denmark  9.54 med         9.54         0\n 4 Denmark  9.54 med         9.54         0\n 5 Denmark  9.54 med         9.54         0\n 6 Denmark  9.54 med         9.54         0\n 7 Denmark  9.54 med         9.54         0\n 8 Denmark  9.54 med         9.54         0\n 9 Denmark  9.54 med         9.54         0\n10 Denmark  9.54 med         9.54         0\n# … with 52 more rows\n\nTo plot the data, they need to be joined back to the original data, we use a left join, joining by country.\n\n\nheights_near_aug <- heights_aug %>% \n  keys_near(var = rss) %>% \n  left_join(heights_aug, \n            by = c(\"country\"))\n\n\nFigure 13 shows those countries closest to the five number summary. Observing this, we see that the minimum RSS for Moldova fits a nearly perfectly straight line, and the maximum residuals for Myanmar have wide spread of values.\n\n\nggplot(heights_near_aug,\n       aes(x = year,\n           y = pred,\n           group = country,\n           colour = country)) + \n  geom_line(colour = \"orange\") + \n  geom_point(aes(y = height_cm)) + \n  scale_x_continuous(breaks = c(1780, 1880, 1980)) +\n  facet_wrap(~stat + country,\n             labeller = label_glue(\"Country: {country} \\nNearest to \\n{stat} RSS\"),\n             nrow = 1) + \n  theme(legend.position = \"none\",\n        aspect.ratio = 1)\n\n\n\nFigure 13: The keys nearest to the five number summary of the residual sums of squares. Moldova and Madagascar are well fit by the model, and are fit by a straight line. The remaining countries with poorer fit have greater variation in height. It is not clear how a better model fit could be achieved.\n\n\n\nWe can also look at the highest and lowest 3 residual sums of squares:\n\n\nheights_near_aug_top_3 <- heights_aug %>% \n  distinct(country, rss) %>% \n  top_n(n = 3,\n        wt = rss)\n\nheights_near_aug_bottom_3 <- heights_aug %>% \n  distinct(country, rss) %>% \n  top_n(n = -3,\n        wt = rss)\n\nheights_near_top_bot_3 <- bind_rows(highest_3 = heights_near_aug_top_3,\n                                    lowest_3 = heights_near_aug_bottom_3,\n                                    .id = \"rank\") %>% \n  left_join(heights_aug,\n            by = c(\"country\", \"rss\"))\n\n\nFigure 14 shows the same information as the previous plot, but with the 3 representative countries for each statistic. This gives us more data on what the stereotypically “good” and “poor” fitting countries to this model.\n\n\n\nFigure 14: Figure of stereotypes for those keys with the three highest and lowest RSS values. Those that fit best tend to be linear, but those that fit worst have wider variation in heights.\n\n\n\n9 Getting Started\nThe brolgar R package can be installed from CRAN using\n\n\n# From CRAN\ninstall.packages(\"brolgar\")\n# Development version\nremotes::install_github(\"njtierney/brolgar\")\n\n\nThe functions are all designed to build upon existing packages, but are predicated on working with tsibble. The package extends upon ggplot2 to provide facets for exploration: facet_sample() and facet_strata(). Extending dplyr’s sample_n() and sample_frac() functions by providing sampling and stratifying based around keys: sample_n_keys(), sample_frac_keys(), and stratify_keys(). New functions are focussed around the use of key, for example key_slope() to find the slope of each key, and keys_near() to find those keys near a summary statistic. Finally, feature calculation is provided by building upon the existing time series feature package, feasts.\nTo get started with brolgar you must first ensure your data is specified as a tsibble - discussed earlier in the paper, there is also a vignette “Longitudinal Data Structures”, which discusses these ideas. The next step we recommend is sampling some of your data with facet_sample(), and facet_strata(). When using facet_strata(), facets can be arranged in order of a variable, using the along argument, which can reveal interesting features.\nTo further explore longitudinal data, we recommend finding summary features of each variable with features, and identifying variables that are near summary statistics, using keys_near to find individuals stereotypical of a statistical value.\n10 Concluding Remarks\nThe brolgar package facilitates exploring longitudinal data in R. It builds upon existing infrastructure from tsibble, and feasts, which work within the tidyverse family of R packages, as well as the newer, tidyverts, time series packages. Users familiar with either of these package families will find a lot of similarity in their use, and first time users will be able to easily transition from brolgar to the tidyverse or tidyverts.\nVisualizing categorical or binary data over a time period can be difficult as the limited number of values on the y axis leads to overplotting. This can conceal the number of values present at a given value. The tools discussed in brolgar facilitate this in the form of facet_sample, and facet_strata. Some special methods could be developed to add jitter or noise around these values on the y axis, while still maintaining the graphical axis and tick marks.\nFuture work will explore more features and stratifications, and stereotypes, and generalise the tools to work for data without time components, and other data types.\n11 Acknowledgements\nWe would like to thank Stuart Lee, Mitchell O’Hara Wild, Earo Wang, and Miles McBain for their discussion on the design of brolgar. We would also like to thank Rob Hyndman, Monash University and ACEMS for their support of this research.\n12 Paper Source\nThe complete source files for the paper can be found at https://github.com/njtierney/rjournal-brolgar. The paper is built using rmarkdown, targets and capsule to ensure R package versions are the same. See the README file on the github repository for details on recreating the paper.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-023.zip\nCRAN packages used\nbrolgar, ts, xts, zoo, pmdplyr, dplyr, panelr, plm, tsibble, fable, feasts, fabletools, mgcv, ggplot2, targets, capsule\nCRAN Task Views implied by cited packages\nBayesian, CausalInference, Databases, Econometrics, Environmetrics, Epidemiology, Finance, HighPerformanceComputing, MissingData, MixedModels, ModelDeployment, Phylogenetics, ReproducibleResearch, Spatial, SpatioTemporal, TeachingStatistics, TimeSeries\n\n\nN. Huntington-Klein and P. Khor. Pmdplyr: ’Dplyr’ extension for common panel data maneuvers. 2020. URL https://CRAN.R-project.org/package=pmdplyr. R package version 0.3.3.\n\n\nR. J. Hyndman and G. Athanasopoulos. Forecasting: Principles and practice. 2nd ed Australia: OTexts, 2018.\n\n\nJ. A. Long. Panelr: Regression models and utilities for repeated measures and panel data. 2020. URL https://cran.r-project.org/package=panelr. R package version 0.7.3.\n\n\nG. Millo. Robust standard error estimators for panel models: A unifying approach. Journal of Statistical Software, 82(3): 1–27, 2017. DOI 10.18637/jss.v082.i03.\n\n\nM. O’Hara-Wild, R. Hyndman and E. Wang. Fable: Forecasting models for tidy time series. 2020a. URL https://CRAN.R-project.org/package=fable. R package version 0.2.1.\n\n\nM. O’Hara-Wild, R. Hyndman and E. Wang. Feasts: Feature extraction and statistics for time series. 2020b. URL https://CRAN.R-project.org/package=feasts. R package version 0.1.5.\n\n\nJ. A. Ryan and J. M. Ulrich. Xts: eXtensible time series. 2020. URL https://CRAN.R-project.org/package=xts. R package version 0.12.1.\n\n\nE. Wang, D. Cook and R. J. Hyndman. A New Tidy Data Structure to Support Exploration and Modeling of Temporal Data. Journal of Computational and Graphical Statistics, 29(3): 466–478, 2020. URL https://doi.org/10.1080/10618600.2019.1695624 [online; last accessed November 26, 2020]. Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10618600.2019.1695624.\n\n\nS. N. Wood. Generalized additive models: An introduction with r. 2nd ed Chapman; Hall/CRC, 2017.\n\n\nA. Zeileis and G. Grothendieck. Zoo: S3 infrastructure for regular and irregular time series. Journal of Statistical Software, 14(6): 1–27, 2005. DOI 10.18637/jss.v014.i06.\n\n\n\n\n",
    "preview": "articles/RJ-2022-023/RJ-2022-023_files/figure-html5/heights-sample-plot-1.png",
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {},
    "preview_width": 7200,
    "preview_height": 2400
  },
  {
    "path": "articles/RJ-2022-025/",
    "title": "akc: A Tidy Framework for Automatic Knowledge Classification in R",
    "description": "Knowledge classification is an extensive and practical approach in domain knowledge management. Automatically extracting and organizing knowledge from unstructured textual data is desirable and appealing in various circumstances. In this paper, the tidy framework for automatic knowledge classification supported by the akc package is introduced. With  powerful support from the R ecosystem, the akc framework can handle multiple procedures in data science workflow, including text cleaning, keyword extraction, synonyms consolidation and data presentation. While focusing on bibliometric analysis, the akc package is extensible to be used in other contexts. This paper introduces the framework and its features in detail. Specific examples are given to guide the potential users and developers to participate in open science of text mining.",
    "author": [
      {
        "name": "Tian-Yuan Huang",
        "url": {}
      },
      {
        "name": "Li Li",
        "url": {}
      },
      {
        "name": "Liying Yang",
        "url": {}
      }
    ],
    "date": "2022-11-11",
    "categories": [],
    "contents": "\n1 Introduction\nCo-word analysis has long been used for knowledge discovery, especially in library and information science (Callon et al. 1986). Based on co-occurrence relationships between words or phrases, this method could provide quantitative evidence of information linkages, mapping the association and evolution of knowledge over time. In conjunction with social network analysis (SNA), co-word analysis could be escalated and yield more informative results, such as topic popularity (Huang and Zhao 2019) and knowledge grouping (Khasseh et al. 2017). Meanwhile, in the area of network science, many community detection algorithms have been proposed to unveil the topological structure of the network (Fortunato 2010; Javed et al. 2018). These methods have then been incorporated into the co-word analysis, assisting to group components in the co-word network. Currently, the co-word analysis based on community detection is flourishing across various fields, including information science, social science and medical science (Hu et al. 2013; Hu and Zhang 2015; Leung et al. 2017; Baziyad et al. 2019).\nFor implementation, interactive software applications, such as CiteSpace (Chen 2006) and VOSviewer (Van Eck and Waltman 2010), have provided freely available toolkits for automatic co-word analysis, making this technique even more popular. Interactive software applications are generally friendlier to users, but they might not be flexible enough for the whole data science workflow. In addition, the manual adjustments could be variant, bringing additional risks to the research reproducibility. In this paper, we have designed a flexible framework for automatic knowledge classification, and presented an open software package akc supported by R ecosystem for implementation. Based on community detection in co-occurrence network, the package could conduct unsupervised classification on the knowledge represented by extracted keywords. Moreover, the framework could handle tasks such as data cleaning and keyword merging in the upstream of data science workflow, whereas in the downstream it provides both summarized table and visualized figure of knowledge grouping. While the package was first designed for academic knowledge classification in bibliometric analysis, the framework is general to benefit a broader audience interested in text mining, network science and knowledge discovery.\n2 Background\nClassification could be identified as a meaningful clustering of experience, turning information into structured knowledge (Kwasnik 1999). In bibliometric research, this method has been frequently used to group domain knowledge represented by author keywords, usually listed as a part of co-word analysis, keyword analysis or knowledge mapping (He 1999; Hu et al. 2013; Leung et al. 2017; Li et al. 2017; Wang and Chai 2018). While all named as (unsupervised) classification or clustering, the algorithm behind could vary widely. For instance, some researches have utilized hierarchical clustering to group keywords into different themes (Hu and Zhang 2015; Khasseh et al. 2017), whereas the studies applying VOSviewer have adopted a weighted variant of modularity-based clustering with a resolution parameter to identify smaller clusters (Van Eck and Waltman 2010). In the framework of akc, we have utilized the modularity-based clustering method known as community detection in network science (Newman 2004; Murata 2010). These functions are supported by the igraph package (Csardi et al. 2006). Main detection algorithms implemented in akc include Edge betweenness (Girvan and Newman 2002), Fastgreedy (Clauset et al. 2004), Infomap (Rosvall and Bergstrom 2007; Rosvall et al. 2009), Label propagation (Raghavan et al. 2007), Leading eigenvector (Newman 2006), Multilevel (Blondel et al. 2008), Spinglass (Reichardt and Bornholdt 2006) and Walktrap (Pons and Latapy 2005). The details of these algorithms and their comparisons have been discussed in the previous studies (Sousa and Zhao 2014; Yang et al. 2016; Garg and Rani 2017; Amrahov and Tugrul 2018).\nIn practical application, the classification result is susceptible to data variation. The upstream procedures, such as information retrieval, data cleaning and word sense disambiguation, play vital roles in automatic knowledge classification. For bibliometric analysis, the author keyword field provides a valuable source of scientific knowledge. It is a good representation of domain knowledge and could be used directly for analysis. In addition, such collections of keywords from papers published in specific fields could provide a professional dictionary for information retrieval, such as keyword extraction from raw text in the title, abstract and full text of literature. In addition to automatic knowledge classification based on community detection in keyword co-occurrence network, the akc framework also provides utilities for keyword-based knowledge retrieval, text cleaning, synonyms merging and data visualization in data science workflow. These tasks might have different requirements in specific backgrounds. Currently, akc concentrates on keyword-based bibliometric analysis of scientific literature. Nonetheless, the R ecosystem is versatile, and the popular tidy data framework is flexible enough to extend to various data science tasks from other different fields (Wickham et al. 2014; Wickham and Grolemund 2016; Silge and Robinson 2017), which benefits both end-users and software developers. In addition, when users have more specific needs in their tasks, they could easily seek other powerful facilities from the R community. For instance, akc provides functions to extract keywords using an n-grams model (utilizing facilities provided by tidytext), but skip-gram modelling is not supported currently. This functionality, on the other hand, could be provided in tokenizers (Mullen et al. 2018) or quanteda (Benoit et al. 2018) package in R. A greater picture of natural language processing (NLP) in R could be found in the CRAN Task View: Natural Language Processing.\n3 Framework\nAn overview of the framework is given in Figure 1. Note that the name akc refers to the overall framework for automatic keyword classification as well as the released R package in this paper. The whole workflow can be divided into four procedures: (1) Keyword extraction (optional); (2) Keyword preprocessing; (3) Network construction and clustering; (4) Results presentation.\n\n\n\nFigure 1: The design of akc framework. Generally, the framework includes four steps, namely: (1) Keyword extraction (optional); (2) Keyword preprocessing; (3) Network construction and clustering; (4) Results presentation.\n\n\n\nKeyword extraction (optional)\nIn bibliometric meta-data entries, the textual information of title, abstract and keyword are usually provided for each paper. If the keywords are used directly, there is no need to do information retrieval. Then we could directly skip this procedure and start from keyword preprocessing. However, sometimes the keyword field is missing, then we would need to extract the keywords from raw text in the title, abstract or full text with an external dictionary. At other times, one might want to get more keywords and their co-occurrence relationships from each entry. In such cases, the keyword field could serve as an internal dictionary for information retrieval in the provided raw text.\nFigure 2 has displayed an example of keyword extraction procedure. First, the raw text would be split into sub-sentences (clauses), which suppresses the generation of cross-clause n-grams. Then the sub-sentences would be tokenized into n-grams. The n could be specified by the users, inspecting the average number of words in keyword phrases might help decide the maximum number of n. Finally, a filter is made. Only tokens that have emerged in the user-defined dictionary are retained for further analysis. The whole keyword extraction procedure could be implemented automatically with keyword_extract function in akc.\n\n\n\nFigure 2: An example of keyword extraction procedure. The raw text would be first divided sentence by sentence, then tokenized to n-grams and yield the target keywords based on a dictionary. The letters are automatically turned to lower case.\n\n\n\nKeyword preprocessing\nIn practice, the textualized contents are seldom clean enough to implement analysis directly. Therefore, the upstream data cleaning process is inevitable. In keyword preprocessing procedure of akc framework, the cleaning part would take care of some details in the preprocess, such as converting the letters to lower case and removing parentheses and contents inside (optional). For merging part, akc help merge the synonymous phrases according to their lemmas or stems. While using lemmatization and stemming might get abnormal knowledge tokens, here in akc we have designed a conversion rule to tackle this problem. We first get the lemmatized or stemmed form of keywords, then group them by their lemma or stem, and use the most frequent keyword in the group to represent the original keyword. This step could be realized by keyword_merge function in akc package. An example could be found in Table 1. After keyword merging, there might still be too many keywords included in the analysis, which poses a great burden for computation in the subsequent procedures. Therefore, a filter should be carried out here, it could exclude the infrequent terms, or extract top TF-IDF terms, or use any criteria that meets the need. Last, a manual validation should be carried out to ensure the final data quality.\n\n\nTable 1: An example of keyword merging rule applied in akc. The keywords with the same lemma or stem would be merged to the highest frequency keyword in the original form.\n\n\nID\n\n\nOriginal form\n\n\nLemmatized form\n\n\nMerged form\n\n\n1\n\n\nhigher education\n\n\nhigh education\n\n\nhigher education\n\n\n2\n\n\nhigher education\n\n\nhigh education\n\n\nhigher education\n\n\n3\n\n\nhigh educations\n\n\nhigh education\n\n\nhigher education\n\n\n4\n\n\nhigher educations\n\n\nhigh education\n\n\nhigher education\n\n\n5\n\n\nhigh education\n\n\nhigh education\n\n\nhigher education\n\n\n6\n\n\nhigher education\n\n\nhigh education\n\n\nhigher education\n\n\n\n\n\nNetwork construction and clustering\nBased on keyword co-occurrence relationship, the keyword pairs would form an edge list for construction of an undirected network. Then the facilities provided by the igraph package would automatically group the nodes (representing the keywords). This procedure could be achieved by using keyword_group function in akc.\nResults presentation\nCurrently, there are two kinds of output presented by akc. One is a summarized result, namely a table with group number and keyword collections (attached with frequency). Another is network visualization, which has two modes. The local mode provides a keyword co-occurrence network by group (use facets in ggplot2), whereas the global mode displays the whole network structure. Note that one might include a huge number of keywords and make a vast network, but for presentation the users could choose how many keywords from each group to be displayed. More details could be found in the following sections.\nThe akc framework could never be built without the powerful support provided by R community. The akc package was developed under R environment, and main packages imported to akc framework include data.table (Dowle and Srinivasan 2021) for high-performance computing, dplyr (Wickham et al. 2022) for tidy data manipulation, ggplot2 (Wickham 2016) for data visualization, ggraph (Pedersen 2021) for network visualization, ggwordcloud (Le Pennec and Slowikowski 2019) for word cloud visualization, igraph (Csardi et al. 2006) for network analysis, stringr (Wickham 2019) for string operations, textstem (Rinker 2018) for lemmatizing and stemming, tidygraph (Pedersen 2022) for network data manipulation and tidytext (Silge and Robinson 2016) for tidy tokenization. Getting more understandings on these R packages could help users utilize more alternative functions, so as to complete more specific and complex tasks. Hopefully, the users might also become potential developers of the akc framework in the future.\n4 Example\nThis section shows how akc can be used in a real case. A collection of bibliometric data of R Journal from 2009 to 2021 is used in this example. The data of this example can be accessed in the GitHub repository. Only the akc package is used in this workflow. First, we would load the package and import the data in the R environment.\n\n\nlibrary (akc)\nrj_bib = readRDS (\"./rj_bib.rds\")\nrj_bib\n\n# A tibble: 568 × 4\n      id title                                           abstr…¹  year\n   <int> <chr>                                           <chr>   <dbl>\n 1     1 Aspects of the Social Organization and Traject… Based …  2009\n 2     2 asympTest: A Simple R Package for Classical Pa… asympT…  2009\n 3     3 ConvergenceConcepts: An R Package to Investiga… Conver…  2009\n 4     4 copas: An R package for Fitting the Copas Sele… This a…  2009\n 5     5 Party on!                                       Random…  2009\n 6     6 Rattle: A Data Mining GUI for R                 Data m…  2009\n 7     7 sos: Searching Help Pages of R Packages         The so…  2009\n 8     8 The New R Help System                           Versio…  2009\n 9     9 Transitioning to R: Replicating SAS, Stata, an… Statis…  2009\n10    10 Bayesian Estimation of the GARCH(1,1) Model wi… This n…  2010\n# … with 558 more rows, and abbreviated variable name ¹​abstract\n\nrj_bib is a data frame with four columns, including id (Paper ID), title (Title of paper), abstract (Abstract of paper) and year (Publication year of paper). Papers in R Journal do not contain a keyword field, thus we have to extract the keywords from the title or abstract field (first step in Figure 1). Here in our case, we use the abstract field as our data source. In addition, we need a user-defined dictionary to extract the keywords, otherwise all the n-grams (meaningful or meaningless) would be extracted and the results would include redundant noise.\n\n\n# import the user-defined dictionary\nrj_user_dict = readRDS (\"./rj_user_dict.rds\")\nrj_user_dict\n\n# A tibble: 627 × 1\n   keyword            \n   <chr>              \n 1 seasonal-adjustment\n 2 unit roots         \n 3 transformations    \n 4 decomposition      \n 5 combination        \n 6 integration        \n 7 competition        \n 8 regression         \n 9 accuracy           \n10 symmetry           \n# … with 617 more rows\n\nNote that the dictionary should be a data.frame with only one column named “keyword”. The user can also use make_dict function to build the dictionary data.frame with a string vector. This function removes duplicated phrases, turns them to lower case and sorts them, which potentially improves the efficiency for the following processes.\n\n\nrj_dict = make_dict (rj_user_dict$keyword)\n\n\nWith the bibliometric data (rj_bib) and dictionary data (rj_dict), we could start the workflow provided in Figure 1.\nKeyword extraction\nIn this step, we need a bibliometric data table with simply two informative columns, namely paper ID (id) and the raw text field (in our case abstract). The parameter dict is also specified to extract only keywords emerging in the user-defined dictionary. The implementation is very simple.\n\n\nrj_extract_keywords = rj_bib %>% \n  keyword_extract (id = \"id\",text = \"abstract\",dict = rj_dict)\n\n\nBy default, only phrases ranging 1 to 4 in length are included as extracted keywords. The user can change this range using parameters n_min and n_max in keyword_extract function. These is also a stopword parameter, allowing users to exclude specific keywords in the extracted phrases. The output of keyword_extract is a data.frame (tibble,tbl_df class provided by tibble package) with two columns, namely paper ID (id) and the extracted keyword (keyword).\nKeyword preprocessing\nFor the preprocessing part, keyword_clean and keyword_merge would be implemented in the cleaning part and merging part respectively. In the cleaning part, the keyword_clean function would: 1) Splits the text with separators (If no separators exist, skip); 2) Removes the contents in the parentheses (including the parentheses, optional); 3) Removes white spaces from start and end of string and reduces repeated white spaces inside a string; 4) Removes all the null character string and pure number sequences (optional); 5) Converts all letters to lower case; 6) Lemmatization (optional). The merging part has been illustrated in the previous section (see Table 1), thus would not be explained again. In the tidy workflow, the preprocessing is implemented via:\n\n\nrj_cleaned_keywords = rj_extract_keywords %>% \n  keyword_clean () %>% \n  keyword_merge ()\n\n\nNo parameters are used in these functions because akc has been designed to input and output tibbles with consistent column names. If the users have data tables with different column names, specify them in arguments (id and keyword) provided by the functions. More details can be found in the help document (use ?keyword_clean and ?keyword_merge in the console).\nNetwork construction and clustering\nTo construct a keyword co-occurrence network, only a data table with two columns (with paper ID and keyword) is needed. All the details have been taken care of in the keyword_group function. However, the user could specify: 1) the community detection function (use com_detect_fun argument); 2) the filter rule of keywords according to frequency (use top or min_freq argument, or both). In our example, we would use the default settings (utilizing Fastgreedy algorithm, only top 200 keywords by frequency would be included).\n\n\nrj_network = rj_cleaned_keywords %>% \n  keyword_group ()\n\n\nThe output object rj_network is a tbl_graph class supported by tidygraph, which is a tidy data format containing the network data. Based on this data, we can present the results in various forms in the next section.\nResults presentation\nCurrently, there are two major ways to display the classified results in akc, namely network and table. A fast way to gain the network visualization is using keyword_vis function:\n\n\nrj_network %>% \n  keyword_vis ()\n\n\n\nFigure 3: Network visualization for knowledge classification of R Journal (2009-2021). The keywords were automatically classified into three groups based on Fastgreedy algorithm. Only the top 10 keywords by frequency are displayed in each group.\n\n\n\nIn Figure 3, the keyword co-occurrence network is clustered into three groups. The size of nodes is proportional to the keyword frequency, while the transparency degree of edges is proportional to the co-occurrence relationship between keywords. For each group, only the top 10 keywords by frequency are showed in each facet. If the user wants to dig into Group 1, keyword_network could be applied. Also, max_nodes parameter could be used to control how many nodes to be showed (in our case, we show 20 nodes in the visualization displayed in Figure 4).\n\n\nrj_network %>% \n  keyword_network (group_no = 1,max_nodes = 20) \n\n\n\nFigure 4: Focus on one cluster of the knowledge network of R journal (2009-2021). Top 20 keywords by frequency are shown in the displayed group.\n\n\n\nAnother displayed form is using table. This could be implemented by keyword_table via:\n\n\nrj_table = rj_network %>% \n  keyword_table () \n\n\nThis would return a data.frame with two columns (see Table 2), namely the group number and the keywords (by default, only the top 10 keywords by frequency would be displayed, and the frequency information is attached).\n\n\nTable 2: Top 10 keywords by frequency in each knowledge classification of R Journal (2009-2021).\n\n\nGroup\n\n\nKeywords (TOP 10)\n\n\n1\n\n\nr package (238); algorithms (117); time (109); software (93); regression (75); number (72); features (60); sets (45); selection (41); simulation (40)\n\n\n2\n\n\nparameters (98); inference (65); framework (58); information (51); distributions (48); performance (47); probability (45); design (44); likelihood (41); optimization (31)\n\n\n3\n\n\npackage (505); model (310); tools (140); tests (48); errors (46); multivariate (42); system (41); hypothesis (18); maps (16); assumptions (15)\n\n\n\n\n\nWord cloud visualization is also supported by akc via ggwordcloud package, which could be implemented by using keyword_cloud function.\nIn our example, we assume R Journal has a large focus on introducing R packages (Group 1 and Group 3 contains “r package” and “package” respectively). Common statistical subjects mentioned in R Journal include “regression” (in Group 1), “optimization” (in Group 2) and “multivariate” (in Group 3). While our example provides a preliminary analysis of knowledge classification in R Journal, an in-depth exploration could be carried out with a more professional dictionary containing more relevant keywords, and more preprocessing could be implemented according to application scenarios (e.g. “r package” and “package” could be merged into one keyword, and unigrams could be excluded if we consider them carrying indistinct information).\n5 Discussion\nThe core functionality of the akc framework is to automatically group the knowledge pieces (keywords) using modularity-based clustering. Because this process is unsupervised, it can be difficult to evaluate the outcome of classification. Nevertheless, the default setting of community detection algorithm was selected after empirical tests via benchmarking. It was found that: 1) Edge betweenness and Spinglass algorithm are most time-consuming; 2) Edge betweenness and Walktrap algorithm could potentially find more local clusters in the network; 3) Label propagation could hardly divide the keywords into groups; 4) Infomap has high standard deviation of node number across groups. In the end, Fastgreedy was chosen as the default community detection algorithm in akc, because its performance is relatively stable, and the number of groups increases proportionally with the network size.\nThough akc currently focuses on automatic knowledge classification based on community detection in keyword co-occurrence network, this framework is rather general in many natural language processing problems. One could utilize part of the framework to complete some specific tasks, such as word consolidating (using keyword merging) and n-gram tokenizing (using keyword extraction with a null dictionary), then export the tidy table and work in another environment. As long as the data follows the rule of tidy data format (Wickham et al. 2014; Silge and Robinson 2017), the akc framework could be easily decomposed and applied in various circumstances. For instance, by considering the nationalities of authors as keywords, akc framework could also investigate the international collaboration behavior in specific domain.\nIn the meantime, the akc framework is still in active development, trying new algorithms to carry out better unsupervised knowledge classification under the R environment. The expected new directions include more community detection functions, new clustering methods, better visualization settings, etc. Note that except for the topology-based community detection approach considering graph structure of the network, there is still another topic-based approach considering the textual information of the network nodes (Ding 2011), such as hierarchical clustering (Newman 2003), latent semantic analysis (Landauer et al. 1998) and Latent Dirichlet Allocation (Blei et al. 2003). These methods are also accessible in R, the relevant packages could be found in the CRAN Task View: Natural Language Processing. With the tidy framework, akc could assimilate more nutrition from the modern R ecosystem, and move forward to create better reproducible open science schemes in the future.\n6 Conclusion\nIn this paper, we have proposed a tidy framework of automatic knowledge classification supported by a collection of R packages integrated by akc. While focusing on data mining based on keyword co-occurrence network, the framework also supports other procedures in data science workflow, such as text cleaning, keyword extraction and consolidating synonyms. Though in the current stage it aims to support analysis in bibliometric research, the framework is quite flexible to extend to various tasks in other fields. Hopefully, this work could attract more participants from both R community and academia to get involved, so as to contribute to the flourishing open science in text mining.\n7 Acknowledgement\nThis study is funded by The National Social Science Fund of China “Research on Semantic Evaluation System of Scientific Literature Driven by Big Data” (21&ZD329). The source code and data for reproducing this paper can be found at: https://github.com/hope-data-science/RJ_akc.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-025.zip\nCRAN packages used\nakc, igraph, tidytext, tokenizers, quanteda, ggplot2, data.table, dplyr, ggraph, ggwordcloud, stringr, textstem, tidygraph, tibble\nCRAN Task Views implied by cited packages\nDatabases, Epidemiology, Finance, GraphicalModels, HighPerformanceComputing, ModelDeployment, NaturalLanguageProcessing, Optimization, Phylogenetics, Spatial, TeachingStatistics, TimeSeries\n\n\nS. E. Amrahov and B. Tugrul. A community detection algorithm on graph data. In 2018 international conference on artificial intelligence and data processing (IDAP), pages. 1–4 2018.\n\n\nH. Baziyad, S. Shirazi, S. Hosseini and R. Norouzi. Mapping the intellectual structure of epidemiology with use of co-word analysis. Journal of Biostatistics and Epidemiology, 2019.\n\n\nK. Benoit, K. Watanabe, H. Wang, P. Nulty, A. Obeng, S. Müller and A. Matsuo. quanteda: An R package for the quantitative analysis of textual data. Journal of Open Source Software, 3(30): 774, 2018.\n\n\nD. M. Blei, A. Y. Ng and M. I. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3: 993–1022, 2003.\n\n\nV. D. Blondel, J.-L. Guillaume, R. Lambiotte and E. Lefebvre. Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment, 2008(10): P10008, 2008.\n\n\nM. Callon, A. Rip and J. Law. Mapping the dynamics of science and technology: Sociology of science in the real world. Springer, 1986.\n\n\nC. Chen. CiteSpace II: Detecting and visualizing emerging trends and transient patterns in scientific literature. Journal of the American Society for Information Science and Technology, 57(3): 359–377, 2006.\n\n\nA. Clauset, M. E. J. Newman and C. Moore. Finding community structure in very large networks. Physical review E, 70: 066111, 2004.\n\n\nG. Csardi, T. Nepusz, et al. The igraph software package for complex network research. InterJournal, Complex Systems, 1695(5): 1–9, 2006.\n\n\nY. Ding. Community detection: Topological vs. topical. Journal of Informetrics, 5(4): 498–514, 2011.\n\n\nM. Dowle and A. Srinivasan. Data.table: Extension of ‘data.frame‘. 2021. URL https://CRAN.R-project.org/package=data.table. R package version 1.14.2.\n\n\nS. Fortunato. Community detection in graphs. Physics Reports, 486(3): 75–174, 2010.\n\n\nN. Garg and R. Rani. A comparative study of community detection algorithms using graphs and R. In 2017 international conference on computing, communication and automation (ICCCA), pages. 273–278 2017. IEEE.\n\n\nM. Girvan and M. E. Newman. Community structure in social and biological networks. Proceedings of the National Academy of Sciences, 99(12): 7821–7826, 2002.\n\n\nQ. He. Knowledge discovery through co-word analysis. Library Trends, 48(1): 133–159, 1999.\n\n\nC.-P. Hu, J.-M. Hu, S.-L. Deng and Y. Liu. A co-word analysis of library and information science in China. Scientometrics, 97(2): 369–382, 2013.\n\n\nJ. Hu and Y. Zhang. Research patterns and trends of Recommendation System in China using co-word analysis. Information Processing & Management, 51(4): 329–339, 2015.\n\n\nT.-Y. Huang and B. Zhao. Measuring popularity of ecological topics in a temporal dynamical knowledge network. PloS One, 14(1): e0208370, 2019.\n\n\nM. A. Javed, M. S. Younis, S. Latif, J. Qadir and A. Baig. Community detection in networks: A multidisciplinary review. Journal of Network and Computer Applications, 108: 87–111, 2018.\n\n\nA. A. Khasseh, F. Soheili, H. S. Moghaddam and A. M. Chelak. Intellectual structure of knowledge in iMetrics: A co-word analysis. Information Processing & Management, 53(3): 705–720, 2017.\n\n\nB. H. Kwasnik. The role of classification in knowledge representation and discovery. Library Trends, 48(1): 22–47, 1999.\n\n\nT. K. Landauer, P. W. Foltz and D. Laham. An introduction to latent semantic analysis. Discourse Processes, 25(2-3): 259–284, 1998.\n\n\nE. Le Pennec and K. Slowikowski. Ggwordcloud: A word cloud geom for ’ggplot2’. 2019. URL https://CRAN.R-project.org/package=ggwordcloud. R package version 0.5.0.\n\n\nX. Y. Leung, J. Sun and B. Bai. Bibliometrics of social media research: A co-citation and co-word analysis. International Journal of Hospitality Management, 66: 35–45, 2017.\n\n\nX. Li, E. Ma and H. Qu. Knowledge mapping of hospitality research- A visual analysis using CiteSpace. International Journal of Hospitality Management, 60: 77–93, 2017.\n\n\nL. A. Mullen, K. Benoit, O. Keyes, D. Selivanov and J. Arnold. Fast, consistent tokenization of natural language text. Journal of Open Source Software, 3(23): 655, 2018.\n\n\nT. Murata. Detecting communities in social networks. In Handbook of social network technologies and applications, Ed B. Furht pages. 269–280 2010. Boston, MA: Springer US. ISBN 978-1-4419-7142-5.\n\n\nM. E. Newman. Fast algorithm for detecting community structure in networks. Physical Review E, 69(6): 066133, 2004.\n\n\nM. E. Newman. Finding community structure in networks using the eigenvectors of matrices. Physical Review E, 74(3): 036104, 2006.\n\n\nM. E. Newman. The structure and function of complex networks. SIAM Review, 45(2): 167–256, 2003.\n\n\nT. L. Pedersen. Ggraph: An implementation of grammar of graphics for graphs and networks. 2021. URL https://CRAN.R-project.org/package=ggraph. R package version 2.0.5.\n\n\nT. L. Pedersen. Tidygraph: A tidy API for graph manipulation. 2022. URL https://CRAN.R-project.org/package=tidygraph. R package version 1.2.1.\n\n\nP. Pons and M. Latapy. Computing communities in large networks using random walks. In International symposium on computer and information sciences, pages. 284–293 2005. Springer.\n\n\nU. N. Raghavan, R. Albert and S. Kumara. Near linear time algorithm to detect community structures in large-scale networks. Physical Review E, 76(3): 036106, 2007.\n\n\nJ. Reichardt and S. Bornholdt. Statistical mechanics of community detection. Physical Review E, 74(1): 016110, 2006.\n\n\nT. W. Rinker. textstem: Tools for stemming and lemmatizing text. Buffalo, New York, 2018. URL http://github.com/trinker/textstem. version 0.1.4.\n\n\nM. Rosvall, D. Axelsson and C. T. Bergstrom. The map equation. The European Physical Journal Special Topics, 178(1): 13–23, 2009.\n\n\nM. Rosvall and C. T. Bergstrom. An information-theoretic framework for resolving community structure in complex networks. Proceedings of the National Academy of Sciences, 104(18): 7327–7331, 2007.\n\n\nJ. Silge and D. Robinson. Text mining with r: A tidy approach. O’Reilly Media, Inc., 2017.\n\n\nJ. Silge and D. Robinson. tidytext: Text Mining and Analysis Using Tidy Data Principles in R. Journal of Open Source Software, 1(3): 2016. URL http://dx.doi.org/10.21105/joss.00037.\n\n\nF. B. de Sousa and L. Zhao. Evaluating and comparing the igraph community detection algorithms. In 2014 brazilian conference on intelligent systems, pages. 408–413 2014. IEEE.\n\n\nN. J. Van Eck and L. Waltman. Software survey: VOSviewer, a computer program for bibliometric mapping. Scientometrics, 84(2): 523–538, 2010.\n\n\nM. Wang and L. Chai. Three new bibliometric indicators/approaches derived from keyword analysis. Scientometrics, 116(2): 721–750, 2018.\n\n\nH. Wickham. ggplot2: Elegant graphics for data analysis. Springer-Verlag New York, 2016. URL https://ggplot2.tidyverse.org.\n\n\nH. Wickham. Stringr: Simple, consistent wrappers for common string operations. 2019. URL https://CRAN.R-project.org/package=stringr. R package version 1.4.0.\n\n\nH. Wickham et al. Tidy data. Journal of statistical software, 59(10): 1–23, 2014.\n\n\nH. Wickham, R. Francois, L. Henry, K. Muller, et al. Dplyr: A grammar of data manipulation. 2022. URL https://CRAN.R-project.org/package=dplyr. R package version 1.0.8.\n\n\nH. Wickham and G. Grolemund. R for data science: Import, tidy, transform, visualize, and model data. O’Reilly Media, Inc., 2016.\n\n\nZ. Yang, R. Algesheimer and C. J. Tessone. A comparative analysis of community detection algorithms on artificial networks. Scientific Reports, 6(1): 1–18, 2016.\n\n\n\n\n",
    "preview": "articles/RJ-2022-025/distill-preview.png",
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {},
    "preview_width": 16000,
    "preview_height": 9000
  },
  {
    "path": "articles/RJ-2022-028/",
    "title": "Quantifying Population Movement Using a Novel Implementation of Digital Image Correlation in the ICvectorfields Package",
    "description": "Movements in imagery captivate the human eye and imagination. They are also of interest in variety of scientific disciplines that study spatiotemporal dynamics. Popular methods for quantifying movement in imagery include particle image velocimetry and digital image correlation. Both methods are widely applied in engineering and materials science, but less applied in other disciplines. This paper describes an implementation of a basic digital image correlation algorithm in R as well as an extension designed to quantify persistent movement velocities in sequences of three or more images. Algorithms are applied in the novel arena of landscape ecology to quantify population movement and to produce vector fields for easy visualization of complex movement patterns across space. Functions to facilitate analyses are available in ICvectorfields \\citep{ICvf}. These methods and functions are likely to produce novel insights in theoretical and landscape ecology because they facilitate visualization and comparison of theoretical and observed data in complex and heterogeneous environments.",
    "author": [
      {
        "name": "Devin W. Goodsman",
        "url": null
      }
    ],
    "date": "2022-10-19",
    "categories": [],
    "contents": "\n1 Introduction\nLiving organisms move through space in complex ways that have inspired many branches of spatial pattern analysis from Turing instabilities (Ruan 1998; Alonso et al. 2002), to complex systems analysis of the emergent properties of individual-level behaviours when organisms live in groups (Parrish and Edelstein-Keshet 1999; Johnson 2009). Moreover, in mathematical ecology there is a long history of deriving analytic expressions for traveling wave speeds from mathematical models of biological systems based on partial differential equations Skellam (1951) and integrodifference equations (Kot et al. 1996). In addition to standard traveling waves and wave-trains, simulation studies have revealed more unusual patterns of population level movement can arise from the way organisms interact and move on the landscape (Hassell et al. 1994). Spiral waves are one example of surprising spatiotemporal dynamics that can arise in biological systems (Hassell et al. 1991).\nTravelling waves and spiral waves emerge from mathematical models of population expansion, which are often based on partial differential equations and integrodifference equations. These types of models, which represent the movement patterns of populations of organisms, are sometimes classified as Eulerian approaches to distinguish them from Lagrangian approaches that focus on the trajectories of individuals. The majority of R packages that quantify organismal movement, however, are Lagrangian as they pertain to the analysis of the tracks or trajectories of individual animals with tracking collars or tracking devices. Integrated step selection models (Avgar et al. 2016), such as those in the amt package (Signer et al. 2019), which incorporate the impact of spatially variable habitat or environmental variables on movement of individuals modeled using a discrete time and discrete space framework, are an example of a Lagrangian approach when fitted to movement data from individuals. Because my focus in this work is on population-level movements that are evident in imagery, I will forego further discussion of Lagrangian models and instead refer the interested reader to a review of R packages for modeling animal movement (Joo et al. 2020). At the time of writing, R packages that focus on the Eulerian approach include IDE (Zammit-Mangion 2019), deSolve (Soetaert et al. 2010), and ReacTran (Soetaert and Meysman 2012). These packages are designed primarily to obtain numerical solutions to Eulerian models, analyze their dynamics, and fit them to data. Recently, movement modeling based on stochastic differential equations, stochastic partial differential equations (Krainski et al. 2018), and other stochastic process models (Buderman et al. 2016) has proliferated. Computationally efficient Bayesian statistical approaches are often required to fit these stochastic models to data due to the ubiquity of noise in spatiotemporal time series in combination with nonlinear dynamical processes (Krainski et al. 2018).\nIn contrast to the R packages and approaches I have cited above, this work is focused on the description of an empirical method for quantifying spatially heterogeneous rates of movement in sequences of images without fitting a model–although I do approach the quantification problem from an Eulerian perspective. Empirically quantifying spread rates without imposing a specific mathematical model allows the user to abandon many of the assumptions implicit in mathematical modeling of population expansion. For example, tractable mathematical models of consumer-resource systems that generate traveling waves, wave-trains, and spiral waves, often rely on assumptions of a homogeneous spatial environment with respect to resources or one in which there are no discontinuities in resources. In contrast, many organisms spread in environments that are spatially heterogeneous (Urban et al. 2008), and in environments subject to persistent directional flows that impact organism movement (Hoffman et al. 2006). The ramifications of this claim are more easily understood using a meteorological analogy. In meteorology, vector fields are frequently used to illustrate the impacts of high and low pressure systems on wind speed and direction, and thus on the movement of weather systems. In such meteorological systems, wind speed and direction are complicated functions of topography and complex atmospheric dynamics. As a result, vector fields representing movement in such systems are often variable at the regional scale, with winds flowing in one direction on one side of a map and possibly in an opposing direction on the other side. In ecology, populations of organisms are like the weather systems in that their movement on the landscape is what is of primary interest to researchers; variable wind causes spatially variable movement of weather systems similarly to how persistent directional fluid flows, including wind, in an organism’s environment impact dispersal, and therefore population movement.\nAt the time of writing, the only tool in R (R Core Team 2021) designed to empirically estimate spreading speed or the speed of wave-trains in populations without fitting a mathematical model is implemented in the ncf R package (Bjornstad 2020). The ncf package relies on lagged non-parametric cross-correlation functions to estimate spreading speed of traveling waves (Bjørnstad and Falck 2001). To do so, it takes two spatiotemporal data sets that differ from one another only in that one is a time-lagged version of the other, and projects their planar coordinates onto lines of varying angles that can be specified using function arguments. After projection onto a line, cross-correlation is estimated using a spline-correlogram approach (Bjørnstad and Falck 2001) and the location of maximum cross-correlation gives an estimate of displacement along the direction of the projection line. This approach was used to estimate the velocity of traveling wave-trains in the larch budmoth system in the European Alps (Bjørnstad et al. 2002).\nProjecting population data from a domain with two spatial coordinates onto a domain that has only one spatial coordinate and then using a correlogram approach precludes quantification of more complicated patterns of movement on the landscape. For example if two spatially separated populations are moving towards one another at the same speed, such an approach will yield enigmatic correlograms. Similarly, if several populations move radially around a central fulcrum, the correlogram will be difficult to translate to an estimate of directional movement.\nIn this paper I present an approach for estimating vector-fields in systems with spatially variable movement that is inspired by a technique from engineering and materials science called Digital Image Correlation or DIC (Anuta 1970; Sutton et al. 2009). Among other things, Digital Image Correlation is used to estimate displacement based on photographs of a planar material before, during, and after a force has been applied to warp its surface (Sutton et al. 2009). A typical DIC approach as well as the extensions described in this paper are implemented in the ICvectorfields package (Goodsman 2021), in which the IC is the abbreviation for Image Correlation. I demonstrate these approaches using the ICvectorfields package to analyze a simulated data set as well as the larch budmoth data set provided with the ncf R package (Bjørnstad et al. 2002; Bjornstad 2020).\n2 Mathematical and Computational Details\nHere I provide mathematical and computational details of the algorithms used in the ICvectorfields R package starting with a standard digital image correlation approach, and following with extensions to estimate persistent movement and to account for spatial variability in persistent movement. The ICvectorfields package capitalizes on the algorithms written in C under the title FFTW which stands for Fastest Fourier Transform in the West , and a convenient wrapper package in R called fftwtools (Rahim 2021). Input raster images and raster stacks are read and manipulated using the terra package (Hijmans 2021).\nDigital image correlation\nOne of the earliest applications of cross-correlation in image analysis was to align images taken from different sensors or at different times using satellites or aircraft (Anuta 1970). The theoretical and computational details I present here loosely follow those in this pioneering application. I will provide the mathematical underpinning of two-dimensional cross-correlation, and then elaborate on its computational implementation, which involves some additional complexity due to the circular nature of discrete fast Fourier transforms. In all descriptions below, I do not normalize the cross-correlation function to obtain Pearson correlation coefficients and therefore, I follow the convention of using the terms cross-correlation and cross-covariance interchangeably.\nGiven two images that have been converted to square matrices \\(f\\) and \\(g\\) of dimension \\(m \\times m\\), two-dimensional cross-correlation can be defined in terms of a convolution:\n\\[\\begin{equation}\n\\left(f \\star g\\right)\\left(x_j, y_i\\right)  =  \\left(\\overline{f\\left(-x_j, -y_i\\right)} * g\\left(x_j, y_i\\right)\\right)\\left(x_j, y_i\\right),\n\\tag{1}\n\\end{equation}\\]\nin which \\(\\left(f \\star g\\right)\\) is the two-dimensional cross-correlation matrix, the \\(*\\) operator denotes convolution, \\(\\overline{f\\left(-x_j, -y_i\\right)}\\) is the complex conjugate of the \\(f\\left(x_j, y_i\\right)\\) matrix, \\(i\\) is the matrix row index, and \\(j\\) is the matrix column index \\(i, j \\in \\mathbb{N} = \\{1, 2, . . .\\}\\). Note that I use array indices that start at one rather than zero. The coordinates of the centroids of each pixel are given by \\(x_j\\) and \\(y_i\\).\nBased on the convolution theorem, equation (1) can be rewritten as\n\\[\\begin{equation}\n\\left(f \\star g\\right)\\left(x_j, y_i\\right)  = \\mathbb{F}^{-1}\\left(\\overline{\\mathbb{F}\\left(f\\left(x_j, y_i\\right)\\right)}\\mathbb{F}\\left(g\\left(x_j, y_i\\right)\\right)\\right)\\left(x_j, y_i\\right),\n\\tag{2}\n\\end{equation}\\]\nwherein \\(\\mathbb{F}\\) denotes the two-dimensional Fourier transform, \\(\\mathbb{F}^{-1}\\) denotes its inverse, and \\(\\overline{\\mathbb{F}\\left(f\\left(x_j, y_i\\right)\\right)}\\) is the complex conjugate of \\(\\mathbb{F}\\left(f\\left(x_j, y_i\\right)\\right)\\). Because \\(\\overline{\\mathbb{F}\\left(f\\left(x_j, y_i\\right)\\right)} = \\mathbb{F}\\left(\\overline{f\\left(-x_j, -y_i\\right)}\\right)\\), and because \\(f\\left(x_j, y_i\\right)\\) contains only real numbers, the complex conjugate can be calculated using matrix multiplication:\n\\[\\begin{equation}\n\\overline{f\\left(-x_j, -y_i\\right)} = r  \\times f \\times r,\n\\tag{3}\n\\end{equation}\\]\nin which the r matrix is a \\(m \\times m\\) matrix that has zeros everywhere except for along the diagonal that runs from its lower left to upper right corners, which contains ones:\n\\[\\begin{equation}\nr =\n\\begin{pmatrix}\n0 & 0 & \\cdots & 0 & 1 \\\\\n0 & 0 & \\cdots & 1 & 0 \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots & \\vdots \\\\\n0 & 1 & \\cdots & 0 & 0\\\\\n1 & 0 & \\cdots & 0 & 0\n\\end{pmatrix}.\n\\tag{4}\n\\end{equation}\\]\nThe matrix calculations in equations (3) and (4) substitute for calculation of the complex conjugate only in the case where all values of the \\(f\\) matrix are real, which is the case in most natural science applications.\nTogether, equations (1) through (4) constitute an elegant way to compute two dimensional cross-correlation. Computer implementation of these, however, requires some additional complexity due to the use of discrete fast Fourier transforms to efficiently compute convolutions. Discrete fast Fourier transforms are inherently circular, which means that what happens on the outer edges of matrices will impact their discrete fast Fourier transform on the opposite side. In order to mitigate this problem, zeros are added to the outer edge on all sides of both the \\(f\\) and \\(g\\) matrices (Anuta 1970). In ICvectorfields, the \\(f\\) and \\(g\\) matrices are padded with as many zeros as there are rows and columns in the original matrix and then additional zeros are added to ensure that both matrices are square with an even number of rows and columns.\nWhen matrices are padded as described above and discrete fast Fourier transforms are used as in equation (2), the non-cyclic components of the convolution end up in the outer corners of the cross-correlation matrix \\(\\left(f \\star g\\right)\\left(x_j, y_i\\right)\\) (Anuta 1970). Thus, to obtain a correct estimate of cross-correlation, \\(\\left(f \\star g\\right)\\left(x_j, y_i\\right)\\) must be divided into four quadrants and each quadrant must be flipped along its horizontal and vertical axes using matrix multiplication. For example, if the zero-padded matrices have dimension \\(k \\times k\\), each quadrant of \\(\\left(f \\star g\\right)\\left(x_j, y_i\\right)\\) will have dimension \\(k/2 \\times k/2\\) and the following operation flips each quadrant matrix:\n\\[\\begin{equation}\nq_f = r \\times q \\times r,\n\\tag{5}\n\\end{equation}\\]\nwhere \\(r\\) is a \\(k/2 \\times k/2\\) matrix as in equation (4). Then the four quadrants can be reassembled into the \\(k \\times k\\) cross-correlation matrix. The mathematical operations in equations (1) through (5) are implemented in the Xcov2D function in the ICvectorfields package.\nOnce a cross-correlation matrix has been estimated, it can be used to compute displacement in the horizontal and vertical directions in terms of the horizontal and vertical shifts in pixel numbers that maximize cross-correlation. In ICvectorfields, shifts to the right and up are designated as positive, whereas shifts to the left and down are considered negative.\nA typical implementation of DIC will define a region of interest within the input images or their corresponding matrices wherein displacement vectors are sought using a bounding box as in the DispFieldbb function in ICvectorfields or using a sub-grid of equal sized regions of interest as in the DispField function in ICvectorfields. Note that all of the functions in ICvectorfields that use DIC or variations of it, translate displacement or velocities in terms of pixel shifts to the spatial units defined in the projection information of the original input rasters. The coordinate information required for translation of pixel shifts to the correct spatial units is obtained using functions in the terra R package (Hijmans 2021).\nExtending DIC to quantify persistent movement\nIn applications of DIC in earth systems with persistent directional flows that influence movement, it is valuable to determine directional movement of populations of interest that persist for more than one time step. In such situations, a spatiotemporal array of images with two space dimensions and one time dimension is required. Often these can be formulated as stacks of raster images, with each layer in the stack representing spatially referenced observations for one time step (step one in Fig. 1). A variation of DIC which I call Spatiotemporal Image Correlation (STIC) permits estimation of persistent directional movement in terms of orthogonal velocity vectors.\nIn STIC, the three dimensional array is first lagged by duplicating it and then removing an integer number of layers from the top of one duplicate and the bottom of the other (steps two and three in Fig. 1. The integer lag is user defined and serves to minimize estimates of zero movement which always would occur in the absence of a lag. To differentiate the duplicate lagged arrays, I will refer to the first as the reference array, and the second as the lagged array. Regions of interest in the reference array are selected and locations outside the region of interest in the reference array are assigned values of zero (grey shaded region in steps two and three in Fig. 1 represent regions of interest). The reference array and the lagged array are then dimension reduced by averaging along rows to obtain one pair of two-dimensional matrices and by averaging along columns to obtain a second pair of two dimensional matrices (step 4 in Fig. 1). The first pair of matrices comprises row-averaged reference and lagged matrices. The second pair of matrices comprises column-averaged reference and lagged matrices. Each matrix in the two pairs has one space dimension and one time dimension.\n\n\n\nFigure 1: The STIC algorithm: The input array in step one is a raster stack of images in which each image layer represents the phenomenon of interest in planar space at a different time instance. In step two, the input array is duplicated and based on a user specified time lag, layers are removed from the top of one array and the bottom of the other. In addition, a region of interest is defined in one of the duplicate arrays represented by the grey shaded region at the top of the prism on the left. In step four the rows are dimension reduced by averaging along one of the axes (either rows or columns). This produces a pair or row-averaged matrices and a pair of column-averaged matrices that are analyzed using cross-correlation to estimate orthogonal velocity vectors.\n\n\n\nCross-correlation between the pairs of reference and lagged matrices is then computed as described for DIC. Recall that one dimension of each of the row or column-averaged matrices is spatial while the other is temporal, which enables calculation of two orthogonal velocity vectors based on space shifts and time shifts obtained by application of DIC:\n\\[\\begin{eqnarray}\nv_x &= s_x/s_{tx}, s_{tx} \\neq 0 \\tag{6}\\\\\nv_y &= s_y/s_{ty}, s_{ty} \\neq 0 \\tag{7}\n\\end{eqnarray}\\]\nin which \\(v_x\\) and \\(v_y\\) are velocity in the horizontal and vertical directions, \\(s_x\\) and \\(s_y\\) are shifts in the horizontal and vertical direction, \\(s_{tx}\\) is the time shift that corresponds to spatial shifts in the horizontal direction, and \\(s_{ty}\\) is the time shift that corresponds to spatial shifts in the vertical direction. Note that due to the time shift, the user-defined time lag does not necessarily pre-determine the denominator of the orthogonal velocity vectors.\nSpatially variable velocities\nWhen the magnitudes of movement velocities are highly spatially variable, a single time lag is not optimal for quantifying orthogonal velocity vectors. For these scenarios a variation on the STIC algorithm called STIC+ allows the user to specify a maximum time lag. The algorithm then repeats the steps described for STIC for each integer time lag from one to the maximum time lag. For each repetition and each location of interest, the total velocity magnitude (speed) is calculated as\n\\[\\begin{equation}\n|v| = \\sqrt{v_x^2 + v_y^2}.\n\\tag{8}\n\\end{equation}\\]\nFor each region of interest, the horizontal and vertical velocity vectors are determined by the time lag STIC calculation that maximizes equation (8).\nA summary table describing which functions in ICvectorfields use each of the algorithms described above is provided (Table 1). Two functions in ICvectorfields use a standard implementation of DIC similar to that described by (Anuta 1970), two functions use the STIC extension and two functions use the STIC+ extension (Table 1).\n\n\n\n\n\nTable 1: ICvectorfields functions, algorithms, and use contexts to facilitate decisions on which function is most applicable. ROI stands for region of interest, which is defined either using a grid or a bounding box, Velocities refers to whether the magnitudes of velocities in the vector field are presumed to be spatially variable or not.\n\n\nfunction\n\n\nalgorithm\n\n\nimages\n\n\nROI\n\n\nvelocities\n\n\nDispField\n\n\nDIC\n\n\n2\n\n\ngrid\n\n\nvariable or not\n\n\nDispFieldbb\n\n\nDIC\n\n\n2\n\n\nbounding box\n\n\nvariable or not\n\n\nDispFieldST\n\n\nSTIC\n\n\n3+\n\n\ngrid\n\n\nless variable\n\n\nDispFieldSTbb\n\n\nSTIC\n\n\n3+\n\n\nbounding box\n\n\nless variable\n\n\nDispFieldSTall\n\n\nSTIC+\n\n\n3+\n\n\ngrid\n\n\nmore variable\n\n\nDispFieldSTbball\n\n\nSTIC+\n\n\n3+\n\n\nbounding box\n\n\nmore variable\n\n\n\n\n\n3 Application\nIn this section I demonstrate the use of DIC and extensions implemented in ICvectorfields (Goodsman 2021) using an example in which data were simulated based on a partial differential equation and using the classic larch budmoth defoliation data that are embedded in the ncf package (Bjornstad 2020). The data from the simulated example are embedded in ICvectorfields. For visualization of results, the demonstrations call functions in several R packages: These include ggplot2 (Wickham 2016), ggnewscale (Campitelli 2021a), metR (Campitelli 2021b), and terra (Hijmans 2021).\nDemonstration using simulated data\nThe model used to simulate data to test and demonstrate the functionality of ICvectorfields is a convection-diffusion equation, which is a partial differential equation with terms for diffusion, advection, and reaction:\n\\[\\begin{equation}\n\\frac{\\partial u}{\\partial t} = \\nabla \\cdot \\left(D \\nabla u \\right) - \\nabla \\cdot \\left( \\mathbf{v} u \\right) + r u,\n\\tag{9}\n\\end{equation}\\]\nin which \\(r\\) is the growth rate with units of per unit time, \\(D\\) is the diffusion coefficient with squared spatial units per unit time, \\(\\mathbf{v}\\) is the bivariate orthogonal velocity vector in units of space per unit time, \\(\\nabla\\) represents the gradient, and \\(\\nabla \\cdot\\) represents divergence. The orthogonal velocity vector is spatially dependent in the simulations that follow:\n\\[\\begin{equation}\n\\mathbf{v} = \\begin{cases} \\tag{10}\n            (0, 0.2), & x < 0, y \\geq 0\\\\\n      (0.2, 0), & x \\geq 0, y \\geq 0\\\\\n      (0, -0.2), & x \\geq 0, y < 0\\\\\n      (-0.2, 0), & x < 0, y < 0\n         \\end{cases}\n\\end{equation}\\]\nNote that by convention in equations (9) and (10), movement to the right and up has a negative sign, and movement to the left and down has a positive sign. This is the opposite convention used in ICvectorfields. Note that the discontinuities at \\(x = 0\\) and \\(y = 0\\) in the advection term in equation (10) create strange model behaviours once concentration reaches \\(x = 0\\) or \\(y = 0\\), and so the model was simulated for only 6 time steps to minimize encounters with these axes. Parameter values for the diffusion coefficient and the growth rate were \\(D = (0.01, 0.01)\\) squared spatial units per unit time and \\(r = 0.5\\) per unit time.\nThe model in equations (9) and (10) was simulated using the ReacTran R package (Soetaert and Meysman 2012), using a finite differencing scheme with backward differencing on a square domain of 202 cells in each direction, each with a width of 0.049 spatial units. The initial condition was a concentration of one units per arbitrary unit of volume in the centre of each quadrant of the spatial domain. Boundary conditions were zero flux (reflecting) on all four sides of the spatial domain. The simulation data are saved in table format within ICvectorfields.\nThe data are imported and then converted from table format to a raster stack using the RastStackData helper function. They can then be visualized as rasters as shown in Fig. 2.\n\n\n\n\n\n# import simulated data\ndata(SimData, package = \"ICvectorfields\")\n\n# convert to raster stack\nSimStack <- ICvectorfields::RastStackData(SimData)\n\n# confirming dimension\n#dim(SimStack)\n\n# plotting\nlayout(matrix(1:6, 2, 3, byrow = TRUE))\n#layout.show(6)\nterra::plot(SimStack[[1]], legend = FALSE, main = \"t1\")\nterra::plot(SimStack[[2]], legend = FALSE, main = \"t2\")\nterra::plot(SimStack[[3]], legend = FALSE, main = \"t3\")\nterra::plot(SimStack[[4]], legend = FALSE, main = \"t4\")\nterra::plot(SimStack[[5]], legend = FALSE, main = \"t5\")\nterra::plot(SimStack[[6]], legend = FALSE, main = \"t6\")\n\n\n\nFigure 2: Visualization of simulation data for six time steps. The initial condition at t0 is not shown. Green colours represent the highest concentrations. The populations in each quadrant move counterclockwise as time steps forward.\n\n\n\nTo analyze displacement based on a pair of images, I chose to use the standard implementation of DIC in the\nDispField function of ICvectorfields. The first two arguments of the DispField function are the input rasters. The first input raster is treated as the reference image and the second is treated as the shifted image. In this case, the first image is the raster layer corresponding to the first time step (t1) and the second image is the raster layer corresponding to the sixth time step (t6). The function selects regions of interests based on a grid of dimensions given in the factv1 and facth1 arguments, which represent to number of rows and columns in each sub-grid. Sub-grids start in the upper left corner and as many sub-grids as fit within the original domain are constructed. In the code below, sub-grids are \\(101 \\times 101\\), which is approximately the size of one quarter of the original spatial domain of the simulation. The restricted argument is by default set to FALSE. In that case, the DIC algorithm cross-correlates each region of interest in the first image with the entirety of the second image. When restricted = TRUE, the algorithm cross correlates both images only within the region of interest. If the user has reason to believe that movement is predominantly occurring within sub-grids the restricted = TRUE option has the added benefit of speeding up computation.\n\n\n# Estimating displacement of simulated data using the DispField function\nVFdf1 <- DispField(SimStack[[1]], SimStack[[6]], factv1 = 101, facth1 = 101, \n                    restricted = TRUE)\n\n\n\n\nTable 2: ICvectorfields output from a call of the DispField function using simulated data. The table is a duplicate of the data table returned after calling the function except that columns 3 through 6 have been omitted so that the table fits within page width limits.\n\n\nrowcent\n\n\ncolcent\n\n\ncentx\n\n\ncenty\n\n\ndispx\n\n\ndispy\n\n\n51\n\n\n51\n\n\n-2.487624\n\n\n2.487624\n\n\n0.0000000\n\n\n-0.9851975\n\n\n152\n\n\n51\n\n\n-2.487624\n\n\n-2.487624\n\n\n0.9851975\n\n\n0.0000000\n\n\n51\n\n\n152\n\n\n2.487624\n\n\n2.487624\n\n\n-0.9851975\n\n\n0.0000000\n\n\n152\n\n\n152\n\n\n2.487624\n\n\n-2.487624\n\n\n0.0000000\n\n\n0.9851975\n\n\n\n\n\nThe output of DispField is in data table format. Because the data table is small, the output is duplicated in Table 2.\nThe directions of movement coincide with the directions of advection in the simulation with movement downwards in the upper left quadrant (first row of Table 2), movement to the right in the lower left quadrant (second row of Table 2), movement to the left in the upper right quadrant (third row of Table 2), and upwards movement in the lower right quadrant (fourth row of Table 2). Speed of movement can be computed by dividing displacement by the number of time steps that passed \\(0.98/5 = 0.196\\), which is slightly slower than the simulated advection speed of 0.2 spatial units per time step. The discrepancy is likely due to the blurring effect of diffusion in the partial differential equation (equation (9)).\nIn situations where the speed is constant, velocity can be estimated from pairs of images as I have demonstrated above. However, the DispFieldST algorithm is designed to return orthogonal velocity vectors and so for confirmation purposes, I demonstrate it below:\n\n\n# Estimating orthogonal velocity vectors of simulated data using the DispFieldST function\nVFdf2 <- DispFieldST(SimStack, lag1 = 1, factv1 = 101, facth1 = 101, restricted = TRUE)\n\n\nThe data table that is returned after running the code above looks similar to the data table duplicated in Table 2 except that under the heading dispx and dispy the algorithm returns horizontal and vertical velocities rather than displacement vectors. The directions of movement are the same as those shown in Table 2, but the speed is 0.196 spatial units per unit time as previously estimated.\nThe vector field and the raw data can be visualized simultaneously using plotting functionality in ggplot2 with extensions in the ggnewscale and metR packages.\n\n\nSimVF <- ggplot() +\n  xlim(c(-5, 5)) +\n  ylim(c(-5, 5)) +\n  geom_raster(data = SimData,\n              aes(x = xcoord, y = ycoord, fill = t1)) +\n  scale_fill_gradient(low = \"white\", high = \"blue\", na.value = NA) +\n  new_scale(\"fill\") +\n  geom_raster(data = SimData,\n              aes(x = xcoord, y = ycoord, fill = t6), alpha = 0.5) +\n  scale_fill_gradient(low = \"white\", high = \"red\", na.value = NA) +\n  geom_vector(data = VFdf2, \n              aes(x = centx, y = centy, \n                  mag = Mag(dispx, dispy), \n                  angle = Angle(dispx, dispy))) + \n  theme(panel.background = element_rect(fill = \"white\", color = \"grey\"),\n        legend.key.size = unit(0.4, 'cm'))\n\nSimVF\n\n\n\nFigure 3: Vector field for radial movement simulated using a convection-diffusion equation. The orthogonal velocity vectors are estimated using the DispFieldST function in the ICvectorfields package.\n\n\n\nThe resulting figure is duplicated in Fig. 3. The velocity vectors in the vector field are consistent with the simulated advection vectors (Fig. 3), although they slightly underestimate movement speed.\nBefore proceeding to the next demonstration I will illustrate one of the potential pitfalls of estimating movement based on cross-correlation. If the argument of the DispFieldST function is left its default restricted = FALSE configuration, the algorithm will search the entire domain for shifts that maximize cross-correlation. Because the simulations in each quadrant of the spatial domain are quite similar, cross-correlation is in fact maximized by shifts that cross quadrants, even though simulated movement was not that large. Therefore, calling DispFieldST with restricted = FALSE produces incorrect output (Table 3): The simulated advection speed is not at all close to the estimated maximum orthogonal advection speed of 3.89 spatial units per unit time.\n\n\n# Estimating orthogonal velocity vectors of simulated data using the DispFieldST function\nVFdf3 <- DispFieldST(SimStack, lag1 = 1, factv1 = 101, facth1 = 101, restricted = FALSE)\n\n\n\n\nTable 3: Output from a call of the DispFieldST function using simulated data. This call is meant to demonstrate a potential pitfall in using the cross-correlation approach because when restricted = FALSE, the algorithm finds positive cross-correlations that are not caused by movement. The table is a duplicate of the data table returned after calling the function except that columns 3 through 6 have been omitted so that the table fits within page width limits.\n\n\nrowcent\n\n\ncolcent\n\n\ncentx\n\n\ncenty\n\n\ndispx\n\n\ndispy\n\n\n51\n\n\n51\n\n\n-2.487624\n\n\n2.487624\n\n\n0.0985198\n\n\n-3.8915302\n\n\n152\n\n\n51\n\n\n-2.487624\n\n\n-2.487624\n\n\n3.8915302\n\n\n0.0985198\n\n\n51\n\n\n152\n\n\n2.487624\n\n\n2.487624\n\n\n-3.8915302\n\n\n-0.0985198\n\n\n152\n\n\n152\n\n\n2.487624\n\n\n-2.487624\n\n\n-0.0985198\n\n\n3.8915302\n\n\n\n\n\nDemonstration using larch budmoth data\nLarch budmoths are lepidopteran defoliators that exhibit periodic outbreaks every 8 to 9 years in the European Alps (Bjørnstad et al. 2002). The larch budmoth data originated from survey information collected by the forest administrative agencies of France, Italy, Switzerland, and Austria from 1961 to 1998. The data record the presence of defoliation by larch budmoth caterpillars within 1 \\(\\times\\) 1 km grid cells (a binary variable). These data were aggregated up to 20 \\(\\times\\) 20 km grid cells so that records at this spatial scale were population proxies for larch budmoth caterpillar abundance (Bjørnstad et al. 2002) based on the assumption that defoliation damage is proportional to the abundance of the causal agents. Grid cells were excluded from the data set if they exhibited less than one percent defoliation or if more than ninety percent of years in which data were collected at that location exhibited no defoliation by larch budmoth (Bjørnstad et al. 2002). The larch budmoth defoliation data exhibit directional traveling wave-trains that travel from the southwest to the northeast along the European Alps (Bjørnstad et al. 2002). These data are embedded in the ncf R package.\nAfter loading the ncf package as well as ICvectorfields, the data can be loaded, converted to a raster stack and visualized as follows:\n\n\n# import larch budmoth data\ndata(lbm, package = \"ncf\")\n\n# convert to raster stack\nLBMStack <- ICvectorfields::RastStackData(lbm)\n\n# confirming dimension\n#dim(LBMStack)\n\n# visualizing\nlayout(matrix(1:6, 2, 3, byrow = TRUE))\n#layout.show(6)\nterra::plot(LBMStack[[1]], legend = FALSE, main = \"1961\")\nterra::plot(LBMStack[[2]], legend = FALSE, main = \"1962\")\nterra::plot(LBMStack[[3]], legend = FALSE, main = \"1963\")\nterra::plot(LBMStack[[4]], legend = FALSE, main = \"1964\")\nterra::plot(LBMStack[[5]], legend = FALSE, main = \"1965\")\n\n\n\nFigure 4: The first five years of the larch budmoth defoliation data included in the ncf package. Green colours represent the highest level of defoliation. Defoliation intensity moves from the southwest to the northeast but not along a straight trajectory between the southwest and northeast corners.\n\n\n\nThis code plots the first five years of the data set (Fig. 4), which show a standard progression of outbreaks from the southwest corner of the Alps to the northeast. This pattern repeats relatively regularly every 8 to 10 years in the data set.\nThe study region covers a large geographic area and so it is likely the population movement speeds vary geographically. For this reason, I elected to use the STIC+ algorithm to analyze the data using DispFieldSTall. In the code below I analyze the first 23 years of the time series (1961 to 1983) as defoliation patterns from 1984 to 1998 are less regular.\n\n\n# calculating velocity field for larch budmoth\nVFdf3 <- DispFieldSTall(LBMStack[[1:23]], lagmax = 3, factv1 = 3, facth1 = 3, restricted = FALSE)\n\n\n\n\n\nFigure 5: Vector field for Larch Budmoth persistent movement. The orthogonal velocity vectors are estimated using the DispFieldSTall function in the ICvectorfields package. Blue colours show the locations and intensities of defoliation in 1962 and red colours show the locations and intensities of defoliation in 1964. Vectors have their own scale that is distinct from the scale of the map. The figure shows that velocities point predominantly to the north on the southwest corner of the map and predominantly to the east on the northeast corner of the map. In addition, outbreaks appear to slow down as they turn the corner from southwest to northeast.\n\n\n\nCalling DispFieldSTall returns a data frame object that is convenient for plotting the vector field. The vector field reveals that moth movement is to the north on the southwestern side of the Alps and then to the east on the northern side of the Alps (Fig. 5). It also reveals deceleration as outbreaks turn the corner and then acceleration as outbreaks move eastward (Fig. 5).\nThe average speed of larch budmoth movement can be computed from the data frame output of DispFieldSTall as follows:\n\n\n# calculating average speed of population movement\nVFdf3$speed <- sqrt((VFdf3$dispx^2) + VFdf3$dispy^2)\n\n# sub-setting to remove locations where speed is zero\nVFdf4 <- subset(VFdf3, speed > 0)\n\n# computing mean, standard deviation and dimension of data frame\n# to obtain sample size\nmean(VFdf4$speed)\n\n[1] 175810\n\n#sd(VFdf4$speed)\n#dim(VFdf4)\n\n# upper and lower Wald-type 95 percent confidence interval on average speed\nmean(VFdf4$speed)/1000 + qt(0.975, dim(VFdf4)[1] - 1)*sd(VFdf4$speed)/1000/sqrt(dim(VFdf4)[1] - 1)\n\n[1] 218.5415\n\nmean(VFdf4$speed)/1000 + qt(0.025, dim(VFdf4)[1] - 1)*sd(VFdf4$speed)/1000/sqrt(dim(VFdf4)[1] - 1)\n\n[1] 133.0786\n\nUsing the approach above, the average movement speed is estimated as \\(176 \\pm 43\\) \\(\\text{km}(\\text{Yr})^{-1}\\), an estimate that is less than the previous speed estimates for northeastern spread of 220 \\(\\text{km}(\\text{Yr})^{-1}\\) (Bjørnstad et al. 2002) and 254 \\(\\text{km}(\\text{Yr})^{-1}\\) (Johnson et al. 2004). The difference between estimates in the literature and estimates produced here are likely due to the direction of movement. The vectors in the larch budmoth vector field point predominantly north and east (Fig. 5. In other words they are orthogonal. In contrast the movement speeds estimated by Bjørnstad et al. (2002) and Johnson et al. (2004) are projected along lines that run to the northeast. A simple application of geometry reveals that an average speed of 176 \\(\\text{km}(\\text{Yr})^{-1}\\) in the north and east directions corresponds to an estimated speed of 249 \\(\\text{km}(\\text{Yr})^{-1}\\) in the northeastern direction (Pythagorean theorem \\(\\sqrt{176^2 + 176^2}\\)). This estimate is consistent with prior speed estimates for larch budmoth population movement (Bjørnstad et al. 2002; Johnson et al. 2004).\n4 Summary\nThe ICvectorfields R package implements standard Digital Image Correlation algorithms in addition to a novel extension that permits estimation of orthogonal velocities of persistent movement in series of three or more images. Here I demonstrate the usefulness of DIC and the extension implemented in ICvectorfields in a new arena: Whereas DIC is often applied in engineering and materials science to quantify the effects of force application on materials (Sutton et al. 2009), it has not been used in landscape ecology. In this field, the approach has potential to provide new insights into how populations move across landscapes and to demonstrate the untenable nature of assumptions of homogeneity inherent in most analyses based on the traveling wave paradigm. Even when models of sufficient complexity to capture environmental heterogeneity can be used, I expect that the methods in ICvectorfields will be useful because they facilitate comparison between modeled and empirical population movement data as demonstrated in the partial differential equation example in this study. Approaches such as this one that estimate movement based on cross-correlation, however, have a weakness: Under certain circumstances, they are prone to finding cross-correlations that are unrelated to movement as was demonstrated in this paper. For this reason, users must exercise vigilance in interpreting the results of vector field analyses like those demonstrated herein. If possible, results should be checked against a standard or against prior published results regarding movement propensity. Nevertheless, the methods described here hold promise for exploratory analyses, hypothesis generation, and synoptic pattern analyses of population movements.\n5 Acknowledgements\nI am grateful for constructive comments from two anonymous reviewers as well as for the patience of editors at The R Journal, which enabled this work navigate the peer review process.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-028.zip\nCRAN packages used\nICvectorfields, amt, IDE, deSolve, ReacTran, ncf, fftwtools, terra, ggplot2, ggnewscale, metR\nCRAN Task Views implied by cited packages\nDifferentialEquations, Epidemiology, Hydrology, Phylogenetics, Spatial, SpatioTemporal, TeachingStatistics, Tracking\n\n\nD. Alonso, F. Bartumeus and J. Catalan. Mutual interference between predators can give rise to turing spatial patterns. Ecology, 83(1): 28–34, 2002.\n\n\nP. E. Anuta. Spatial registration of multispectral and multitemporal digital imagery using fast fourier transform techniques. IEEE transactions on Geoscience Electronics, 8(4): 353–368, 1970.\n\n\nT. Avgar, J. R. Potts, M. A. Lewis and M. S. Boyce. Integrated step selection analysis: Bridging the gap between resource selection and animal movement. Methods in Ecology and Evolution, 7(5): 619–630, 2016.\n\n\nO. N. Bjornstad. Ncf: Spatial covariance functions. 2020. URL https://CRAN.R-project.org/package=ncf. R package version 1.2-9.\n\n\nO. N. Bjørnstad and W. Falck. Nonparametric spatial covariance functions: Estimation and testing. Environmental and Ecological Statistics, 8(1): 53–70, 2001.\n\n\nO. N. Bjørnstad, M. Peltonen, A. M. Liebhold and W. Baltensweiler. Waves of larch budmoth outbreaks in the european alps. Science, 298(5595): 1020–1023, 2002.\n\n\nF. E. Buderman, M. B. Hooten, J. S. Ivan and T. M. Shenk. A functional model for characterizing long-distance movement behaviour. Methods in Ecology and Evolution, 7(3): 264–273, 2016.\n\n\nE. Campitelli. Ggnewscale: Multiple fill and colour scales in ’ggplot2’. 2021a. URL https://CRAN.R-project.org/package=ggnewscale. R package version 0.4.5.\n\n\nE. Campitelli. metR: Tools for easier analysis of meteorological fields. 2021b. URL https://github.com/eliocamp/metR. R package version 0.9.2.\n\n\nD. Goodsman. ICvectorfields: Vector fields from spatial time series of population abundance. 2021. URL https://CRAN.R-project.org/package=ICvectorfields. R package version 0.0.2.\n\n\nM. P. Hassell, H. N. Comins and R. M. May. Species coexistence and self-organizing spatial dynamics. Nature, 370(6487): 290–292, 1994.\n\n\nM. P. Hassell, H. N. Comins and R. M. Mayt. Spatial structure and chaos in insect population dynamics. Nature, 353(6341): 255–258, 1991.\n\n\nR. J. Hijmans. Terra: Spatial data analysis. 2021. URL https://CRAN.R-project.org/package=terra. R package version 1.2-10.\n\n\nA. L. Hoffman, J. D. Olden, J. B. Monroe, N. LeRoy Poff, T. Wellnitz and J. A. Wiens. Current velocity and habitat patchiness shape stream herbivore movement. Oikos, 115(2): 358–368, 2006.\n\n\nD. M. Johnson, O. N. Bjørnstad and A. M. Liebhold. Landscape geometry and travelling waves in the larch budmoth. Ecology Letters, 7(10): 967–974, 2004.\n\n\nN. Johnson. Simply complexity: A clear guide to complexity theory. Simon; Schuster, 2009.\n\n\nR. Joo, M. E. Boone, T. A. Clay, S. C. Patrick, S. Clusella-Trullas and M. Basille. Navigating through the r packages for movement. Journal of Animal Ecology, 89(1): 248–267, 2020.\n\n\nA. N. Kolmogorov. Étude de l’équation de la diffusion avec croissance de la quantité de matière et son application à un problème biologique. Bull. Univ. Moskow, Ser. Internat., Sec. A, 1: 1–25, 1937.\n\n\nM. Kot, M. A. Lewis and P. van den Driessche. Dispersal data and the spread of invading organisms. Ecology, 77(7): 2027–2042, 1996.\n\n\nE. Krainski, V. Gómez-Rubio, H. Bakka, A. Lenzi, D. Castro-Camilo, D. Simpson, F. Lindgren and H. Rue. Advanced spatial modeling with stochastic partial differential equations using r and INLA. Chapman; Hall/CRC, 2018.\n\n\nJ. K. Parrish and L. Edelstein-Keshet. Complexity, pattern, and evolutionary trade-offs in animal aggregation. Science, 284(5411): 99–101, 1999.\n\n\nR Core Team. R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing, 2021. URL https://www.R-project.org/.\n\n\nK. Rahim. Fftwtools: Wrapper for ’FFTW3’ includes: One-dimensional, two-dimensional, three-dimensional, and multivariate transforms. 2021. URL https://CRAN.R-project.org/package=fftwtools. R package version 0.9-11.\n\n\nS. Ruan. Turing instability and travelling waves in diffusive plankton models with delayed nutrient recycling. IMA journal of applied mathematics, 61(1): 15–32, 1998.\n\n\nJ. Signer, J. Fieberg and T. Avgar. Animal movement tools (amt): R package for managing tracking data and conducting habitat selection analyses. Ecology and Evolution, 9: 880–890, 2019. URL https://doi.org/10.1002/ece3.4823.\n\n\nJ. G. Skellam. Random dispersal in theoretical populations. Biometrika, 38(1/2): 196–218, 1951.\n\n\nK. Soetaert and F. Meysman. Reactive transport in aquatic ecosystems: Rapid model prototyping in the open source software r. Environmental Modelling & Software, 32: 49–60, 2012.\n\n\nK. Soetaert, T. Petzoldt and R. W. Setzer. Solving differential equations in R: Package deSolve. Journal of Statistical Software, 33(9): 1–25, 2010. DOI 10.18637/jss.v033.i09.\n\n\nM. A. Sutton, J. J. Orteu and H. Schreier. Image correlation for shape, motion and deformation measurements: Basic concepts, theory and applications. Springer Science & Business Media, 2009.\n\n\nM. C. Urban, B. L. Phillips, D. K. Skelly and R. Shine. A toad more traveled: The heterogeneous invasion dynamics of cane toads in australia. The American Naturalist, 171(3): E134–E148, 2008.\n\n\nH. Wickham. ggplot2: Elegant graphics for data analysis. Springer-Verlag New York, 2016. URL https://ggplot2.tidyverse.org.\n\n\nA. Zammit-Mangion. IDE: Integro-difference equation spatio-temporal models. 2019. URL https://CRAN.R-project.org/package=IDE. R package version 0.3.0.\n\n\n\n\n",
    "preview": "articles/RJ-2022-028/distill-preview.png",
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "articles/RJ-2022-034/",
    "title": "iccCounts: An R Package to Estimate the Intraclass Correlation Coefficient for Assessing Agreement with Count Data",
    "description": "The intraclass correlation coefficient (ICC) is a widely used index to assess agreement with continuous data. The common approach for estimating the ICC involves estimating the variance components of a linear mixed model under assumptions such as linearity and normality of effects. However, if the outcomes are counts these assumptions are not met and the ICC estimates are biased and inefficient. In this situation, it is necessary to use alternative approaches that are better suited for count data. Here, the iccCounts R package is introduced for estimating the ICC under the Poisson, Negative Binomial, Zero-Inflated Poisson and Zero-Inflated Negative Binomial distributions. The utility of the iccCounts package is illustrated by three examples that involve the assessment of repeatability and concordance with count data.",
    "author": [
      {
        "name": "Josep L. Carrasco",
        "url": "https://webgrec.ub.edu/webpages/personal/ang/005037_jlcarrasco.ub.edu.html"
      }
    ],
    "date": "2022-10-19",
    "categories": [],
    "contents": "\n1 Introduction\nRepeated measurements are often collected and hierarchically structured in clusters (commonly, in subjects). These repeated measurements can be interchangeable within subjects (i.e. they are replicates). This case is often known as the evaluation of repeatability (Nakagawa and Schielzeth 2010) or intra-rater reliability (DeVet et al. 2011). Moreover, the repeated measurements may be structured (not interchangeable) because they were obtained under different experimental conditions, involving different methods or observers. In this case the analysis of agreement is often known as concordance analysis, method comparison analysis (Choudhary and Nagaraja 2017) or inter-rater reliability (DeVet et al. 2011).\nWhatever the structure of the repeated measurements, the intraclass correlation coefficient (ICC) is a common index used to assess agreement with continuous data (Fleiss 1986; Carrasco and Jover 2003). The general definition of the ICC is the ratio of the between-clusters variance to the total variance. However, the appropriate ICC has to be defined to afford the different variance components that are involved in the total variance besides the between-clusters variance. These variance components are typically estimated by means of a linear mixed model with common assumptions: that there is linearity between the outcome expectation and the effects (cluster, method,…); and that the random effects and the random error follow normal distributions. Currently, there are several R packages that estimate the ICC under the Normality assumption for assessing repeatability or concordance (Wolak et al. 2012; Carrasco et al. 2013; Stoffel et al. 2017).\nHowever, if the outcomes are counts, such assumptions are not met and the ICC estimates are biased and inefficient (Carrasco and Jover 2005). In this situation, it is necessary to use alternative approaches that are better suited to the properties of count data. The methodology for estimating the ICC for non-normal distributions using generalized linear mixed models (GLMM), and in particular for count data, was developed in Carrasco (2010). The ICC is therefore estimated by the variance components from the appropriate GLMM. The cluster random effect is still distributed as a Normal distribution but the within-cluster variability is assumed to follow a probability distribution function for counts. Stoffel et al. (2017) introduced the rptR package which can be used to estimate the ICC assuming a Poisson model for the within-subjects variability.\nIn the iccCounts package introduced here, besides the Poisson distribution, other models as the Negative Binomial, the Zero-Inflated Poisson and the Zero-Inflated Negative Binomial are also considered. These models are useful when overdispersion arises in the Poisson model. Overdispersion means that the variability assumed by the model is lower than that from the data. Therefore, the within-subjects variability and, by extension, the total variance are underestimated and the ICC and its standard error are biased. Thus, the validity of the ICC estimate is closely linked to the validity of the model, so that a goodness-of-fit (GOF) analysis of the model must be performed.\nThe article is structured as follows: the section introduces the definition of the ICC, their expressions depending on the model GLLM chosen, some inferential aspects and the validation approach of the GLMM; the issues of the package are described in section; in section three examples are introduced. Two of them are cases of the repeatability setting whereas the remaining one shows the case of a concordance setting. Finally, the main contributions are summarized in the section.\n2 Methodology background\nExperimental design\nAs mentioned in the introduction, the experimental design depends on the aim of the study: concordance or repeatability. In the case of a concordance study, a sample of n subjects are measured m times by k methods. In this setting, the aim is to analyse the degree of concordance of the measurement methods when assessing the subjects. Note that within-subjects repeated measurements are not interchangeable because they belong to one specific method. It is worthy to noting that the term “methods” is the conventional way to describe the experimental condition of the repeated measurements across subjects in this context, but it might be referred to differently name depending on the context. In this setting, \\(Y_{ijk}\\) stands for the k-th reading made by the j-th method on the i-th subject, with \\(i=1,\\ldots,n\\), \\(j=1,\\ldots,m\\) and \\(k=1,\\ldots,s\\). Hence, there will be three variance components to consider when assessing agreement among the repeated measurements: between-subjects, between-methods and random error variabilities.\nIn a repeatability study, a sample of n subjects are measured m times. In this case the repeated measurements share the same experimental condition across subjects, therefore they can be considered as interchangeable. Thus, in this setting \\(Y_{ik}\\) stands for the k-th reading made on the i-th subject, with \\(i=1,\\ldots,n\\), and \\(k=1,\\ldots,s\\). In this case, only two variance components are involved in the evaluation of the agreement: between-subjects and random error variabilities.\nGeneralized linear mixed model\nThe estimation of the variance components is carried out by means of generalized linear mixed models (GLMM). The GLMM for the concordance setting (considering subjects and methods effects) is defined as follows:\nLet \\(\\alpha_i\\) and \\(\\beta_j\\) be the subjects and methods random effects respectively, with \\(i=1,\\ldots,n\\), \\(j=1,\\ldots,m\\), that follow Normal distributions with mean 0 and variance \\(\\sigma_{\\alpha}^2\\) and \\(\\sigma_{\\beta}^2\\). Although the method effect could be a fixed effect by design, when defining the agreement index it is convenient to consider it as a random effect to account for the systematic differences between observers as a source of disagreement (Fleiss 1986; Carrasco and Jover 2003).\nThe conditional distribution of \\(Y_{ijk}\\) given \\(\\alpha_i\\) and \\(\\beta_j\\), \\(f\\left(Y_{ijk}|\\alpha_i,\\beta_j\\right)\\), is a probability density function from the exponential family.\nThe conditional mean of \\(Y_{ijk}\\) given \\(\\alpha_i\\) and \\(\\beta_j\\) is\n\\[\\begin{equation}\n\\mu_{ij}=E\\left(Y_{ijk}|\\alpha_i,\\beta_j\\right)=g^{-1}\\left(\\lambda_i+\\alpha_i+\\beta_j\\right).\n\\tag{1}\n\\end{equation}\\]\nwhere \\(g\\) is called the link function. Here, \\(\\lambda_i\\) is the linear combination of the mean modifying covariates for the i-th subject.\nFurthermore, the conditional variance of \\(Y_{ijk}\\) given \\(\\alpha_i\\) and \\(\\beta_j\\) is defined as \\(Var\\left(Y_{ijk}|\\alpha_i,\\beta_j\\right)=\\phi h\\left(\\mu_{ij}\\right)\\) where \\(\\phi\\) is the dispersion parameter and \\(h\\left(\\right)\\) is variance function.\nIf the methods effect is removed, the GLMM for the repeatability setting is obtained.\nThus, depending on the nature of the data the appropriate conditional probability model and link function must be chosen. When analysing count data, the logarithm is commonly used as a link function and models such as Poisson or Negative Binomial are considered.\nIntraclass correlation coefficient for count data\nThe intraclass correlation coefficient (ICC) is calculated as:\n\\[\\begin{equation}\nICC=\\frac{Cov\\left(Y_{ijk},Y_{ij'k'}\\right)}{Var\\left(Y_{ijk}\\right)}.\n\\tag{2}\n\\end{equation}\\]\nwhere the \\(Cov\\left(Y_{ijk},Y_{ij'k'}\\right)\\) is the marginal (over subjects and observers) covariance between any pair of data from the same subject, whereas \\(Var\\left(Y_{ijk}\\right)\\) is the marginal variance of data.\nFurthermore, the marginal variance and covariance can be developed as functions of the GLMM parameters (Carrasco 2010):\n\\[\\begin{equation}\nICC=\\frac{Cov\\left(\\mu_{ij},\\mu_{ij'}\\right)}{Var\\left(\\mu_{ij}\\right)+E\\left(\\phi h\\left(\\mu_{ij}\\right)\\right)}\n\\tag{3}\n\\end{equation}\\]\nThis result allows the ICC to be generalized to any distribution fitted with a GLMM.\nCarrasco (2010) developed the ICC for Poisson and Negative Binomial distributions, the latter with variance increasing quadratically with the mean (NegBin2), \\(Var\\left(Y_{ijk}|\\alpha_i,\\beta_j\\right)=\\mu_{ij} \\left(1+r\\mu_{ij}\\right)\\) (Table ??). A Negative Binomial model with variance increasing linearly with the mean is also considered (NegBin1) (Hardin and Hilbe 2007; Brooks et al. 2017), \\(Var\\left(Y_{ijk}|\\alpha_i,\\beta_j\\right)=\\mu_{ij} \\left(1+r\\right)\\). It is worth to noting that NegBin1 does not belong to the exponential family (Hardin and Hilbe 2007), therefore this model would not be a proper GLMM. However, it can still be useful to model count data that show overdispersion in a Poisson model.\nAdditionally, it is possible to define the ICC for the cases of zero inflated models (Table ??). Let’s define \\(B_{ijk}\\) as a Bernoulli variable that takes a value of 1 if the reading \\(k\\) on subject \\(i\\) and method \\(j\\) is a structural zero with probability \\(\\pi\\) and 0 otherwise. The observed data, \\(X_{ijk}\\) is the result of \\(X_{ijk}=Y_{ijk}\\left(1-B_{ijk}\\right)\\), where \\(Y_{ijk}\\) is the count variable as defined before. The marginal covariance and variance of \\(X_{ijk}\\) are:\n\\[\\begin{equation}\nCov\\left(X_{ijk},X_{ij'k'}\\right)=Cov\\left(Y_{ijk},Y_{ij'k'}\\right)\\left(1-\\pi\\right)^2\n\\tag{4}\n\\end{equation}\\]\n\\[\\begin{equation}\nVar\\left(X_{ijk}\\right)=Var\\left(Y_{ijk}\\right)\\left(1-\\pi\\right)+E^2\\left(Y_{ijk}\\right)\\pi\\left(1-\\pi\\right)\n\\tag{5}\n\\end{equation}\\]\nand the general expression of the ICC for zero-inflated data becomes:\n\\[\\begin{equation}\nICC=\\frac{Cov\\left(Y_{ijk},Y_{ij'k'}\\right)\\left(1-\\pi\\right)}{Var\\left(Y_{ijk}\\right)+E^2\\left(Y_{ijk}\\right)\\pi}\n\\tag{6}\n\\end{equation}\\]\nwhere \\(\\pi\\) stands for the probability of excess of zeros.\nNotice that ICCs appearing in Table ?? are for the concordance setting where \\(\\sigma^2_\\beta\\) stands for the variability between methods. The ICCs for the repeatability setting are obtained just removing \\(\\sigma^2_\\beta\\) from the equations, i.e. by setting \\(\\sigma^2_\\beta=0\\).\nEstimation of ICC\nThe estimation of the ICC involves estimating the GLMM parameters. However, maximum likelihood approach is not straightforward because there is no closed analytical expression for the marginal likelihood besides the Normal case (linear mixed model). Thus, it is necessary to apply numerical methods to approximate the marginal likelihood and to obtain maximum likelihood estimates (Bolker et al. 2009).\nWith regards to the standard error of the ICC, let \\(\\theta\\) be the GLMM parameters involved in the ICC expression (see Table ??) and \\(\\Sigma\\) the variance-covariance matrix of \\(\\theta\\). The asymptotic standard error can be estimated by applying the delta method (Hoef 2012):\n\\[\\begin{equation}\nVar\\left(ICC\\right) \\approx \\Delta'\\Sigma\\Delta\n\\tag{7}\n\\end{equation}\\]\nwhere \\(\\Delta\\) stand for the vector containing the derivatives of ICC respect to \\(\\theta\\). Confidence intervals for the ICC are based on asymptotic Normal distribution using the inverse hyperbolic tangent function or Fisher’s Z-transformation (Carrasco and Jover 2003).\nValidation\nThe goodness-of-fit (GOF) analysis can be carried out by the computation of randomized quantile residuals (RQR) (Dunn and Smyth 1996; Feng et al. 2020). Briefly, the GOF analysis involve the comparison of the RQR from the original data to those obtained by simulation under the fitted model. Simulations of counts based on the fitted model are generated and the model is refitted to each simulated dataset. Using the simulated RQR, envelopes are built as the appropriate quantiles (in relation to the level of significance) of RQR from the refitted models. If the model fits correctly the data it is expected that the original RQR will completely lie within the simulated envelopes.\nAdditionally, dispersion as well as zero-inflation can be checked by comparing the dispersion and proportion of zeros from the simulated data to those from the original data. Thus, tests for dispersion and zero inflation are carried out by comparing the RQR dispersion and the number of zeros from the original model and data to those from the refitted models and simulated data.\n3 Package description\nThe main function in the iccCounts package is which estimates the ICC under different models for count data. The argument identifies the data set to be analysed. This data set has to be a object with at at least two columns: outcome and subject identifier (arguments and respectively).\nIn the case of estimating the ICC for the concordance setting, a third column with the method identifier must be provided (the argument ). The argument is used to identify the setting in which the ICC should be estimated. Valid values are: (default) for the repeatability setting; and for the concordance setting. The repeatability setting requires that repeated measurements are interchangeable within subjects. This means the experimental conditions of the measurements are the same (replicates), and they come from the same probability distribution function (conditioned to subjects). On the other hand, in the concordance setting the repeated measurements are not interchangeable because their experimental conditions are different, and therefore their probability distribution function, conditioned to subjects, is different (commonly in the mean).\nThe argument is used to identify the within-subjects probability model. Valid options are: (default) for Poisson model; and for Negative Binomial model with variance increasing linearly and quadratically with the mean respectively; for zero-inflated Poisson model; and for zero-inflated Negative Binomial model with variance increasing linearly and quadratically with the mean.\nOnce the appropriate setting and model have been chosen, the GLMM is estimated by maximum likelihood via Laplace approximation using the glmmTMB package (Brooks et al. 2017). The output of the function is an object of class which is a list with the following components: which contains the generalized linear mixed model estimates; which includes the ICC estimate and its standard error and confidence interval; and with the variance components and parameters related to the ICC. Finally, the function runs the goodness of fit (GOF) analysis of the GLMM fitted to data. This function has three arguments: to denote the object to apply the GOF analysis; the argument that stands for the number of simulations to run which default value is set to 100; and the \\(\\alpha\\) argument to set the level of significance.\nThe output of is an object of class which is a list with the following components: , a plot of RQR envelopes with the original RQR; , a plot of the simulated RQR dispersion; , a plot of the count of zeros in the simulated datasets; , the dispersion of RQR from the original sample; , the proportion of simulated RQR dispersion that are greater than the original dispersion that can be interpreted as a simulated P-value to check the goodness of fit on dispersion; , the count of zeros in the original sample; and , the proportion of simulated zero count that are greater than that of the original sample. It can be interpreted as a simulated P-value to check the hypothesis of zero-inflation. The plots in the list are objects of class , hence users may change the plot themes or add modifications to the components.\nAdditionally, to describe the differences among the repeated measurements from the same subjects, the function draws the Bland-Altman plot (Bland and Altman 1995). The difference between each pair of data from the same subject is represented on the y-axis. The mean of data from the same subject is represented on the x-axis. Additionally, a bar plot with the proportions of differences can be drawn. This plot is a useful way to describe the differences when the range of observed values is small relative to the number of observations (Smith et al. 2010).\nThe arguments of function are: , a data frame containing at least the columns of the outcome and subject’s identifier; , a character string indicating the name of the outcome column in the data set; a character string indicating the name of the subjects column in the data set; , a character string indicating the name of the column that stands for the repeated measurements in the data set. This argument is only needed to identify the differences; , argument used to choose the plot to be drawn. Valid values are: (default) for the Bland-Altman plot; and for the bar plot of the differences. Besides the plots, the function provides a dataframe object that contains the data used to generate the plot.\n4 Examples\nThe package includes three real data sets as examples that covers the repeatability and concordance settings.\nSparrow fledglings paternity example\nIn the Sparrow fledglings paternity example, the incidence of extra-pair paternity (EPP) was monitored over 3 breeding seasons in a sparrow colony in Lundy, an island off the southwest coast of England (Schroeder et al. 2012). One of the aims of the study was to assess the repeatability of counts of fledglings that a male had in every breeding season. Thus, the repeated measurements are assumed to be exchangeable replicates. However, the means of the Social variable by year seem to differ:\n\n\nlibrary(iccCounts)\nlibrary(dplyr)\nEPP %>% group_by(Year) %>% summarize(Mean=mean(Social),SD=sd(Social))\n\n# A tibble: 3 × 3\n   Year  Mean    SD\n  <int> <dbl> <dbl>\n1  2003  3.19  3.10\n2  2004  2.53  2.21\n3  2005  4.5   2.79\n\nIn case these means were significantly different the repeated measurements could not be considered as exchangeable, and consequently the differences among the means of the repeated measurements should be included in the agreement index. This led us to the concordance setting considering Year as the methods effect.\nThe first model to consider is the Poisson model. The default option in the function is the Poisson model, and so it is necessary to specify the name of the data set, the count variable (Social), the subjects identifier (id), the methods variable (Year), and the concordance setting (type=“con”).\n\n\nEPP_P<-icc_counts(EPP,y=\"Social\",id=\"id\",met=\"Year\",type=\"con\")\nICC(EPP_P)\n\n           ICC    SE ICC 95% CI LL 95% CI UL\n[1,] 0.5284404 0.0866857 0.3383706 0.6770822\n\nVarComp(EPP_P)\n\n       mu     BSVar     BMVar\n 3.172783 0.4002965 0.0798841\n\nThe function applied to the object shows that the ICC estimate is 0.53 (95% confidence interval: 0.34 - 0.68). Moreover, the function gives the parameters involved in the ICC estimator, which in this case are the overall mean, the between-subjects variance and the between-methods variability (the two latter in log-scale).\nHowever, as mentioned in the previous section, the validity of the ICC estimate is linked to the validity of the model. The function is applied to the object to run the simulations and to compute the RQR. The random seed is set for the sake of the reproducibility of the example.\n\n\nset.seed(100)\nEPP_P.gof <- GOF_check(EPP_P)\n\n\n\n\nplot(EPP_P.gof)\n\n\nFigure 1a shows the plot of RQR with envelopes generated by simulation. Points on the plot stand for the RQR from the original sample. Notice that a substantial number of points lie outside the envelopes, indicating the fit of the model is unsuitable. The next plot (Figure 1c) shows the density of the RQR variances computed in the simulated samples. The RQR variance from the initial sample is 2.15 (shown inside the square) which is extreme compared to those from the simulations. Indeed, the proportion of simulated variances that are higher than that from the initial sample can be interpreted as a p-value generated by Monte Carlo simulation. This p-value is shown by applying the function to the object.\n\n\nDispersionTest(EPP_P.gof)\n\n        S    P_value\n 2.077549 0.00990099\n\nAdditionally, the Social variable has a considerable proportion of zero values (\\(26.4\\%\\)), and so the excess of zeros could be the cause of the unsuitable fitting of the Poisson model. To check this hypothesis, the third plot generated shows the proportion of zeros in the simulated data sets (Figure 1e). The count of zeros in the sample is 51, which exceeds the expected count under the Poisson model. Again, the proportion of simulated zero counts that are higher than that from the initial sample can be interpreted as a p-value generated by Monte Carlo simulation. This p-value is obtained by applying the function to the object.\n\n\nZeroTest(EPP_P.gof)\n\n Count    P_value\n    51 0.00990099\n\nThus, it is necessary to use a model able to provide a proportion of zeros higher than that expected under the Poisson assumption. This model could be the Zero-Inflated Poisson (ZIP) model.\n\n\nEPP_ZIP<-icc_counts(EPP,y=\"Social\",id=\"id\",met=\"Year\",type=\"con\",fam=\"zip\")\nICC(EPP_ZIP)\n\n           ICC    SE ICC 95% CI LL 95% CI UL\n[1,] 0.0477628 0.0362002  -0.02331 0.1183553\n\nVarComp(EPP_ZIP)\n\n       mu    BSVar     BMVar        Pi\n 4.487117 0.033328 0.0328427 0.2446678\n\nIn this case, the ICC is much lower than in the Poisson model (0.05, 95% CI: -0.02, 0.12) indicating a non-significant ICC. The ICC components are the same as those in the Poisson case (with different values) plus the proportion of excess of zeros (0.24). Next step is to check whether the model correctly fits the data.\n\n\nset.seed(100)\nEPP_ZIP.gof <- GOF_check(EPP_ZIP)\n\n\n\n\nplot(EPP_ZIP.gof)\n\n\n\n\n\nFigure 1: Goodness of fit for Sparrow fledglings paternity example. The Randomized Quantile Residuals (RQR) and counts of zeros of original data are compared to those from simulated data under the fitted model. The plots shown are RQR with envelopes, dispersion of RQR and count of zeros. Left column shows results for Poisson model while the plots for Zero Inflated Poisson (ZIP) model are on right column.\n\n\n\nFigure 1b shows the model to be appropriate because all the RQR are within the envelopes. Additionally, the dispersion and the proportion of zeros from the initial sample are within the values expected under the ZIP model (Figures 1d and 1f). This fact can be also verified by verifying that dispersion and excess of zeros tests are non significant.\n\n\nDispersionTest(EPP_ZIP.gof)\n\n        S   P_value\n 3.214152 0.5643564\n\nZeroTest(EPP_ZIP.gof)\n\n Count   P_value\n    51 0.4752475\n\nThus, the ZIP model fits the data appropriately. The next step is to check if the differences in means between years are significant and the concordance setting is therefore justified. With this aim let us apply the function to the ZIP model but in the repeatability setting.\n\n\nEPP_ZIP_0<-icc_counts(EPP,y=\"Social\",id=\"id\",fam=\"zip\")\n\n\nThe component in the object is an object of class that contains the generalized linear mixed model fit. The method applied to the model objects gives a comparison of deviances and a likelihood ratio test:\n\n\nanova(EPP_ZIP$model,EPP_ZIP_0$model)\n\nData: data\nModels:\nEPP_ZIP_0$model: y ~ (1 | id), zi=~1, disp=~1\nEPP_ZIP$model: y ~ met + (1 | id), zi=~1, disp=~1\n                Df    AIC    BIC  logLik deviance  Chisq Chi Df\nEPP_ZIP_0$model  3 847.63 857.42 -420.82   841.63              \nEPP_ZIP$model    5 836.35 852.67 -413.18   826.35 15.279      2\n                Pr(>Chisq)    \nEPP_ZIP_0$model               \nEPP_ZIP$model    0.0004811 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe result of the comparison confirms a significant difference between the yearly means, and the convenience of the concordance setting.\nNext, let us apply the function to generate the Bland-Altman plot and the bar plot of the differences in EPP between years. The plots are shown in Figures 4a and 4b.\n\n\nEPP.BA<-plot_BA(EPP,y=\"Social\",id=\"id\",rm=\"Year\") # Bland-Altman plot\n\n\n\n\nplot_BA(EPP,y=\"Social\",id=\"id\",type=\"bars\") # Bar plot\n\n\nIt can be seen that the magnitude of the differences grows as the mean increases. This heteroscedastic pattern is expected in counts because of the relation between the within-subjects variance and mean. Furthermore, we can compute some descriptive statistics of the differences:\n\n\nsummary(EPP.BA$data$Diff)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-7.0000 -1.0000  1.0000  0.9737  3.0000 10.0000 \n\nquantile(EPP.BA$data$Diff,probs=c(0.05,0.95))\n\n 5% 95% \n -4   6 \n\nBriefly, the mean of the differences between years is 0.97 fledglings, and the median is 1 fledgling. Ninety percent of the differences are between -4 and 6 fledglings between years.\nCD34+ count cell example\nThe dataset includes data where a new method of flow cytometry for counting CD34+ cells is compared to the readings obtained by a standard approach (Fornas et al., 2000). Both methods (coded as 1 and 3 in the dataset) were applied to a sample of 20 subjects. The aim of the study is to assess the interchangeability between the methods so it is necessary to evaluate the degree of concordance. Hence, the ICC used here concurs with the concordance correlation coefficient estimated by variance components (Carrasco and Jover 2003).\nLet’s firstly consider the Poisson model. As we are facing a concordance analysis, in the function the name of the method’s variable () has to be provided along with the counts variable () and subject’s identifier (). Additionally, it is necessary to specify the concordance analysis in the type argument because the default is the repeatability analysis.\n\n\nAF_P <- icc_counts(AF, y = \"y\", id = \"id\", met = \"met\", type = \"con\")\nICC(AF_P)\n\n           ICC   SE ICC 95% CI LL 95% CI UL\n[1,] 0.8472696 0.021989 0.7982025 0.8851678\n\nVarComp(AF_P)\n\n      mu    BSVar     BMVar\n 761.809 1.234619 0.1199439\n\nThe function applied to the object shows that the ICC estimate is 0.85 (95% confidence interval: 0.80, 0.89). Moreover, the function gives the parameters involved in the ICC estimator. In this case are the overall mean (mu), the between-subjects variance (BSVar) and the between-methods variability (BMVar) (the two last in log-scale).\nNext, let’s check the validity of the model by applying the function to the object.\n\n\nset.seed(100)\nAF_P.gof <- GOF_check(AF_P)\n\n\nFigure 2a shows the plot of RQR with envelopes generated by simulation. Points on the plot stand for the RQR from the original sample. Most of points lie outside of the envelopes indicating the unsuitable fit of the model. Next plot (Figure 2b) shows the density of the RQR variances computed at the simulated samples. The RQR variance from the initial sample is 32.2 which is much larger than those from the simulations. The p-value to test overdispersion is obtained by applying the function to the . With regards the zero inflation, no zeros were found in data so it is unnecessary to check this issue.\n\n\nDispersionTest(AF_P.gof)\n\n        S    P_value\n 32.20049 0.00990099\n\nConsequently, it is necessary to use a model able to afford the overdispersion as the Negative Binomial distribution.\n\n\nAF_NB2 <- icc_counts(AF, y = \"y\", id = \"id\", met = \"met\", type = \"con\", fam = \"nbinom2\")\nICC(AF_NB2)\n\n          ICC    SE ICC 95% CI LL 95% CI UL\n[1,] 0.834794 0.0454062 0.7212048 0.9046669\n\nVarComp(AF_NB2)\n\n       mu    BSVar     BMVar         r\n 777.1946 1.188904 0.0809433 0.0488122\n\nIn this case, the ICC is quite similar to that from the Poisson model (0.83, 95% CI: 0.72, 0.90) but the confidence interval is wider. The ICC components are the same as those from the Poisson case (with different values) plus the Negative Binomial dispersion parameter (0.049). Concerning the fit of the model,\n\n\nset.seed(100)\nAF_NB2.gof <- GOF_check(AF_NB2)\n\n\n\n\nplot(AF_NB2.gof)\n\n\n\n\n\nFigure 2: Goodness of fit for CD34 cell count example. The Randomized Quantile Residuals (RQR) of original data are compared to those from simulated data under the fitted model. The plots shown are RQR with envelopes, and dispersion of RQR. First row shows results for Poisson model while the plots for Negative Binomial model are on second row.\n\n\n\nin Figure 2c all the RQR are within the envelopes indicating the appropriateness of the model. Additionally, the dispersion from the initial sample is within the expected values (Figure 2d). This result is also confirmed by running the dispersion test:\n\n\nDispersionTest(AF_NB2.gof)\n\n         S   P_value\n 0.8002765 0.4059406\n\nFigures 4c and 4d show the Bland-Altman plot and the Bar plot of the differences between method 1 and 2 that are generated by the following commands:\n\n\nAF.BA <- plot_BA(AF,y=\"y\",id=\"id\", rm=\"met\") # Bland-Altman plot\n\n\n\n\nplot_BA(AF,y=\"y\",id=\"id\", type=\"bars\") # Bar plot\n\n\nIt can be seen that for values of the mean below 700 the within-subjects differences are very close to 0. However, for larger values of the mean there is a trend in the differences in relation to the mean values.\nTick counts example\nIn this study, the repeatability of line transects survey method to estimate tick abundance was assessed (Kjellander et al. 2021) in the area of Grimso (Sweden). With this aim, sampling was performed by two parallel transects separated by 1m-2m where the total count of ticks was recorded. Here, the transects are the cluster variable and every pair of data from the same transect are considered as replicates. Data is stored in the package as the object.\nAs seen before the first model to consider is the Poisson model. As it is a repeatability analysis, in the function we just need to provide the name of the counts variable () and subject’s identifier ().\n\n\nG_P <- icc_counts(Grimso, y = \"Tot\", id = \"TransectID\")\nICC(G_P)\n\n           ICC    SE ICC 95% CI LL 95% CI UL\n[1,] 0.3494333 0.1369518 0.0589753 0.5853431\n\nVarComp(G_P)\n\n        mu    BSVar\n 0.2072297 1.278685\n\nThe function applied to the object shows that the ICC estimate is 0.35 (95% confidence interval: 0.06, 0.59). The function gives the parameters involved in the ICC estimator: the overall mean and the between-subjects variance (the latter in log-scale).\nLet’s apply the function to the object to check the validity of the model.\n\n\nset.seed(100)\nG_P.gof <- GOF_check(G_P)\n\n\n\n\nplot(G_P.gof)\n\n\nFigure 3a shows the plot of RQR with envelopes generated by simulation. All points on the plot lie within the envelopes indicating the fit of the model is correct. Additionally, Figure 3b shows the RQR variance from the initial sample (1.84) is compatible with the dispersion observed in the simulated samples. The overdispersion test is run by applying the function to the object.\n\n\nDispersionTest(G_P.gof)\n\n       S   P_value\n 1.83724 0.4158416\n\nThe p-value is 0.416 so the null hypothesis of no overdispersion is not rejected and no further models have to be fitted.\n\n\n\nFigure 3: Goodness of fit for Tick counts example. The Randomized Quantile Residuals (RQR) of original data are compared to those from simulated data under the fitted model. The plots shown are RQR with envelopes, and dispersion of RQR for Poisson model.\n\n\n\nThe Bland-Altman plot and the Bar plot of the within-subjects differences are shown in Figures 4e and 4f.\n\n\nG.BA <- plot_BA(Grimso,y=\"Tot\",id=\"TransectID\",rm=\"Round\") # Bland-Altman plot\n\n\n\n\nplot_BA(Grimso,y=\"Tot\",id=\"TransectID\", type=\"bars\") # Bar plot\n\n\nAs in the case of the sparrow fledgling paternity counts, we can observe a heteroscedastic pattern with the variability of the differences increasing with the mean. Most of the differences are 0 (75%) and 90% of the differences lie between -1 and 1.\n\n\nquantile(G.BA$data$Diff, probs=c(0.05,0.95))\n\n 5% 95% \n -1   1 \n\n\n\n\n\n\n\nFigure 4: Bland-Altman and Bar plots. The first column shows the Bland-Altman plots where difference between pairs of data from the same subject (Y-axis) is plotted against mean of data from the same subject (X-axis). The second column contains the Bar plots of the differences between pairs of data from the same subject. The plots for Sparrow fledglings paternity example are on the first row, the CD34+ count cell example plots are on second row, and plots for Tick counts example are on third row.\n\n\n\n5 Conclusion\nThe statistical assessment of agreement is an issue that has received a considerable attention in recent years. It is possible to find statistical software to carry out such analysis for qualitative or continuous data (see for example Revelle (2021);Carrasco et al. (2013);Feng (2020))\n. However, there is a lack of tools when dealing with discrete data. Here, the package have been introduced to assess the agreement with such type of data considering both repeatability and concordance settings. Furthermore, the package provides the methodology to assess the validity of the model fitted to data.\nIt is important to note that no factors or predictors other than subjects and methods have been considered in the linear predictor of the GLMM. When fitting a GLMM, the inclusion of further covariates allows controlling for confounding effects. In this case, the ICC computed after controlling for confounding effects is referred to as adjusted repeatability (Nakagawa and Schielzeth 2010). Including covariates in the linear predictor make sense when the aim is to estimate the magnitude of an effect (difference in means, odds ratio or ratio of means, for instance) adjusted by the covariates. When estimating the ICC in a linear model, the inclusion of covariates will lead to a change in the variance components estimates but they remain as common estimates for all subjects. However, when facing the models for count data, the addition of covariates in the linear predictor leads to different ICCs because the value of the marginal mean \\(\\mu\\) will be different for every level of the covariates. For this reason, in the case of count data, it is preferable to segregate the data to estimate a different ICC according to the covariates. In this way, both the variance components and the mean will be different.\nFurthermore, in the Normal model setting the ICC takes values from 0 to 1. A value of 0 means independence among the measures from the same cluster (no cluster effect) whilst a value of 1 implies perfect agreement, i.e. all data from the same subject are equal. However, it is not possible to reach a value of 1 in the counts setting. The reason for this is that some within-subject variability is unavoidable because of the relation between the variance and the mean in these models. Thus, it i s not possible to observe perfect agreement with count data but the interpretation remains the same: the proportion of the total variance accounted for between-subjects variability.\n6 Availability\nThe current stable version of the package requires R 4.0 and can be downloaded from CRAN . Furthermore, depends on the following R packages: glmmTMB (Brooks et al. 2017); ggplot2 (Wickham 2016); Deriv (Clausen and Sokol 2020); gridExtra (Auguie 2017); and dplyr (Wickham et al. 2020).\n7 Acknowledgments\nI would like to thank to Shinichi Nakagawa from UNSW Sydney (Australia) for kindly providing the Sparrow fledglings paternity data. I am also indebted to Pia Kjellander from Linköping University (Sweden) for sharing the Tick count data.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-034.zip\nCRAN packages used\nrptR, iccCounts, glmmTMB, ggplot2, Deriv, gridExtra, dplyr\nCRAN Task Views implied by cited packages\nDatabases, Epidemiology, MixedModels, ModelDeployment, NumericalMathematics, Phylogenetics, Spatial, TeachingStatistics\n\n\nB. Auguie. gridExtra: Miscellaneous functions for \"grid\" graphics. R package version 2.3. 2017. URL https://CRAN.R-project.org/package=gridExtra.\n\n\nJ. M. Bland and D. G. Altman. Comparing methods of measurement: Why plotting difference against standard method is misleading. The lancet, 346(8982): 1085–1087, 1995.\n\n\nB. M. Bolker, M. E. Brooks, C. J. Clark, S. W. Geange, J. R. Poulsen, M. H. H. Stevens and J.-S. S. White. Generalized linear mixed models: A practical guide for ecology and evolution. Trends in Ecology & Evolution, 24(3): 127–135, 2009.\n\n\nM. Brooks, K. Kristensen, K. van Benthem, A. Magnusson, C. Berg, A. Nielsen, H. Skaug, M. Maechler and B. Bolker. glmmTMB balances speed and flexibility among packages for zero-inflated generalized linear mixed modeling. The R Journal, 9: 299–314, 2017.\n\n\nJ. L. Carrasco. A generalized concordance correlation coefficient based on the variance components generalized linear mixed models for overdispersed count data. Biometrics, 66: 897–904, 2010.\n\n\nJ. L. Carrasco and L. Jover. Concordance correlation coefficient applied to discrete data. Statistics in Medicine, 24: 4021–4034, 2005.\n\n\nJ. L. Carrasco and L. Jover. Estimating the generalized concordance correlation coefficient through variance components. Biometrics, 59: 849–858, 2003.\n\n\nJ. L. Carrasco, B. Phillips, J. Puig-Martinez, T. King and V. Chinchilli. Estimation of the concordance correlation coefficient for repeated measures using SAS and R. Computer Methods and Programs in Biomedicine, 109: 293–304, 2013.\n\n\nP. Choudhary and H. Nagaraja. Measuring agreement: Models, methods, and applications. Hoboken: Wiley, 2017.\n\n\nA. Clausen and S. Sokol. Deriv: R-based symbolic differentiation. Deriv package version 4.1. 2020. URL https://CRAN.R-project.org/package=Deriv.\n\n\nH. DeVet, C. Terwee, L. Mokkink and D. Knol. Measurement in medicine. Cambridge: Cambridge University Press, 2011.\n\n\nP. Dunn and G. Smyth. Randomized quantile residuals. Journal Computational and Graphical Statistics, 5: 236–244, 1996.\n\n\nC. Feng, L. Li and A. Sadeghpour. A comparison of residual diagnosis tools for diagnosing regression models for count data. BMC Medical Research Methodology, 20: 175, 2020.\n\n\nD. Feng. agRee: Various methods for measuring agreement. 2020. URL https://CRAN.R-project.org/package=agRee. R package version 0.5.3.\n\n\nJ. Fleiss. The design and analysis of clinical experiments. New York: Wiley, 1986.\n\n\nW. Hardin and J. Hilbe. Generalized linear models and extensions. Stata Press, 2007.\n\n\nJ. V. Hoef. Who invented the delta method? The American Statistician, 66: 124–127, 2012.\n\n\nP. Kjellander, M. Aronsson, U. Bergvall, J. L. Carrasco, M. Christensson, P. Lindgren, M. Akesson and P. Kjellander. Validating a common tick survey method: Cloth-dragging and line transects. Experimental and Applied Acarolog, 83: 131–146, 2021.\n\n\nS. Nakagawa and H. Schielzeth. Repeatability for gaussian and non-gaussian data: A practical guide for biologists. Biological Reviews, 85: 935–956, 2010.\n\n\nW. Revelle. Psych: Procedures for psychological, psychometric, and personality research. Evanston, Illinois: Northwestern University, 2021. URL https://CRAN.R-project.org/package=psych. R package version 2.1.9.\n\n\nJ. Schroeder, T. Burke, M. Mannarelli, D. Dawson and S. Nakagawa. Maternal effects and heritability of annual productivity. Journal of Evolutionary Biology, 25: 149–156, 2012.\n\n\nM. W. Smith, J. Ma and R. S. Stafford. Bar charts enhance bland–altman plots when value ranges are limited. Journal of Clinical Epidemiology, 63(2): 180–184, 2010.\n\n\nM. Stoffel, S. Nakagawa and H. Schielzeth. rptR: Repeatability estimation and variance decomposition by generalized linear mixed-effects models. Methods in Ecology and Evolution, 8: 1639–1644, 2017.\n\n\nH. Wickham. ggplot2: Elegant graphics for data analysis. New York: Springer-Verlag, 2016.\n\n\nH. Wickham, R. François, L. Henry and K. Müller. Dplyr: A grammar of data manipulation. R package version 1.0.2. 2020. URL https://CRAN.R-project.org/package=dplyr.\n\n\nM. Wolak, J. Fairbairn and R. Paulsen. Guidelines for estimating repeatability. Methods in Ecology and Evolution, 3: 129–137, 2012.\n\n\n\n\n",
    "preview": "articles/RJ-2022-034/distill-preview.png",
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 1728
  },
  {
    "path": "articles/RJ-2022-035/",
    "title": "An Open-Source Implementation of the CMPS Algorithm for Assessing Similarity of Bullets",
    "description": "In this paper, we introduce the R package cmpsR, an open-source implementation of the Congruent Matching Profile Segments (CMPS) method developed at the National Institute of Standards and Technology (NIST) for objective comparison of striated tool marks. The functionality of the package is showcased by examples of bullet signatures that come with the package. Graphing tools are implemented in the package as well for users to assess and understand the CMPS results. Initial tests were performed on bullet signatures generated from two sets of 3D scans in the Hamby study under the framework suggested by the R package `bulletxtrctr`. New metrics based on CMPS scores are introduced and compared with existing metrics. A measure called sum of squares ratio is included, and how it can be used for evaluating different scans, metrics, or parameters is showcased with the Hamby study data sets. An open-source implementation of the CMPS algorithm makes the algorithm more accessible, generates reproducible results, and facilitates further studies of the algorithm such as method comparisons.",
    "author": [
      {
        "name": "Wangqian Ju",
        "url": "https://github.com/willju-wangqian"
      },
      {
        "name": "Heike Hofmann",
        "url": "https://github.com/heike"
      }
    ],
    "date": "2022-10-13",
    "categories": [],
    "contents": "\n\n\n\n\n\n\nIntroduction\nIn this paper, we present an open-source implementation of the algorithm of the Congruent Matching Profile Segments (CMPS) method.\nChen et al. (2019) developed the CMPS method for “objective comparison of striated tool marks” and demonstrated its use in some examples of comparing bullet signature correlations.\nAlthough Chen et al. (2019) conceptually describe the CMPS algorithm in their paper, the authors did not release an implementation of their method.\nThus, our effort here is to introduce the CMPS method to the R community and provide an open-source, publicly available implementation of the algorithm to use, review, and improve.\nOur implementation is made available as part of the R package cmpsR on CRAN.\nAccording to the Uniform Crime Reporting Program of the FBI (Federal Bureau of Investigation), “more than 76 percent (76.7) of the homicides for which the FBI received weapons data in 2020 involved the use of firearms” (United States Department of Justice, Federal Bureau of Investigation.). At the same time, the number of murder victims jumped between 2019 and 2020 by more than 23 percent (23.4) to 17754 (United States Department of Justice, Federal Bureau of Investigation.). This increase is unprecedented and highlights the important role that firearm examination plays in all these cases.\n\nAn important task of firearm examination is to answer the question of whether two pieces of evidence come from the same source or whether a piece of evidence matches a sample obtained from a specific firearm.\nHere, in particular, we are interested to determine whether two bullets were fired from the same gun barrel.\nAssessing the similarity between two bullets is based on a comparison of striation marks acquired during the firing process as bullets are propelled through the barrel.\nThe current state of the art sees firearms examiners make an assessment of similarity based on a visual comparison, generally, using a comparison microscope (AFTE Criteria for Identification Committee 1992).\nThis practice has been criticized for its lack of objectivity and the associated problem of determining valid error rates (President’s Council of Advisors on Science and Technology 2016).\nA report published by the Committee on Identifying the Needs of the Forensic Sciences of the National Research Council (2009) states that “[m]uch forensic evidence-including, for example, bite marks and firearm and toolmark identification—is introduced in criminal trials without any meaningful scientific validation, determination of error rates, or reliability testing to explain the limits of the discipline.” To overcome those criticisms and concerns, researchers have been making an effort to build databases and develop frameworks and algorithms that bring an objective and quantitative assessment into the field.\nThe database of Zheng (2016) with digital 3D topographic scans from various studies by Brundage (1998), Hamby et al. (2009), Hamby et al. (2019) and others, provides a great resource for researchers. Algorithms that can be validated and tested and generate quantitative results are developed for 2D and 3D surface texture (Song et al. 2005), tool marks (Chumbley et al. 2010), and striae on Land Engraved Areas (LEAs) of bullets Chen et al. (2019).\n\nThe notion of bullet signatures is one of the results of these efforts, and we will use it to explain the CMPS algorithm and showcase the cmpsR implementation.\nBullet signatures can be extracted from bullet LEAs, typically one for each land engraved area, and how those bullet signatures are extracted from scans will be discussed in the Background section.\nBullet signatures capture striation marks on the bullet in a numeric format and therefore serve as the foundation for algorithms. These scans played an important role in bringing objectivity into the field.\nIn the following sections, we will: review the background of bullet signature comparisons, discuss how we followed the idea described by Chen et al. (2019) for the implementation of the CMPS algorithm, propose new metrics based on the CMPS score and a principled evaluation framework for algorithmic results comparison, and present results of applying our implementation to real data.\nBackground\nHamby data set\n\nThe datasets we worked with come from the James Hamby Consecutively Rifled Ruger Barrel Study (Brundage 1998; Hamby et al. 2009, 2019), in particular, Hamby set 252 and Hamby set 44.\nFor each Hamby set, a total of 35 bullets is fired from (the same) ten consecutively manufactured Ruger P-85 pistol barrels.\nTwo bullets are fired from each barrel, making up a set of 20 reference bullets.\nAn additional 15 bullets are fired from these ten barrels in a fashion unknown to the study participant.\nThe aim of the Hamby Study was to have firearms examiners identify which barrel each of the 15 questioned bullets was fired from.\nThe Ruger P-85 barrels are traditionally rifled barrels with six grooves and lands as shown in Figure 1.\nDuring the firing process, grooves and lands are engraved on a bullet.\nFirearms examiners use striation marks on land engraved areas (LEAs) for their visual comparison.\nFor algorithmic purposes, 3D topographical images of land engraved areas were obtained and stored in x3p format (XML 3-D Surface Profile).\nThe x3p format provides a standard way of exchanging 2D and 3D profile data.\nIt conforms to the ISO5436-2 standard adopted by the OpenFMC (Open Forensic Metrology Consortium), a group of firearm forensics researchers who contributes to the establishment of best practices of using metrology in forensic science.\nHamby set 252 was scanned using a NanoFocus lens at 20x magnification with the scan resolution being 1.5625 m \\(\\times\\) 1.5625 m per pixel.\nHamby set 44 was scanned at the Roy J Carver High-Resolution microscopy lab at Iowa State.\nThese scans were acquired with a Sensofar Confocal Light Microscope at 20x magnification for a nominal resolution of 0.645 m \\(\\times\\) 0.645 m per pixel.\nBoth Hamby set 252 and Hamby set 44 are publicly available from the NIST Ballistics Database Project (Zheng 2016).\nExtracting signatures from LEA scans\n\nThe automated framework for extracting signatures from x3p files used in this paper was proposed by Hare et al. (2017) and is implemented in the R packages x3ptools (Hofmann et al. 2020) and bulletxtrctr (Hofmann et al. 2019).\nx3ptools is a package to read, write, and generally, process x3p files.\nThe bulletxtrctr package implements a pipeline for extracting and comparing signatures from scans of land-engraved areas.\nFigure 2 gives an overview of all of the steps in the process from scan to signatures.\nFigure 2(a) shows a rendering of a 3D scan of a bullet land engraved area.\nThe raised portion of the surface on the left and right of the scan are parts of the adjacent groove engraved areas (GEAs), the middle area shows a land engraved area with well expressed striation marks.\nThe first step of obtaining the bullet signature is to extract a cross-section at a fixed height on the land engraving.\nThe thin white horizontal line in Figure 2(a) indicates which cross-section was identified by the algorithm to represent the LEA; Figure 2(b) shows the corresponding cross-sectional view.\nGroove engraved areas are removed from the analysis as indicated by the vertical blue lines in Figure 2(c) and Figure 2(d).\nA non-parametric LOESS smooth (Cleveland et al. 1991) is fitted to capture the bullet curvature (Figure 2(e)) and, finally, the bullet signature (Figure 2(f)) is obtained as residuals of the cross-section and the fitted smooth.\nNote that in Chen et al. (2019) bullet signatures are referred to as bullet profiles.\nHowever, to avoid confusion, we distinguish the notion of bullet signatures from bullet profiles.\nBullet profiles are shown in panels (b), (c), and (d) of Figure 2, while Figure 2(f) shows the corresponding bullet signature.\nIdentifying the groove engraved areas correctly is fundamental for a correct downstream analysis of the signatures.\nWe have provided an interactive web application to allow for a human-in-the-middle inspection and intervention. It is implemented as an R Shiny App (Chang et al. 2021) named bulletinspectR for identifying and correcting those errors.\nAn example of the extraction process with corresponding code and parameter settings can be found in the “Supplementary materials”. Note that the process of extracting signatures might be different from the one used in Chen et al. (2019) because no code or parameter settings are made available publicly.\n\n\n\nFigure 1: Photo of a traditionally rifled gun barrel (left) and a fired bullet (right).\n\n\n\n\n\n\nFigure 2: A framework of obtaining a bullet signature. (a) rendering from the 3D topographic scan of a land engraved area (LEA). The selected crosscut location is indicated by a thin white horizontal line. (b) view of the cross-section of the land engraved area at the white line in (a). (c) the crosscut data plotted in 2D; blue vertical lines indicate the position of the left and right grooves. (d) the crosscut data after chopping the left and right grooves. (e) the fitted curvature using LOESS. (f) after removing the curvature from the crosscut data, the bullet signature is obtained.\n\n\n\nConceptual idea of CMPS\n\nMost algorithms for comparing striation marks are based on the digitized signatures and produce a similarity score Krishnan and Hofmann (2019).\nThe congruent matching profile segments (CMPS) algorithm, developed by Chen et al. (2019) for “objective comparison of striated tool marks”, is one such algorithm.\nThe algorithm’s main idea is to take a set of consecutive and non-overlapping basis segments from the comparison and for each segment find the “best” registration position on the reference (the other bullet signature) with respect to their cross-correlation values.\nFrom a comparison of these registration positions, a congruent registration position is identified, and the number of basis segments taking the congruent registration position is the CMPS score.\nNote that researchers in Chen et al. (2019) refer to the origin of basis segments as the reference, but in this paper we refer to it as the comparison signature.\nHigh CMPS scores are achieved between more similar signatures and are therefore indicative of a same-source pair.\nLow scores between pairs of signatures are attributed to different source pairs.\nHowever, a specific threshold of the CMPS score to distinguish between same-source and different-source comparisons is not provided in Chen et al. (2019), instead the threshold depends on the underlying structure of the data and the choice of parameters. In a legal setting this variability is problematic because it allows for situations in which experts could choose parameters based on whether they are witnesses for the defense or the prosecution.\nFurther research is needed to understand how to determine optimal threshold settings.\nThe CMPS algorithm can assist firearms examiners with drawing a conclusion about the source of a comparison pair.\nThus, in this paper we present an open-source implementation of the CMPS algorithm in the R package cmpsR available from both CRAN and Github. This publicly available implementation calculates the CMPS score of a comparison using the following code:\n\n\n# install.packages(\"cmpsR\")\n\nlibrary(cmpsR)\ndata(bullets)\n\nsig1 <- bullets$sigs[[2]]$sig\nsig2 <- bullets$sigs[[9]]$sig\nsig3 <- bullets$sigs[[10]]$sig\n\ncmps_result_KM <- extract_feature_cmps(sig1, sig2)\ncmps_result_KNM <- extract_feature_cmps(sig1, sig3)\n\n\nIn this example, the comparison between sig1 and sig2, two signatures coming from the same source (a known-match comparison), gets a CMPS score of 17; the comparison between sig1 and sig3, two signatures coming from different sources (a known non-match comparison), gets a CMPS score of 1.\nWe also implemented graphing tools for users to better understand these results as well as the algorithm itself.\nThe section “Implementation” will go through the algorithm and show how to use the cmpsR package.\nA further example that illustrates the main points is also included.\nIn the section on “Evaluation metrics”, we introduce new CMPS metrics that summarize land-level CMPS scores and a sum of squares ratio that can be used to evaluate algorithmic results.\nThe section “Results” presents the results of evaluating the cmpsR package using Hamby set 252 and Hamby set 44. The results from Hamby set 252 are used to verify that our implementation is, at least qualitatively, comparable to the algorithm described in Chen et al. (2019). Results from Hamby 44 show the need for a further investigation of the parameter choices even in the case of bullets fired from the same barrels.\nThe last section covers some final discussion and conclusions.\nImplementation\nAlgorithm\nConceptually, the CMPS algorithm consists of three main steps:\ncut the comparison signature into consecutive, non-overlapping, and equal-length basis segments: The command get_segs(x, len=50) implements this step: it takes bullet signature x in the format of a numeric vector and cuts it into consecutive, non-overlapping and equal-length segments of length len, which are referred to as “basis segments”. Note that the parameter len determines the length of a basis segment and thus affects the total number of basis segments, which is the upper limit of the CMPS score of a comparison. The default value of len is 50, which will result in about 25 basis segments for the example data of the package.\nidentify candidate positions: For each basis segment a set of candidate registration positions on the comparison signature is identified based on the segment’s similarity to the reference signature.\nIn the first step, the cross-correlation function of the segment to the reference is calculated, then a number of positions with high correlation values are identified as candidate positions.\nIn case multiple segment lengths are considered, the length of each basis segment is expanded (by default it is doubled) and these two steps are repeated.\nOnly when candidate positions coincide (or are similar enough), they are considered further.\nFigure 5 and Figure 6 illustrate these ideas.\nCalculate the cross-correlation curve: Calculate the cross-correlation curve between a basis segment x and the reference signature y using the function get_ccf4(x, y, ...) as shown in Figure 5(b). The position indicates the lag by which a basis segment is moved with respect to its original placement. A position is considered “good” if it results in a peak in the cross-correlation between the basis segment and the reference.\nCorrelation peaks: Two strategies referred to as “multi-peak inspection” and “multi-peak inspection at different segment lengths” in Chen et al. (2019) are used for identifying positions of correlation peaks as candidate positions. The latter is also called the “multi-segment lengths strategy”. The parameter npeaks_set in extract_feature_cmps(...) determines which strategy to use and the number of candidate positions:\nIf npeaks_set is an integer vector of length 1, for example, npeaks_set = 5, the positions of the top five peaks in the cross-correlation curve are identified as candidate positions for registration.\nIf npeaks_set is an integer vector of length more than 1, for example, npeaks_set = c(5, 3, 1), the multi-segment lengths strategy will be used: calculate the cross-correlation function between a basis segment and the reference and identify positions of the top five peaks; double the segment length to a specified value, re-calculate the cross-correlation function, and identify three peaks; repeat this process and identify a single peak in the newly computed cross-correlation function. Figure 6 shows an example of three levels of basis segment 6 and their corresponding cross-correlation curves and identified peaks. Note that in Chen et al. (2019) the segment length is doubled at each level of a basis segment, but in the present implementation users are allowed to choose the segment length at each level.\nget_ccr_peaks(comp, segments, seg_outlength, nseg = 1, npeaks = 5) computes the cross-correlation curve between a basis or increased segment and the reference signature and finds peaks in the cross-correlation curve. The number of peaks detected is equal to npeaks, which is an integer. segments, seg_outlength, and nseg determine the segment in the cross-correlation computation, and comp gives the reference signature. If the multi-segment lengths strategy is used, then get_ccr_peaks(...) is called in a lapply() for each level of the basis segment. The resulting list is called ccr_list.\n\nmulti-segment lengths strategy: with the multi-segment lengths strategy being used, a position is identified as a candidate position for registration and is called a “consistent correlation peak” if it results in a top peak in the cross-correlation curve with a tolerance zone determined by Tx in all segment levels. Note that in Chen et al. (2019), a segment at its largest scale (highest level) always identifies one peak, but we do not have this requirement in our implementation.\nthe function get_seg_scale(segments, nseg, out_length) is used to obtain the (potentially increased) version of a basis segment. segments, which is a list containing all basis segments generated by the function get_segs(...) in step 1, and nseg are used to determine the basis segment to be increased. out_length specifies the length of the output segment.\nget_ccp(ccr_list, Tx = 25) tries to identify the “consistent correlation peak”. ccr_list is the result of lapply() and get_ccr_peaks(...), and Tx determines the size of a tolerance zone used in identifying the consistent correlation peak. get_ccp(...) returns NULL if there is no consistent correlation peak.\n\n\ndetermine the congruent registration position: A candidate position “receives” votes from basis segments that identify it or a close position within a tolerance zone of Tx as a candidate position in step 2.\nVotes for all candidate positions are tallied, and the position with the highest number of votes gets chosen as the congruent registration position, indicating that most of the basis segments find their highly similar counterpart in the reference signature in terms of correlation at this registration position.\nIn the case of ties, the middle position is taken as the congruent registration position.\nBasis segments with a congruent registration position are called “congruent matching profile segments” (CMPS).\nThe total number of CMPS is the CMPS score of the comparison.\nget_CMPS(input_ccp, Tx = 25) is the function that tallies the votes and determines the congruent registration position and congruent matching profile segments (CMPS).\nNote that there are several parameters in the CMPS algorithm that will affect the final results and are left to the users to decide, such as the length of basis segments seg_length in step 1, the number of peaks npeaks_set identified on each level in step 2, and the length of the tolerance zone Tx in both step 2 and 3.\nIn our implementation of the CMPS algorithm, we used the parameters given in the original CMPS paper (Chen et al. 2019) as the default values for these parameters.\nHowever, the authors state that no cross-validation has been done - there might also be issues with respect to the resolution of the scans.\nFurther research is needed, until then users are advised to think of default values as starting values and consider alternatives. However, the evaluation framework based on the sum of squares ratio introduced in the later section could be used to evaluate the choices of these parameters.\nThe main function that combines all steps in the CMPS algorithm described above is called extract_feature_cmps(...).\nHere we present it with its default parameters.\n\n\nextract_feature_cmps(\n  x,\n  y,\n  seg_length = 50,\n  Tx = 25,\n  npeaks_set = c(5, 3, 1),\n  include = NULL,\n  outlength = NULL\n)\n\n\nThe function extract_feature_cmps allows for the following input from users besides the previously discussed parameters seg_length, npeaks_set, and Tx:\nx and y are two signatures: x serves as the comparison signature (which will be divided into basis segments) and y is the reference signature;\ninclude determines the format of the function result.\nBesides the CMPS_score, other aspects of the comparison help in understanding how the CMPS_score is computed.\nBy default include is set to NULL and only the CMPS_score is returned; further results are included when include is (an abbreviation of) one of or a vector of the following strings: \"nseg\", \"congruent_pos\", \"congruent_seg_idx\", \"segments\", \"parameters\", and \"full_result\".\nIf include is specified as \"full_result\" (or its abbreviation), the output includes everything listed below.\nnseg: the number of basis segments from the comparison signature; this is also the highest possible CMPS score of the comparison;\ncongruent_pos: the congruent registration position;\ncongruent_seg_idx: the indices of all congruent matching profile segments;\nccp_list: a list showing identified candidate positions of all basis segments;\npos_df: a data frame containing all candidate positions and their respective number of votes;\nsegments: a list containing all basis segments;\nparameters: a list containing all input arguments of extract_feature_cmps;\n\noutlength specifies the segment length of a basis segment at each level under the multi-segment lengths strategy.\nBy default outlength is set to NULL, indicating that a basis segment should double its segment length at the next level and conforming to the description in Chen et al. (2019) .\nIn the remainder of the paper, we showcase the use of the cmpsR functionality on some examples and present the results of applying it to two datasets.\nInstallation\nThe cmpsR package is publicly available from CRAN and can be installed by\n\n\ninstall.packages(\"cmpsR\")\n\n\nMoreover, its development version is also available from Github and can be installed by\n\n\n# install.packages(\"remotes\")\nremotes::install_github(\"willju-wangqian/cmpsR\")\n\n\nAn example\nThe cmpsR package contains a simple example to illustrate the basic usage of the package.\nThe data in this example are twelve bullet signatures obtained from two bullets in Hamby set 252 (Hamby et al. 2009).\nThe procedure for generating signatures from high-resolution 3D topographic scans of bullet lands used here follows the methodology described in Hare et al. (2017) (as discussed above).\nThe two bullets under consideration are known to have been fired from the same gun barrel, so for the 36 pairwise land-by-land comparisons, six comparisons are from same-source pairs (known matches) while thirty are from different-source pairs (known non-matches).\nTo access the example data, we use\n\n\nlibrary(cmpsR)\ndata(bullets)\n\n\nbullets$sigs is a list of twelve numeric vectors corresponding to the twelve bullet signatures shown in Figure 3.\nbullets$source contains the URLs to the corresponding x3p file containing the topographic scan from the NIST Ballistics Toolmark Research Database (Zheng 2016).\n\n\n\nFigure 3: Signatures of all lands of bullet 1 in the top row, and of bullet 2 in the bottom row. Signatures in the second row are ordered to be in phase with the signatures above, i.e. matching signatures are displayed on top of each other. On the x-axis is the length of the scan in millimeter, and on the y-axis is the relative height in micron.\n\n\n\nThe signatures of Land 4 of Bullet 1 and Land 5 of Bullet 2 are stored in objects sigs2 and sigs1, respectively.\nThis comparison consists of a pair of signatures that are known to be a match – a KM (known match) comparison.\nWe compute the CMPS score using two versions of the CMPS algorithm:\n\n\nsigs1 <- bullets$sigs[bullets$bulletland == \"2-5\"][[1]]\nsigs2 <- bullets$sigs[bullets$bulletland == \"1-4\"][[1]]\n\n# compute cmps\n\n# algorithm with multi-peak insepction at three different segment levels\ncmps_with_multi_scale <-\n  extract_feature_cmps(sigs1$sig, sigs2$sig,\n    npeaks_set = c(5, 3, 1), include = \"full_result\"\n  )\n\n# algorithm with multi-peak inspection at the basis scale only\ncmps_without_multi_scale <-\n  extract_feature_cmps(sigs1$sig, sigs2$sig,\n    npeaks_set = 5, include = \"full_result\"\n  )\n\n\nIn the first example, npeaks_set is a vector of three integers, i.e. the algorithm uses the multi-segment lengths strategy to create the result object cmps_with_multi_scale.\nFor cmps_without_multi_scale each basis segment is linked to the top 5 candidate positions.\nWe use include = \"full_result\" to capture all results.\nIn this example, the CMPS is 9 when using multiple segments, and 12 when using a single segment.\nAs discussed in Chen et al. (2019), using multi-segment lengths strategy can reduce the number of false positives when identifying candidate positions; however, any score-based method is walking the line between false positives and false negatives.\nAs the number of false positives is reduced the number of false negatives might rise.\nMore discussion and comparisons between the two versions of the CMPS algorithm will be presented in later sections.\nNote that the multi-segment lengths method is slower because the algorithm is run once for each segment length.\n\n\n\nVisualize and understand CMPS results\nWe also implemented graphing tools for visualizing the results of the CMPS algorithm.\nThe goal is to provide users with tools to inspect each of the basis segments and to help them have a better understanding of how the algorithm works.\nFigure 4 shows the plots generated by the first graphing function, cmps_signature_plot(), and continues with the example above.\ncmps_signature_plot() takes the output of extract_feature_cmps(..., include = \"full_result\") and returns a list of 5 elements.\nIt creates an overall impression of how the comparison signature aligns with the reference signature at the congruent registration position.\nThe first element is a plot called segment_shift_plot, shown in Figure 4(a). On this plot the reference signature is drawn as a black line, congruent matching profile segments from the comparison signature are overlaid in red at the congruent registration position.\n\n\nsig_plot <- cmps_signature_plot(\n  cmps_with_multi_scale\n)\n\n# (a)\nsig_plot$segment_shift_plot\n\n\nThe second plot is called signature_shift_plot, shown in Figure 4(b). This visual presents both the comparison signature and the reference signature. The comparison signature is aligned with the reference signature based on the congruent registration position. Congruent matching profile segments are highlighted by solid red lines.\n\n\n# (b)\nsig_plot$signature_shift_plot\n\n\n\n\n\nFigure 4: In (a) the black line shows the comparison signature; each red line segment shows one congruent matching profile segment. Each grey rectangle highlights one congruent matching profile segment. In (b) the black line shows the reference signature; the red line shows the comparison signature. Solid part shows the congruent matching profile segments, and the dashed part shows segments that do not agree with the congruent registration position.\n\n\n\nOther elements of this list are seg_shift and sig_shift. sig_shift gives the congruent registration position, while seg_shift is a data frame showing the congruent matching profile segments and their identified candidate position closest to the congruent registration position.\n\n\nsig_plot$seg_shift\n\n#>    seg_idx seg_shift\n#> 7        7         0\n#> 8        8        -1\n#> 14      14         5\n#> 16      16         8\n#> 17      17         8\n#> 18      18         8\n#> 19      19         9\n#> 20      20         9\n#> 22      22        12\n\nWhile cmps_signature_plot() focuses on the signature level, cmps_segment_plot() focuses on the segment level.\nIt provides the “full result” of extract_feature_cmps(), but also takes an argument, seg_idx, indicating which segment should be inspected.\nWhen checking sig_plot$seg_shift we notice that segment number 6 is not one of the congruent matching profile segments.\nWe can therefore set seg_idx = 6 in cmps_segment_plot() and investigate the reason why this segment disagrees with the congruent registration position.\nFor each segment scale, we have two plots: segment_plot and scale_ccf_plot, as shown in Figure 5 for the example of segment number 6:\nFigure 5(a) is the segment_plot for basis segment 6 at level one (in its original length). We used npeaks_set = c(5, 3, 1) in extract_feature_cmps() when calculating the CMPS score. Therefore the top five peaks are identified in the cross-correlation curve at level one. Segment 6 is plotted at the positions where these five peaks are identified with dashed lines in the segment_plot. The solid thick black line shows the segment at its original position (which in this example is very close to the actual registration position).\n\n\nseg_plot <- cmps_segment_plot(\n  cmps_with_multi_scale,\n  seg_idx = 6\n)\n\n# (a)\nseg_plot[[1]]$segment_plot\n\n\nFigure 5(b) is the scale_ccf_plot of basis segment 6 at level one. It shows the cross-correlation curve computed by the reference signature and the level-one basis segment 6. The five highest peaks are marked by dots on the curve.\n\n\n# (b)\nseg_plot[[1]]$scale_ccf_plot\n\n\n\n\n\nFigure 5: Plot (a) shows segment_plot for segment 6 at level one. The original position of segment 6 is indicated by the solid black line. Positions, where the segment achieves the 5 highest cross-correlations, are indicated by the dashed line segments. The scale_ccf_plot in plot (b) shows the cross-correlation curve between the reference signature and segment 6 at level one. The five highest peaks are marked by dots. The vertical red dashed line indicates the congruent registration position; the green dashed line shows a peak position in the highest segment level; the blue dashed lines show the tolerance zone around the green dashed line. We can see that none of the five highest peaks at level one falls within the tolerance zone, indicating that there is no consistent correlation peak or a candidate position identified by basis segment 6 under the multi-segment lengths strategy. Thus, the basis segment 6 doesn’t vote for the congruent registration position and is not a cmps.\n\n\n\nAdditionally, users can have more insights about why segment 6 is not a congruent matching profile segment if we put the segment_plot and scale_ccf_plot of all three segment levels together, as shown in Figure 6 with the help of ggpubr::ggarrange().\n\n\nlibrary(ggpubr)\n\nggarrange(\n  plotlist =\n    unlist(seg_plot,\n      recursive = FALSE\n    ),\n  ncol = 2,\n  nrow = 3\n)\n\n\n\nFigure 6: Put segment_plot and scale_ccf_plot of all three levels together. We are identifying the five highest peaks at level one, three peaks at level two, and one peak at level three since npeaks_set = c(5, 3, 1). The highest peak position at level three is marked by the green dashed line across all segment levels. However, the highest peak on level three does not coincide with any of the top five highest peaks at level one. This indicates that there is no consistent correlation peak or a candidate position for basis segment 6 under the multi-segment lengths strategy.\n\n\n\nIn Figure 6, the red vertical dashed line indicates the congruent registration position.\nWe can see that the basis segment 6 does obtain a peak near the congruent registration position at level two and level three, respectively; however, this position doesn’t give one of the five highest peaks at level one.\nAs a result, segment 6 fails to identify the consistent correlation peak (ccp) and fails to become one of the congruent matching profile segments according to the multi-segment lengths strategy.\nThe identified top five peaks at level one are also examples of “false positive” peaks.\nThe “true positive” peak (the peak within the tolerance zone of the congruent registration position) is identified at level two and three by increasing the segment length, which justifies the usage of the multi-segment lengths strategy.\nEvaluation metrics\nMetrics based on CMPS scores\nThe CMPS algorithm measures the similarity between two signatures resulting in a similarity score of a land-to-level comparison.\nBullets fired from traditionally rifled barrels have multiple land and groove engraved areas.\nHere, we are working with bullets fired from Ruger P85 barrels with six lands and grooves.\nA comparison of two bullets, therefore, involves 36 land-to-land comparisons, resulting in 36 CMPS scores (as shown in Figure 7).\nIn order to obtain a single similarity score of a bullet-level comparison, we need to summarize these 36 CMPS scores.\nTwo similarity metrics for bullet-level comparisons have been introduced in the literature (Chen et al. 2019): \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\).\n\\(\\mathrm{CMPS_{max}}\\) is the highest CMPS score obtained among all land-level comparisons, while \\(\\mathrm{\\overline{CMPS}_{max}}\\) is the highest possible mean CMPS score of land-level comparisons that are in the same phase:\nIn general, we assume each bullet has \\(n\\) land engravings (in our case \\(n=6\\)).\nLet \\(c_{ij}\\) denote the CMPS score of a comparison between bullet 1 land \\(i\\) and bullet 2 land \\(j\\), for \\(i,j = 1, \\dots, n\\).\nLet \\(\\mathcal{P}_k\\) denote bullet land pairs in phase \\(k\\) for \\(k = 0, \\dots, n-1\\), and\n\\[\\begin{align}\n\\mathcal{P}_k = \\{ \\left(i,j\\right): i = 1, \\dots, n ; \\; j = \\left(i + k\\right) \\;\\mathrm{mod}\\; n \\}\n\\end{align}\\]\n, where mod denotes the modulo operation.\nFor example, \\(\\mathcal{P}_1 = \\{ \\left(1,2\\right), \\left(2,3\\right), \\left(3,4\\right), \\left(4,5\\right), \\left(5,6\\right), \\left(6,1\\right) \\}\\) when \\(n = 6\\).\nLet \\(k^*\\) denote the index of the highest phase.\nWith that, the two measures to evaluate accuracy used in Chen et al. (2019) are defined as\n\\[\\begin{align}\n\\mathrm{CMPS_{max}} &= \\max_{i,j} c_{ij} \\text{ , and} \\\\\n\\mathrm{\\overline{CMPS}_{max}} &= \\frac{1}{n} \\sum_{(i,j) \\in \\mathcal{P}_{k^*}} c_{ij} \\text{ , where} \\\\\nk^* &= \\text{arg}\\max\\limits_{k} \\left[  \\frac{1}{n} \\sum_{(i,j) \\in \\mathcal{P}_k} c_{ij}\\right]\n\\end{align}\\]\nWe can continue with the example used in previous sections.\nbullets contains bullet signatures of two bullets, bullet1 and bullet2.\nAs mentioned before, each bullet has six land engravings, resulting in six bullet signatures.\nThus, there are 36 pairwise bullet signature comparisons, resulting in 36 \\(c_{ij}\\) values in total.\nWe use multi-segment lengths strategy with default parameters to compute these CMPS scores, and the result is shown in Figure 7.\nWe can see that in this example,\n\\[\n\\mathrm{CMPS_{max}} =  \\max_{i,j} c_{ij} = 17\n\\]\nand since bullet lands in phase \\(\\mathcal{P}_1\\) gives the highest mean CMPS score (\\(k^* = 1\\)), we have\n\\[\n\\begin{aligned}\n\\mathrm{\\overline{CMPS}_{max}} &= \\frac{1}{6} \\sum_{(i,j) \\in \\mathcal{P}_1} c_{ij} \\\\\n                        &= \\frac{1}{6} \\left(c_{12} + c_{23} + c_{34} + c_{45} + c_{56} + c_{61}\\right) \\\\\n                        &= \\frac{1}{6} \\left(3+17+14+10+15+16\\right) \\\\\n                        &= 12.5\n\\end{aligned}\n\\]\n\n\n\nFigure 7: CMPS scores of all 36 pairwise bullet signature comparisons for two bullets. Land engraving pairs generated by the same land (KM comparisons) are highlighted. Note that in this example the axis along Bullet 2 starts with Land 2. This corresponds to Phase 1 in equation (1).\n\n\n\nHowever, both \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\) consider only relatively high CMPS scores and ignore the rest.\nSo we introduce a new metric based on CMPS scores called \\(\\mathrm{\\overline{CMPS}_{diff}}\\), which is the difference between \\(\\mathrm{\\overline{CMPS}_{max}}\\) and the mean of all other CMPS scores.\nWith our notation above, we have:\n\\[\\begin{align}\n\\mathrm{\\overline{CMPS}_{diff}} = \\left[  \\frac{1}{n} \\sum_{(i,j) \\in \\mathcal{P}_{k^*}} c_{ij}\\right] - \\left[  \\frac{1}{n\\left(n-1\\right)} \\sum_{(i,j) \\notin \\mathcal{P}_{k^*}} c_{ij}\\right]\n\\end{align}\\]\n\\(\\mathrm{\\overline{CMPS}_{diff}}\\) highlights the difference between CMPS scores of matching and non-matching comparisons.\nIf two bullets are non-matching, all 36 CMPS scores are expected to be small with relatively the same values, resulting in a \\(\\mathrm{\\overline{CMPS}_{diff}}\\) value close to 0.\nFor the example above, \\(\\mathrm{\\overline{CMPS}_{diff}} = 12.5 - 1.53 = 10.97\\) \nScaled CMPS scores\nAnother issue with the CMPS score is that the highest possible CMPS score (the total number of basis segments) might differ across comparisons (as shown in Figure 8(a)) due to different lengths of bullet signatures and different lengths of basis segments specified by the parameter.\nA CMPS score of 5 might indicate a non-match if the highest possible CMPS score is 30 but indicate a match if the highest possible CMPS score is 6.\nThus, we introduce the scaled CMPS score, denoted as \\(c^*_{ij}\\).\nLet \\(s_{ij}\\) denote the highest possible CMPS score or the total number of basis segments, then the scaled CMPS score \\(c^*_{ij}\\) is defined as the ratio of raw score and maximum score:\n\\[\\begin{align}\nc^*_{ij} = \\frac{c_{ij}}{s_{ij}}\n\\end{align}\\]\nThe scaled CMPS scores of the above example are shown in Figure 8(b).\nCompared to the original CMPS scores, scaled scores have values within the interval \\([0, 1]\\) regardless of the length of the basis segments and therefore make a comparison of values possible across different parameter choices.\nSimilar to the original CMPS scores, we will denote the scaled CMPS scores adjusted for out-of-phase background values by \\(\\mathrm{\\overline{CMPS^*}_{diff}}\\).\nFor example, \\(\\mathrm{\\overline{CMPS^*}_{diff}} = 0.498\\) for Figure 8(b)\n\n\n\nFigure 8: Plot (a) shows the highest possible CMPS scores (the total number of basis segments) for the 36 comparisons. (b) shows the scaled CMPS scores for the 36 comparisons.\n\n\n\nSum of squares ratio\nThe “sum of squares ratio” quantifies how well two groups of values separate.\nLet \\(n_T\\) denote the total number of observations, \\(n_k\\) denote the number of observations in group \\(k\\), and \\(y_{kl}\\) denote the \\(l\\)-th observation in group \\(k\\), for \\(k = 1,2\\) and \\(l = 1, \\dots, n_k\\).\nLet \\(\\bar{y}_{k.} = \\frac{1}{n_k} \\sum_{l=1}^{n_k} y_{kl}\\) denote the mean value in group \\(k\\) and \\(\\bar{y}_{..} = \\frac{1}{n_T} \\left( \\sum_{k} \\sum_{l = 1}^{n_k} y_{kl} \\right)\\) denote the mean value of all observations.\nConsider the following model:\n\\[\\begin{align}\ny_{kl} = \\mu_k + e_{kl}\n\\end{align}\\]\nwhere \\(\\mu_k\\) is the true mean of group \\(k\\) and \\(e_{kl}\\) is a random effect of different observations.\nThen we can define the sum of squares ratio \\(V\\) as:\n\\[\\begin{align}\nV = \\frac{\\sum_k n_k \\left(\\bar{y}_{k.} - \\bar{y}_{..} \\right)^2}{\\sum_k \\sum_l^{n_k} \\left(y_{kl} - \\bar{y}_{k.} \\right)^2 }.\n\\end{align}\\]\nThe numerator of the sum of squares ratio \\(V\\) quantifies the variation between the two groups, while the denominator quantifies the variation within each group.\nThe sum of squares ratio \\(V\\) can be used as an index for evaluating scans, metrics, and different sets of parameters if the same data set is being used.\nSome examples will be presented in the following section.\nIf we impose the normality and independence assumptions on the random effects \\(e_{kl}\\), the sum of squares ratio \\(V\\) becomes a scaled F-statistic with degrees of freedom of \\(k-1\\) and \\(n_T - k\\) and \\(F = \\frac{n_T - k}{k- 1} V\\).\nIf we want to compare different data sets, stating that a certain setup can achieve better separation on one data set than another, we can scale the sum of squares ratio \\(V\\) and obtain the F-statistic and obtain the corresponding p-value as an index for comparison.\nUsing the sum of squares ratio \\(V\\) as an evaluation metric, we are able to construct a pipeline that aims to find the optimal parameter values for the CMPS algorithm by maximizing the sum of squares ratio.\nNote that other measures of an algorithm, such as the accuracy and the AUC (Area Under the Curve), are also important and useful. But when algorithms achieve 100 percent accuracy and AUC value of 1, we need other measures such as the sum of squares ratio to further distinguish the performance of algorithms. In the following section, we will use the sum of squares ratio to compare the CMPS metrics introduced earlier and investigate the effects of different parameter settings.\nResults\n\n\n\n\n\n\n\n\n\nAs presented in the work of Chen et al. (2019), researchers applied the CMPS method to scans of one of the Hamby sets.\nWhile it is not explicitly stated in the paper, we presume this to be Hamby 252, as only those scans were publicly available at the time.\nIn order to show that our implementation of the CMPS algorithm is able to reproduce the results in Chen et al. (2019) and be used for other data sets, we applied our implementation to both Hamby set 252 and Hamby set 44.\nHere we present how we obtained bullet signatures from the Hamby set data: for both Hamby 252 and Hamby 44, we started with scans in the form of x3p files in the database.\nFollowing the framework proposed by Hare et al. (2017), we used the same set of parameters, removed damaged bullet scans, obtained bullet signatures for each bullet land engraving, and removed outliers in bullet signatures.\nNote that researchers of Chen et al. (2019) applied the CMPS algorithm to bullet signatures as well but used a framework different from ours.\nHowever, since their work is not open-source, we were not able to follow their framework and were only able to reproduce the results for Hamby set 252 qualitatively.\nHamby 252\nFigure 9 shows the distribution of \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\) after we applied the CMPS algorithm to Hamby set 252 with the multi-segment lengths strategy.\nThe parameters we used in extract_feature_cmps for the CMPS algorithm are:\n\n\nextract_feature_cmps(\n  x, y,\n  seg_length = 50,\n  Tx = 25,\n  npeaks_set = c(5, 3, 1),\n  include = \"nseg\"\n)\n\n\nAs noted above, the CMPS scores we found here are not exactly the same as those presented in Chen et al. (2019) since we were not able to follow their framework, but the results presented in Figure 9 are qualitatively equivalent to those presented in Chen et al. (2019), showing a clear separation between scores based on comparisons from known matches (KM) and scores from comparisons of known non-matches (KNM) for both \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\).\nAdditionally, to mimic the parameters used in Chen et al. (2019), we set seg_length = 50 and Tx = 25 to make sure that each basis segment has a length of 78.125 m and the tolerance zone is \\(\\pm 39.0625\\) m (one unit represents 1.5625 m for Hamby set 252).\nThe sum of squares ratios are 20.64 and 28.96 for \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\), respectively.\nThis indicates that even though scores from \\(\\mathrm{CMPS_{max}}\\) for known-match comparisons are larger than scores from the averaged version of \\(\\mathrm{\\overline{CMPS}_{max}}\\), these scores achieve a better separation between the two groups of comparisons.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Distribution of \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\) for Hamby 252; outliers are removed in bullet signatures; seg_length = 50, Tx = 25, npeaks_set = c(5,3,1); instead of showing the counts on the y-axis, we present the observed proportions conditioned on KM group and KNM group to enhance the visibility of the bars.\n\n\n\nHamby 44\nSimilar procedures are applied to Hamby set 44, and Figure 10 shows the distribution of \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\), respectively.\nThe parameters used in extract_feature_cmps are:\n\n\nextract_feature_cmps(\n  x, y,\n  seg_length = 61,\n  Tx = 30,\n  npeaks_set = c(5, 3, 1),\n  include = \"nseg\"\n)\n\n\nSince the resolution of Hamby 44 is set to 1.29 m per unit, we make seg_length = 61 and Tx = 30 to ensure that the setup of Hamby set 44 is similar to that of Hamby set 252, resulting in basis segments of 78.69 m and the tolerance zone of \\(\\pm 38.7\\) m.\nAs shown in Figure 10, again, we are able to see a clear separation between the known match comparisons and the known non-match comparisons, even though the separation is relatively small compared with that of Hamby set 252, which is also indicated by the sum of squares ratios.\nFor this specific set of parameters, the sum of squares ratios are 8.87 and 10.64 for \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\), respectively.\nThis might suggest that we could enlarge the separation in terms of the sum of squares ratio by using other CMPS metrics and other sets of parameters.\n\n\n\nFigure 10: Distribution of \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\) for Hamby 44; outliers are removed in bullet signatures; seg_length = 61, Tx = 30, npeaks_set = c(5,3,1); instead of showing the counts on the y-axis, we present the observed proportions conditioned on KM group and KNM group to enhance the visibility of the bars.\n\n\n\nComparing CMPS metrics and parameters\n\n\n\nFigure 11: Comparison of results from the CMPS algorithm based on different basis segment lengths (parameter seg_length). Only the \\(\\mathrm{CMPS_{max}}\\) metric suggests that the default values for the basis segment length result in the best separation. Better separation is achieved based on the modified CMPS metrics, including the newly suggested ones. For Hamby 252 these metrics agree on a segment length of 75, and a segment length of 122 for Hamby 44 yields better results.\n\n\n\n\n\n\nFigure 12: Comparison of CMPS results based on different strategies of number of peak selections. Starred results compare CMPS performance with results published in the literature. Results for the random forest score are represented with circles because the metrics are computed not based on the CMPS scores, but on the random forest scores with the same logic. Since random forest scores lie within the interval \\([0, 1]\\), scaling the random forest scores will not change the results.\n\n\n\nWe investigated the effects of different sets of parameters in the example of both Hamby sets 252 and 44.\nMore specifically, we investigated the separation achieved using the cmpsR implementation under various segment lengths (controlled by the parameter seg_length). Specifically, we fixed the parameter npeaks_set that controls the number of peaks at each segment level to be npeaks_set = c(5, 3, 1) and modified the parameter seg_length.\nFigure 11 shows that the default values of seg_length for Hamby 252 and Hamby 44 (50 for Hamby 252 and 61 for Hamby 44, which represent 78.125 m and 78.69 m, respectively) result in high values of the sum of squares ratio, no matter which CMPS metrics are used for an evaluation.\nHowever, we also see, that for Hamby set 252 a basis segment length of 75 is a better choice than the default segment length; a basis segment length of 122 results in a higher value of the sum of squares ratio for Hamby 44.\nFigure 12 shows the results of the CMPS algorithm using different strategies for identifying peaks in the correlation structure between signatures. Basis segment length is fixed to the default level for this evaluation.\nAs can be seen in Figure 12, the default value of npeaks_set (npeaks_set = c(5, 3, 1)) leads to promising results in terms of the sum of squares ratio; however, other choices of npeaks_set match and exceed this sum of squares ratio value.\nAs seen before, the results suggest that the \\(\\mathrm{CMPS_{max}}\\) metric produces the least amount of separation compared with the other three CMPS metrics.\nThey also suggest that the two newly proposed metrics, \\(\\mathrm{\\overline{CMPS}_{diff}}\\) and \\(\\mathrm{\\overline{CMPS^*}_{diff}}\\), lead to equally good or even better results as \\(\\mathrm{\\overline{CMPS}_{max}}\\).\nBecause \\(\\mathrm{\\overline{CMPS^*}_{diff}}\\) relies on a scaled version of CMPS scores, it is more comparable to other similarity scores and summarizes the out-of-phase background CMPS scores, making it superior to the other CMPS metrics.\nWhat we can also observe in Figure 11 and Figure 12 is that the values of the sum of squares ratio for Hamby 44 is lower than those for Hamby 252.\nThis might be because determining the source is a harder task for Hamby 44 than for Hamby 252, but also suggests that the choice of parameters also depends on the resolution or the scanning process of the data set.\nThe same set of parameters might work for one data set, but not work equally well for another.\nThe purpose of the results shown in Figure 11 and Figure 12 is not to determine the “best” parameters for the CMPS algorithm, but to show that the sum of squares ratio can be used as an evaluation measure to compare different parameters, metrics, or scans.\nA pipeline that maximizes the sum of squares ratio might help researchers determine the set of parameters that work best for their data.\nBut a large database that is representative is what we really need in order to fully understand and cross-validate the parameters of the CMPS algorithm.\nComparing with original results and the random forest model\nChen et al. (2019) present histograms of \\(\\mathrm{CMPS_{max}}\\) and \\(\\mathrm{\\overline{CMPS}_{max}}\\) for npeaks_set = 5 and npeaks_set = c(5, 3, 1) with (presumably) Hamby 252.\nThe values in these histograms allow us to calculate the sum of squares ratios and include the results in Figure 12 as well.\nThey are marked by an asterisk at the top right corner in Figure 12.\nThe sum of squares ratios we obtained for npeaks_set = 5 and npeaks_set = c(5, 3, 1) is slightly higher than those obtained from the histograms of Chen et al. (2019).\nIt’s curious to see that for the Hamby 252 results published in Chen et al. (2019) the \\(\\mathrm{{CMPS}_{max}}\\) metric achieves values of the sum of squares ratio higher than those achieved by the \\(\\mathrm{\\overline{CMPS}_{max}}\\) metric.\nSince the researchers of Chen et al. (2019) did not use \\(\\mathrm{\\overline{CMPS}_{diff}}\\) or \\(\\mathrm{\\overline{CMPS^*}_{diff}}\\), and they did not apply the CMPS algorithm to the Hamby set 44, we were not able to compare those results.\nIn Figure 12, we also included the sum of squares ratios computed from the random forest scores (Hare et al. 2017) for different metrics.\nThe random forest model presented in Hare et al. (2017) was trained at the Center for Statistics and Applications in Forensic Evidence (CSAFE) and is publicly available in the R package bulletxtrctr (Hofmann et al. 2019).\nSimilar to the CMPS algorithm, this random forest model produces a score to quantify the similarity of a land-by-land comparison.\nThe RF scores lie in an interval of [0,1] making them most comparable to the scaled CMPS scores.\nWe applied the logic of \\(\\mathrm{{CMPS}_{max}}\\), \\(\\mathrm{\\overline{CMPS}_{max}}\\), and \\(\\mathrm{\\overline{CMPS}_{diff}}\\) to the random forest scores and obtained results of the random forest model for different metrics.\nAs shown in Figure 12, the random forest model does not achieve the sum of squares ratio as high as those achieved by the top CMPS algorithms for Hamby 252, but it does achieve better results for Hamby 44.\nThis might suggest that inclusion of the CMPS score as an additional feature in the random forest model training might be beneficial for an overall separation to determine the source.\nFor details about the implementation, examples, and results, please refer to the “Supplementary materials”.\nConclusion\nIn this paper, we present the cmpsR package, an open-source implementation of the Congruent Matching Profile Segments (CMPS) algorithm (Chen et al. 2019), and apply it to two datasets in Hamby study (Hamby et al. 2009) to show its potential for further research.\nThe CMPS algorithm was proposed by NIST in 2019 and was made for objective tool marks comparisons.\nWe introduce the basic logic of the CMPS algorithm and layout its implementation in the cmpsR package.\nWe also showcase the functionality of the cmpsR package with a small dataset example that is included in the package.\nIn the cmpsR package we implement some graphing tools for users to visualize results and to gain a better understanding of both the algorithm and the results.\nAdditionally, we propose two new metrics based on the CMPS scores and compare the new metrics with the existing metrics.\nWe also introduce a principled evaluation framework of algorithmic results using a measure based on the sum of squares ratio.\nWe showcase the implementation with two datasets in the Hamby study (Hamby set 252 and Hamby set 44) and compare the CMPS algorithm using different sets of parameters and different metrics, following the evaluation framework based on the sum of squares ratio.\nThe results obtained are promising: we were both able to reproduce the results in Chen et al. (2019) qualitatively and achieve a clear separation between the known match (KM) comparisons and known non-match (KNM) comparisons in another bullet study.\nHowever, the comparisons among different sets of parameters suggest that the optimal choice for parameter settings varies between different datasets.\nNote, that the main difference between Hamby sets 44 and 252 is that they were taken with different resolution scanning devices. The bullets for Hamby 44 and Hamby 252 are fired from the same ten consecutively rifled P-85 Ruger barrels. The difference in parameter settings for optimal differentiation between same-source comparisons and different-source comparisons is therefore quite surprising. In the next steps, we need validation studies similar to Vanderplas et al. (2020) - trying out the CMPS algorithm on different firearms and ammunition combinations to test the limits of the algorithm. The evaluation framework we proposed based on the sum of squares ratio will facilitate such studies and other validation of the algorithm.\nComparisons of the R implementation of the CMPS algorithm with the random forest model proposed by Hare et al. (2017) suggest that adding the CMPS score as an additional feature in the random forest model might add further separation between known match (KM) comparisons and known non-match (KNM) comparisons.\nThe open-source implementation of the CMPS algorithm provided by the cmpsR package is just one step towards to the framework of open science. Open science is particularly important to fields like forensic science where transparency and accuracy are critical to fair justice.\nOpen-source implementations not only allow a peer-review but also facilitate further research, such as parameter cross-validations, method comparisons, and the development of statistical methods for modeling KM and KNM CMPS score distributions, which can then be used for error-rate estimations and fair applications that were called for by the PCAST (President’s Council of Advisors on Science and Technology 2016) report.\nHowever, having open-source implementations is not enough for open science.\nIn order to actually build the world of open science, many other efforts, such as open evaluation, open access publication, open educational resources, etc., are still needed.\nThe database built and maintained by the National Institute of Standards and Technology is a good example of open data.\nWhat we are aiming for is an open system that is able to collect open results, compare multiple algorithms using multiple datasets, and evaluate algorithmic variation and accuracy.\nThe evaluation metric we proposed in this paper can be used to compare different algorithms or even different datasets and is a snippet of this open system.\nBut the core of this open system is the open science culture and contributions of the community.\nAcknowledgement\nThis work was funded (or partially funded) by the Center for Statistics and Applications in Forensic Evidence (CSAFE) through Cooperative Agreements 70NANB15H176 and 70NANB20H019 between NIST and Iowa State University, which includes activities carried out at Carnegie Mellon University, Duke University, University of California Irvine, University of Virginia, West Virginia University, University of Pennsylvania, Swarthmore College and University of Nebraska, Lincoln.\nSupplementary materials\nThe zip file “supplementary-files.zip” contains R scripts for reproducing all aspects of this paper.\nData not included in “supplementary-files.zip”\nThe processed data of Hamby set 252 and Hamby set 44 are stored in two .rds files:\nBulletSignatures252.rds\nBulletSignatures44.rds\nAnd the ground truth of Hamby set 252 is provided in StudyInfo.xlsx\nDue to the size of the file, these processed data are not included in the “supplementary-files.zip”, but the links are provided for download.\nIn order to reproduce the results in the paper, please download “BulletSignatures252.rds”, “BulletSignatures44.rds”, and “StudyInfo.xlsx” and save them in a folder named “bullet_signatures_etc”. And place the folder “bullet_signatures_etc” into the folder of all the R scripts of “supplementary-files.zip”.\nPlease check out the folder structure of the reproducible folder for reference.\nFile description\nhamby*_result_generator.R: these R scripts take the processed data of Hamby set 252 and Hamby set 44, generate preliminary results used in the paper, and save the results in the .csv format in the folder data-csv. Please make sure that the package versions of bulletxtrctr and cmpsR meet the requirements.\nrds_generator.R: this R script takes the generated .csv files and produce CMPSpaper_results.rds. CMPSpaper_results.rds is identical to data/CMPSpaper_results.rds and is used generate figures and other results presented in the paper.\nData included in “supplementary-files.zip”\ndata-csv: this folder contains .csv files we generated using R scripts hamby*_result_generator.R\nCMPSpaper_results.rds: this is the .rds file we generated using R script rds_generator.R\nThese data can be used as reference or example results of the reproducible codes\ncsafe_rf2.rds: this .rds file contains the random forest model used in this paper\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-035.zip\nCRAN packages used\ncmpsR, x3ptools\nCRAN Task Views implied by cited packages\n\n\nAFTE Criteria for Identification Committee. Theory of identification, range striae comparison reports and modified glossary definitions. AFTE Journal, 24(3): 336–340, 1992.\n\n\nD. J. Brundage. The Identification of Consecutively Rifled Gun Barrels. AFTE Journal, 30(3): 438–444, 1998.\n\n\nW. Chang, J. Cheng, J. Allaire, C. Sievert, B. Schloerke, Y. Xie, J. Allen, J. McPherson, A. Dipert and B. Borges. shiny: Web Application Framework for R. 2021. URL https://CRAN.R-project.org/package=shiny. R package version 1.6.0.\n\n\nZ. Chen, W. Chu, J. A. Soons, R. M. Thompson, J. Song and X. Zhao. Fired bullet signature correlation using the Congruent Matching Profile Segments (CMPS) method. Forensic Science International, 305: Article 109964, (10 pages), 2019. URL https://www.sciencedirect.com/science/article/pii/S0379073819303767.\n\n\nL. S. Chumbley, M. D. Morris, M. J. Kreiser, C. Fisher, J. Craft, L. J. Genalo, S. Davis, D. Faden and J. Kidd. Validation of tool mark comparisons obtained using a quantitative, comparative, statistical algorithm: VALIDATION OF TOOL MARK COMPARISONS. Journal of forensic sciences, 55(4): 953–961, 2010. DOI https://doi.org/10.1111/j.1556-4029.2010.01424.x.\n\n\nW. S. Cleveland, E. Grosse and W. M. Shyu. Local regression models. In Statistical models in s, Eds J. M. Chambers and T. J. Hastie pages. 309–376 1991. Boca Raton, Florida: Chapman; Hall/CRC.\n\n\nCommittee on Identifying the Needs of the Forensic Sciences of the National Research Council. Strengthening Forensic Science in the United States: A Path Forward. 2009.\n\n\nJ. E. Hamby, D. J. Brundage, N. D. K. Petraco and J. W. Thorpe. A Worldwide Study of Bullets Fired From 10 Consecutively Rifled 9MM RUGER Pistol Barrels—Analysis of Examiner Error Rate. Journal of Forensic Sciences, 64(2): 551–557, 2019. DOI 10.1111/1556-4029.13916.\n\n\nJ. E. Hamby, D. J. Brundage and J. W. Thorpe. The Identification of Bullets Fired from 10 Consecutively Rifled 9mm Ruger Pistol Barrels: A Research Project Involving 507 Participants from 20 Countries. AFTE Journal, 41(2): 99–110, 2009.\n\n\nE. Hare, H. Hofmann and A. Carriquiry. Automatic matching of bullet land impressions. Ann. Appl. Stat., 11(4): 2332–2356, 2017. DOI 10.1214/17-AOAS1080.\n\n\nH. Hofmann, S. Vanderplas and G. Krishnan. Bulletxtrctr: Automatic matching of bullet striae. 2019. URL https://heike.github.io/bulletxtrctr/. R package version 0.2.0.\n\n\nH. Hofmann, S. Vanderplas, G. Krishnan and E. Hare. x3ptools: Tools for Working with 3D Surface Measurements. 2020. URL https://github.com/heike/x3ptools. R package version 0.0.3.\n\n\nG. Krishnan and H. Hofmann. Adapting the Chumbley Score to Match Striae on Land Engraved Areas (LEAs) of Bullets. J Forensic Sci, 64(3): 728–740, 2019.\n\n\nPresident’s Council of Advisors on Science and Technology. Report on forensic science in criminal courts: Ensuring scientific validity of feature-comparison methods. 2016.\n\n\nJ. Song, L. Ma, E. Whitenton and T. Vorburger. 2D and 3D surface texture comparisons using autocorrelation functions. In Measurement technology and intelligent instruments VI, pages. 437–440 2005. Trans Tech Publications Ltd. DOI 10.4028/www.scientific.net/KEM.295-296.437.\n\n\nUnited States Department of Justice, Federal Bureau of Investigation. Crime in the United States, 2019.URL https://ucr.fbi.gov/crime-in-the-u.s/2019/crime-in-the-u.s.-2019/tables/expanded-homicide-data-table-8.xls. Expanded Homicide Data Table 8.\n\n\nUnited States Department of Justice, Federal Bureau of Investigation. Expanded Homicide Tables, 2020.URL https://s3-us-gov-west-1.amazonaws.com/cg-d4b776d0-d898-4153-90c8-8336f86bdfec/CIUS/downloads/2020/expanded-homicide-2020.zip. Data available through the link, not published to the website yet.\n\n\nS. Vanderplas, M. Nally, T. Klep, C. Cadevall and H. Hofmann. Comparison of three similarity scores for bullet LEA matching. Forensic Science International, 308: 110167, 2020. URL https://www.sciencedirect.com/science/article/pii/S0379073820300293.\n\n\nX. A. Zheng. NIST Ballistics Toolmark Research Database (NBTRB). 2016. URL https://tsapps.nist.gov/NRBTD.\n\n\n\n\n",
    "preview": "articles/RJ-2022-035/img/barrel_bullet_ps.png",
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {},
    "preview_width": 836,
    "preview_height": 516
  },
  {
    "path": "articles/RJ-2022-036/",
    "title": "rassta: Raster-Based Spatial Stratification Algorithms",
    "description": "Spatial stratification of landscapes allows for the development of efficient  sampling surveys, the inclusion of domain knowledge in data-driven modeling  frameworks, and the production of information relating the spatial variability  of response phenomena to that of landscape processes. This work presents the  rassta package as a collection of algorithms dedicated to the spatial  stratification of landscapes, the calculation of landscape correspondence  metrics across geographic space, and the application of these metrics for  spatial sampling and modeling of environmental phenomena. The theoretical  background of rassta is presented through references to several studies  which have benefited from landscape stratification routines. The functionality  of rassta is presented through code examples which are complemented with  the geographic visualization of their outputs.",
    "author": [
      {
        "name": "Bryan A. Fuentes",
        "url": {}
      },
      {
        "name": "Minerva J. Dorantes",
        "url": {}
      },
      {
        "name": "John R. Tipton",
        "url": {}
      }
    ],
    "date": "2022-10-13",
    "categories": [],
    "contents": "\n1 Introduction\nThe application of robust, quantitative approaches for the spatial modeling of\nenvironmental phenomena has increased in the past few decades mainly due to an\nincrease in computational power, advances in statistical modeling, and the\navailability of geospatial layers of environmental information (Scull et al. 2003; Elith and Leathwick 2009). Most of these approaches aim at building explicit quantitative\nrelationships between environmental controls and response phenomena through\nstatistical learning. Examples of these approaches include digital soil mapping\n(DSM) (McBratney et al. 2003), species distribution modeling (SDM) (Guisan and Zimmermann 2000), land\nuse/land cover classification (Ham et al. 2005), and forest fire modeling\n(Chuvieco et al. 2010). Despite the extensively documented success of these approaches,\nthere are still some challenges that limit their application. For instance, poor\nstatistical performance is often reported in studies where input data is too\nlimited to accurately represent control-response relationships (Araújo and Guisan 2006).\nMoreover, model parsimony and interpretation of results can be compromised when\nusing ‘black-box’ algorithms (Arrouays et al. 2020). Similarly, including a priori\nknowledge about natural processes in purely statistical approaches can be\nchallenging to achieve (Heuvelink and Webster 2001).\nSeveral studies have suggested embedding spatial stratification routines within\napproaches such as DSM, SDM, land use/cover mapping, forest fire modeling, and\nothers to overcome the challenges limiting their application. In such studies,\nthe spatial stratification of landscapes creates units with reduced spatial\nvariability of environmental phenomena as compared to the overall variability\nacross a landscape. The use of these units allows the researcher to (a) obtain\nbalanced representations of control-response relationships (Guisan and Zimmermann 2000; West et al. 2016); (b) include expert knowledge of physical processes for improving\nmodeling with limited data (Zhu et al. 2008); (c) improve the performance of\nparameterization of mechanistic models (Park and Van De Giesen 2004; Baldwin et al. 2017); and, (d)\nfacilitate the interpretation of environmental conditions and their influence on\nthe spatiotemporal variability of processes of interest (Rodrigues et al. 2019).\nIn general, landscape stratification routines follow fundamental ecological\nconcepts that explain the hierarchical and multi-scale nature of relationships\nbetween environmental phenomena across space (Allen and Starr 1982). Therefore, landscape\nstratification methods have been applied in many studies that use geospatial\ninformation for environmental modeling, such as those previously cited. However,\nfew packages exist in the R environment with functions strictly aimed at\nlandscape stratification routines using geospatial data. Although one could\nimplement custom stratification algorithms using multiple all-purpose geospatial\nanalysis packages such as terra (Hijmans 2021) and sf\n(Pebesma 2018), the ease of use, reproducibility, and replicability of analysis\nis often enhanced when algorithms are implemented as part of a dedicated\npackage. The motif package (Nowosad 2021) is the only example the\nauthors could find of a package that is fully dedicated to landscape\nstratification in R using geospatial data. Although the methods offered by\nmotif are effective for large-scale studies (Jasiewicz et al. 2015; Nowosad 2021), their application is currently limited to rasters of categorical\ndata. Thus, motif is not practical for the modeling of spatially\ncontinuous environmental phenomena, which is often a goal of landscape\nstratification routines.\nThis work presents the rassta package as a collection of algorithms\nfor the spatial stratification of landscapes, sampling, and modeling of\nenvironmental phenomena. The rassta package is not intended as a drop-in\nreplacement for statistically-robust environmental modeling approaches. Rather,\nit is intended to serve as a generalized framework to derive geospatial\ninformation that can be used to improve inference with these statistical\napproaches.\n2 Conceptual overview and functionality\nThe algorithms in the rassta package assist in the analysis of\nenvironmental information related to the spatial variability of natural\nphenomena across landscapes. These functions focus on integrating standard\ngeospatial techniques and quantitative analysis in a generalized framework for\nlandscape stratification, sampling, and modeling. All of the functions in the\nrassta package take geospatial data in raster format as input. In the\ncontext of geographic information systems (GIS), the raster format can be\nconsidered a graphical representation of a matrix that is organized in rows and\ncolumns, and which may be stacked in multiple layers (e.g., multi-band satellite\nimagery). Each cell (pixel) in the raster contains a value representing a\nspatially-varying phenomenon, such as elevation or precipitation. A few\nfunctions in rassta also produce geospatial data in vector format. Vector\ndata represents geometric entities in the form of points, lines, and polygons.\nThe rassta package uses the highly efficient terra package as the\nbackbone for handling raster and vector data. Most of the geospatial data\nmanipulation with terra is performed in C++ and is based on two main R\ndata types (classes): SpatRaster and SpatVector. Note that terra\nimports the Rcpp package (Eddelbuettel and François 2011) since terra uses\nC++ (including external pointers) to manipulate these classes.\nMost of the functions implemented in rassta are interrelated in the sense\nthat the outputs from some functions can be used as the inputs for others. This\nfunctional interrelation allows for a generalized framework to conduct spatial\nstratification, sampling, and modeling in a single package following a\nproject-oriented approach. In general, the functions of rassta can be\ngrouped into five categories: (a) landscape stratification; (b) landscape\ncorrespondence metrics; (c) stratified sampling; (d) spatial modeling; and (e)\nmiscellaneous (Figure 1). Each category and its corresponding\nfunctions (except for miscellaneous) are theoretically founded on several\nstudies focused on understanding spatially-varying natural phenomena across\nlandscapes. In the next sections, the rationale behind each category and its\nfunctions is described. This description is complemented with references to\ncorresponding scientific literature and includes code examples showing the\napplication of each function with extensive use of plotting functions (for\nvisualization purposes only). Most of the plotting functions are derived from\nthe terra package using the SpatRaster and SpatVector classes. [Note:\nTo reduce the extension of code examples, all the map and graph plotting\nfunctions were consolidated in the function figure()].\n\n\n\nFigure 1: Functions of the rassta package. Connectors relate the inputs and outputs of the functions. The functions can be grouped in five categories: landscape stratification, landscape correspondence metrics, stratified sampling, predictive modeling, and miscellaneous.\n\n\n\nLandscape stratification\nSeveral studies have suggested the need to account for the hierarchical and\nmulti-scale nature of landscape processes. Allen and Starr (1982) suggested that landscape\nprocesses can be explained through hierarchical multivariate structures given\ntheir multiple spatial and temporal scales. Based on Dokuchaeiv’s theory of soil\nformation (Glinka 1927) and the soil-landscape paradigm (Hudson 1992),\nMcSweeney et al. (1994) proposed a nested model of soil-landscape processes at the\nphysiographic, geomorphometric, and within-soil domains. Flügel (1995) suggested\nthat the regionalization of hydrology-related processes should consider the\nmulti-scale landscape heterogeneity in terms of soil, topography, geology,\nclimate, and vegetation. These ideas have led these and other authors to\nformulate frameworks for the creation of spatial entities that stratify the\nlandscape. The general purpose of these entities has been to define\nspatially-explicit domains that represent distinctive landscape processes and/or\ninteractions (McSweeney et al. 1994). Accordingly, spatial stratification using\nrassta focuses on the creation of such domains (hereafter referred to as\nunits).\nThe landscape stratification process with rassta follows a hierarchical\napproach similar to Austin and Heyligers (1989), who individually classified gradients of\nprecipitation and elevation into intervals that were intersected with geologic\nclasses for sampling purposes. Similarly, in rassta, a set of first-level\nunits is created separately for each landscape factor under analysis. Then,\nmultiple sets of first-level units are integrated into a single set of\nsecond-level units. The first-level units, called classification units, can be\ncreated outside of rassta via multicriteria analysis, statistical\nlearning, or other methods. Moreover, the classification units can be formally\ndefined through classification schemes, such as those based on taxonomic keys.\nThe second-level units, called stratification units, result from the spatial\nintersection of multiple sets of classification units. Note that both\nclassification and stratification units represent a spatial stratification for a\ngiven landscape. Figure 2 shows an example of a simple landscape\nstratification process based on two landscape factors, each with three raster\nlayers representing continuous variables.\n\n\n\nFigure 2: Schematic of a landscape stratification process. Raster layers of variables related to landscape factors are the inputs. The outputs are raster layers representing classification and stratification units.\n\n\n\nThere are three important aspects of the stratification approach used within\nrassta that must be considered. (a) One can simply create stratification\nunits by incorporating variables from multiple landscape factors in a single\nclassification process. However, the interpretation of results is often\ncompromised when using a large number of variables in “all-in-the-bag”\nstatistically driven classification schemes. (b) Multiple sets of classification\nunits can belong to a single landscape factor, and each set can be created from\nvariables at a distinct spatial scale. Presumably, this can account for the\nmulti-scale nature of landscape factors in the stratification process. (c) A\nlandscape factor can be represented by a single categorical variable, as in the\ncase of geologic units or soil parent material. In this case, the landscape\nfactor/variable is already in the form of classification units. Figure\n3 shows a landscape stratification scenario like that addressed in\n(b) and (c).\n\n\n\nFigure 3: Schematic of a multi-scale landscape stratification process including a categorical variable. The stratification is based on three landscape factors: local scale terrain, regional scale terrain, and geology. Each terrain landscape factor is represented by raster layers of variables (local scale: slope and convergence index and regional scale: aspect and regional terrain). Geology is represented by a single categorical raster layer. Three sets of classification units (CU), one each for local terrain, regional terrain, and geology, are intersected to produce one set of stratification units (SU).\n\n\n\nClassification units\nA set of n classification units represents n distinct landscape\nconfigurations related to a single landscape factor. Note that the term\nlandscape configuration is used here as a generic term for a particular\npattern in the spatial variability of one or multiple variables belonging to a\nlandscape factor. Currently, rassta allows the creation of classification\nunits via unsupervised learning thanks to its functions som_gap() and\nsom_pam(). The function som_gap() performs dimension reduction based on the\nself-organizing map (SOM) proposed by Kohonen (1990). The R package\nkohonen (Wehrens and Kruisselbrink 2018) is called internally by som_gap() to produce\nthe SOM. The function som_gap() also performs cluster analysis on the SOM\ncodes based on the partitioning around medoids (PAM) (Kaufman and Rousseeuw 1990), with\nestimation of the optimum number of clusters (k) through the gap statistic\n(Tibshirani et al. 2001). It is important to mention that the output SOM object\nreturned by som_gap() can be used as input for any other clustering algorithm\n(e.g., hierarchical, spectral, etc.) or statistical analysis outside of\nrassta.\nThe code below shows how som_gap() reduces the feature space and selects k\nclusters from four terrain variables. Note that the processing time of\nsom_gap() is significant (around 162 seconds on a 4-cores Intel processor at\n3.2 GHz for the following example). The processing time increases as the number\nof cells/layers in the argument var.rast increases, and/or as the argument\nK.max increases.\n\n\n# Load the rassta and terra packages\nlibrary(rassta)\nlibrary(terra)\n# Note that terra imports Rcpp, but if Rcpp is not automatically loaded then:\nlibrary(Rcpp)\n# Get the data required to run the examples from rassta’s installation folder\nwasoil <- system.file(\"exdat/wasoil.zip\", package = \"rassta\")\n# Copy data to current working directory and extract files\nfile.copy(from = wasoil, to = getwd())\nunzip(\"wasoil.zip\")\n\n# Set seed\nset.seed(963)\n# Multi-layer SpatRaster with 4 terrain variables\nterr.var <- rast(c(\"height.tif\", \"midslope.tif\", \"slope.tif\", \"wetness.tif\"))\n# Scale variables to mean = 0 and standard deviation = 1\nterr.varscale <- scale(terr.var)\n# Dimensionality reduction and estimation of optimum k (max k to evaluate: 12)\nterr.som <- som_gap(terr.varscale, xdim = 10, ydim = 10, K.max = 12)\n# Plot results\nfigure(4, d = list(terr.var, terr.som))\n\n\n\n\n\nFigure 4: Dimension reduction and selection of number of clusters (k). The top row shows four terrain variables (height, midslope, slope, and wetness) that are used to generate the self-organizing map (SOM). The bottom row shows the reduced feature space of each variable and the Gap statistic that is used to select k for the construction of classification units.\n\n\n\nThe function som_pam() creates raster versions from the outputs of\nsom_gap(). The code below shows how som_pam() creates raster versions of the\nSOM grid and PAM clustering computed in the previous example.\n\n\n# Rasterization of terrain SOM grid and terrain PAM clustering\nterr.sompam <- som_pam(ref.rast = terr.var[[1]], kohsom = terr.som$SOM,\n                       k = terr.som$Kopt)\n# Plot results\nfigure(5, d = list(terr.sompam, terr.var))\n\n\n\n\n\nFigure 5: SOM grid and PAM clustering. Rasterized versions of the terrain SOM grid (left) and the terrain PAM clustering (right) are produced. The resulting clusters represent the classification units for the terrain landscape factor.\n\n\n\nNote that the approach for creating classification units should not be limited\nto that offered by som_gap() and som_pam(). There are many other approaches\noutside of rassta that can be followed, such as supervised classification\nbased on statistical learning, or GIS-based multicriteria analysis. The best\napproach may depend on the research question(s) being addressed. Therefore, the\nselection of the proper approach and the optional use of other R packages and/or\nGIS software is left to the user. Also, note that classification units created\noutside of rassta are completely compatible with rassta objects and\nmethods if the units are represented through the SpatRaster class from\nterra.\nStratification units\nA set of n stratification units represents n distinct landscape\nconfigurations related to multiple landscape factors. Note that the term\nlandscape configuration is used here as a generic term for a particular\npattern in the spatial variability of multiple variables belonging to multiple\nlandscape factors, or to the same factor represented at multiple spatial scales.\nThe function strata() allows the spatial intersection of multiple sets of\nclassification units into a single set of stratification units. This function\nalso assigns a unique numeric code to each stratification unit. The numeric code\nmakes it possible to trace back each classification unit composing a given\nstratification unit. The code below shows the construction of stratification\nunits with strata() using classification units from three landscape factors\n(climate, soil parent material, and terrain).\n\n\n# Multi-layer SpatRaster with 3 sets of classification units\nall.cu <- rast(c(\"climate.tif\", \"material.tif\", \"terrain.tif\"))\n# Stratification units\nsu <- strata(cu.rast = all.cu)\n# Plot results\nfigure(6, d = list(su, all.cu))\n\n\n\n\n\nFigure 6: Creation of stratification units from sets of classification units. A set of classification units is produced for each of three landscape factors: climate, soil parent material, and terrain. The spatial intersection of these sets results in the stratification units for the landscape (upper left map).\n\n\n\nMetrics of landscape correspondence\nThere are two metrics of landscape correspondence that can be calculated with\nrassta: (a) the spatial signature of classification units, and (b) the\nlandscape similarity to stratification units. These metrics quantify the\nrelative correspondence between any location across geographic space and\nlandscape configurations represented by classification and stratification units.\nSeveral studies have applied similar concepts related to continuous\ncorrespondence between landscape configurations for the modeling of\nspatially-varying phenomena. Early examples include studies using multivariate\ndistance metrics in the feature space for SDM (Carpenter et al. 1993) and studies\napplying the fuzzy set theory (Zadeh 1965) for multicriteria evaluation\n(Burrough 1989), DSM (Zhu and Band 1994) and landform classification (MacMillan et al. 2000).\nSpatial signature of classification units\nThe spatial patterns of the degree of correspondence between any landscape\nconfiguration and the configuration represented by a given classification unit\nare defined as the spatial signature. The spatial signature is represented by\na raster layer of continuous values that results from the cell-wise aggregation\nof empirical distribution functions mapped over geographic space. Each\ndistribution function corresponds to one variable and relates the classification\nunit to “typical” values of the variable within the classification unit. The\nconcept of spatial signature is based on the work of Pike and Rozema (1975) and Pike (1988).\nThese authors used the term geometric signature to describe a set of sample\nstatistics (e.g., mean, standard deviation) of terrain variables (e.g., slope,\ncurvature) used to distinguish “geomorphically disparate landscapes”\n(Pike 1988).\nThe spatial signature in rassta replaces the geometric signature’s\nmeasurements of central tendency and dispersion statistics with statistical\ndistribution functions generated across geographic space. The statistical\ndistribution functions used in rassta are: (a) the probability density\nfunction (PDF) based on the kernel density estimation, (b) the empirical\ncumulative distribution function (ECDF), and (c) an inverted version of the ECDF\n(iECDF). Note that the spatial signature concept is somewhat similar to the\nvirtual ecological niche (Hirzel et al. 2001) and the multivariate environmental\nsimilarity surface (Elith et al. 2010), which are implemented in R through the packages\nvirtualspecies (Leroy et al. 2016) and dismo (Hijmans et al. 2020),\nrespectively. Figure 7 and Figure 8 show an illustration\nand a pseudocode of the process to calculate the spatial signature of a\nclassification unit, respectively. Note that the function FUNSIG() in the\npseudocode is just a placeholder to encompass the three functions from\nrassta that are required to calculate spatial signatures. These functions\nare select_functions(), predict_functions(), and signature(), each will be\nfurther discussed next.\n\n\n\nFigure 7: Schematic of the calculation process for spatial signatures. A set of classification units is produced using three variables. A distribution function is calculated for each variable within classification unit 1, and then predicted across geographic space. The predicted functions for unit 1 are aggregated, which results in the spatial signature of that unit.\n\n\n\n\n\n\nFigure 8: Pseudocode of the calculation process for spatial signatures. The calculation process involves the selection, prediction, and aggregation of distribution functions. The spatial signature is calculated for each classification unit in a set.\n\n\n\nAn important assumption is made when using the PDF, ECDF, and iECDF to\ncharacterize the typical values of a given variable within a given\nclassification unit. The position of a value within the distribution function is\nan indicator of how typical the value is in terms of the variable’s distribution\nwithin the classification unit. For instance, values closer to, or at the peak\nof the PDF are assumed to be the most typical values of the variable within the\nclassification unit. Contrarily, values at the tails of the PDF are the less\ntypical. Although one could simply use the PDF as a generalized function to\ndenote typical values, this function assigns the same weight to values at the\ntails of the distribution regardless of the tail’s location (left or right). In\nsome cases, a priori knowledge can dictate that typical values of a variable\nwithin a given classification unit are those approaching \\(+\\infty\\), or those\napproaching \\(-\\infty\\). The use of the ECDF and the iECDF is intended for those\ncases. More specifically, if a classification unit is known to be associated\nwith a variable’s extreme values toward \\(+\\infty\\), then the ECDF can be used to\nrepresent this association. Conversely, if the classification unit is associated\nwith those variable’s extreme values toward \\(-\\infty\\), then the iECDF can be\nused.\nThe function select_functions() allows the user to select the statistical\ndistribution function used to represent the typical values for a given variable\nwithin a specific classification unit. Both automatic and interactive selection\nmodes are supported, with the latter based on a shiny app\n(Chang et al. 2021). The automatic selection of distribution functions is based on\nwithin-unit statistics, also referred to as zonal statistics in the GIS\nliterature, and it follows the criteria described next:\nPDF = when the mean (or median) of the variable’s values within the\nclassification unit is neither the maximum nor the minimum of all the mean (or\nmedian) values across all the units.\nECDF = when the mean (or median) of the variable’s values within the\nclassification unit is the maximum of all the mean (or median) values across all\nthe units.\niECDF = when the mean (or median) of the variable’s values within the\nclassification unit is the minimum of all the mean (or median) values across all\nthe units.\nThe code below shows the automatic selection of statistical distribution\nfunctions for four climatic classification units and two variables with\nselect_functions().\n\n\n# Multi-layer SpatRaster with 2 climatic variables\nclim.var <- rast(c(\"precipitation.tif\", \"temperature.tif\"))\n# Single-layer SpatRaster with 4 climatic classification units\nclim.cu <- rast(\"climate.tif\")\n# Automatic selection of statistical distribution functions\nclim.difun <- select_functions(cu.rast = clim.cu,\n                               var.rast = clim.var,\n                               mode = \"auto\")\n# Plot results\nfigure(8, d = list(clim.difun, clim.cu, clim.var))\n\n\n\n\n\nFigure 9: Selection of distribution functions. A set of four climatic classification units are produced using two variables: precipitation and temperature. A distribution function is selected for each variable within each classification unit.\n\n\n\nThe selected distribution functions can be used to generate predictions of\ndistribution function values over geographic space with the function\npredict_functions() as shown in the code below. The predictions are generated\nby fitting a locally estimated scatterplot smoothing (LOESS) regression with the\nwithin-unit distribution function’s values (y) and the within-unit variable’s\nvalues (x). The fitted LOESS and the raster layer of the variable are then\nused to predict new distribution function values across geographic space.\n\n\n# Multi-layer SpatRaster of climatic variables and classification units\nclim.all <- c(clim.var, clim.cu)\n# Ouput table from select_functions()\ndf <-  clim.difun$distfun\n# Predicted distribution functions for climatic variables\nclim.pdif <- predict_functions(cuvar.rast = clim.all,\n                               cu.ind = 3,\n                               cu = df$Class.Unit,\n                               vars = df$Variable,\n                               dif = df$Dist.Func)\n# Plot results\nfigure(9, d = list(clim.pdif, clim.cu))\n\n\n\n\n\nFigure 10: Prediction of distribution functions. A selected distribution function for each variable is predicted across geographic space. The predicted distribution function relates the landscape to a classification unit with regard to a variable.\n\n\n\nThe function signature() calculates the spatial signature of a given\nclassification unit by aggregating all of the predicted distribution functions\nassociated with the unit. The code below shows the calculation of spatial\nsignatures with signature(). Note that the arguments inprex and outname\nallow the user to identify the raster layers representing the predicted\ndistribution functions associated with each classification unit in a set, and to\nassign a unique name to each resulting raster layer of spatial signature,\nrespectively.\n\n\n# Spatial signatures from distribution functions predicted for climatic variables\nclim.sig <- signature(pdif.rast = clim.pdif,\n                      inprex = paste(seq(1, 4), \"_\", sep = \"\"),\n                      outname = paste(\"climate_\", seq(1, 4), sep = \"\"))\n# Plot results\nfigure(10, d = list(clim.sig, clim.cu))\n\n\n\n\n\nFigure 11: Calculation of spatial signatures. For each climatic classification unit (1 thru 4), the distribution functions (see Figure 10) are aggregated (e.g., mean pixel value) to produce the spatial signature of the unit. The spatial signature relates each position in the landscape to the landscape configuration represented by a classification unit.\n\n\n\nLandscape similarity to stratification units\nThe spatial patterns of the degree of correspondence between any landscape\nconfiguration and the landscape configuration represented by a given\nstratification unit are defined as the landscape similarity. The landscape\nsimilarity is represented by a raster layer of continuous values, which results\nfrom the cell-wise aggregation of the spatial signatures of multiple\nclassification units. This aggregation is possible because any given\nstratification unit is the result of the spatial intersection of multiple\nclassification units, commonly one per landscape factor or factor scale (see\nFigure 2 and 3). Moreover, each classification unit has\none spatial signature associated with it. Therefore, any given stratification\nunit will be associated with multiple spatial signatures, which can be cell-wise\naggregated to calculate the landscape similarity. Figure 12 shows an\nexample of the calculation process for a layer of landscape similarity to\nstratification unit.\n\n\n\nFigure 12: Schematic of the calculation process for landscape similarities. Sets of variables for each landscape factor (terrain and climate) are combined to produce sets of classification units (two each for terrain and climate), which are further combined to produce stratification units (12, 11, 21, and 22). Thus, each stratification unit has two classification units associated with it. Moreover, each classification unit has a spatial signature associated with it. Aggregating the spatial signatures of classification unit 1 for climate and unit 1 for terrain, both associated with stratification unit 11, results in the landscape similarity to that stratification unit.\n\n\n\nThe function similarity() calculates the landscape similarity layer for each\nstratification units in a given set (with the set being represented by a\nsingle-layer SpatRaster object), as shown in the following example. The\nargument su.code indicates the name of the landscape factors/factor scales\nused to create the stratification units, and the digit position (start, end) of\nthe classification units’ ID in the stratification unit’s numeric code.\n\n\n# Multi-layer SpatRaster with spatial signatures of classification units\nclim.sig <- rast(list.files(pattern = \"climate_\")) # For climatic units\nmat.sig <- rast(list.files(pattern = \"material_\")) # For soil parent material units\nterr.sig <- rast(list.files(pattern = \"terrain_\")) # For terrain units\n# Single-layer SpatRaster of stratification units\nsu <- rast(\"su.tif\")\n# Landscape similarity to stratification units\nsu.ls <- similarity(su.rast = su, sig.rast = c(clim.sig, mat.sig, terr.sig),\n                    su.code = list(climate = c(1, 1), material = c(2, 2),\n                                   terrain = c(3, 3)))\n# Plot results\nfigure(12, d = list(su.ls, su, clim.sig, mat.sig, terr.sig))\n\n\n\n\n\nFigure 13: Metrics of landscape correspondence. Landscape similarity (extreme left) and spatial signatures for climate, parent material (material), and terrain associated with stratification units (SU) 111 (top row) and 468 (bottom row). The red polygons indicate the boundaries of the corresponding SU defined through the aggregation (i.e., mean pixel value) of the set of spatial signatures for that SU.\n\n\n\nStratified non-probability sampling\nStratified sampling is an efficient technique for achieving an adequate\nrepresentation of environmental variability, reducing cost of field work, and\nimproving modeling with limited observations (Austin and Heyligers 1989; Wessels et al. 1998; Guisan and Zimmermann 2000; Zhu et al. 2008; West et al. 2016). Accordingly, sampling with rassta to\nselect observations/sampling locations is performed in a stratified fashion\nusing stratification units. Additionally, the raster layers of landscape\nsimilarity to stratification units can be included in the sampling process.\nIncluding the landscape similarity layers results in a non-probability sample.\nFor each stratification unit, the sampling process selects the\nobservation(s)/sampling location(s) at the raster cell where the highest\nlandscape similarity value occurs, resulting in a stratified, non-probability\nsample that is biased towards maximizing the representativeness of landscape\nconfigurations. This idea of biased, stratified sampling is based on the work of\nGillison (1983); Gillison and Brewer (1985), Austin and Heyligers (1989), and Zhu et al. (2008). These authors have\nsuggested that bias related to landscape configurations is relevant for the\nmaximization of environmental representativeness, detection of maximum\ndiversity, and representation of non-stochastic control-response relationships.\nThe function observation() performs the automatic selection of the\nrepresentative response observation for each stratification unit in a given set.\nGiven a stratification unit, the unit’s representative response observation is\nthat whose value best reflects the influence that the unit’s landscape\nconfiguration exerts on the response. This function requires a set of\nobservations/samples already collected for a set of stratification units.\nCurrently, observation() selects observations based on the following methods:\n(a) mls: select the observation at the raster cell with the maximum landscape\nsimilarity value; (b) mrv: select the observation whose response value is the\nmedian of all the values; and (c) random: select an observation at random.\nNote that the latter represents a case of stratified random sampling.\nThe code below shows the selection of representative soil organic carbon (SOC)\nobservations based on the maximum landscape similarity method. Note that the\narguments su.rast and ls.rast require the stratification units and landscape\nsimilarity layers previously created with strata() and similarity(),\nrespectively.\n\n\n# SpatVector with SOC observations for stratification units\nsoc.obs <- vect(\"soc.shp\")\n# Representative SOC observation for each stratification unit\nsu.obs <- observation(su.rast = su, obs = soc.obs, col.id = 1, col.resp = 2,\n                      method = \"mls\", ls.rast = su.ls$landsim)\n# Plot results\nfigure(13, d = list(su.obs, soc.obs, su))\n\n\n\n\n\nFigure 14: Selection of representative observations. Green points in the map represent the complete set of observations. Blue points represent the representative observation for each stratification unit.\n\n\n\nThe function locations() performs the automatic selection of the\nrepresentative sampling location(s) for each stratification unit in a given set,\nwhere the representative sampling location is the raster cell where the highest\nlandscape similarity value occurs. Currently, locations() implements two\nselection methods: (a) buffer: select sampling locations within areas with\nlandscape similarity values above a certain threshold; and (b) absolute:\nselect sampling locations with the highest landscape similarity values. The code\nbelow shows the use of locations() based on the buffer method.\n\n\n# Representative sampling location and its buffer area for each stratification unit\nsu.samp <- locations(ls.rast = su.ls$landsim, su.rast = su, method = \"buffer\")\n# Plot results\nfigure(14, d = list(su.samp, su))\n\n\n\n\n\nFigure 15: Selection of representative sampling locations. Green points in the map represent the sampling location for each stratification unit. Green polygons represent the buffer area for each sampling location.\n\n\n\nPredictive modeling\nPredictive modeling with rassta is based on the assumption that each\nstratification unit represents a distinct landscape configuration and that this\nconfiguration influences a natural phenomenon in a distinctive manner. It is\nassumed that the influence that a stratification unit’s landscape configuration\nhas on response phenomena at a specific location (i.e, raster cell) is\nproportional to the unit’s landscape similarity value at that raster cell.\nTherefore, given a stratification unit x, the corresponding raster layer of\nlandscape similarity ls, the response y, and a raster cell c, the greater\nthe value of ls at c, the more similar y at c will be to the typical y\nfor x. The typical (i.e., representative) value of a response phenomenon for a\ngiven stratification unit can be defined in several ways. For instance, if a\nresponse phenomenon was sampled/measured multiple times within a given\nstratification unit, the typical response value could be that from the\nsample/measurement at the raster cell with the highest landscape similarity\nvalue (see observation()).\nSeveral studies have used landscape similarity layers to model the spatial\nvariability of natural phenomena. These studies argue that the use of similarity\nlayers is appropriate in cases when (a) available observations for modeling are\nlimited (Zhu et al. 2008); (b) initial spatial distribution patterns are needed for\nsurvey design (Carpenter et al. 1993); (c) expert-driven selection of informative\nvariables is possible (Knick and Dyer 1997); (d) a priori knowledge of response-control\nrelationships in the form of conceptual models is available (Schmidt et al. 2005; Zhu et al. 2010); and (e) discriminating between (ecologically) positive and\nnegative deviations from reference environments is required (Watrous et al. 2006).\nAccordingly, engine() allows the modeling of environmental phenomena with a\nnumber of training observations as few as the number of landscape similarity\nlayers [cases (a) and (b)]; training observations and landscape similarity\nlayers as outcomes of expert-driven landscape stratification [cases (c) and\n(d)]; and landscape similarity layers derived from spatial signatures that\ndiscriminate between the tails of distribution functions [case (e)].\nModeling with rassta is performed using the function engine(). For\ncontinuous responses, engine() performs a weighted average involving\nrepresentative response values and landscape similarity layers. For a raster\ncell c, the modeled response value is equal to the weighted average of the\nrepresentative values for those stratification units with the highest landscape\nsimilarity values at c. The stratification units with the highest landscape\nsimilarity values at c can be considered as the nearest neighbors (in\nfeature space) of the landscape configuration at c. These nearest neighbors\nare called winning stratification units, and the weight of their corresponding\nrepresentative value is proportional to the winning unit’s landscape similarity\nvalue at c. For categorical responses, the modal response value of the winning\nstratification units replaces the weighted average. Figure 16 shows\nan example of the modeling process for continuous responses with rassta.\n\n\n\nFigure 16: Schematic of the modeling process with rassta. The modeling process is performed in a cell-wise fashion. The inputs required are the raster layers of landscape similarity and the representative observations for each stratification unit.\n\n\n\nNote that the weighted average for modeling phenomena across geographic space\nhas been widely applied in GIS-based multicriteria decision analysis (GIS-MCDA).\nIn GIS-MCDA, attributes (i.e., variables) in the form of raster layers are\nweighted according to expert criteria. The weighted variables are then combined\nthrough (cell-wise) overlay operators such as multiplication, addition and\n(ordered) averaging. The resulting value at each cell represents the relative\nsuitability for a certain condition/decision (Malczewski 2006). The function\nengine() generalizes the weighted overlay process of GIS-MCDA by allowing the\nuse of sampled/measured data of a response phenomenon in conjunction with the\nlandscape similarity layers acting as weighted variables. This generalization\nallows the modeling of real-valued phenomena in continuous or categorical form.\nThe modeling approach of engine() is almost the same as that proposed by\nZhu (1997) to model landscape attributes across geographic space. The difference\nbetween engine() and the approach of Zhu (1997) is that engine() allows\nthe selection of the number of landscape similarity layers for the weighted\naverage calculation. Presumably, restricting the number of layers will reduce\nthe shortening (‘shrinking’) effect that weighted averaging has on the range of\nmodeled continuous response values (Nolan et al. 2019).\nThe code below demonstrates the use of engine() for the predictive modeling of\nsoil organic carbon. Note that the representative response values (argument\nsu.repobs) are those previously selected with observation(), and that the\nlayers of landscape similarity (argument ls.rast) are those previously created\nwith similarity().\n\n\n# Table with the numeric code of stratification units and representative SOC values\nsu.soc <- su.obs$su_repobs[, c(\"SU\", \"soc\")]\n# engine() requires a (tiled) SpatVector with the boundaries of the area of interest\naoi <- vect(\"aoi.shp\")\n# engine() writes results directly on disk\nif (dir.exists(\"soc\") == FALSE) {dir.create(\"soc\")}  # Create directory\n# Spatial modeling of SOC across the landscape based on 3 winning stratification units\nsoc <- engine(ls.rast = su.ls$landsim, n.win = 3, su.repobs = su.soc,\n              tiles = aoi, outdir = \"soc\", overwrite = TRUE)\nfigure(16, d = list(soc, \"soc_valid.shp\")) # Plot results\n\n\n\n\n\nFigure 17: Modeled soil organic carbon (SOC) content (percent). The map shows the modeled SOC values across the landscape. The plot shows the the modeled (y) versus the measured (x) SOC values based on 62 independent observations.\n\n\n\nMiscellaneous\nThe spatial signature only applies to classification units created from\ncontinuous variables. Thus, spatial signatures cannot be calculated for\nclassification units that represent categorical variables, such as land use/land\ncover. In such cases, a one-hot encoding can be applied to produce binary layers\nfor the units. These layers are considered the spatial signatures of the\nclassification units. The code below shows the creation of binary layers for\nsoil parent material units with dummies().\n\n\n# Multi-layer SpatRaster of soil parent material units\nmat.cu <- rast(\"material.tif\")\n# Binary layers for each soil parent material unit and their maps\nmat.sig <- dummies(mat.cu, preval = 100, absval = 0)\nfigure(17, d = mat.sig) # Plot results\n\n\n\n\n\nFigure 18: Construction of binary layers. Binary layers act as the spatial signatures for categorical variables. In this example, soil parent material acts as both landscape factor and classification units.\n\n\n\nThe function plot3D() produces interactive maps showing the 3-dimensional\n(XYZ) variability in raster layers representing continuous variables. The\nXYZ reference positions are obtained from a user-supplied elevation layer. For\nlarge raster layers (large spatial coverage and/or high spatial resolution),\nthis function allows the option to decrease resolution and subset the data. The\ncode below shows how plot3D() creates an interactive\n3D map for SOC, as modeled with engine().\n\n\n# Single-layer SpatRaster of terrain elevation\nelev <- rast(\"elevation.tif\")\n# Interactive 3D map\nsoc3D <- plot3D(c(elev, soc), z = 1, ex = 0.2, pals = \"Fall\", rev = TRUE)\nsoc3D$soc\n\n\n\n\nFigure 19: Interactive 3D map of SOC (percent). The Z dimension is obtained from a reference terrain model. The interactive aspect of the map is achieved thanks to the R package plotly.\n\n\n\n\n\n\n3 Future versioning and summary\nThis work presented the rassta package for spatial stratification,\nsampling, and modeling of environmental phenomena within the R environment.\nFuture versioning of the rassta package will focus on developing new\napproaches for spatial stratification. Stratification based on spatial\nintersection may not be feasible to implement in highly complex landscapes\nbecause these landscapes may require many (sets of) classification units to\naccurately represent the spatial variability of landscape factors, leading to\nover-stratification, and thus, greater demand for samples/observations to\nconduct predictive modeling based on landscape similarity. One plausible\nsolution is the application of the stratification methods presented by\nJasiewicz et al. (2015), Jasiewicz et al. (2018), Nowosad (2021), and Nowosad and Stepinski (2021).\nHowever, these methods have been purposely designed for studies with\ncontinental/global applications. Therefore, these methods should be adapted for\nrassta to tailor their application at local scales to allow for more\nprecise representations of natural phenomena and their spatial variability.\nAnother focus of versioning can be new functions to visualize the variability of\nresponse phenomena relative to the hierarchical structure represented by the\nstratification units. Lastly, future versioning of rassta should also\nconsider the user’s experiences to ensure its general applicability.\nThe core ideas implemented in the rassta package include the multi-scale,\nhierarchical landscape stratification based on spatial intersection, the\napplication of non-parametric distribution estimators to define the typical\nlandscape configuration of stratification units, and the use of spatially\nexplicit landscape correspondence metrics for non-probability sampling and\npredictive modeling. Some of these ideas have previously been implemented in R\nthrough a few packages dedicated to the analysis of geospatial data.\nNevertheless, rassta offers a unified, generalized framework to conduct\nmultiple landscape stratification routines through a dedicated set of\nalgorithms. Moreover, spatially-explicit information created with rassta,\nlike stratification units, landscape similarity layers, and representative\nobservations, can be embedded into statistically robust modeling approaches to\noptimize the analysis of environmental phenomena.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-036.zip\nCRAN packages used\nterra, sf, motif, rassta, Rcpp, kohonen, virtualspecies, dismo, shiny\nCRAN Task Views implied by cited packages\nChemPhys, HighPerformanceComputing, NumericalMathematics, Spatial, SpatioTemporal, WebTechnologies\n\n\nT. F. H. Allen and B. Starr. Hierarchy: Perspectives for ecological complexity. University of Chicago Press, 1982. URL https://doi.org/10.7208/chicago/9780226489711.001.0001.\n\n\nM. B. Araújo and A. Guisan. Five (or so) challenges for species distribution modelling. Journal of Biogeography, 33(10): 1677–1688, 2006. URL https://doi.org/10.1111/j.1365-2699.2006.01584.x.\n\n\nD. Arrouays, A. McBratney, J. Bouma, Z. Libohova, A. Richer-de-Forges, C. Morgan, P. Roudier, L. Poggio and V. Mulder. Impressions of digital soil maps: The good, the not so good, and making them ever better. Geoderma Regional, 20: e00255, 2020. URL https://doi.org/10.1016/j.geodrs.2020.e00255.\n\n\nM. P. Austin and P. C. Heyligers. New approach to vegetation survey design: Gradsect sampling. Nature Conservation: Cost Effective Biological Surveys and Data Analysis, 5: 31–36, 1989.\n\n\nD. Baldwin, K. Naithani and H. Lin. Combined soil-terrain stratification for characterizing catchment-scale soil moisture variation. Geoderma, 285: 260–269, 2017. URL https://doi.org/10.1016/j.geoderma.2016.09.031.\n\n\nP. Burrough. Fuzzy mathematical methods for soil survey and land evaluation. Journal of Soil Science, 40(3): 477–492, 1989. URL https://doi.org/10.1111/j.1365-2389.1989.tb01290.x.\n\n\nG. Carpenter, A. N. Gillison and J. Winter. DOMAIN: A flexible modelling procedure for mapping potential distributions of plants and animals. Biodiversity & Conservation, 2(6): 667–680, 1993. URL https://doi.org/10.1007/bf00051966.\n\n\nW. Chang, J. Cheng, J. Allaire, C. Sievert, B. Schloerke, Y. Xie, J. Allen, J. McPherson, A. Dipert and B. Borges. : Web application framework for r. 2021. URL https://CRAN.R-project.org/package=shiny. R package version 1.6.0.\n\n\nE. Chuvieco, I. Aguado, M. Yebra, H. Nieto, J. Salas, M. Pilar Martín, L. Vilar, J. Martínez, S. Martín, P. Ibarra, et al. Development of a framework for fire risk assessment using remote sensing and geographic information system technologies. Ecological Modelling, 221(1): 46–58, 2010. URL https://doi.org/10.1016/j.ecolmodel.2008.11.017.\n\n\nE. Eddelbuettel and R. François. : Seamless R and C++ integration. Journal of Statistical Software, 40(8): 1–18, 2011. URL https://10.18637/jss.v040.i08.\n\n\nJ. Elith, M. Kearney and S. Phillips. The art of modelling range-shifting species. Methods in Ecology and Evolution, 1(4): 330–342, 2010. URL https://doi.org/10.1111/j.2041-210x.2010.00036.x.\n\n\nJ. Elith and J. R. Leathwick. Species distribution models: Ecological explanation and prediction across space and time. Annual Review of Ecology, Evolution, and Systematics, 40: 677–697, 2009. URL https://doi.org/10.1146/annurev.ecolsys.110308.120159.\n\n\nW.-A. Flügel. Delineating hydrological response units by geographical information system analyses for regional hydrological modelling using PRMS/MMS in the drainage basin of the river Bröl, Germany. Hydrological Processes, 9(3-4): 423–436, 1995. URL https://doi.org/10.1002/hyp.3360090313.\n\n\nA. Gillison. Gradient oriented sampling for resource surveys - the gradsect method. Survey Methods for Nature Conservation, 2: 349–374, 1983.\n\n\nA. Gillison and K. Brewer. The use of gradient directed transects or gradsects in natural resource surveys. Journal of Environmental Management, 20: 103–127, 1985.\n\n\nK. Glinka. Dokuchaev’s ideas in the development of pedology and cognate sciences. Russian Pedological Investigations, 1: 1927.\n\n\nA. Guisan and N. Zimmermann. Predictive habitat distribution models in ecology. Ecological Modelling, 135(2): 147–186, 2000. URL https://doi.org/10.1016/S0304-3800(00)00354-9.\n\n\nJ. Ham, Y. Chen, M. Crawford and J. Ghosh. Investigation of the random forest framework for classification of hyperspectral data. IEEE Transactions on Geoscience and Remote Sensing, 43(3): 492–501, 2005. URL https://doi.org/10.1109/tgrs.2004.842481.\n\n\nG. Heuvelink and R. Webster. Modelling soil variation: Past, present, and future. Geoderma, 100(3): 269–301, 2001. URL https://doi.org/10.1016/S0016-7061(01)00025-8.\n\n\nR. J. Hijmans. : Spatial data analysis. 2021. URL https://CRAN.R-project.org/package=terra. R package version 1.3-4.\n\n\nR. J. Hijmans, S. Phillips, J. Leathwick and J. Elith. : Species distribution modeling. 2020. URL https://CRAN.R-project.org/package=dismo. R package version 1.3-3.\n\n\nA. Hirzel, V. Helfer and F. Metral. Assessing habitat-suitability models with a virtual species. Ecological Modelling, 145(2-3): 111–121, 2001. URL https://doi.org/10.1016/S0304-3800(01)00396-9.\n\n\nB. Hudson. The soil survey as paradigm-based science. Soil Science Society of America Journal, 56(3): 836–841, 1992. URL https://doi.org/10.2136/sssaj1992.03615995005600030027x.\n\n\nJ. Jasiewicz, P. Netzel and T. Stepinski. GeoPAT: A toolbox for pattern-based information retrieval from large geospatial databases. Computers & Geosciences, 80: 62–73, 2015. URL https://doi.org/10.1016/j.cageo.2015.04.002.\n\n\nJ. Jasiewicz, T. Stepinski and J. Niesterowicz. Multi-scale segmentation algorithm for pattern-based partitioning of large categorical rasters. Computers & Geosciences, 118: 122–130, 2018. URL https://doi.org/10.1016/j.cageo.2018.06.003.\n\n\nL. Kaufman and P. Rousseeuw. Finding groups in data: An introduction to cluster analysis. John Wiley & Sons, 1990.\n\n\nS. Knick and D. Dyer. Distribution of black-tailed jackrabbit habitat determined by GIS in Southwestern Idaho. The Journal of Wildlife Management, 75–85, 1997. URL https://doi.org/10.2307/3802416.\n\n\nT. Kohonen. The self-organizing map. Proceedings of the IEEE, 78(9): 1464–1480, 1990. URL https://doi.org/10.1016/s0925-2312(98)00030-7.\n\n\nB. Leroy, C. Meynard, C. Bellard and F. Courchamp., an R package to generate virtual species distributions. Ecography, 39(6): 599–607, 2016. URL https://doi.org/10.1111/ecog.01388.\n\n\nR. MacMillan, W. Pettapiece, S. Nolan and T. Goddard. A generic procedure for automatically segmenting landforms into landform elements using DEMs, heuristic rules and fuzzy logic. Fuzzy Sets and Systems, 113(1): 81–109, 2000. URL https://doi.org/10.1016/S0165-0114(99)00014-7.\n\n\nJ. Malczewski. GIS-based multicriteria decision analysis: A survey of the literature. International Journal of Geographical Information Science, 20(7): 703–726, 2006. URL https://doi.org/10.1080/13658810600661508.\n\n\nA. McBratney, M. L. Mendonça Santos and B. Minasny. On digital soil mapping. Geoderma, 117(1-2): 3–52, 2003. URL https://doi.org/10.1016/S0016-7061(03)00223-4.\n\n\nK. McSweeney, B. K. Slater, R. David Hammer, J. C. Bell, P. E. Gessler and G. W. Petersen. Towards a new framework for modeling the soil-landscape continuum. Factors of Soil Formation: A Fiftieth Anniversary Retrospective, 33: 127–145, 1994. URL https://doi.org/10.2136/sssaspecpub33.c8.\n\n\nC. Nolan, J. Tipton, R. K. Booth, M. B. Hooten and S. T. Jackson. Comparing and improving methods for reconstructing peatland water-table depth from testate amoebae. The Holocene, 29(8): 1350–1361, 2019. URL https://doi.org/10.1177/0959683619846969.\n\n\nJ. Nowosad. : An open-source R tool for pattern-based spatial analysis. Landscape Ecology, 36(1): 29–43, 2021. URL https://doi.org/10.1007/s10980-020-01135-0.\n\n\nJ. Nowosad and T. Stepinski. Pattern-based identification and mapping of landscape types using multi-thematic data. International Journal of Geographical Information Science, 35(8): 1634–1649, 2021. URL https://doi.org/10.1080/13658816.2021.1893324.\n\n\nS. J. Park and N. Van De Giesen. Soil-landscape delineation to define spatial sampling domains for hillslope hydrology. Journal of Hydrology, 295(1-4): 28–46, 2004. URL https://doi.org/10.1016/j.jhydrol.2004.02.022.\n\n\nE. Pebesma. Simple features for R: Standardized support for spatial vector data. The R Journal, 10(1): 439–446, 2018. URL https://doi.org/10.32614/RJ-2018-009.\n\n\nR. Pike. The geometric signature: Quantifying landslide-terrain types from digital elevation models. Mathematical Geology, 20(5): 491–511, 1988. URL https://doi.org/10.1007/BF00890333.\n\n\nR. Pike and W. Rozema. Spectral analysis of landforms. Annals of the Association of American Geographers, 65(4): 499–516, 1975. URL https://doi.org/10.1111/j.1467-8306.1975.tb01058.x.\n\n\nM. Rodrigues, S. Costafreda-Aumedes, C. Comas and C. Vega-García. Spatial stratification of wildfire drivers towards enhanced definition of large-fire regime zoning and fire seasons. Science of the Total Environment, 689: 634–644, 2019. URL https://doi.org/10.1016/j.scitotenv.2019.06.467.\n\n\nJ. Schmidt, P. Tonkin and A. Hewitt. Quantitative soil - landscape models for the Haldon and Hurunui soil sets, New Zealand. Soil Research, 43(2): 127, 2005. URL https://doi.org/10.1071/sr04074.\n\n\nP. Scull, J. Franklin, O. Chadwick and D. McArthur. Predictive soil mapping: A review. Progress in Physical Geography, 27(2): 171–197, 2003. URL https://doi.org/10.1191/0309133303pp366ra.\n\n\nR. Tibshirani, G. Walther and T. Hastie. Estimating the number of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society: Series B (statistical Methodology), 63(2): 411–423, 2001. URL https://doi.org/10.1111/1467-9868.00293.\n\n\nK. Watrous, T. Donovan, R. Mickey, S. Darling, A. Hicks and S. Von Oettingen. Predicting minimum habitat characteristics for the Indiana bat in the Champlain Valley. The Journal of Wildlife Management, 70(5): 1228–1237, 2006. URL https://doi.org/10.2193/0022-541X(2006)70[1228:PMHCFT]2.0.CO;2.\n\n\nR. Wehrens and J. Kruisselbrink. Flexible self-organizing maps in kohonen 3.0. Journal of Statistical Software, 87(1): 1–18, 2018. URL https://doi.org/10.18637/jss.v087.i07.\n\n\nK. J. Wessels, S. Van Jaarsveld, J. D. Grimbeek and M. J. Van der Linde. An evaluation of the gradsect biological survey method. Biodiversity & Conservation, 7(8): 1093–1121, 1998. URL https://doi.org/10.1023/a:1008899802456.\n\n\nA. West, S. Kumar, C. Brown, T. Stohlgren and J. Bromberg. Field validation of an invasive species maxent model. Ecological Informatics, 36: 126–134, 2016. URL https://doi.org/10.1016/j.ecoinf.2016.11.001.\n\n\nL. A. Zadeh. Fuzzy sets. Information and Control, 8(1): 338–353, 1965. URL https://doi.org/10.1142/9789814261302_0001.\n\n\nA. X. Zhu. A similarity model for representing soil spatial information. Geoderma, 77(2-4): 217–242, 1997. URL https://doi.org/10.1016/S0016-7061(97)00023-2.\n\n\nA. X. Zhu and L. Band. A knowledge-based approach to data integration for soil mapping. Canadian Journal of Remote Sensing, 20(4): 408–418, 1994. URL https://doi.org/10.1080/07038992.1994.10874583.\n\n\nA. X. Zhu, F. Qi, A. Moore and J. Burt. Prediction of soil properties using fuzzy membership values. Geoderma, 158(3-4): 199–206, 2010. URL https://doi.org/10.1016/j.geoderma.2010.05.001.\n\n\nA. X. Zhu, L. Yang, B. Li, C. Qin, E. English, J. Burt and C. Zhou. Purposive sampling for digital soil mapping for areas with limited data. In Digital soil mapping with limited data, pages. 233–245 2008. Springer. URL https://doi.org/10.1007/978-1-4020-8592-5_20.\n\n\n\n\n",
    "preview": "articles/RJ-2022-036/distill-preview.png",
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {},
    "preview_width": 1760,
    "preview_height": 1084
  },
  {
    "path": "articles/RJ-2022-024/",
    "title": "htestClust: A Package for Marginal Inference of Clustered Data Under Informative Cluster Size",
    "description": "When observations are collected in/organized into observational units, within which observations may be dependent, those observational units are often referred to as \\\"clustered\\\" and the data as \\\"clustered data\\\". Examples of clustered data include repeated measures or hierarchical shared association (e.g., individuals within families). This paper provides an overview of the R package [htestClust](https://CRAN.R-project.org/package=htestClust), a tool for the marginal analysis of such clustered data with potentially informative cluster and/or group sizes. Contained in htestClust are clustered data analogues to the following classical hypothesis tests: rank-sum, signed rank, $t$-, one-way ANOVA, F, Levene, Pearson/Spearman/Kendall correlation, proportion, goodness-of-fit, independence, and McNemar. Additional functions allow users to visualize and test for informative cluster size. This package has an easy-to-use interface mimicking that of classical hypothesis-testing functions in the R environment. Various features of this package are illustrated through simple examples.",
    "author": [
      {
        "name": "Mary Gregg",
        "url": {}
      },
      {
        "name": "Somnath Datta",
        "url": {}
      },
      {
        "name": "Douglas Lorenz",
        "url": {}
      }
    ],
    "date": "2022-10-11",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-024.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-026/",
    "title": "APCI: An R and Stata Package for Visualizing and Analyzing Age-Period-Cohort Data",
    "description": "Social scientists have frequently attempted to assess the relative contribution of age, period, and cohort variables to the overall trend in an outcome. We develop an R package [APCI](https://CRAN.R-project.org/package=APCI) (and Stata command apci) to implement the age-period-cohort-interaction (APC-I) model for estimating and testing age, period, and cohort patterns in various types of outcomes for pooled cross-sectional data and multi-cohort panel data. Package [APCI](https://CRAN.R-project.org/package=APCI) also provides a set of functions for visualizing the data and modeling results. We demonstrate the usage of package [APCI](https://CRAN.R-project.org/package=APCI) with empirical data from the Current Population Survey. We show that package [APCI](https://CRAN.R-project.org/package=APCI) provides useful visualization and analytical tools for understanding age, period, and cohort trends in various types of outcomes.",
    "author": [
      {
        "name": "Jiahui Xu",
        "url": {}
      },
      {
        "name": "Liying Luo",
        "url": {}
      }
    ],
    "date": "2022-10-11",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-026.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-027/",
    "title": "shinybrms: Fitting Bayesian Regression Models Using a Graphical User Interface for the R Package brms",
    "description": "Despite their advantages, the application of Bayesian regression models is still the exception compared to frequentist regression models. Here, we present our R package [shinybrms](https://CRAN.R-project.org/package=shinybrms) which provides a graphical user interface for fitting Bayesian regression models, with the frontend consisting of a [shiny](https://CRAN.R-project.org/package=shiny) app and the backend relying on the R package [brms](https://CRAN.R-project.org/package=brms) which in turn relies on Stan. With shinybrms, we hope that Bayesian regression models (and regression models in general) will become more popular in applied research, data analyses, and teaching. Here, we illustrate our graphical user interface by the help of an example from medical research.",
    "author": [
      {
        "name": "Frank Weber",
        "url": {}
      },
      {
        "name": "Katja Ickstadt",
        "url": {}
      },
      {
        "name": "Änne Glass",
        "url": {}
      }
    ],
    "date": "2022-10-11",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-027.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-029/",
    "title": "Refreg: An R Package for Estimating Conditional Reference Regions",
    "description": "Multivariate reference regions (MVR) represent the extension of the reference interval concept to the multivariate setting. A reference interval is defined by two threshold points between which a high percentage of healthy subjects' results, usually 95%, are contained. Analogously, an MVR characterizes the values of several diagnostic tests most frequently found among non-diseased subjects by defining a convex hull containing 95% of the results. MVRs have great applicability when working with diseases that are diagnosed via more than one continuous test, e.g., diabetes or hypothyroidism. The present work introduces refreg, an R package for estimating conditional MVRs. The reference region is non-parametrically estimated using a multivariate kernel density estimator, and its shape allowed to change under the influence of covariates. The effects of covariates on the multivariate variable means, and on their variance-covariance matrix, are estimated by flexible additive predictors. Continuous covariate non-linear effects can be estimated by penalized spline smoothers. The package allows the user to propose, for instance, an age-specific diagnostic rule based on the joint distribution of two non-Gaussian, continuous test results. The usefulness of the refreg package in clinical practice is illustrated with a real case in diabetes research, with an age-specific reference region proposed for the joint interpretation of two glycemia markers (fasting plasma glucose and glycated hemoglobin). To show that the refreg package can also be used in other, and indeed very different fields, an example is provided for the joint prediction of two atmospheric pollutants (SO$_2$, and NO$_x$). Additionally, the text discusses how, conceptually, this method could be extended to more than two dimensions.",
    "author": [
      {
        "name": "Óscar Lado-Baleato",
        "url": {}
      },
      {
        "name": "Javier Roca-Pardiñas",
        "url": {}
      },
      {
        "name": "Carmen Cadarso-Suárez",
        "url": {}
      },
      {
        "name": "Francisco Gude",
        "url": {}
      }
    ],
    "date": "2022-10-11",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-029.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-030/",
    "title": "TensorTest2D: Fitting Generalized Linear Models with Matrix Covariates",
    "description": "The [TensorTest2D](https://CRAN.R-project.org/package=TensorTest2D) package provides the means to fit generalized linear models on second-order tensor type data. Functions within this package can be used for parameter estimation (e.g., estimating regression coefficients and their standard deviations) and hypothesis testing. We use two examples to illustrate the utility of our package in analyzing data from different disciplines. In the first example, a tensor regression model is used to study the effect of multi-omics predictors on a continuous outcome variable which is associated with drug sensitivity. In the second example, we draw a subset of the MNIST handwritten images and fit to them a logistic tensor regression model. A significance test characterizes the image pattern that tells the difference between two handwritten digits. We also provide a function to visualize the areas as effective classifiers based on a tensor regression model. The visualization tool can also be used together with other variable selection techniques, such as the LASSO, to inform the selection results.",
    "author": [
      {
        "name": "Ping-Yang Chen",
        "url": {}
      },
      {
        "name": "Hsing-Ming Chang",
        "url": {}
      },
      {
        "name": "Yu-Ting Chen",
        "url": {}
      },
      {
        "name": "Jung-Ying Tzeng",
        "url": {}
      },
      {
        "name": "Sheng-Mao Chang",
        "url": {}
      }
    ],
    "date": "2022-10-11",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-030.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-031/",
    "title": "wavScalogram: An R Package with Wavelet Scalogram Tools for Time Series Analysis",
    "description": "In this work we present the wavScalogram R package, which contains methods based on wavelet scalograms for time series analysis. These methods are related to two main wavelet tools: the windowed scalogram difference and the scale index. The windowed scalogram difference compares two time series, identifying if their scalograms follow similar patterns at different scales and times, and it is thus a useful complement to other comparison tools such as the squared wavelet coherence. On the other hand, the scale index provides a numerical estimation of the degree of non-periodicity of a time series and it is widely used in many scientific areas.",
    "author": [
      {
        "name": "Vicente J. Bolós",
        "url": {}
      },
      {
        "name": "Rafael Benı́tez",
        "url": {}
      }
    ],
    "date": "2022-10-11",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-031.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-032/",
    "title": "ClusTorus: An R Package for Prediction and Clustering on the Torus by Conformal Prediction",
    "description": "Protein structure data consist of several dihedral angles, lying on a multidimensional torus. Analyzing such data has been and continues to be key in understanding functional properties of proteins. However, most of the existing statistical methods assume that data are on Euclidean spaces, and thus they are improper to deal with angular data. In this paper, we introduce the package ClusTorus specialized to analyzing multivariate angular data. The package collects some tools and routines to perform algorithmic clustering and model-based clustering for data on the torus. In particular, the package enables the construction of conformal prediction sets and predictive clustering, based on kernel density estimates and mixture model estimates. A novel hyperparameter selection strategy for predictive clustering is also implemented, with improved stability and computational efficiency. We demonstrate the use of the package in clustering protein dihedral angles from two real data sets.",
    "author": [
      {
        "name": "Seungki Hong",
        "url": {}
      },
      {
        "name": "Sungkyu Jung",
        "url": {}
      }
    ],
    "date": "2022-10-11",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-032.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-033/",
    "title": "kStatistics: Unbiased Estimates of Joint Cumulant Products from the Multivariate Faà Di Bruno's Formula",
    "description": "kStatistics is a package in `R` that serves as a unified framework for estimating univariate and multivariate cumulants as well as products of univariate and multivariate cumulants of a random sample, using unbiased estimators with minimum variance. The main computational machinery of kStatistics  is an algorithm for computing multi-index partitions. The same algorithm underlies the general-purpose multivariate Faà di Bruno's formula, which therefore has been included in the last release of the package. This formula gives the coefficients of formal power series compositions as well as the partial derivatives of multivariable function compositions. One of the most significant applications of this formula is the possibility to generate many well-known polynomial families as special cases. So, in the package, there are special functions for generating very popular polynomial families, such as the  Bell polynomials. However, further families can be obtained, for suitable choices of the formal power series involved in the composition or when suitable symbolic strategies are employed. In both cases, we give examples on how to modify the `R` codes of the package to accomplish this task. Future developments are addressed at the end of the paper",
    "author": [
      {
        "name": "Elvira Di Nardo",
        "url": "https://www.elviradinardo.it"
      },
      {
        "name": "Giuseppe Guarino",
        "url": {}
      }
    ],
    "date": "2022-10-11",
    "categories": [],
    "contents": "\n1 Introduction\nJoint cumulants are usually employed for measuring interactions among two or more random variables simultaneously, extending the familiar notion of covariance to higher orders. More in details, suppose \\(\\boldsymbol{Y}\\) a random vector with moment generating function \\(M_{\\boldsymbol{Y}}(\\boldsymbol{z}),\\) for \\(\\boldsymbol{z}=(z_1, \\ldots, z_m)\\)\nin a suitable neighborhood of \\(\\boldsymbol{0}.\\) Thus \\(M_{\\boldsymbol{Y}}(\\boldsymbol{z})\\) can be expressed as\n\\[\\begin{equation}\nM_{\\boldsymbol{Y}}(\\boldsymbol{z}) = \\exp\\big(K_{\\boldsymbol{Y}}(\\boldsymbol{z}) \\big)\n\\label{(1bis)}\n\\end{equation}\\]\nwhere \\(K_{\\boldsymbol{Y}}(\\boldsymbol{z})\\) is the cumulant generating function of \\(\\boldsymbol{Y}.\\) If1 \\(\\boldsymbol{i}\\in {\\mathbb N}_0^m\\) and\n\\[\\begin{equation}\nM_{\\boldsymbol{Y}}(\\boldsymbol{z})=1 + \\sum_{|\\boldsymbol{i}| > 0} \\frac{{\\mathbb E}[\\boldsymbol{Y}^{\\boldsymbol{i}}]}{\\boldsymbol{i}!} \\boldsymbol{z}^i \\, \\qquad \\, K_{\\boldsymbol{Y}}(\\boldsymbol{z}) = \\sum_{|\\boldsymbol{i}| > 0} \\frac{k_{\\boldsymbol{i}}(\\boldsymbol{Y})}{\\boldsymbol{i}!} \\boldsymbol{z}^{\\boldsymbol{i}}\n\\label{(2)}\n\\end{equation}\\]\nthen \\(\\{k_{\\boldsymbol{i}}(\\boldsymbol{Y})\\}\\) are said the joint cumulants of \\(\\{{\\mathbb E}[\\boldsymbol{Y}^{\\boldsymbol{i}}]\\}.\\) From a theoretical point of view, cumulants are a useful sequence due to the following properties (Di Nardo 2011):\nOrthogonality: Joint cumulants of independent random vectors are zero, that is \\(k_{\\boldsymbol{i}}(\\boldsymbol{Y}) = 0\\) for \\(|\\boldsymbol{i}| > 0\\) if \\(\\boldsymbol{Y} = (\\boldsymbol{Y}_1, \\boldsymbol{Y}_2)\\) with \\(\\boldsymbol{Y}_1\\) independent of \\(\\boldsymbol{Y}_2.\\)\nAdditivity: Cumulants linearize on independent random vectors, that is\\(k_{\\boldsymbol{i}}(\\boldsymbol{Y}_1 + \\boldsymbol{Y}_2) =k_{\\boldsymbol{i}}(\\boldsymbol{Y}_1) + k_{\\boldsymbol{i}}(\\boldsymbol{Y}_2)\\)\nfor \\(|\\boldsymbol{i}|> 0\\) with \\(\\boldsymbol{Y}_1\\) independent of \\(\\boldsymbol{Y}_2.\\)\nMultilinearity: \\(k_{\\boldsymbol{i}}(A \\boldsymbol{Y}) = \\sum_{j_1, \\ldots, j_m} (A)_{\\scriptscriptstyle{i_1}}^{\\scriptscriptstyle{j_1}} \\cdots (A)_{\\scriptscriptstyle{i_m}}^{\\scriptscriptstyle{j_m}} k_{\\boldsymbol{j}}(\\boldsymbol{Y})\\) for \\(|\\boldsymbol{i}|>0\\) with \\(A \\in {\\mathbb R}^m \\times {\\mathbb R}^m.\\)\nSemi-invariance: If \\(\\boldsymbol{b} \\in {\\mathbb R}^m\\) then  \\(k_{\\boldsymbol{i}}(\\boldsymbol{Y} + \\boldsymbol{b}) = k_{\\boldsymbol{i}}(\\boldsymbol{Y})\\) for \\(|\\boldsymbol{i}| \\geq 2\\).\nThanks to all these properties, joint cumulants have a wide range of applications: from statistical inference and time series (Jammalamadaka et al. 2006) to asymptotic theory (Rao and Wong 1999), from spatial statistics modeling (Dimitrakopoulos et al. 2010) to signal processing (Giannakis 1987), from non-linear systems identification (Oualla et al. 2021) to Wiener chaos (Peccati and Taqqu 2011), just to mention a few. Indeed it is also well known that cumulants of order greater than two are zero for random vectors which are Gaussian. Therefore, higher order cumulants are often used in testing for multivariate Gaussianity (Jammalamadaka et al. 2006).\nThe \\(\\boldsymbol{i}\\)-th multivariate \\(k\\)-statistic is a symmetric function of the multivariate random sample whose expectation is the joint cumulant of order \\(\\boldsymbol{i}\\) of the population characters. These estimators have minimum variance when compared to all other unbiased estimators and are built by free-distribution methods without using sample moments. Due to the properties of joint cumulants, multivariate \\(k\\)-statistics are employed to check multivariate gaussianity (Ferreira et al. 1997) or to quantify high-order interactions among data (Geng et al. 2011), for applications in topology inference (Smith et al. 2022), in neuronal science (Staude et al. 2010) and in mathematical finance (Di Nardo et al. 2020). Polykays are unbiased estimators of cumulant products (Robson 1957) and are particularly useful in estimating covariances between \\(k\\)-statistics (McCullagh 1987). In the kStatistics package (Di Nardo and Guarino 2021), the nPolyk function provides \\(k\\)-statistics and polykays as well as their multivariate generalizations. Further implementations are in Phyton (Smith 2020), in Maple (Guarino et al. 2009) and in Mathematica (Rose and Smith 2002).\nAll these estimators are described with a wealth of details by Stuart and Ord (1994) and McCullagh (1987) and their construction relied on some well-known change of bases in the ring of symmetric polynomials. In Di Nardo (2011) a different approach is followed using suitable polynomial families and symbolic strategies. This procedure was the core of the first release (version 1.0) of the kStatistics package (Di Nardo and Guarino 2019), as the initial goal was to implement tools for the estimation of cumulants and cumulant products, both in the univariate and in the multivariate case. As the referred polynomial families can be traced back to the generalized (complete exponential) Bell polynomials, the latest version of the package (Di Nardo and Guarino 2021) has also included procedures to generate these polynomials together with a number of special cases.\nLet us recall that the generalized (complete exponential) Bell polynomials are a family of polynomials involving multivariable Sheffer sequences (Brown 1979). Among its various applications, we recall the cumulant polynomial sequences and their connection with special families of stochastic processes (Di Nardo 2016a). Indeed, cumulant polynomials allow us to compute moments and cumulants of multivariate Lévy processes (Di Nardo and Oliva 2011), subordinated multivariate Lévy processes (Di Nardo et al. 2020) and multivariate compound Poisson processes (Di Nardo 2016b). Further examples can be found in Reiner (1976), Shrivastava (2002), Withers and Nadarajah (2010) or Privault (2021).\nThe generalized (complete exponential) Bell polynomials arise from the multivariate Faà di Bruno’s formula, whose computation has been included in the latest version of the kStatistics package. In enumerative combinatorics, Faà di Bruno’s formula is employed in dealing with formal power series. In particular the multivariate Faà di Bruno’s formula gives the \\(\\boldsymbol{i}\\)-th coefficient of the composition (Di Nardo et al. 2011)\n\\[\\begin{equation}\nh(\\boldsymbol{z}) = f\\left(g_1(\\boldsymbol{z})-1, \\ldots,g_n(\\boldsymbol{z})-1\\right)\n\\label{mfaa1}\n\\end{equation}\\]\nwhere \\(f\\) and \\(g_j\\) for \\(j=1, \\ldots, n\\) are (exponential) formal power series\n\\[\\begin{equation} \\label{multps1}\nf(\\boldsymbol{x})= \\sum_{|\\boldsymbol{t}| \\geq 0} f_{\\boldsymbol{t}} \\frac{\\boldsymbol{x}^{\\boldsymbol{t}}}{\\boldsymbol{t}!}\n\\quad \\hbox{and} \\quad\ng_j(\\boldsymbol{z}) = \\sum_{|\\boldsymbol{s}| \\geq 0} g_{j; \\boldsymbol{s}} \\frac{\\boldsymbol{z}^{\\boldsymbol{s}}}{\\boldsymbol{s}!},\n\\end{equation}\\]\nwith \\(\\boldsymbol{x}=(x_1, \\ldots, x_n),\\boldsymbol{z}=(z_1, \\ldots, z_m)\\) and2 \\(\\boldsymbol{x}^{\\boldsymbol{t}} = x_1^{t_1} \\cdots x_n^{t_n},\\)\n\\({\\boldsymbol{z}}^{\\boldsymbol{s}} = z_1^{s_1} \\cdots z_m^{s_m},\\) \\(f_{\\boldsymbol{t}} = f_{t_1, \\ldots, t_n},\\) \\(g_{j; \\boldsymbol{s}} = g_{j; s_1, \\ldots, s_m}\\) for \\(j=1,\\ldots,n,\\) and \\(f_{\\boldsymbol{0}}=g_{1; \\boldsymbol{0}}= \\cdots = g_{n; \\boldsymbol{0}}=1.\\) For instance, from \\(\\eqref{(1bis)}\\) and \\(\\eqref{(2)}\\) joint moments can be recovered from joint cumulants using the multivariate Faà di Bruno’s formula for \\(n=1,\\) \\(g(\\boldsymbol{z}) = 1 + K_{\\boldsymbol{Y}}(\\boldsymbol{z})\\) and \\(f(x)=\\exp(x).\\) As\n\\(1 + K_{\\boldsymbol{Y}}(\\boldsymbol{z})=1 +\\)\n\\(\\log([M_{\\boldsymbol{Y}}(\\boldsymbol{z})-1]+1)\\) then joint cumulants can be recovered from joint moments using the multivariate Faà di Bruno’s formula for \\(n=1, g(\\boldsymbol{z}) = M_{\\boldsymbol{Y}}(\\boldsymbol{z})\\) and \\(f(x)= 1 + \\log (1 + x).\\) Let us remark that the exponential form \\(\\eqref{multps1}\\) of the formal power series \\(f\\) and \\(\\{g_j\\}\\) is not a constraint. To work with ordinary formal power series, the multi-index sequence \\(\\{f_{\\boldsymbol{t}}\\}\\) needs to be replaced by the sequence \\(\\{\\boldsymbol{t}! f_{\\boldsymbol{t}}\\}\\) as well as the multi-index sequence \\(\\{g_{j; \\boldsymbol{s}}\\}\\) by the sequence \\(\\{\\boldsymbol{s}! g_{j; \\boldsymbol{s}}\\}\\) for \\(j=1, \\ldots,n.\\) In this case, the multivariate Faà di Bruno’s formula gives the coefficient \\({\\boldsymbol{i}!} \\tilde{h}_{\\boldsymbol{i}}\\) with \\(\\tilde{h}_{\\boldsymbol{i}}\\) the \\({\\boldsymbol{i}}\\)-th coefficient of the (ordinary) formal power series composition \\(\\eqref{mfaa1}\\).\nThe problem of finding suitable and easily manageable expressions of the multivariate Faà di Bruno’s formula has received attention from several researchers over the years. This is because the multivariate Faà di Bruno’s formula is a very general-purpose tool with many applications. We refer to the paper of Leipnik and Pearce (2007) for a detailed list of references on this subject and a detailed account of its applications. Further applications can be found in Savits (2006), Chacón and Duong (2015), Shabat and Efendiev (2017) and Nguwi et al. (2022). A classical way to generate the multivariate Faà di Bruno’s formula involves the partial derivatives of a composition of multivariable functions. Suppose \\(f(\\boldsymbol{x})\\) and \\(g_1(\\boldsymbol{z}), \\ldots, g_n(\\boldsymbol{z})\\) in \\(\\eqref{mfaa1}\\) be differentiable functions a certain number of times. The multivariate Faà di Bruno’s formula gives the partial derivative of order \\(\\boldsymbol{i}\\) of \\(h(\\boldsymbol{z})\\) in \\(\\boldsymbol{z}_0\\)\n\\[\\begin{equation}\\label{(hi)}\nh_{\\boldsymbol{i}} = \\frac{\\partial^{|\\boldsymbol{i}|}}{\\partial z_1^{i_1} \\cdots\n\\partial z_m^{i_m}} h(z_1, \\ldots, z_m) \\!\\!\\Bigm\\lvert_{\\boldsymbol{z}=\\boldsymbol{z}_0} \\qquad \\hbox{for $|\\boldsymbol{i}|>0,$}\n\\end{equation}\\]\nassuming the partial derivatives of order\n\\(\\boldsymbol{t}\\) of \\(f(\\boldsymbol{x})\\) exist in \\(\\boldsymbol{x}_0=\\) \\(\\left(g_1(\\boldsymbol{z}_0), \\ldots,g_n(\\boldsymbol{z}_0)\\right)\\)\n\\[ \\qquad f_{\\boldsymbol{t}} = \\frac{\\partial^{|\\boldsymbol{t}|}}{\\partial x_1^{t_1} \\cdots\n\\partial x_n^{t_n}} f(x_1, \\ldots, x_n) \\!\\!\\Bigm\\lvert_{\\boldsymbol{x}=\\boldsymbol{x}_0}  \\qquad \\hbox{for $0 < |\\boldsymbol{t}| \\leq |\\boldsymbol{i}|,$}\\]\nand the partial derivatives of order \\(\\boldsymbol{s}\\) of \\(g_j(\\boldsymbol{z})\\) exist in \\(\\boldsymbol{z}_0\\) for \\(j=1,\\ldots,n\\)\n\\[g_{j,\\boldsymbol{s}} =\n\\frac{\\partial^{|\\boldsymbol{s}|}}{\\partial z_1^{s_1} \\cdots\n\\partial z_m^{s_m}} g_j(z_1, \\ldots, z_m) \\!\\!\\Bigm\\lvert_{\\boldsymbol{z}=\\boldsymbol{z}_0} \\qquad \\hbox{for $0 < |\\boldsymbol{s}| \\leq |\\boldsymbol{i}|$}.\\]\nThere are various ways to express \\(h_{\\boldsymbol{i}}\\) in \\(\\eqref{(hi)}\\), see for example Mishkov (2000), Hernández Encinas and Muñoz Masqué (2003) and Ma (2009). Symbolic manipulation using Macsyma, Maple, Mathematica, etc. can produce any required order of \\(\\eqref{(hi)}\\), by applying the chain rule recursively and using a function that provides partial derivatives. Also in R, there are some functions for computing partial derivatives (Clausen and Sokol 2020). Despite its conceptual simplicity, applications of the chain rule become impractical for its cumbersome computation even for small values of its order. As the number of additive terms becomes huge, the output is often untidy and further manipulations are required to simplify the result. By using combinatorial methods, Constantine and Savits (1996) have carried out the following expression of the multivariate Faà di Bruno’s formula\n\\[\\begin{equation} \\label{multfaa}\nh_{\\boldsymbol{i}} = \\boldsymbol{i}! \\sum_{1 \\leq |\\boldsymbol{t}| \\leq |\\boldsymbol{i}|}\nf_{\\boldsymbol{t}} \\sum_{k=1}^{|\\boldsymbol{i}|} \\sum_{p_k(\\boldsymbol{i}, \\boldsymbol{t})} \\prod_{j=1}^k \\frac{({\\mathfrak g}_{\\boldsymbol{l}_j})^{\\boldsymbol{q}_j}}{\\boldsymbol{q}_j! (\\boldsymbol{l}_j!)^{|\\boldsymbol{q}_j|}}\n\\end{equation}\\]\nwhere \\(({\\mathfrak g}_{\\boldsymbol{s}})^{\\boldsymbol{q}}=\\prod_{j=1}^{n} (g_{j,\\boldsymbol{s}})^{q_j}\\) with \\({\\boldsymbol{q}}=(q_1, \\ldots, q_n)\\) and\n\\[\n%\\label{ptset}\np_k(\\boldsymbol{i}, \\boldsymbol{t}) = \\left\\{(\\boldsymbol{q}_1, \\ldots, \\boldsymbol{q}_k;\n\\boldsymbol{l}_1, \\ldots, \\boldsymbol{l}_k): |\\boldsymbol{q}_j|>0, \\sum_{j=1}^k \\boldsymbol{q}_j = \\boldsymbol{t},  \\sum_{j=1}^k |\\boldsymbol{q}_j|\\boldsymbol{l}_j  = \\boldsymbol{i}\n\\right\\}\\]\nwith \\(\\boldsymbol{q}_1, \\ldots, \\boldsymbol{q}_k \\in {\\mathbb N}_0^{n}\\) and \\(\\boldsymbol{l}_1, \\ldots, \\boldsymbol{l}_k \\in {\\mathbb N}_0^{m}\\) such that3 \\(\\boldsymbol{0} \\prec \\boldsymbol{l}_1 \\prec \\ldots \\prec \\boldsymbol{l}_k.\\)\nA completely different approach concerns the combinatorics of partial derivatives as Hardy (2006) pointed out for the univariate-multivariate composition using multisets and collapsing partitions. Motivated by his results and using the umbral calculus, which is a symbolic method particularly useful in dealing with formal power series \\(\\eqref{multps1}\\), the combinatorics behind \\(\\eqref{multfaa}\\) has been simplified and a different expression has been given in Di Nardo et al. (2011). The key tool is the notion of partition of a multi-index which parallels the multiset partitions given in Hardy (2006).\nThe contribution of this paper is multi-sided. We explain how to recover in R a multi-index partition, which is a combinatorial device. For statistical purposes, we show how to recover \\(k\\)-statistics and their multivariate generalizations using the referred polynomial approach and multi-index partitions. Then, we explain the main steps of the MFB function producing the multivariate Faà di Bruno’s formula, without any reference to the umbral calculus or chain rules and whose applications go beyond statistical purposes. The main idea is to expand the multivariable polynomial\n\\[\n\\sum {\\boldsymbol{i} \\choose \\boldsymbol{s}_1,\\ldots,\\boldsymbol{s}_n} q_{1,\\boldsymbol{s}_1}(y_1) \\cdots q_{n,\\boldsymbol{s}_n}(y_n)\n%\\label{(GCBell1)}\n\\]\nwhere \\(q_{1,\\boldsymbol{s}_1}(y_1) \\ldots q_{n,\\boldsymbol{s}_n}(y_n)\\) are suitable polynomials and the sum is over all the compositions of \\(\\boldsymbol{i}\\) in \\(n\\) parts, that is all the \\(n\\)-tuples\n\\((\\boldsymbol{s}_1,\\ldots,\\boldsymbol{s}_n)\\) of non-negative integer \\(m\\)-tuples such that \\(\\boldsymbol{s}_1 + \\cdots + \\boldsymbol{s}_n = \\boldsymbol{i}.\\)\nReaders interested in the umbral setting may refer to Di Nardo (2011) and references therein.\nConsequently, the MFB function gives an efficient computation of the following compositions:\nunivariate with univariate, that is \\(n=m=1;\\)\nunivariate with multivariate, that is \\(n=1\\) and \\(m >1;\\)\nmultivariate with univariate, that is \\(n >1\\) and \\(m=1;\\)\nmultivariate with multivariate, that is \\(n >1\\) and \\(m>1.\\)\nThe kStatistics package includes additional functions, for some of the most widespread applications of the multivariate Faà di Bruno’s formula. Indeed, not only this formula permits to generate joint cumulants and their inverse relations, but also further general families of polynomials. Therefore, we have set up special procedures for those families used very often in applications. These functions should be considered an easy to manage interfaces of the MFB function, with the aim of simplifying its application. Moreover, since the R codes are free, the user might follow similar steps to generate polynomial families not included in the package but\nalways coming from the multivariate Faà di Bruno’s formula. The construction of new families of polynomials can be done mainly in two ways. The first way is to choose appropriately the coefficients \\(\\{f_{\\boldsymbol{t}}\\}\\) and \\(\\{g_{j; \\boldsymbol{s}}\\}\\) in \\(\\eqref{multps1}\\). The second way is to use some suitable symbolic strategies, as discussed in Di Nardo (2011). For both cases, we provide examples.\nThe paper is organized as follows. The next section explains the main steps of the algorithm that produces multi-index partitions with particular emphasis on its combinatorics. Then we present the symbolic strategy to generate \\(k\\)-statistics and their generalizations using suitable polynomial sequences and multi-index partitions. The subsequent section deals with generalized (complete exponential) Bell polynomials and some special cases corresponding to well-known families of polynomials. We have also included the procedures to generate joint cumulants from joint moments and vice versa. In the last section we explain the main steps of the algorithm to produce the multivariate Faà di Bruno’s formula. We give examples of how to build new polynomials not included in the package. Some concluding remarks end the paper.\n2 Partitions of a multi-index\nMost routines of the kStatistics package use the partitions of a multi-index \\(\\boldsymbol{i}.\\) Therefore, before describing any of these routines, we recall the notion of multi-index partition and describe the algorithm for its construction as implemented in the mkmSet function of the package.\nA partition of the multi-index \\(\\boldsymbol{i} = (i_1, \\ldots, i_m) \\in {\\mathbb N}_0^m\\) is a matrix \\(\\Lambda = (\\boldsymbol{\\lambda}_1^{r_1}, \\boldsymbol{\\lambda}_2^{r_2}, \\ldots)\\) of non-negative integers with \\(m\\) rows and no zero columns such that\n\\(r_1 \\geq 1\\) columns are equal to \\(\\boldsymbol{\\lambda}_1,\\) \\(r_2 \\geq 1\\) columns are equal to \\(\\boldsymbol{\\lambda}_2\\) and so on;\nthe columns \\(\\boldsymbol{\\lambda}_1 < \\boldsymbol{\\lambda}_2 < \\ldots\\) are in lexicographic order4;\nthe sum of the integers in the \\(t\\)-th row is equal to \\(i_t,\\) that is \\(\\lambda_{t 1}+\\lambda_{t 2}+\\cdots = i_t\\) for \\(t = 1,2,\\ldots,m.\\)\nWe write \\(\\Lambda \\vdash \\boldsymbol{i}\\) to denote that \\(\\Lambda\\) is a partition of \\(\\boldsymbol{i}.\\) Some further notations are:\n\\(\\mathfrak{m}(\\Lambda)=(r_1, r_2, \\ldots),\\) the vector of multiplicities of \\(\\boldsymbol{\\lambda}_1, \\boldsymbol{\\lambda}_2, \\ldots\\)\n\\(l(\\Lambda)=|\\mathfrak{m}(\\Lambda)|=r_1 + r_2 + \\cdots,\\) the number of columns of \\(\\Lambda\\) with \\(l(\\Lambda )=0\\) if \\(\\Lambda \\vdash \\boldsymbol{0}\\)\n\\(\\Lambda! = (\\boldsymbol{\\lambda}_1!)^{r_1} (\\boldsymbol{\\lambda}_2!)^{r_2} \\cdots\\)\n Example 1: The partitions of \\(\\boldsymbol{i}=(2,1)\\) are the matrices\n\\[\n\\begin{pmatrix}\n2 \\\\\n1\n\\end{pmatrix}, \\begin{pmatrix}\n0 & 2 \\\\\n1 & 0\n\\end{pmatrix}, \\begin{pmatrix}\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}, \\begin{pmatrix}\n0 & 1 & 1 \\\\\n1 & 0 & 0\n\\end{pmatrix} = (\\boldsymbol{\\lambda}_1, \\boldsymbol{\\lambda}_2^2),\\]\nwith\n\\[\n\\boldsymbol{\\lambda}_1 = \\begin{pmatrix}\n0 \\\\\n1\n\\end{pmatrix} \\quad \\hbox{and} \\quad \\boldsymbol{\\lambda}_2 = \\begin{pmatrix}\n1 \\\\\n0\n\\end{pmatrix}.\n\\]\nThe algorithm to get all the partitions of a multi-index resorts to multiset subdivisions. Let’s start by recalling the notion of multiset. A multiset \\(M\\) is a “set with multiplicities”. Suppose \\(a \\in M.\\) Then the multiplicity of \\(a\\) is the number of times \\(a\\) occurs in \\(M\\) as a member. For example, the integers \\(3\\) and \\(2\\) are the multiplicities of \\(a\\) and \\(b\\) respectively in \\(M = \\{a,a,a,b,b\\}.\\) A subdivision of the multiset \\(M\\) is a multiset of sub-multisets of \\(M,\\) such that their disjoint union returns \\(M\\). Examples of subdivisions of \\(M = \\{a,a,a,b,b\\}\\) are\n\\[\\begin{equation}\nS_1 = \\{\\{a\\},\\{a,b\\},\\{a,b\\}\\}, \\qquad\nS_2 = \\{\\{a\\},\\{a,a,b\\},\\{b\\}\\},\n\\label{subdivision}\n\\end{equation}\\]\n\\[\\begin{equation}\nS_3  = \\{\\{a\\},\\{a,a\\},\\{b\\},\\{b\\}\\}.\n\\label{subdivision1}\n\\end{equation}\\]\nThe subdivisions of the multiset \\(M = \\{a,a,a,b,b\\}\\) are in one-to-one correspondence with the partitions \\(\\Lambda \\vdash (3,2).\\) For example, the subdivisions \\(\\eqref{subdivision}\\) correspond to the partitions \\(\\Lambda_1 = (\\boldsymbol{\\lambda}_2, \\boldsymbol{\\lambda}_3^2)\\) and \\(\\Lambda_2 = (\\boldsymbol{\\lambda}_1, \\boldsymbol{\\lambda}_2,\\boldsymbol{\\lambda}_5)\\) respectively, while \\(\\eqref{subdivision1}\\) to \\(\\Lambda_3 = (\\boldsymbol{\\lambda}_1^2, \\boldsymbol{\\lambda}_2,\\boldsymbol{\\lambda}_4)\\) with\n\\[ \\boldsymbol{\\lambda}_1={0 \\choose 1} \\! \\rightarrow \\! \\{b\\} \\,\\,\\, \\boldsymbol{\\lambda}_2={1 \\choose 0}  \\! \\rightarrow \\! \\{a\\}\\] \\[\\boldsymbol{\\lambda}_3={1 \\choose 1}  \\! \\rightarrow \\! \\{a,b\\} \\,\\,\\, \\boldsymbol{\\lambda}_4={2 \\choose 0}  \\! \\rightarrow \\! \\{a,a\\} \\,\\,\\,  \\boldsymbol{\\lambda}_5={2 \\choose 1}  \\! \\rightarrow \\!\\{a,a,b\\}.\\]\nMultiset subdivisions can be recovered by using collapsing set partitions (Hardy 2006). If the members \\(1, 2, 3\\) of the set \\(\\{ 1, 2, 3, 4, 5\\}\\) are made indistinguishable from each other and called \\(a\\), and \\(4\\) and \\(5\\) are made indistinguishable from each other and called \\(b\\), then the set \\(\\{ 1, 2, 3, 4, 5\\}\\) has “collapsed” to the multiset \\(M = \\{a,a,a,b,b\\}.\\) Therefore the subdivisions of \\(M\\) can be recovered using the same substitution in the partitions of \\(\\{ 1, 2, 3, 4, 5\\}.\\) For example, \\(S_1\\) in \\(\\eqref{subdivision}\\) can be recovered from \\(\\{\\{1,4\\},\\{2,5\\}, \\{3\\}\\}\\) or \\(\\{\\{3,5\\},\\{2,4\\}, \\{1\\}\\}\\) and so on. As this last example shows, a subdivision might correspond to several partitions. The number of partitions corresponding to the same subdivision can be computed using the countP function of the package. However, to find multi-index partitions using set partitions is not a particularly efficient algorithm since the computational cost is proportional to the \\(n\\)-th Bell number, if \\(n\\) is the sum of the multi-index components (Charalambides 2002).\nThe mkmSet function is based on a different strategy which takes into account the partitions of the multi-index components. When \\(m=1,\\) the mkmSet function lists all the partitions \\(\\lambda\\) of the integer \\(i.\\) Recall that a partition of an integer \\(i\\) is a sequence \\(\\lambda = (\\lambda_1, \\lambda_2, \\ldots)\\) of weakly decreasing positive integers, named parts of \\(\\lambda,\\) such that \\(\\lambda_1 + \\lambda_2 + \\cdots = i.\\) A different notation is \\(\\lambda = (1^{r_1}, 2^{r_2}, \\ldots),\\) where \\(r_1, r_2, \\ldots\\) are the number of parts of \\(\\lambda\\) equal to \\(1,2,\\ldots\\) respectively. The length of the partition is \\(l(\\lambda)=r_1 + r_2 + \\cdots.\\) We write \\(\\lambda \\vdash i\\) to denote that \\(\\lambda\\) is a partition of \\(i.\\)\nIn the following, we describe the main steps of the mkmSet function by working on an example.\nSuppose we want to generate all the partitions of \\((3,2).\\) First consider the partitions of \\((3,0)\\) obtained from the partitions \\((3), (1,2), (1^3)\\) of the integer \\(3,\\) and the partitions of \\((0,2)\\) obtained from the partitions \\((2),(1^2)\\) of the integer \\(2,\\) that is\n\\[\\begin{equation}\n\\Lambda_1= \\begin{pmatrix}\n3 \\\\\n0\n\\end{pmatrix}\\!\\!, \\, \\Lambda_2= \\begin{pmatrix}\n1 & 2 \\\\\n0 & 0\n\\end{pmatrix}\\!\\!, \\, \\Lambda_3=\\begin{pmatrix}\n1 & 1 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix} \\vdash {3 \\choose 0}\n\\label{(firstsubdivisions)}\n\\end{equation}\\]\n\\[\\begin{equation}\n\\Lambda_4= \\begin{pmatrix}\n0 \\\\\n2\n\\end{pmatrix}\\!\\!, \\, \\Lambda_5= \\begin{pmatrix}\n0 & 0 \\\\\n1 & 1\n\\end{pmatrix} \\vdash {0 \\choose 2}.\n\\label{(firstsubdivisions1)}\n\\end{equation}\\]\nThe following iterated adding-appending rule is thus implemented.\n 1. Consider the partition \\(\\Lambda_5\\) in \\(\\eqref{(firstsubdivisions1)}\\).\n 1.1 Add the first column of \\(\\Lambda_5\\) to each column of \\(\\Lambda_1, \\Lambda_2\\) and \\(\\Lambda_3\\) in \\(\\eqref{(firstsubdivisions)}\\) one by one with the following rules: the sum must be done only once (if the column has multiplicities greater than one) taking as reference the first column; the sum can be done only to columns whose second component is zero and without subsequent elements (in the same row) greater than or equal to the integer we are adding. Then we have\n\\[\\begin{equation}\n\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!{\\small \\Lambda^{(1,1)}_1= \\begin{pmatrix}\n3 \\\\\n1\n\\end{pmatrix} \\Lambda^{(1,1)}_2 = \\begin{pmatrix}\n1 & 2 \\\\\n1 & 0\n\\end{pmatrix} \\Lambda^{(2,1)}_2 = \\begin{pmatrix}\n1 & 2 \\\\\n0 & 1\n\\end{pmatrix}  \\Lambda^{(1,1)}_3=\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 0 & 0\n\\end{pmatrix}}.\n\\label{steps1}\n\\end{equation}\\]\n1.2 Append the same column to each partition\n\\(\\Lambda_1, \\Lambda_2\\) and \\(\\Lambda_3\\) in \\(\\eqref{(firstsubdivisions1)},\\) that is\\[\\begin{equation}\n{\\small \\Lambda^{(1,2)}_1= \\begin{pmatrix}\n3 & 0 \\\\\n0 & 1\n\\end{pmatrix}  \\Lambda^{(1,2)}_2= \\begin{pmatrix}\n1 & 2 & 0\\\\\n0 & 0 & 1\n\\end{pmatrix}  \\Lambda^{(1,2)}_3=\\begin{pmatrix}\n1 & 1 & 1 & 0\\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}}.\n\\label{steps2}\n\\end{equation}\\]\n1.3 Repeat steps 1.1 and 1.2 for the second\ncolumn of \\(\\Lambda_5\\) with respect to the partitions generated in \\(\\eqref{steps1}\\) and \\(\\eqref{steps2}:\\)\\[{\\small \\begin{array}{lll}\n\\Lambda^{(1,1)}_1 = \\begin{pmatrix}\n3 \\\\\n1\n\\end{pmatrix} & \\!\\!\\!\\!\\hbox{add} \\Rightarrow \\hbox{rule out} & \\!\\!\\!\\!\\!\\hbox{append} \\Rightarrow \\begin{pmatrix}\n3 & 0 \\\\\n1 & 1\n\\end{pmatrix} \\\\\n\\Lambda^{(1,1)}_2 = \\begin{pmatrix}\n1 & 2 \\\\\n1 & 0\n\\end{pmatrix} &\\!\\!\\!\\!\\hbox{add} \\Rightarrow \\begin{pmatrix}\n1 & 2 \\\\\n1 & 1\n\\end{pmatrix} & \\!\\!\\!\\!\\!\\hbox{append} \\Rightarrow \\begin{pmatrix}\n1 & 2 & 0\\\\\n1 & 0 & 1\n\\end{pmatrix}\\\\\n\\Lambda^{(2,1)}_2=\\begin{pmatrix}\n1 & 2 \\\\\n0 & 1\n\\end{pmatrix} & \\!\\!\\!\\!\\hbox{add} \\Rightarrow \\hbox{rule out} &  \\!\\!\\!\\!\\!\\hbox{append} \\Rightarrow \\begin{pmatrix}\n1 & 2 & 0\\\\\n0 & 1 & 1\n\\end{pmatrix} \\\\\n\\Lambda^{(1,1)}_3 = \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 0 & 0\n\\end{pmatrix} &  \\!\\!\\!\\!\\hbox{add} \\Rightarrow \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 0\n\\end{pmatrix} &  \\!\\!\\!\\!\\!\\hbox{append} \\Rightarrow \\begin{pmatrix}\n1 & 1 & 1 & 0 \\\\\n1 & 0 & 0 & 1\n\\end{pmatrix} \\\\\n\\Lambda^{(1,2)}_1 = \\begin{pmatrix}\n3 & 0 \\\\\n0 & 1\n\\end{pmatrix} & \\!\\!\\!\\!\\hbox{add} \\Rightarrow \\hbox{rule out} &  \\!\\!\\!\\!\\!\\hbox{append} \\Rightarrow \\begin{pmatrix}\n3 & 0 & 0 \\\\\n0 & 1 & 1\n\\end{pmatrix} \\\\\n\\Lambda^{(1,2)}_2 = \\begin{pmatrix}\n1 & 2 & 0\\\\\n0 & 0 & 1\n\\end{pmatrix} & \\!\\!\\!\\!\\hbox{add} \\Rightarrow \\hbox{rule out} &  \\!\\!\\!\\!\\!\\hbox{append} \\Rightarrow \\begin{pmatrix}\n1 & 2 & 0 & 0\\\\\n0 & 0 & 1 & 1\n\\end{pmatrix} \\\\\n\\Lambda^{(1,2)}_3 = \\begin{pmatrix}\n1 & 1 & 1 & 0\\\\\n0 & 0 & 0 & 1\n\\end{pmatrix} & \\!\\!\\!\\!\\hbox{add} \\Rightarrow \\hbox{rule out} & \\!\\!\\!\\!\\! \\hbox{append} \\Rightarrow \\begin{pmatrix}\n1 & 1 & 1 & 0 & 0\\\\\n0 & 0 & 0 & 1 & 1\n\\end{pmatrix}\n\\end{array}}\\]\n2. Repeat step 1 for \\(\\Lambda_4\\) in \\(\\eqref{(firstsubdivisions1)}:\\)\\[{\\small \\begin{array}{lll}\n\\Lambda_1= \\begin{pmatrix}\n3 \\\\\n0\n\\end{pmatrix} & \\!\\!\\!\\!\\hbox{add} \\Rightarrow \\begin{pmatrix}\n3 \\\\\n2\n\\end{pmatrix}  & \\!\\!\\!\\!\\!\\hbox{append} \\Rightarrow \\begin{pmatrix}\n3 & 0 \\\\\n0 & 2\n\\end{pmatrix}\\\\\n\\Lambda_2= \\begin{pmatrix}\n1 & 2 \\\\\n0 & 0\n\\end{pmatrix} &\\!\\!\\!\\!\\hbox{add} \\Rightarrow \\begin{pmatrix}\n1 & 2 \\\\\n2 & 0\n\\end{pmatrix}, \\begin{pmatrix}\n1 & 2 \\\\\n0 & 2\n\\end{pmatrix} & \\!\\!\\!\\!\\!\\hbox{append} \\Rightarrow \\begin{pmatrix}\n1 & 2 & 0\\\\\n0 & 0 & 2\n\\end{pmatrix} \\\\\n\\Lambda_3=\\begin{pmatrix}\n1 & 1 & 1 \\\\\n0 & 0 & 0\n\\end{pmatrix} & \\!\\!\\!\\!\\hbox{add} \\Rightarrow \\begin{pmatrix}\n1 & 1 & 1 \\\\\n2 & 0 & 0\n\\end{pmatrix} &  \\!\\!\\!\\!\\!\\hbox{append} \\Rightarrow \\begin{pmatrix}\n1 & 1 & 1 & 0 \\\\\n0 & 0 & 0 & 2\n\\end{pmatrix}\n\\end{array}}\\]\nMore generally, the mkmSet function lists all the partitions \\(\\Lambda \\vdash \\boldsymbol{i},\\) with the columns reordered in increasing lexicographic order, together with\nthe number of set partitions corresponding to the same multi-index partition, that is \\(\\boldsymbol{i}!/\\Lambda! \\mathfrak{m}(\\Lambda)!.\\) In the latest version of the kStatistics package, among the input parameters of the mkmSet function, an input flag parameter has been inserted aiming to print the multi-index partitions in a more compact form. See the following example.\nExample 2: To get all the partitions of \\((2,1)\\) run\n> mkmSet(c(2,1),TRUE)\n[( 0 1 )( 1 0 )( 1 0 ),  1 ]\n[( 0 1 )( 2 0 ),  1 ]\n[( 1 0 )( 1 1 ),  2 ]   \n[( 2 1 ),  1 ] \n Note that the integers \\(1,1,2,1\\) correspond to the coefficients \\(2! 1!/\\Lambda! \\mathfrak{m}(\\Lambda)!.\\)\nExample 3: To get all the partitions of the integer \\(3\\) run\n> mkmSet(c(3),TRUE)\n[( 1 )( 1 )( 1 ),  1 ]\n[( 1 )( 2 ),  3 ]\n[( 3 ),  1 ]\nThe mkmSet function is called by the intPart function, specifically designed with the purpose of listing only all the partitions of a given integer in increasing order. The input flag parameter allows us to print the partitions in a more compact form.\nExample 4: To get all the partitions of the integer \\(4\\) run\n> intPart(4,TRUE)\n[ 1 1 1 1 ]\n[ 1 1 2 ]\n[ 2 2 ]\n[ 1 3 ]\n[ 4 ]\nThe parts function of the partitions package (Hankin 2006) lists all the partitions of a given integer, but in decreasing order. Instead the get.partitions function of the nilde package (Arnqvist et al. 2021) finds all the partitions of a given integer with a fixed length \\(l(\\lambda)\\) (Voinov and Pya Arnqvist 2017). If \\(l(\\lambda)\\) is equal to the given integer, the get.partitions function lists all the partitions in increasing order.\n3 kStatistics\nThe \\(i\\)-th \\(k\\)-statistic \\(\\kappa_i\\) is the (unique) symmetric estimator whose expectation is the \\(i\\)-th cumulant \\(k_i(Y)\\) of a population character \\(Y\\) and whose variance is a minimum relative to all other unbiased estimators.\nThe nKS function generates the numerical value of the \\(i\\)-th \\(k\\)-statistic starting from a data sample. The computation relies on the following polynomials\n\\[\\begin{equation}\n{\\mathcal P}_t(y) = \\sum_{j=1}^t y^j S(t,j) (-1)^{j-1} (j-1)! \\qquad \\hbox{for} \\quad t=1, \\ldots, i\n\\label{(ptpol)}\n\\end{equation}\\]\nwhere \\(\\{S(t,j)\\}\\) are the Stirling numbers of second kind, generated trough the nStirling2 function. In detail, suppose to have a sample \\(\\{a_1, \\ldots, a_N\\}\\) of \\(N\\) numerical data and denote with \\(p_t\\) the \\(t\\)-th power sum in the numerical data\n\\[\\begin{equation}\np_t(a_1, \\ldots, a_N)  = \\sum_{j=1}^N a_j^t, \\qquad \\hbox{for} \\,\\,  t\\geq 1.\n\\label{ps}\n\\end{equation}\\]\nTo carry out the numerical value of the \\(i\\)-th \\(k\\)-statistic for \\(i \\leq N,\\) the nKS function computes the explicit expression of the polynomial of degree \\(i\\)\n\\[\\begin{equation}\nQ_i(y) = \\sum_{\\lambda \\vdash i} d_{\\lambda} {\\mathcal P}_{\\lambda}(y) p_{\\lambda}\n\\label{(Qi)}\n\\end{equation}\\]\nwhere the sum is over all the partitions \\(\\lambda=(1^{r_1},2^{r_2},\\ldots) \\vdash i,\\) and\n\\[\\begin{equation}\n\\hskip-1.5cm{\\small d_{\\lambda}=\\frac{i!}{(1!)^{r_1} r_1! (2!)^{r_2} r_2! \\cdots} \\,\\,\\, {\\mathcal P}_{\\lambda}(y) =  [{\\mathcal P}_{1}(y)]^{r_1} [{\\mathcal P}_{2}(y)]^{r_2} \\cdots \\,\\,\\, {p}_{\\lambda} =  [p_{1}]^{r_1} [p_{2}]^{r_2} \\cdots} \\!\\!\\!\n\\label{dlambda}\n\\end{equation}\\]\nwith \\(\\{{\\mathcal P}_{t}(y)\\}\\) and \\(\\{p_{t}\\}\\) given in \\(\\eqref{(ptpol)}\\) and \\(\\eqref{ps}\\) respectively. The final step is to replace the powers \\(y^t\\) in the explicit form of the polynomial \\(\\eqref{(Qi)}\\) with \\((-1)^{t-1} (t-1)!/(N)_t\\) for \\(t=1, \\ldots,i.\\)\nThe main steps of the nKS function are summarized in the following.\n\n\nFunction nKS\n\ni) Compute the power sums \\(p_t\\) in \\(\\eqref{ps}\\) for \\(t=1, \\ldots,i.\\)\n\n\nii) Compute \\(S(t,j) (-1)^{j-1} (j-1)!\\) in \\(\\eqref{(ptpol)}\\) for \\(j=1, \\ldots,t\\) and \\(t=1,\\ldots,i.\\)\n\n\niii) Using the mkmSet function, compute all the partitions \\(\\lambda \\vdash i.\\)\n\n\niv) For a given partition \\(\\lambda\\), expand the product \\({\\mathcal P}_{\\lambda}(y)\\) in \\(\\eqref{(Qi)}\\) and compute the coefficient \\(d_{\\lambda} p_{\\lambda}\\) of each monomial in \\(Q_i(y)\\) using \\(\\eqref{dlambda}.\\)\n\n\nv) For \\(t=1, \\ldots, i\\) multiply \\((-1)^{t-1} (t-1)!/(N)_t\\) with the coefficients of the monomial of degree \\(t\\) carried out at the previous step and do the sum over all the resulting numerical values.\n\n\nvi) Repeat steps iv) and v) for all the partitions \\(\\lambda\\) carried out at step iii) and do the sum over all the resulting numerical values.\n\n\n\nExample 5: Using \\(\\eqref{(Qi)}\\) for \\(i=1,\\) we have \\(Q_1(y) = {\\mathcal P}_1(y) p_1 = y \\sum_{j=1}^N a_j\\) and plugging \\(1/N\\) in place of \\(y,\\) the sample mean is recovered. Using \\(\\eqref{(Qi)}\\) for \\(i=2,\\) we have\n\\[Q_2(y) = {\\mathcal P}_2(y) p_2 +  \\big({\\mathcal P}_1(y) p_1\\big)^2 =  y \\sum_{j=1}^N a_j^2 + y^2 \\bigg( \\big(\\sum_{j=1}^N a_j\\big)^2 - \\sum_{j=1}^N a_j^2\\bigg)\\]\nand plugging \\(1/N\\) in place of \\(y\\) and \\(-1/N(N-1)\\) in place of \\(y^2,\\) the sample variance is recovered. Compare the values of the sample mean, computed with the nKS function and the mean function, and the sample variance, computed with the nKS function and the var function, for the following dataset:\n> data<-c(16.34, 10.76, 11.84, 13.55, 15.85, 18.20, 7.51, 10.22, 12.52, 14.68, \n16.08, 19.43, 8.12, 11.20, 12.95, 14.77, 16.83, 19.80, 8.55, 11.58, 12.10, 15.02, \n16.83, 16.98, 19.92, 9.47, 11.68, 13.41, 15.35, 19.11)\n> nKS(1,data)\n[1] 14.02167\n> mean(data)\n[1] 14.02167\n> nKS(2,data)\n[1] 12.65007\n> var(data)\n[1] 12.65007\nUsing the nKS function, for instance, the sample skewness and the sample kurtosis can be computed. Let us recall that the sample skewness is a measure of the central tendency of a univariate sample and can be computed as \\(\\kappa_3/\\kappa_2^{3/2}\\) where \\(\\kappa_2\\) and \\(\\kappa_3\\) are the second and the third \\(k\\)-statistics respectively (Joanes and Gill 1998). The sample kurtosis is a measure of the tail-heaviness of a sample distribution. The sample excess kurtosis is defined as the sample kurtosis minus 3 and can be computed as \\(\\kappa_4/\\kappa_2^{2}\\) where \\(\\kappa_2\\) and \\(\\kappa_4\\) are the second and the fourth \\(k\\)-statistics respectively (Joanes and Gill 1998).\n> nKS(3,data)/sqrt(nKS(2,data))^(3/2)\n[1] -0.03216229\n> nKS(4,data)/nKS(2,data)^2 + 3\n[1] 2.114708\n\nA similar strategy is employed to compute multivariate \\(k\\)-statistics (the nKM function) of a sample data matrix whose columns each represent a population character. To simplify the notation, in the following we deal with the case of a bivariate data set \\(\\{(a_{1,1},a_{2,1}) \\ldots, (a_{1,N},a_{2,N})\\}\\) of \\(N\\) paired numerical data. Denote with \\(p_{(s,t)}\\) the bivariate power sum in the paired data\n\\[\\begin{equation}\n\\!\\!\\!p_{(s,t)}[(a_{1,1},a_{2,1}), \\ldots, (a_{1,N},a_{2,N})] = \\sum_{j=1}^N a^s_{1,j} a^t_{2,j}\n\\quad \\hbox{for} \\,\\, s,t \\geq 1.\n\\label{doubleps}\n\\end{equation}\\]\nSuppose \\(\\boldsymbol{i}=(i_1, i_2)\\) with \\(i_1, i_2 \\leq N\\) and set \\(i=i_1+i_2.\\) To carry out the numerical value of the \\(\\boldsymbol{i}\\)-th multivariate \\(k\\)-statistic, the nKM function finds the explicit expression of the polynomial\n\\[\\begin{equation}\n{\\mathcal Q}_{\\boldsymbol{i}}(y) =  \\sum_{\\Lambda \\vdash \\boldsymbol{i}} d_{\\Lambda}  P_{\\Lambda}(y)\np_{\\Lambda}\n\\label{Qi1}\n\\end{equation}\\]\nwhere the sum is over all the partitions \\(\\Lambda=(\\boldsymbol{\\lambda}_1^{r_1}, \\boldsymbol{\\lambda}_2^{r_2},\\ldots) \\vdash \\boldsymbol{i},\\) and\n\\[\\begin{equation}\n\\hskip-1.5cm{\\small d_{\\Lambda} = \\frac{\\boldsymbol{i}!}{\\Lambda! \\, \\mathfrak{m}(\\Lambda)!} \\,\\,\nP_{\\Lambda}(y)  =\n  [{\\mathcal P}_{|\\boldsymbol{\\lambda}_1|}(y)]^{r_1}  [{\\mathcal P}_{|\\boldsymbol{\\lambda}_2|}(y)]^{r_2} \\cdots \\,\\, p_{\\Lambda} =\n[p_{\\boldsymbol{\\lambda}_1}]^{r_1} [p_{\\boldsymbol{\\lambda}_2}]^{r_2}  \\cdots\n\\label{dlm}}\\!\\!\\!\n\\end{equation}\\]\nwith \\(\\{{\\mathcal P}_{t}(y)\\}\\) and \\(\\{p_{(s,t)}\\}\\) given in \\(\\eqref{(ptpol)}\\) and \\(\\eqref{doubleps}\\) respectively. As for the univariate \\(k\\)-statistics, the final step consists in replacing the powers \\(y^j\\) in the explicit expression of the polynomial \\(\\eqref{Qi1}\\) with the numerical values \\((-1)^{j-1} (j-1)!/(N)_j\\) for \\(j=1, \\ldots,i.\\)\n The main steps of the nKM function are summarized in the following.\n\n\nFunction nKM\n\ni) Compute the bivariate power sums \\(p_{(s,t)}\\) in \\(\\eqref{doubleps}\\) for \\(s=1, \\ldots,i_1\\) and \\(t=1, \\ldots, i_2.\\)\n\n\nii) For \\(i=i_1+i_2,\\) compute \\(S(t,j) (-1)^{j-1} (j-1)!\\) in \\(\\eqref{(ptpol)}\\) for \\(j=1, \\ldots,t\\) and \\(t=1,\\ldots,i.\\)\n\n\niii) Using the mkmSet function, compute all the partitions \\(\\Lambda \\vdash \\boldsymbol{i}.\\)\n\n\niv) For a given partition \\(\\Lambda\\), expand the product \\(P_{\\Lambda}(y)\\) in \\(\\eqref{Qi1}\\) and compute the coefficient \\(d_{\\Lambda} p_{\\Lambda}\\) of each monomial in \\({\\mathcal Q}_{\\boldsymbol{i}}(y)\\) using \\(\\eqref{dlm}.\\)\n\n\nv) For \\(j=1, \\ldots, i,\\) multiply \\((-1)^{j-1} (j-1)!/(N)_j\\) with the coefficient of the monomial of degree \\(j\\) carried out at the previous step and do the sum over all the resulting numerical values.\n\n\nvi) Repeat steps iv) and v) for all the partitions\n\\(\\Lambda\\) carried out at step iii) and do the sum over all the resulting numerical values.\n\n\n\nExample 6: To estimate the joint cumulant \\(c_{2,1}\\) of the following dataset, run\n> data1<-list(c(5.31,11.16),c(3.26,3.26),c(2.35,2.35),c(8.32,14.34),\nc(13.48,49.45),c(6.25,15.05),c(7.01,7.01),c(8.52,8.52),c(0.45,0.45),\nc(12.08,12.08),c(19.39,10.42))\n> nKM(c(2,1),data1)\n[1] -23.7379\nIf the first column are observations of a population character \\(X\\) and the second column observations of a population character \\(Y,\\) then \\(c_{2,1}\\) measures how far from connectedness (as opposite to independence) are \\(X^2\\) and \\(Y\\) (Di Nardo et al. 2020). A similar meaning has the estimation of the joint cumulant \\(c_{2,2,2}\\) of the following dataset:\n> data2<-list(c(5.31,11.16,4.23),c(3.26,3.26,4.10),c(2.35,2.35,2.27),\nc(4.31,10.16,6.45),c(3.1,2.3,3.2),c(3.20, 2.31, 7.3))\n> nKM(c(2,2,2),data2)\n[1] 678.1045\n4 Polykays\nSimilarly to \\(k\\)-statistics, polykays are symmetric unbiased estimators of cumulant products. More in detail, when evaluated on a random sample, the \\(\\boldsymbol{i}\\)-th polykay gives an estimation of the product \\(k_{i_1}(Y) \\cdots k_{i_m}(Y),\\) where \\(\\boldsymbol{i} = (i_1, \\ldots, i_m) \\in {\\mathbb N}_0^m\\) and \\(\\{k_{i_j}(Y)\\}\\) are cumulants of a population character \\(Y.\\)\nTo simplify the notation, in the following we show how to compute the \\(\\boldsymbol{i}\\)-th polykay of \\(N\\) numerical data \\(\\{a_1, \\ldots, a_N\\}\\) using the nPS function for \\(\\boldsymbol{i} = (i_1, i_2)\\). Set \\(i=i_1+i_2\\) and suppose \\(i \\leq N.\\) The computation relies on the so-called logarithmic polynomials\n\\[\\begin{equation}\\label{Pj}\n\\hskip-0.5cm \\tilde{P}_{t}(y_1, \\ldots, y_i) =  \\sum_{\\lambda \\vdash t}  y_{\\lambda} d_{\\lambda}\n(-1)^{l(\\lambda)-1} (l(\\lambda)-1)!\n\\end{equation}\\]\nfor \\(t=1, \\ldots, i\\) where the sum is over all the partitions \\(\\lambda=(1^{r_1},2^{r_2},\\ldots) \\vdash t,\\) \\(d_{\\lambda}\\) is given in \\(\\eqref{dlambda}\\) and \\(y_{\\lambda} = y_1^{r_1} y_2^{r_2} \\cdots.\\) To compute the polykay of order \\((i_1, i_2)\\), the nPS function finds the explicit expression of the polynomial\n\\[\\begin{equation}\\label{Qi2}\nA_i (y_1, \\ldots, y_i) = \\sum_{\\lambda \\vdash i} d_{\\lambda} \\tilde{P}_{\\lambda}(y_1, \\ldots, y_i) p_{\\lambda}\n\\end{equation}\\]\nwhere the sum is over all the partitions \\(\\lambda=(1^{r_1},2^{r_2},\\ldots) \\vdash i,\\) \\(d_{\\lambda}\\) and \\(p_{\\lambda}\\) are given in \\(\\eqref{dlambda}\\) and\n\\[\\tilde{P}_{\\lambda}(y_1, \\ldots, y_i)  = [\\tilde{P}_{1}(y_1, \\ldots, y_i)]^{r_1} [\\tilde{P}_{2}(y_1, \\ldots, y_i)]^{r_2} \\cdots\\]\nwith \\(\\{\\tilde{P}_{t}(y_1, \\ldots, y_i)\\}\\) given in \\(\\eqref{Pj}.\\) Note that the monomials in \\(A_i (y_1, \\ldots, y_i)\\) are of type \\(y_{\\lambda}= y_1^{r_1} y_2^{r_2} \\cdots\\) with \\(\\lambda = (1^{r_1}, 2^{r_2}, \\ldots) \\vdash i.\\) The final step is to plug suitable numerical values in place of \\(y_{\\lambda}\\) depending on how the partition \\(\\lambda\\) is constructed. Indeed, set\n\\[\\begin{equation}\n\\hskip-0.5cm{\\small \\tilde{q}(i_1,i_2) = \\bigg\\{ \\lambda^{\\prime} + \\lambda^{\\prime \\prime} \\vdash i  \\, \\big| \\, \\lambda^{\\prime}=(1^{s_1}, 2^{s_2}, \\ldots) \\vdash i_1, \\lambda^{\\prime \\prime} = (1^{t_1}, 2^{t_2},\\ldots) \\vdash i_2\\bigg\\}} \\!\\!\\!\n\\label{pol2}\n\\end{equation}\\]\nwhere \\(\\lambda^{\\prime}+\\lambda^{\\prime \\prime} = (1^{r_1}, 2^{r_2}, \\ldots)\\) with \\(r_j = s_j + t_j\\) for \\(j=1,2,\\ldots.\\) Then\n\\(y_{\\lambda}\\) is replaced by \\(0\\) if \\(\\lambda \\not \\in \\tilde{q}(i_1,i_2)\\) otherwise by\n\\[\\begin{equation}\n\\frac{(-1)^{l(\\lambda^{\\prime})-1}(l(\\lambda^{\\prime})-1)! (-1)^{l(\\lambda^{\\prime \\prime})-1}(l(\\lambda^{\\prime \\prime})-1)!}{(N)_{l(\\lambda^{\\prime \\prime})+l(\\lambda^{\\prime \\prime})}} \\frac{d_{\\lambda^{\\prime}} d_{\\lambda^{\\prime \\prime}}}{d_{\\lambda^{\\prime}+\\lambda^{\\prime \\prime}}}.\n\\label{pol1}\n\\end{equation}\\]\nNote that\n\\(d_{\\lambda^{\\prime}}\\) and \\(d_{\\lambda^{\\prime \\prime}}\\) in \\(\\eqref{pol1}\\) are recovered from \\(\\eqref{dlambda}.\\)\n The main steps of the nPS function are summarized in the following.\n\n\nFunction nPS\n\ni) Set \\(i=i_1+i_2\\) and compute the power sums \\(p_t\\) in \\(\\eqref{ps}\\) for \\(t=1, \\ldots,i.\\)\n\n\nii) Generate the polynomials \\(\\tilde{P}_{t}(y_1, \\ldots, y_i)\\) in \\(\\eqref{Pj}\\) for \\(t=1, \\ldots, i.\\)\n\n\niii) Using the mkmSet function, compute all the partitions \\(\\lambda \\vdash i.\\)\n\n\niv) For a given partition \\(\\lambda\\), expand the product \\(\\tilde{P}_{\\lambda}(y_1, \\ldots, y_i)\\) in \\(\\eqref{Qi2};\\) then plug \\(\\eqref{pol1}\\) or \\(0\\) in each monomial \\(y_{\\lambda},\\) depending if \\(\\lambda\\) is or not in the set \\(\\tilde{q}(i_1,i_2)\\) given in \\(\\eqref{pol2}.\\)\n\n\nv) Multiply the numerical value of \\(\\tilde{P}_{\\lambda}\\) carried out at step iv) with \\(d_{\\lambda} p_{\\lambda}\\) given in \\(\\eqref{dlambda}\\).\n\n\nvi) Repeat steps iv) and v) for all the partitions \\(\\lambda\\) carried out at step iii) and do the sum over all the resulting numerical values.\n\n\n\nExample 7: Suppose we need to estimate the square of the variance \\(\\sigma^2\\) of the population character \\(Y\\) from which the data of Example 5 have been sampled. We have\n> nKS(2,data)^2\n[1] 160.0243\n> var(data)^2\n[1] 160.0243\n but \\(k_2^2\\) is not an unbiased estimator of the square of \\(\\sigma^2.\\) An unbiased estimator of such a square is the polykay of order \\((2,2),\\) that is\n> nPS(c(2,2),data)\n[1] 154.1177\n\nMultivariate polykays are unbiased estimators of products of multivariate cumulants and the nPM function returns a numerical value for these estimators when evaluated on a random sample. As before, to show how the nPM function works, we consider a bivariate sample of \\(N\\) numerical data, that is \\(\\{(a_{1,1},a_{2,1}) \\ldots, (a_{1,N},a_{2,N})\\}.\\) If we choose \\(\\boldsymbol{i}=(i_1, i_2)\\) and \\(\\boldsymbol{j}=(j_1, j_2)\\) with \\(i_1 + i_2 + j_1 + j_2 \\leq N\\) as input of the nPM function, the output is a numerical value which represents an estimated value of the product \\(k_{\\boldsymbol{i}}(X,Y) k_{\\boldsymbol{j}}(X,Y),\\) where \\(k_{\\boldsymbol{i}}(X,Y)\\) and \\(k_{\\boldsymbol{j}}(X,Y)\\) are cumulants of the population characters \\((X,Y).\\) The computation relies on suitable polynomials in the indeterminates \\(\\{y_{(s,t)}\\}\\) for \\(s=0,\\ldots, w_1, t=0,\\ldots, w_2,\\) with \\(s+t>0\\) and \\(w_1=i_1+j_1, w_2=i_2+j_2.\\)\nThese polynomials are a multivariable generalization of \\(\\eqref{Pj},\\) that is\n\\[\\begin{equation}\\label{Pjm}\n\\tilde{P}_{\\boldsymbol{k}}\\left( \\{y_{(s,t)}\\} \\right)  =  \\sum_{\\Lambda \\vdash \\boldsymbol{k}}\ny_{\\Lambda} d_{\\Lambda} (-1)^{l(\\Lambda)-1} (l(\\Lambda)-1)!\n\\end{equation}\\]\nfor \\(\\boldsymbol{0} < \\boldsymbol{k} \\leq \\boldsymbol{w}=(w_1,w_2),\\) where the sum is over all the partitions \\(\\Lambda=(\\boldsymbol{\\lambda}_1^{r_1}, \\boldsymbol{\\lambda}_2^{r_2},\\ldots) \\vdash \\boldsymbol{k}\\) and \\(y_{\\Lambda} = y_{\\boldsymbol{\\lambda}_1}^{r_1} y_{\\boldsymbol{\\lambda_2}}^{r_2} \\cdots.\\) To compute the multivariate polykay of order \\((\\boldsymbol{i}, \\boldsymbol{j}),\\) the nPM function finds the explicit expression of the polynomial\n\\[\\begin{equation}\\label{Ai}\n{\\mathcal A}_{\\boldsymbol{w}} \\left( \\{y_{(s,t)}\\} \\right) =  \\sum_{\\Lambda \\vdash \\boldsymbol{w}} d_{\\Lambda}  \\tilde{P}_{\\Lambda}\\left( \\{y_{(s,t)}\\} \\right) p_{\\Lambda}\n\\end{equation}\\]\nwhere the sum is over all the partitions \\(\\Lambda=(\\boldsymbol{\\lambda}_1^{r_1}, \\boldsymbol{\\lambda}_2^{r_2},\\ldots) \\vdash \\boldsymbol{w},\\) \\(d_{\\Lambda}\\) and \\(p_{\\Lambda}\\) are given in \\(\\eqref{dlm}\\), and\n\\[\\tilde{P}_{\\Lambda}\\left( \\{y_{(s,t)}\\} \\right)  = [\\tilde{P}_{\\boldsymbol{\\lambda}_1}\n\\left( \\{y_{(s,t)}\\} \\right) ]^{r_1} [\\tilde{P}_{\\boldsymbol{\\lambda}_2}\\left( \\{y_{(s,t)}\\} \\right) ]^{r_2}\n\\cdots\\]\nwith \\(\\{\\tilde{P}_{\\boldsymbol{\\lambda}}\\left( \\{y_{(s,t)}\\} \\right)\\}\\) given in \\(\\eqref{Pjm}.\\)\nThe monomials in \\({\\mathcal A}_{\\boldsymbol{w}}\\left( \\{y_{(s,t)}\\} \\right)\\) are of type \\(y_{\\Lambda}\\) with \\(\\Lambda \\vdash \\boldsymbol{w}.\\) The final step is to plug suitable numerical values in place of \\(y_{\\Lambda}\\) depending on how the partition \\(\\Lambda\\) is constructed. Indeed, set\n\\[\\begin{equation}\\label{qw}\n\\hskip-1.3cm{\\small \\tilde{q}(\\boldsymbol{w}) = \\bigg\\{  \\Lambda^{\\prime}+\\Lambda^{\\prime \\prime} \\vdash \\boldsymbol{w} \\, \\big| \\, \\Lambda^{\\prime}=(\\boldsymbol{\\lambda}_1^{\\prime s_1}, \\boldsymbol{\\lambda}_2^{\\prime s_2}, \\ldots) \\vdash \\boldsymbol{i}, \\Lambda^{\\prime \\prime} = (\\boldsymbol{\\lambda}_1^{\\prime \\prime t_1}, \\boldsymbol{\\lambda}_2^{\\prime \\prime t_2},\\ldots) \\vdash \\boldsymbol{j} \\bigg\\}},\\!\\!\\!\n\\end{equation}\\]\nwhere\n\\(\\Lambda^{\\prime}+\\Lambda^{\\prime \\prime}=(\\tilde{\\boldsymbol{\\lambda}}_1^{r_1}, \\tilde{\\boldsymbol{\\lambda}}_2^{r_2}, \\ldots)\\) is built with the columns of\n\\(\\Lambda^{\\prime}\\) and \\(\\Lambda^{\\prime \\prime}\\) rearranged in increasing lexicographic order and such that \\(r_j=s_j\\) if \\(\\tilde{\\boldsymbol{\\lambda}}_j = \\boldsymbol{\\lambda}^{\\prime}_j\\) or \\(r_j=t_j\\) if \\(\\tilde{\\boldsymbol{\\lambda}}_j = \\boldsymbol{\\lambda}^{\\prime \\prime}_j\\) or \\(r_j=s_j+t_j\\) if \\(\\tilde{\\boldsymbol{\\lambda}}_j = \\boldsymbol{\\lambda}^{\\prime}_j = \\boldsymbol{\\lambda}^{\\prime \\prime}_j.\\)\nTherefore in the explicit expression of \\(\\eqref{Ai},\\) \\(y_{\\Lambda}\\) is replaced by \\(0\\) if \\(\\Lambda \\not \\in \\tilde{q}(\\boldsymbol{w})\\) otherwise by\n\\[\\begin{equation}\\label{ybmtau}\n\\frac{{(-1)^{l(\\Lambda^{\\prime})-1}(l(\\Lambda^{\\prime})-1)!(-1)^{l(\\Lambda^{\\prime \\prime})-1}(l(\\Lambda^{\\prime \\prime})-1)!}}{(N)_{l(\\Lambda^{\\prime})+l(\\Lambda^{\\prime \\prime})}} \\frac{d_{\\Lambda^{\\prime}} d_{\\Lambda^{\\prime \\prime}}}{d_{\\Lambda^{\\prime}+\\Lambda^{\\prime \\prime}}}.\n\\end{equation}\\]\nNote that \\(d_{\\Lambda^{\\prime}}\\) and \\(d_{\\Lambda^{\\prime \\prime}}\\) in \\(\\eqref{ybmtau}\\) are recovered from \\(\\eqref{dlm}.\\)\n The main steps of the nPM function are summarized in the following.\n\n\nFunction nPM\n\ni) Set \\(w_1=i_1+j_1\\) and \\(w_2=i_2+j_2;\\) compute the power sums \\(p_{(s,t)}\\) in \\(\\eqref{doubleps}\\) for \\(s=1, \\ldots, w_1\\) and \\(t=1, \\ldots, w_2.\\)\n\n\nii) Generate the polynomials \\(\\tilde{P}_{\\boldsymbol{k}} \\left( \\{y_{(s,t)}\\} \\right)\\) in \\(\\eqref{Pjm}\\) for \\(0<\\boldsymbol{k} \\leq \\boldsymbol{w}=(w_1,w_2).\\)\n\n\niii) Using the mkmSet function, compute all the partitions \\(\\Lambda \\vdash \\boldsymbol{w}.\\)\n\n\niv) For a given partition \\(\\Lambda\\), expand the product \\(\\tilde{P}_{\\Lambda}\\left( \\{y_{(s,t)}\\}\\right)\\) in \\(\\eqref{Ai}\\) and plug \\(\\eqref{ybmtau}\\) or \\(0\\) in each obtained monomial of type \\(y_{\\Lambda}\\)\ndepending if \\(\\Lambda\\) is or not in \\(\\tilde{q}(\\boldsymbol{w})\\) given in \\(\\eqref{qw}.\\)\n\n\nv) Multiply the numerical value of \\(\\tilde{P}_{\\Lambda}\\) obtained at step iv) with \\(d_{\\Lambda} p_{\\Lambda}\\) given in \\(\\eqref{dlm}.\\)\n\n\nvi) Repeat steps iv) and v) for all the partitions \\(\\Lambda\\) carried out at step iii) and do the sum over all the resulting numerical values.\n\n\n\nExample 8: For the same dataset employed in Example 6, to\nestimate \\(k_{(2,1)}(X,Y) k_{(1,0)}(X,Y)\\) run\n> nPM(list(c(2,1),c(1,0)),data1)\n[1] 48.43243\n\n\nRemark 1: The master nPolyk function runs one of the nKS, nKM, nPS and nPM functions depending if we ask for simple \\(k\\)-statistics, multivariate \\(k\\)-statistics, simple polykays or multivariate polykays.\n5 Bell polynomials and generalizations\nThe algorithms to produce \\(k\\)-statistics and polykays rely on handling suitable polynomial families which are special cases of generalizations of Bell polynomials, as introduced in this section. Moreover, there are further families of polynomials widely used in applications which are special cases of these polynomials. For the most popular ones, we have implemented special functions in the kStatistics package. The list is not exhaustive, see for instance Roman (1984). Furthermore additional families of polynomials might be recovered using the multivariate Faà di Bruno’s formula. We will give some examples in the next section.\nThe \\(\\boldsymbol{i}\\)-th generalized (complete exponential) Bell polynomial in the indeterminates \\(y_1, \\ldots, y_n\\) is\n\\[\\begin{equation}\n\\hskip-0.5cm{\\small h_{\\boldsymbol{i}}(y_1, \\ldots, y_n) = \\boldsymbol{i}! \\sum_{{\\Lambda} \\vdash \\boldsymbol{s}_1, \\ldots, \\tilde{\\Lambda} \\vdash  \\boldsymbol{s}_n \\atop \\boldsymbol{s}_1 + \\cdots + \\boldsymbol{s}_n = \\boldsymbol{i}} y_1^{l(\\Lambda)} \\cdots y_n^{l(\\tilde{\\Lambda})} \\frac{g_{1,\\Lambda} \\cdots g_{n,\\tilde{\\Lambda} }}{\\Lambda!\n\\cdots \\tilde{\\Lambda}! \\, \\mathfrak{m}(\\Lambda)! \\cdots \\mathfrak{m}(\\tilde{\\Lambda})!}}\\!\\!\\!\n\\label{(sol22ter)}\n\\end{equation}\\]\nwhere the sum is over all the partitions \\({\\Lambda} \\vdash \\boldsymbol{s}_1, \\ldots, \\tilde{\\Lambda} \\vdash \\boldsymbol{s}_n\\) with \\(\\boldsymbol{s}_1,\\ldots,\\boldsymbol{s}_n\\) \\(m\\)-tuples of non-negative integers such that \\(\\boldsymbol{s}_1 + \\cdots + \\boldsymbol{s}_n = \\boldsymbol{i}\\) and\n\\[\\begin{equation}\n\\begin{array}{rcl}\ng_{1,\\Lambda} & = & g_{1; \\boldsymbol{\\lambda}_1}^{r_1} g_{1; \\boldsymbol{\\lambda}_2}^{r_2} \\cdots \\qquad \\hbox{for}  \\, \\Lambda=(\\boldsymbol{\\lambda}_1^{r_1} , \\boldsymbol{\\lambda}_2^{r_2}, \\ldots) \\\\\n& \\vdots & \\\\\ng_{n,\\tilde{\\Lambda}} & = &  g_{n; \\tilde{\\boldsymbol{\\lambda}}_1}^{t_1} g_{n; \\tilde{\\boldsymbol{\\lambda}}_2}^{t_2} \\cdots \\qquad \\hbox{for}  \\,\\, \\tilde{\\Lambda}=(\\tilde{\\boldsymbol{\\lambda}}_1^{t_1} , \\tilde{\\boldsymbol{\\lambda}}_2^{t_2}, \\ldots)\n\\label{sequences}\n\\end{array}\n\\end{equation}\\]\nwith \\(\\{ g_{1; \\boldsymbol{\\lambda}}\\}, \\ldots, \\{ g_{n; \\boldsymbol{\\tilde{\\lambda}}}\\}\\) multi-indexed sequences. These polynomials are the output of the GCBellPol function.\nExample 9: To get \\(h_{(1,1)}(y_1, y_2)\\) run\n> GCBellPol(c(1,1),2)\n[1] (y1)(y2)g1[0,1]g2[1,0] + (y1)(y2)g1[1,0]g2[0,1] + (y1^2)g1[0,1]g1[1,0] +\n(y1)g1[1,1] + (y2^2)g2[0,1]g2[1,0] + (y2)g2[1,1]\nThe e_GCBellPol function evaluates \\(h_{\\boldsymbol{i}}(y_1, \\ldots, y_n)\\) when its indeterminates \\(y_1, \\ldots, y_n\\) and/or its coefficients are substituted with numerical values.\nExample 10: To plug the values from \\(1\\) to \\(6\\) respectively into the coefficients g1[ , ] and g2[ , ] of the polynomial \\(h_{(1,1)}(y_1, y_2)\\) given in Example 9 run\n> e_GCBellPol(c(1,1), 2, \"g1[0,1]=1, g1[1,0]=2, g1[1,1]=3, g2[0,1]=4, g2[1,0]=5, \ng2[1,1]=6\")\n[1] 13(y1)(y2) + 2(y1^2) + 3(y1) + 20(y2^2) + 6(y2)\nTo evaluate \\(h_{(1,1)}(1, 5)\\) run\n> e_GCBellPol(c(1,1), 2, \"y1=1,  y2=5, g1[0,1]=1, g1[1,0]=2, g1[1,1]=3, g2[0,1]=4, \ng2[1,0]=5, g2[1,1]=6\")\n[1] 600\nWhen the multi-indexed sequences \\(\\{ g_{1; \\boldsymbol{\\lambda}}\\}, \\ldots, \\{ g_{n; \\boldsymbol{\\tilde{\\lambda}}}\\}\\) are all equal, the number of distinct addends in \\(\\eqref{(sol22ter)}\\) might reduce and the corresponding generalized Bell polynomial is denoted by \\(\\tilde{h}_{\\boldsymbol{i}}(y_1, \\ldots, y_n)\\). To deal with this special case, we have inserted an input flag parameter in the e_GCBellPol function.\nExample 11: To compare \\(\\tilde{h}_{(1,1)}(y_1, y_2)\\) with \\(h_{(1,1)}(y_1, y_2)\\) given in Example 9 run\n> GCBellPol(c(1,1),2,TRUE)\n[1] 2(y1)(y2)g[0,1]g[1,0] + (y1^2)g[0,1]g[1,0] + (y1)g[1,1] + (y2^2)g[0,1]g[1,0] +\n(y2)g[1,1]\nSet \\(n=1\\) in \\(\\eqref{(sol22ter)}\\). Then \\(h_{\\boldsymbol{i}}(y_1, \\ldots, y_n)\\) reduces to the univariate polynomial\\[\\begin{equation}\nh_{\\boldsymbol{i}}(y) =  \\sum_{{\\Lambda} \\vdash  \\boldsymbol{i}} y^{l(\\Lambda)} d_{\\Lambda}  \ng_{\\Lambda}\n\\label{(redGC)}\n\\end{equation}\\]\nwhere the sum is over all the partitions \\(\\Lambda=(\\boldsymbol{\\lambda}_1^{r_1} , \\boldsymbol{\\lambda}_2^{r_2}, \\ldots) \\vdash \\boldsymbol{i},\\) \\(d_{\\Lambda}\\) is given in \\(\\eqref{dlm}\\) and\n\\(g_{\\Lambda} = g_{\\boldsymbol{\\lambda}_1}^{r_1} g_{\\boldsymbol{\\lambda}_2}^{r_2} \\cdots.\\)\nExample 12: To get \\(h_{(1,1)}(y)\\) run\n> GCBellPol(c(1,1),1)\n[1] (y^2)g[0,1]g[1,0] + (y)g[1,1]\nRemark 2: For all \\(\\boldsymbol{i} \\in {\\mathbb N}_0^m,\\) we have \\(h_{\\boldsymbol{i}}(y_1 + \\cdots + y_n)= \\tilde{h}_{\\boldsymbol{i}}(y_1, \\ldots, y_n),\\) where \\(\\tilde{h}_{\\boldsymbol{i}}(y_1, \\ldots, y_n)\\) is the \\(\\boldsymbol{i}\\)-th generalized Bell polynomial \\(\\eqref{(sol22ter)}\\) corresponding to all equal multi-indexed sequences \\(\\{ g_{1, \\boldsymbol{\\lambda}}\\}, \\ldots, \\{ g_{n, \\boldsymbol{\\tilde{\\lambda}}}\\}\\) (Di Nardo 2011). Therefore the e_GCBellPol function, with the input flag TRUE, produces also an explicit expression of \\(h_{\\boldsymbol{i}}(y_1 + \\cdots + y_n).\\)\nThe algorithm to generate joint moments in terms of joint cumulants and vice versa follows the same pattern designed to generate \\(\\{h_{\\boldsymbol{i}}(y)\\}.\\) Indeed if \\(\\{k_{\\boldsymbol{i}}(\\boldsymbol{Y})\\}\\) and \\(\\{m_{\\boldsymbol{i}}(\\boldsymbol{Y})\\}\\) denote the sequences of joint cumulants and joint moments of a random vector \\(\\boldsymbol{Y}\\) respectively, then\n\\[\\begin{equation}\\label{cummom}\n\\hskip-1.3cm{\\small m_{\\boldsymbol{i}}(\\boldsymbol{Y}) = \\sum_{{\\Lambda} \\vdash  \\boldsymbol{i}} d_{\\Lambda} k_{\\Lambda}(\\boldsymbol{Y}) \\,\\,  \\hbox{and} \\,\\, k_{\\boldsymbol{i}}(\\boldsymbol{Y}) = \\sum_{{\\Lambda} \\vdash  \\boldsymbol{i}} (-1)^{l(\\Lambda)-1} (l(\\Lambda)-1)! d_{\\Lambda} m_{\\Lambda}(\\boldsymbol{Y})},\n\\end{equation}\\]\nwhere the sum is over all the partitions \\(\\Lambda=(\\boldsymbol{\\lambda}_1^{r_1} , \\boldsymbol{\\lambda}_2^{r_2}, \\ldots) \\vdash \\boldsymbol{i},\\) \\(d_{\\Lambda}\\) is given in \\(\\eqref{dlm}\\) and\n\\[{\\small m_{\\Lambda}(\\boldsymbol{Y})= [m_{{\\boldsymbol{\\lambda}}_1}(\\boldsymbol{Y})]^{r_1} [m_{{\\boldsymbol{\\lambda}}_2}(\\boldsymbol{Y})]^{r_2} \\cdots \\quad k_{\\Lambda}(\\boldsymbol{Y})= [k_{{\\boldsymbol{\\lambda}}_1}(\\boldsymbol{Y})]^{r_1} [k_{{\\boldsymbol{\\lambda}}_2}(\\boldsymbol{Y})]^{r_2} \\cdots}.\\]\nIn particular\nthe mom2cum function returns the right hand side of the first equation in \\(\\eqref{cummom}\\), using the same algorithm producing \\(h_{\\boldsymbol{i}}(y)\\) in \\(\\eqref{(redGC)}\\) with the sequence \\(\\{k_{\\boldsymbol{\\lambda}}\\}\\) in place of \\(\\{g_{\\boldsymbol{\\lambda}}\\}\\) and with \\(1\\) in place of \\(y;\\)\nthe cum2mom function returns the right hand side of the latter equation in \\(\\eqref{cummom}\\), using the same algorithm producing \\(h_{\\boldsymbol{i}}(y)\\) in \\(\\eqref{(redGC)}\\) with the sequence \\(\\{m_{\\boldsymbol{\\lambda}}\\}\\) in place of \\(\\{g_{\\boldsymbol{\\lambda}}\\}\\) and with \\((-1)^{j-1} (j-1)!\\) in place of the powers \\(y^j\\) for \\(j=1,\\ldots,|\\boldsymbol{i}|.\\)\nWhen the multi-index \\(\\boldsymbol{i}\\) reduces to an integer \\(i,\\) formulae \\(\\eqref{cummom}\\) are the classical expressions of univariate moments in terms of univariate cumulants and vice versa. The mom2cum and cum2mom functions do the same when the input is an integer.\nExample 13: To get \\(m_{(3,1)}\\) in terms of \\(k_{(i,j)}\\)\nrun\n> mom2cum(c(3,1))\n[1] k[0,1]k[1,0]^3 + 3k[0,1]k[1,0]k[2,0] + k[0,1]k[3,0] + 3k[1,0]^2k[1,1] +\n3k[1,0]k[2,1] + 3k[1,1]k[2,0] + k[3,1]\nTo get \\(k_{(3,1)}\\) in terms of \\(m_{(i,j)}\\) run\n> cum2mom(c(3,1))\n[1]  - 6m[0,1]m[1,0]^3 + 6m[0,1]m[1,0]m[2,0] - m[0,1]m[3,0] + \n6m[1,0]^2m[1,1] - 3m[1,0]m[2,1] - 3m[1,1]m[2,0] + m[3,1]\nRemark 3: There are different functions in R\nperforming similar computations for cumulants and moments: for instance see De Leeuw, J. (2012) for the multivariate case. A different strategy would rely on the recursive relations between cumulants and moments (Domino et al. 2018).\nSimilarly to \\(\\eqref{cummom}\\), some of the polynomials employed in the previous sections are generated using the same pattern developed to find the explicit expression of \\(h_{\\boldsymbol{i}}(y)\\) in \\(\\eqref{(redGC)}\\):\nThe generation of an explicit expression of \\({\\mathcal Q}_{\\boldsymbol{i}}(y)\\) in \\(\\eqref{Qi1}\\) parallels the one implemented for \\(h_{\\boldsymbol{i}}(y)\\) with \\(1\\) in place of \\(y\\) and with the polynomial sequence \\(\\{{\\mathcal P}_{|\\boldsymbol{\\lambda}|}(y) p_{\\boldsymbol{\\lambda}}\\}\\) in place of the sequence \\(\\{g_{\\boldsymbol{\\lambda}}\\};\\)\nthe same for the polynomials \\(\\tilde{P}_{\\boldsymbol{k}}\\left( \\{y_{(s,t)}\\} \\right)\\) in \\(\\eqref{Pjm}\\) with \\((-1)^{j-1} (j-1)!\\) for \\(j=1,\\ldots,|\\boldsymbol{i}|\\) in place of the powers \\(y^j\\) and with the polynomial sequence \\(\\{y_{\\boldsymbol{\\lambda}}\\}\\) in place of the sequence \\(\\{g_{\\boldsymbol{\\lambda}}\\};\\)\nthe same for the polynomials \\({\\mathcal A}_{\\boldsymbol{w}} \\left( \\{y_{(s,t)}\\} \\right)\\) in \\(\\eqref{Ai}\\) with \\(1\\) in place of \\(y\\) and with the polynomial sequence \\(\\bigg\\{\\tilde{P}_{\\boldsymbol{\\lambda}}\\left( \\{y_{(s,t)}\\} \\right)p_{\\boldsymbol{\\lambda}}\\bigg\\}\\) in place of the sequence \\(\\{g_{\\boldsymbol{\\lambda}}\\}.\\)\nNote that when the multi-index \\(\\boldsymbol{i}\\) in \\(\\eqref{(redGC)}\\) reduces to a positive integer \\(i,\\) then the polynomial \\(h_{\\boldsymbol{i}}(y)\\) becomes\n\\[\\begin{equation}\nh_i(y) = \\sum_{\\lambda \\vdash i}  d_{\\lambda}  y^{l(\\lambda)} g_{\\lambda}\n\\label{(ffaal)}\n\\end{equation}\\]\nwhere the sum is over all the partitions \\(\\lambda=(1^{r_1}, 2^{r_2}, \\ldots) \\vdash i,\\) \\(d_{\\lambda}\\) is given in \\(\\eqref{dlambda}\\) and \\(g_{\\lambda}= g_1^{r_1} g_2^{r_2} \\ldots\\) with \\(\\{g_j\\}\\) a suitable sequence.\nExample 14: To get \\(h_{3}(y)\\) run\n> GCBellPol(c(3),1)\n[1] (y^3)g[1]^3 + 3(y^2)g[1]g[2] + (y)g[3]\nWith a combinatorial structure very similar to \\(\\eqref{(ffaal)}\\), the \\(i\\)-th general partition polynomial has the following expression in the indeterminates \\(y_1, \\ldots, y_i\\)\n\\[\\begin{equation}\nG_i( a_1, \\ldots, a_i; y_1, \\ldots, y_i) = \\sum_{\\lambda \\vdash i}  d_{\\lambda}  a_{l(\\lambda)} y_{\\lambda}\n\\label{(ffaa)}\n\\end{equation}\\]\nwhere the sum is over all the partitions \\(\\lambda=(1^{r_1}, 2^{r_2}, \\ldots) \\vdash i,\\) \\(d_{\\lambda}\\) is given in \\(\\eqref{dlambda},\\) \\(\\{a_j\\}\\) is a suitable numerical sequence and \\(y_{\\lambda} = y_1^{r_1} y_2^{r_2} \\ldots.\\) It’s a straightforward exercise to prove that\n\\[\\begin{equation}\nG_i( a_1, \\ldots, a_i; y_1, \\ldots, y_i) = \\sum_{j=1}^i a_j B_{i,j}(y_1, \\ldots, y_{i-j+1}),\n\\label{gpp}\n\\end{equation}\\]\nwhere \\(\\{B_{i,j}\\}\\) are the (partial) exponential Bell polynomials\n\\[\\begin{equation}\nB_{i,j}(y_1,  \\ldots, y_{i-j+1}) =  \\sum_{\\bar{p}(i,j)} d_{\\lambda} y_{\\lambda}\n\\label{(parexpBell)}\n\\end{equation}\\]\nwhere \\(\\bar{p}(i,j) = \\{\\lambda=(1^{r_1}, 2^{r_2}, \\ldots) \\vdash i | l(\\lambda)=j\\},\\) \\(d_{\\lambda}\\) is given in \\(\\eqref{dlambda}\\) and \\(y_{\\lambda} =y_1^{r_1} y_2^{r_2} \\cdots.\\) The polynomials in \\(\\eqref{(ffaa)}\\) are widely used in applications such as combinatorics, probability theory and statistics (Charalambides 2002). As particular cases, they include the exponential polynomials and their inverses, the logarithmic polynomials \\(\\eqref{Pj}\\), the potential polynomials and many others (Roman 1984). The general partition polynomials are the output of the gpPart function.\nExample 15: To get \\(G_4( a_1, a_2, a_3, a_4; y_1, y_2, y_3, y_4)\\) run\n> gpPart(4)\n[1] a4(y1^4) + 6a3(y1^2)(y2) + 3a2(y2^2) + 4a2(y1)(y3) + a1(y4)\nWhen \\(a_1 = \\ldots = a_i =1,\\) the \\(i\\)-th general partition polynomial in \\(\\eqref{gpp}\\) reduces to\nthe complete (exponential) Bell polynomial\n\\[\\begin{equation}\nG_i( 1, \\ldots, 1; y_1, \\ldots, y_i) =  \\sum_{j=1}^i B_{i,j}(y_1, \\ldots, y_{i-j+1})\n\\label{BC}\n\\end{equation}\\]\nwhere \\(\\{B_{i,j}\\}\\) are the (partial) exponential Bell polynomials \\(\\eqref{(parexpBell)}\\).\nFor instance, the polynomial \\(Q_i(y)\\) in \\(\\eqref{(Qi)}\\) is generated using the same pattern developed to generate \\(\\eqref{BC}\\) with \\({\\mathcal P}_{j}(y) p_{j}\\) in place of \\(y_j.\\)\nThe eBellPol function returns the complete (exponential) Bell polynomials \\(\\eqref{BC}\\). The same function also produces the (partial) exponential Bell polynomial \\(B_{i,j}(y_1, \\ldots, y_{i-j+1})\\) using \\(\\eqref{(ffaa)}\\) with \\(a_k=\\delta_{k,j}\\) (the Kronecker delta) for \\(k=1, \\ldots,i.\\) Mihoubi (2008) gives a rather extensive survey of applications of these homogeneous polynomials.\nExample 16: To get \\(B_{5,3}(y_1, y_2, y_{3})\\) run\n> eBellPol(5,3)\n[1] 15(y1)(y2^2) + 10(y1^2)(y3)\nTo get \\(G_4(1, 1, 1, 1; y_1, y_2, y_3, y_4)\\) run\n> eBellPol(4)\n[1] (y1^4) + 6(y1^2)(y2) + 3(y2^2) + 4(y1)(y3) + (y4)\nThe oBellPol function returns the partial (ordinary) Bell polynomials\n\\[\\hat{B}_{i,j}(y_1, \\ldots, y_{i-j+1}) = \\frac{j!}{i!} B_{i,j}(1! y_1, 2! y_2, \\ldots, (i-j+1)! y_{i-j+1})\\]\nand the complete (ordinary) Bell polynomials\n\\[\\hat{G}_i(y_1, \\ldots, y_i) =  G_i(1, \\ldots, 1; 1! y_1, 2! y_2, \\ldots, i! y_i).\\]\nExample 17: To get \\(\\hat{B}_{5,3}(y_1, y_2,y_3)\\) run\n> oBellPol(5,3)\n[1] 1/120( 360(y1)(y2^2) + 360(y1^2)(y3) )\nTo get \\(\\hat{G}_3(y_1, y_2, y_3, y_4)\\) run\n> oBellPol(4)\n[1] 1/24( 24(y1^4) + 72(y1^2)(y2) + 24(y2^2) + 48(y1)(y3) + 24(y4) )\nThe e_eBellPol function evaluates the exponential Bell polynomials when the indeterminates are substituted with numerical values. In Table 1 some special sequence of numbers are given obtained using this procedure.\n Table 1: Numerical sequences (second column) obtained evaluating the exponential Bell polynomials (last column) when suitable numerical values replace indeterminates.\n\\[{\\small \\begin{array}{c|c|c}\n                           \\hbox{ }              & \\hbox{Sequence}                 &  \\hbox{Bell polynomials} \\\\  \\hline\n\\hbox{Lah numbers}                             & \\frac{i!}{j!} \\begin{pmatrix}\ni-1 \\\\j-1\n\\end{pmatrix} & B_{i,j} (1!, 2!, 3!, \\ldots) \\\\ \\hline\n\\hbox{Stirling numbers of first kind}          & s(i,j)                          & B_{i,j} (0!, -1!, 2!, \\ldots) \\\\ \\hline  \n\\hbox{unsigned Stirling numbers of first kind} & |s(i,j)|                        & B_{i,j} (0!, 1!, 2!, \\ldots) \\\\ \\hline\n\\hbox{Stirling numbers of second kind}         & S(i,j)                          & B_{i,j} (1, 1, 1, \\ldots) \\\\ \\hline\n\\hbox{idempotent numbers}                      & \\begin{pmatrix}\ni \\\\j\n\\end{pmatrix} j^{i-j}           & B_{i,j} (1, 2, 3, \\ldots) \\\\ \\hline\n\\hbox{Bell numbers}                            & B_i                             & \\sum_{j=0}^i B_{i,j} (1, 1, 1, \\ldots)\n\\end{array}}\\]\nBy default, the e_eBellPol function returns the Stirling numbers of second kind, as the following example shows.\nExample 18: To get \\(S(5,3)\\) run\n> e_eBellPol(5,3)\n[1] 25\n> e_eBellPol(5,3,c(1,1,1,1,1))\n[1] 25\nTo get the \\(5\\)-th Bell number \\(B_5\\) run\n> e_eBellPol(5)\n[1] 52\nTo get \\(s(5,3)\\) run\n> e_eBellPol(5,3, c(1,-1,2,-6,24))\n[1] 35\n6 Composition of formal power series\nIn \\(\\eqref{mfaa1}\\), suppose \\(f_{\\boldsymbol{t}}\\) the \\(\\boldsymbol{t}\\)-th coefficient of \\(f(\\boldsymbol{x})\\) and \\(g_{1; \\boldsymbol{s}_1}, \\ldots, g_{n; \\boldsymbol{s}_n}\\) the \\(\\boldsymbol{s}_1\\)-th,…,\\(\\boldsymbol{s}_n\\)-th coefficients of \\(g_1(\\boldsymbol{z}), \\ldots, g_n(\\boldsymbol{z})\\) respectively. Using multi-index partitions, the multivariate Faà di Bruno’s formula \\(\\eqref{multfaa}\\) can be written as (Di Nardo et al. 2011)\n\\[\\begin{equation}\\label{multfaa2}\nh_{\\boldsymbol{i}} = \\boldsymbol{i}! \\sum_{{\\Lambda} \\vdash \\boldsymbol{s}_1, \\ldots, \\tilde{\\Lambda} \\vdash  \\boldsymbol{s}_n \\atop \\boldsymbol{s}_1 + \\cdots + \\boldsymbol{s}_n = \\boldsymbol{i}} f_{(l(\\Lambda),\\ldots,l(\\tilde{\\Lambda}))} \\frac{g_{1, \\Lambda} \\cdots g_{n, \\tilde{\\Lambda}}}{\\Lambda!\n\\cdots \\tilde{\\Lambda}! \\, \\mathfrak{m}(\\Lambda)! \\cdots \\mathfrak{m}(\\tilde{\\Lambda})!}\n\\end{equation}\\]\nwhere \\(g_{1,\\Lambda}, \\ldots, g_{n,\\tilde{\\Lambda}}\\) are given in \\(\\eqref{sequences}\\)\nand the sum is over all the partitions \\({\\Lambda} \\vdash \\boldsymbol{s}_1, \\ldots, \\tilde{\\Lambda} \\vdash \\boldsymbol{s}_n,\\) with \\(\\boldsymbol{s}_1,\\ldots,\\boldsymbol{s}_n\\) \\(m\\)-tuples of non-negative integers such that \\(\\boldsymbol{s}_1 + \\cdots + \\boldsymbol{s}_n = \\boldsymbol{i}.\\)\nThe MFB function generates all the summands of \\(\\eqref{multfaa2}\\). Its first step is to find the set \\(\\tilde{p}(n,\\boldsymbol{i})\\) of all the compositions of \\(\\boldsymbol{i}\\) in \\(n\\) parts, that is all the \\(n\\)-tuples\n\\((\\boldsymbol{s}_1,\\ldots,\\boldsymbol{s}_n)\\) of non-negative integer \\(m\\)-tuples such that \\(\\boldsymbol{s}_1 + \\cdots + \\boldsymbol{s}_n = \\boldsymbol{i}.\\) This task is performed by the mkT function.\n\n\nFunction mkT\n\ni) Find all the partitions \\(\\Lambda \\vdash \\boldsymbol{i},\\) using the mkmSet function.\n\n\nii) Select the first partition \\(\\Lambda.\\) If\n\\(l(\\Lambda) = n,\\) then the columns of \\(\\Lambda\\) are the \\(m\\)-tuples \\((\\boldsymbol{s}_1, \\ldots, \\boldsymbol{s}_n)\\) such that \\(\\boldsymbol{s}_1 + \\ldots + \\boldsymbol{s}_n = \\boldsymbol{i}.\\) If \\(l(\\Lambda) < n,\\) add \\(n-l(\\Lambda)\\) zero columns to \\(\\Lambda.\\)\n\n\niii) Generate all the permutations of the columns of \\(\\Lambda\\) as collected at step ii).\n\n\niv) Repeat steps ii) and iii) for each partition\n\\(\\Lambda\\) carried out at step i).\n\n\n\n\nIn the mkT function an input flag variable permits to obtain the output in a more compact set up. See the following example.\nExample 19: Suppose we are looking for the elements of the set \\(\\tilde{p}(2,(2,1)),\\)\nthat is the pairs \\((\\boldsymbol{s}_1, \\boldsymbol{s}_2)\\) such that \\(\\boldsymbol{s}_1 + \\boldsymbol{s}_2 = (2,1).\\) Then run\n> mkT(c(2,1),2,TRUE)\n[( 0 1 )( 2 0 )]\n[( 2 0 )( 0 1 )]\n[( 1 0 )( 1 1 )]\n[( 1 1 )( 1 0 )]\n[( 2 1 )( 0 0 )]\n[( 0 0 )( 2 1 )]\nConsider the partitions of \\((2,1)\\) as given in Example 2. Note that [( 2 1 )( 0 0 )] and [( 0 0 )( 2 1 )] are obtained adding a zero column to the partition [( 2 1 ),  1 ], and then permuting the two columns. No zero columns are added to [( 2 0 )( 0 1 )] as the length of the partition is \\(2.\\) The same is true for [( 0 1 )( 2 0 )] or [( 1 1 )( 1 0 )] which are only permuted.\nThe MFB function produces the multivariate Faà di Bruno’s formula \\(\\eqref{multfaa2}\\) making use of the following steps.\n\n\nFunction MFB\n\ni) Find all the \\(m\\)-tuples \\((\\boldsymbol{s}_1,\\ldots,\\boldsymbol{s}_n)\\) in \\(\\tilde{p}(n,\\boldsymbol{i})\\) using the mkT function.\n\n\nii) Let \\(y_1, \\ldots, y_n\\) be indeterminates. For each \\(j=1, \\ldots, n,\\) compute all the partitions \\(\\Lambda \\vdash \\boldsymbol{s}_j\\) using the mkmSet function and find the explicit expression of the polynomial\n\\[\nq_{j,\\boldsymbol{s}_j}(y_j) = \\boldsymbol{\\boldsymbol{s}_j}! \\sum_{{\\Lambda} \\vdash \\boldsymbol{s}_j} y_j^{l(\\Lambda)} \\frac{g_{j, \\Lambda}}{\\Lambda!\\mathfrak{m}(\\Lambda)!}.\n\\]\n\n\niii) Make the products \\(q_{1,\\boldsymbol{s}_1}(y_1) \\cdots q_{n,\\boldsymbol{s}_n}(y_n)\\) in the multivariable polynomial\n\\[{\\small h_{\\boldsymbol{i}}(y_1, \\ldots, y_n)=\\sum_{(\\boldsymbol{s}_1,\\ldots,\\boldsymbol{s}_n) \\in \\tilde{p}(n,\\boldsymbol{i})} {\\boldsymbol{i} \\choose \\boldsymbol{s}_1,\\ldots,\\boldsymbol{s}_n} q_{1,\\boldsymbol{s}_1}(y_1) \\cdots q_{n,\\boldsymbol{s}_n}(y_n)}\\]\nand compute its explicit expression.\n\n\niv) In the explicit expression of the polynomial \\(h_{\\boldsymbol{i}}(y_1, \\ldots, y_n)\\) as carried out at the previous step iii), replace the occurrences of the products \\(y_1^{l(\\Lambda)} \\cdots y_n^{l(\\tilde{\\Lambda})}\\) with \\(f_{(l(\\Lambda),\\ldots,l(\\tilde{\\Lambda}))}.\\)\n\n\n\n\nStep iii) is performed by the joint function. This function is not directly accessible in the package, as defined locally in the MFB function. The joint function realizes a recursive pair matching: each coefficient \\(g_{1, \\Lambda}\\) of \\(q_{1,\\boldsymbol{s}_1}(y_1)\\) is matched with each coefficient \\(g_{2, \\tilde{\\Lambda}}\\) of \\(q_{2,\\boldsymbol{s}_2}(y_2),\\) then each paired coefficient \\(g_{1, \\Lambda} g_{2, \\tilde{\\Lambda}}\\) is matched with each coefficient \\(g_{3, \\Lambda^{\\!*}}\\) of \\(q_{3,\\boldsymbol{s}_3}(y_3)\\) and so on. Step iv) consists of multiplying each coefficient found at step iii) with \\(f_{\\boldsymbol{t}},\\) where \\(\\boldsymbol{t}\\) is the multi-index whose \\(j\\)-th component gives how many times \\(g_{j, \\cdot}\\) appears in this coefficient. See the following example.\nExample 20: Suppose \\(n=m=2\\) and \\(\\boldsymbol{i}=(1,1).\\)\nTo get \\(h_{(1,1)}\\) in \\(\\eqref{multfaa2}\\) run\n> MFB(c(1,1),2)\n[1] f[1,1]g1[0,1]g2[1,0] + f[1,1]g1[1,0]g2[0,1] + f[2,0]g1[0,1]g1[1,0] + \nf[1,0]g1[1,1] + f[0,2]g2[0,1]g2[1,0] + f[0,1]g2[1,1]\nTaking into account \\(\\eqref{multps1}\\), in the previous output f[i,j] corresponds to \\(f_{(i,j)}\\) as well as g1[i,j] and g2[i,j] correspond to \\(g_{1;(i,j)}\\) and \\(g_{2;(i,j)}\\) respectively for \\(i,j=0,1,2.\\) Note that g1[1,1] is multiplied with f[1,0] as there is one occurrence of g1 and no occurrence of g2. In the same way, g1[1,0]g1[0,1] is multiplied with f[2,0] as there are two occurrences of g1 and no occurrence of g2 and g1[1,0]g2[0,1] is multiplied with f[1,1] as there is one occurrence of g1 and one occurrence of g2 and so on. Compare the previous output with the one obtained in Maple running\n\ndiff(f(g1(x1,x2),g2(x1,x2),x1,x2):\n\\[\n\\begin{array}{l}\nD_{{2,2}}(f)(g1(x1,x2), g2(x1,x2))  \\left(\\!{\\frac {\\partial }{\\partial {\\it x1}}}{\\it g2}(x1,x2)\\!\\right)\\!\\! \\left(\\!{\\frac {\\partial }{\\partial x2}} g2(x1,x2)\\!\\right) \\\\ + D_{{1,2}}(f)(g1(x1,x2),g2(x1,x2)) \\left(\\!{\\frac {\\partial }{\\partial x2}}g1(x1,x2)\\!\\right)\\!\\!\n\\left(\\!{\\frac {\\partial }{\\partial x1}}g2(x1,x2)\\!\\right) \\\\\n+ D_{1,2}(f)(g1(x1,x2),g2(x1,x2)) \\!\\!\n\\left(\\!{\\frac {\\partial }{\\partial x1}}g1(x1,x2))\\!\\right) \\!\\!\n\\left(\\!{\\frac {\\partial }{\\partial x2}}g2(x1,x2)\\!\\right) \\\\\n+ D_{{1,1}}(f)(g1(x1,x2), g2(x1,x2))\n\\left(\\! {\\frac {\\partial}{\\partial x1}}g1(x1,x2))\\!\\right)\\!\\!\n\\left(\\!{\\frac {\\partial }{\\partial x2}}g1(x1,x2))\\!\\right) \\\\\n+ D_{{2}}(f)(g1(x1,x2), g2(x1,x2))\n\\left({\\frac {\\partial^{2}}{\\partial x2 \\partial x1}}g2(x1,x2)\\right)\\\\\n+ D_{{1}}(f)(g1(x1,x2),g2(x1,x2)) \\left({\\frac {\\partial^{2}}{\\partial x2 \\partial x1}} g1(x1,x2)\\right)\n\\end{array}\\]\nwhere \\(D_{1}(f)\\) denotes \\(\\partial f(x_1,x_2)/\\partial x_1, D_{2}(f)\\) denotes\n\\(\\partial f(x_1,x_2)/\\partial x_2\\) and similarly\n\\[{\\small D_{1,1}(f) \\leftarrow  \\frac{\\partial^2 f(x_1,x_2)}{\\partial x_1^2}, \\, D_{2,2}(f) \\leftarrow  \\frac{\\partial^2 f(x_1,x_2)}{\\partial x_2^2}, \\, D_{1,2}(f) \\leftarrow \\frac{\\partial^2 f(x_1,x_2)}{\\partial x_1 \\partial x_2}}.\\]\n\nThe eMFB function evaluates the multivariate Faà di Bruno’s formula \\(\\eqref{multfaa2}\\) when the coefficients of the formal power series \\(f\\) and \\(g_1, \\ldots, g_n\\) in \\(\\eqref{multps1}\\) are substituted with numerical values.\nExample 21: To evaluate the output of Example 20 for some numerical values of the coefficients, run\n> cfVal<-\"f[0,1]=2, f[0,2]=5, f[1,0]=13, f[1,1]=-4, f[2,0]=0\"\n> cgVal<-\"g1[0,1]=-2.1, g1[1,0]=2,g1[1,1]=3.1,g2[0,1]=5,g2[1,0]=0,g2[1,1]=6.1\"\n> cVal<-paste0(cfVal,\",\",cgVal)\n> e_MFB(c(1,1),2,cVal)\n[1] 12.5\nThe polynomial families discussed in the previous sections are generated using the MFB function. Indeed, the generalized (complete exponential) Bell polynomials in \\(\\eqref{(sol22ter)}\\) are coefficients of the following formal power series\n\\[\\begin{equation}\n\\hskip-1cm{\\small H(y_1, \\ldots, y_n; \\boldsymbol{z}) = 1 + \\sum_{|\\boldsymbol{i}| > 0} h_{\\boldsymbol{i}}(y_1, \\ldots, y_n) \\frac{{\\boldsymbol{z}}^{\\boldsymbol{i}}}{\\boldsymbol{i}!} = \\exp \\bigg[ \\sum_{i=1}^n y_i (g_i(\\boldsymbol{z})-1) \\bigg]},\n\\label{(GCBellH)}\n\\end{equation}\\]\nwhich turns to be a composition \\(\\eqref{mfaa1}\\), with \\(f(x_1, \\ldots, x_n) =\\) \\(\\exp(x_1 y_1 + \\cdots + x_n y_n)\\) and \\(f_{\\boldsymbol{t}} = y_1^{t_1} \\cdots y_n^{t_n}\\) for \\(\\boldsymbol{t} \\in {\\mathbb N}_0^n.\\) In this case, \\(y_1, \\ldots, y_n\\) play the role of indeterminates. The \\(\\boldsymbol{i}\\)-th coefficient \\(h_{\\boldsymbol{i}}(y_1, \\ldots, y_n)\\) - output of the GCBellPol function - is obtained from the multivariate Faà di Bruno’s formula \\(\\eqref{multfaa2}\\) dealing with \\(y_1, \\ldots, y_n\\) as they were constants. When \\(\\{g_1(\\boldsymbol{z}), \\ldots, g_n(\\boldsymbol{z})\\}\\) are the same formal power series \\(g(\\boldsymbol{z}),\\) the formal power series \\(H(y_1, \\ldots, y_n;\\boldsymbol{z})\\) in \\(\\eqref{(GCBellH)}\\) reduces to\n\\[\\begin{equation}\n{\\small H(y_1, \\ldots, y_n; \\boldsymbol{z}) = \\exp \\big[ (y_1 + \\cdots + y_n) (g(\\boldsymbol{z})-1) \\big]}\n\\label{Hequal}\n\\end{equation}\\]\nwith coefficients \\(\\tilde{h}_{\\boldsymbol{i}}(y_1, \\ldots, y_n)\\) as given in the previous section.\nIf \\(n=1\\) then \\(H(y_1, \\ldots, y_n; \\boldsymbol{z})\\) reduces to the composition \\(\\exp \\big[ y (g(\\boldsymbol{z})-1)]\\) whose coefficients are the polynomials given in \\(\\eqref{(redGC)}\\). More in general the coefficients of \\(f(g(\\boldsymbol{z})-1)\\) are\n\\[\\begin{equation}\nh_{\\boldsymbol{i}} =  \\sum_{{\\Lambda} \\vdash  \\boldsymbol{i}} d_{\\Lambda}  f_{l(\\Lambda)}\ng_{\\Lambda}\n\\label{redGC1}\n\\end{equation}\\]\nwhere the sum is over all the partitions \\(\\Lambda=(\\boldsymbol{\\lambda}_1^{r_1} , \\boldsymbol{\\lambda}_2^{r_2}, \\ldots) \\vdash \\boldsymbol{i},\\) \\(d_{\\Lambda}\\) is given in \\(\\eqref{dlm}\\) and\n\\(g_{\\Lambda} = g_{\\boldsymbol{\\lambda}_1}^{r_1} g_{\\boldsymbol{\\lambda}_2}^{r_2} \\cdots.\\) If also \\(m=1,\\) then \\(h_{\\boldsymbol{i}}\\) in \\(\\eqref{redGC1}\\) reduces to\n\\[\\begin{equation}\nh_{i} =  \\sum_{{\\lambda} \\vdash  i} d_{\\lambda}  f_{l(\\lambda)}\ng_{\\lambda}\n\\label{redGC2}\n\\end{equation}\\]\nwhere the sum is over all the partitions \\(\\lambda=(1^{r_1} , 2^{r_2}, \\ldots) \\vdash i,\\) \\(d_{\\lambda}\\) is given in \\(\\eqref{dlambda}\\) and\n\\(g_{\\lambda} = g_1^{r_1} g_{2}^{r_2} \\cdots.\\) Formula \\(\\eqref{redGC2}\\) corresponds to the univariate Faà di Bruno’s formula and gives the \\(i\\)-th coefficient of\n\\(f(g(z)-1)\\) with\n\\[f(x)=1+\\sum_{j \\geq 1} f_j \\frac{x^j}{j!} \\qquad \\hbox{and} \\qquad g(z)=1+\\sum_{s \\geq 1} g_s \\frac{z^s}{s!}.\\]\nExample 22: To get \\(h_{5}\\) in \\(\\eqref{redGC2}\\) run\n> MFB(c(5), 1)\n[1] f[5]g[1]^5 + 10f[4]g[1]^3g[2] + 15f[3]g[1]g[2]^2 + 10f[3]g[1]^2g[3] + \n10f[2]g[2]g[3] + 5f[2]g[1]g[4] + f[1]g[5]\nFor instance, the \\(i\\)-th general partition polynomial in \\(\\eqref{(ffaa)}\\) is generated using the MFB function: in such a case the univariate Faà di Bruno’s formula \\(\\eqref{redGC2}\\) is generated with \\(\\{y_s\\}\\) in place of \\(\\{g_s\\}\\) and \\(\\{a_j\\}\\) in place of \\(\\{f_j\\}.\\)\nExamples of how to generate polynomials not included in the kStatistics package\nIn the following we give some suggestions on how to use the R codes of the kStatistics package to generate additional polynomial families.\nThe pPart function is an example of how to use the univariate Faà di Bruno’s formula and a symbolic strategy different from those presented so far. Indeed the pPart function generates the so-called partition polynomial \\(F_i(y)\\) of degree \\(i,\\) whose coefficients are the number of partitions of \\(i\\) with \\(j\\) parts for \\(j=1, \\ldots, i\\) (Boyer and Goh 2008). The partition polynomial \\(F_i(y)\\) is obtained from the univariate Faà di Bruno’s formula \\(\\eqref{redGC2}\\) setting\n\\[\\begin{equation}\nf_j = 1/i! \\qquad \\hbox{and} \\qquad g_{s}^{r_s} =(s!)^{r_s} r_s! y^{r_s}\n\\label{ppart}\n\\end{equation}\\]\nfor \\(s=1, \\ldots, i-j+1, j=1,\\ldots,i\\) and \\(r_s=1, \\ldots, i.\\) Note the symbolic substitution of \\(g_{s}^{r_s}\\) with the powers \\(y^{r_s}.\\)\nExample 23: To get \\(F_5(y)\\) run\n> pPart(5)\n[1] y^5 + y^4 + 2y^3 + 2y^2 + y\nNote that \\(F_5(y)\\) is obtained from the output of Example 22 using \\(\\eqref{ppart}.\\)\nExample 24: The following code shows how to evaluate \\(F_{11}(y)\\) in \\(y=7.\\)\n> s<-pPart(11)         # generate the partition polynomial of degree 11\n> s<-paste0(\"1\",s)     # add the coefficient to the first term  \n> s<-gsub(\" y\",\"1y\",s) # replace the variable y without coefficient  \n> s<-gsub(\"y\", \"*7\",s) # assign y = 7\n> eval(parse(text=s))  # evaluation of the expression\n[1] 3.476775e+182\nWe give a further example on how to generate a polynomial family not introduced so far but still coming from \\(\\eqref{redGC2}\\) for suitable choices of \\(\\{f_j\\}\\) and \\(\\{g_{s}\\}.\\) Consider the elementary symmetric polynomials in the indeterminates \\(y_1, \\ldots, y_n\\)\n\\[\\begin{eqnarray}  \ne_i(y_1, \\ldots, y_n) = \\left\\{ \\begin{array}{cl}\n\\displaystyle{\\sum_{1\\leq j_1 < \\cdots < j_i \\leq n}} y_{j_1} \\cdots y_{j_i}, & 1 \\leq i \\leq n, \\\\\n0,      &  i > n.\n\\end{array}\n\\right.\n\\end{eqnarray}\\]\nA well-known result (Charalambides 2002) states that these polynomials can be expressed in terms of the power sum symmetric polynomials \\(\\eqref{ps}\\) in the same indeterminates \\(y_1, \\ldots, y_n,\\) using the general partition polynomials \\(\\eqref{gpp}\\), that is\n\\[\\begin{equation}\ne_i = \\frac{(-1)^i}{i!} G_i (1, \\ldots, 1; - p_1, - 1! p_2, - 2! p_3, \\ldots, -(i-1)! p_i)\n\\label{elps1}\n\\end{equation}\\]\nfor \\(i=1, \\ldots, n.\\) The following e2p function expresses the \\(i\\)-th elementary symmetric polynomial \\(e_i\\) in terms of the power sum symmetric polynomials \\(p_1, \\ldots, p_i,\\) using \\(\\eqref{elps1}\\) and the MFB function.\n> e2p <-  function(n=0){\n+     v<-MFB(n,1);                      # Call the MFB Function\n+     v<-MFB2Set( v );                  # Expression to vector\n+     for (j in 1:length(v)) {\n+         # ----- read -----------[ fix block ]-----------------------#\n+         c <- as.character(v[[j]][2]);    # coefficient\n+         x <- v[[j]][3];                  # variable\n+         i <- v[[j]][4];                  # subscript\n+         k <- strtoi(v[[j]][5]);          # power\n+         # ----- change --------------------------------------------#\n+         if (x==\"f\") {\n+             c<-paste0(c,\"*( (-1)^\",n,\")\");\n+             x<-\"\";\n+             i<-\"\";\n+         }\n+         else if (x==\"g\") {\n+             c<-paste0(c,\"*((-factorial(\",strtoi(i)-1,\"))^\",k,\")\");\n+             x<-paste0(\"(p\",i,ifelse(k>1,paste0(\"^\",k),\"\"),\")\");\n+             i<-\"\";k<-1;\n+         }\n+         # ----- write ---------[ fix block ]-----------------------#\n+         v[[j]][2] <- c;\n+         v[[j]][3] <- x;\n+         v[[j]][4] <- i;\n+         v[[j]][5] <- k;\n+         # ---------------------------------------------------------#\n+     }\n+     noquote(paste0(\"1/\",factorial(n),\"( \",Set2expr(v), \" )\"));\n+ }\nThis function starts by initializing the vector v with \\(\\eqref{redGC2}\\) by means of the MFB function. There is a first code snippet [fix block] for extracting a set with the coefficients, variables, indexes and powers of v by means of the MFB2Set function. This first code snippet should not be changed whatever polynomial family we are generating. The second code snippet change includes instructions that can be changed according to the expressions of the coefficients \\(\\{f_j\\}\\) and \\(\\{g_{s}\\}\\) in \\(\\eqref{redGC2}\\). To get \\(\\eqref{elps1}\\), we set \\(f_j=1\\) and \\(g_{s} = - (s-1)! p_s.\\) Once these coefficients have been changed, the last code snippet [fix block] updates the vector v. The Set2expr function assembles the final expression.\nExample 25: To get \\(e_4\\) in \\(\\eqref{elps1}\\) run\n> e2p(4)\n[1] 1/24( (p1^4) - 6(p1^2)(p2) + 3(p2^2) + 8(p1)(p3) - 6(p4) )\n7 Concluding remarks\nWe have developed the kStatistics package with the aim to generate univariate and multivariate \\(k\\)-statistics/polykays, togheter with the multivariate Faà di Bruno’s formula and various user-friendly functions related to this formula. The paper briefly introduces the combinatorial tools involved in the package and presents, in detail, the core function of the package which generates multi-index partitions. We emphasize that the algorithms presented here have been designed with the aid of the umbral calculus, even if we did not mentioned this method in the paper.\nOne of the main applications we have dealt with is the generation and evaluation of various families of polynomials: from generalized complete Bell polynomials to general partition polynomials, from partial Bell polynomials to complete Bell polynomials. Numerical sequences obtained from the Bell polynomials can also be generated.\nAll these utilities intend to make the kStatistics package a useful tool not only for statisticians but also for users who need to work with families of polynomials usually available in symbolic software or tables. Indeed, we have provided examples on how to generate polynomial families not included in the package but which can still be recovered using the Faà di Bruno’s formula and suitable strategies, both numerical and symbolic. Following this approach, also the estimations of joint cumulants or products of joint cumulants is one further example of symbolic strategy coming from the multivariate Faà di Bruno’s formula.\nFuture works consist in expanding the kStatistics package by including extensions of the multivariate Faà di Bruno’s formula, as addressed in Bernardini et al. (2005) and references therein, aiming to manage nested compositions, as the BellY function in the Wolfram Language and System does. Moreover, further procedures can be inserted relied on symbolic strategies not apparently related to the multivariate Faà di Bruno’s formula but referable to this formula, as for example the central Bell polynomials (Kim et al. 2019).\nThe results in this paper were obtained using the kStatistics \\(2.1.1\\) package. The package is currently available with a general public license (GPL) from the Comprehensive R Archive Network.\n8 Acknowledgements\nThe authors would like to thank the reviewers for their constructive feedback.\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-033.zip\nCRAN packages used\nkStatistics, partitions, nilde\nCRAN Task Views implied by cited packages\nNumericalMathematics, Optimization\n\n\nN. P. Arnqvist, V. Voinov, R. Makarov and Y. Voinov. Nilde: Nonnegative integer solutions of linear diophantine equations with applications. 2021. URL https://CRAN.R-project.org/package=nilde. R package version 1.1-6.\n\n\nA. Bernardini, P. Natalini and P. E. Ricci. Multidimensional Bell polynomials of higher order. Comput. Math. Appl., 50(10-12): 1697–1708, 2005. URL https://doi.org/10.1016/j.camwa.2005.05.008.\n\n\nR. P. Boyer and W. M. Y. Goh. Partition polynomials: Asymptotics and zeros. In Tapas in experimental mathematics, pages. 99–111 2008. Amer. Math. Soc., Providence, RI. URL https://doi.org/10.1090/conm/457/08904.\n\n\nJ. W. Brown. On multivariable Sheffer sequences. J. Math. Anal. Appl., 69(2): 398–410, 1979. URL https://doi.org/10.1016/0022-247X(79)90151-3.\n\n\nJ. E. Chacón and T. Duong. Efficient recursive algorithms for functionals based on higher order derivatives of the multivariate Gaussian density. Statistics and Computing, 25(5): 959–974, 2015. URL https://doi.org/10.1007/s11222-014-9465-1.\n\n\nC. A. Charalambides. Enumerative combinatorics. Chapman & Hall/CRC, Boca Raton, FL, 2002.\n\n\nA. Clausen and S. Sokol. Deriv: R-based symbolic differentiation. 2020. URL https://CRAN.R-project.org/package=Deriv. Deriv package version 4.1.\n\n\nG. M. Constantine and T. H. Savits. A multivariate Faà di Bruno formula with applications. Transactions of the American Mathematical Society, 348(2): 503–520, 1996. URL https://doi.org/10.1090/S0002-9947-96-01501-2.\n\n\nDe Leeuw, J. Multivariate Cumulates in R. 2012. URL https://escholarship.org/uc/item/1fw1h53c.\n\n\nE. Di Nardo. On multivariable cumulant polynomial sequences with applications. Journal of Algebraic Statistics, 7(1): 72–89, 2016a. URL https://doi.org/10.18409/jas.v7i1.49.\n\n\nE. Di Nardo. On photon statistics parametrized by a non-central Wishart random matrix. Journal of Statistical Planning and Inference, 169: 1–12, 2016b. URL https://doi.org/10.1016/j.jspi.2015.07.002.\n\n\nE. Di Nardo. Symbolic calculus in mathematical statistics: A review. Séminaire Lotharingien de Combinatoire, 67: Art. B67a, 72, 2011. URL https://www.mat.univie.ac.at/~slc/wpapers/s67dinardo.pdf.\n\n\nE. Di Nardo and G. Guarino. kStatistics: Unbiased estimators for cumulant products and faa di bruno’s formula. 2021. URL https://CRAN.R-project.org/package=kStatistics. R package version 2.1.\n\n\nE. Di Nardo and G. Guarino. Unbiased estimators for cumulant products. 2019. URL https://cran.r-project.org/web/packages/kStatistics/index.html. R package version 1.0.\n\n\nE. Di Nardo, G. Guarino and D. Senato. A new algorithm for computing the multivariate Faà di Bruno’s formula. 217(13): 6286–6295, 2011. URL https://doi.org/10.1016/j.amc.2011.01.001.\n\n\nE. Di Nardo, M. Marena and P. Semeraro. On non-linear dependence of multivariate subordinated Lévy processes. Statistics & Probability Letters, 166: 108870–108877, 2020. URL https://doi.org/10.1016/j.spl.2020.108870.\n\n\nE. Di Nardo and I. Oliva. On a symbolic version of multivariate lévy processes. American Institute of Physics Conference Proceedings, 1389(1): 345–348, 2011. URL https://doi.org/10.1063/1.3636735.\n\n\nR. Dimitrakopoulos, H. Mustapha and E. Gloaguen. High-order statistics of spatial random fields: Exploring spatial cumulants for modeling complex non-Gaussian and non-linear phenomena. Mathematical Geosciences, 42(1): 65–99, 2010. URL https://doi.org/10.1007/s11004-009-9258-9.\n\n\nK. Domino, P. Gawron and Ł. Pawela. Efficient computation of higher-order cumulant tensors. SIAM J. Sci. Comput., 40(3): A1590–A1610, 2018. URL https://doi.org/10.1137/17M1149365.\n\n\nP. G. Ferreira, J. Magueijo and J. Silk. Cumulants as non-gaussian qualifiers. Physical Review D, 56(8): 4592, 1997. URL https://doi.org/10.1103/PhysRevD.56.4592.\n\n\nM. Geng, H. Liang and J. Wang. Research on methods of higher-order statistics for phase difference detection and frequency estimation. In 2011 4th international congress on image and signal processing, pages. 2189–2193 2011. URL https://doi.org/10.1109/CISP.2011.6100593.\n\n\nG. B. Giannakis. Cumulants: A powerful tool in signal processing. Proceedings of the IEEE, 75(9): 1333–1334, 1987. URL https://doi.org/10.1109/PROC.1987.13884.\n\n\nG. Guarino, D. Senato and E. Di Nardo. Fast maple algorithms for \\(k\\)-statistics, polykays and their multivariate generalization. 2009. URL https://www.maplesoft.com/applications/view.aspx?SID=33041.\n\n\nR. K. S. Hankin. Additive integer partitions in r. Journal of Statistical Software, Code Snippets, 16: 2006.\n\n\nM. Hardy. Combinatorics of partial derivatives. Electronic Journal of Combinatorics, 13(1): Research Paper 1, 13, 2006. URL http://www.combinatorics.org/Volume_13/Abstracts/v13i1r1.html.\n\n\nL. Hernández Encinas and J. Muñoz Masqué. A short proof of the generalized Faà di Bruno’s formula. Applied Mathematics Letters. An International Journal of Rapid Publication, 16(6): 975–979, 2003. URL https://doi.org/10.1016/S0893-9659(03)90026-7.\n\n\nS. R. Jammalamadaka, T. S. Rao and G. Terdik. Higher order cumulants of random vectors and applications to statistical inference and time series. Sankhyā. The Indian Journal of Statistics, 68(2): 326–356, 2006. URL https://www.jstor.org/stable/25053499.\n\n\nD. N. Joanes and C. A. Gill. Comparing measures of sample skewness and kurtosis. Journal of the Royal Statistical Society: Series D (The Statistician), 47(1): 183–189, 1998.\n\n\nT. Kim, D. S. Kim and G.-W. Jang. On central complete and incomplete bell polynomials i. Symmetry, 11(2): 2019. URL https://www.mdpi.com/2073-8994/11/2/288.\n\n\nR. B. Leipnik and C. E. M. Pearce. The multivariate Faà di Bruno formula and multivariate Taylor expansions with explicit integral remainder term. The ANZIAM Journal. The Australian & New Zealand Industrial and Applied Mathematics Journal, 48(3): 327–341, 2007. URL https://doi.org/10.1017/S1446181100003527.\n\n\nT.-W. Ma. Higher chain formula proved by combinatorics. Electronic Journal of Combinatorics, 16(1): N21, 7, 2009. URL https://doi.org/10.37236/259.\n\n\nP. McCullagh. Tensor methods in statistics. Chapman & Hall, London, 1987.\n\n\nM. Mihoubi. Polynômes multivariés de bell et polynômes de type binomial. 2008.\n\n\nR. L. Mishkov. Generalization of the formula of Faà di Bruno for a composite function with a vector argument. International Journal of Mathematics and Mathematical Sciences, 24(7): 481–491, 2000. URL https://doi.org/10.1155/S0161171200002970.\n\n\nJ. Y. Nguwi, G. Penent and N. Privault. A deep branching solver for fully nonlinear partial differential equations. 2022. URL https://arxiv.org/abs/2203.03234.\n\n\nH. Oualla, R. Fateh, A. Darif, S. Safi, M. Pouliquen and M. Frikel. Channel identification based on cumulants, binary measurements, and kernels. Systems, 9(2): 2021. URL https://doi.org/10.3390/systems9020046.\n\n\nG. Peccati and M. S. Taqqu. Combinatorial expressions of cumulants and moments. In Wiener chaos: Moments, cumulants and diagrams., Ed M. Springer pages. 201–213 2011. Bocconi & Springer Series.\n\n\nN. Privault. Recursive computation of the hawkes cumulants. Statistics and Probability Letters, 109161, 2021. URL https://doi.org/10.1016/j.spl.2021.109161.\n\n\nT. S. Rao and W. K. Wong. Some contributions to multivariate nonlinear time series and to bilinear models. In Asymptotics, nonparametrics, and time series, pages. 259–294 1999. Dekker, New York.\n\n\nD. L. Reiner. Multivariate sequences of binomial type. Studies in Applied Mathematics, 57(2): 119–133, 1976. URL https://doi.org/10.1002/sapm1977572119.\n\n\nD. S. Robson. Applications of multivariate polykays to the theory of unbiased ratio-type estimation. Journal of the American Statistical Association, 52(280): 511–522, 1957. URL https://www.tandfonline.com/doi/abs/10.1080/01621459.1957.10501407.\n\n\nS. Roman. The umbral calculus. Academic Press, Inc. [Harcourt Brace Jovanovich, Publishers], New York, 1984.\n\n\nC. Rose and M. D. Smith. Mathematical statistics with Mathematica\\(^\\circledR\\). Springer-Verlag, New York, 2002. URL https://doi.org/10.1007/978-1-4612-2072-5.\n\n\nT. H. Savits. Some statistical applications of faà di Bruno. Journal of Multivariate Analysis, 97(10): 2131–2140, 2006. URL https://doi.org/10.1016/j.jmva.2006.03.001.\n\n\nA. B. Shabat and M. Kh. Efendiev. On applications of Faà-di-Bruno formula. Ufa Mathematical Journal, 9(3): 131–136, 2017. URL https://doi.org/10.1007/s12572-017-0181-x.\n\n\nH. Shrivastava. Multiindex multivariable Hermite polynomials. Math. Comput. Appl., 7(2): 139–149, 2002. URL https://doi.org/10.3390/mca7020139.\n\n\nK. D. Smith. A tutorial on multivariate \\(k\\)-statistics and their computation. 2020. URL https://arxiv.org/abs/2005.08373.\n\n\nK. D. Smith, S. Jafarpour, A. Swami and F. Bullo. Topology inference with multivariate cumulants: The möbius inference algorithm. IEEE/ACM Transactions on Networking, 1–15, 2022. URL https://doi.org/10.1109/TNET.2022.3164336.\n\n\nB. Staude, S. Rotter and S. Grün. CuBIC: Cumulant based inference of higher-order correlations in massively parallel spike trains. Journal of Computational Neuroscience, 29(1-2): 327–350, 2010. URL https://doi.org/10.1007/s10827-009-0195-x.\n\n\nA. Stuart and J. K. Ord. Kendall’s advanced theory of statistics. Vol. 1. Sixth Edward Arnold, London; copublished in the Americas by Halsted Press [John Wiley & Sons, Inc.], New York, 1994. Distribution theory.\n\n\nV. Voinov and N. Pya Arnqvist. R-software for additive partitioning of positive integers. Mathematical Journal, 17(1): 69–76, 2017. URL http://www.math.kz/media/journal/journal2018-05-1574083.pdf.\n\n\nC. S. Withers and S. Nadarajah. Multivariate Bell polynomials. International Journal of Computer Mathematics, 87(11): 2607–2611, 2010. URL https://doi.org/10.1080/00207160802702418.\n\n\nIf \\(\\boldsymbol{i}\\in {\\mathbb N}_0^m\\) is a multi-index then we set \\(\\boldsymbol{i}! = i_1! \\cdots i_m!\\) and \\(|\\boldsymbol{i}|=i_1 + \\cdots + i_m.\\)↩︎\nWe use these notations independently if the powers or the subscripts are row vectors or column vectors.↩︎\nIf \\(\\boldsymbol{\\mu}, \\boldsymbol{\\nu} \\in {\\mathbb N}_0^m\\) we have \\(\\boldsymbol{\\mu} \\prec \\boldsymbol{\\nu}\\) if \\(|\\boldsymbol{\\mu}| < |\\boldsymbol{\\nu}|\\) or \\(|\\boldsymbol{\\mu}| = |\\boldsymbol{\\nu}|\\) and \\(\\mu_1 < \\nu_1\\) or \\(|\\boldsymbol{\\mu}| = |\\boldsymbol{\\nu}|\\) and \\(\\mu_1 = \\nu_1, \\ldots, \\mu_k = \\nu_k, \\mu_{k+1} < \\nu_{k+1}\\) for some \\(1 \\leq k < m.\\)↩︎\nAs example \\((a_1,b_1) < (a_2,b_2)\\) if \\(a_1 < a_2\\) or \\(a_1=a_2\\) and \\(b_1<b_2.\\)\n\n↩︎\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-037/",
    "title": "PDFEstimator: An R Package for Density Estimation and Analysis",
    "description": "This article presents PDFEstimator, an R package for nonparametric probability density estimation and analysis, as both a practical enhancement and alternative to kernel-based estimators. PDFEstimator creates fast, highly accurate, data-driven probability density estimates for continuous random data through an intuitive interface. Excellent results are obtained for a diverse set of data distributions ranging from 10 to $10^6$ samples when invoked with default parameter definitions in the absence of user directives. Additionally, the package contains methods for assessing the quality of any estimate, including robust plotting functions for detailed visualization and trouble-shooting. Usage of PDFEstimator is illustrated through a variety of examples, including comparisons to several kernel density methods.",
    "author": [
      {
        "name": "Jenny Farmer",
        "url": {}
      },
      {
        "name": "Donald Jacobs",
        "url": {}
      }
    ],
    "date": "2022-10-11",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-037.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-038/",
    "title": "reclin2: a Toolkit for Record Linkage and Deduplication",
    "description": "The goal of record linkage and deduplication is to detect which records belong to the same object in data sets where the identifiers of the objects contain errors and missing values. The main design considerations of reclin2 are: modularity/flexibility, speed and the ability to handle large data sets. The first points makes it easy for users to extend the package with custom process steps. This flexibility is obtained by using simple data structures and by following as close as possible common interfaces in R. For large problems it is possible to distribute the work over multiple worker nodes. A benchmark comparison to other record linkage packages for R, shows that for this specific benchmark, the fastLink package performs best. However, this package only performs one specific type of record linkage model. The performance of reclin2 is not far behind the of fastLink while allowing for much greater flexibility.",
    "author": [
      {
        "name": "D. Jan van der Laan",
        "url": {}
      }
    ],
    "date": "2022-10-11",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-038.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-039/",
    "title": "The Concordance Test, an Alternative to Kruskal-Wallis Based on the Kendall-$\\tau$ Distance: An R Package",
    "description": "The Kendall rank correlation coefficient, based on the Kendall-$\\tau$ distance, is used to measure the ordinal association between two measurements. In this paper, we introduce a new coefficient also based on the Kendall-$\\tau$ distance, the Concordance coefficient, and a test to measure whether different samples come from the same distribution. This work also presents a new R package, ConcordanceTest, with the implementation of the proposed coefficient. We illustrate the use of the Concordance coefficient to measure the ordinal association between quantity and quality measures when two or more samples are considered. In this sense, the Concordance coefficient can be seen as a generalization of the Kendall rank correlation coefficient and an alternative to the non-parametric mean rank-based methods for comparing two or more samples. A comparison of the proposed Concordance coefficient and the classical Kruskal-Wallis statistic is presented through a comparison of the exact distributions of both statistics.",
    "author": [
      {
        "name": "Javier Alcaraz",
        "url": {}
      },
      {
        "name": "Laura Anton-Sanchez",
        "url": {}
      },
      {
        "name": "Juan Francisco Monge",
        "url": {}
      }
    ],
    "date": "2022-10-11",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-039.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-040/",
    "title": "R-miss-tastic: a unified platform for missing values methods and workflows",
    "description": "Missing values are unavoidable when working with data. Their occurrence is exacerbated as more data from different sources become available. However, most statistical models and visualization methods require complete data, and improper handling of missing data results in information loss or biased analyses. Since the seminal work of Rubin (1976), a burgeoning literature on missing values has arisen, with heterogeneous aims and motivations. This led to the development of various methods, formalizations, and tools. For practitioners, however, it remains a challenge to decide which method is most appropriate for their problem, in part because this topic is not systematically covered in statistics or data science curricula. To help address this challenge, we have launched the `R-miss-tastic` platform, which aims to provide an overview of standard missing values problems, methods, and relevant implementations of methodologies. Beyond gathering and organizing a large majority of the material on missing data (bibliography, courses, tutorials, implementations), `R-miss-tastic` covers the development of standardized analysis workﬂows. Indeed, we have developed several pipelines in R and Python to allow for hands-on illustration of and recommendations on missing values handling in various statistical tasks such as matrix completion, estimation, and prediction, while ensuring reproducibility of the analyses. Finally, the platform is dedicated to users who analyze incomplete data, researchers who want to compare their methods and search for an up-to-date bibliography, and teachers who are looking for didactic materials (notebooks, recordings, lecture notes).",
    "author": [
      {
        "name": "Imke Mayer",
        "url": {}
      },
      {
        "name": "Aude Sportisse",
        "url": {}
      },
      {
        "name": "Julie Josse",
        "url": {}
      },
      {
        "name": "Nicholas Tierney",
        "url": {}
      },
      {
        "name": "Nathalie Vialaneix",
        "url": {}
      }
    ],
    "date": "2022-10-11",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:37+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-022/",
    "title": "Power and Sample Size for Longitudinal Models in R -- The longpower Package and Shiny App",
    "description": "Longitudinal studies are ubiquitous in medical and clinical research. Sample size computations are critical to ensure that these studies are sufficiently powered to provide reliable and valid inferences. There are several methodologies for calculating sample sizes for longitudinal studies that depend on many considerations including the study design features, outcome type and distribution, and proposed analytical methods. We briefly review the literature and describe sample size formulas for continuous longitudinal data. We then apply the methods using example studies comparing treatment versus control groups in randomized trials assessing treatment effect on clinical outcomes. We also introduce a Shiny app that we developed to assist researchers with obtaining required sample sizes for longitudinal studies by allowing users to enter required pilot estimates. For Alzheimer's studies, the app can estimate required pilot parameters using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Illustrative examples are used to demonstrate how the package and app can be used to generate sample size and power curves. The package and app are designed to help researchers easily assess the operating characteristics of study designs for Alzheimer's clinical trials and other research studies with longitudinal continuous outcomes. Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu).",
    "author": [
      {
        "name": "Samuel Iddi",
        "url": {}
      },
      {
        "name": "Michael C Donohue",
        "url": {}
      }
    ],
    "date": "2022-07-04",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-022.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-019/",
    "title": "fairmodels: a Flexible Tool for Bias Detection, Visualization, and Mitigation in Binary Classification Models",
    "description": "Machine learning decision systems are becoming omnipresent in our lives. From dating apps to rating loan seekers, algorithms affect both our well-being and future. Typically, however, these systems are not infallible. Moreover, complex predictive models are eager to learn social biases present in historical data that may increase discrimination. If we want to create models responsibly, we need tools for in-depth validation of models also from potential discrimination. This article introduces an R package fairmodels that helps to validate fairness and eliminate bias in binary classification models quickly and flexibly. The fairmodels package offers a model-agnostic approach to bias detection, visualization, and mitigation. The implemented functions and fairness metrics enable model fairness validation from different perspectives. In addition, the package includes a series of methods for bias mitigation that aim to diminish the discrimination in the model.  The package is designed to examine a single model and facilitate comparisons between multiple models.",
    "author": [
      {
        "name": "Jakub Wiśniewski",
        "url": {}
      },
      {
        "name": "Przemysław Biecek",
        "url": "https://pbiecek.github.io/"
      }
    ],
    "date": "2022-06-27",
    "categories": [],
    "contents": "\nIntroduction\nResponsible machine learning and, in particular, fairness is gaining attention within the machine learning community. This is because predictive algorithms are becoming more and more decisive and influential in our lives. This impact could be less or more significant in areas ranging from user feeds on social platforms, displayed ads, and recommendations at an online store to loan decisions, social scoring, and facial recognition systems used by police and authorities. Sometimes it leads to automated systems that learn some undesired bias preserved in data for some historical reason. Whether seeking a job (Lahoti et al. 2019) or having one’s data processed by court systems (Angwin et al. 2016), sensitive attributes such as sex, race, religion, ethnicity, etc., might play a significant role in the decision. Even if such variables are not directly included in the model, they are often captured by proxy variables such as zip code (a proxy for the race and wealth), purchased products (a proxy for gender and age), eye colour (a proxy for ethnicity). As one would expect, they can give an unfair advantage to a privileged group. Discrimination takes the form of more favorable predictions or higher accuracy for a privileged group. For example, some popular commercial gender classifiers were found to perform the worst on darker females (Buolamwini and Gebru 2018). From now on, such unfair and harmful decisions towards people with specific sensitive attributes will be called biased.\nThe list of protected attributes may depend on the region and domain for which the model is built. For example, the European Union law is summarized in the Handbook on European non-discrimination law European Union Agency for Fundamental Rights and Council of Europe (2018), which lists the following protected attributes that cannot be the basis for inferior treatment: sex, gender identity, sexual orientation, disability, age, race, ethnicity, nationality or national origin, religion or belief, social origin, birth, and property, language, political or other opinions. This list, though long, does not include all potentially relevant items, e.g. in the USA, a protected attribute is also pregnancy, the status of a war veteran, or genetic information.\nWhile there are historical and economic reasons for this to happen, such decisions are unacceptable in society, where nobody should have an unfair advantage.\nThe problem is not simple, especially when the only criterion set for the system is performance. We observe a trade-off between accuracy and fairness in some cases where lower discrimination leads to lower performance (Kamiran and Calders 2011). Sometimes labels, which are considered ground truth, might also be biased (Wick et al. 2019), and when controlling for that bias, the performance and fairness might improve simultaneously. However fairness is not a concept that a single number can summarize, so most of the time, when we want to improve fairness from one perspective, it becomes worse in another (Barocas et al. 2019).\nThe bias in machine learning systems has potentially many different sources. Mehrabi et al. (2019) categorized bias into its types like historical bias, where unfairness is already embedded into the data reflecting the world, observer bias, sampling bias, ranking and social biases, and many more. That shows how many dangers are potentially hidden in the data itself. Whether one would like to act on it or not, it is essential to detect bias and make well-informed decisions whose consequences could potentially harm many groups of people. Repercussions of such systems can be unpredictable. As argued by Barocas et al. (2019), machine learning systems can even aggravate the disparities between groups, which is called by the authors’ feedback loops. Sometimes the risk of potential harm resulting from the usage of such systems is high. This was noticed, for example, by the Council of Europe that wrote the set of guidelines where it states that the usage of facial recognition for the sake of determining a person’s sex, age, origin, or even emotions should be mostly prohibited (Council of Europe 2021).\nNot every difference in treatment is discrimination. Cirillo et al. (2020) presents examples of desirable and undesirable biases based on the medical domain. For example, in the case of cardiovascular diseases, documented medical knowledge indicates that different treatments are more effective for different genders. So different treatment regimens according to medical knowledge are examples of desirable bias. Later in this paper, we present tools to identify differences between groups defined by some protected attribute but note that this does not automatically mean that there is discrimination.\nWe would also like to point out that focusing on the machine learning model may not be enough in some cases, and sometimes the design of the data acquisition and/or annotation cause the model to be biased (Barocas et al. 2019).\nRelated work\nAssembling predictive models is getting easier nowadays. Packages like h2o (H2O.ai 2017) provide AutoML frameworks where non-experts can train quickly accurate models without deep domain knowledge. Model validation should also be that simple. Yet this is not the case. There are still very few tools to support the fairness diagnostics of the model.\nTwo main kinds of fairness are a concern to multiple stakeholders. These are group and individual fairness. The first one concerns groups of people with the same protected attributes (gender, race, etc.). It focuses on measuring if these groups are treated similarly by the model. The second one is focused on the individual. It is most intuitively defined as treating similar individuals similarly (Dwork et al. 2012). Both concepts are sometimes considered to conflict with each other, but they don’t need to be if we factor in certain assumptions, such as whether the disparities are due to personal choices or unjust structures (Binns 2020).\nSeveral frameworks have emerged for Python to verify various fairness criteria, the most popular are aif360 (Bellamy et al. 2018), fairlearn (Bird et al. 2020), or aequitas (Saleiro et al. 2018). They have various features for detecting, visualization, and mitigating bias in machine learning models.\nFor the R language, until recently, the only available tool was the fairness (Kozodoi and V. Varga 2021) package which compares various fairness metrics for specified subgroups. The fairness package is very helpful, but it lacks some features. For example, it does not allow comparing the machine learning models and aggregating fairness metrics to facilitate the visualization. Still, most of all, it does not give a quick verdict on whether a model is fair or not. Package fairadapt aims at removing bias from machine learning models by implementing pre-processing procedure described in Plečko and Meinshausen (2019). Our package tries to combine the detection and mitigation processes. It encourages the user to experiment with the bias, try different mitigation methods and compare results. The package fairmodels not only allows for that comparison between models and multiple exposed groups of people, but it gives direct feedback if the model is fair or not (more on that in the next section). Our package also equips the user with a so-called fairness_object, an object aggregating possibly many models, information about data, and fairness metrics. fairness_object can later be transformed into many other objects that can facilitate the visualization of metrics and models from different perspectives. If a model does not meet fairness criteria, various pre-processing and post-processing bias mitigation algorithms are implemented and ready to use. It aims to be a complete tool for dealing with discriminatory models in a group fairness setting.\nIn particular, in the following sections, we show how to use this package to address four key questions: How to measure bias? How to detect bias? How to visualize bias? and How to mitigate bias?\nIt is important to remember that fairness is not a binary concept that can be unambiguously defined, and there is no silver bullet that will make any model fair. The presented tools allow for fairness exploratory analysis, thanks to which we will be able to detect differences in the behavior of the model for different protected groups. But such analysis will not guarantee that all possible fairness problems have been detected. Also, fairness analysis is only one of a wide range of techniques for Explanatory Model Analysis (Biecek and Burzykowski 2021).\nLike other explanatory tools, it should be used with caution and awareness.\nMeasuring and detecting bias\nIn model fairness analysis, a distinction is often made between group fairness and individual fairness analysis. The former is defined by the equality of certain statistics determined on protected subgroups, and we focus on this approach in this section. We write more about the latter later in this paper.\nFairness metrics\nMachine learning models, just like human-based decisions, can be biased against observations related to people with certain sensitive attributes, which are also called protected groups. This is because they consist of subgroups - people who share the same sensitive attribute, like gender, race, or other features.\nTo address this problem, we need first to introduce fairness criteria. Following Barocas et al. (2019), we will present these criteria based on the following notation.\nLet \\(A \\in \\{a,b, ...\\}\\) mean protected group and values \\(A \\neq a\\) denote membership to unprivileged subgroups while \\(A = a\\) membership to privileged subgroup. To simplify the notation, we will treat this as a binary variable (so \\(A = b\\) will denote membership to unprivileged subgroup), but all results hold if \\(A\\) has a larger number of groups.\nLet \\(Y \\in \\{0,1\\}\\) be a binary label (binary target = binary classification) where \\(1\\) is preferred, favorable outcome.\nLet \\(R \\in [0,1]\\) be a probabilistic response of the model, and \\(\\hat{Y} \\in \\{0,1\\}\\) is the binarised model response, so \\(\\hat{Y} = 1\\) when \\(R \\geq 0.5\\), otherwise \\(\\hat{Y} = 0\\).\nFigure 1 summarizes possible situations for the subgroup \\(A=a\\). We can draw up the same table for each of the subgroups.\n\n\n\nFigure 1: Summary of possible model outcomes for subpopulation \\(A = a\\). We assume that outcome \\(Y = 1\\) is favourable.\n\n\n\nAccording to Barocas et al. (2019) most discrimination criteria can be derived as tests that validate the following probabilistic definitions:\nIndependence, i.e. \\(R \\perp A\\),\nSeparation, i.e. \\(R \\perp A \\mid Y\\),\nSufficiency, i.e. \\(Y \\perp A \\mid R\\).\nThose criteria and their relaxations might be expressed via different metrics based on a confusion matrix for a certain subgroup. To check if those fairness criteria are addressed, we propose checking five metrics among privileged group (a) and unprivileged group (b):\nStatistical parity: \\(P(\\hat{Y} = 1 | A = a) = P(\\hat{Y} = 1 | A = b)\\). Statistical parity (STP) ensures that fractions of assigned positive labels are the same in subgroups. It is equivalent of Independence (Dwork et al. 2012). In other words, the values in the last column of Figure 1 are the same for each subgroup.\nEqual opportunity: \\(P(\\hat{Y} = 1 | A = a, Y = 1) = P(\\hat{Y} = 1 | A = b, Y = 1)\\). Checks if classifier has equal True Positive Rate (TPR) for each subgroup. In other words, the column normalized values in the second column of Figure 1 are the same for each subgroup. It is a relaxation of Separation (Hardt et al. 2016).\nPredictive parity: \\(P(Y = 1 | A = a, \\hat{Y} = 1) = P(Y = 1 | A = b, \\hat{Y} = 1)\\). Measures if a model has equal Positive Predictive Value (PPV) for each subgroup. In other words, the row normalized values in the second row of Figure 1 are the same for each subgroup. It is relaxation of Sufficiency (Chouldechova 2016).\nPredictive equality: \\(P(\\hat{Y} = 1 | A = a, Y = 0) = P(\\hat{Y} = 1 | A = b, Y = 0)\\). Warrants that classifiers have equal False Positive Rate (FPR) for each subgroup. In other words, the column normalized values in the third column of Figure 1 are the same for each subgroup. It is relaxation of Separation (Corbett-Davies et al. 2017).\n(Overall) Accuracy equality: \\(P(\\hat{Y} = Y | A = a) = P(\\hat{Y} = Y | A = b)\\). Makes sure that models have the same Accuracy (ACC) for each subgroup. (Berk et al. 2017)\nThe reader should note that if the classifier passes Equal opportunity and Predictive equality, it also passes Equalized Odds (Hardt et al. 2016), which is equivalent to Separation criteria.\nLet us illustrate the intuition behind Independence, Separation, and Sufficiency criteria using the well-known example of the COMPAS model for estimating recidivism risk.\nFulfilling the Independence criterion means that the rate of sentenced prisoners should be equal in each subpopulation. It can be said that such an approach is fair from society’s perspective.\nFulfilling the Separation criterion means that the fraction of innocents/guilty sentenced should be equal in subgroups. Such an approach is fair from the prisoner’s perspective. The reasoning is the following: “If I am innocent, I should have the same chance of acquittal regardless of sub-population”. This was the expectation presented by the ProPublica Foundation in their study.\nMeeting the Sufficiency criterion means that there should be an equal fraction of innocents among the convicted, similarly, for the non-convicted. This approach is fair from the judge’s perspective. The reasoning is the following: “If I convicted someone, he should have the same chance of being innocent regardless of the sub-population”. This approach is presented by the company developing the COMPAS model, Northpointe.\nUnfortunately, as we have already written, it is not possible to meet all these criteria at the same time.\nWhile defining the metrics above, we assumed only two subgroups. This was done to facilitate notation, but there might be more unprivileged subgroups. A perfectly fair model would pass all criteria for each subgroup (Barocas et al. 2019).\nNot all fairness metrics are equally important in all cases. The metrics above aim to give a more holistic view into the fairness of the machine learning model. Practitioners informed in the domain may consider only those metrics that are relevant and beneficial from their point of view. For example, in Kozodoi et al. (2021) in the fair credit scoring use case, the authors concluded that the separation is the most suitable non-discrimination criteria. More general instructions can also be found in European Union Agency for Fundamental Rights (2018), along with examples of protected attributes. Sometimes, however, non-technical solutions to fairness problems might be beneficial. Note that group fairness metrics will discover not all types of unfairness, and the end-user should decide whether a model is acceptable in terms of bias or not.\nHowever tempting it is to think that all the criteria described above can be met at the same time, unfortunately, this is not possible. Barocas et al. (2019) shows that, apart from a few hypothetical situations, no two of {Independence, Separation, Sufficiency} can be fulfilled simultaneously. So we are left balancing between the degree of imbalance of the different criteria or deciding to control only one criterion.\nAcceptable amount of bias\nIt would be hard for any classifier to maintain the same relations between subgroups. That is why some margins around the perfect agreement are needed. To address this issue, we accepted the four-fifths rule (Code of Federal Regulations 1978) as the benchmark for discrimination rate, which states that “A selection rate for any race, sex, or ethnic group which is less than four-fifths (\\(\\frac{4}{5}\\)) (or eighty percent) of the rate for the group with the highest rate will generally be regarded by the Federal enforcement agencies as evidence of adverse impact[…].” The selection rate is originally represented by statistical parity, but we adopted this rule to define acceptable rates between subgroups for all metrics. There are a few caveats to the preceding citation concerning the size of the sample and the boundary itself. Nevertheless, the four-fifths rule is an excellent guideline to adhere to. In the implementation, this boundary is represented by \\(\\varepsilon\\), and it is adjustable by the user, but the default value will be 0.8.\nThis rule is often used, but users should check if the fairness criteria should be set differently in each case.\nLet \\(\\varepsilon > 0\\) be the acceptable amount of a bias. In this article, we would say that the model is not discriminatory for a particular metric if the ratio between every unprivileged \\(b, c, ...\\) and privileged subgroup \\(a\\) is within \\((\\varepsilon, \\frac{1}{\\varepsilon})\\). The common choice for the epsilon is 0.8, which corresponds to the four-fifths rule. For example, for the metric Statistical Parity (\\(STP\\)), a model would be \\(\\varepsilon\\)-non-discriminatory for privileged subgroup \\(a\\) if it satisfies.\n\\[\\begin{equation}\n\\forall_{b \\in A \\setminus \\{a\\}} \\;\\;\n   \\varepsilon < STP_{ratio} = \\frac{STP_b}{STP_a} < \\frac{1}{\\varepsilon}.\n  \\tag{1}\n\\end{equation}\\]\nEvaluating fairness\nThe main function in the fairmodels package is fairness_check. It returns fairness_object, which can be visualized or processed by other functions. This will be further explained in the “Structure” section. When calling fairness_check for the first time, the following three arguments are mandatory:\nexplainer - an object that combines model and data that gives a unified interface for predictions. It is a wrapper over a model created with the DALEX (Biecek 2018) package.\nprotected - a factor, vector containing sensitive attributes (protected group). It does not need to be binary. Instead, each level denotes a distinct subgroup. The most common examples are gender, race, nationality, etc.\nprivileged - a character/factor denoting a level in the protected vector which is suspected to be the most privileged one.\nExample\nIn the following example, we are using German Credit Data dataset (Dua and Graff 2017). In the dataset, there is information about people like age, sex, purpose, credit amount, etc. For each person, there is a risk assessed with taking credit, either good or bad. Therefore, it will be a target variable. We will train the model on the whole dataset and then measure fairness metrics to facilitate the notation (as opposed to training and testing on different subsets, which is also possible and advisable).\nFirst, we create a model. Let’s start with logistic regression.\n\n\nlibrary(\"fairmodels\")\ndata(\"german\")\n\nlm_model <- glm(Risk~., data = german, family = binomial(link = \"logit\"))\n\n\n\nThen, create a wrapper that unifies the model interface.\n\n\nlibrary(\"DALEX\")\n\ny_numeric <- as.numeric(german$Risk) -1\nexplainer_lm <- DALEX::explain(lm_model, data = german[,-1], y = y_numeric)\n\n\n\nNow we are ready to calculate and plot the fairness checks. Resulting plot is presented in Figure 2.\n\n\nfobject <- fairness_check(explainer_lm,\n                protected = german$Sex, privileged = \"male\",\n                verbose = FALSE)\n\n\n\n\n\nplot(fobject)\n\n\n\n\nFigure 2: The Fairness Check plot summarises the ratio of fairness measures between unprivileged and privileged subgroups. The light green areas correspond to values within \\((\\varepsilon, \\frac{1}{\\varepsilon})\\) and signify an acceptable difference in fairness metrics. They are bounded by red rectangles indicating values that do not meet the 4/5 rule. Fairness metrics names are given along the formulas used to calculate the score in some subgroups to facilitate interpretation. For example, the ratio here means that after metric scores were calculated, the values for unprivileged groups (female) were divided by values for the privileged subgroup (male). In this example, except for the predictive equality ratio, the other measures are \\(\\varepsilon\\)-non-discriminatory.\n\n\n\nFor a quick assessment, if a model passes fairness criteria, the object created with fairness_check() might be summarized with the print() function. Total loss is the sum of all fairness metrics. See equation (2) for more details.\n\n\nprint(fobject, colorize = FALSE)\n\n\n\nFairness check for models: lm \n\nlm passes 4/5 metrics\nTotal loss :  0.6153324 \n\nIn this example, fairness criteria are satisfied in all but one metric. The logistic regression model has a lower false-positive rate (FP/(FP+TN))) in the unprivileged group than in the privileged group. It exceeds the acceptable limit set by \\(\\varepsilon\\). Thus it does not satisfy the Predictive Equality ratio criteria.\nMore detailed visualizations are available, like Metric scores plot. It might be helpful to understand the intuition behind the Fairness check plot presented above. See an example in Figure 3. This plot might be a good first point for understanding the Fairness check plot. In fact, checks can be directly derived from the Metric scores plot. To do this, we need to divide the score denoted by the dot with the score denoted by the vertical line. This way, we obtain a value indicated by the height of the barplot. The orientation of the barplot depends on whether the value is bigger or lower than 1. Intuitively the longer the horizontal line in the figure below (the one connecting the dot with the vertical line) is, the longer the bar will be in Fairness check plot. If the scores of privileged and unprivileged subgroups are the same, then the bar will start from 1 and point to 1, so it will have a height equal to 0.\n\n\nplot(metric_scores(fobject))\n\n\n\n\nFigure 3: The Metric Scores plot summarises raw fairness metrics scores for subgroups. The dots stand for unprivileged subgroups (female) while vertical lines stans for the privileged subgroup (male). The horizontal lines act as a visual aid for measuring the difference between the scores of the metrics between the privileged and unprivileged subgroups.\n\n\n\nIt is rare that a model perfectly meets all the fairness criteria. Therefore, a handy feature is the ability to compare several models on the same scale. We add two more explainers to the fairness assessment in the example below. Now fairness_object (in code: fobject) wraps three models together with different labels and cutoffs for subgroups. The fairness_object can be later used as a basis for another fairness_object. In detail, while running fairness_check() for the first time, explainer/explainers have to be provided along with three arguments described at the start of this section. However, as shown below, when providing explainers with a fairness_object, those arguments are not necessary as they are already a part of the previously created fairness_object.\nFirst, let us create two more models based on the German Credit Data. The first will be a logistic regression model that uses fewer columns and has access to the Sex feature. The second is random forest from ranger (Wright and Ziegler 2017). It will be trained on the whole dataset.\n\n\ndiscriminative_lm_model <- glm(Risk~.,\n         data   = german[c(\"Risk\", \"Sex\",\"Age\",\n                \"Checking.account\", \"Credit.amount\")],\n         family = binomial(link = \"logit\"))\n\nlibrary(\"ranger\")\nrf_model <- ranger::ranger(Risk ~.,\n         data = german, probability = TRUE,\n         max.depth = 4, seed = 123)\n\n\n\nThese models differ in the way how the predict function works. To unify operations on these models, we need to create DALEX explainer objects. The label argument specifies how these models are named on plots.\n\n\nexplainer_dlm <- DALEX::explain(discriminative_lm_model,\n        data = german[c(\"Sex\", \"Age\", \"Checking.account\", \"Credit.amount\")],\n        y = y_numeric,\n        label = \"discriminative_lm\") \n\nexplainer_rf <- DALEX::explain(rf_model, \n        data = german[,-1], y = y_numeric)\n\n\n\nNow we are ready to assess fairness. The resulting plot is presented in Figure 4.\n\n\nfobject <- fairness_check(explainer_rf, explainer_dlm, fobject)\nplot(fobject)\n\n\n\n\nFigure 4: The Fairness Check plot for multiple models. It helps to compare models based on five selected fairness measures.\n\n\n\nWhen plotted, new bars appear on the fairness check plot. Those are new metric scores for added models. This information can be summarized in a numerical way with the print() function.\n\n\nprint(fobject, colorize = FALSE)\n\n\n\nFairness check for models: ranger, discriminative_lm, lm \n\nranger passes 5/5 metrics\nTotal loss :  0.1699186 \n\ndiscriminative_lm passes 3/5 metrics\nTotal loss :  0.7294678 \n\nlm passes 4/5 metrics\nTotal loss :  0.6153324 \n\nPackage architecture\nThe fairmodels package provides a unified interface for predictive models independently of their internal structure. Using a model agnostic approach with DALEX explainers facilitates this process (Biecek 2018). There is a unified way for each explainer to check if explained model lives up to user fairness standards. Checking fairness with fairmodels is straightforward and can be done with the three-step pipeline.\nclassification model   |>   explain()   |>   fairness_check() \nThe output of such a pipeline is an object of class fairness_object, a unified structure to wrap model explainer or multiple model explainers and other fairness_objects in a single container. Aggregation of fairness measures is done based on groups defined by model labels. This is why model explainers (even those wrapped by fairness_objects) must have different labels. Moreover, some visualizations for model comparison assume that all models are created from the same data. Of course, each model can use different variables or different feature transformations, but the order and number of rows shall stay the same. To facilitate aggregation of models fairmodels allows creating fairness_objects in other ways:\nexplainers |> fairness_check() - possibly many explainers can be passed to fairness_check(),\nfairness_objects |> fairness_check() - explainers stored in fairness_objects passed to fairness_check() will be aggregated into one fairness_object,\nexplainer & fairness_objects |> fairness_check() - explainers passed directly and explainers from fairness_objects will be aggregated into one fairness_object.\nWhen using the last two pipelines, protected vectors and privileged parameters are assumed to be the same, so passing them to fairness_check() is unnecessary.\nTo create a fairness_object, at least one explainer needs to be passed to fairness_check() function, which returns the said object. fairness_object metrics for each subgroup are calculated from the separate confusion matrices.\nThe fairness_object has numerous fields. Some of them are:\nparity_loss_metric_data - data.frame containing parity loss for each metric and classifier,\ngroups_data - list of metric scores for each metric and model,\ngroup_confusion_matrices - list of values in confusion matrices for each model and metric,\nexplainers - list of DALEX explainers. When explainers and/or fairness_object are added, then explainers and/or explainers extracted from fairness_object are added to that list,\nlabel - character vector of labels for each explainer.\n... - other fields.\nThe fairness_object methods are used to create numerous objects that help to visualize bias. In the next sections, we list more detailed functions for deeper exploration of bias. Detailed relations between objects created with fairmodels are depicted in Figure 5.\nThe general overview of the workflow is presented in Figure 6.\n\n\n\nFigure 5: Class diagram for objects created by functions from the fairmodels package. Each rectangle corresponds to one class, the name of this class is in the header of the rectangle. Each of these classes is a list containing a certain list of objects. The top slot lists the names and types of each object the list. The bottom slot contains a list of functions that can be performed on objects of the specified class. If two classes are connected by a line ending in a diamond it means that one class contains objects of the other class. If two rectangles are connected by a dashed line, it means that on the basis of one object, an object of another class can be produced. In this case, more detailed fairness statistics can be produced from the central object of the fairness check class. See the full resolution at https://bit.ly/3HNbNvo\n\n\n\n\n\n\nFigure 6: Flowchart for the fairness assessment with the fairmodels package. The arrows describe typical sequences of actions when exploring the fairness of the models. For ease of use, the names of the functions that can be used in a given step are indicated. Note that this procedure is intended to look at the model from multiple perspectives in order to track down potential problems in the model. Merely satisfying the fairness criteria does not automatically mean that the model is free of any errors\n\n\n\nVisualizing bias\nIn fairmodels there are 12 metrics based on confusion matrices for each subgroup, see the following table for the complete list. Some of them were already introduced before.\n\n\nFairness metrics implemented in the fairmodels\npackage.\n\n\nMetric\n\n\nFormula\n\n\nName\n\n\nFairness criteria\n\n\nTPR\n\n\n\\(\\frac{TP}{TP + FN}\\)\n\n\nTrue positive rate\n\n\nEqual opportunity (Hardt et al. 2016)\n\n\nTNR\n\n\n\\(\\frac{TN}{TN + FP}\\)\n\n\nTrue negative rate\n\n\n\n\nPPV\n\n\n\\(\\frac{TP}{TP + FP}\\)\n\n\nPositive predictive value\n\n\nPredictive parity (Chouldechova 2016)\n\n\nNPV\n\n\n\\(\\frac{TN}{TN + FN}\\)\n\n\nNegative predictive value\n\n\n\n\nFNR\n\n\n\\(\\frac{FN}{FN + TP}\\)\n\n\nFalse negative rate\n\n\n\n\nFPR\n\n\n\\(\\frac{FP}{FP + TN}\\)\n\n\nFalse positive rate\n\n\nPredictive equality (Corbett-Davies et al.\n2017)\n\n\nFDR\n\n\n\\(\\frac{FP}{FP + TP}\\)\n\n\nFalse discovery rate\n\n\n\n\nFOR\n\n\n\\(\\frac{FN}{FN + TN}\\)\n\n\nFalse omission rate\n\n\n\n\nTS\n\n\n\\(\\frac{TP}{TP + FN + FP}\\)\n\n\nThreat score\n\n\n\n\nF1\n\n\n\\(\\frac{2 \\cdot PPV * TPR}{PPV + TPR}\\)\n\n\nF1 score\n\n\n\n\nSTP\n\n\n\\(\\frac{TP + FP}{TP + FP + TN + FN}\\)\n\n\nPositive rate\n\n\nStatistical parity\n\n\nACC\n\n\n\\(\\frac{TP + TN}{TP + TN + FP + FN}\\)\n\n\nAccuracy\n\n\nOverall accuracy equality\n\n\n\n\n\nNot all metrics are needed to determine if the discrimination exists, but they are helpful to acquire a fuller picture. To facilitate the visualization over many subgroups, we introduce a function that maps metric scores among subgroups to a single value. This function, which we call parity_loss, has an attractive property. Due to the usage of the absolute value of the natural logarithm, it will return the same value whether the ratio is inverted or not.\nSo, for example, when we would like to know the parity loss of Statistical Parity between unprivileged (b) and privileged (a) subgroups, we mean value like this:\n\\[\\begin{equation}\nSTP_{\\textit{parity loss}} = \\Big | \\ln \\Big( \\frac{STP_b}{STP_a} \\Big)\\Big|.\n\\end{equation}\\]\nThis notation is very helpful because it allows to accumulate \\(STP_{\\textit{parity loss}}\\) overall unprivileged subgroups, so not only in the binary case.\n\\[\\begin{equation}\nSTP_{\\textit{parity loss}} = \\sum_{i \\in \\{a, b, ...\\}} \\Big|\\ln \\Big(\\frac{STP_i}{STP_a} \\Big)\\Big|.  \n  \\tag{2}\n\\end{equation}\\]\nThe parity_loss relates strictly to ratios. The classifier is more fair if parity_loss is low. This property is helpful in visualizations.\nThere are several modifying functions that operate on fairness_object. Their usage will return other objects. The relations between them is depicted on the class diagram (Figure 5). The objects can then be plotted with a generic plot() function. Additionally, a special plotting function works immediately on fairness_object, which is plot_density. The user can directly specify which metrics shall be visible in the plot in some functions. The detailed technical introduction for all these functions is presented in fairmodels.\nPlots visualizing different aspects of parity_loss can be created with one of the following pipelines:\nfairness_object |> modifying_function(...) |> plot()\nThis pipe is preferred and allows setting parameters in both modifying functions and certain plot functions, which is not the case with the next pipeline.\nfairness_object |> plot_fairmodels(type = modifying_function, ...)\nAdditional parameters are passed to the modifying functions and not to the plot function.\nUsing the pipelines, different plots can be obtained by superseding the modifying_function with function names.\nFour examples of additional graphical functions available in the fairmodels can be seen in Figure 7. This package implements a total of 8 different diagnostic plots, each describing a different fairness perspective. To see different aspects of fairness and bias, the user can choose the model with the smallest bias, find out the similarity between metrics and models, compare models in both fairness and performance, and see how cutoff manipulation might change the parity_loss. Find more information about each of them in the documentation.\n\n\nfp1  <- plot(ceteris_paribus_cutoff(fobject, \"male\", cumulated=TRUE))\nfp2  <- plot(fairness_heatmap(fobject))\nfp3  <- plot(stack_metrics(fobject))\nfp4  <- plot(plot_density(fobject))\n\n\n\n\n\nlibrary(\"patchwork\")\nfp1 + fp2 + fp3 + fp4 + \n  plot_layout(ncol = 2)\n\n\n\n\nFigure 7: Four examples of additional graphical functions are available in the fairmodels package that facilitates model and bias exploration. The Ceteris Paribus Cuttoff plot helps select the cutoff values for each model to maximize a particular measure of fairness. In this case, the suggested cutoff point for both linear models is similar. However, the ranger model does not have calibrated probabilities and thus requires a different cutoff. The Heatmap plot is very helpful when comparing large numbers of models. It shows profiles of selected fairness measures for each of the models under consideration. In this case, the fairness profiles for both linear models are similar. The Stacked Metric plot helps you compare models by summing five different fairness measures. The different layers of this plot allow you to compare individual measures, but if you don’t know which one to focus on, it is useful to look at the sum of the measures. In this case, the ranger model has the highest fairness values. Finally, the Density plot helps to compare the score distributions of the models between the advantaged and disadvantaged groups. In this case, we find that for females the distributions of the scores are lower in all models, with the largest difference for the lm model.\n\n\n\nBias mitigation\nWhat can be done if the model does not meet the fairness criteria? Machine learning practitioners might use other algorithms or variables to construct unbiased models, but this does not guarantee passing the fairness_check(). An alternative is to use bias mitigation techniques that adjust the data or model to meet fairness conditions.\nThere are essentially three types of such methods. The first is data pre-processing. There are many ways to “correct” the data when there are unwanted correlations between variables or sample sizes among subgroups in data. The second one is in-processing, which is, for example, optimizing classifiers not only to reduce classification error but also to minimize a fairness metric. Last but not least is post-processing which modifies model output so that predictions and miss-predictions among subgroups are more alike.\nThe fairmodels package offers five functions for bias mitigation, three for pre-processing, and two for post-processing. Most of these approaches are also implemented in (Bellamy et al. 2018). However, in fairmodels there are separate implementations of them in R. There are a lot of useful mitigation techniques that are not in fairmodels like those in Hardt et al. (2016) and numerous in-processing algorithms.\nData pre-processing\nDisparate impact remover\nIn fairmodels geometric repair, an algorithm originally introduced by Feldman et al. (2015), works on ordinal, numeric features. Depending on the \\(\\lambda \\in [0,1]\\) parameter, this method will transform the distribution of a given feature. The idea is simple. Given feature distribution in different subgroups, the algorithm finds optimal distribution (according to earth mover’s distance) and transforms distribution for each subgroup to match the optimal one. For example, if age is an important feature and its distribution is different in two subgroups, and we want to change that, then the geometric repair will map each individual’s age to a new distribution (different age). It will be preserving the order - the ranks (in our case, seniority) of observations are preserved. Parameter \\(\\lambda\\) is responsible for the repair degree, so for full repair, lambda should be set to 1. The method does not focus on a particular metric but rather tries to level out them by transforming potentially harmful feature distributions.\nReweighting\nReweighting is a rather straightforward approach. This method was implemented according to Kamiran and Calders (2011). It computes weights by dividing the theoretical probability of assigning favorable labels for a subgroup by real (observed) probability (based on the data). Theoretic probability for a subgroup is computed by multiplying the probability of assigning a favorable label (for all populations) by picking observation from a certain subgroup. It focuses on mitigating statistical parity.\nResampling\nResampling is based on weights calculated in reweighting. Each weight for a subgroup is multiplied by the size of the subgroup. Then, whether the subgroup is deprived or not (if weight is higher than one, the subgroup is considered deprived), observations are duplicated from either one that were assigned a favorable label or not. There are two types of resampling- uniform and preferential. The uniform is making algorithm pick or omit observations randomly without considering its probabilistic score. Preferential uses another probabilistic classifier, potentially different from the main model for final predictions. In Kamiran and Calders (2011) it is called ranker - it predicts the probabilities for the observations to decide which observations are close to the cutoff border (usually 0.5). Based on the probabilistic output of the ranker, the observations are sorted, and the ones with the highest/lowest ranks are either left out or duplicated depending on the case—more on that on Kamiran and Calders (2011). The fairmodels implementation, instead of training the ranker as in the aforementioned paper, uses a vector of previously calculated probabilities provided by the user. With this, it shifts the decision and responsibility of choosing a ranker to the user. It focuses on mitigating statistical parity.\nModel post-processing\nReject Option based Classification Pivot\nThe roc_pivot method is implemented based on Kamiran et al. (2012) in the fairmodels package. Let \\(\\theta \\in (0,1)\\) be the value that determines the radius of the so-called critical region, which is an area around the cutoff. The user specifies the \\(\\theta\\), and it should describe how big the critical region should be. For example if \\(\\theta = 0.1\\) and cutoff is 0.6, then the critical region will be (0.5, 0.7). Let’s assume that we are predicting a favorable outcome. If the assigned probability of observation is in the described region, then the probabilities are pivoting on the other side of the cutoff with a certain assumption. If an observation in a critical region is considered to be the privileged and it is on the right side of the cutoff, then its probabilities are pivoting from the right side of the cutoff to the left. So if an observation is in the critical region and it is considered unprivileged, then if it is on the left side of the cutoff, it will pivot to the right side. Pivoting here means changing the side of the cutoff so that the distance from the cutoff stays unchanged. It does not intend to mitigate a single metric but rather changes predictions in the critical region (the region with low certainty). By pivoting the predictions, it might lower more metrics.\nCutoff manipulation\nThe fairmodels package supports setting cutoff for each subgroup. Users may pick parity_loss metrics of their choice and find the minimal parity_loss. It is part of ceteris_paribus_cutoff() function. Based on picked metrics, the sum of parity loss is calculated for each cutoff of the chosen subgroup. Then the minimal value is found—this way, optimal values might be found for metrics of interest. The minimum is marked with a dashed vertical line (see Figure 7). This approach however might be to some extent concerning. Some might argue that setting different cutoffs for different subgroups is unfair and is punishing privileged subgroups for something they have no control of. Especially in the individual fairness field, it would be concerning if two similar people with different sensitive attributes would have two different thresholds and potentially two different outcomes. This is a valid point, and this method should be used with knowledge of all its drawbacks. The cutoff manipulation method targets metrics chosen by the user.\nAll pre-processing methods can be used with two pipelines, whereas post-processing can be used in one specific way.\nPre-processing pipelines\ndata/explainer |> method\nReturns either weights, indexes, or changed data depending on the method used.\ndata/explainer |> pre_process_data(data, protected, y, type = ...)\nAlways returns data.frame. In case of weights data has additional column called _weights_.\n\nPost-processing pipelines\nfairness_object |> ceteris_paribus_cutoff(subgroup, ...) |> print/plot\nThis is the pipeline for creating ceteris paribus cutoff print and plot.\nexplainer |> roc_pivot(protected, privileged, ...)\nThe pipeline will return explainer with y_hat field changed.\n\nThe user should be aware that debiasing one metric might enhance bias in another. It is a so-called fairness-fairness trade-off. There is also a fairness-performance trade-off where debiasing one metric leads to worse performance. Another thing to remember is, as found in Agrawal et al. (2020), metrics might not generalize well to out-of-distribution examples, so it is advised also to check the fairness metrics on a separate test set.\nExample\nNow we will show an example usage of one pre-processing and one post-processing method. As before, the German Credit Data will be used along with the previously created lm_model. So firstly, we create a new dataset using pre_process_data and then we use it to train the logistic regression classifier.\n\n\nresampled_german   <- german |> pre_process_data(protected = german$Sex,\n                y_numeric, type = 'resample_uniform')\n\nlm_model_resample  <- glm(Risk~.,\n                data   = resampled_german,\n                family = binomial(link = \"logit\"))\n\nexplainer_lm_resample <- DALEX::explain(lm_model_resample,\n                data = german[,-1], y = y_numeric, verbose = FALSE)\n\n\n\nThen we make other explainers. We use previously created explainer_lm with the post-processing function roc_pivot. We set parameter theta = 0.05 for a rather narrow area of a pivot.\n\n\nnew_explainer <- explainer_lm |> roc_pivot(protected = german$Sex,\n                privileged = \"male\", theta = 0.05)\n\n\n\nIn the end, we create fairness_object with explainers obtained with the code above and one created in the first example to see the difference.\n\n\nfobject <- fairness_check(explainer_lm_resample, new_explainer, explainer_lm,\n                protected = german$Sex, privileged = \"male\",\n                label = c(\"resample\", \"roc\", \"base\"),\n                verbose = FALSE)\n\nfobject |> plot()\n\n\n\n\nFigure 8: Graphical summary of a base model (blue bars) and model after applying two bias mitigation techniques (red and green bars). By comparing adjacent rectangles one can read how the respective technique affected the corresponding fairness measure\n\n\n\nThe result of the code above is presented in Figure 8. The mitigation methods successfully eliminated bias in all of the metrics. Both models are better than the original base. This is not always the case - sometimes, eliminating bias in one metric may increase bias in another metric. For example, let’s consider a perfectly accurate model, but some subgroups receive few positive predictions (bias in Statistical parity). In that case, mitigating the bias in Statistical parity would decrease the Accuracy equality ratio.\nSummary and future work\nThis paper showed that checking for bias in machine learning models can be done conveniently and flexibly. The package fairmodels described above is a self-sufficient tool for bias detection, visualization, and mitigation in classification machine learning models. We presented theory, package architecture, suggested usage, and examples along with plots. Along the way, we introduced the core concepts and assumptions that come along the bias detection and plot interpretation. The package is still improved and enhanced, which can be seen by adding the announced regression module based on Steinberg et al. (2020). We did not cover it in this article because it is still an experimental tool. Another tool for in-processing classification closely related to fairmodels has also been added and can be found on https://github.com/ModelOriented/FairPAN.\nThe source code of the package, vignettes, examples, and documentation can be found at https://modeloriented.github.io/fairmodels/. The stable version is available on CRAN. The code and the development version can be found on GitHub https://github.com/ModelOriented/fairmodels. This is also a place to report bugs or requests (through GitHub issues).\nIn the future, we plan to enhance the spectrum of bias visualization plots and introduce regression and individual fairness methods. The potential way to explore would be an in-processing bias mitigation - training models that minimize cost function and adhere to certain fairness criteria. This field is heavily developed in Python and lacks appropriate attention in R.\nAcknowledgements\nWork on this package was financially supported by the NCN Sonata Bis-9 grant 2019/34/E/ST6/00052.\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-019.zip\nCRAN packages used\nfairmodels, h2o, fairness, fairadapt, DALEX, ranger\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, MachineLearning, ModelDeployment, Survival, TeachingStatistics\n\n\nA. Agrawal, F. Pfisterer, B. Bischl, J. Chen, S. Sood, S. Shah, F. Buet-Golfouse, B. A. Mateen and S. Vollmer. Debiasing classifiers: Is reality at variance with expectation? Electronic, 2020.\n\n\nJ. Angwin, J. Larson, S. Mattu and and Lauren Kirchner. Machine bias: There’s software used across the country to predict future criminals. And it’s biased against blacks. ProPublica, 2016. URL https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\n\n\nS. Barocas, M. Hardt and A. Narayanan. Fairness and machine learning. fairmlbook.org, 2019. http://www.fairmlbook.org.\n\n\nR. K. E. Bellamy, K. Dey, M. Hind, S. C. Hoffman, S. Houde, K. Kannan, P. Lohia, J. Martino, S. Mehta, A. Mojsilovic, et al. AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. 2018. URL https://arxiv.org/abs/1810.01943.\n\n\nR. Berk, H. Heidari, S. Jabbari, M. Kearns and A. Roth. Fairness in criminal justice risk assessments: The state of the art. Sociological Methods & Research, 2017. URL https://doi.org/10.1177/0049124118782533.\n\n\nP. Biecek. DALEX: Explainers for Complex Predictive Models in R. Journal of Machine Learning Research, 19(84): 1–5, 2018. URL http://jmlr.org/papers/v19/18-416.html.\n\n\nP. Biecek and T. Burzykowski. Explanatory Model Analysis. Chapman; Hall/CRC, New York, 2021. URL https://pbiecek.github.io/ema/.\n\n\nR. Binns. On the apparent conflict between individual and group fairness. In Proceedings of the 2020 conference on fairness, accountability, and transparency, pages. 514–524 2020. New York, NY, USA: Association for Computing Machinery. ISBN 9781450369367. URL https://doi.org/10.1145/3351095.3372864.\n\n\nS. Bird, M. Dudík, R. Edgar, B. Horn, R. Lutz, V. Milan, M. Sameki, H. Wallach and K. Walker. Fairlearn: A toolkit for assessing and improving fairness in AI. MSR-TR-2020-32. Microsoft. 2020. URL https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/.\n\n\nJ. Buolamwini and T. Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Proceedings of the 1st conference on fairness, accountability and transparency, Eds S. A. Friedler and C. Wilson pages. 77–91 2018. New York, NY, USA. URL http://proceedings.mlr.press/v81/buolamwini18a.html.\n\n\nA. Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big Data, 5: 2016. URL https://doi.org/10.1089/big.2016.0047.\n\n\nD. Cirillo, S. Catuara-Solarz, C. Morey, E. Guney, L. Subirats, S. Mellino, A. Gigante, A. Valencia, M. J. Rementeria, A. S. Chadha, et al. Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare. npj Digital Medicine, 3(1): 81, 2020. URL https://doi.org/10.1038/s41746-020-0288-5 [online; last accessed October 24, 2021].\n\n\nCode of Federal Regulations. SECTION 4D, UNIFORM GUIDELINES ON EMPLOYEE SELECTION PROCEDURES (1978). 1978. URL https://www.govinfo.gov/content/pkg/CFR-2014-title29-vol4/xml/CFR-2014-title29-vol4-part1607.xml.\n\n\nS. Corbett-Davies, E. Pierson, A. Feller, S. Goel and A. Huq. Algorithmic decision making and the cost of fairness. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pages. 797–806 2017. New York, NY, USA: Association for Computing Machinery. URL https://doi.org/10.1145/3097983.3098095.\n\n\nCouncil of Europe. Guidelines on facial recognition. 2021. URL https://www.coe.int/en/web/portal/-/facial-recognition-strict-regulation-is-needed-to-prevent-human-rights-violations-.\n\n\nD. Dua and C. Graff. UCI machine learning repository. 2017. URL https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data).\n\n\nC. Dwork, M. Hardt, T. Pitassi, O. Reingold and R. Zemel. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pages. 214–226 2012. New York, NY, USA: Association for Computing Machinery. URL https://doi.org/10.1145/2090236.2090255.\n\n\nEuropean Union Agency for Fundamental Rights and Council of Europe. Handbook on european non-discrimination law. Luxembourg: Publications Office of the European Union, 2018. https://fra.europa.eu/en/publication/2018/handbook-european-non-discrimination-law-2018-edition.\n\n\nEuropean Union Agency for Fundamental Rights. Handbook on european non-discrimination law. 2018. DOI https://doi.org/10.2811/792676.\n\n\nM. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger and S. Venkatasubramanian. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages. 259–268 2015. New York, NY, USA: Association for Computing Machinery. URL https://doi.org/10.1145/2783258.2783311.\n\n\nH2O.ai. H2O AutoML. 2017. URL http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html. H2O version 3.30.0.1.\n\n\nM. Hardt, E. Price, E. Price and N. Srebro. Equality of opportunity in supervised learning. In Advances in neural information processing systems 29, Eds D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon and R. Garnett pages. 3315–3323 2016. Curran Associates, Inc. URL http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf.\n\n\nF. Kamiran and T. Calders. Data pre-processing techniques for classification without discrimination. Knowledge and Information Systems, 33: 2011. URL https://doi.org/10.1007/s10115-011-0463-8.\n\n\nF. Kamiran, A. Karim and X. Zhang. Decision theory for discrimination-aware classification. In 2012 IEEE 12th international conference on data mining, pages. 924–929 2012. URL https://doi.org/10.1109/ICDM.2012.45.\n\n\nN. Kozodoi, J. Jacob and S. Lessmann. Fairness in credit scoring: Assessment, implementation and profit implications. European Journal of Operational Research, 2021. URL https://doi.org/10.1016/j.ejor.2021.06.023.\n\n\nN. Kozodoi and T. V. Varga. Fairness: Algorithmic fairness metrics. 2021. URL https://CRAN.R-project.org/package=fairness. R package version 1.2.1.\n\n\nP. Lahoti, K. P. Gummadi and G. Weikum. [iFair: Learning individually fair data representations for algorithmic decision making]. In 2019 IEEE 35th international conference on data engineering (ICDE), pages. 1334–1345 2019. URL https://doi.org/10.1109/ICDE.2019.00121.\n\n\nN. Mehrabi, F. Morstatter, N. Saxena, K. Lerman and A. Galstyan. A survey on bias and fairness in machine learning. 2019. URL https://arxiv.org/abs/1908.09635.\n\n\nD. Plečko and N. Meinshausen. Fair data adaptation with quantile preservation. 2019. URL https://arxiv.org/abs/1911.06685.\n\n\nP. Saleiro, B. Kuester, A. Stevens, A. Anisfeld, L. Hinkson, J. London and R. Ghani. Aequitas: A bias and fairness audit toolkit. 2018. URL https://arxiv.org/abs/1811.05577.\n\n\nD. C. Steinberg, A. Reid and S. T. O’Callaghan. Fairness measures for regression via probabilistic classification. ArXiv, abs/2001.06089: 2020.\n\n\nM. Wick, S. Panda and J.-B. Tristan. Unlocking fairness: A trade-off revisited. In Advances in neural information processing systems, Eds H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox and R. Garnett pages. 8783–8792 2019. Curran Associates, Inc. URL https://proceedings.neurips.cc/paper/2019/file/373e4c5d8edfa8b74fd4b6791d0cf6dc-Paper.pdf.\n\n\nM. N. Wright and A. Ziegler. ranger: A fast implementation of random forests for high dimensional data in C++ and R. Journal of Statistical Software, 77(1): 1–17, 2017. DOI 10.18637/jss.v077.i01.\n\n\n\n\n",
    "preview": "articles/RJ-2022-019/table1.png",
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {},
    "preview_width": 2696,
    "preview_height": 506
  },
  {
    "path": "articles/RJ-2022-001/",
    "title": "blindrecalc - An R Package for Blinded Sample Size Recalculation",
    "description": "Besides the type 1 and type 2 error rate and the clinically relevant effect size, the sample size of a clinical trial depends on so-called nuisance parameters for which the concrete values are usually unknown when a clinical trial is planned. When the uncertainty about the magnitude of these parameters is high, an internal pilot study design with a blinded sample size recalculation can be used to achieve the target power even when the initially assumed value for the nuisance parameter is wrong. In this paper, we present the R-package blindrecalc that helps with planning a clinical trial with such a design by computing the operating characteristics and the distribution of the total sample size under different true values of the nuisance parameter. We implemented methods for continuous and binary outcomes in the superiority and the non-inferiority setting.",
    "author": [
      {
        "name": "Lukas Baumann",
        "url": {}
      },
      {
        "name": "Maximilian Pilz",
        "url": {}
      },
      {
        "name": "Meinhard Kieser",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-001.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-002/",
    "title": "tvReg: Time-varying Coefficients in Multi-Equation Regression in R",
    "description": "This article explains the usage of R package [tvReg](https://CRAN.R-project.org/package=tvReg), publicly available for download from the Comprehensive R Archive Network, via its application to economic and finance problems. The six basic functions in this package cover the kernel estimation of semiparametric panel data, seemingly unrelated equations, vector autoregressive, impulse response, and linear regression models whose coefficients may vary with time or any random variable. Moreover, this package provides methods for the graphical display of results, forecast, prediction, extraction of the residuals and fitted values, bandwidth selection and nonparametric estimation of the time-varying variance-covariance matrix of the error term. Applications to risk management, portfolio management, asset management and monetary policy are used as examples of these functions usage.",
    "author": [
      {
        "name": "Isabel Casas",
        "url": {}
      },
      {
        "name": "Rubén Fernández-Casal",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-002.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-003/",
    "title": "RKHSMetaMod: An R Package to Estimate the Hoeffding Decomposition of a Complex Model by Solving RKHS Ridge Group Sparse Optimization Problem",
    "description": "In this paper, we propose an R package, called [RKHSMetaMod](https://CRAN.R-project.org/package=RKHSMetaMod), that implements a procedure for estimating a meta-model of a complex model. The meta-model approximates the Hoeffding decomposition of the complex model and allows us to perform sensitivity analysis on it. It belongs to a reproducing kernel Hilbert space that is constructed as a direct sum of Hilbert spaces. The estimator of the meta-model is the solution of a penalized empirical least-squares minimization with the sum of the Hilbert norm and the empirical $L^2$-norm. This procedure, called RKHS ridge group sparse, allows both to select and estimate the terms in the Hoeffding decomposition, and therefore, to select and estimate the Sobol indices that are non-zero. The [RKHSMetaMod](https://CRAN.R-project.org/package=RKHSMetaMod) package provides an interface from the R statistical computing environment to the C++ libraries Eigen and GSL. In order to speed up the execution time and optimize the storage memory, except for a function that is written in R, all of the functions of this package are written using the efficient C++ libraries through [RcppEigen](https://CRAN.R-project.org/package=RcppEigen) and [RcppGSL](https://CRAN.R-project.org/package=RcppGSL) packages. These functions are then interfaced in the R environment in order to propose a user-friendly package.",
    "author": [
      {
        "name": "Halaleh Kamari",
        "url": {}
      },
      {
        "name": "Sylvie Huet",
        "url": {}
      },
      {
        "name": "Marie-Luce Taupin",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-003.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-004/",
    "title": "Measuring the Extent and Patterns of Urban Shrinkage for Small Towns Using R",
    "description": "Urban shrinking is a phenomenon as common as urban expansion nowadays and it affects urban settlements of all sizes, especially from developed and industrialized countries in Europe, America and Asia. The paper aims to assess the patterns of shrinkage for small and medium sized towns in Oltenia region (Romania), considering demographic, economic and social indicators with a methodological approach which considers the use of different functions and applications of R packages. Thirteen selected indicators are analysed to perform the multivariate analysis on Principal Component Analysis using the prcomp() function and the [ggplot2](https://CRAN.R-project.org/package=ggplot2) package to visualize the patterns of urban shrinkage. Two composite indicators were additionally created to measure the extent of urban shrinkage: CSI (Composite Shrinking Index) and RDC (Regional Demographic Change) for two-time intervals. Based on the CSI, three major categories of shrinking were observed: persistent shrinkage, mild shrinking or slow evolution toward shrinking, where the vast majority of towns are found (including mining towns, where there still is a delayed restructuring of state-owned enterprises, and towns characterised by the agrarization of local economies), and stagnant/stabilized shrinkage.",
    "author": [
      {
        "name": "Cristiana Vîlcea",
        "url": {}
      },
      {
        "name": "Liliana Popescu",
        "url": {}
      },
      {
        "name": "Alin Clincea",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-004.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-005/",
    "title": "cpsurvsim: An R Package for Simulating Data from Change-Point Hazard Distributions",
    "description": "Change-point hazard models have several practical applications, including modeling processes such as cancer mortality rates and disease progression. While the inverse cumulative distribution function (CDF) method is commonly used for simulating data, we demonstrate the shortcomings of this approach when simulating data from change-point hazard distributions with more than a scale parameter. We propose an alternative method of simulating this data that takes advantage of the memoryless property of survival data and introduce the R package cpsurvsim which implements both simulation methods. The functions of cpsurvsim are discussed, demonstrated, and compared.",
    "author": [
      {
        "name": "Camille J. Hochheimer, PhD",
        "url": {}
      },
      {
        "name": "Roy T. Sabo, PhD",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-005.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-006/",
    "title": "A Computational Analysis of the Dynamics of R Style Based on 108 Million Lines of Code from All CRAN Packages in the Past 21 Years",
    "description": "The flexibility of R and the diversity of the R community leads to a large number of programming styles applied in R packages. We have analyzed 108 million lines of R code from CRAN and quantified the evolution in popularity of 12 style-elements from 1998 to 2019. We attribute 3 main factors that drive changes in programming style: the effect of style-guides, the effect of introducing new features, and the effect of editors. We observe in the data that a consensus in programming style is forming, such as using lower snake case for function names (e.g. softplus_func) and \\<- rather than = for assignment.",
    "author": [
      {
        "name": "Chia-Yi Yen",
        "url": {}
      },
      {
        "name": "Mia Huai-Wen Chang",
        "url": {}
      },
      {
        "name": "Chung-hong Chan",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-006.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-007/",
    "title": "rmonad: pipelines you can compute on",
    "description": "The [rmonad](https://CRAN.R-project.org/package=rmonad) package presents a monadic pipeline toolset for chaining functions into stateful, branching pipelines. As functions in the pipeline are run, their results are merged into a graph of all past operations. The resulting structure allows downstream computation on node documentation, intermediate data, performance stats, and any raised messages, warnings or errors, as well as the final results. [rmonad](https://CRAN.R-project.org/package=rmonad) is a novel approach to designing reproducible, well-documented, and maintainable workflows in R.",
    "author": [
      {
        "name": "Zebulun Arendsee",
        "url": {}
      },
      {
        "name": "Jennifer Chang",
        "url": {}
      },
      {
        "name": "Eve Wurtele",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-007.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-008/",
    "title": "A Software Tool For Sparse Estimation Of A General Class Of High-dimensional GLMs",
    "description": "Generalized linear models are the workhorse of many inferential problems. Also in the modern era with high-dimensional settings, such models have been proven to be effective exploratory tools. Most attention has been paid to Gaussian, binomial and Poisson settings, which have efficient computational implementations and where either the dispersion parameter is largely irrelevant or absent. However, general GLMs have dispersion parameters $\\phi$ that affect the value of the log-likelihood. This in turn, affects the value of various information criteria such as AIC and BIC, and has a considerable impact on the computation and selection of the optimal model. The R-package [dglars](https://CRAN.R-project.org/package=dglars) is one of the standard packages to perform high-dimensional analyses for GLMs. Being based on fundamental likelihood considerations, rather than arbitrary penalization, it naturally extends to the general GLM setting. In this paper, we present an improved predictor-corrector (IPC) algorithm for computing the differential geometric least angle regression (dgLARS) solution curve, proposed in [@Augug13] and [@pazira]. We describe the implementation of a stable estimator of the dispersion parameter proposed in [@pazira] for high-dimensional exponential dispersion models. A simulation study is conducted to test the performance of the proposed methods and algorithms. We illustrate the methods using an example. The described improvements have been implemented in a new version of the R-package [dglars](https://CRAN.R-project.org/package=dglars).",
    "author": [
      {
        "name": "Hassan Pazira",
        "url": {}
      },
      {
        "name": "Luigi Augugliaro",
        "url": {}
      },
      {
        "name": "Ernst C. Wit",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-008.zip\n\n\nL. Augugliaro, A. M. Mineo and E. C. Wit. Differential geometric least angle regression: A differential geometric approach to sparse generalized linear models. Journal of the Royal Statistical Society: Series B, 75(3): 471–498, 2013.\n\n\nH. Pazira, L. Augugliaro and E. C. Wit. Extended differential geometric LARS for high-dimensional GLMs with general dispersion parameter. Statistics and Computing, 28(4): 753–774, 2018. URL http://dx.doi.org/10.1007/s11222-017-9761-7.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-009/",
    "title": "bayesanova: An R package for Bayesian Inference in the Analysis of Variance via Markov Chain Monte Carlo in Gaussian Mixture Models",
    "description": "This paper introduces the R package [bayesanova](https://CRAN.R-project.org/package=bayesanova), which performs Bayesian inference in the analysis of variance (ANOVA). Traditional ANOVA based on null hypothesis significance testing (NHST) is prone to overestimating effects and stating effects if none are present. Bayesian ANOVAs developed so far are based on Bayes factors (BF), which also enforce a hypothesis testing stance. Instead, the Bayesian ANOVA implemented in bayesanova focusses on effect size estimation and is based on a Gaussian mixture with known allocations, for which full posterior inference for the component parameters is implemented via Markov-Chain-Monte-Carlo (MCMC). Inference for the difference in means, standard deviations and effect sizes between each of the groups is obtained automatically. Estimation of the parameters instead of hypothesis testing is embraced via the region of practical equivalence (ROPE), and helper functions provide checks of the model assumptions and visualization of the results.",
    "author": [
      {
        "name": "Riko Kelter",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-009.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-010/",
    "title": "Revisiting Historical Bar Graphics on Epidemics in the Era of R ggplot2",
    "description": "This study is motivated by an article published in a local history magazine on \"Pandemics in the History\". That article was also motivated by a government report involving several statistical graphics which were drawn by hand in 1938 and used to summarize official statistics on epidemics occurred between the years 1923 and 1937. Due to the aesthetic information design available on these historical graphs, in this study, we would like to investigate how graphical elements of the graphs such as titles, axis lines, axis tick marks, tick mark labels, colors, and data values are presented on these graphics and how to reproduce these historical graphics via well-known data visualization package [ggplot2](https://CRAN.R-project.org/package=ggplot2) in our era.",
    "author": [
      {
        "name": "Sami Aldag",
        "url": {}
      },
      {
        "name": "Dogukan Topcuoglu",
        "url": {}
      },
      {
        "name": "Gul Inan",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-010.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-011/",
    "title": "PSweight: An R Package for Propensity Score Weighting Analysis",
    "description": "Propensity score weighting is an important tool for comparative effectiveness research. Besides the inverse probability of treatment weights (IPW), recent development has introduced a general class of balancing weights, corresponding to alternative target populations and estimands. In particular, the overlap weights (OW) lead to optimal covariate balance and estimation efficiency, and a target population of scientific and policy interest. We develop the R package [PSweight](https://CRAN.R-project.org/package=PSweight) to provide a comprehensive design and analysis platform for causal inference based on propensity score weighting. PSweight supports (i) a variety of balancing weights, (ii) binary and multiple treatments, (iii) simple and augmented weighting estimators, (iv) nuisance-adjusted sandwich variances, and (v) ratio estimands. PSweight also provides diagnostic tables and graphs for covariate balance assessment. We demonstrate the functionality of the package using a data example from the National Child Development Survey (NCDS), where we evaluate the causal effect of educational attainment on income.",
    "author": [
      {
        "name": "Tianhui Zhou",
        "url": {}
      },
      {
        "name": "Guangyu Tong",
        "url": {}
      },
      {
        "name": "Fan Li",
        "url": {}
      },
      {
        "name": "Laine E. Thomas",
        "url": {}
      },
      {
        "name": "Fan Li",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-011.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-012/",
    "title": "RFpredInterval: An R Package for Prediction Intervals with Random Forests and Boosted Forests",
    "description": "Like many predictive models, random forests provide point predictions for new observations. Besides the point prediction, it is important to quantify the uncertainty in the prediction. Prediction intervals provide information about the reliability of the point predictions. We have developed a comprehensive R package, [RFpredInterval](https://CRAN.R-project.org/package=RFpredInterval), that integrates 16 methods to build prediction intervals with random forests and boosted forests. The set of methods implemented in the package includes a new method to build prediction intervals with boosted forests (PIBF) and 15 method variations to produce prediction intervals with random forests, as proposed by [@roy_prediction_2020]. We perform an extensive simulation study and apply real data analyses to compare the performance of the proposed method to ten existing methods for building prediction intervals with random forests. The results show that the proposed method is very competitive and, globally, outperforms competing methods.",
    "author": [
      {
        "name": "Cansu Alakus",
        "url": {}
      },
      {
        "name": "Denis Larocque",
        "url": {}
      },
      {
        "name": "Aurélie Labbe",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-012.zip\n\n\nM.-H. Roy and D. Larocque. Prediction intervals with random forests. Statistical Methods in Medical Research, 29(1): 205–229, 2020. URL https://doi.org/10.1177/0962280219829885 [online; last accessed May 4, 2021].\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-013/",
    "title": "etrm: Energy Trading and Risk Management in R",
    "description": "This paper introduces [etrm](https://CRAN.R-project.org/package=etrm), an R package with tools for trading and financial risk management in energy markets. Contracts for electric power and natural gas differ from most other commodities due to the fact that physical delivery takes place over a time interval, and not at a specific point in time. There is typically strong seasonality, limited storage and transmission capacity and strong correlation between price and required volume. Such characteristics need to be taken into account when pricing contracts and managing financial risk related to energy procurement. Tools for these task are usually bundled into proprietary Energy Trading Risk Management (ETRM) systems delivered by specialized IT vendors. The [etrm](https://CRAN.R-project.org/package=etrm) package offers a transparent solution for building a forward price curve for energy commodities which is consistent with methods widely used in the industry. The user's fundamental market view may be combined with contract price quotes to form a forward curve that replicate current market prices, as described in @ollmar2003analysis and @benth2007extracting. [etrm](https://CRAN.R-project.org/package=etrm) also provides implementations of five portfolio insurance trading strategies for energy price risk management. The forward market curve and the energy price hedging strategies are core elements in an ETRM system, which to the best of the author's knowledge has not been previously available in the R ecosystem.",
    "author": [
      {
        "name": "Anders D. Sleire",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-013.zip\n\n\nF. E. Benth, S. Koekkebakker and F. Ollmar. Extracting and applying smooth forward curves from average-based commodity contracts with seasonal variation. The Journal of Derivatives, 15(1): 52–66, 2007. URL https://doi.org/10.3905/jod.2007.694791 .\n\n\nF. Ollmar. An analysis of derivative prices in the Nordic power market. 2003. URL http://hdl.handle.net/11250/164248.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-014/",
    "title": "fcaR, Formal Concept Analysis with R",
    "description": "Formal concept analysis (FCA) is a solid mathematical framework to manage information based on logic and lattice theory. It defines two explicit representations of the knowledge present in a dataset as concepts and implications. This paper describes an R package called [fcaR](https://CRAN.R-project.org/package=fcaR) that implements FCA's core notions and techniques. Additionally, it implements the extension of FCA to fuzzy datasets and a simplification logic to develop automated reasoning tools. This package is the first to implement FCA techniques in R. Therefore, emphasis has been put on defining classes and methods that could be reusable and extensible by the community. Furthermore, the package incorporates an interface with the [arules](https://CRAN.R-project.org/package=arules) package, probably the most used package regarding association rules, closely related to FCA. Finally, we show an application of the use of the package to design a recommender system based on logic for diagnosis in neurological pathologies.",
    "author": [
      {
        "name": "Pablo Cordero",
        "url": {}
      },
      {
        "name": "Manuel Enciso",
        "url": {}
      },
      {
        "name": "Domingo López-Rodríguez",
        "url": {}
      },
      {
        "name": "Ángel Mora",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-014.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-015/",
    "title": "FMM: An R Package for Modeling Rhythmic Patterns in Oscillatory Systems",
    "description": "This paper is dedicated to the R package FMM which implements a novel approach to describe rhythmic patterns in oscillatory signals. The frequency modulated Möbius (FMM) model is defined as a parametric signal plus a Gaussian noise, where the signal can be described as a single or a sum of waves. The FMM approach is flexible enough to describe a great variety of rhythmic patterns. The FMM package includes all required functions to fit and explore single and multi-wave FMM models, as well as a restricted version that allows equality constraints between parameters representing a priori knowledge about the shape to be included. Moreover, the FMM package can generate synthetic data and visualize the results of the fitting process. The potential of this methodology is illustrated with examples of such biological oscillations as the circadian rhythm in gene expression, the electrical activity of the heartbeat and the neuronal activity.",
    "author": [
      {
        "name": "Itziar Fernández",
        "url": {}
      },
      {
        "name": "Alejandro Rodríguez-Collado",
        "url": {}
      },
      {
        "name": "Yolanda Larriba",
        "url": {}
      },
      {
        "name": "Adrián Lamela",
        "url": {}
      },
      {
        "name": "Christian Canedo",
        "url": {}
      },
      {
        "name": "Cristina Rueda",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-015.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-016/",
    "title": "spherepc: An R Package for Dimension Reduction on a Sphere",
    "description": "Dimension reduction is a technique that can compress given data and reduce noise. Recently, a dimension reduction technique on spheres, called spherical principal curves (SPC), has been proposed. SPC fits a curve that passes through the middle of data with a stationary property on spheres. In addition, a study of local principal geodesics (LPG) is considered to identify the complex structure of data. Through the description and implementation of various examples, this paper introduces an R package [spherepc](https://CRAN.R-project.org/package=spherepc) for dimension reduction of data lying on a sphere, including existing methods, SPC and LPG.",
    "author": [
      {
        "name": "Jongmin Lee",
        "url": {}
      },
      {
        "name": "Jang-Hyun Kim",
        "url": {}
      },
      {
        "name": "Hee-Seok Oh",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-016.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-017/",
    "title": "The smoots Package in R for Semiparametric Modeling of Trend Stationary Time Series",
    "description": "This paper is an introduction to the new package in R called [smoots](https://CRAN.R-project.org/package=smoots) (smoothing time series), developed for data-driven local polynomial smoothing of trend-stationary time series. Functions for data-driven estimation of the first and second derivatives of the trend are also built-in. It is first applied to monthly changes of the global temperature. The quarterly US-GDP series shows that this package can also be well applied to a semiparametric multiplicative component model for non-negative time series via the log-transformation. Furthermore, we introduced a semiparametric Log-GARCH and a semiparametric Log-ACD model, which can be easily estimated by the smoots package. Of course, this package applies to suitable time series from any other research area. The smoots package also provides a useful tool for teaching time series analysis, because many practical time series follow an additive or a multiplicative component model.",
    "author": [
      {
        "name": "Yuanhua Feng",
        "url": {}
      },
      {
        "name": "Thomas Gries",
        "url": {}
      },
      {
        "name": "Sebastian Letmathe",
        "url": {}
      },
      {
        "name": "Dominik Schulz",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-017.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-018/",
    "title": "starvars: An R Package for Analysing Nonlinearities in Multivariate Time Series",
    "description": "Although linear autoregressive models are useful to practitioners in different fields, often a nonlinear specification would be more appropriate in time series analysis. In general, there are many alternative approaches to nonlinearity modelling, one consists in assuming multiple regimes. Among the possible specifications that account for regime changes in the multivariate framework, smooth transition models are the most general, since they nest both linear and threshold autoregressive models. This paper introduces the [starvars](https://CRAN.R-project.org/package=starvars) package which estimates and predicts the Vector Logistic Smooth Transition model in a very general setting which also includes predetermined variables. In comparison to the existing R packages, starvars offers the estimation of the Vector Smooth Transition model both by maximum likelihood and nonlinear least squares. The package allows also to test for nonlinearity in a multivariate setting and detect the presence of common breaks. Furthermore, the package computes multi-step-ahead forecasts. Finally, an illustration with financial time series is provided to show its usage.",
    "author": [
      {
        "name": "Andrea Bucci",
        "url": {}
      },
      {
        "name": "Giulio Palomba",
        "url": {}
      },
      {
        "name": "Eduardo Rossi",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-018.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-020/",
    "title": "Palmer Archipelago Penguins Data in the palmerpenguins R Package - An Alternative to Anderson's Irises",
    "description": "In 1935, Edgar Anderson collected size measurements for 150 flowers from three species of *Iris* on the Gaspé Peninsula in Quebec, Canada. Since then, Anderson's *Iris* observations have become a classic dataset in statistics, machine learning, and data science teaching materials. It is included in the base R datasets package as `iris`, making it easy for users to access without knowing much about it. However, the lack of data documentation, presence of non-intuitive variables (e.g. \"sepal width\"), and perfectly balanced groups with zero missing values make `iris` an inadequate and stale dataset for teaching and learning modern data science skills. Users would benefit from working with a more representative, real-world environmental dataset with a clear link to current scientific research. Importantly, Anderson’s *Iris* data appeared in a 1936 publication by R. A. Fisher in the *Annals of Eugenics* (which is often the first-listed citation for the dataset), inextricably linking `iris` to eugenics research. Thus, a modern alternative to `iris` is needed. In this paper, we introduce the palmerpenguins R package [@R-palmerpenguins], which includes body size measurements collected from 2007 - 2009 for three species of *Pygoscelis* penguins that breed on islands throughout the Palmer Archipelago, Antarctica. The `penguins` dataset in palmerpenguins provides an approachable, charismatic, and near drop-in replacement for `iris` with topical relevance for polar climate change and environmental impacts on marine predators. Since the release on CRAN in July 2020, the palmerpenguins package has been downloaded over 462,000 times, highlighting the demand and widespread adoption of this viable `iris` alternative.  We directly compare the `iris` and `penguins` datasets for selected analyses to demonstrate that R users, in particular teachers and learners currently using `iris`, can switch to the Palmer Archipelago penguins for many use cases including data wrangling, visualization, linear modeling, multivariate analysis (e.g., PCA), cluster analysis and classification (e.g., by k-means).",
    "author": [
      {
        "name": "Allison M. Horst",
        "url": {}
      },
      {
        "name": "Alison Presmanes Hill",
        "url": {}
      },
      {
        "name": "Kristen B. Gorman",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\nIntroduction\n\nIn 1935, American botanist Edgar Anderson measured petal and sepal structural dimensions (length and width) for 50 flowers from three Iris species: Iris setosa, Iris versicolor, and Iris virginica (Anderson 1935). The manageable but non-trivial size (5 variables and 150 total observations) and characteristics of Anderson’s Iris dataset, including linear relationships and multivariate normality, have made it amenable for introducing a wide range of statistical methods including data wrangling, visualization, linear modeling, multivariate analyses, and machine learning. The Iris dataset is built into a number of software packages including the auto-installed datasets package in R (as iris, R Core Team 2021), Python’s scikit-learn machine learning library (Pedregosa et al. 2011), and the SAS Sashelp library (SAS Institute, Cary NC), which has facilitated its widespread use. As a result, eighty-six years after the data were initially published, the Iris dataset remains ubiquitous in statistics, computational methods, software documentation, and data science courses and materials.\nThere are a number of reasons that modern data science practitioners and educators may want to move on from iris. First, the dataset lacks metadata (Anderson 1935), which does not reinforce best practices and limits meaningful interpretation and discussion of research methods, analyses, and outcomes. Of the five variables in iris, two (Sepal.Width and Sepal.Length) are not intuitive for most non-botanists. Even with explanation, the difference between petal and sepal dimensions is not obvious. Second, iris contains equal sample sizes for each of the three species (n = 50) with no missing values, which is cleaner than most real-world data that learners are likely to encounter. Third, the single factor (Species) in iris limits options for analyses. Finally, due to its publication in the Annals of Eugenics by statistician R.A. Fisher (Fisher 1936), iris is burdened by a history in eugenics research, which we are committed to addressing through the development of new data science education products as described below.\nGiven the growing need for fresh data science-ready datasets, we sought to identify an alternative dataset that could be made easily accessible for a broad audience. After evaluating the positive and negative features of iris in data science and statistics materials, we established the following criteria for a suitable alternative:\nAvailable by appropriate license like a Creative Commons 0 license (CC0 “no rights reserved”)\nFeature intuitive subjects and variables that are interesting and understandable to learners across disciplines\nComplete metadata and documentation\nManageable (but not trivial) in size\nMinimal data cleaning and pre-processing required for most analyses\nReal-world (not manufactured) modern data\nProvides similar opportunities for teaching and learning R, data science, and statistical skills\nCan easily replace iris for most use cases\nHere, we describe an alternative to iris that largely satisfies these criteria: a refreshing, approachable, and charismatic dataset containing real-world body size measurements for three Pygoscelis penguin species that breed throughout the Western Antarctic Peninsula region, made available through the United States Long-Term Ecological Research (US LTER) Network. By comparing data structure, size, and a range of analyses side-by-side for the two datasets, we demonstrate that the Palmer Archipelago penguin data are an ideal substitute for iris for many use cases in statistics and data science education.\n\n\n\nFigure 1: The palmerpenguins package hex sticker designed by Allison Horst\n\n\n\nData source\nBody size measurements (bill length and depth, flipper length - flippers are the modified “wings” of penguins used for maneuvering in water, and body mass), clutch (i.e., egg laying) observations (e.g., date of first egg laid, and clutch completion), and carbon (13C/12C, \\(\\delta\\)13C) and nitrogen (15N/14N, \\(\\delta\\)15N) stable isotope values of red blood cells for adult male and female Adélie (P. adeliae), chinstrap (P. antarcticus), and gentoo (P. papua) penguins on three islands (Biscoe, Dream, and Torgersen) within the Palmer Archipelago were collected from 2007 - 2009 by Dr. Kristen Gorman in collaboration with the Palmer Station LTER, part of the US LTER Network. For complete data collection methods and published analyses, see Gorman et al. (2014). Throughout this paper, penguins species are referred to as “Adélie”, “Chinstrap”, and “Gentoo”.\nThe data in the palmerpenguins R package are available for use by CC0 license (“No Rights Reserved”) in accordance with the Palmer Station LTER Data Policy and the LTER Data Access Policy, and were imported from the Environmental Data Initiative (EDI) Data Portal at the links below:\nAdélie penguin data (Palmer Station Antarctica LTER and Gorman 2020a): KNB-LTER Data Package 219.5\nGentoo penguin data (Palmer Station Antarctica LTER and Gorman 2020c): KNB-LTER Data Package 220.5\nChinstrap penguin data (Palmer Station Antarctica LTER and Gorman 2020b): KNB-LTER Data Package 221.6\nThe palmerpenguins R package\nR users can install the palmerpenguins package from CRAN:\ninstall.packages(\"palmerpenguins\")\nInformation, examples, and links to community-contributed materials are available on the palmerpenguins package website: allisonhorst.github.io/palmerpenguins/. See the Appendix for how Python and Julia users can access the same data.\nThe palmerpenguins R package contains two data objects: penguins_raw and penguins. The penguins_raw data consists of all raw data for 17 variables, recorded completely or in part for 344 individual penguins, accessed directly from EDI (penguins_raw properties are summarized in Appendix B). We generally recommend using the curated data in penguins, which is a subset of penguins_raw retaining all 344 observations, minimally updated (Appendix A) and reduced to the following eight variables:\nspecies: a factor denoting the penguin species (Adélie, Chinstrap, or Gentoo)\nisland: a factor denoting the Palmer Archipelago island in Antarctica where each penguin was observed (Biscoe Point, Dream Island, or Torgersen Island)\nbill_length_mm: a number denoting length of the dorsal ridge of a penguin bill (millimeters)\nbill_depth_mm: a number denoting the depth of a penguin bill (millimeters)\nflipper_length_mm: an integer denoting the length of a penguin flipper (millimeters)\nbody_mass_g: an integer denoting the weight of a penguin’s body (grams)\nsex: a factor denoting the sex of a penguin sex (male, female) based on molecular data\nyear: an integer denoting the year of study (2007, 2008, or 2009)\nThe same data exist as comma-separated value (CSV) files in the package (“penguins_raw.csv” and “penguins.csv”), and can be read in using the built-in path_to_file() function in palmerpenguins. For example,\nlibrary(palmerpenguins)\ndf <- read.csv(path_to_file(\"penguins.csv\"))\nwill read in “penguins.csv” as if from an external file, thus automatically parsing species, island, and sex variables as characters instead of factors. This option allows users opportunities to practice or demonstrate reading in data from a CSV, then updating variable class (e.g., characters to factors).\nComparing iris and penguins\nThe penguins data in palmerpenguins is useful and approachable for data science and statistics education, and is uniquely well-suited to replace the iris dataset. Comparisons presented are selected examples for common iris uses, and are not exhaustive.\n\n\n\n\n\nTable 1: Overview comparison of penguins and iris dataset features and characteristics.\n\n\nFeature\n\n\niris\n\n\npenguins\n\n\nYear(s) collected\n\n\n1935\n\n\n2007 - 2009\n\n\nDimensions (col x row)\n\n\n5 x 150\n\n\n8 x 344\n\n\nDocumentation\n\n\nminimal\n\n\ncomplete metadata\n\n\nVariable classes\n\n\ndouble (4), factor (1)\n\n\ndouble (2), int (3), factor (3)\n\n\nMissing values?\n\n\nno (n = 0; 0.0%)\n\n\nyes (n = 19; 0.7%)\n\n\n\n\n\nData structure and sample size\nBoth iris and penguins are in tidy format (Wickham 2014) with each column denoting a single variable and each row containing measurements for a single iris flower or penguin, respectively. The two datasets are comparable in size: dimensions (columns × rows) are 5 × 150 and 8 × 344 for iris and penguins, respectively, and sample sizes within species are similar (Tables 1 & 2).\nNotably, while sample sizes in iris across species are all the same, sample sizes in penguins differ across the three species. The inclusion of three factor variables in penguins (species, island, and sex), along with year, create additional opportunities for grouping, faceting, and analysis compared to the single factor (Species) in iris.\nUnlike iris, which contains only complete cases, the penguins dataset contains a small number of missing values (nmissing = 19, out of 2,752 total values). Missing values and unequal sample sizes are common in real-world data, and create added learning opportunity to the penguins dataset.\n\n\n\n\n\nTable 2: Grouped sample size for iris (by species; n = 150 total) and penguins (by species and sex; n = 344 total). Data in penguins can be further grouped by island and study year.\n\n\n\niris sample size (by species)\n\n\n\n\npenguins sample size (by species and sex)\n\n\n\nIris species\n\n\nSample size\n\n\nPenguin species\n\n\nFemale\n\n\nMale\n\n\nNA\n\n\nsetosa\n\n\n50\n\n\nAdélie\n\n\n73\n\n\n73\n\n\n6\n\n\nversicolor\n\n\n50\n\n\nChinstrap\n\n\n34\n\n\n34\n\n\n0\n\n\nvirginica\n\n\n50\n\n\nGentoo\n\n\n58\n\n\n61\n\n\n5\n\n\n\n\n\nContinuous quantitative variables\nDistributions, relationships between variables, and clustering can be visually explored between species for the four structural size measurements in penguins (flipper length, body mass, bill length and depth; Figure 2) and iris (sepal width and length, petal width and length; Figure 3).\n\n\n\n\nFigure 2: Distributions and correlations for numeric variables in the penguins data (flipper length (mm), body mass (g), bill length (mm) and bill depth (mm)) for the three observed species: Gentoo (green, triangles); Chinstrap (blue, circles); and Adélie (orange, squares). Significance indicated for bivariate correlations: *p < 0.05; **p < 0.01; ***p < 0.001.\n\n\n\n\n\n\n\nFigure 3: Distributions and correlations for numeric variables in iris (petal length (cm), petal width (cm), sepal length (cm) and sepal width (cm)) for the three included iris species: Iris setosa (light gray, circles); Iris versicolor (dark gray, triangles); and Iris virginica (black, squares). Significance indicated for bivariate correlations: *p < 0.05; **p < 0.01; ***p < 0.001.\n\n\n\nBoth penguins and iris offer numerous opportunities to explore linear relationships and correlations, within and across species (Figures 2 & 3). A bivariate scatterplot made with the iris dataset reveals a clear linear relationship between petal length and petal width. Using penguins (Figure 4), we can create a uniquely similar scatterplot with flipper length and body mass. The overall trend across all three species is approximately linear for both iris and penguins. Teachers may encourage students to explore how simple linear regression results and predictions differ when the species variable is omitted, compared to, for example, multiple linear regression with species included (Figure 4).\n\n\n\n\nFigure 4: Representative linear relationships for (A): penguin flipper length (mm) and body mass (g) for Adélie (orange circles), Chinstrap (blue triangles), and Gentoo (green squares) penguins; (B): iris petal length (cm) and width (cm) for Iris setosa (light gray circles), Iris versicolor (dark gray triangles) and Iris virginica (black squares). Within-species linear model is visualized for each penguin or iris species.\n\n\n\nNotably, distinctions between species are clearer for iris petals - particularly, the much smaller petals for Iris setosa - compared to penguins, in which Adélie and Chinstrap penguins are largely overlapping in body size (body mass and flipper length), and are both generally smaller than Gentoo penguins.\nSimpson’s Paradox is a data phenomenon in which a trend observed between variables is reversed when data are pooled, omitting a meaningful variable. While often taught and discussed in statistics courses, finding a real-world and approachable example of Simpson’s Paradox can be a challenge. Here, we show one (of several possible - see Figure 2) Simpson’s Paradox example in penguins: exploring bill dimensions with and without species included (Figure 5). When penguin species is omitted (Figure 5A), bill length and depth appear negatively correlated overall. The trend is reversed when species is included, revealing an obviously positive correlation between bill length and bill depth within species (Figure 5B).\n\n\n\n\nFigure 5: Trends for penguin bill dimensions (bill length and bill depth, millimeters) if the species variable is excluded (A) or included (B), illustrating Simpson’s Paradox. Note: linear regression for bill dimensions without including species in (A) is ill-advised; the linear trendline is only included to visualize trend reversal for Simpson’s Paradox when compared to (B).\n\n\n\nPrincipal component analysis\nPrincipal component analysis (PCA) is a dimensional reduction method commonly used to explore patterns in multivariate data. The iris dataset frequently appears in PCA tutorials due to multivariate normality and clear interpretation of variable loadings and clustering.\nA comparison of PCA with the four variables of structural size measurements in penguins and iris (both normalized prior to PCA) reveals highly similar results (Figure 6). For both datasets, one species is distinct (Gentoo penguins, and setosa irises) while the other two species (Chinstrap/Adélie and versicolor/virginica) appear somewhat overlapping in the first two principal components (Figure 6 A,B). Screeplots reveal that the variance explained by each principal component (PC) is very similar across the two datasets, particularly for PC1 and PC2: for penguins, 88.15% of total variance is captured by the first two PCs, compared to 95.81% for iris, with a similarly large percentage of variance captured by PC1 and PC2 in each (Figure 6 C,D).\n\n\n\n\nFigure 6: Principal component analysis biplots and screeplots for structural size measurements in penguins (A,C) and iris (B,D), revealing similarities in multivariate patterns, variable loadings, and variance explained by each component. For penguins, variables are flipper length (mm), body mass (g), bill length (mm) and bill depth (mm); groups are visualized by species (Adélie = orange circles, Chinstrap = blue triangles, Gentoo = green squares). For iris, variables are petal length (cm), petal width (cm), sepal length (cm) and sepal width (cm); groups are visualized by species (Iris setosa = light gray circles, Iris versicolor = dark gray triangles, Iris virginica = black squares). Values above screeplot columns (C,D) indicate percent of total variance explained by each of the four principal components.\n\n\n\nK-means clustering\nUnsupervised clustering by k-means is a common and popular entryway to machine learning and classification, and again, the iris dataset is frequently used in introductory examples. The penguins data provides similar opportunities for introducing k-means clustering. For simplicity, we compare k-means clustering using only two variables for each dataset: for iris, petal width and petal length, and for penguins, bill length and bill depth. All variables are scaled prior to k-means. Three clusters (k = 3) are specified for each, since there are three species of irises (Iris setosa, Iris versicolor, and Iris virginica) and penguins (Adélie, Chinstrap and Gentoo).\nK-means clustering with penguin bill dimensions and iris petal dimensions yields largely distinct clusters, each dominated by one species (Figure 7). For iris petal dimensions, k-means yields a perfectly separated cluster (Cluster 3) containing all 50 Iris setosa observations and zero misclassified Iris virginica or Iris versicolor (Table 3). While clustering is not perfectly distinct for any penguin species, each species is largely contained within a single cluster, with little overlap from the other two species. For example, considering Adélie penguins (orange observations in Figure 7A): 147 (out of 151) Adélie penguins are assigned to Cluster 3, zero are assigned to Cluster 1, and 4 are assigned to the Chinstrap-dominated Cluster 2 (Table 3). Only 5 (of 68) Chinstrap penguins and 1 (of 123) Gentoo penguins are assigned to the Adélie-dominated Cluster 3 (Table 3).\n\n\n\n\nFigure 7: K-means clustering outcomes for penguin bill dimensions (A) and iris petal dimensions (B). Numbers indicate the cluster to which an observation was assigned, revealing a high degree of separation between species for both penguins and iris. Penguin species (Adélie = orange, Chinstrap = blue, Gentoo = green) and iris species (setosa = light gray, versicolor = medium gray, virginica = dark gray), along with bill dimensions and cluster number, are included in the tooltip when hovering.\n\n\n\n\n\nTable 3: K-means cluster assignments by species based on penguin bill length (mm) and depth (mm), and iris petal length (cm) and width (cm).\n\n\n\nPenguins cluster assignments\n\n\n\n\nIris cluster assignments\n\n\n\nCluster\n\n\nAdélie\n\n\nChinstrap\n\n\nGentoo\n\n\nCluster\n\n\nsetosa\n\n\nversicolor\n\n\nvirginica\n\n\n1\n\n\n0\n\n\n9\n\n\n116\n\n\n1\n\n\n0\n\n\n2\n\n\n46\n\n\n2\n\n\n4\n\n\n54\n\n\n6\n\n\n2\n\n\n0\n\n\n48\n\n\n4\n\n\n3\n\n\n147\n\n\n5\n\n\n1\n\n\n3\n\n\n50\n\n\n0\n\n\n0\n\n\nConclusion\nHere, we have shown that structural size measurements for Palmer Archipelago Pygoscelis penguins, available as penguins in the palmerpenguins R package, offer a near drop-in replacement for iris in a number of common use cases for data science and statistics education including exploratory data visualization, linear correlation and regression, PCA, and clustering by k-means. In addition, teaching and learning opportunities in penguins are increased due to a greater number of variables, missing values, unequal sample sizes, and Simpson’s Paradox examples. Importantly, the penguins dataset encompasses real-world information derived from several charismatic marine predator species with regional breeding populations notably responding to environmental change occurring throughout the Western Antarctic Peninsula region of the Southern Ocean (see Bestelmeyer et al. (2011), Gorman et al. (2014), Gorman et al. (2017), Gorman et al. (2021)). Thus, the penguins dataset can facilitate discussions more broadly on biodiversity responses to global change - a contemporary and critical topic in ecology, evolution, and the environmental sciences.\nPenguins data processing\nData in the penguins object have been minimally updated from penguins_raw as follows:\nAll variable names are converted to lower snake case (e.g. from Flipper Length (mm) to flipper_length_mm)\nEntries in species are truncated to only include the common name (e.g. “Gentoo”, instead of “gentoo penguin (Pygoscelis papua)”)\nRecorded sex for penguin N36A1, originally recorded as “.”, is updated to NA\nculmen_length_mm and culmen_depth_mm variable names are updated to bill_length_mm and bill_depth_mm, respectively\nClass for categorical variables (species, island, sex) is updated to factor\nVariable year was pulled from clutch observations\nSummary of the penguins_raw dataset\n\n\nFeature\n\n\npenguins_raw\n\n\nYear(s) collected\n\n\n2007 - 2009\n\n\nDimensions (col x row)\n\n\n17 x 344\n\n\nDocumentation\n\n\ncomplete metadata\n\n\nVariable classes\n\n\ncharacter (9), Date (1), numeric (7)\n\n\nMissing values?\n\n\nyes (n = 336; 5.7%)\n\n\npalmerpenguins for other programming languages\nPython: Python users can load the palmerpenguins datasets into their Python environment using the following code to install and access data in the palmerpenguins Python package:\npip install palmerpenguins\nfrom palmerpenguins import load_penguins\npenguins = load_penguins()\nJulia: Julia users can access the penguins data in the PalmerPenguins.jl package. Example code to import the penguins data through PalmerPenguins.jl (more information on PalmerPenguins.jl from David Widmann can be found here):\njulia> using PalmerPenguins\njulia> table = PalmerPenguins.load()\nTensorFlow: TensorFlow users can access the penguins data in TensorFlow Datasets. Information and examples for penguins data in TensorFlow can be found here.\nAcknowledgements\nAll analyses were performed in the R language environment using version 4.1.2 (R Core Team 2021). Complete code for this paper is shared in the Supplemental Material. We acknowledge the following R packages used in analyses, with gratitude to developers and contributors:\nGGally (Schloerke et al. 2021): for pairs plots\nggiraph (Gohel and Skintzos 2022): for interactive ggplot2 graphics\nggplot2 (Wickham et al. 2021): for data visualizations\nkableExtra (Zhu 2021): for finalized tables\npaletteer (Hvitfeldt 2021): for the Okabe Ito color palette, provided by the colorblindr package\npatchwork (Pedersen 2020): for compound figures\nplotly (Sievert et al. 2021): for interactive graphics\nrecipes (Kuhn and Wickham 2021) and broom (Robinson et al. 2022): for modeling\nshadowtext (Yu 2022): to add a background color to text labels\ntidyverse (Wickham et al. 2019): for data import and cleaning\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-020.zip\nCRAN packages used\ndatasets, palmerpenguins, GGally, ggiraph, ggplot2, kableExtra, paletteer, colorblindr, patchwork, plotly, recipes, broom, shadowtext, tidyverse\nCRAN Task Views implied by cited packages\nSpatial, TeachingStatistics, WebTechnologies\n\n\nE. Anderson. The irises of the Gaspé Peninsula. Bulletin of the American Iris Society, 59: 2–5, 1935.\n\n\nB. T. Bestelmeyer, A. M. Ellison, W. R. Fraser, K. B. Gorman, S. J. Holbrook, C. M. Laney, M. D. Ohman, D. P. C. Peters, F. C. Pillsbury, A. Rassweiler, et al. Analysis of abrupt transitions in ecological systems. Ecosphere, 2(12): art129, 2011. URL http://doi.wiley.com/10.1890/ES11-00216.1 [online; last accessed March 27, 2021].\n\n\nR. A. Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2): 179–188, 1936. URL http://doi.wiley.com/10.1111/j.1469-1809.1936.tb02137.x [online; last accessed July 1, 2020].\n\n\nD. Gohel and P. Skintzos. Ggiraph: Make ’ggplot2’ graphics interactive. 2022. URL https://CRAN.R-project.org/package=ggiraph. R package version 0.8.2.\n\n\nK. B. Gorman, K. E. Ruck, T. D. Williams and W. R. Fraser. Advancing the Sea Ice Hypothesis: Trophic Interactions Among Breeding pygoscelis Penguins With Divergent Population Trends Throughout the Western Antarctic Peninsula. Frontiers in Marine Science, 8: 526092, 2021. URL https://www.frontiersin.org/articles/10.3389/fmars.2021.526092/full [online; last accessed September 25, 2021].\n\n\nK. B. Gorman, S. L. Talbot, S. A. Sonsthagen, G. K. Sage, M. C. Gravely, W. R. Fraser and T. D. Williams. Population genetic structure and gene flow of Adélie penguins (Pygoscelis adeliae) breeding throughout the western Antarctic Peninsula. Antarctic Science, 29(6): 499–510, 2017. URL https://www.cambridge.org/core/product/identifier/S0954102017000293/type/journal_article [online; last accessed March 27, 2021].\n\n\nK. B. Gorman, T. D. Williams and W. R. Fraser. Ecological Sexual Dimorphism and Environmental Variability within a Community of Antarctic Penguins (Genus pygoscelis). PLoS ONE, 9(3): e90081, 2014. URL https://dx.plos.org/10.1371/journal.pone.0090081 [online; last accessed July 1, 2020].\n\n\nA. Horst, A. Hill and K. Gorman. Palmerpenguins: Palmer archipelago (antarctica) penguin data. 2020. URL https://CRAN.R-project.org/package=palmerpenguins. R package version 0.1.0.\n\n\nE. Hvitfeldt. Paletteer: Comprehensive collection of color palettes. 2021. URL https://github.com/EmilHvitfeldt/paletteer. R package version 1.3.0.\n\n\nM. Kuhn and H. Wickham. Recipes: Preprocessing and feature engineering steps for modeling. 2021. URL https://CRAN.R-project.org/package=recipes. R package version 0.1.17.\n\n\nPalmer Station Antarctica LTER and K. B. Gorman. Structural size measurements and isotopic signatures of foraging among adult male and female Adélie penguins (Pygoscelis adeliae) nesting along the Palmer Archipelago near Palmer Station, 2007-2009. 2020a. URL https://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-pal.219.5 [online; last accessed July 1, 2020].\n\n\nPalmer Station Antarctica LTER and K. B. Gorman. Structural size measurements and isotopic signatures of foraging among adult male and female Chinstrap penguin (Pygoscelis antarctica) nesting along the Palmer Archipelago near Palmer Station, 2007-2009. 2020b. URL https://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-pal.221.6 [online; last accessed July 1, 2020].\n\n\nPalmer Station Antarctica LTER and K. B. Gorman. Structural size measurements and isotopic signatures of foraging among adult male and female Gentoo penguin (Pygoscelis papua) nesting along the Palmer Archipelago near Palmer Station, 2007-2009. 2020c. URL https://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-pal.220.5 [online; last accessed July 1, 2020].\n\n\nT. L. Pedersen. Patchwork: The composer of plots. 2020. URL https://CRAN.R-project.org/package=patchwork. R package version 1.1.1.\n\n\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12: 2825–2830, 2011.\n\n\nR Core Team. R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing, 2021. URL https://www.R-project.org/.\n\n\nD. Robinson, A. Hayes and S. Couch. Broom: Convert statistical objects into tidy tibbles. 2022. URL https://CRAN.R-project.org/package=broom. R package version 0.7.11.\n\n\nB. Schloerke, D. Cook, J. Larmarange, F. Briatte, M. Marbach, E. Thoen, A. Elberg and J. Crowley. GGally: Extension to ggplot2. 2021. URL https://CRAN.R-project.org/package=GGally. R package version 2.1.2.\n\n\nC. Sievert, C. Parmer, T. Hocking, S. Chamberlain, K. Ram, M. Corvellec and P. Despouy. Plotly: Create interactive web graphics via plotly.js. 2021. URL https://CRAN.R-project.org/package=plotly. R package version 4.10.0.\n\n\nH. Wickham. Tidy Data. Journal of Statistical Software, 59(10): 2014. URL http://www.jstatsoft.org/v59/i10/ [online; last accessed July 1, 2020].\n\n\nH. Wickham, M. Averick, J. Bryan, W. Chang, L. D. McGowan, R. François, G. Grolemund, A. Hayes, L. Henry, J. Hester, et al. Welcome to the tidyverse. Journal of Open Source Software, 4(43): 1686, 2019. DOI 10.21105/joss.01686.\n\n\nH. Wickham, W. Chang, L. Henry, T. L. Pedersen, K. Takahashi, C. Wilke, K. Woo, H. Yutani and D. Dunnington. ggplot2: Create elegant data visualisations using the grammar of graphics. 2021. URL https://CRAN.R-project.org/package=ggplot2. R package version 3.3.5.\n\n\nG. Yu. Shadowtext: Shadow text grob and layer. 2022. URL https://github.com/GuangchuangYu/shadowtext/. R package version 0.1.1.\n\n\nH. Zhu. kableExtra: Construct complex table with kable and pipe syntax. 2021. URL https://CRAN.R-project.org/package=kableExtra. R package version 1.3.4.\n\n\n\n\n",
    "preview": "https://allisonhorst.github.io/palmerpenguins/logo.png",
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2022-021/",
    "title": "Advancing Reproducible Research by Publishing R Markdown Notebooks as Interactive Sandboxes Using the learnr Package",
    "description": "Various R packages and best practices have played a pivotal role to promote the Findability, Accessibility, Interoperability, and Reuse (FAIR) principles of open science. For example, (1) well-documented R scripts and notebooks with rich narratives are deposited at a trusted data centre, (2) R Markdown interactive notebooks can be run on-demand as a web service, and (3) R Shiny web apps provide nice user interfaces to explore research outputs. However, notebooks require users to go through the entire analysis, while Shiny apps do not expose the underlying code and require extra work for UI design. We propose using the learnr package to expose certain code chunks in R Markdown so that users can readily experiment with them in guided, editable, isolated, executable, and resettable code sandboxes. Our approach does not replace the existing use of notebooks and Shiny apps, but it adds another level of abstraction between them to promote reproducible science.",
    "author": [
      {
        "name": "Chak Hau Michael Tso",
        "url": {}
      },
      {
        "name": "Michael Hollaway",
        "url": {}
      },
      {
        "name": "Rebecca Killick",
        "url": {}
      },
      {
        "name": "Peter Henrys",
        "url": {}
      },
      {
        "name": "Don Monteith",
        "url": {}
      },
      {
        "name": "John Watkins",
        "url": {}
      },
      {
        "name": "Gordon Blair",
        "url": {}
      }
    ],
    "date": "2022-06-21",
    "categories": [],
    "contents": "\nIntroduction\nThere has been considerable recognition of the need to promote open and reproducible science in the past decade. The FAIR principles (Wilkinson et al. 2016; Stall et al. 2019) (https://www.go-fair.org/fair-principles/) of reproducible research are now known to most scientists. While significant advances has been made through the adoption of various best practices and policies (e.g. requirements from funders and publishers to archive data and source code, metadata standards), there remains considerable barriers to further advance open science and meet reproducible science needs. One of such issues the availability of various levels of abstraction of the same underlying analysis and code base to collaborate and engage with different stakeholders of diverse needs (Blair et al. 2019; Hollaway et al. 2020). For complex analysis or analysis that utilize a more advanced computing environment, it is essential to provide the capability to allow users to interact with the analysis at a higher level.\nExisting approach to reproducible research focuses on either documenting an entire analysis or allows user-friendly interaction. Within the R ecosystem, R scripts and notebooks allow researchers to work together and to view the entire workflow, while R Shiny apps (Chang et al. 2019) allows rapid showcase of methods and research outcomes to users with less experience. R Shiny has been widely adopted to share research output and engage stakeholders since its conception in 2013. A recent review (Kasprzak et al. 2021) shows that bioinformatics is the subject with the most Shiny apps published in journals while earth and environmental science ranks second. Shiny apps are especially helpful to create reproducible analysis (e.g. examples in Hollaway et al. 2020) and explore different scenarios (e.g. Whateley et al. 2015; Mose et al. 2018). Finally, the interactivity of Shiny apps makes it an excellent tool for teaching (e.g. Williams and Williams 2017; Field 2020). However, not all users fit nicely into this dichotomy. Some users may only want to adopt a small fraction of an analysis for their work, while others may simply want to modify a few parts of the analysis in order to test alternative hypothesis. Current use of notebooks do not seem to support such diverse needs as notebook output (e.g. figures and tables) are not easily reproducible. This issue essentially applies to all coding languages.\nOne potential way to address the problem described above is to allow\nusers to experiment with the code in protected computing environment.\nThis is not limited to creating instances for users to re-run the entire\ncode. Rather, this can also be done by exposing specific parts of a\nnotebook as editable and executable code boxes, as seen in many\ninteractive tutorial web pages for various coding languages. Recently,\nwhile discussing next steps for fostering reproducible research in\nartificial intelligence, Carter et al. (2019) lists creating a protected\ncomputing environment (‘data enclave’ or ‘sandbox’) for reviewers to log\nin and explore as one of the solutions. In software engineering, a\nsandbox is a testing environment that isolates untested code changes and\noutright experimentation from the production environment or repository,\nin the context of software development including Web development and\nrevision control. Making a sandbox environment available for users to\ntest and explore various changes to the code that leads to research\noutputs is a great step to further open science. Current practice of\nopen science largely requires users to assemble the notebooks, scripts\nand data files provided in their own computing environment, which\nrequires significant amount of time and effort. A sandbox environment\ncan greatly reduce such barriers and if such sandboxes are available as\na web service, users can explore and interact with the code that\ngenerates the research outputs at the convenience of their own web browser\non demand.\nIn this paper, we describe a rapid approach to create and publish\n‘interactive sandboxes’ R Shiny apps from R Markdown documents using\nthe learnr package, with the aim to bridge the gap between\ntypical R Markdown notebook and typical Shiny apps in terms of levels of\nabstraction. While code and markdown documents gives full details of the\ncode, standard R Shiny apps has too much limitations on users to\ninteract with the code and users often cannot see the underlying code.\nOur approach allows users to interact with selected parts of the code in\nan isolated manner, by specifying certain code chunks in a R Markdown\ndocument as executable code boxes.\nThe learnr R package\nlearnr (Schloerke et al. 2020) is an R package developed by RStudio to\nrapidly create interactive tutorials. It follows the general (the file has .Rmd extensions,\nhttps://rmarkdown.rstudio.com/index.html) architecture and\nessentially creates a pre-rendered Shiny document similar to the way\nShiny user interface (UI) components can be added to any R Markdown documents.\nPre-rendered Shiny documents\n(https://rmarkdown.rstudio.com/authoring_shiny_prerendered.HTML)\nis a key enabling technology for the learnr package since it\nallows users to specify the execution context in each code chunk of a R\nMarkdown document that is used to render a R Shiny web app. Its use\ncircumvents the need of a full document render for each end user browser\nsession so that this type of R Shiny apps can load quickly. To create a\nlearnr tutorial in RStudio after learnr is\ninstalled, the user chooses a learnr R Markdown template from\na list after clicking the “create new R Markdown document” button. This\ntemplate is not different from other .Rmd files, except it requires\nadditional chunk arguments to control the sandbox appearances. The two\nmain features of the learnr package are the “exercise” and\n“quiz” options. The former allows users to directly type in code,\nexecute it, and see its results to test their knowledge while the latter\nallows other question types such as multiple choice. Both of these\noptions include auto-graders, hints, and instructor feedback options.\nAdditional overall options include setting time limits and an option to\nforbid users to skip sections. Like any Shiny apps, learnr\napps can be easily embedded to other web pages, as seen in Higgins (2021).\nAlthough the learnr package has existed for a few years now,\nit is relatively not well known to scientists as a potential use of R\nShiny Apps and it has mostly been used for simple tutorial apps designed\nfor R beginners. We propose a novel application of the learnr\npackage to advance reproducible research, which we outline in the next\nsection.\nApproach: Using learnr for reproducible research ‘sandboxes’\nlearnr allows users to create executable code boxes. Our\napproach is to publish R notebooks and serve parts of the notebooks as\ninteractive sandboxes to allow users to re-create certain elements of a\npublished notebook containing research outputs. We do not use the\nauto-graders or any quiz-like functionality of learnr while\nkeeping the sandboxes. Notebook authors can go through their notebook\nand select the code chunks that they would allow users to experiment,\nwhile the others are rendered as static code snippets.\nRecognizing learnr documents are themselves R Shiny web apps,\nour approach essentially allows the publication of notebooks in the form\nof web apps. However, unlike a typical R Shiny web app, users do not\nneed to prepare a separate UI (i.e. user interface) layout. Advanced\nusers can modify the site appearance by supplying custom design in .css\nfiles.\nHere, we first show the skeleton of a R Markdown (.Rmd) file for a\nlearnr document (Figure 1). Notice that it is very similar to\na typical .Rmd file where there is a mixture of narratives written in\nmarkdown and R code chunks, in addition to a YAML header. However,\nthere are a couple of important exceptions, namely the use of the\n“exercise” chunk option (i.e. editable and executable code boxes)\nand different output type in the YAML header.\nNext, we outline the steps an author needs to take to publish notebooks (i.e. R Markdown documents) as interactive sandboxes:\nAll research output is included in the form of a well-documented R Markdown document.\nOpen a new learnr R Markdown template. Copy the content of the original notebook.\nFor the code chunks that you would like to become sandboxes, add exercise=TRUE. Make sure it has a unique chunk name. It may look something like this:\n```{r fig2, warning=FALSE, exercise=TRUE, exercise.lines=30,fig.fullwidth=TRUE}\nBefore any interactive code chunks, call the first code chunk ‘setup’.\nThis will pre-load everything that will be used later.\nCheck whether you would like to link any of the interactive code\nsnippets (by default each of them are independent, and only depends on\nthe output of the ‘setup’ chunk) You may want to modify your code\nchunks accordingly.\nDone! Knit the notebook to view outputs as an interactive web page.\nPublish it just like a Shiny app.\nThe entire process took us a few hours of effort and can be incorporated\nto the proof-reading of an R Markdown document. However, we note that as\nin any preparation of research output or web development several\niterations are often needed and the time required increases accordingly\nas the complexity of the analysis increases.\n\n\n\nFigure 1: A comparison of minimal examples of a typical .Rmd document and a .Rmd document for an interactive sandbox app.\n\n\n\nIn our implementation in DataLabs\n(https://datalab.datalabs.ceh.ac.uk/), the environment and folder\nto create the research is made available to the Shiny app in a read-only\nfashion. Therefore, the authors do not have to worry about versions of\npackages of data or a different software setup. Using DataLabs\nstraightforward visual tools to publish R Shiny apps, we can publish an\nR Markdown notebook with interactive code snippets to reproduce certain\nparts of research readily in a few clicks.\nDeployment\nIn general, learnr tutorial apps can be published the same way\nas R Shiny web apps in Shiny servers, such as the ones provided by cloud\nservice providers or https://shinyapps.io. The learnr\npackage vignettes provide additional help on deployment.\nWe also describe our deployment of these apps in DataLabs, a UK NERC\nvirtual research environment that is being developed. DataLabs is a\ncollaborative virtual research environment (Hollaway et al. 2020) (https://datalab.datalabs.ceh.ac.uk/) for environmental scientist\nto work together where data, software, and methods are all centrally\nlocated in projects. DataLabs provide a space for scientists from\ndifferent domains (data science, statisticians, environmental science\nand computer science) to work together and draw on each other’s\nexpertise. It includes an easy-to-use user interface where users can\npublish R Shiny apps with a few clicks, and this applies to these\nnotebooks with interactive code chunks as well. Importantly, when\nprovisioning a instance of R Shiny, this is deployed in a Docker\ncontainer with read-only access to the project data store being used for\nanalysis. This allows an unprecedented level of transparency as parts of\nthe analysis are readily exposed for users to experiment from the exact\nenvironments, datasets (can be large and includes many files), and\nversions of software that created the analysis. The use of Docker\ndeployed onto a Kubernetes infrastructure allows strict limits to be\nplaced on what visitors can do through the use of resource constraints\nand tools such as RAppArmor (Ooms 2013). While access to\nproject files is read-only, some author discretion is still advised to\nensure that visitors should not be able to view or list any private code\nor data. We also note that future releases of learnr will\ncontain external exercise evaluators, so that the code sandboxes can be\nexecuted by an independent engine (such as Google Cloud) and give the\nbenefit of not having to rely on RAppArmor.\nExample: GB rainfall paper\nTo demonstrate our concept, we have turned an R Markdown notebook for one of our recent papers (Tso et al. 2022) into a learnr site\n(https://cptecn-sandboxdemo.datalabs.ceh.ac.uk/) using the\nprocedures described in the previous sections. The paper investigates\nthe effect of weather and rainfall types on rainfall chemistry in the\nUK. As can be seen in Figure 2, the code chunks to generate certain\nparts of the paper is exposed. But unlike a static notebook site, the\ncode chunk is not only available for copy and paste but allows users to\nmodify and run on-demand. This makes it very straightforward for user to\nexperiment with various changes of the original analysis, thereby\npromoting transparency and trust.\nSince learnr apps are R Markdown documents, Shiny UI elements\ncan be easily added. We repeat one of the examples by replacing the\ninteractive code box by a simple selector, with minimal modification of\nthe code itself. This approach to publish Shiny apps requires\nsignificantly less work than typical R Shiny web apps since no UI design\nis needed and researchers can rapidly turn an R Markdown document to an R\nShiny web app. For some cases, the use of certain datasets may require a\nlicense, as in this example. A pop-up box is shown when the site is\nloaded and visitors are required to check the boxes to acknowledge the\nuse of the appropriate data licenses (an alternative is to require\nusers to register and load a token file) before they can proceed.\n\n\n\nFigure 2: A screenshot of the GB rainfall interactive notebook site. The main feature is the code box. When the site loads, the code that generates the published version of the figure is in the box and the published version of the figure is below it. Users can make edits and re-run the code in the code box and the figure will update accordingly. Users can use the “Start Over” button to see the published version of the code at any point without refreshing the entire site.\n\n\n\nEvaluation\nThe main strength of our approach is that it fills nicely the gap of\nexisting approaches in terms of levels of abstraction. While code and\nmarkdown documents gives full details of the code, standard R Shiny apps\nhas too much limitations on users to interact with the code (Figure 3)\nand users often cannot see the underlying code. Recently, it has become\npopular to publish ‘live’ Jupyter notebooks on Binder and Google Colab.\nWhile this is a great contribution to open science, users are still\nrequired to run and go through the entire notebook step-by-step and\nit can be easy to break it if users change something in between. Our\napproach allows users to interact with portions of the code in a guided\nand isolated manner, without the need to understand all the other parts\nof a notebook or the fear to break it (Table 1). We emphasize that R\nscripts/notebooks and R Shiny apps work well for their intended uses,\nbut our approach adds an additional level of accessibility to users.\nThe openness and ease-to-access our approach provides can benefit many\ndifferent stakeholders (Table 2). Researchers can more rapidly reproduce\nof the analysis of their choice without studying the entire\nnotebook or installing software or downloading all the data. They can\nquickly test alternative hypothesis and stimulate scientific\ndiscussions. For funders, encouraging the use of this approach means\nless time is needed for future projects to pick up results from previous\nwork. And since this is based on learnr which is originally\ndesigned as a tutorial tool, this approach will no doubt speed up the\nprocess to train other users to use similar methods. Overall, it\npromotes open science and make a better value of public funds.\nAn obvious limitation of our approach is that it does not work well for\nideal conditions where other R file formats are designed for. For\ninstance, R scripts and R notebooks are much better suited for more\ncomplex analysis for users to adopt to their own problems. Meanwhile, R\nShiny web apps provides a much richer user experience and is most suited\nwhen the exposed code is generally not useful to stakeholders.\nNevertheless, as discussed above, our approach is designed for users to\nreproduce of an analysis. The user should evaluate these\noptions carefully, paying special attention to the needs of intended\nusers.\nServing notebooks as a web service will inevitably face provenance\nissues. It is surely beneficial if the author’s institution can host\nthese interactive notebooks for a few years after its publication (and\nthat of its related publications). In the future, publishers and data\ncentres may consider providing services to provide longer term\nprovenance of serving these interactive notebooks online. As for any web\napps, funding for the computation servers can be a potential issue. This\nwork uses DataLabs computation time which is part of the UK research\nfunding that develops it. However, a more rigorous funding model may be\nneeded in the future to ensure provenance of these notebooks.\nOur approach focuses on improving reproducibility by exposing parts of R\nscript for users to run them live on an R Shiny web app, leveraging the\noption to render R Markdown documents as R Shiny web apps and the\nlearnr package. It focuses on the R scripts and R Markdown\ndocuments. Users, however, may want to improve reproducibility from the\nopposite direction, namely to allow outputs from an R Shiny web app to\nbe reproducible outside of the Shiny context. For such a requirement, we\nrecommend the use of the shinymeta package,\nwhich allows users to capture the underlying code of selected output\nelements and allows users to download it as well as the underlying data\nto re-create the output in their own R instance. The shinymeta\napproach can be more involved and requires more effort than\nlearnr so we think it is more suitable for users that are\nfocusing their effort on the R Shiny app (particularly the UI). In\nsummary, these two approaches complements each other and we recommend\nusers to consider them to improve reproducibility of their work.\n\n\n\nFigure 3: The various levels of abstraction of various types of R documents. Our approach fills nicely the gap between R Markdown or Jupyter notebooks and Shiny apps.\n\n\n\n\n\n\n\n\n\n\n\nTable 1: Advantages of the proposed approach to various stakeholders\n\n\nAdvantages\n\n\nAuthors\n\n\nVery little extra work required in additional to writing R markdown document.\n\n\nNo experience to generate web interfaces required.\n\n\nMuch greater impact in research output.\n\n\nOther researchers (those wanting to try or compare the method)\n\n\nA much more enriched experience to try methods and data and to test alternative hypothesis and scenarios.\n\n\nNo need to download data and scripts/notebooks and install packages to try a method.\n\n\nMore efficient to learn the new method.\n\n\nOther researchers (those curious about the results)\n\n\nTry running different scenarios quickly than the published ones without the hassle of full knowledge of the code, downloading the code and data, and setting up the software environment.\n\n\nQuickly reset to the published version of code snippet.\n\n\nNo need to worry about breaking the code.\n\n\nData Centres\n\n\nA new avenue to demonstrate impact to funders if end users try methods or datasets hosted by them in sandboxes.\n\n\nFunders\n\n\nBetter value of investment if even small parts of a research is readily reproducible.\n\n\nTime saving to fund related work that builds on research documented this way.\n\n\nWider research community and general public\n\n\nPromotes trust and confidence in research through transparency.\n\n\nSummary and outlook\nWe have proposed and demonstrated a rapid approach to publish R Markdown\nnotebooks as interactive sandboxes to allow users to experiment with\nchanges with various elements of a research output. It provides an\nadditional level of abstraction for users to interact with research\noutputs and the codes that generates down. Since it can be linked to the\nenvironment and data that generated the published output and has\nindependent document object identifiers (DOI), it is a suitable\ncandidate to preserve research workflow while exposing parts of it to\nallow rapid experimentation by users. Our work is a demonstration on how\nwe may publish a notebook from virtual research environments such as\nDataLabs, with data, packages, and workflow pre-loaded in a coding\nenvironment, accompanied by rich narratives. While this paper outlines\nthe approach using R, the same approach can benefit other coding\nlanguages such as Python. In fact, this can already be achieved as\nlearnr can run Python chunks (as well as other execution\nengines knitr supports such as SAS and mySQL) as long as the\nusers generate and host the document using R. This paper contributes to\nthe vision towards publishing interactive notebooks as standalone\nresearch outputs and the advancement of open science practices.\nData availability and acknowledgements\nThe GB rainfall example notebook is accessible via this URL\n(https://cptecn-sandboxdemo.datalabs.ceh.ac.uk/) and the R\nMarkdown file is deposited in the NERC Environmental Information Data\nCentre (EIDC) (Tso 2022). The DataLab code stack is available at\nhttps://github.com/NERC-CEH/datalab. We thank the DataLabs\ndevelopers team (especially Iain Walmsley, UKCEH) for the assistance to\ndeploy interactive R Markdown documents on DataLabs. This work is\nsupported by NERC Grant NE/T006102/1, Methodologically Enhanced Virtual\nLabs for Early Warning of Significant or Catastrophic Change in\nEcosystems: Changepoints for a Changing Planet, funded under the\nConstructing a Digital Environment Strategic Priority Fund. Additional\nsupport is provided by the UK Status, Change and Projections of the\nEnvironment (UK-SCAPE) programme started in 2018 and is funded by the\nNatural Environment Research Council (NERC) as National Capability\n(award number NE/R016429/1). The initial development work of DataLabs\nwas supported by a NERC Capital bid as part of the Environmental Data\nServices (EDS).\n\n\n\n\n\n\n\n\nTable 2: Advantages of the proposed approach over existing approaches\n\n\nPotential Issues\n\n\nHow our approach can help?\n\n\nR script\n\n\nLimited narrative. Needs to run all the scripts.\n\n\nMuch richer narrative and interactive experience.\n\n\nStatic notebooks\n\n\nNeeds to download the code, data and package to try it out.\n\n\nCan instantly try out the code in a controlled manner, using the published data/packages/software environment.\n\n\nWeb apps (e.g. Shiny)\n\n\nWhile web apps helpful to some stakeholders, it can be too high-level to some.\n\n\nUsers can interact with the code within the code snippet sandboxes themselves.\n\n\nLots of extra work to create web interface.\n\n\nThe published version of the code is shown to users.\n\n\nDoes not expose the code to generate results.\n\n\nUsers can run the code snippets live.\n\n\nBinder/Google Colab\n\n\nUsers change the entire notebook.\n\n\nA much more enriched and guided experience.\n\n\nUsers need to run all cells about the section they are interested in.\n\n\nUsers can choose to only run the sandboxes they are interested in.\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2022-021.zip\nCRAN packages used\nRAppArmor, knitr\nCRAN Task Views implied by cited packages\nReproducibleResearch\n\n\nG. S. Blair, P. Henrys, A. Leeson, J. Watkins, E. Eastoe, S. Jarvis and P. J. Young. Data Science of the Natural Environment: A Research Roadmap. Frontiers in Environmental Science, 7: 2019. URL https://www.frontiersin.org/article/10.3389/fenvs.2019.00121/full.\n\n\nR. E. Carter, Z. I. Attia, F. Lopez-Jimenez and P. A. Friedman. Pragmatic considerations for fostering reproducible research in artificial intelligence. npj Digital Medicine, 2(1): 2019. URL https://doi.org/10.1038/s41746-019-0120-2.\n\n\nW. Chang, J. Cheng, J. Allaire, Y. Xie and J. McPherson. Shiny: Web application framework for r. 2019. URL https://CRAN.R-project.org/package=shiny. R package version 1.4.0.\n\n\nA. Field. adventr: Interactive R Tutorials to Accompany Field (2016), \"An Adventure in Statistics\". 2020. URL https://cran.r-project.org/web/packages/adventr/index.html.\n\n\nP. Higgins. Reproducible medical research with r. 2021. URL https://bookdown.org/pdr_higgins/rmrwr/.\n\n\nM. J. Hollaway, G. Dean, G. S. Blair, M. Brown, P. A. Henrys and J. Watkins. Tackling the challenges of 21st-century open science and beyond: A data science lab approach. Patterns, 1(7): 100103, 2020. URL https://doi.org/10.1016/j.patter.2020.100103.\n\n\nP. Kasprzak, L. Mitchell, O. Kravchuk and A. Timmins. Six years of shiny in research - collaborative development of web tools in R. R Journal, (3): 155–162, 2021. URL https://journal.r-project.org/archive/2021/RJ-2021-009/RJ-2021-009.pdf.\n\n\nV. N. Mose, D. Western and P. Tyrrell. Application of open source tools for biodiversity conservation and natural resource management in east africa. Ecological Informatics, 47: 35–44, 2018. URL https://doi.org/10.1016/j.ecoinf.2017.09.006.\n\n\nJ. Ooms. The RAppArmor package: Enforcing security policies in R using dynamic sandboxing on linux. Journal of Statistical Software, 55(7): 1–34, 2013. URL http://www.jstatsoft.org/v55/i07/.\n\n\nB. Schloerke, J. Allaire and B. Borges. Learnr: Interactive tutorials for r. 2020. URL https://CRAN.R-project.org/package=learnr. R package version 0.10.1.\n\n\nS. Stall, L. Yarmey, J. Cutcher-Gershenfeld, B. Hanson, K. Lehnert, B. Nosek, M. Parsons, E. Robinson and L. Wyborn. Make scientific data FAIR. 2019. URL https://doi.org/10.1038/d41586-019-01720-7.\n\n\nC.-H. M. Tso. R shiny sandbox app demonstrator for advancing reproducible research. 2022. URL https://doi.org/10.5285/df57b002-2a42-4a7d-854f-870dd867618c.\n\n\nC.-H. M. Tso, D. Monteith, T. Scott, H. Watson, B. Dodd, M. G. Pereira, P. Henrys, M. Hollaway, S. Rennie, A. Lowther, et al. The evolving role of weather types on rainfall chemistry under large reductions in pollutant emissions. Environmental Pollution, 299: 118905, 2022. URL https://doi.org/10.1016/j.envpol.2022.118905.\n\n\nS. Whateley, J. D. Walker and C. Brown. A web-based screening model for climate risk to water supply systems in the northeastern united states. Environmental Modelling & Software, 73: 64–75, 2015. URL https://doi.org/10.1016/j.envsoft.2015.08.001.\n\n\nM. D. Wilkinson, M. Dumontier, Ij. J. Aalbersberg, G. Appleton, M. Axton, A. Baak, N. Blomberg, J. W. Boiten, L. B. da Silva Santos, P. E. Bourne, et al. Comment: The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 2016. URL https://doi.org/10.1038/sdata.2016.18.\n\n\nI. J. Williams and K. K. Williams. Using an R shiny to enhance the learning experience of confidence intervals. Teaching Statistics, 40(1): 24–28, 2017. URL https://doi.org/10.1111/test.12145.\n\n\n\n\n",
    "preview": "articles/RJ-2022-021/distill-preview.png",
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {},
    "preview_width": 405,
    "preview_height": 442
  },
  {
    "path": "articles/RJ-2021-106/",
    "title": "RPESE: Risk and Performance Estimators Standard Errors with Serially Dependent Data",
    "description": "The R package RPESE (Risk and Performance Estimators Standard Errors) implements a new method for computing accurate standard errors of risk and performance estimators when returns are serially dependent. The new method makes use of the representation of a risk or performance estimator as a summation of a time series of influence-function (IF) transformed returns, and computes estimator standard errors using a sophisticated method of estimating the spectral density at frequency zero of the time series of IF-transformed returns. Two additional packages used by RPESE are introduced, namely RPEIF which computes and provides graphical displays of the IF of risk and performance estimators, and RPEGLMEN which implements a regularized Gamma generalized linear model polynomial fit to the periodogram of the time series of the IF-transformed returns. A Monte Carlo study shows that the new method provides more accurate estimates of standard errors for risk and performance estimators compared to well-known alternative methods in the presence of serial correlation.",
    "author": [
      {
        "name": "Anthony-Alexander Christidis",
        "url": {}
      },
      {
        "name": "R. Douglas Martin",
        "url": {}
      }
    ],
    "date": "2022-01-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRPESE, RPEIF, RPEGLMEN, PerformanceAnalytics, RobStatTM, nse, sandwich\nCRAN Task Views implied by cited packages\nEconometrics, Finance, Robust, SocialSciences\n\n\n",
    "preview": "articles/RJ-2021-106/preview.png",
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {},
    "preview_width": 2331,
    "preview_height": 1287
  },
  {
    "path": "articles/RJ-2021-097/",
    "title": "BayesSenMC: an R package for Bayesian Sensitivity Analysis of Misclassification",
    "description": "In case–control studies, the odds ratio is commonly used to summarize the association be tween a binary exposure and a dichotomous outcome. However, exposure misclassification frequently appears in case–control studies due to inaccurate data reporting, which can produce bias in measures of association. In this article, we implement a Bayesian sensitivity analysis of misclassification to provide a full posterior inference on the corrected odds ratio under both non-differential and differen tial misclassification. We present an R (R Core Team, 2018) package BayesSenMC, which provides user-friendly functions for its implementation. The usage is illustrated by a real data analysis on the association between bipolar disorder and rheumatoid arthritis.",
    "author": [
      {
        "name": "Jinhui Yang",
        "url": {}
      },
      {
        "name": "Lifeng Lin",
        "url": {}
      },
      {
        "name": "Haitao Chu",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nBayesSenMC, episensr, lme4, rstan, ggplot2\nCRAN Task Views implied by cited packages\nBayesian, Econometrics, Environmetrics, OfficialStatistics, Phylogenetics, Psychometrics, SocialSciences, SpatioTemporal, TeachingStatistics\n\n\n",
    "preview": "articles/RJ-2021-097/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 754,
    "preview_height": 521
  },
  {
    "path": "articles/RJ-2021-098/",
    "title": "EMSS: New EM-type algorithms for the Heckman selection model in R",
    "description": "When investigators observe non-random samples from populations, sample selectivity problems may occur. The Heckman selection model is widely used to deal with selectivity problems. Based on the EM algorithm, Zhao et al. (2020) developed three algorithms, namely, ECM, ECM(NR), and ECME(NR), which also have the EM algorithm’s main advantages: stability and ease of imple mentation. This paper provides the implementation of these three new EM-type algorithms in the package EMSS and illustrates the usage of the package on several simulated and real data examples. The comparison between the maximum likelihood estimation method (MLE) and three new EM-type algorithms in robustness issues is further discussed.",
    "author": [
      {
        "name": "Kexuan Yang",
        "url": {}
      },
      {
        "name": "Sang Kyu Lee",
        "url": {}
      },
      {
        "name": "Jun Zhao",
        "url": {}
      },
      {
        "name": "Hyoung-Moon Kim",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsampleSelection, mvtnorm\nCRAN Task Views implied by cited packages\nDistributions, Econometrics, Finance, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-099/",
    "title": "Analysis of Corneal Data in R with the rPACI Package",
    "description": "In ophthalmology, the early detection of keratoconus is still a crucial problem. Placido disk corneal topographers are essential in clinical practice, and many indices for diagnosing corneal irregularities exist. The main goal of this work is to present the R package rPACI, providing several functions to handle and analyze corneal data. This package implements primary indices of corneal irregularity (based on geometrical properties) and compound indices built from the primary ones, either using a generalized linear model or as a Bayesian classifier using a hybrid Bayesian network and performing approximate inference. rPACI aims to make the analysis of corneal data accessible for practitioners and researchers in the field. Moreover, a shiny app was developed to use rPACI in any web browser in a truly user-friendly graphical interface without installing R or writing any R code. It is openly deployed at https://admaldonado.shinyapps.io/rPACI/.",
    "author": [
      {
        "name": "Darío Ramos-López",
        "url": {}
      },
      {
        "name": "Ana D. Maldonado",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrPACI, shiny, bnlearn\nCRAN Task Views implied by cited packages\nBayesian, GraphicalModels, HighPerformanceComputing, TeachingStatistics, WebTechnologies\n\n\n",
    "preview": "articles/RJ-2021-099/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 980,
    "preview_height": 500
  },
  {
    "path": "articles/RJ-2021-100/",
    "title": "DRHotNet: An R package for detecting differential risk hotspots on a linear network",
    "description": "One of the most common applications of spatial data analysis is detecting zones, at a certain scale, where a point-referenced event under study is especially concentrated. The detection of such zones, which are usually referred to as hotspots, is essential in certain fields such as criminology, epidemiology, or traffic safety. Traditionally, hotspot detection procedures have been developed over areal units of analysis. Although working at this spatial scale can be suitable enough for many research or practical purposes, detecting hotspots at a more accurate level (for instance, at the road segment level) may be more convenient sometimes. Furthermore, it is typical that hotspot detection procedures are entirely focused on the determination of zones where an event is (overall) highly concentrated. It is less common, by far, that such procedures focus on detecting zones where a specific type of event is overrepresented in comparison with the other types observed, which have been denoted as differential risk hotspots. The R package DRHotNet provides several functionalities to facilitate the detection of differential risk hotspots within a linear network. In this paper, DRHotNet is depicted, and its usage in the R console is shown through a detailed analysis of a crime dataset.",
    "author": [
      {
        "name": "Álvaro Briz-Redón",
        "url": {}
      },
      {
        "name": "Francisco Martínez-Ruiz",
        "url": {}
      },
      {
        "name": "Francisco Montes",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspdep, DCluster, spatstat.linnet, spatstat, DRHotNet, sp, sf, sfnetworks, maptools, spatstat.geom, SpNetPrep, rgeos, spatstat.data, tigris, raster, crimedata, lubridate\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, ReproducibleResearch, Survival, TimeSeries\n\n\n",
    "preview": "articles/RJ-2021-100/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 504,
    "preview_height": 504
  },
  {
    "path": "articles/RJ-2021-101/",
    "title": "Automatic Time Series Forecasting with Ata Method in R: ATAforecasting Package",
    "description": "Ata method is a new univariate time series forecasting method that provides innovative solutions to issues faced during the initialization and optimization stages of existing methods. The Ata method’s forecasting performance is superior to existing methods in terms of easy implementation and accurate forecasting. It can be applied to non-seasonal or deseasonalized time series, where",
    "author": [
      {
        "name": "Ali Sabri Taylan",
        "url": {}
      },
      {
        "name": "Güçkan Yapar",
        "url": {}
      },
      {
        "name": "Hanife Taylan Selamlar",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nATAforecasting, forecast, stats, stlplus, stR, tsibbledata, ucra, tseries, seasonal, Rcpp, RcppArmadillo, urca, uroot, xts, timeSeries, TSA, Mcomp, fable.ata, fabletools, fable\nCRAN Task Views implied by cited packages\nTimeSeries, Econometrics, Finance, MissingData, Environmetrics, NumericalMathematics, HighPerformanceComputing, OfficialStatistics, SpatioTemporal\n\n\n",
    "preview": "articles/RJ-2021-101/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 768
  },
  {
    "path": "articles/RJ-2021-102/",
    "title": "spNetwork: A Package for Network Kernel Density Estimation",
    "description": "This paper introduces the new package spNetwork that provides functions to perform Network Kernel Density Estimate analysis (NKDE). This method is an extension of the classical Kernel Density Estimate (KDE), a non parametric approach to estimate the intensity of a spatial process. More specifically, it adapts the KDE for cases when the study area is a network, constraining the location of events (such as accidents on roads, leaks in pipes, fish in rivers, etc.). We present and discuss in this paper the three main versions of NKDE: simple, discontinuous, and continuous that are implemented in spNetwork. We illustrate how to apply the three methods and map their results using a sample from a real dataset representing bike accidents in a central neighborhood of Montreal. We also describe the optimization techniques used to reduce calculation time and investigate their impacts when applying the three NKDE to a city-wide dataset.",
    "author": [
      {
        "name": "Jeremy Gelb",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspNetwork, spatstat, rgdal, sp, rgeos, maptools, igraph, Rcpp, future, SpNetwork, SearchTrees, future.apply, RcppArmadillo\nCRAN Task Views implied by cited packages\nSpatial, HighPerformanceComputing, NumericalMathematics, SpatioTemporal, GraphicalModels, Optimization, Survival\n\n\n",
    "preview": "articles/RJ-2021-102/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 815,
    "preview_height": 443
  },
  {
    "path": "articles/RJ-2021-103/",
    "title": "bssm: Bayesian Inference of Non-linear and Non-Gaussian State Space Models in R",
    "description": "We present an R package bssm for Bayesian non-linear/non-Gaussian state space modeling. Unlike the existing packages, bssm allows for easy-to-use approximate inference based on Gaussian approximations such as the Laplace approximation and the extended Kalman filter. The package also accommodates discretely observed latent diffusion processes. The inference is based on fully automatic, adaptive Markov chain Monte Carlo (MCMC) on the hyperparameters, with optional importance sampling post-correction to eliminate any approximation bias. The package also implements a direct pseudo-marginal MCMC and a delayed acceptance pseudo-marginal MCMC using intermediate approximations. The package offers an easy-to-use interface to define models with linear-Gaussian state dynamics with non-Gaussian observation models and has an Rcpp interface for specifying custom non-linear and diffusion models.",
    "author": [
      {
        "name": "Jouni Helske",
        "url": {}
      },
      {
        "name": "Matti Vihola",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nbssm, Rcpp, pomp, rbi, nimbleSMC, rstan, ramcmc, RcppArmadillo, KFAS, sde, coda, ggplot2, dplyr\nCRAN Task Views implied by cited packages\nTimeSeries, Bayesian, DifferentialEquations, NumericalMathematics, Databases, Finance, GraphicalModels, HighPerformanceComputing, ModelDeployment, Phylogenetics, TeachingStatistics\n\n\n",
    "preview": "articles/RJ-2021-103/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 468,
    "preview_height": 324
  },
  {
    "path": "articles/RJ-2021-104/",
    "title": "Generalized Linear Randomized Response Modeling using GLMMRR",
    "description": "Randomized response (RR) designs are used to collect response data about sensitive behaviors (e.g., criminal behavior, sexual desires). The modeling of RR data is more complex since it requires a description of the RR process. For the class of generalized linear mixed models (GLMMs), the RR process can be represented by an adjusted link function, which relates the expected RR to the linear predictor for most common RR designs. The package GLMMRR includes modified link functions for four different cumulative distributions (i.e., logistic, cumulative normal, Gumbel, Cauchy) for GLMs and GLMMs, where the package lme4 facilitates ML and REML estimation. The mixed modeling framework in GLMMRR can be used to jointly analyze data collected under different designs (e.g., dual questioning, multilevel, mixed mode, repeated measurements designs, multiple-group designs). Model-fit tests, tools for residual analyses, and plot functions to give support to a profound RR data analysis are added to the well-known features of the GLM and GLMM software (package lme4). Data of Höglinger and Jann (2018) and Höglinger, Jann, and Diekmann (2014) are used to illustrate the methodology and software.",
    "author": [
      {
        "name": "Jean-Paul Fox",
        "url": {}
      },
      {
        "name": "Konrad Klotzke",
        "url": {}
      },
      {
        "name": "Duco Veen",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrr, RRreg, GLMMRR, stats, lme4\nCRAN Task Views implied by cited packages\nOfficialStatistics, Psychometrics, Econometrics, Environmetrics, SocialSciences, SpatioTemporal\n\n\n",
    "preview": "articles/RJ-2021-104/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 376,
    "preview_height": 264
  },
  {
    "path": "articles/RJ-2021-105/",
    "title": "Visual Diagnostics for Constrained Optimisation with Application to Guided Tours",
    "description": "A guided tour helps to visualise high-dimensional data by showing low-dimensional projections along a projection pursuit optimisation path. Projection pursuit is a generalisation of principal component analysis in the sense that different indexes are used to define the interestingness of the projected data. While much work has been done in developing new indexes in the literature, less has been done on understanding the optimisation. Index functions can be noisy, might have multiple local maxima as well as an optimal maximum, and are constrained to generate orthonormal projection frames, which complicates the optimization. In addition, projection pursuit is primarily used for exploratory data analysis, and finding the local maxima is also useful. The guided tour is especially useful for exploration because it conducts geodesic interpolation connecting steps in the optimisation and shows how the projected data changes as a maxima is approached. This work provides new visual diagnostics for examining a choice of optimisation procedure based on the provision of a new data object which collects information throughout the optimisation. It has helped to diagnose and fix several problems with projection pursuit guided tour. This work might be useful more broadly for diagnosing optimisers and comparing their performance. The diagnostics are implemented in the R package [ferrn](https://github.com/huizezhang-sherry/ferrn).",
    "author": [
      {
        "name": "H. Sherry Zhang",
        "url": {}
      },
      {
        "name": "Dianne Cook",
        "url": {}
      },
      {
        "name": "Ursula Laa",
        "url": {}
      },
      {
        "name": "Nicolas Langrené",
        "url": {}
      },
      {
        "name": "Patricia Menéndez",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\n1 Introduction\nVisualisation is widely used in exploratory data analysis (Tukey 1977; Unwin 2015; Healy 2018; Wilke 2019). Presenting information in graphics often unveils insights that would otherwise not be discovered and provides a more comprehensive understanding of the problem at hand. Task-specific tools such as Li et al. (2020) show how visualisation can be used to understand, for instance, the behaviour of the optimisation for the example of neural network classification models. However, no general visualisation tool is available for diagnosing optimisation procedures. The work presented in this paper brings visualization tools into optimisation problems with the aim to better understand the performance of optimisers in practice.\nThe focus of this paper is on the optimisation problem arising in the projection pursuit guided tour (Buja et al. 2005), an exploratory data analysis technique used for detecting interesting structures in high-dimensional data through a set of lower-dimensional projections (Cook et al. 2008). The goal of the optimisation is to identify the projection, represented by the projection matrix, that gives the most interesting low-dimensional view. A view is said to be interesting if it can show some structures of the data that depart from normality, such as bimodality, clustering, or outliers.\nThe optimization challenges encountered in the projection pursuit guided tour problem are common to those of optimization in general. Examples include the existence of multiple optima (local and global), the trade-off between computational burden and proximity to the optima, or dealing with noisy objective functions that might be non-smooth and non-differentiable (Jones et al. 1998). The visualization tools, optimization methods, and conceptual framework presented in this paper can therefore be applied to other optimization problems.\nThe remainder of the paper is organised as follows. The next section provides an overview of optimisation methods, specifically random search and line search methods. A review of the projection pursuit guided tour, an overview of the optimisation problem and, outlines of three existing algorithms follows. The third section presents the new visual diagnostics, including the design of a data structure to capture information during the optimisation, from which several diagnostic plots are created. An illustration of how the diagnostic plots can be used to examine the performance of different optimisers and guide improvements to existing algorithms is shown using simulated data. Finally, an explanation of the implementation in the R package, ferrn (Zhang et al. 2021), is provided.\n2 Optimisation methods\nThe type of optimisation problem considered in this paper is constrained optimization (Bertsekas 2014), assuming it is not possible to find a solution to the problem in the way of a closed-form. That is, the problem consists in finding the minimum or maximum of a function \\(f \\in L^p\\) in the constrained space \\(\\mathcal{A}\\), where \\(L^p\\) defines the vector space of function \\(f\\), whose \\(p\\)th power is integrable.\nGradient-based methods are commonly used to optimise an objective function, with the most notable one being the gradient ascent (descent) method. Although these methods are popular, they rely on the availability of the objective function derivatives. As will be shown in the next section, the independent variables in our optimisation problem are the entries of a projection matrix, and the computational time required to perform differentiation on a matrix could impede the rendering of tour animation. In addition, some objective functions rely on the empirical distribution of the data, which makes it in general not possible to get the gradient. Hence, gradient-based methods are not the focus of this paper, and consideration will be given to derivative-free methods.\nDerivative-free methods (Conn et al. 2009; Rios and Sahinidis 2013), which do not rely on the knowledge of the gradient, are more generally applicable. Derivative-free methods have been developed over the years, where the emphasis is on finding, in most cases, a near-optimal solution. Here we consider three derivative-free methods, two of which are random search methods: creeping random search and simulated annealing, and the other one is pseudo-derivative search.\nRandom search methods (Romeijn 2009; Zabinsky 2013; Andradóttir 2015) have a random sampling component as part of their algorithms and have been shown to have the ability to optimise non-convex and non-smooth functions. The initial random search algorithm, pure random search (Brooks 1958), draws candidate points from the entire space without using any information of the current position and updates the current position when an improvement on the objective function is made. As the dimension of the space becomes larger, sufficient sampling from the entire space would require a long time for convergence to occur, despite a guaranteed global convergence (Spall 2005). Various algorithms have thus been developed to improve pure random search by either concentrating on a narrower sampling space or using a different updating mechanism. Creeping random search (White 1971) is such a variation, where a candidate point is generated within a neighbourhood of the current point. This makes creeping random search faster to compute but global convergence is no longer guaranteed. On the other hand, simulated annealing (Kirkpatrick et al. 1983; Bertsimas and Tsitsiklis 1993), introduces a different updating mechanism. Rather than only updating the current point when an improvement is made, it uses a Metropolis acceptance criterion, where worse candidates still have a chance to be accepted. The convergence of simulated annealing algorithms has been widely researched (Mitra et al. 1986; Granville et al. 1994) and the global optimum can be attained under mild regularity conditions.\nThe pseudo-derivative search uses a common search scheme in optimisation: line search. In line search methods, users are required to provide an initial estimate \\(x_{1}\\) and, at each iteration, a search direction \\(S_k\\) and a step size \\(\\alpha_k\\) are generated. Then one moves on to the next point following \\(x_{k+1} = x_k + \\alpha_kS_k\\) and the process is repeated until the desired convergence is reached. In derivative-free methods, local information of the objective function is used to determine the search direction. The choice of step size also needs consideration, as inadequate step sizes might prevent the optimisation method from converging to an optimum. An ideal step size can be chosen by finding the value of \\(\\alpha_k \\in \\mathbb{R}\\) that maximises \\(f(x_k + \\alpha_kS_k)\\) with respect to \\(\\alpha_k\\) at each iteration.\n3 Projection pursuit guided tour\nA projection pursuit guided tour combines two different methods (projection pursuit and guided tour) to explore interesting features in a high-dimensional space. Projection pursuit, coined by Friedman and Tukey (1974), detects interesting structures (e.g., clustering, outliers, and skewness) in multivariate data via low-dimensional projections. Guided tour (Cook et al. 1995) is one variation of a broader class of data visualisation methods, tour (Buja et al. 2005), which displays high-dimensional data through a series of animated projections.\nLet \\(\\mathbf{X}_{n \\times p}\\) be the data matrix with \\(n\\) observations in \\(p\\) dimensions. A \\(d\\)-dimensional projection is a linear transformation from \\(\\mathbb{R}^p\\) into \\(\\mathbb{R}^d\\) defined as \\(\\mathbf{Y} = \\mathbf{X} \\cdot \\mathbf{A}\\), where \\(\\mathbf{Y}_{n \\times d}\\) is the projected data and \\(\\mathbf{A}_{p\\times d}\\) is the projection matrix. We define \\(f: \\mathbb{R}^{n \\times d} \\mapsto \\mathbb{R}\\) to be an index function that maps the projected data \\(\\mathbf{Y}\\) onto a scalar value. This is commonly known as the projection pursuit index function, or just index function, and is used to measure the “interestingness” of a given projection. An interesting projection shows structures that are non-normal since theoretical proofs from Diaconis and Freedman (1984) have shown that projections tend to be normal as \\(n\\) and \\(p\\) approach infinity under certain conditions. There have been many index functions proposed in the literature, here are a few examples: early indexes that can be categorised as measuring the \\(L^2\\) distance between the projection and a normal distribution: Legendre index (Friedman and Tukey 1974); Hermite index (Hall 1989); natural Hermite index (Cook et al. 1993); chi-square index (Posse 1995) for detecting spiral structure; LDA index (Lee et al. 2005) and PDA (Lee and Cook 2010) index for supervised classification; kurtosis index (Loperfido 2020) and skewness index (Loperfido 2018) for detecting outliers in financial time series; and most recently, scagnostic indexes (Laa and Cook 2020) for summarising structures in scatterplot matrices based on eight scagnostic measures (Wilkinson et al. 2005; Wilkinson and Wills 2008).\nAs a general visualisation method, tour produces animations of high-dimensional data via rotations of low-dimensional planes. There are different versions depending on how the high-dimensional space is investigated: grand tour (Cook et al. 2008) selects the planes randomly to provide a general overview; manual tour (Cook and Buja 1997) gradually phases in and out one variable to understand the contribution of that variable in the projection. Guided tour, the main interest of this paper, chooses the planes with the aid of projection pursuit to gradually reveal the most interesting projection. Given a random start, projection pursuit iteratively finds bases with higher index values, and the guided tour constructs a geodesic interpolation between these planes to form a tour path. Figure 1 shows a sketch of the tour path where the blue squares represent planes (targets) selected by the projection pursuit optimisation, and the white squares represent planes in the geodesic interpolation between targets. Mathematical details of the geodesic interpolation can be found in Buja et al. (2005). (Note that the term frame used in Buja’s paper refers to a particular set of orthonormal vectors defining a plane. This is also conventionally referred to as a basis, which is used in this paper and the associated software.) The aforementioned tour method has been implemented in the R package tourr (Wickham et al. 2011).\n\n\n\nFigure 1: An illustration for demonstrating the frames in a tour path. Each square (frame) represents the projected data with a corresponding basis. Blue frames are returned by the projection pursuit optimisation and white frames are constructed between two blue frames by geodesic interpolation.\n\n\n\nOptimisation in the tour\nIn projection pursuit, the optimisation aims at finding the global and local maxima that give interesting projections according to an index function. That is, it starts with a given randomly selected basis \\(\\mathbf{A}_1\\) and aims at finding an optimal final projection basis \\(\\mathbf{A}_T\\) that satisfies the following optimisation problem:\n\\[\\begin{align}\n\\arg \\max_{\\mathbf{A} \\in \\mathcal{A}} f(\\mathbf{X} \\cdot \\mathbf{A})  ~~~ s.t. ~~~ \\mathbf{A}^{\\prime} \\mathbf{A} = I_d \\,,\n\\end{align}\\]\nwhere \\(f\\) and \\(\\mathbf{X}\\) are defined as in the previous section, \\(\\mathcal{A}\\) is the set of all \\(p\\)-dimensional projection bases, \\(I_d\\) is the \\(d\\)-dimensional identity matrix, and the constraint ensures the projection bases, \\(\\mathbf{A}\\), to be orthonormal. It is worth noticing the following: 1) The optimisation is constrained, and the orthonormality constraint imposes a geometrical structure on the bases space: it forms a Stiefel manifold. 2) There may be index functions for which the objective function might not be differentiable. 3) While finding the global optimum is the goal of the optimisation problem, interesting projections may also appear in the local optimum. 4) The optimisation should be fast to compute since the tour animation is viewed by the users during the optimisation.\nExisting algorithms\nThree optimisers have been implemented in the tourr (Wickham et al. 2011) package: creeping random search (CRS), simulated annealing (SA), and pseudo-derivative (PD). Creeping random search (CRS) is a random search optimiser that samples a candidate basis \\(\\mathbf{A}_{l}\\) in the neighbourhood of the current basis \\(\\mathbf{A}_{\\text{cur}}\\) by \\(\\mathbf{A}_{l} = (1- \\alpha)\\mathbf{A}_{\\text{cur}} + \\alpha \\mathbf{A}_{\\text{rand}}\\) where \\(\\alpha \\in [0,1]\\) controls the radius of the sampling neighbourhood and \\(\\mathbf{A}_{\\text{rand}}\\) is generated randomly. \\(\\mathbf{A}_{l}\\) is then orthonormalised to fulfil the basis constraint. If \\(\\mathbf{A}_{l}\\) has an index value higher than the current basis \\(\\mathbf{A}_{\\text{cur}}\\), the optimiser outputs \\(\\mathbf{A}_{l}\\) for a guided tour to construct an interpolation path. The neighbourhood parameter \\(\\alpha\\) is adjusted by a cooling parameter: \\(\\alpha_{j+1} = \\alpha_j * \\text{cooling}\\) before the next iteration starts. The optimiser terminates when the maximum number of iteration \\(l_{\\max}\\) is reached before a better basis can be found. The algorithm of CRS can be found in the appendix. Posse (1995) has proposed a slightly different cooling scheme by introducing a halving parameter \\(c\\). In his proposal, \\(\\alpha\\) is only adjusted if the last iteration takes more than \\(c\\) times to find a better basis.\nSimulated annealing (SA) uses the same sampling process as CRS but allows a probabilistic acceptance of a basis with lower index value than the current one. Given an initial value of \\(T_0 \\in \\mathbb{R^{+}}\\), the “temperature” at iteration \\(l\\) is defined as \\(T(l) = \\frac{T_0}{\\log(l + 1)}\\). When a candidate basis fails to have an index value larger than the current basis, SA gives it a second chance to be accepted with probability \\[P= \\min\\left\\{\\exp\\left[-\\frac{\\mid I_{\\text{cur}} - I_{l} \\mid}{T(l)}\\right],1\\right\\} \\,,\\] where \\(I_{(\\cdot)} \\in \\mathbb{R}\\) denotes the index value of a given basis. This implementation allows the optimiser to make a move and explore the basis space even if the candidate basis does not have a higher index value. Hence it enables the optimiser to jump out of a local optimum. The second algorithm in the appendix highlights how SA differs from CRS in the inner loop.\nPseudo-derivative (PD) search uses a different strategy than CRS and SA. Rather than randomly sample the basis space, PD first computes a search direction by evaluating bases close to the current basis. The step size is then chosen along the corresponding geodesic by another optimisation over a 90 degree angle from \\(-\\pi/4\\) to \\(\\pi/4\\). The resulting candidate basis \\(\\mathbf{A}_{**}\\) is returned for the current iteration if it has a higher index value than the current one. The third algorithm in the appendix summarises the inner loop of the PD.\n4 Visual diagnostics\nA data structure for diagnosing optimisers in projection pursuit guided tour is first defined. With this data structure, four types of diagnostic plots are presented.\nData structure for diagnostics\nThree main pieces of information are recorded during the projection pursuit optimisation: 1) projection bases \\(\\mathbf{A}\\), 2) index values \\(I\\), and 3) state \\(S\\). For CRS and SA, possible states include random_search, new_basis, and interpolation. Pseudo-derivative (PD) has a wider variety of states, including new_basis, direction_search, best_direction_search, best_line_search, and interpolation. Multiple iterators index the information collected at different levels: \\(t\\) is a unique identifier prescribing the natural ordering of each observation; \\(j\\) and \\(l\\) are the counter of the outer and inner loop, respectively. Other parameters of interest recorded, \\(V\\), include method that tags the name of the optimiser, and alpha that indicates the sampling neighbourhood size for searching observations. A matrix notation describing the data structure is:\n\n\\(t\\)\n\\(\\mathbf{A}\\)\n\\(I\\)\n\\(S\\)\n\\(j\\)\n\\(l\\)\n\\(V_{1}\\)\n\\(V_{2}\\)\nDescription\n\\(1\\)\n\\(\\mathbf{A}_1\\)\n\\(I_1\\)\n\\(S_1\\)\n1\n1\n\\(V_{11}\\)\n\\(V_{12}\\)\nstart basis\n\\(2\\)\n\\(\\mathbf{A}_2\\)\n\\(I_2\\)\n\\(S_2\\)\n2\n1\n\\(V_{21}\\)\n\\(V_{22}\\)\nsearch\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n2\n\\(l_2\\)\n…\n…\nsearch (accepted)\n…\n…\n…\n…\n2\n1\n…\n…\ninterpolation\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n2\n\\(k_2\\)\n…\n…\ninterpolation\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\\(J\\)\n1\n…\n…\nsearch\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\\(T\\)\n\\(\\mathbf{A}_T\\)\n\\(I_T\\)\n\\(S_T\\)\n\\(J\\)\n\\(l_J\\)\n\\(V_{T1}\\)\n\\(V_{T2}\\)\nsearch (final)\n…\n…\n…\n…\n\\(J\\)\n1\n…\n…\ninterpolation\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\\(J\\)\n\\(k_J\\)\n…\n…\ninterpolation\n…\n…\n…\n…\n\\(J+1\\)\n1\n…\n…\nsearch (last round)\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\\(T^{\\prime}\\)\n\\(\\mathbf{A}_{T^{\\prime}}\\)\n\\(I_{T^{\\prime}}\\)\n\\(S_{T^{\\prime}}\\)\n\\(J+1\\)\n\\(l_{J+1}\\)\n\\(V_{{T}^{\\prime}1}\\)\n\\(V_{{T}^{\\prime}2}\\)\nsearch (last round)\nwhere \\(T^{\\prime} = T + k_{J}+ l_{J+1}\\). Note that there is no output in iteration \\(J + 1\\) since the optimiser does not find a better basis in the last iteration and terminates. The final basis found is \\(A_T\\) with index value \\(I_T\\).\nThe data structure constructed above meets the tidy data principle (Wickham 2014) that requires each observation to form a row and each variable to form a column. With tidy data structure, data wrangling and visualisation can be significantly simplified by well-developed packages such as dplyr (Wickham et al. 2020) and ggplot2 (Wickham 2016).\nDiagnostic 1: Checking how hard the optimiser is working\nA starting point of diagnosing an optimiser is to understand how many searches it has conducted, i.e., we want to summarise how the index is increasing over iterations and how many basis need to be sampled at each iteration. This is achieved using the function explore_trace_search(): a boxplot shows the distribution of index values for each try, where the accepted basis (corresponding to the highest index value) is always shown as a point. When there are only few tries at a given iteration, showing the data points directly is preferred over the boxplot and this is controlled via the cutoff argument. Additional annotations are added to facilitate better reading of the plot, and these include 1) the number of points searched in each iteration can be added as text label at the bottom of each iteration; 2) the anchor bases to interpolate are connected and highlighted in a larger size; 3) the colour of the last iteration is in greyscale to indicate no better basis found in this iteration.\nFigure 2 shows an example of the search plot for CRS (left) and SA (right). Both optimisers quickly find better bases in the first few iterations and then take longer to find one in the later iterations. The anchor bases, the ones found with the highest index value in each iteration, always have an increased index value in the optimiser CRS while this is not the case for SA. This feature gives CRS an advantage in this simple example to quickly find the optimum.\n\n\n\nFigure 2: A comparison of the searches by two optimisers: CRS (left) and SA (right) on a 2D projection problem of a six-variable dataset, using the holes index. Both optimisers reach the final basis with a similar index value, while it takes SA longer to find the final basis. In the earlier iterations, optimisers quickly find a better basis to proceed, while in the later iterations, most sampled bases fail to make an improvement on the index value, and a boxplot is used to summarise the distribution of the index values. There is no better basis found in the last iteration, 9 (left) and 15 (right), before reaching the maximum number of try and hence it is coloured grey. The colour scale is from the customised botanical palette in the ferrn package.\n\n\n\nDiagnostic 2: Examining the optimisation progress\nAnother interesting feature to examine is the changes in the index value between interpolating bases since the projection on these bases is shown in the tour animation. Trace plots are created by plotting the index value against time. Figure 3 presents the trace plot of the same optimisations as Figure 2, and one can observe that the trace is smooth in both cases. It may seem bizarre at first sight that the interpolation sometimes passes bases with higher index values before it decreases to a lower target. This happens because, on the one hand, the probabilistic acceptance in SA implies that some worse bases will be accepted by the optimiser. In addition, the guided tour interpolates between the current and target basis to provide a smooth transition between projections, and sometimes a higher index value will be observed along the interpolation path. This indicates that a non-monotonic interpolation cannot be avoided, even for CRS. Later, in Section A problem of non-monotonicity, there will be a discussion on improving the non-monotonic interpolation for CRS.\n\n\n\nFigure 3: An inspection of the index values as the optimisation progress for two optimisers: CRS (left) and SA (right). The holes index is optimised for a 2D projection problem on the six-variable dataset . Lines indicate the interpolation, and dots indicate new target bases generated by the optimisers. Interpolation in both optimisation is smooth, while SA is observed to first pass by some bases with higher index values before reaching the target bases in time 76-130.\n\n\n\nDiagnostic 3a: Understanding the optimiser’s coverage of the search space\nApart from checking the search and progression of an optimiser, looking at where the bases are positioned in the basis space is also of interest. Given the orthonormality constraint, the space of projection bases \\(\\mathbf{A}_{p \\times d}\\) is a Stiefel manifold. For one-dimensional projections, this forms a \\(p\\)-dimensional sphere. A dimensionality reduction method, e.g., principal component analysis, is applied to first project all the bases onto a 2D space. In a projection pursuit guided tour optimisation, there are various types of bases involved: 1) The starting basis; 2) The search bases that the optimiser evaluated to produce the anchor bases; 3) The anchor bases that have the highest index value in each iteration; 4) The interpolating bases on the interpolation path; and finally, 5) the end basis. The importance of these bases differs but the most important ones are the starting, interpolating, and end bases. Sometimes, two optimisers can start with the same basis but finish with bases of opposite signs. This happens because the projection is invariant to the orientation of the basis, and so is the index value. However, this creates difficulties for comparing the optimisers since the end bases will be symmetric to the origin. A sign flipping step is conducted to flip the signs of all the bases in one routine if different optimisations finish at opposite places.\nSeveral annotations have been made to help understand this plot. As mentioned previously, the original basis space is a high-dimensional sphere, and random bases on the sphere can be generated via the geozoo (Schloerke 2016) package. We use PCA to project and visualize the parameters/ bases in 2D. The centre of the 2D view is the first two PCs of the data matrix. It theoretically should be a circle but may have some irregular edges due to finite sampling. Thus the edge is smoothed by using a radius estimated as the largest distance from the centre to any basis. In the simulation, the theoretical best basis is known and can be labelled to compare how close to this that the optimisers stopped. Various aesthetics, i.e., size, alpha (transparency), and colour, are applicable to emphasize critical elements and adjust for the presentation. For example, anchor points and search points are less important, and hence a smaller size and alpha are used. Alpha can also be applied on the interpolation paths to show start to finish from transparent to opaque.\nFigure 4 shows the PCA plot of CRS and PD for a 1D projection problem. Both optimisers find the optimum, but PD gets closer. With the PCA plot, one can visually appreciate the nature of these two optimisers: PD first evaluates points in a small neighbourhood for a promising direction, while CRS evaluates points randomly in the search space to search for the next target. There are dashed lines annotated for CRS, and it describes the interruption of the interpolation, which will be discussed in detail in Section A problem of non-monotonicity.\n\n\n\nFigure 4: Search paths of CRS (green) and PD (brown) in the PCA-reduced basis space for 1D projection problem on the five-variable dataset, using holes index. The basis space, a 5D unit sphere, is projected onto a 2D circle by PCA. The black star represents the theoretical best basis the optimisers are aiming to find. All the bases in PD have been flipped for easier comparison of the final bases, and a grey dashed line has been annotated to indicate the symmetry of the two start bases.\n\n\n\nDiagnostic 3b: Animating the diagnostic plots\nAnimation is another type of display to show how the search progresses from start to finish in the space. Figure 5 shows the animated version (six frames from the animation if viewed in pdf) of the PCA plot in Figure 4. An additional piece of information one can learn from this animation is that CRS finds its end basis quicker than PD since CRS finishes its search in the 5th frame while PD is still making more progress.\n\n\n\n\n\n\nFigure 5: Animated version of Figure 4. With animation, the progression of the search paths from start to finish can better identified and CRS (green) finishes the optimisation quicker than PD (brown).\n\n\n\nDiagnostic 4a: The tour looking at itself\nAs mentioned previously, the original \\(p \\times d\\) dimension space can be simulated via randomly generated bases in the geozoo (Schloerke 2016) package. While the PCA plot projects the bases from the direction that maximises the variance, the tour plot displays the original high-dimensional space from various directions using animation. Figure 6 shows some frames from the tour plot of the same two optimisations in its original space.\n\n\n\n\n\n\nFigure 6: Animating the two paths from Figure 4 and 5 in the full basis space. The full basis space in this example is a 5D unit sphere, on which points (grey) are randomly generated via the CRAN package geozoo.\n\n\n\nDiagnostic 4b: Forming a torus\nWhile the previous few examples have looked at the space of 1D basis in a unit sphere, this section visualises the space of 2D basis. Recall that the columns in a 2D basis are orthogonal to each other, so the space of \\(p \\times 2\\) bases is a torus in the \\(p\\)-D space (Buja and Asimov 1986). For \\(p = 3\\) one would see a classical 3D torus shape as shown by the grey points in Figure 7. The two circles of the torus can be observed to be perpendicular to each other and this can be linked back to the orthogonality condition. Two paths from CRS and PD are plotted on top of the torus and coloured in green and brown, respectively, to match the previous plots. The final basis found by PD and CRS are shown in a larger shape and printed below, respectively:\n\n             [,1]        [,2]\n[1,]  0.001196285  0.03273881\n[2,] -0.242432715  0.96965761\n[3,] -0.970167484 -0.24226493\n\n\n            [,1]         [,2]\n[1,]  0.05707994 -0.007220138\n[2,] -0.40196202 -0.915510160\n[3,] -0.91387549  0.402230054\n\nBoth optimisers have found the third variable in the first direction and the second variable in the second direction. Note, however, the different orientation of the basis, following from the different sign in the second column. One would expect to see this in the torus plot as the final bases match each other when projected onto one torus circle (due to the same sign in the first column) and are symmetric when projected onto the other (due to the different sign in the second column). In Figure 7, this can be seen most clearly in frame 5 where the two circles are rotated into a line from our view.\n\n\n\n\n\n\nFigure 7: Animation of rotating the 2D basis space along with two search paths optimised by PD (brown) and CRS (green). The projection problem is a 2D projection with three variables using the holes index. The grey points are randomly generated 2D projection bases in the space and it can be observed that these points form a torus.\n\n\n\n5 Diagnosing an optimiser\nIn this section, several examples will be presented to show how the diagnostic plots discover something unexpected in projection pursuit optimisation, and guide the implementation of new features.\nSimulation setup\nRandom variables with different distributions have been simulated as follows:\n\\[\\begin{align}\nx_1 \\overset{d}{=} x_8 \\overset{d}{=} x_9 \\overset{d}{=} x_{10}& \\sim \\mathcal{N}(0, 1) \\\\\nx_2 &\\sim 0.5 \\mathcal{N}(-3, 1) + 0.5 \\mathcal{N}(3, 1)\\\\\n\\Pr(x_3) &=\n\\begin{cases}\n0.5 & \\text{if $x_3 = -1$ or $1$}\\\\\n0 & \\text{otherwise}\n\\end{cases}\\\\\nx_4 &\\sim 0.25 \\mathcal{N}(-3, 1) + 0.75 \\mathcal{N}(3, 1)\\\\\nx_5 &\\sim \\frac{1}{3} \\mathcal{N}(-5, 1) + \\frac{1}{3} \\mathcal{N}(0, 1) + \\frac{1}{3} \\mathcal{N}(5, 1)\\\\\nx_6 &\\sim 0.45 \\mathcal{N}(-5, 1) + 0.1 \\mathcal{N}(0, 1) + 0.45 \\mathcal{N}(5, 1)\\\\\nx_7 &\\sim 0.5 \\mathcal{N}(-5, 1) + 0.5 \\mathcal{N}(5, 1)\n\\end{align}\\]\nVariables x1, x8 to x10 are normal noise with zero mean, and unit variance and x2 to x7 are normal mixtures with varied weights and locations. All the variables have been scaled to have overall unit variance before projection pursuit. The holes index (Cook et al. 2008), used for detecting bimodality of the variables, is used throughout the examples unless otherwise specified.\nA problem of non-monotonicity\nAn example of non-monotonic interpolation has been given in Figure 3: a path that passes bases with a higher index value than the target one. For SA, a non-monotonic interpolation is justified since target bases do not necessarily have a higher index value than the current one, while this is not the case for CRS. The original trace plot for a 2D projection problem, optimised by CRS, is shown on the left panel of Figure 8, and one can observe that the non-monotonic interpolation has undermined the optimiser to realise its full potential. Hence, an interruption is implemented to stop at the best basis found in the interpolation. The right panel of Figure 8 shows the trace plot after implementing the interruption, and while the first two interpolations are identical, the basis at time 61 has a higher index value than the target in the third interpolation. Rather than starting the next iteration from the target basis on time 65, CRS starts the next iteration at time 61 on the right panel and reaches a better final basis.\n\n\n\nFigure 8: Comparison of the interpolation before and after implementing the interruption for the 2D projection problem on data using holes index, optimised by CRS. On the left panel, the basis with a higher index value is found during the interpolation but not used. On the right panel, the interruption stops the interpolation at the basis with the highest index value for each iteration and results in a final basis with a higher index value, as shown on the right panel.\n\n\n\nClose but not close enough\nOnce the final basis has been found by an optimiser, one may want to push further in the close neighbourhood to see if an even better basis can be found. A polish search takes the final basis of an optimiser as the start of a new guided tour to search for local improvements. The polish algorithm is similar to the CRS but with three distinctions: 1) a hundred rather than one candidate bases are generated each time in the inner loop; 2) the neighbourhood size is reduced in the inner loop, rather than in the outer loop; and 3) three more termination conditions have been added to ensure the new basis generated is distinguishable from the current one in terms of the distance in the space, the relative change in the index value, and neighbourhood size:\nthe distance between the basis found and the current needs to be larger than 1e-3;\nthe relative change of the index value needs to be larger than 1e-5; and\nthe alpha parameter needs to be larger than 0.01.\nFigure 9 presents the projected data and trace plot of a 2D projection, optimised by CRS and followed by the polish step. The top row shows the initial projection, the final projection after CRS, and the final projection after polish, respectively. The end basis found by CRS reveals the four clusters in the data, but the edges of each cluster are not clean-cut. Polish works with this end basis and further pushes the index value to produce clearer edges of the cluster, especially along the vertical axis.\n\n\n\nFigure 9: Comparison of the projected data before and after using polishing for a 2D projection problem on data using holes index. The top row shows the initial projected data and the final views after CRS and polish search, and the second row traces the index value. The clustering structure in the data is detected by CRS (top middle panel), but the polish step improves the index value and produces clearer boundaries of the clusters (top right panel), especially along the vertical axis. Note that the parameter is set to 400 in this experiment for CRS to do its best.\n\n\n\nSeeing the signal in the noise\nThe holes index function used for all the examples before this section produces a smooth interpolation, while this is not the case for all the indexes. An example of a noisy index function for 1D projections compares the projected data, \\(\\mathbf{Y}_{n \\times 1}\\), to a randomly generated normal distribution, \\(\\mathcal{N}_{n \\times 1}\\), using the Kolmogorov test. Let \\(F_{.}(n)\\) be the empirical cumulative distribution function (ECDF) with two possible subscripts, \\(Y\\) and \\(\\mathcal{N}\\), representing the projected and randomly generated data, and \\(n\\) denoting the number of observations, the Kolmogorov index \\(I^{nk}(n)\\), is defined as:\n\\[I^{K}(n) = \\max \\left[F_{Y}(n) - F_{\\mathcal{N}}(n)\\right].\\] With a non-smooth index function, two research questions are raised:\nwhether any optimiser fails to optimise this non-smooth index; and\nwhether the optimisers can find the global optimum despite the presence of a local optimum.\nFigure 10 presents the trace and PCA plots of all three optimisers, and as expected, none of the interpolated paths are smooth. There is barely any improvement made by PD, indicating its failure in optimising non-smooth index functions. While CRS and SA have managed to get close to the index value of the theoretical best, the trace plot shows that it takes SA much longer to find the final basis. This long interpolation path is partially due to the fluctuation in the early iterations, where SA tends to generously accept inferior bases before concentrating near the optimum. The PCA plot shows the interpolation path and search points, excluding the last termination iteration. Pseudo-Derivative (PD) quickly gets stuck near the starting position. Comparing the amount of random search done by CRS and SA, it is surprising that SA does not carry as many samples as CRS. Combining the insights from both the trace and PCA plot, one can learn the two different search strategies by CRS and SA: CRS frequently samples in the space and only make a move when an improvement is guaranteed to be made, while SA first broadly accepts bases in the space and then starts the extensive sampling in a narrowed subspace. The specific decision of which optimiser to use will depend on the index curve in the basis space, but if the basis space is non-smooth, accepting inferior bases at first, as SA has done here, can lead to a more efficient search in terms of the overall number of points evaluated.\n\n\n\nFigure 10: Comparison of the three optimisers in optimising \\(I^{nk}(n)\\) index for a 1D projection problem on a five-variable dataset, . Both CRS and SA succeed in the optimisation, PD fails to optimise this non-smooth index. Further, SA takes much longer than CRS to finish the optimisation, but finishes off closer to the theoretical best.\n\n\n\nThe next experiment compares the performance of CRS and SA when a local maximum exists. Two search neighbourhood sizes, 0.5 and 0.7, are compared to understand how a large search neighbourhood would affect the final basis and the length of the search. Figure 11 shows 80 paths simulated using 20 seeds in the PCA plot, faceted by the optimiser and search size. With CRS and a search size of 0.5, despite being the simplest and fastest, the optimiser fails in three instances where the final basis lands neither near the local nor the global optimum. With a larger search size of 0.7, more seeds have found the global maximum. Comparing CRS and SA for a search size of 0.5, SA does not seem to improve the final basis found, despite having longer interpolation paths. However, the denser paths near the local maximum are an indicator that SA is working hard to examine if there is any other optimum in the basis space, but the relatively small search size has diminished its ability to reach the global maximum. With a larger search size, almost all the seeds (16 out of 20) have found the global maximum, and some final bases are much closer to the theoretical best, as compared to the three other cases. This indicates that SA, with a reasonable large search window, is able to overcome the local optimum and optimise close towards the global optimum.\n\n\n\nFigure 11: Comparing 20 search paths in the PCA-projected basis space faceted by two optimisers: CRS and SA, and two search sizes: 0.5 and 0.7. The optimisation is on the 1D projection index, \\(I^{nk}(n)\\), for data, where a local optimum, annotated by the cross (x), is presented in this experiment, along with the global optimum (*).\n\n\n\nReconciling the orientation\nOne interesting situation observed in the previous examples is that, for some simulations, as shown on the left panel of Figure 12, the target basis is generated on the other half of the basis space, and the interpolator seems to draw a straight line to interpolate. Bases with opposite signs do not affect the projection and index value, but we would prefer the target to have the same orientation as the current basis. The orientation of two bases can be computationally checked by calculating the determinant – a negative value suggests the two bases have a different orientation. For 1D bases, this can be corrected by flipping the sign on one basis. For higher dimensions, it can be a bit more difficult because the orthonormality of the basis needs to be also maintained when an individual vector is flipped. Here, an orientation check is carried out once a new target basis is generated, and the sign in the target basis will be flipped if a negative determinant is obtained. The interpolation after implementing the orientation check is shown on the right panel of Figure 12, where the unsatisfactory interpolation no longer exists.\n\n\n\nFigure 12: Comparison of the interpolation in the PCA-projected basis space before and after reconciling the orientation of the target basis. Optimisation is on the 1D projection index, \\(I^{nk}(n)\\), for boa6 data using CRS with seed 2463. The dots represent the target basis in each iteration, and the path shows the interpolation. On the left panel, one target basis is generated with an opposite orientation to the current basis (hence appear on the other side of the basis space), and the interpolator crosses the origin to perform the interpolation. The right panel shows the same interpolation after implementing an orientation check, and the undesirable interpolation disappears.\n\n\n\n6 Implementation\nThis project contributes to the software development in two packages: a data collection object is implemented in tourr (Wickham et al. 2011), while the visual diagnostics of the optimisers is implemented in ferrn (Zhang et al. 2021). The functions in the ferrn (Zhang et al. 2021) package are listed below:\nMain plotting functions:\nexplore_trace_search() produces summary plots in Figure 2.\nexplore_trace_interp() produces trace plots for the interpolation points in Figure 3.\nexplore_space_pca() produces the PCA plot of projection bases on the reduced space. Figure 4 includes the additional details of anchor and search bases, which can be turned on by the argument details = TRUE. The animated version in Figure 5 is produced with argument animate = TRUE.\nexplore_space_tour() produces animated tour view on the full space of the projection bases in Figure 6.\n\nget_*() extracts and manipulates certain components from the existing data object.\nget_anchor() extracts target observations.\nget_basis_matrix() flattens all the bases into a matrix.\nget_best() extracts the observation with the highest index value in the data object.\nget_dir_search() extracts directional search observations for PD search.\nget_interp() extracts interpolated observations.\nget_interp_last() extracts the ending interpolated observations in each iteration.\nget_interrupt() extracts the ending interpolated observations and the target observations if the interpolation is .interrupted\nget_search() extracts search observations.\nget_search_count() extracts the count of search observations.\nget_space_param() produces the coordinates of the centre and radius of the basis space.\nget_start() extracts the starting observation.\nget_theo() extracts the theoretical best observations, if given.\n\nbind_*() incorporates additional information outside the tour optimisation into the data object.\nbind_theoretical() binds the theoretical best observation in simulated experiment.\nbind_random() binds randomly generated bases in the projection bases space to the data object.\nbind_random_matrix() binds randomly generated bases and outputs in a matrix format.\n\nadd_*() provides wrapper functions to create ggprotos for different components for the PCA plot\nadd_anchor() for plotting anchor bases.\nadd_anno() for annotating the symmetry of start bases.\nadd_dir_search() for plotting the directional search bases with magnified distance.\nadd_end() for plotting end bases.\nadd_interp() for plotting the interpolation path.\nadd_interp_last() for plotting the last interpolation bases for comparing with target bases when interruption is used.\nadd_interrupt() for linking the last interpolation bases with target ones when interruption is used.\nadd_search() for plotting search bases.\nadd_space() for plotting the circular space.\nadd_start() for plotting start bases.\nadd_theo() for plotting theoretical best bases, if applicable.\n\nUtilities\ntheme_fern() and format_label() for better display of the grid lines and axis formatting.\nclean_method() to clean up the name of the optimisers.\nbotanical_palettes() is a collection of colour palettes from Australian native plants. Quantitative palettes include daisy, banksia, and cherry, and sequential palettes contain fern and acacia.\nbotanical_pal() as the colour interpolator.\nscale_color_*() and scale_fill_*() for scaling the colour and fill of the plot.\n\n7 Conclusion\nThis paper has provided several visual diagnostics that can be used for understanding a complex optimisation procedure and are implemented in the ferrn package. The methods were illustrated using the optimisers available for projection pursuit guided tour. Here the constraint is the orthonormality condition of the projection bases, which corresponds to optimisation over spheres and torii. The approach described broadly applies to other constrained optimisers. Although the manifold in \\(p\\)-space might be different the diagnostic techniques are the same. A researcher would begin by saving the path of the optimiser in a form required to input into the ferrn package, as described in this paper. One might generally make more samples from the constrained space upon which to assess and compare the optimisation paths. These high-dimensional data objects can then be viewed using the tour.\nThe progressive optimisation of a target function and its coverage of the search space can be viewed in both reduced 2D space and the full space. These visualisations can lead to insights for evaluating and comparing the performance of multiple optimisers operating on the same task. They can provide a better understanding of existing methods or motivate the development of new approaches. For example, we have compared how three optimisers perform when maximising a non-smooth index function and have illustrated how the pseudo-derivative search fails in this setting. The observations from our experiments have also been translated into improved optimisation methods for the guided tour, e.g., we introduced the option to interrupt the search if a better basis is found along the path.\nThis work might be considered an effort to bring transparency into algorithms. Although little attention is paid by algorithm developers to providing ways to output information during intermediate steps, this is an important component for enabling others to understand and diagnose the performance. Algorithms are an essential component of artificial intelligence that is used to make daily life easier. Interpretability of algorithms is important to guard against aspects like bias and inappropriate use. The data object underlying the visual diagnostics here is an example of what might be useful in algorithm development generally.\n8 Acknowledgements\nThis article is created using knitr (Xie 2015) and rmarkdown (Xie et al. 2018) in R. The source code for reproducing this paper can be found at: https://github.com/huizezhang-sherry/paper-ferrn.\n\nCRAN packages used\nferrn, tourr, dplyr, ggplot2, geozoo, knitr, rmarkdown\nCRAN Task Views implied by cited packages\nReproducibleResearch, Databases, ModelDeployment, Multivariate, Phylogenetics, TeachingStatistics\n\n\nS. Andradóttir. A review of random search methods. In Handbook of simulation optimization, pages. 277–292 2015. Springer. URL https://doi.org/10.1007/978-1-4939-1384-8.\n\n\nD. P. Bertsekas. Constrained optimization and lagrange multiplier methods. Academic press, 2014. URL https://doi.org/10.1016/C2013-0-10366-2.\n\n\nD. Bertsimas and J. Tsitsiklis. Simulated annealing. Statistical Science, 8(1): 10–15, 1993. URL https://doi.org/10.1214/ss/1177011077.\n\n\nS. H. Brooks. A discussion of random methods for seeking maxima. Operations Research, 6(2): 244–251, 1958. URL https://doi.org/10.1287/opre.6.2.244.\n\n\nA. Buja and D. Asimov. Grand tour methods: An outline. In Proceedings of the seventeenth symposium on the interface of computer sciences and statistics, pages. 63–67 1986. USA: Elsevier North-Holland, Inc. ISBN 9780444700186. URL https://dl.acm.org/doi/10.5555/26036.26046.\n\n\nA. Buja, D. Cook, D. Asimov and C. Hurley. Computational methods for high-dimensional rotations in data visualization. Handbook of Statistics, 24: 391–413, 2005. URL https://doi.org/10.1016/S0169-7161(04)24014-7.\n\n\nA. R. Conn, K. Scheinberg and L. N. Vicente. Introduction to derivative-free optimization. SIAM, 2009. URL https://doi.org/10.1137/1.9780898718768.\n\n\nD. Cook and A. Buja. Manual controls for high-dimensional data projections. Journal of Computational and Graphical Statistics, 6(4): 464–480, 1997. URL https://doi.org/10.2307/1390747.\n\n\nD. Cook, A. Buja and J. Cabrera. Projection pursuit indexes based on orthonormal function expansions. Journal of Computational and Graphical Statistics, 2(3): 225–250, 1993. URL https://doi.org/10.2307/1390644.\n\n\nD. Cook, A. Buja, J. Cabrera and C. Hurley. Grand tour and projection pursuit. Journal of Computational and Graphical Statistics, 4(3): 155–172, 1995. URL https://doi.org/10.1080/10618600.1995.10474674.\n\n\nD. Cook, A. Buja, E.-K. Lee and H. Wickham. Grand tours, projection pursuit guided tours, and manual controls. In Handbook of data visualization, pages. 295–314 2008. Springer. URL https://doi.org/10.1007/978-3-540-33037-0_13.\n\n\nP. Diaconis and D. Freedman. Asymptotics of graphical projection pursuit. The Annals of Statistics, 793–815, 1984. URL https://doi.org/10.1214/aos/1176346703.\n\n\nJ. H. Friedman and J. W. Tukey. A projection pursuit algorithm for exploratory data analysis. IEEE Transactions on Computers, 100(9): 881–890, 1974. URL https://doi.org/10.1109/T-C.1974.224051.\n\n\nV. Granville, M. Krivánek and J.-P. Rasson. Simulated annealing: A proof of convergence. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(6): 652–656, 1994. URL https://doi.org/10.1109/34.295910.\n\n\nP. Hall. On polynomial-based projection indices for exploratory projection pursuit. The Annals of Statistics, 17(2): 589–605, 1989. URL https://doi.org/10.1214/aos/1176347127.\n\n\nK. Healy. Data visualization: A practical introduction. Princeton University Press, 2018. URL https://socviz.co/.\n\n\nD. R. Jones, M. Schonlau and W. J. Welch. Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13(4): 455–492, 1998. URL https://doi.org/10.1023/A:1008306431147.\n\n\nS. Kirkpatrick, C. D. Gelatt and M. P. Vecchi. Optimization by simulated annealing. Science, 220(4598): 671–680, 1983. URL https://doi.org/10.1126/science.220.4598.671.\n\n\nU. Laa and D. Cook. Using tours to visually investigate properties of new projection pursuit indexes with application to problems in physics. Computational Statistics, 1–35, 2020. URL https://doi.org/10.1007/s00180-020-00954-8.\n\n\nE. Lee, D. Cook, S. Klinke and T. Lumley. Projection pursuit for exploratory supervised classification. Journal of Computational and Graphical Statistics, 14(4): 831–846, 2005. URL https://doi.org/10.1198/106186005X77702.\n\n\nE.-K. Lee and D. Cook. A projection pursuit index for large p small n data. Statistics and Computing, 20(3): 381–392, 2010. URL https://doi.org/10.1007/s11222-009-9131-1.\n\n\nM. Li, Z. Zhao and C. Scheidegger. Visualizing neural networks with the grand tour. Distill, 2020. URL https://doi.org/10.23915/distill.00025.\n\n\nN. Loperfido. Kurtosis-based projection pursuit for outlier detection in financial time series. The European Journal of Finance, 26(2-3): 142–164, 2020. URL https://doi.org/10.1080/1351847X.2019.1647864.\n\n\nN. Loperfido. Skewness-based projection pursuit: A computational approach. Computational Statistics and Data Analysis, 120(C): 42–57, 2018. URL https://doi.org/10.1016/j.csda.2017.11.001.\n\n\nD. Mitra, F. Romeo and A. Sangiovanni-Vincentelli. Convergence and finite-time behavior of simulated annealing. Advances in Applied Probability, 18(3): 747–771, 1986. URL https://doi.org/10.1109/CDC.1985.268600.\n\n\nC. Posse. Projection pursuit exploratory data analysis. Computational Statistics & Data Analysis, 20(6): 669–687, 1995. URL https://doi.org/10.1016/0167-9473(95)00002-8.\n\n\nL. M. Rios and N. V. Sahinidis. Derivative-free optimization: A review of algorithms and comparison of software implementations. Journal of Global Optimization, 56(3): 1247–1293, 2013. URL https://doi.org/10.1007/s10898-012-9951-y.\n\n\nH. E. Romeijn. Random search methods. In Encyclopedia of optimization, pages. 3245–3251 2009. Boston, MA: Springer US. URL https://doi.org/10.1007/978-0-387-74759-0_556.\n\n\nB. Schloerke. Geozoo: Zoo of geometric objects. 2016. URL https://CRAN.R-project.org/package=geozoo. R package version 0.5.1.\n\n\nJ. C. Spall. Introduction to stochastic search and optimization: Estimation, simulation, and control. John Wiley & Sons, 2005. URL https://www.jhuapl.edu/ISSO/.\n\n\nJ. W. Tukey. Exploratory data analysis. Reading, MA, 1977.\n\n\nA. Unwin. Graphical data analysis with r. CRC Press, 2015. URL https://doi.org/10.1201/9781315370088.\n\n\nR. C. White. A survey of random methods for parameter optimization. Simulation, 17(5): 197–205, 1971. URL https://doi.org/10.1177/003754977101700504.\n\n\nH. Wickham. ggplot2: Elegant graphics for data analysis. Springer-Verlag New York, 2016. URL https://doi.org/10.1007/978-0-387-98141-3.\n\n\nH. Wickham. Tidy data. Journal of Statistical Software, 59(10): 1–23, 2014. URL https://doi.org/10.18637/jss.v059.i10.\n\n\nH. Wickham, D. Cook, H. Hofmann and A. Buja. tourr: An R package for exploring multivariate data with projections. Journal of Statistical Software, 40(2): 1–18, 2011. URL http://doi.org/10.18637/jss.v040.i02.\n\n\nH. Wickham, R. François, L. Henry and K. Müller. Dplyr: A grammar of data manipulation. 2020. URL https://CRAN.R-project.org/package=dplyr. R package version 1.0.2.\n\n\nC. O. Wilke. Fundamentals of data visualization: A primer on making informative and compelling figures. O’Reilly Media, 2019. URL https://clauswilke.com/dataviz/.\n\n\nL. Wilkinson, A. Anand and R. Grossman. Graph-theoretic scagnostics. In IEEE symposium on information visualization, 2005. INFOVIS 2005., pages. 157–164 2005.\n\n\nL. Wilkinson and G. Wills. Scagnostics distributions. Journal of Computational and Graphical Statistics, 17(2): 473–491, 2008.\n\n\nY. Xie. Dynamic documents with R and knitr. 2nd ed Boca Raton, Florida: Chapman; Hall/CRC, 2015. URL https://yihui.name/knitr/. ISBN 978-1498716963.\n\n\nY. Xie, J. J. Allaire and G. Grolemund. R markdown: The definitive guide. Boca Raton, Florida: Chapman; Hall/CRC, 2018. URL https://bookdown.org/yihui/rmarkdown. ISBN 978-1138359338.\n\n\nZ. B. Zabinsky. Stochastic adaptive search for global optimization. Springer Science & Business Media, 2013. URL https://doi.org/10.1007/978-1-4419-9182-9.\n\n\nH. S. Zhang, D. Cook, U. Laa, N. Langrené and P. Menéndez. Ferrn: Facilitate exploration of touRR optimisatioN. 2021. URL https://github.com/huizezhang-sherry/ferrn/. R package version 0.0.1.\n\n\n\n\n",
    "preview": "articles/RJ-2021-105/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 720,
    "preview_height": 360
  },
  {
    "path": "articles/RJ-2021-107/",
    "title": "RobustBF: An R Package for Robust Solution to the Behrens-Fisher Problem",
    "description": "Welch’s two-sample t-test based on least squares (LS) estimators is generally used to test the equality of two normal means when the variances are not equal. However, this test loses its power when the underlying distribution is not normal. In this paper, two different tests are proposed to test the equality of two long-tailed symmetric (LTS) means under heterogeneous variances. Adaptive modified maximum likelihood (AMML) estimators are used in developing the proposed tests since they are highly efficient under LTS distribution. An R package called RobustBF is given to show the implementation of these tests. Simulated Type I error rates and powers of the proposed tests are also given and compared with Welch’s t-test based on LS estimators via an extensive Monte Carlo simulation study.",
    "author": [
      {
        "name": "Gamze Güven",
        "url": {}
      },
      {
        "name": "Şükrü Acıtaş",
        "url": {}
      },
      {
        "name": "Hatice Şamkar",
        "url": {}
      },
      {
        "name": "Birdal Şenoğlu",
        "url": {}
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRobustBF, asht, WRS2\nCRAN Task Views implied by cited packages\nRobust\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-108/",
    "title": "Software Engineering and R Programming: A Call for Research",
    "description": "Although R programming has been a part of research since its origins in the 1990s, few studies address scientific software development from a Software Engineering (SE) perspective. The past few years have seen unparalleled growth in the R community, and it is time to push the boundaries of SE research and R programming forwards. This paper discusses relevant studies that close this gap Additionally, it proposes a set of good practices derived from those findings aiming to act as a call-to-arms for both the R and RSE (Research SE) community to explore specific, interdisciplinary paths of research.",
    "author": [
      {
        "name": "Melina Vidoni",
        "url": {}
      }
    ],
    "date": "2021-12-14",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngenthat, roxygen2, pkgdown, covr, testthat, tidyverse\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-109/",
    "title": "We Need Trustworthy R Packages",
    "description": "There is a need for rigorous software engineering in R packages, and there is a need for new research to bridge scientific computing with more traditional computing. Automated tools, interdisciplinary graduate courses, code reviews, and a welcoming developer community will continue to democratize best practices. Democratized software engineering will improve the quality, correctness, and integrity of scientific software, and by extension, the disciplines that rely on it.",
    "author": [
      {
        "name": "William Michael Landau",
        "url": {}
      }
    ],
    "date": "2021-12-14",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntestthat, covr, R6\nCRAN Task Views implied by cited packages\nDatabases\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-110/",
    "title": "The R Developer Community Does Have a Strong Software Engineering Culture",
    "description": "There is a strong software engineering culture in the R developer community. We recommend creating, updating and vetting packages as well as keeping up with community standards. We invite contributions to the rOpenSci project, where participants can gain experience that will shape their work and that of their peers.",
    "author": [
      {
        "name": "Maëlle Salmon",
        "url": "https://masalmon.eu"
      },
      {
        "name": "Karthik Ram",
        "url": "https://ram.berkeley.edu/"
      }
    ],
    "date": "2021-12-14",
    "categories": [],
    "contents": "\nIntroduction\nThe R programming language was originally created for statisticians, by statisticians, but evolved over time to attract a “massive pool of talent that was previously untapped” (Hadley Wickham in Thieme (2018)).\nDespite the fact that most R users are academic researchers and business data analysts without a background in software engineering, we are witnessing a rapid rise in software engineering within the community.\nIn this comment we spotlight recent progress in tooling, dissemination and support, including specific efforts led by the rOpenSci project.\nWe hope that readers will take advantage of and participate in the tools and practices we describe.\nThe modern R package developer toolbox: user-friendlier, more comprehensive\nThe basic infrastructure for creating, building, installing, and checking packages has been in place since the early days of the R language. During this time (1998-2011), the barriers to entry were very high and access to support and Q&A for beginners were extremely limited. With the introduction of the devtools (Wickham et al. 2021b) package in 2011, the process of creating and updating packages became substantially easier. Documentation also became simpler to maintain. The roxygen2 (Wickham et al. 2021a) package allowed developers to keep documentation in sync with changes in code, similar to the doxygen approach that was embraced in more mature languages. Combined with the rise in popularity of StackOverflow and the growth of rstats blogs, the number of packages on the Comprehensive R Archive Network (CRAN) skyrocketed from 400 new packages in 2010 to 1000 new packages by 2014. As of this writing, there are nearly 19k packages on CRAN.\nFor novices without substantial software engineer experience, the early testing frameworks were also difficult to use. With the release of testthat (Wickham 2011), testing also became smoother.\nThere are now several actively maintained testing frameworks such as tinytest (van der Loo 2020); as well as testthat-compatible specialized tooling for testing database interactions (dittodb (Keane and Vargas 2020)), web resources (vcr (Chamberlain 2021)), httptest (Richardson 2021), and webfakes (Csárdi 2021) which enables the use of an embedded C/C++ web server for testing HTTP clients like httr2 (Wickham 2021)).\nThe testthat package has recently been improved with snapshot tests that make it possible to test plot outputs.\nThe rOpenSci project has released autotest (Padgham 2021), a package that supports automatic mutation testing.\nBeyond checking for compliance with R CMD CHECK, several other packages such as goodpractice (Csárdi and Frick 2018), riskmetric (R Validation Hub et al. 2021), rOpenSci’s pkgcheck (Padgham and Salmon 2021) check packages against a large list of actionable, community recommended best practices for software development. Collectively these tools allow domain researchers to release software packages that meet high standards for software engineering.\nThe development and testing ecosystem of R is rich and has sometimes borrowed successful implementations from other languages (e.g. the vcr R package is a port, i.e. translation to R, of the vcr Ruby gem; testthat snapshot tests were inspired by JS Jest1).\nEmergence of a welcoming community\nAs underlined in Thieme (2018), community is the strong suit of the R language. Many organizations and venues offer dedicated support for package developers.\nExamples include Q&A on the r-package-devel mailing list2, and the package development category of the RStudio community forum3, and the rstats section of StackOverflow4. Traditionally, R package developers have been mostly male and white. Although the status quo remains similar, efforts from groups such as R-Ladies5 meetups, Minorities in R (Scott and Smalls-Perkins 2020), and the package development modules offered by Forwards for underrepresented groups6 have made considerable inroads towards improving diversity. These efforts have\nworked hard to put the spotlight on developers beyond the “usual suspects”.\nrOpenSci community and software review\nThe rOpenSci organization (Boettiger et al. 2015) is an attractive venue for developers & supporters of scientific R software. One of our most successful and continuing initiatives is our Software Peer Review system (Ram et al. 2019), a combination of academic peer-review and code review from industry.\nAbout 150 packages have been reviewed by volunteers to date, creating better packages as well as a growing knowledgebase in our development guide (rOpenSci et al. 2021) while also building a living community of practice.\nOur model has been the fundamental inspiration for projects such as the Journal of Open Source Software (Smith et al. 2018), and PyOpenSci [Wasser and Holdgraf (2019)](Trizna et al. 2021).\nWe are continuously improving our system and reducing cognitive overload on editors and reviewers by automating repetitive tasks. Most recently we have expanded our offerings to peer review of packages that implement statistical methods (Statistical Software Peer Review) (Padgham et al. 2021).\nBeside software review, rOpenSci community is a safe, welcoming and informative place for package developers, with Q&A happening on our public forum and semi-open Slack workspace. (Butland and LaZerte 2020)\nCreation and dissemination of resources for R programmers\nThe aforementioned tools, venues and organizations benefit from and support crucial dissemination efforts.\nPublishing technical know-how is crucial for progress of the R community. R news has been circulating on Twitter7, R Weekly8 and R-Bloggers9.\nSome sources have been more specifically aimed at R package developers of various experience and interests.\nWhile “Writing R Extensions” 10 is the official & exhaustive reference on writing R packages, it is a reference rather than a learning resource: many R package developers, if not learning by example, get introduced to R package development via introductory blog posts or tutorials, and the R packages book by Hadley Wickham and Jenny Bryan [Wickham (2015)](Wickham and Bryan) that accompany the devtools suite of packages is freely available online and strives to improving the R package development experience.\nThe rOpenSci guide “rOpenSci Packages: Development, Maintenance, and Peer Review” (rOpenSci et al. 2021) contains our community-contributed guidance on how to develop packages and review them.\nIt features opinionated requirements such as the use of roxygen2 (Wickham et al. 2021a) for package documentation; criteria helping make an informed decision on gray area topics such as limiting dependencies; advice on widely accepted and emerging tools.\nAs it is a living document also used as reference for editorial decisions, we maintain a changelog11, and summarize each release in a blog post12.\nrOpenSci also hosts a book on a specialized topic, HTTP testing in R13, that presents both principles for testing packages that interact with web resources, as well as relevant packages.\nBeside these examples of long-form documentation, knowledge around R software engineering is shared through blogs and talks.\nIn the R blogging world, the rOpenSci blog posts14, technical notes15 and a section of our monthly newsletter16 feature some topics relevant to package developers, as do some of the posts on the Tidyverse blog17.\nThe blog of the R-hub project18 contains information on package development topics, in particular about common problems such as sharing data via R packages or understanding CRAN checks.\nExpert programmers have been sharing their R specific wisdom as well as software engineering lessons learned from other languages (e.g. Jenny Bryan’s useR! Keynote address “code feels, code smells”19).\nConclusion\nIn summary, we observe that there is already a strong software engineering culture in the R developer community. By surfacing the rich suite of resources to new developers we can but only hope the future will bring success to all aforementioned initiatives.\nWe recommend creating, updating and vetting packages with the tools we mentioned as well as keeping up with community standards with the venues we mentioned in the previous section.\nWe invite contributions to the rOpenSci project, where participants can gain experience that will shape their work and that of their peers.\nThanks to these efforts, we hope the R community will continue to be a thriving place of application for software engineering, by diverse practitioners from many different paths.\n\nCRAN packages used\ndevtools, roxygen2, testthat, tinytest, dittodb, vcr, httptest, webfakes, httr2, autotest, goodpractice, riskmetric, pkgcheck\nCRAN Task Views implied by cited packages\nWebTechnologies, Databases\n\n\nC. Boettiger, S. Chamberlain, E. Hart and K. Ram. Building software, building community: Lessons from the rOpenSci project. Journal of Open Research Software, 3(1): e8, 2015. DOI 10.5334/jors.bu.\n\n\nS. Butland and S. LaZerte. rOpenSci community contributing guide. Zenodo, 2020. URL https://contributing.ropensci.org/.\n\n\nS. Chamberlain. Vcr: Record ’HTTP’ calls to disk. 2021. URL https://CRAN.R-project.org/package=vcr. R package version 1.0.2.\n\n\nG. Csárdi. Webfakes: Fake web apps for HTTP testing. 2021. https://webfakes.r-lib.org/, https://github.com/r-lib/webfakes.\n\n\nG. Csárdi and H. Frick. Goodpractice: Advice on r package building. 2018. URL https://CRAN.R-project.org/package=goodpractice. R package version 1.0.2.\n\n\nJ. Keane and M. Vargas. Dittodb: A test environment for database requests. 2020. URL https://CRAN.R-project.org/package=dittodb. R package version 0.1.3.\n\n\nM. Padgham. Autotest: Automatic package testing. 2021. https://docs.ropensci.org/autotest/, https://github.com/ropensci-review-tools/autotest.\n\n\nM. Padgham and M. Salmon. Pkgcheck: rOpenSci package checks. 2021. https://docs.ropensci.org/pkgcheck/, https://github.com/ropensci-review-tools/pkgcheck.\n\n\nM. Padgham, M. Salmon, N. Ross, J. Nowosad, R. FitzJohn, yilong zhang, C. Sax, F. Rodriguez-Sanchez, F. Briatte and L. Collado-Torres. ropensci/statistical-software-review-book: Official first standards versions. Zenodo, 2021. URL https://doi.org/10.5281/zenodo.5556756.\n\n\nR Validation Hub, D. Kelkhoff, M. Gotti, E. Miller, K. K, Y. Zhang, E. Milliman and J. Manitz. Riskmetric: Risk metrics to evaluating r packages. 2021. https://pharmar.github.io/riskmetric/, https://github.com/pharmaR/riskmetric.\n\n\nK. Ram, C. Boettiger, S. Chamberlain, N. Ross, M. Salmon and S. Butland. A community of practice around peer review for long-term research software sustainability. Computing in Science Engineering, 21(2): 59–65, 2019. DOI 10.1109/MCSE.2018.2882753.\n\n\nN. Richardson. Httptest: A test environment for HTTP requests. 2021. https://enpiar.com/r/httptest/, https://github.com/nealrichardson/httptest.\n\n\nrOpenSci, B. Anderson, S. Chamberlain, L. DeCicco, J. Gustavsen, A. Krystalli, M. Lepore, L. Mullen, K. Ram, N. Ross, et al. rOpenSci Packages: Development, Maintenance, and Peer Review. Zenodo, 2021. URL https://doi.org/10.5281/zenodo.4554776.\n\n\nD. Scott and D. Smalls-Perkins. Introducing MiR: A community for underrepresented minority users of r. Medium, 2020. URL https://medium.com/@doritolay/introducing-mir-a-community-for-underrepresented-users-of-r-7560def7d861.\n\n\nA. M. Smith, K. E. Niemeyer, D. S. Katz, L. A. Barba, G. Githinji, M. Gymrek, K. D. Huff, C. R. Madan, A. C. Mayes, K. M. Moerman, et al. Journal of open source software (JOSS): Design and first-year review. PeerJ Computer Science, 4: e147, 2018. URL https://doi.org/10.7717/peerj-cs.147.\n\n\nN. Thieme. R generation. Significance, 15(4): 14–19, 2018. URL https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1740-9713.2018.01169.x.\n\n\nM. Trizna, L. A. Wasser and D. Nicholson. pyOpenSci: Open and reproducible research, powered by python. Biodiversity Information Science and Standards, 5: e75688, 2021. URL https://doi.org/10.3897/biss.5.75688.\n\n\nM. van der Loo. A method for deriving information from running r code. The R Journal, Accepted for publication, 2020. URL https://arxiv.org/abs/2002.07472.\n\n\nL. A. Wasser and C. Holdgraf. pyOpenSci Promoting Open Source Python Software To Support Open Reproducible Science. In AGU fall meeting abstracts, pages. NS21A–13 2019.\n\n\nH. Wickham. httr2: Perform HTTP requests and process the responses. 2021. URL https://CRAN.R-project.org/package=httr2. R package version 0.1.1.\n\n\nH. Wickham. R packages. O’Reilly Media, 2015.\n\n\nH. Wickham. Testthat: Get started with testing. The R Journal, 3: 5–10, 2011. URL https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf.\n\n\nH. Wickham and J. Bryan. R packages.URL https://r-pkgs.org/.\n\n\nH. Wickham, P. Danenberg, G. Csárdi and M. Eugster. roxygen2: In-line documentation for r. 2021a. URL https://CRAN.R-project.org/package=roxygen2. R package version 7.1.2.\n\n\nH. Wickham, J. Hester and W. Chang. Devtools: Tools to make developing r packages easier. 2021b. URL https://CRAN.R-project.org/package=devtools. R package version 2.4.2.\n\n\nhttps://www.tidyverse.org/blog/2020/10/testthat-3-0-0/#snapshot-testing↩︎\nhttps://stat.ethz.ch/mailman/listinfo/r-package-devel↩︎\nhttps://community.rstudio.com/c/package-development/11↩︎\nhttps://stackoverflow.com/questions/tagged/r?tab=Newest↩︎\nhttp://rladies.org/↩︎\nhttps://buzzrbeeline.blog/2021/02/09/r-forwards-package-development-modules-for-women-and-other-underrepresented-groups/↩︎\nhttps://www.t4rstats.com/↩︎\nhttps://rweekly.org/↩︎\nhttps://www.r-bloggers.com/↩︎\nhttps://cran.r-project.org/doc/manuals/R-exts.html↩︎\nhttps://devguide.ropensci.org/booknews.html↩︎\nhttps://ropensci.org/tags/dev-guide/↩︎\nhttps://books.ropensci.org/http-testing/↩︎\nhttps://ropensci.org/blog/↩︎\nhttps://ropensci.org/technotes/↩︎\nhttps://ropensci.org/news/↩︎\nhttps://www.tidyverse.org/categories/programming/↩︎\nhttps://blog.r-hub.io/post/↩︎\nhttps://github.com/jennybc/code-smells-and-feels↩︎\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-111/",
    "title": "The R Quest: from Users to Developers",
    "description": "R is not a programming language, and this produces the inherent dichotomy between analytics and software engineering. With the emergence of data science, the opportunity exists to bridge this gap, especially through teaching practices.",
    "author": [
      {
        "name": "Simon Urbanek",
        "url": {}
      }
    ],
    "date": "2021-12-14",
    "categories": [],
    "contents": "\nGenesis: How did we get here?\nThe article “Software Engineering and R Programming: A Call to Action” summarizes the dichotomy between analytics and software engineering in the R ecosystem, provides examples where this leads to problems and proposes what we as R users can do to bridge the gap.\nData Analytic Language\nThe fundamental basis of the dichotomy is inherent in the evolution of S and R: they are not programming languages, but they ended up being mistaken for such. S was designed to be a data analytic language: to turn ideas into software quickly and faithfully, often used in “non-programming” style (Chambers 1998). Its original goal was to enable the statisticians to apply code which was written in programming languages (at the time mostly FORTRAN) to analyze data quickly and interactively - for some suitable definition of “interactive” at the time (Becker 1994). The success of S and then R can be traced to the ability to perform data analysis by applying existing tools to data in creative ways. A data analysis is a quest - at every step we learn more about the data which informs our decision about next steps. Whether it is an exploratory data analysis leveraging graphics or computing statistics or fitting models - the final goal is typically not known ahead of time, it is obtained by an iterative process of applying tools that we as analysts think may lead us further (Tukey 1977). It is important to note that this is exactly the opposite of software engineering where there is a well-defined goal: a specification or desired outcome, which simply needs to be expressed in a way understandable to the computer.\nFreedom for All\nThe second important design aspect rooted in the creativity required is the freedom the language provides. Given that the language can be computed upon means that a given expression may have different meaning depending on how the called function decides to treat it and such deviations are not entirely uncommon, typically referred to as non-standard evaluation. Probably the best example is the sub-language defined by the package (Dowle and Srinivasan 2021) featuring the := operator which is parsed, but not even used by the R language.\nAnalogously, there is no specific, prescribed object system, but rather one is free to implement any idea desirable, as witnessed by the fact that there are more than a handful of object system definitions available in R and contributed packages. This freedom is what makes R great for experimentation with new ideas or concepts, but very hard to treat as a programming language.\nWe have a language that is built on the idea of applying tools and which allows freedom to express new ideas so the last important step is how to define new tools. R add-on packages (R Core Team 2021) are the vehicle by which new tools can be defined and distributed to R users. Note that true to design goals, packages are not limited to R code but rather can also include code written in programming languages such as C, C++ or Fortran. That in turn makes it possible to write packages that expand the scope of tools to other languages such as Java with RJava (Urbanek 2021) or Python with reticulate (Ushey et al. 2022) simply by creating an R package which defines the interface.\nSharing Packages\nBut this is also where we are entering the realm of software engineering. Now we are in the business of defining the tools as opposed to just using the tools. It also means that the tools have to worry about programming interfaces, defining behavior and all those pesky things we as statisticians don’t want to worry about. Although we originally started as R users, the moment we want to share any re-usable piece of code with others we are becoming developers. Since no developer would mistake R for a programming language, it is analysts with background in various fields which use statistics one way or another that are more likely to use R. However, as we become more comfortable with R, we start using it as a programming language, not just analytic language, often because it is simply more convenient than having to learn a programming language. This explains the empirical evidence (Pinto et al. 2018) of R package authors not being trained software engineers, but often scientists from other fields and any consequences thereof.\nHowever, as R packages started to emerge, it became clear that a loosely coupled structure is not enough and have to introduce software engineering concepts such as documentation and testing. R includes tools for automated checking for packages to be able to provide at least some basic guarantees. Packages provide examples which are supposed to be illustrative, but soon were used to perform limited testing. R itself is using the same package structure and it was clear early that a test suite is important and so was introduced. Consequently, the same facilities were available to packages, but only very few were using it. There are, however, no built-in tools for creating test suites. In core R those are hand-curated by experienced developers, but that does not scale to package space.\nOver 18,000 packages are now present in the Comprehensive R Archive Network (CRAN), a repository which has arguably played major role in the success of R (Hornik and Leisch 2002). This is not only a valuable resource for users, but today this rich collection of contributed R code in being used as an automated test-suite for R. This is no coincidence, the importance of software engineering concepts has been identified by the CRAN team long time ago and the tools in R have been enhanced for that purpose (Hornik 2016). CRAN has been an invaluable asset for the development of R based on examples and limited tests alone. It allows us the R Core Team to test changes in R against code that was written by ingenious people that do not necessarily follow documentation, but instead write code that seems to work - possibly in ways not intended in the first place. Consequently, improving the quality and coverage of tests in packages has not only positive impact on the individual package, but on the quality of the entire CRAN ecosystem and R itself.\nCRAN performs reverse-dependency checks where packages are not allowed to break dependent package which is an important software engineering concept. One can see CRAN as performing continuous integration and continuous testing if we consider all submitted packages as one big project. This is not universally liked among package authors, though. Some find it too tedious to be responsible for software in the way a software engineer would be - a concern which is also highlighted by the article.\nSteal and Borrow\nOne perhaps surprising finding of the article was the analysis of code fragment re-use (Claes et al. 2015). A quite recent example how dangerous such practice is was a piece of badly written JavaScript code from Stack Overflow (StackOverflow) which was copied so often that it made it into the popular Unity game engine, effectively forcing browsers to lie about macOS versions (Chromium Bugs) just to not break millions of released products. R code fragments are less likely to have such world-wide impact, but can be equally frustrating. The historically relatively high cost of loading other packages was an incentive to simply copy fragments instead, but the performance impact has been diminishing with advancements in the R implementation. Still, I believe the exact reasons for fragment re-use deserve further examination and may reveal other, more benign motives.\nEvery Project Needs a Conductor\nAnother good example of introducing software engineering principles into the R world successfully is the Bioconductor project (Gentleman et al. 2004). The authors realized early that the project is too big for it to allow organic growth and have strongly encouraged the use of the S4 class system to build a class hierarchy specific to the tasks common to the Bioconductor packages. This enabled optimizations of implementation as a core part of the system as opposed to individual approaches in each package. Bioconductor was also encouraging unit tests and has maintained a build and reporting system similar to that of CRAN, in the early days even pioneering functionality that was later added to core R.\nThe Gospel of Data Science\nI believe the Call to Action is a very timely contribution. Many R users start as statisticians or data analysts in some domain since that is the main strength of R. Consequently, a lot of R code is never publicly visible. Code written for data analyses is not software development and is not published as software. So any global statistics about R code have to be taken with that in mind. When considering R packages we are talking only about a fraction of the code written in R. However, building new tools is an important part of the R ecosystem and it has to be made clear that it is different from data analysis and thus requires different skills and tools.\nThe main realization here is that at some point an R user may become an R developer, crossing the line from analysis into software engineering. And we are often unprepared for that, in part because of our diverse background. When I asked my junior colleagues at the Labs what they find most challenging yet valuable, the top item was learning software engineering skills on the job. We were lucky to have both the authors of S as well as the authors of Unix on the same floor, so we were able to bridge the gap, but generally our schools don’t prepare for that. That’s why I believe we must teach statistical computing together with software engineering skills such as re-usability and testing concepts. The current popularity of data science which bridges both worlds is a good excuse to make it actually happen in practice.\n\nCRAN packages used\ndata.table, RJava, reticulate\nCRAN Task Views implied by cited packages\nFinance, HighPerformanceComputing, ModelDeployment, NumericalMathematics, TimeSeries\n\n\nR. A. Becker. A brief history of S. AT&T Bell Laboratories. 1994.\n\n\nJ. M. Chambers. Programming with data: A guide to the s language. 1st ed Berlin, Heidelberg: Springer-Verlag, 1998.\n\n\nChromium Bugs. Nearly all Unity WebGL games fail to run in Chrome on macOS 11 because of userAgent.,URL https://bugs.chromium.org/p/chromium/issues/detail?id=1171998.\n\n\nM. Claes, T. Mens, N. Tabout and P. Grosjean. An empirical study of identical function clones in CRAN. In 2015 IEEE 9th International Workshop on Software Clones (IWSC), pages. 19–25 2015. DOI 10.1109/IWSC.2015.7069885.\n\n\nM. Dowle and A. Srinivasan. Data.table: Extension of ‘data.frame‘. 2021. URL https://CRAN.R-project.org/package=data.table. R package version 1.14.2.\n\n\nR. C. Gentleman, V. J. Carey, D. M. Bates, B. Bolstad, M. Dettling, S. Dudoit, B. Ellis, L. Gautier, Y. Ge, J. Gentry, et al. Bioconductor: Open software development for computational biology and bioinformatics. Genome Biology, 5(10): R80, 2004. URL https://doi.org/10.1186/gb-2004-5-10-r80.\n\n\nK. Hornik. Are there too many R packages? Austrian Journal of Statistics, 41(1): 59–66, 2016. DOI 10.17713/ajs.v41i1.188.\n\n\nK. Hornik and F. Leisch. Vienna and R: Love, marriage and the future. Festschrift 50 Jahre Österreichische Statistische Gesellschaft, 61–70, 2002.\n\n\nG. Pinto, I. Wiese and L. F. Dias. How do scientists develop scientific software? An external replication. In 2018 IEEE 25th international conference on software analysis, evolution and reengineering (SANER), pages. 582–591 2018. DOI 10.1109/SANER.2018.8330263.\n\n\nR Core Team. Writing R extensions. Vienna, Austria: R Foundation for Statistical Computing, 2021. URL https://cran.r-project.org/doc/manuals/r-release/R-exts.html.\n\n\nStackOverflow. How to find the operating system details using JavaScript.,URL https://stackoverflow.com/questions/9514179/how-to-find-the-operating-system-details-using-javascript.\n\n\nJ. W. Tukey. Exploratory data analysis. Addison-Wesley Publishing Company, 1977.\n\n\nS. Urbanek. rJava: Low-level R to Java interface. 2021. URL https://CRAN.R-project.org/package=rJava. R package version 1.0-6.\n\n\nK. Ushey, J. Allaire and Y. Tang. Reticulate: Interface to Python. 2022. URL https://CRAN.R-project.org/package=reticulate. R package version 1.23.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-112/",
    "title": "Rejoinder: Software Engineering and R Programming",
    "description": "It is a pleasure to take part in such fruitful discussion about the relationship between Software Engineering and R programming, and what could be gain by allowing each to look more closely at the other. Several discussants make valuable arguments that ought to be further discussed.",
    "author": [
      {
        "name": "Melina Vidoni",
        "url": {}
      }
    ],
    "date": "2021-12-14",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:36+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-096/",
    "title": "msae: An R Package of Multivariate Fay-Herriot Models for Small Area Estimation",
    "description": "The paper introduces an R Package of multivariate Fay-Herriot models for small area estimation named msae. This package implements four types of Fay-Herriot models, including univariate Fay-Herriot model (model 0), multivariate Fay-Herriot model (model 1), autoregressive multivariate Fay-Herriot model (model 2), and heteroskedastic autoregressive multivariate Fay-Herriot model (model 3). It also contains some datasets generated based on multivariate Fay-Herriot models. We describe and implement functions through various practical examples. Multivariate Fay-Herriot models produce a more efficient parameter estimation than direct estimation and univariate model.",
    "author": [
      {
        "name": "Novia Permatasari",
        "url": {}
      },
      {
        "name": "Azka Ubaidillah",
        "url": {}
      }
    ],
    "date": "2021-11-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsae, rsae, nlme, hbsae, JoSAE, BayesSAE, mme, saery, msae\nCRAN Task Views implied by cited packages\nOfficialStatistics, Bayesian, ChemPhys, Econometrics, Environmetrics, Finance, Psychometrics, SocialSciences, Spatial, SpatioTemporal\n\n\n",
    "preview": "articles/RJ-2021-096/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 1153,
    "preview_height": 379
  },
  {
    "path": "articles/RJ-2021-048/",
    "title": "A Unifying Framework for Parallel and Distributed Processing in R using Futures",
    "description": "A *future* is a programming construct designed for concurrent and asynchronous evaluation of code, making it particularly useful for parallel processing. The future package implements the *Future API* for programming with futures in R. This minimal API provides sufficient constructs for implementing parallel versions of well-established, high-level map-reduce APIs. The future ecosystem supports exception handling, output and condition relaying, parallel random number generation, and automatic identification of globals lowering the threshold to parallelize code. The *Future API* bridges parallel frontends with parallel backends, following the philosophy that end-users are the ones who choose the parallel backend while the developer focuses on what to parallelize. A variety of backends exist, and third-party contributions meeting the specifications, which ensure that the same code works on all backends, are automatically supported. The future framework solves several problems not addressed by other parallel frameworks in R.",
    "author": [
      {
        "name": "Henrik Bengtsson",
        "url": {}
      }
    ],
    "date": "2021-11-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-048.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-087/",
    "title": "NGSSEML: Non-Gaussian State Space with Exact Marginal Likelihood",
    "description": "The number of packages/software for Gaussian State Space models has increased over recent decades. However, there are very few codes available for non-Gaussian State Space (NGSS) models due to analytical intractability that prevents exact calculations. One of the few tractable exceptions is the family of NGSS with exact marginal likelihood, named NGSSEML. In this work, we present the wide range of data formats and distributions handled by NGSSEML and a package in the R language to perform classical and Bayesian inference for them. Special functions for filtering, forecasting, and smoothing procedures and the exact calculation of the marginal likelihood function are provided. The methods implemented in the package are illustrated for count and volatility time series and some reliability/survival models, showing that the codes are easy to handle. Therefore, the NGSSEML family emerges as a simple and interesting option/alternative for modeling non-Gaussian time-varying structures commonly encountered in time series and reliability/survival studies. Keywords: Bayesian, classical inference, reliability, smoothing, time series, software R",
    "author": [
      {
        "name": "Thiago R. Santos",
        "url": {}
      },
      {
        "name": "Glaura C. Franco",
        "url": {}
      },
      {
        "name": "Dani Gamerman",
        "url": {}
      }
    ],
    "date": "2021-10-25",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nStructTS, dlm, dlmodeler, SSsimple, MARSS, sspir, pomp, KFAS, bssm, dynamichazard, NGSSEML, coda\nCRAN Task Views implied by cited packages\nTimeSeries, Bayesian, DifferentialEquations, Finance, GraphicalModels\n\n\n",
    "preview": "articles/RJ-2021-087/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 498,
    "preview_height": 497
  },
  {
    "path": "articles/RJ-2021-088/",
    "title": "PAsso: an R Package for Assessing Partial Association between Ordinal Variables",
    "description": "Partial association, the dependency between variables after adjusting for a set of covariates, is an important statistical notion for scientific research. However, if the variables of interest are ordered categorical data, the development of statistical methods and software for assessing their partial association is limited. Following the framework established by Liu et al. (2021), we develop an R package PAsso for assessing Partial Associations between ordinal variables. The package provides various functions that allow users to perform a wide spectrum of assessments, including quantification, visualization, and hypothesis testing. In this paper, we discuss the implementation of PAsso in detail and demonstrate its utility through an analysis of the 2016 American National Election Study.",
    "author": [
      {
        "name": "Shaobo Li",
        "url": {}
      },
      {
        "name": "Xiaorui Zhu",
        "url": {}
      },
      {
        "name": "Yuejie Chen",
        "url": {}
      },
      {
        "name": "Dungang Liu",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nPAsso, sure, MASS, stats, pcaPP, copBasic, rms, ordinal, VGAM, GGally, ggplot2, plotly\nCRAN Task Views implied by cited packages\nEconometrics, Psychometrics, Distributions, Multivariate, SocialSciences, Environmetrics, Robust, Survival, TeachingStatistics, ChemPhys, ExtremeValue, NumericalMathematics, Phylogenetics, ReproducibleResearch, WebTechnologies\n\n\n",
    "preview": "articles/RJ-2021-088/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 1193,
    "preview_height": 576
  },
  {
    "path": "articles/RJ-2021-089/",
    "title": "Robust and Efficient Optimization Using a Marquardt-Levenberg Algorithm with R Package marqLevAlg",
    "description": "Implementations in R of classical general-purpose algorithms for local optimization generally have two major limitations which cause difficulties in applications to complex problems: too loose convergence criteria and too long calculation time. By relying on a Marquardt-Levenberg algorithm (MLA), a Newton-like method particularly robust for solving local optimization problems, we provide with marqLevAlg package an efficient and general-purpose local optimizer which (i) prevents con vergence to saddle points by using a stringent convergence criterion based on the relative distance to minimum/maximum in addition to the stability of the parameters and of the objective function; and (ii) reduces the computation time in complex settings by allowing parallel calculations at each iteration. We demonstrate through a variety of cases from the literature that our implementation reli ably and consistently reaches the optimum (even when other optimizers fail) and also largely reduces computational time in complex settings through the example of maximum likelihood estimation of different sophisticated statistical models.",
    "author": [
      {
        "name": "Viviane Philipps",
        "url": {}
      },
      {
        "name": "Boris P. Hejblum",
        "url": {}
      },
      {
        "name": "Mélanie Prague",
        "url": {}
      },
      {
        "name": "Daniel Commenges",
        "url": {}
      },
      {
        "name": "Cécile Proust-Lima",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nbase, optimx, minpack.lm, nlmrt, marqLevAlg, doParallel, foreach, JM, lcmm, optimParallel, optim, roptim, DEoptim, GA, rgenoud, hydroPSO\nCRAN Task Views implied by cited packages\nOptimization, HighPerformanceComputing, ChemPhys, Cluster, Hydrology, MachineLearning, Survival\n\n\n",
    "preview": "articles/RJ-2021-089/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 772,
    "preview_height": 366
  },
  {
    "path": "articles/RJ-2021-090/",
    "title": "An R package for Non-Normal Multivariate Distributions: Simulation and Probability Calculations from Multivariate Lomax (Pareto Type II) and Other Related Distributions",
    "description": "Convenient and easy-to-use programs are readily available in R to simulate data from and probability calculations for several common multivariate distributions such as normal and t. However, functions for doing so from other less common multivariate distributions, especially those which are asymmetric, are not as readily available, either in R or otherwise. We introduce the R package NonNorMvtDist to generate random numbers from multivariate Lomax distribution, which constitutes a very flexible family of skewed multivariate distributions. Further, by applying certain useful properties of multivariate Lomax distribution, multivariate cases of generalized Lomax, Mardia’s Pareto of Type I, Logistic, Burr, Cook-Johnson’s uniform, F, and inverted beta can be also considered, and random numbers from these distributions can be generated. Methods for the probability and the equicoordinate quantile calculations for all these distributions are then provided. This work substantially enriches the existing R toolbox for nonnormal or nonsymmetric multivariate probability distributions.",
    "author": [
      {
        "name": "Zhixin Lun",
        "url": {}
      },
      {
        "name": "Ravindra Khattree",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nNonNorMvtDist, VGAM, stats, cubature, plot3D, ggplot2\nCRAN Task Views implied by cited packages\nDistributions, Econometrics, Environmetrics, ExtremeValue, Multivariate, NumericalMathematics, Phylogenetics, Psychometrics, SocialSciences, Survival, TeachingStatistics\n\n\n",
    "preview": "articles/RJ-2021-090/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 460
  },
  {
    "path": "articles/RJ-2021-091/",
    "title": "cat.dt: An R package for fast construction of accurate Computerized Adaptive Tests using Decision Trees",
    "description": "This article introduces the cat.dt package for the creation of Computerized Adaptive Tests (CATs). Unlike existing packages, the cat.dt package represents the CAT in a Decision Tree (DT) structure. This allows building the test before its administration, ensuring that the creation time of the test is independent of the number of participants. Moreover, to accelerate the construction of the tree, the package controls its growth by joining nodes with similar estimations or distributions of the ability level and uses techniques such as message passing and pre-calculations. The constructed tree, as well as the estimation procedure, can be visualized using the graphical tools included in the package. An experiment designed to evaluate its performance shows that the cat.dt package drastically reduces computational time in the creation of CATs without compromising accuracy.",
    "author": [
      {
        "name": "Javier Rodríguez-Cuadrado",
        "url": {}
      },
      {
        "name": "Juan C. Laria",
        "url": {}
      },
      {
        "name": "David Delgado-Gómez",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncatR, mirtCAT, catIrt, Matrix, Rglpk, ggplot2\nCRAN Task Views implied by cited packages\nPsychometrics, Econometrics, Multivariate, NumericalMathematics, Optimization, Phylogenetics, TeachingStatistics\n\n\n",
    "preview": "articles/RJ-2021-091/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 442,
    "preview_height": 272
  },
  {
    "path": "articles/RJ-2021-092/",
    "title": "SIQR: An R Package for Single-index Quantile Regression",
    "description": "We develop an R package SIQR that implements the single-index quantile regression (SIQR) models via an efficient iterative local linear approach in Wu et al. (2010). Single-index quantile regression models are important tools in semiparametric regression to provide a comprehensive view of the conditional distributions of a response variable. It is especially useful when the data is heterogeneous or heavy-tailed. The package provides functions that allow users to fit SIQR models, predict, provide standard errors of the single-index coefficients via bootstrap, and visualize the estimated univariate function. We apply the R package SIQR to a well-known Boston Housing data.",
    "author": [
      {
        "name": "Tianhai Zu",
        "url": {}
      },
      {
        "name": "Yan Yu",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nquantreg, KernSmooth\nCRAN Task Views implied by cited packages\nEconometrics, Environmetrics, Multivariate, Optimization, ReproducibleResearch, Robust, SocialSciences, Survival\n\n\n",
    "preview": "articles/RJ-2021-092/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 468,
    "preview_height": 324
  },
  {
    "path": "articles/RJ-2021-093/",
    "title": "mgee2: An R package for marginal analysis of longitudinal ordinal data with misclassified responses and covariates",
    "description": "Marginal methods have been widely used for analyzing longitudinal ordinal data due to their simplicity in model assumptions, robustness in inference results, and easiness in the implementation. However, they are often inapplicable in the presence of measurement errors in the variables. Under the setup of longitudinal studies with ordinal responses and covariates subject to misclassification, Chen et al. (2014) developed marginal methods for misclassification adjustments using the second-order estimating equations and proposed a two-stage estimation approach when the validation subsample is available. Parameter estimation is conducted through the Newton-Raphson algorithm, and the asymptotic distribution of the estimators is established. While the methods of Chen et al. (2014) can successfully correct the misclassification effects, its implementation is not accessible to general users due to the lack of a software package. In this paper, we develop an R package, mgee2, to implement the marginal methods proposed by Chen et al. (2014). To evaluate the performance and illustrate the features of the package, we conduct numerical studies.",
    "author": [
      {
        "name": "Yuliang Xu",
        "url": {}
      },
      {
        "name": "Shuo Shuo Liu",
        "url": {}
      },
      {
        "name": "Grace Y. Yi",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmgee2, SAMBA, misclassGLM, augSIMEX, kml, kml3d, gee, wgeesel, swgee, MASS, Matrix, ggplot2, mgee2k, mgee2v, ordGEE2\nCRAN Task Views implied by cited packages\nCluster, Distributions, Econometrics, Environmetrics, MissingData, MixedModels, NumericalMathematics, Phylogenetics, Psychometrics, Robust, Spatial, TeachingStatistics\n\n\n",
    "preview": "articles/RJ-2021-093/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 9600,
    "preview_height": 6000
  },
  {
    "path": "articles/RJ-2021-094/",
    "title": "PASSED: Calculate Power and Sample Size for Two Sample Tests",
    "description": "Power and sample size estimation are critical aspects of study design to demonstrate minimized risk for subjects and justify the allocation of time, money, and other resources. Researchers often work with response variables that take the form of various distributions. Here, we present an R package, PASSED, that allows flexibility with seven common distributions and multiple options to accommodate sample size or power analysis. The relevant statistical theory, calculations, and examples for each distribution using PASSED are discussed in this paper.",
    "author": [
      {
        "name": "Jinpu Li",
        "url": {}
      },
      {
        "name": "Ryan P. Knigge",
        "url": {}
      },
      {
        "name": "Kaiyi Chen",
        "url": {}
      },
      {
        "name": "Emily V. Leary",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nPASSED, samplesize, TrialSize, simglm, stats, pwr, MESS, pwr2ppl, WebPower, MKmisc\nCRAN Task Views implied by cited packages\nClinicalTrials\n\n\n",
    "preview": "articles/RJ-2021-094/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 762,
    "preview_height": 394
  },
  {
    "path": "articles/RJ-2021-095/",
    "title": "openSkies - Integration of Aviation Data into the R Ecosystem",
    "description": "Aviation data has become increasingly more accessible to the public thanks to the adoption of technologies such as Automatic Dependent Surveillance-Broadcast (ADS-B) and Mode S, which provide aircraft information over publicly accessible radio channels. Furthermore, the OpenSky Network provides multiple public resources to access such air traffic data from a large network of ADS-B receivers. Here, we present openSkies, the first R package for processing public air traffic data. The package provides an interface to the OpenSky Network resources, standardized data structures to represent the different entities involved in air traffic data, and functionalities to analyze and visualize such data. Furthermore, the portability of the implemented data structures makes openSkies easily reusable by other packages, therefore laying the foundation of aviation data engineering in R.",
    "author": [
      {
        "name": "Rafael Ayala",
        "url": {}
      },
      {
        "name": "Daniel Ayala",
        "url": {}
      },
      {
        "name": "Lara Sellés Vidal",
        "url": {}
      },
      {
        "name": "David Ruiz",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nopenSkies, ggmap\nCRAN Task Views implied by cited packages\nSpatial, WebTechnologies\n\n\n",
    "preview": "articles/RJ-2021-095/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 477,
    "preview_height": 229
  },
  {
    "path": "articles/RJ-2021-079/",
    "title": "lg: An R package for Local Gaussian Approximations",
    "description": "The package lg for the R programming language provides implementations of recent methodological advances on applications of the local Gaussian correlation. This includes the estimation of the local Gaussian correlation itself, multivariate density estimation, conditional density estimation, various tests for independence and conditional independence, as well as a graphical module for creating dependence maps. This paper describes the lg package, its principles, and its practical use.",
    "author": [
      {
        "name": "Håkon Otneim",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nlg, localgauss, magrittr, dplyr, ggplot2, mvtnorm\nCRAN Task Views implied by cited packages\nDatabases, Distributions, Finance, ModelDeployment, Multivariate, Phylogenetics, TeachingStatistics, WebTechnologies\n\n\n",
    "preview": "articles/RJ-2021-079/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 360,
    "preview_height": 288
  },
  {
    "path": "articles/RJ-2021-080/",
    "title": "Multiple Imputation and Synthetic Data Generation with NPBayesImputeCat",
    "description": "In many contexts, missing data and disclosure control are ubiquitous and challenging issues. In particular, at statistical agencies, the respondent-level data they collect from surveys and censuses can suffer from high rates of missingness. Furthermore, agencies are obliged to protect respondents’ privacy when publishing the collected data for public use. The NPBayesImputeCat R package, introduced in this paper, provides routines to i) create multiple imputations for missing data and ii) create synthetic data for statistical disclosure control, for multivariate categorical data, with or without structural zeros. We describe the Dirichlet process mixture of products of the multinomial distributions model used in the package and illustrate various uses of the package using data samples from the American Community Survey (ACS). We also compare results of the missing data imputation to the mice R package and those of the synthetic data generation to the synthpop R package.",
    "author": [
      {
        "name": "Jingchen Hu",
        "url": {}
      },
      {
        "name": "Olanrewaju Akande",
        "url": {}
      },
      {
        "name": "Quanli Wang",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nNPBayesImputeCat, mice, synthpop, bayesplot, tidyverse\nCRAN Task Views implied by cited packages\nMissingData, OfficialStatistics, Multivariate, SocialSciences\n\n\n",
    "preview": "articles/RJ-2021-080/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 432,
    "preview_height": 288
  },
  {
    "path": "articles/RJ-2021-081/",
    "title": "A GUIded tour of Bayesian regression",
    "description": "This paper presents a Graphical User Interface (GUI) to carry out a Bayesian regression analysis in a very friendly environment without any programming skills (drag and drop). This paper is designed for teaching and applied purposes at an introductory level. Our GUI is based on an interactive web application using shiny and libraries from R. We carry out some applications to highlight the potential of our GUI for applied researchers and practitioners. In addition, the Help option in the main tap panel has an extended version of this paper, where we present the basic theory underlying all regression models that we developed in our GUI and more applications associated with each model.",
    "author": [
      {
        "name": "Andrés Ramírez–Hassan",
        "url": {}
      },
      {
        "name": "Mateo Graciano-Londoño",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nshinystan\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": "articles/RJ-2021-081/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 2351,
    "preview_height": 1017
  },
  {
    "path": "articles/RJ-2021-082/",
    "title": "miRecSurv Package: Prentice-Williams-Peterson Models with Multiple Imputation of Unknown Number of Previous Episodes",
    "description": "Left censoring can occur with relative frequency when analyzing recurrent events in epi demiological studies, especially observational ones. Concretely, the inclusion of individuals that were already at risk before the effective initiation in a cohort study may cause the unawareness of prior episodes that have already been experienced, and this will easily lead to biased and inefficient estimates. The miRecSurv package is based on the use of models with specific baseline hazard, with multiple imputation of the number of prior episodes when unknown by means of the COMPoisson distribution, a very flexible count distribution that can handle over, sub, and equidispersion, with a stratified model depending on whether the individual had or had not previously been at risk, and the use of a frailty term. The usage of the package is illustrated by means of a real data example based on an occupational cohort study and a simulation study.",
    "author": [
      {
        "name": "David Moriña",
        "url": {}
      },
      {
        "name": "Gilma Hernández-Herrera",
        "url": {}
      },
      {
        "name": "Albert Navarro",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmiRecSurv, compoisson, survsim\nCRAN Task Views implied by cited packages\nSurvival\n\n\n",
    "preview": "articles/RJ-2021-082/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 1073,
    "preview_height": 589
  },
  {
    "path": "articles/RJ-2021-083/",
    "title": "bcmixed: A Package for Median Inference on Longitudinal Data with the Box–Cox Transformation",
    "description": "This article illustrates the use of the bcmixed package and focuses on the two main functions: bcmarg and bcmmrm. The bcmarg function provides inference results for a marginal model of a mixed ef fect model using the Box–Cox transformation. The bcmmrm function provides model median inferences based on the mixed effect models for repeated measures analysis using the Box–Cox transformation for longitudinal randomized clinical trials. Using the bcmmrm function, analysis results with high power and high interpretability for treatment effects can be obtained for longitudinal randomized clinical trials with skewed outcomes. Further, the bcmixed package provides summarizing and visualization tools, which would be helpful to write clinical trial reports.",
    "author": [
      {
        "name": "Kazushi Maruo",
        "url": {}
      },
      {
        "name": "Ryota Ishii",
        "url": {}
      },
      {
        "name": "Yusuke Yamaguchi",
        "url": {}
      },
      {
        "name": "Masahiko Gosho",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nbcmixed, nlme, glme, lme4, CLME, PLmixed, MCMCglmm, ggplot2, MissMech\nCRAN Task Views implied by cited packages\nPsychometrics, SocialSciences, Econometrics, Environmetrics, OfficialStatistics, Phylogenetics, SpatioTemporal, Bayesian, ChemPhys, Finance, Spatial, Survival, TeachingStatistics\n\n\n",
    "preview": "articles/RJ-2021-083/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 576,
    "preview_height": 360
  },
  {
    "path": "articles/RJ-2021-085/",
    "title": "spfilteR: An R package for Semiparametric Spatial Filtering with Eigenvectors in (Generalized) Linear Models",
    "description": "Eigenvector-based Spatial filtering constitutes a highly flexible semiparametric approach to account for spatial autocorrelation in a regression framework. It combines judiciously selected eigenvectors from a transformed connectivity matrix to construct a synthetic spatial filter and remove spatial patterns from model residuals. This article introduces the spfilteR package that provides several useful and flexible tools to estimate spatially filtered linear and generalized linear models in R. While the package features functions to identify relevant eigenvectors based on different selection criteria in an unsupervised fashion, it also helps users to perform supervised spatial filtering and to select eigenvectors based on alternative user-defined criteria. Besides a brief discussion of the eigenvector-based spatial filtering approach, this article presents the main functions of the package and illustrates their usage. Comparison to alternative implementations in other R packages highlights the added value of the spfilteR package.",
    "author": [
      {
        "name": "Sebastian Juhl",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspfilteR, spatialreg, spmoran, adespatial, vegan\nCRAN Task Views implied by cited packages\nSpatial, Econometrics, Environmetrics, Multivariate, Phylogenetics, Psychometrics\n\n\n",
    "preview": "articles/RJ-2021-085/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 300,
    "preview_height": 300
  },
  {
    "path": "articles/RJ-2021-086/",
    "title": "The vote Package: Single Transferable Vote and Other Electoral Systems in R",
    "description": "We describe the vote package in R, which implements the plurality (or first-past-the-post), two-round runoff, score, approval, and Single Transferable Vote (STV) electoral systems, as well as methods for selecting the Condorcet winner and loser. We emphasize the STV system, which we have found to work well in practice for multi-winner elections with small electorates, such as committee and council elections, and the selection of multiple job candidates. For single-winner elections, STV is also called Instant Runoff Voting (IRV), Ranked Choice Voting (RCV), or the alternative vote (AV) system. The package also implements the STV system with equal preferences, for the first time in a software package, to our knowledge. It also implements a new variant of STV, in which a minimum number of candidates from a specified group are required to be elected. We illustrate the package with several real examples.",
    "author": [
      {
        "name": "Adrian E. Raftery",
        "url": {}
      },
      {
        "name": "Hana Ševčíková",
        "url": {}
      },
      {
        "name": "Bernard W. Silverman",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nvote, votesys, rcv, STV, HighestMedianRules, electoral, esaps\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": "articles/RJ-2021-086/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 504,
    "preview_height": 504
  },
  {
    "path": "articles/RJ-2021-069/",
    "title": "Estimating Social Influence Effects in Networks Using A Latent Space Adjusted Approach in R ",
    "description": "Social influence effects have been extensively studied in various empirical network research. However, many challenges remain in estimating social influence effects in networks, as influence effects are often entangled with other factors, such as homophily in the selection process and the common social-environmental factors that individuals are embedded in. Methods currently available either do not solve these problems or require stringent assumptions. Recent works by Xu (2018) and others have shown that a latent space adjusted approach based on the latent space model has the potential to disentangle the influence effects from other processes, and the simulation evidence has shown that this approach outperforms other state-of-the-art approaches in terms of recovering the true social influence effect when there is an unobserved trait co-determining influence and selection. In this paper, I will further illustrate how the latent space adjusted approach can account for bias in the estimation of social influence effects and how this approach can be easily implemented in R.",
    "author": [
      {
        "name": "Ran Xu",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nlavaan, plm, latentnet, statnet, RSiena\nCRAN Task Views implied by cited packages\nSocialSciences, Econometrics, Cluster, HighPerformanceComputing, MissingData, OfficialStatistics, Psychometrics, SpatioTemporal\n\n\n",
    "preview": "articles/RJ-2021-069/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 1113,
    "preview_height": 522
  },
  {
    "path": "articles/RJ-2021-070/",
    "title": "survidm: An R package for Inference and Prediction in an Illness-Death Model",
    "description": "Multi-state models are a useful way of describing a process in which an individual moves through a number of finite states in continuous time. The illness-death model plays a central role in the theory and practice of these models, describing the dynamics of healthy subjects who may move to an intermediate \"diseased\" state before entering into a terminal absorbing state. In these models, one important goal is the modeling of transition rates which is usually done by studying the relationship between covariates and disease evolution. However, biomedical researchers are also interested in reporting other interpretable results in a simple and summarized manner. These include estimates of predictive probabilities, such as the transition probabilities, occupation probabilities, cumulative incidence functions, and the sojourn time distributions. The development of survidm package has been motivated by recent contribution that provides answers to all these topics. An illustration of the software usage is included using real data.",
    "author": [
      {
        "name": "Gustavo Soutinho",
        "url": {}
      },
      {
        "name": "Marta Sestelo",
        "url": {}
      },
      {
        "name": "Luís Meira-Machado",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsurvidm, p3state.msm, TPmsm, etm, mstate, TP.idm, cmprsk, timereg, msSurv, msm, ggplot2, plotly, survival, KernSmooth\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Distributions, Econometrics, Multivariate, Phylogenetics, SocialSciences, TeachingStatistics, WebTechnologies\n\n\n",
    "preview": "articles/RJ-2021-070/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 456,
    "preview_height": 317
  },
  {
    "path": "articles/RJ-2021-071/",
    "title": "dad: an R Package for Visualisation, Classification and Discrimination of Multivariate Groups Modelled by their Densities",
    "description": "Multidimensional scaling (MDS), hierarchical cluster analysis (HCA), and discriminant analysis (DA) are classical techniques which deal with data made of n individuals and p variables. When the individuals are divided into T groups, the R package dad associates with each group a multivariate probability density function and then carries out these techniques on the densities, which are estimated by the data under consideration. These techniques are based on distance measures between densities: chi-square, Hellinger, Jeffreys, Jensen-Shannon, and L p for discrete densities, Hellinger , Jeffreys, L2 , and 2-Wasserstein for Gaussian densities, and L2 for numeric non-Gaussian densities estimated by the Gaussian kernel method. Practical methods help the user to give meaning to the outputs in the context of MDS and HCA and to look for an optimal prediction in the context of DA based on the one-leave-out misclassification ratio. Some functions for data management or basic statistics calculations on groups are annexed.",
    "author": [
      {
        "name": "Rachid Boumaza",
        "url": {}
      },
      {
        "name": "Pierre Santagostini",
        "url": {}
      },
      {
        "name": "Smail Yousfi",
        "url": {}
      },
      {
        "name": "Sabine Demotes-Mainard",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nstats, MASS, ade4, FactoMineR, cluster, dad, fda, fda.usc, fdadensity, compositions, Compositional, robCompositions\nCRAN Task Views implied by cited packages\nMultivariate, Distributions, Environmetrics, FunctionalData, Psychometrics, Robust, ChemPhys, Cluster, Econometrics, MissingData, NumericalMathematics, SocialSciences, Spatial, TeachingStatistics\n\n\n",
    "preview": "articles/RJ-2021-071/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 504,
    "preview_height": 504
  },
  {
    "path": "articles/RJ-2021-072/",
    "title": "diproperm: An R Package for the DiProPerm Test",
    "description": "High-dimensional low sample size (HDLSS) data sets frequently emerge in many biomedical applications. The direction-projection-permutation (DiProPerm) test is a two-sample hypothesis test for comparing two high-dimensional distributions. The DiProPerm test is exact, i.e., the type I error is guaranteed to be controlled at the nominal level for any sample size, and thus is applicable in the HDLSS setting. This paper discusses the key components of the DiProPerm test, introduces the diproperm R package, and demonstrates the package on a real-world data set.",
    "author": [
      {
        "name": "Andrew G. Allmon",
        "url": {}
      },
      {
        "name": "J.S. Marron",
        "url": {}
      },
      {
        "name": "Michael G. Hudgens",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndiproperm, DWDLargeR, Matrix\nCRAN Task Views implied by cited packages\nEconometrics, Multivariate, NumericalMathematics\n\n\n",
    "preview": "articles/RJ-2021-072/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 1956,
    "preview_height": 1437
  },
  {
    "path": "articles/RJ-2021-073/",
    "title": "MatchThem:: Matching and Weighting after Multiple Imputation",
    "description": "Balancing the distributions of the confounders across the exposure levels in an observational study through matching or weighting is an accepted method to control for confounding due to these variables when estimating the association between an exposure and outcome and reducing the degree of dependence on certain modeling assumptions. Despite the increasing popularity in practice, these procedures cannot be immediately applied to datasets with missing values. Multiple imputation of the missing data is a popular approach to account for missing values while preserving the number of units in the dataset and accounting for the uncertainty in the missing values. However, to the best of our knowledge, there is no comprehensive matching and weighting software that can be easily implemented with multiply imputed datasets. In this paper, we review this problem and suggest a framework to map out the matching and weighting of multiply imputed datasets to 5 actions as well as the best practices to assess balance in these datasets after matching and weighting. We also illustrate these approaches using a companion package for R, MatchThem.",
    "author": [
      {
        "name": "Farhad Pishgar",
        "url": {}
      },
      {
        "name": "Noah Greifer",
        "url": {}
      },
      {
        "name": "Clémence Leyrat",
        "url": {}
      },
      {
        "name": "Elizabeth Stuart",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMatchThem, MatchIt, WeightIt, cobalt, mice, Amelia, survey\nCRAN Task Views implied by cited packages\nOfficialStatistics, SocialSciences, MissingData, Multivariate, Survival\n\n\n",
    "preview": "articles/RJ-2021-073/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 2400,
    "preview_height": 540
  },
  {
    "path": "articles/RJ-2021-074/",
    "title": "MAINT.Data: Modelling and Analysing Interval Data in R",
    "description": "We present the CRAN R package MAINT.Data for the modelling and analysis of multivariate interval data, i.e., where units are described by variables whose values are intervals of IR, representing intrinsic variability. Parametric inference methodologies based on probabilistic models for interval variables have been developed, where each interval is represented by its midpoint and log-range, for",
    "author": [
      {
        "name": "A. Pedro Duarte Silva",
        "url": {}
      },
      {
        "name": "Paula Brito",
        "url": {}
      },
      {
        "name": "Peter Filzmoser",
        "url": {}
      },
      {
        "name": "José G. Dias",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsurvreg, crch, MAINT.Data, symbolicDA, RSDA, iRegression, GPCSIV, sn, Rcpp, RcppArmadillo, rrcov, mclust, nycflights13, tidyverse\nCRAN Task Views implied by cited packages\nCluster, Distributions, Econometrics, Environmetrics, HighPerformanceComputing, NumericalMathematics, Robust\n\n\n",
    "preview": "articles/RJ-2021-074/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 720,
    "preview_height": 540
  },
  {
    "path": "articles/RJ-2021-075/",
    "title": "tramME: Mixed-Effects Transformation Models Using Template Model Builder",
    "description": "Linear transformation models constitute a general family of parametric regression models for discrete and continuous responses. To accommodate correlated responses, the model is extended by incorporating mixed effects. This article presents the R package tramME, which builds on existing implementations of transformation models (mlt and tram packages) as well as Laplace approximation and automatic differentiation (using the TMB package), to calculate estimates and perform likelihood inference in mixed-effects transformation models. The resulting framework can be readily applied to a wide range of regression problems with grouped data structures.",
    "author": [
      {
        "name": "Bálint Tamási",
        "url": {}
      },
      {
        "name": "Torsten Hothorn",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nnlme, lme4, tramME, mlt, tram, TMB, glmmTMB, survival, boxcoxmix, ordinalCont, coxme, parfm, frailtypack, ordinal\nCRAN Task Views implied by cited packages\nSurvival, Econometrics, Psychometrics, SocialSciences, Environmetrics, OfficialStatistics, SpatioTemporal, ChemPhys, ClinicalTrials, Finance, Spatial\n\n\n",
    "preview": "articles/RJ-2021-075/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 504,
    "preview_height": 360
  },
  {
    "path": "articles/RJ-2021-076/",
    "title": "CompModels: A Suite of Computer Model Test Functions for Bayesian Optimization",
    "description": "The CompModels package for R provides a suite of computer model test functions that can be used for computer model prediction/emulation, uncertainty quantification, and calibration. Moreover, the CompModels package is especially well suited for the sequential optimization of computer models. The package is a mix of real-world physics problems, known mathematical functions, and black-box functions that have been converted into computer models with the goal of Bayesian (i.e., sequential) optimization in mind. Likewise, the package contains computer models that represent either the constrained or unconstrained optimization case, each with varying levels of difficulty. In this paper, we illustrate the use of the package with both real-world examples and black-box functions by solving constrained optimization problems via Bayesian optimization. Ultimately, the package is shown to provide users with a source of computer model test functions that are reproducible, shareable, and that can be used for benchmarking of novel optimization methods.",
    "author": [
      {
        "name": "Tony Pourmohamad",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nCompModels, laGP\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": "articles/RJ-2021-076/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 504,
    "preview_height": 504
  },
  {
    "path": "articles/RJ-2021-077/",
    "title": "volesti: Volume Approximation and Sampling for Convex Polytopes in R",
    "description": "Sampling from high-dimensional distributions and volume approximation of convex bodies are fundamental operations that appear in optimization, finance, engineering, artificial intelligence, and machine learning. In this paper, we present volesti, an R package that provides efficient, scalable algorithms for volume estimation, uniform, and Gaussian sampling from convex polytopes. volesti scales to hundreds of dimensions, handles efficiently three different types of polyhedra and pro vides non existing sampling routines to R. We demonstrate the power of volesti by solving several challenging problems using the R language.",
    "author": [
      {
        "name": "Apostolos Chalkis",
        "url": {}
      },
      {
        "name": "Vissarion Fisikopoulos",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nvolesti, tmg, multinomineq, lineqGPR, restrictedMVN, tmvmixnorm, hitandrun, limSolve, HybridMC, rhmc, mcmc, MHadaptive, geometry, Rcpp, Rfast, coda, SimplicialCubature, cubature, stats, methods, BH, RcppEigen, testthat, ggplot2, plotly, rgl\nCRAN Task Views implied by cited packages\nNumericalMathematics, Bayesian, Multivariate, Distributions, GraphicalModels, HighPerformanceComputing, Optimization, Phylogenetics, SpatioTemporal, TeachingStatistics, WebTechnologies\n\n\n",
    "preview": "articles/RJ-2021-077/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 595,
    "preview_height": 842
  },
  {
    "path": "articles/RJ-2021-078/",
    "title": "Elliptical Symmetry Tests in R",
    "description": "The assumption of elliptical symmetry has an important role in many theoretical develop ments and applications. Hence, it is of primary importance to be able to test whether that assumption actually holds true or not. Various tests have been proposed in the literature for this problem. To the best of our knowledge, none of them has been implemented in R. This article describes the R package ellipticalsymmetry which implements several well-known tests for elliptical symmetry together with some recent tests. We demonstrate the testing procedures with a real data example.",
    "author": [
      {
        "name": "Slad̄ana Babić",
        "url": {}
      },
      {
        "name": "Christophe Ley",
        "url": {}
      },
      {
        "name": "Marko Palangetić",
        "url": {}
      }
    ],
    "date": "2021-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nellipticalsymmetry\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-065/",
    "title": "The bdpar Package: Big Data Pipelining Architecture for R",
    "description": "In the last years, big data has become a useful paradigm for taking advantage of multiple sources to find relevant knowledge in real domains (such as the design of personalized marketing campaigns or helping to palliate the effects of several fatal diseases). Big data programming tools and methods have evolved over time from a MapReduce to a pipeline-based archetype. Concretely the use of pipelining schemes has become the most reliable way of processing and analyzing large amounts of data. To this end, this work introduces bdpar, a new highly customizable pipeline-based framework (using the OOP paradigm provided by [R6](https://CRAN.R-project.org/package=R6) package) able to execute multiple preprocessing tasks over heterogeneous data sources. Moreover, to increase the flexibility and performance, bdpar provides helpful features such as (i) the definition of a novel object-based pipe operator ( %\\>\\|%), (ii) the ability to easily design and deploy new (and customized) input data parsers, tasks, and pipelines, (iii) only-once execution which avoids the execution of previously processed information (instances), guaranteeing that only new both input data and pipelines are executed, (iv) the capability to perform serial or parallel operations according to the user needs, (v) the inclusion of a debugging mechanism which allows users to check the status of each instance (and find possible errors) throughout the process.",
    "author": [
      {
        "name": "Miguel Ferreiro-Díaz",
        "url": {}
      },
      {
        "name": "Tomás R. Cotos-Yáñez",
        "url": {}
      },
      {
        "name": "José R. Méndez",
        "url": {}
      },
      {
        "name": "David Ruano-Ordás",
        "url": {}
      }
    ],
    "date": "2021-07-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-065.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-064/",
    "title": "g2f as a Novel Tool to Find and Fill Gaps in Metabolic Networks",
    "description": "During the building of a genome-scale metabolic model, there are several dead-end metabo lites and substrates which cannot be imported, produced, nor used by any reaction incorporated in the network. The presence of these dead-end metabolites can block out the net flux of the objective function when it is evaluated through Flux Balance Analysis (FBA), and when it is not blocked, bias in the biological conclusions increase. In this aspect, the refinement to restore the connectivity of the network can be carried out manually or using computational algorithms. The g2f package was designed as a tool to find the gaps from dead-end metabolites and fill them from the stoichiometric reactions of a reference, filtering candidate reactions using a weighting function. Additionally, this algorithm allows downloading all the sets of gene-associated stoichiometric reactions for a specific organism from the KEGG database. Our package is compatible with both 4.0.0 and 3.6.0 R versions.",
    "author": [
      {
        "name": "Daniel Osorio",
        "url": {}
      },
      {
        "name": "Kelly Botero",
        "url": {}
      },
      {
        "name": "Andrés Pinzón Velasco",
        "url": {}
      },
      {
        "name": "Nicolás Mendoza-Mejía",
        "url": {}
      },
      {
        "name": "Felipe Rojas-            Rodríguez",
        "url": {}
      },
      {
        "name": "George Barreto",
        "url": {}
      },
      {
        "name": "Janneth González",
        "url": {}
      }
    ],
    "date": "2021-07-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ng2f, sybil\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-066/",
    "title": "ROCnReg: An R Package for Receiver Operating Characteristic Curve Inference With and Without Covariates",
    "description": "This paper introduces the package ROCnReg that allows estimating the pooled ROC curve, the covariate-specific ROC curve, and the covariate-adjusted ROC curve by different methods, both from (semi) parametric and nonparametric perspectives and within Bayesian and frequentist paradigms. From the estimated ROC curve (pooled, covariate-specific, or covariate-adjusted), several summary measures of discriminatory accuracy, such as the (partial) area under the ROC curve and the Youden index, can be obtained. The package also provides functions to obtain ROC-based optimal threshold values using several criteria, namely, the Youden index criterion and the criterion that sets a target value for the false positive fraction. For the Bayesian methods, we provide tools for assessing model fit via posterior predictive checks, while the model choice can be carried out via several information criteria. Numerical and graphical outputs are provided for all methods. This is the only package implementing Bayesian procedures for ROC curves.",
    "author": [
      {
        "name": "María Xosé Rodríguez-Álvarez",
        "url": {}
      },
      {
        "name": "Vanda Inácio",
        "url": {}
      }
    ],
    "date": "2021-07-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-066.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-067/",
    "title": "A New Versatile Discrete Distribution",
    "description": "This paper introduces a new flexible distribution for discrete data. Approximate moment estimators of the parameters of the distribution, to be used as starting values for numerical opti mization procedures, are discussed. “Exact” moment estimation, effected via a numerical procedure, and maximum likelihood estimation, are considered. The quality of the results produced by these estimators is assessed via simulation experiments. Several examples are given of fitting instances of the new distribution to real and simulated data. It is noted that the new distribution is a member of the exponential family. Expressions for the gradient and Hessian of the log-likelihood of the new distribution are derived. The former facilitates the numerical maximization of the likelihood with optim(); the latter provides means of calculating or estimating the covariance matrix of of the parame ter estimates. A discrepancy between estimates of the covariance matrix obtained by inverting the Hessian and those obtained by Monte Carlo methods is discussed.",
    "author": [
      {
        "name": "Rolf Turner",
        "url": {}
      }
    ],
    "date": "2021-07-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nhmm.discnp, spcadjust, rmutil\nCRAN Task Views implied by cited packages\nDistributions\n\n\n",
    "preview": "articles/RJ-2021-067/preview.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 504,
    "preview_height": 432
  },
  {
    "path": "articles/RJ-2021-068/",
    "title": "BayesSPsurv: An R Package to Estimate Bayesian (Spatial) Split-Population Survival Models",
    "description": "Survival data often include a fraction of units that are susceptible to an event of interest as well as a fraction of \"immune\" units. In many applications, spatial clustering in unobserved risk factors across nearby units can also affect their survival rates and odds of becoming immune. To address these methodological challenges, this article introduces our [BayesSPsurv](https://CRAN.R-project.org/package=BayesSPsurv) R-package, which fits parametric Bayesian Spatial split-population survival (cure) models that can account for spatial autocorrelation in both subpopulations of the user's time-to-event data. Spatial autocorrelation is modeled with spatially weighted frailties, which are estimated using a conditionally autoregressive prior. The user can also fit parametric cure models with or without nonspatial i.i.d. frailties, and each model can incorporate time-varying covariates. BayesSPsurv also includes various functions to conduct pre-estimation spatial autocorrelation tests, visualize results, and assess model performance, all of which are illustrated using data on post-civil war peace survival.",
    "author": [
      {
        "name": "Brandon Bolte",
        "url": {}
      },
      {
        "name": "Nicolás Schmidt",
        "url": {}
      },
      {
        "name": "Sergio Béjar",
        "url": {}
      },
      {
        "name": "Nguyen Huynh",
        "url": {}
      },
      {
        "name": "Bumba Mukherjee",
        "url": {}
      }
    ],
    "date": "2021-07-15",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-056/",
    "title": "A Method for Deriving Information from Running R Code",
    "description": "It is often useful to tap information from a running R script. Obvious use cases include monitoring the consumption of resources (time, memory) and logging. Perhaps less obvious cases include tracking changes in R objects or collecting the output of unit tests. In this paper, we demonstrate an approach that abstracts the collection and processing of such secondary information from the running R script. Our approach is based on a combination of three elements. The first element is to build a customized way to evaluate code. The second is labeled *local masking* and it involves temporarily masking a user-facing function so an alternative version of it is called. The third element we label *local side effect*. This refers to the fact that the masking function exports information to the secondary information flow without altering a global state. The result is a method for building systems in pure R that lets users create and control secondary flows of information with minimal impact on their workflow and no global side effects.",
    "author": [
      {
        "name": "Mark P.J. van der Loo",
        "url": {}
      }
    ],
    "date": "2021-07-13",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-056.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-053/",
    "title": "Reproducible Summary Tables with the gtsummary Package",
    "description": "The gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R. A critical part of the work of statisticians, data scientists, and analysts is summarizing data sets and regression models in R and publishing or sharing polished summary tables. The gtsummary package was created to streamline these everyday analysis tasks by allowing users to easily create reproducible summaries of data sets, regression models, survey data, and survival data with a simple interface and very little code. The package follows a tidy framework, making it easy to integrate with standard data workflows, and offers many table customization features through function arguments, helper functions, and custom themes.",
    "author": [
      {
        "name": "Daniel D. Sjoberg",
        "url": {}
      },
      {
        "name": "Karissa Whiting",
        "url": {}
      },
      {
        "name": "Michael Curry",
        "url": {}
      },
      {
        "name": "Jessica A. Lavery",
        "url": {}
      },
      {
        "name": "Joseph Larmarange",
        "url": {}
      }
    ],
    "date": "2021-06-22",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-053.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-057/",
    "title": "garchx: Flexible and Robust GARCH-X Modeling",
    "description": "The garchx package provides a user-friendly, fast, flexible, and robust framework for the estimation and inference of GARCH($p,q,r$)-X models, where $p$ is the ARCH order, $q$ is the GARCH order, $r$ is the asymmetry or leverage order, and 'X' indicates that covariates can be included. Quasi Maximum Likelihood (QML) methods ensure estimates are consistent and standard errors valid, even when the standardized innovations are non-normal or dependent, or both. Zero-coefficient restrictions by omission enable parsimonious specifications, and functions to facilitate the non-standard inference associated with zero-restrictions in the null-hypothesis are provided. Finally, in the formal comparisons of precision and speed, the garchx package performs well relative to other prominent GARCH-packages on CRAN.",
    "author": [
      {
        "name": "Genaro Sucarrat",
        "url": {}
      }
    ],
    "date": "2021-06-22",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-057.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-060/",
    "title": "gofCopula: Goodness-of-Fit Tests for Copulae",
    "description": "The last decades show an increased interest in modeling various types of data through copulae. Different copula models have been developed, which lead to the challenge of finding the best fitting model for a particular dataset. From the other side, a strand of literature developed a list of different Goodness-of-Fit (GoF) tests with different powers under different conditions. The usual practice is the selection of the best copula via the $p$-value of the GoF test. Although this method is not purely correct due to the fact that non-rejection does not imply acception, this strategy is favored by practitioners. Unfortunately, different GoF tests often provide contradicting outputs. The proposed R-package brings under one umbrella 13 most used copulae - plus their rotated variants - together with 16 GoF tests and a hybrid one. The package offers flexible margin modeling, automatized parallelization, parameter estimation, as well as a user-friendly interface, and pleasant visualizations of the results. To illustrate the functionality of the package, two exemplary applications are provided.",
    "author": [
      {
        "name": "Ostap Okhrin",
        "url": {}
      },
      {
        "name": "Simon Trimborn",
        "url": {}
      },
      {
        "name": "Martin Waltz",
        "url": {}
      }
    ],
    "date": "2021-06-22",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-059/",
    "title": "The HBV.IANIGLA Hydrological Model",
    "description": "Over the past 40 years, the HBV (Hydrologiska Byråns Vattenbalansavdelning) hydrological model has been one of the most used worldwide due to its robustness, simplicity, and reliable results. Despite these advantages, the available versions impose some limitations for research studies in mountain watersheds dominated by ice-snow melt runoff (i.e., no glacier module, a limited number of elevation bands, among other constraints). Here we present HBV.IANIGLA, a tool for hydroclimatic studies in regions with steep topography and/or cryospheric processes which provides a modular and extended implementation of the HBV model as an R package. To our knowledge, this is the first modular version of the original HBV model. This feature can be very useful for teaching hydrological modeling, as it offers the possibility to build a customized, open-source model that can be adjusted to different requirements of students and users.",
    "author": [
      {
        "name": "Ezequiel Toum",
        "url": {}
      },
      {
        "name": "Mariano H. Masiokas",
        "url": {}
      },
      {
        "name": "Ricardo Villalba",
        "url": {}
      },
      {
        "name": "Pierre Pitte",
        "url": {}
      },
      {
        "name": "Lucas Ruiz",
        "url": {}
      }
    ],
    "date": "2021-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-059.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-061/",
    "title": "penPHcure: Variable Selection in Proportional Hazards Cure Model with Time-Varying Covariates",
    "description": "We describe the [penPHcure](https://CRAN.R-project.org/package=penPHcure) R package, which implements the semiparametric proportional-hazards (PH) cure model of @Sy_Taylor_2000 extended to time-varying covariates and the variable selection technique based on its SCAD-penalized likelihood proposed by @Beretta_Heuchenne_2019. In survival analysis, cure models are a useful tool when a fraction of the population is likely to be immune from the event of interest. They can separate the effects of certain factors on the probability of being susceptible and on the time until the occurrence of the event. Moreover, the penPHcure package allows the user to simulate data from a PH cure model, where the event-times are generated on a continuous scale from a piecewise exponential distribution conditional on time-varying covariates, with a method similar to @Hendry_2014. We present the results of a simulation study to assess the finite sample performance of the methodology and illustrate the functionalities of the penPHcure package using criminal recidivism data.",
    "author": [
      {
        "name": "Alessandro Beretta",
        "url": {}
      },
      {
        "name": "Cédric Heuchenne",
        "url": {}
      }
    ],
    "date": "2021-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-061.zip\n\n\nA. Beretta and C. Heuchenne. Variable selection in proportional hazards cure model with time-varying covariates, application to US bank failures. Journal of Applied Statistics, 46(9): 1529–1549, 2019. URL https://doi.org/10.1080/02664763.2018.1554627.\n\n\nD. J. Hendry. Data generation for the cox proportional hazards model with time-dependent covariates: A method for medical researchers. Statistics in medicine, 33: 436–454, 2014. URL https://doi.org/10.1002/sim.5945.\n\n\nJ. P. Sy and J. M. G. Taylor. Estimation in a cox proportional hazards cure model. Biometrics, 56(1): 227–236, 2000. URL https://doi.org/10.1111/j.0006-341X.2000.00227.x.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-062/",
    "title": "Unidimensional and Multidimensional Methods for Recurrence Quantification Analysis with crqa",
    "description": "Recurrence quantification analysis is a widely used method for characterizing patterns in time series. This article presents a comprehensive survey for conducting a wide range of recurrence-based analyses to quantify the dynamical structure of single and multivariate time series and capture coupling properties underlying leader-follower relationships. The basics of recurrence quantification analysis (RQA) and all its variants are formally introduced step-by-step from the simplest auto-recurrence to the most advanced multivariate case. Importantly, we show how such RQA methods can be deployed under a single computational framework in R using a substantially renewed version of our crqa 2.0 package. This package includes implementations of several recent advances in recurrence-based analysis, among them applications to multivariate data and improved entropy calculations for categorical data. We show concrete applications of our package to example data, together with a detailed description of its functions and some guidelines on their usage.",
    "author": [
      {
        "name": "Moreno I. Coco",
        "url": {}
      },
      {
        "name": "Dan Mønster",
        "url": {}
      },
      {
        "name": "Giuseppe Leonardi",
        "url": {}
      },
      {
        "name": "Rick Dale",
        "url": {}
      },
      {
        "name": "Sebastian Wallot",
        "url": {}
      }
    ],
    "date": "2021-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-062.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-063/",
    "title": "stratamatch: Prognostic Score Stratification Using a Pilot Design",
    "description": "Optimal propensity score matching has emerged as one of the most ubiquitous approaches for causal inference studies on observational data. However, outstanding critiques of the statistical properties of propensity score matching have cast doubt on the statistical efficiency of this technique, and the poor scalability of optimal matching to large data sets makes this approach inconvenient if not infeasible for sample sizes that are increasingly commonplace in modern observational data. The [stratamatch](https://CRAN.R-project.org/package=stratamatch) package provides implementation support and diagnostics for 'stratified matching designs,' an approach that addresses both of these issues with optimal propensity score matching for large-sample observational studies. First, stratifying the data enables more computationally efficient matching of large data sets. Second, stratamatch implements a 'pilot design' approach in order to stratify by a prognostic score, which may increase the precision of the effect estimate and increase power in sensitivity analyses of unmeasured confounding.",
    "author": [
      {
        "name": "Rachael C. Aikens",
        "url": {}
      },
      {
        "name": "Joseph Rigdon",
        "url": {}
      },
      {
        "name": "Justin Lee",
        "url": {}
      },
      {
        "name": "Michael Baiocchi",
        "url": {}
      },
      {
        "name": "Andrew B. Goldstone",
        "url": {}
      },
      {
        "name": "Peter Chiu",
        "url": {}
      },
      {
        "name": "Y. Joseph Woo",
        "url": {}
      },
      {
        "name": "Jonathan H. Chen",
        "url": {}
      }
    ],
    "date": "2021-06-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-063.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-055/",
    "title": "distr6: R6 Object-Oriented Probability Distributions Interface in R",
    "description": "distr6 is an object-oriented (OO) probability distributions interface leveraging the extensibility and scalability of R6 and the speed and efficiency of Rcpp. Over 50 probability distributions are currently implemented in the package with 'core' methods, including density, distribution, and generating functions, and more 'exotic' ones, including hazards and distribution function anti-derivatives. In addition to simple distributions, distr6 supports compositions such as truncation, mixtures, and product distributions. This paper presents the core functionality of the package and demonstrates examples for key use-cases. In addition, this paper provides a critical review of the object-oriented programming paradigms in R and describes some novel implementations for design patterns and core object-oriented features introduced by the package for supporting distr6 components.",
    "author": [
      {
        "name": "Raphael Sonabend",
        "url": {}
      },
      {
        "name": "Franz J. Király",
        "url": {}
      }
    ],
    "date": "2021-06-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-055.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-044/",
    "title": "OneStep : Le Cam's One-step Estimation Procedure",
    "description": "The OneStep package proposes principally an eponymic function that numerically computes Le Cam's one-step estimator, which is asymptotically efficient and can be computed faster than the maximum likelihood estimator for large datasets. Monte Carlo simulations are carried out for several examples (discrete and continuous probability distributions) in order to exhibit the performance of Le Cam's one-step estimation procedure in terms of efficiency and computational cost on observation samples of finite size.",
    "author": [
      {
        "name": "Alexandre Brouste",
        "url": {}
      },
      {
        "name": "Christophe Dutang",
        "url": {}
      },
      {
        "name": "Darel Noutsa Mieniedou",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-044.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-045/",
    "title": "The R Package smicd: Statistical Methods for Interval-Censored Data",
    "description": "The package allows the use of two new statistical methods for the analysis of interval-censored data: 1) direct estimation/prediction of statistical indicators and 2) linear (mixed) regression analysis. Direct estimation of statistical indicators, for instance, poverty and inequality indicators, is facilitated by a non parametric kernel density algorithm. The algorithm is able to account for weights in the estimation of statistical indicators. The standard errors of the statistical indicators are estimated with a non parametric bootstrap. Furthermore, the package offers statistical methods for the estimation of linear and linear mixed regression models with an interval-censored dependent variable, particularly random slope and random intercept models. Parameter estimates are obtained through a stochastic expectation-maximization algorithm. Standard errors are estimated using a non parametric bootstrap in the linear regression model and by a parametric bootstrap in the linear mixed regression model. To handle departures from the model assumptions, fixed (logarithmic) and data-driven (Box-Cox) transformations are incorporated into the algorithm.",
    "author": [
      {
        "name": "Paul Walter",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-046/",
    "title": "krippendorffsalpha: An R Package for Measuring Agreement Using Krippendorff's Alpha Coefficient",
    "description": "R package [krippendorffsalpha](https://CRAN.R-project.org/package=krippendorffsalpha) provides tools for measuring agreement using Krippendorff's $\\alpha$ coefficient, a well-known nonparametric measure of agreement (also called inter-rater reliability and various other names). This article first develops Krippendorff's $\\alpha$ in a natural way and situates $\\alpha$ among statistical procedures. Then, the usage of package [krippendorffsalpha](https://CRAN.R-project.org/package=krippendorffsalpha) is illustrated via analyses of two datasets, the latter of which was collected during an imaging study of hip cartilage. The package permits users to apply the $\\alpha$ methodology using built-in distance functions for the nominal, ordinal, interval, or ratio levels of measurement. User-defined distance functions are also supported. The fitting function can accommodate any number of units, any number of coders, and missingness. Bootstrap inference is supported, and the bootstrap computation can be carried out in parallel.",
    "author": [
      {
        "name": "John Hughes",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-046.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-047/",
    "title": "Working with CRSP/COMPUSTAT in R: Reproducible Empirical Asset Pricing",
    "description": "It is common to come across SAS or Stata manuals while working on academic empirical finance research. Nonetheless, given the popularity of open-source programming languages such as R, there are fewer resources in R covering popular databases such as CRSP and COMPUSTAT. The aim of this article is to bridge the gap and illustrate how to leverage R in working with both datasets. As an application, we illustrate how to form size-value portfolios with respect to [@fama1993common] and study the sensitivity of the results with respect to different inputs. Ultimately, the purpose of the article is to advocate reproducible finance research and contribute to the recent idea of \"Open Source Cross-Sectional Asset Pricing\", proposed by @chen2020open.",
    "author": [
      {
        "name": "Majeed Simaan",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-047.zip\n\n\nA. Y. Chen and T. Zimmermann. Open source cross-sectional asset pricing. Available at SSRN, 2020.\n\n\nE. F. Fama and K. R. French. Common risk factors in the returns on stocks and bonds. Journal of Finance, 1993.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-049/",
    "title": "Analyzing Dependence between Point Processes in Time Using IndTestPP",
    "description": "The need to analyze the dependence between two or more point processes in time appears in many modeling problems related to the occurrence of events, such as the occurrence of climate events at different spatial locations or synchrony detection in spike train analysis. The package IndTestPP provides a general framework for all the steps in this type of analysis, and one of its main features is the implementation of three families of tests to study independence given the intensities of the processes, which are not only useful to assess independence but also to identify factors causing dependence. The package also includes functions for generating different types of dependent point processes, and implements computational statistical inference tools using them. An application to characterize the dependence between the occurrence of extreme heat events in three Spanish locations using the package is shown.",
    "author": [
      {
        "name": "Ana C. Cebrián",
        "url": {}
      },
      {
        "name": "Jesús Asín",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-049.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-050/",
    "title": "Conversations in Time: Interactive Visualization to Explore Structured Temporal Data",
    "description": "Temporal data often has a hierarchical structure, defined by categorical variables describing different levels, such as political regions or sales products. The nesting of categorical variables produces a hierarchical structure. The tsibbletalk package is developed to allow a user to interactively explore temporal data, relative to the nested or crossed structures. It can help to discover differences between category levels, and uncover interesting periodic or aperiodic slices. The package implements a shared `tsibble` object that allows for linked brushing between coordinated views, and a shiny module that aids in wrapping timelines for seasonal patterns. The tools are demonstrated using two data examples: domestic tourism in Australia and pedestrian traffic in Melbourne.",
    "author": [
      {
        "name": "Earo Wang",
        "url": {}
      },
      {
        "name": "Dianne Cook",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n1 Introduction\nTemporal data typically arrives as a set of many observational units measured over time. Some variables may be categorical, containing a hierarchy in the collection process, that may be measurements taken in different geographic regions, or types of products sold by one company. Exploring these multiple features can be daunting. Ensemble graphics (Unwin and Valero-Mora 2018) bundle multiple views of a data set together into one composite figure. These provide an effective approach for exploring and digesting many different aspects of temporal data. Adding interactivity to the ensemble can greatly enhance the exploration process.\nThis paper describes new software, the tsibbletalk package, for exploring temporal data using linked views and time wrapping. We first provide some background to the approach based on setting up data structures and workflow, and give an overview of interactive systems in R. The section following introduces the tsibbletalk package. We explain the mechanism for constructing interactivity, to link between multiple hierarchical data objects and hence plots, and describe the set up for interactively slicing and dicing time to wrap a series on itself to investigate periodicities.\n2 Background: tidy temporal data and workflow\nThe tsibble package (Wang et al. 2020) introduced a unified temporal data structure, referred to as a tsibble, to represent time series and longitudinal data in a tidy format (Wickham 2014). A tsibble extends the data.frame and tibble classes with the temporal contextual metadata: index and key. The index declares a data column that holds time-related indices. The key identifies a collection of related series or panels observed over the index-defined period, which can comprise multiple columns. An example of a tsibble can be found in the monthly Australian retail trade turnover data (aus_retail), available in the tsibbledata package (O’Hara-Wild et al. 2020c), shown below. The Month column holds year-months as the index. State and Industry are the identifiers for these 152 series, which form the key. Note that the column Series ID could be an alternative option for setting up the key, but State and Industry are more readable and informative. The index and key are “sticky” columns to a tsibble, forming critical pieces for fluent downstream temporal data analysis.\n\n#> # A tsibble: 64,532 x 5 [1M]\n#> # Key:       State, Industry [152]\n#>   State                        Industry       Serie…¹    Month Turno…²\n#>   <chr>                        <chr>          <chr>      <mth>   <dbl>\n#> 1 Australian Capital Territory Cafes, restau… A33498… 1982 Apr     4.4\n#> 2 Australian Capital Territory Cafes, restau… A33498… 1982 May     3.4\n#> 3 Australian Capital Territory Cafes, restau… A33498… 1982 Jun     3.6\n#> 4 Australian Capital Territory Cafes, restau… A33498… 1982 Jul     4  \n#> 5 Australian Capital Territory Cafes, restau… A33498… 1982 Aug     3.6\n#> # … with 64,527 more rows, and abbreviated variable names\n#> #   ¹​`Series ID`, ²​Turnover\n\nIn the spirit of tidy data from the tidyverse (Wickham et al. 2019), the tidyverts suite features tsibble as the foundational data structure, and helps to build a fluid and fluent pipeline for time series analysis. Besides tsibble, the feasts (O’Hara-Wild et al. 2020b) and fable (O’Hara-Wild et al. 2020a) packages fill the role of statistical analysis and forecasting in the tidyverts ecosystem. During all the steps of a time series analysis, the series of interest, denoted by the key variable, typically persist, through the trend modeling and also forecasting. We would typically want to examine the series across all of the keys.\nFigure 1 illustrates examining temporal data with many keys. The data has 152 series corresponding to different industries in retail data. The multiple series are displayed using an overlaid time series plot, along with a scatterplot of two variables (trend versus seasonal strength) from feature space, where each series is represented by a dot. The feature space is computed using the features() function from feasts, which summarises the original data for each series using various statistical features. This function along with other tidyverts functions is tsibble-aware, and outputs a table in a reduced form where each row corresponds to a series, which can be graphically displayed as in Figure 1 (right).\n\n\n\nFigure 1: Plots for the data, with the series of strongest seasonal strength highlighted. (a) An overlaid time series plot. (b) A scatter plot drawn from their time series features, where each dot represents a time series from (a).\n\n\n\nFigure 1 has also been highlighted to focus on the one series with the strongest seasonality. To create this highlighting, one needs to first filter the interesting series from the features table, and join back to the original tsibble in order to examine its trend in relation to others. This procedure can soon grow cumbersome if many series are to be explored. It illustrates a need to query interesting series on the fly. Although these two plots are static, we can consider them as linked views because the common key variables link between the two data tables producing the two plots. This motivates the work in this package, described in this paper, to enable interactivity of tsibble and tsibble-derived objects for rapid exploratory data analysis.\n3 Overview of interactivity\nThere is a long history of interactive data visualization research and corresponding systems. Within R, the systems can be roughly divided into systems utilizing web technology and those that do not.\nR shiny (Chang et al. 2020) and htmlwidgets (Vaidyanathan et al. 2019) provide infrastructure connecting R with HTML elements and JavaScript that support the interactivity. The htmlwidgets package makes it possible to embed JavaScript libraries into R so that users are able to write only R code to generate web-based plots. Many JavaScript charting libraries have been ported to R as HTML widgets, including plotly (Sievert 2020), rbokeh (Hafen and Continuum Analytics, Inc. 2020), and leaflet (Cheng et al. 2019) for maps. Interactions between different widgets can be achieved with shiny or crosstalk (Cheng 2020). The crosstalk extends htmlwidgets with shared R6 instances to support linked brushing and filtering across widgets, without relying on shiny.\nSystems without the web technology include grDevices, loon (Waddell and Oldford 2020), based on Tcl/Tk, and cranvas (Xie et al. 2014) based on Qt. They offer a wide array of pre-defined interactions, such as selecting and zooming, to manipulate plots via mouse action, keyboard strokes, and menus. The cranvastime package (Cheng et al. 2016) is an add-on to cranvas, which provides specialized interactions for temporal data, such as wrapping and mirroring.\nThe techniques implemented in the work described in this paper utilize web technology, including crosstalk, plotly, and R shiny.\n4 Using a shared temporal data object for interactivity\nThe tsibbletalk package introduces a shared tsibble instance built on a tsibble. This allows for seamless communication between different plots of temporal data. The as_shared_tsibble() function turns a tsibble into a shared instance, SharedTsibbleData, which is a subclass of SharedData from crosstalk. This is an R6 object driving data transmission across multiple views, due to its mutable and lightweight properties. The tsibbletalk package aims to streamline interactive exploration of temporal data, with the focus of temporal elements and structured linking.\nLinking between plots\nAs opposed to one-to-one linking, tsibbletalk defaults to categorical variable linking, where selecting one or more observations in one category will broadcast to all other observations in this category. That is, linking is by key variables: within the time series plot, click on any data point, and the whole line will be highlighted in response. The as_shared_tsibble() uses tsibble’s key variables to achieve these types of linking.\nThe approach can also accommodate temporal data of nesting and crossing structures. These time series are referred to as hierarchical and grouped time series in the literature (Hyndman and Athanasopoulos 2017). The aus_retail above is an example of grouped time series. Each series in the data corresponds to all possible combinations of the State and Industry variables, which means they are intrinsically crossed with each other. When one key variable is nested within another, such as regional areas within a state, this is considered to be a hierarchical structure.\nThe spec argument in as_shared_tsibble() provides a means to construct hybrid linking, that incorporates hierarchical and categorical linking. A symbolic formula can be passed to the spec argument, to define the crossing and/or nesting relationships among the key variables. Adopting Wilkinson and Rogers (1973)’s notation for factorial models, the spec follows the / and * operator conventions to declare nesting and crossing variables, respectively. The spec for the aus_retail data is therefore specified as State * Industry or Industry * State, which is the default for the presence of multiple key variables. If there is a hierarchy in the data, using / is required to indicate the parent-child relation, for a strictly one directional parent/child.\nTo illustrate nesting and crossing we use the tourism_monthly dataset (Tourism Research Australia 2020) packaged in tsibbletalk. It contains monthly domestic overnight trips across Australia. The key is comprised of three identifying variables: State, Region, and Purpose (of the trip), in particular State nesting of Region, crossed together with Purpose. This specification can be translated as follows:\n\n\nlibrary(tsibble)\nlibrary(tsibbletalk)\nlibrary(dplyr)\ntourism_shared <- tourism_monthly %>%\n  # Comment out the next line to run the full example\n  filter(State %in% c(\"Tasmania\", \"Western Australia\")) %>%\n  mutate(Region = stringr::str_replace(Region, \"Australia's \", \"WA's \")) %>%\n  as_shared_tsibble(spec = (State / Region) * Purpose)\n\n\n\n\n\nThere is a three-level hierarchy: the root node is implicitly Australia, geographically disaggregated to states, and lower-level tourism regions. A new handy function plotly_key_tree() has been implemented to help explore the hierarchy. It interprets hierarchies in the shared tsibble’s spec as a tree view, built with plotly. The following code line produces the linked tree diagram (left panel of Figure 2). The visual for the tree hierarchy detangles a group of related series and provides a bird’s eye view of the data organization.\n\n\np_l <- plotly_key_tree(tourism_shared, height = 800, width = 800)\n\n\nThe tree plot provides the graphics skeleton, upon which the rest of the data plots can be attached. In this example, small multiples of line plots are placed at the top right of Figure 2 to explore the temporal trend across regions by the trip purpose. The shared tsibble data can be directly piped into ggplot2 code to create this.\n\n\nlibrary(ggplot2)\np_tr <- tourism_shared %>%\n  ggplot(aes(x = Month, y = Trips)) +\n  geom_line(aes(group = Region), alpha = .5, linewidth = .4) +\n  facet_wrap(~ Purpose, scales = \"free_y\") +\n  scale_x_yearmonth(date_breaks = \"5 years\", date_labels = \"%Y\")\n\n\nThese line plots are heavily overplotted. To tease apart structure in the multiple time series, the features() function computes interesting characteristics, including the measures of trend and seasonality. These are displayed in the scatterplot at the bottom right, where one dot represents one series.\n\n\nlibrary(feasts)\ntourism_feat <- tourism_shared %>%\n  features(Trips, feat_stl)\np_br <- tourism_feat %>%\n  ggplot(aes(x = trend_strength, y = seasonal_strength_year)) +\n  geom_point(aes(group = Region), alpha = .8, size = 2)\n\n\nThere is one final step, to compose the three plots into an ensemble of coordinated views for exploration, shown in Figure 2. (This is the interactive realization of Figure 1). \n\n\nlibrary(plotly)\nsubplot(p_l,\n  subplot(\n    ggplotly(p_tr, tooltip = \"Region\", width = 700),\n    ggplotly(p_br, tooltip = \"Region\", width = 700),\n    nrows = 2),\n  widths = c(.4, .6)) %>%\n  highlight(dynamic = TRUE)\n\n\n\n\nFigure 2: Exploring an ensemble of linked plots of the Australian tourism data, built on a object. Click one of the nodes in the hierarchical tree to enable persistent linked brushing to compare two groups. Points and lines can also be selected in other plots. (Only Western Australia and Tasmania are included for the interactive plot in the html version, so size reasons.)\n\n\n\nSince all plots are created from one shared tsibble data source, they are self-linking views. Nodes, lines, and points are hoverable and clickable. Given the spec, clicking either one element in any plot highlights all points that match the Region category, that is, categorical linking. Figure 2 provides an interactive exploration. The steps in getting to this point were:\nA branch of the tree corresponding to Western Australia was first selected. (The names of the regions are a little odd, which is a quirk of the data set, but all four areas, Australia’s South West, …., correspond to tourist destinations in Western Australia. Hovering over the node on the branch brings up the state name.) This generated the response in the line plots and the scatterplot that colored corresponding time series and points as blue.\nTo enable persistent selection, in order to compare regions or states, “Shift” and click on the tree was done, after switching the color to red. This generated the response that points and time series corresponding to Sydney were highlighted in red.\nHovering over the points brings up the label for Sydney.\nDomestic tourism sees Sydney as one of the most popular destinations in the realm of business and friends visiting over the years. Despite the relatively weaker performance in Western Australia, Australia’s North West region sees a strongest upward trend in business, bypassing Sydney in some years.\nIn summary, shared tsibble data nicely bridges between the crosstalk and tidyverts ecosystems for temporal data using the common “key”. The as_shared_tsibble() provides a symbolic user interface for the effortless construction of a hybrid of hierarchical and categorical linking between plots. The plotly_key_tree() function, in turn, decodes the hierarchical specification to plot a tree for data overview and navigation, when accompanied by more detailed plots.\nSlicing and dicing time\nAn important aspect of temporal data is the time context. Time has a cyclical structure, that may correspond to seasonal patterns to be discovered. The index component of the (shared) tsibble data forms the basis for exploring seasonality. To investigate for periodic or aperiodic patterns, series should be wrapped on themselves, where the index is broken into temporal components like quarter or day. We shall explore this with pedestrian traffic in Melbourne, Australia.\n\n\n\n\n\n\nFigure 3: Animations showing wrapping after slicing the data at different intervals, including daily and weekly. This type of interaction is made possible with Shiny elements.\n\n\n\nThe city of Melbourne has sensors installed at various locations, to record hourly counts of pedestrians, in order to capture the daily rhythms of the downtown (City of Melbourne 2020). Figure 3 shows the first five months of 2020 foot traffic at four different locations, with the slicing and wrapping of the series into daily and weekly sections, respectively. Multiple seasonalities pop out. There tends to be a daily pattern, especially visible at the main train station, Southern Cross Station and QV market. There is also a weekday vs weekend pattern, also most visible at Southern Cross Station.\nThe wrapping procedure involves slicing the time index into seasonal periods of interest, and the result is diced time. For example, hourly pedestrian data can be decomposed into 24-hour blocks, which then overlays the counts for all respective days, as done in plot 3. For exploration, this slice position should be controlled interactively, so that many different slices can be examined rapidly. This can be achieved using shiny, with the functions provided in the tsibbletalk.\nThis shiny module, decoupled to tsibbleWrapUI() and tsibbleWrapServer(), presents a clean interface and forms a reusable component that could be embedded in any shiny application. In general, a shiny module provides a vehicle for modularising shiny applications, relevant for both users and developers. As with all shiny modules, the first argument in both functions in tsibbletalk requires a user-supplied id string that must be unique. The UI function tsibbleWrapUI() simply shows a slider that animates or controls the number of periods to be diced. The workhorse is the server function tsibbleWrapServer(), encapsulating the algorithm that transforms data and sends messages to update the plot accordingly. The plot argument expects a ggplot or plotly object, where one can plot data using either lines or other graphical elements (such as boxplots). As the function name suggests, a (shared) tsibble is needed to start the engine, so that the time index can be retrieved for dissection. The period option semantically takes a desired number of seasonal periods to be shifted, for example data shifted by “1 day”, “2 days”, or “1 week”, etc. In other words, the period defines the grind level. For date-times (represented by POSIXt), the granularity ranges from fine “day” to a much coarser “year”. The following code snippet generates Figure 3. The creation of the pedestrian20 data is available in supplementary R files.\n\n\n\n\n\nlibrary(shiny)\np_line <- pedestrian20 %>%\n  ggplot(aes(x = Date_Time, y = Count, colour = Lockdown)) +\n  geom_line(size = .3) +\n  facet_wrap(~ Sensor, scales = \"free_y\") +\n  labs(x = \"Date Time\") +\n  scale_colour_brewer(palette = \"Dark2\") +\n  theme(legend.position = \"none\")\n\nui <- fluidPage(\n  tsibbleWrapUI(\"dice\")\n)\nserver <- function(input, output, session) {\n  tsibbleWrapServer(\"dice\", ggplotly(p_line, height = 700), period = \"1 day\")\n}\nshinyApp(ui, server)\n\n\nFigure 3 corresponds to the initial state, with the slider incremented by 1-day units. The “play” button near the end of the slider can automatically animate the slicing and dicing process, walking the viewer through all 24 hours of the 152 days. Alternatively, users can drag the slider to examine selected slices.\nIn response to the slider input, the plot will be updated and loaded with newly transformed data. At its core, keeping the application as performant as possible is the top priority. Without completely redrawing the plot, the plotlyProxy() react method is invoked internally for talking to shiny. The underlying tsibble data is being called back and processed in R. Only transformed data gets fed back to the shiny server, for updating with resetting the x-axis ranges and breaks. The other plot configurations, such as marks, y-axes, and layouts, are cached and used as is.\nThe new shiny module exploits the temporal aspect for a tsibble object, available through the index attribute. It allows users to slide through relative periods to digest seasonal behaviors, with a nimble user experience.\n5 Summary\nAt the heart of the tsibbletalk package is a blending of the best bits from tsibble, crosstalk, plotly, and shiny.\nThe as_shared_tsibble() turns a tsibble object to a shared data class, with an option to express any nesting and crossing structures from the key attribute. If nesting is found in the data, the plotly_key_tree() creates an interactive hierarchical tree to help with the data overview. This sets the stage for hierarchical and categorical linking between multiple views from one shared tsibble.\nA new shiny module, tsibbleWrapUI() and tsibbleWrapServer(), provides a lens for looking at temporal aspects of a tsibble, in particular seasonal or cyclical variations. The slicing and dicing technique efficiently wrap time lines for user-defined plots. The plotlyProxy() react method makes it possible to send wrapped data to the server and amend the plot straight way.\n\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-050.zip\nCRAN packages used\ntsibbletalk, tsibble, tsibbledata, tidyverse, feasts, fable, shiny, htmlwidgets, plotly, rbokeh, leaflet, crosstalk, grDevices, loon, ggplot2\nCRAN Task Views implied by cited packages\nMissingData, Phylogenetics, Spatial, TeachingStatistics, TimeSeries, WebTechnologies\n\n\nW. Chang, J. Cheng, J. Allaire, Y. Xie and J. McPherson. Shiny: Web application framework for r. 2020. URL https://CRAN.R-project.org/package=shiny. R package version 1.5.0.\n\n\nJ. Cheng. Crosstalk: Inter-widget interactivity for HTML widgets. 2020. URL https://CRAN.R-project.org/package=crosstalk. R package version 1.1.0.1.\n\n\nJ. Cheng, B. Karambelkar and Y. Xie. Leaflet: Create interactive web maps with the JavaScript ’leaflet’ library. 2019. URL https://CRAN.R-project.org/package=leaflet. R package version 2.0.3.\n\n\nX. Cheng, D. Cook and H. Hofmann. Enabling interactivity on displays of multivariate time series and longitudinal data. Journal of Computational and Graphical Statistics, 25(4): 1057–1076, 2016. URL https://www.tandfonline.com/doi/full/10.1080/10618600.2015.1105749.\n\n\nCity of Melbourne. Pedestrian volume in melbourne. City of Melbourne, Australia, 2020. URL http://www.pedestrian.melbourne.vic.gov.au.\n\n\nR. Hafen and Continuum Analytics, Inc. Rbokeh: R interface for bokeh. 2020. URL https://CRAN.R-project.org/package=rbokeh. R package version 0.5.1.\n\n\nR. J. Hyndman and G. Athanasopoulos. Forecasting: Principles and practice. Melbourne, Australia: OTexts, 2017. URL OTexts.org/fpp2.\n\n\nM. O’Hara-Wild, R. Hyndman and E. Wang. Fable: Forecasting models for tidy time series. 2020a. URL https://CRAN.R-project.org/package=fable. R package version 0.2.1.\n\n\nM. O’Hara-Wild, R. Hyndman and E. Wang. Feasts: Feature extraction and statistics for time series. 2020b. URL https://CRAN.R-project.org/package=feasts. R package version 0.1.5.\n\n\nM. O’Hara-Wild, R. Hyndman and E. Wang. Tsibbledata: Diverse datasets for ’tsibble’. 2020c. URL https://CRAN.R-project.org/package=tsibbledata. R package version 0.2.0.\n\n\nC. Sievert. Interactive web-based data visualization with r, plotly, and shiny. Chapman; Hall/CRC, 2020. URL https://plotly-r.com.\n\n\nTourism Research Australia. Australian domestic overnight trips. Tourism Research Australia, Australia, 2020. URL https://www.tra.gov.au.\n\n\nA. Unwin and P. Valero-Mora. Ensemble Graphics. Journal of Computational and Graphical Statistics, 27(1): 157–165, 2018. URL https://www.tandfonline.com/doi/full/10.1080/10618600.2017.1383264 [online; last accessed April 23, 2019].\n\n\nR. Vaidyanathan, Y. Xie, J. Allaire, J. Cheng and K. Russell. Htmlwidgets: HTML widgets for r. 2019. URL https://CRAN.R-project.org/package=htmlwidgets. R package version 1.5.1.\n\n\nA. Waddell and R. W. Oldford. Loon: Interactive statistical data visualization. 2020. URL https://CRAN.R-project.org/package=loon. R package version 1.3.1.\n\n\nE. Wang, D. Cook and R. J. Hyndman. A new tidy data structure to support exploration and modeling of temporal data. Journal of Computational and Graphical Statistics, 29(3): 466–478, 2020. DOI 10.1080/10618600.2019.1695624.\n\n\nH. Wickham. Tidy data. Journal of Statistical Software, 59(10): 1–23, 2014.\n\n\nH. Wickham, M. Averick, J. Bryan, W. Chang, L. D. McGowan, R. François, G. Grolemund, A. Hayes, L. Henry, J. Hester, et al. Welcome to the tidyverse. Journal of Open Source Software, 4(43): 1686, 2019. URL https://doi.org/10.21105/joss.01686.\n\n\nG. N. Wilkinson and C. E. Rogers. Symbolic description of factorial models for analysis of variance. Journal of the Royal Statistical Society. Series C (Applied Statistics), 22(3): 392–399, 1973. URL http://www.jstor.org/stable/2346786.\n\n\nY. Xie, H. Hofmann and X. Cheng. Reactive programming for interactive graphics. Statistical Science, 29(2): 201–213, 2014. URL http://projecteuclid.org/euclid.ss/1408368571.\n\n\n\n\n",
    "preview": "articles/RJ-2021-050/figure/highlight-retail-1.png",
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 691
  },
  {
    "path": "articles/RJ-2021-051/",
    "title": "Automating Reproducible, Collaborative Clinical Trial Document Generation with the listdown Package",
    "description": "The conveyance of clinical trial explorations and analysis results from a statistician to a clinical investigator is a critical component of the drug development and clinical research cycle. Automating the process of generating documents for data descriptions, summaries, exploration, and analysis allows the statistician to provide a more comprehensive view of the information captured by a clinical trial, and efficient generation of these documents allows the statistican to focus more on the conceptual development of a trial or trial analysis and less on the implementation of the summaries and results on which decisions are made. This paper explores the use of the listdown package for automating reproducible documents in clinical trials that facilitate the collaboration between statisticians and clinicians as well as defining an analysis pipeline for document generation.",
    "author": [
      {
        "name": "Michael Kane",
        "url": {}
      },
      {
        "name": "Xun Jiang",
        "url": {}
      },
      {
        "name": "Simon Urbanek",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-051.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-052/",
    "title": "Towards a Grammar for Processing Clinical Trial Data",
    "description": "The goal of this paper is to help define a path toward a grammar for processing clinical trials by a) defining a format in which we would like to represent data from standardized clinical trial data b) describing a standard set of operations to transform clinical trial data into this format, and c) to identify a set of verbs and other functionality to facilitate data processing and encourage reproducibility in the processing of these data. It provides a background on standard clinical trial data and goes through a simple preprocessing example illustrating the value of the proposed approach through the use of the forceps package, which is currently being used for data of this kind.",
    "author": [
      {
        "name": "Michael J. Kane",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-052.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-054/",
    "title": "Regularized Transformation Models: The tramnet Package",
    "description": "The [tramnet](https://CRAN.R-project.org/package=tramnet) package implements regularized linear transformation models by combining the flexible class of transformation models from [tram](https://CRAN.R-project.org/package=tram) with constrained convex optimization implemented in [CVXR](https://CRAN.R-project.org/package=CVXR). Regularized transformation models unify many existing and novel regularized regression models under one theoretical and computational framework. Regularization strategies implemented for transformation models in tramnet include the Lasso, ridge regression, and the elastic net and follow the parameterization in [glmnet](https://CRAN.R-project.org/package=glmnet). Several functionalities for optimizing the hyperparameters, including model-based optimization based on the [mlrMBO](https://CRAN.R-project.org/package=mlrMBO) package, are implemented. A multitude of S3 methods is deployed for visualization, handling, and simulation purposes. This work aims at illustrating all facets of tramnet in realistic settings and comparing regularized transformation models with existing implementations of similar models.",
    "author": [
      {
        "name": "Lucas Kook, Torsten Hothorn",
        "url": {}
      }
    ],
    "date": "2021-06-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-054.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-026/",
    "title": "SEEDCCA: An Integrated R-Package for Canonical Correlation Analysis and Partial Least Squares",
    "description": "Canonical correlation analysis (CCA) has a long history as an explanatory statistical method in high-dimensional data analysis and has been successfully applied in many scientific fields such as chemometrics, pattern recognition, genomic sequence analysis, and so on. The so-called seedCCA is a newly developed R package that implements not only the standard and seeded CCA but also partial least squares. The package enables us to fit CCA to large-$p$ and small-$n$ data. The paper provides a complete guide. Also, the seeded CCA application results are compared with the regularized CCA in the existing R package. It is believed that the package, along with the paper, will contribute to high-dimensional data analysis in various science field practitioners and that the statistical methodologies in multivariate analysis become more fruitful.",
    "author": [
      {
        "name": "Bo-Young Kim, Researcher",
        "url": {}
      },
      {
        "name": "Yunju Im, Postdoctoral Associate",
        "url": {}
      },
      {
        "name": "Jae Keun Yoo, Professor",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-027/",
    "title": "npcure: An R Package for Nonparametric Inference in Mixture Cure Models",
    "description": "Mixture cure models have been widely used to analyze survival data with a cure fraction. They assume that a subgroup of the individuals under study will never experience the event (cured subjects). So, the goal is twofold: to study both the cure probability and the failure time of the uncured individuals through a proper survival function (latency). The R package npcure implements a completely nonparametric approach for estimating these functions in mixture cure models, considering right-censored survival times. Nonparametric estimators for the cure probability and the latency as functions of a covariate are provided. Bootstrap bandwidth selectors for the estimators are included. The package also implements a nonparametric covariate significance test for the cure probability, which can be applied with a continuous, discrete, or qualitative covariate.",
    "author": [
      {
        "name": "Ana López-Cheda",
        "url": {}
      },
      {
        "name": "M. Amalia Jácome",
        "url": {}
      },
      {
        "name": "Ignacio López-de-Ullibarri",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-027.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-028/",
    "title": "JMcmprsk: An R Package for Joint Modelling of Longitudinal and Survival Data with Competing Risks",
    "description": "In this paper, we describe an R package named **JMcmprsk**, for joint modelling of longitudinal and survival data with competing risks. The package in its current version implements two joint models of longitudinal and survival data proposed to handle competing risks survival data together with continuous and ordinal longitudinal outcomes respectively [@elashoff2008joint; @li2010joint]. The corresponding R implementations are further illustrated with real examples. The package also provides simulation functions to simulate datasets for joint modelling with continuous or ordinal outcomes under the competing risks scenario, which provide useful tools to validate and evaluate new joint modelling methods.",
    "author": [
      {
        "name": "Hong Wang",
        "url": {}
      },
      {
        "name": "Ning Li",
        "url": {}
      },
      {
        "name": "Shanpeng Li",
        "url": {}
      },
      {
        "name": "Gang Li\\*",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-028.zip\n\n\nR. M. Elashoff, G. Li and N. Li. A joint model for longitudinal measurements and survival data in the presence of multiple failure types. Biometrics, 64(3): 762–771, 2008.\n\n\nN. Li, R. M. Elashoff, G. Li and J. Saver. Joint modeling of longitudinal ordinal data and competing risks survival times and analysis of the NINDS rt-PA stroke trial. Statistics in medicine, 29(5): 546–557, 2010.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-029/",
    "title": "Wide-to-tall Data Reshaping Using Regular Expressions and the nc Package",
    "description": "Regular expressions are powerful tools for extracting tables from non-tabular text data. Capturing regular expressions that describe the information to extract from column names can be especially useful when reshaping a data table from wide (few rows with many regularly named columns) to tall (fewer columns with more rows). We present the R package nc (short for named capture), which provides functions for wide-to-tall data reshaping using regular expressions. We describe the main new ideas of nc, and provide detailed comparisons with related R packages (stats, utils, data.table, tidyr, tidyfast, tidyfst, reshape2, cdata).",
    "author": [
      {
        "name": "Toby Dylan Hocking",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-029.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-030/",
    "title": "Linear Regression with Stationary Errors: the R Package slm",
    "description": "This paper introduces the R package [slm](https://CRAN.R-project.org/package=slm), which stands for Stationary Linear Models. The package contains a set of statistical procedures for linear regression in the general context where the error process is strictly stationary with a short memory. We work in the setting of [@hannan1973central], who proved the asymptotic normality of the (normalized) least squares estimators (LSE) under very mild conditions on the error process. We propose different ways to estimate the asymptotic covariance matrix of the LSE and then to correct the type I error rates of the usual tests on the parameters (as well as confidence intervals). The procedures are evaluated through different sets of simulations.",
    "author": [
      {
        "name": "Emmanuel Caron",
        "url": {}
      },
      {
        "name": "Jérôme Dedecker",
        "url": {}
      },
      {
        "name": "Bertrand Michel",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-030.zip\n\n\nE. J. Hannan. Central limit theorems for time series regression. Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete, 26(2): 157–170, 1973.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-031/",
    "title": "exPrior: An R Package for the Formulation of Ex-Situ Priors",
    "description": "The [exPrior](https://CRAN.R-project.org/package=exPrior) package implements a procedure for formulating informative priors of geostatistical properties for a target field site, called *ex-situ priors* and introduced in [@Cucchi2019]. The procedure uses a Bayesian hierarchical model to assimilate multiple types of data coming from multiple sites considered as similar to the target site. This prior summarizes the information contained in the data in the form of a probability density function that can be used to better inform further geostatistical investigations at the site. The formulation of the prior uses ex-situ data, where the data set can either be gathered by the user or come in the form of a structured database. The package is designed to be flexible in that regard. For illustration purposes and for easiness of use, the package is ready to be used with the worldwide hydrogeological parameter database (WWHYPDA) [@Comunian2009].",
    "author": [
      {
        "name": "Falk Heße",
        "url": {}
      },
      {
        "name": "Karina Cucchi",
        "url": {}
      },
      {
        "name": "Nura Kawa",
        "url": {}
      },
      {
        "name": "Yoram Rubin",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\n\n\nA. Comunian and P. Renard. Introducing wwhypda: A world-wide collaborative hydrogeological parameters database. Hydrogeology Journal, 17(2): 481–489, 2009. DOI 10.1007/s10040-008-0387-x.\n\n\nK. Cucchi, F. Heße, N. Kawa, C. Wang and Y. Rubin. Ex-situ priors: A Bayesian hierarchical framework for defining informative prior distributions in hydrogeology. Advances in Water Resources, 126: 65–78, 2019. DOI 10.1016/j.advwatres.2019.02.003.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-032/",
    "title": "clustcurv: An R Package for Determining Groups in Multiple Curves ",
    "description": "In many situations, it could be interesting to ascertain whether groups of curves can be performed, especially when confronted with a considerable number of curves. This paper introduces an R package, known as [clustcurv](https://CRAN.R-project.org/package=clustcurv), for determining clusters of curves with an automatic selection of their number. The package can be used for determining groups in multiple survival curves as well as for multiple regression curves. Moreover, it can be used with large numbers of curves. An illustration of the use of clustcurv is provided, using both real data examples and artificial data.",
    "author": [
      {
        "name": "Nora M. Villanueva",
        "url": {}
      },
      {
        "name": "Marta Sestelo",
        "url": {}
      },
      {
        "name": "Luís Meira-Machado",
        "url": {}
      },
      {
        "name": "Javier Roca-Pardiñas",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-032.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-033/",
    "title": "Benchmarking R packages for Calculation of Persistent Homology",
    "description": "Several persistent homology software libraries have been implemented in R. Specifically, the Dionysus, GUDHI, and Ripser libraries have been wrapped by the **TDA** and **TDAstats** CRAN packages. These software represent powerful analysis tools that are computationally expensive and, to our knowledge, have not been formally benchmarked. Here, we analyze runtime and memory growth for the 2 R packages and the 3 underlying libraries. We find that datasets with less than 3 dimensions can be evaluated with persistent homology fastest by the GUDHI library in the **TDA** package. For higher-dimensional datasets, the Ripser library in the TDAstats package is the fastest. Ripser and **TDAstats** are also the most memory-efficient tools to calculate persistent homology.",
    "author": [
      {
        "name": "Eashwar V. Somasundaram",
        "url": {}
      },
      {
        "name": "Shael E. Brown",
        "url": {}
      },
      {
        "name": "Adam Litzler",
        "url": {}
      },
      {
        "name": "Jacob G. Scott",
        "url": {}
      },
      {
        "name": "Raoul R. Wadhwa",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-033.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-034/",
    "title": "Statistical Quality Control with the qcr Package",
    "description": "The R package [qcr](https://CRAN.R-project.org/package=qcr) for Statistical Quality Control (SQC) is introduced and described. It includes a comprehensive set of univariate and multivariate SQC tools that completes and increases the SQC techniques available in R. Apart from integrating different R packages devoted to SQC ([qcc](https://CRAN.R-project.org/package=qcc), [MSQC](https://CRAN.R-project.org/package=MSQC)), [qcr](https://CRAN.R-project.org/package=qcr) provides nonparametric tools that are highly useful when Gaussian assumption is not met. This package computes standard univariate control charts for individual measurements, $\\bar{x}$, $S$, $R$, $p$, $np$, $c$, $u$, EWMA, and CUSUM. In addition, it includes functions to perform multivariate control charts such as Hotelling T$^2$, MEWMA and MCUSUM. As representative features, multivariate nonparametric alternatives based on data depth are implemented in this package: $r$, $Q$ and $S$ control charts. The [qcr](https://CRAN.R-project.org/package=qcr) library also estimates the most complete set of capability indices from first to the fourth generation, covering the nonparametric alternatives, and performing the corresponding capability analysis graphical outputs, including the process capability plots. Moreover, Phase I and II control charts for functional data are included.",
    "author": [
      {
        "name": "Miguel Flores",
        "url": {}
      },
      {
        "name": "Rubén Fernández-Casal",
        "url": {}
      },
      {
        "name": "Salvador Naya",
        "url": {}
      },
      {
        "name": "Javier Tarrío-Saavedra",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-034.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-035/",
    "title": "pdynmc: A Package for Estimating Linear Dynamic Panel Data Models Based on Nonlinear Moment Conditions",
    "description": "This paper introduces [pdynmc](https://CRAN.R-project.org/package=pdynmc), an R package that provides users sufficient flexibility and precise control over the estimation and inference in linear dynamic panel data models. The package primarily allows for the inclusion of nonlinear moment conditions and the use of iterated GMM; additionally, visualizations for data structure and estimation results are provided. The current implementation reflects recent developments in literature, uses sensible argument defaults, and aligns commercial and noncommercial estimation commands. Since the understanding of the model assumptions is vital for setting up plausible estimation routines, we provide a broad introduction of linear dynamic panel data models directed towards practitioners before concisely describing the functionality available in pdynmc regarding instrument type, covariate type, estimation methodology, and general configuration. We then demonstrate the functionality by revisiting the popular firm-level dataset of @AreBon1991.",
    "author": [
      {
        "name": "Markus Fritsch",
        "url": {}
      },
      {
        "name": "Andrew Adrian Yu Pua",
        "url": {}
      },
      {
        "name": "Joachim Schnurbus",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\n\n\nM. Arellano and S. Bond. Some Tests of Specification for Panel Data: Monte Carlo Evidence and an Application to Employment Equations. The Review of Economic Studies, 58(2): 277–297, 1991. URL https://doi.org/10.2307/2297968.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-036/",
    "title": "DChaos: An R Package for Chaotic Time Series Analysis",
    "description": "Chaos theory has been hailed as a revolution of thoughts and attracting ever-increasing attention of many scientists from diverse disciplines. Chaotic systems are non-linear deterministic dynamic systems which can behave like an erratic and apparently random motion. A relevant field inside chaos theory is the detection of chaotic behavior from empirical time-series data. One of the main features of chaos is the well-known initial-value sensitivity property. Methods and techniques related to testing the hypothesis of chaos try to quantify the initial-value sensitive property estimating the so-called Lyapunov exponents. This paper describes the main estimation methods of the Lyapunov exponent from time series data. At the same time, we present the DChaos library. R users may compute the delayed-coordinate embedding vector from time series data, estimates the best-fitted neural net model from the delayed-coordinate embedding vectors, calculates analytically the partial derivatives from the chosen neural nets model. They can also obtain the neural net estimator of the Lyapunov exponent from the partial derivatives computed previously by two different procedures and four ways of subsampling by blocks. To sum up, the DChaos package allows the R users to test robustly the hypothesis of chaos in order to know if the data-generating process behind time series behaves chaotically or not. The package's functionality is illustrated by examples.",
    "author": [
      {
        "name": "Julio E. Sandubete",
        "url": {}
      },
      {
        "name": "Lorenzo Escot",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-036.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-038/",
    "title": "IndexNumber: An R Package for Measuring the Evolution of Magnitudes",
    "description": "Index numbers are descriptive statistical measures useful in economic settings for comparing simple and complex magnitudes registered, usually in two time periods. Although this theory has a large history, it still plays an important role in modern today's societies where big amounts of economic data are available and need to be analyzed. After a detailed revision on classical index numbers in literature, this paper is focused on the description of the R package [IndexNumber](https://CRAN.R-project.org/package=IndexNumber) with strong capabilities for calculating them. Two of the four real data sets contained in this library are used for illustrating the determination of the index numbers in this work. Graphical tools are also implemented in order to show the time evolution of considered magnitudes simplifying the interpretation of the results.",
    "author": [
      {
        "name": "Alejandro Saavedra-Nieves",
        "url": {}
      },
      {
        "name": "Paula Saavedra-Nieves",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-038.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-039/",
    "title": "StratigrapheR: Concepts for Litholog Generation in R ",
    "description": "The [StratigrapheR](https://CRAN.R-project.org/package=StratigrapheR) package proposes new concepts for the generation of lithological logs, or lithologs, in R. The generation of lithologs in a scripting environment opens new opportunities for the processing and analysis of stratified geological data. Among the new concepts presented: new plotting and data processing methodologies, new general R functions, and computer-oriented data conventions are provided. The package structure allows for these new concepts to be further improved, which can be done independently by any R user. The current limitations of the package are highlighted, along with the limitations in R for geological data processing, to help identify the best paths for improvements.",
    "author": [
      {
        "name": "Sébastien Wouters",
        "url": {}
      },
      {
        "name": "Anne-Christine Da Silva",
        "url": {}
      },
      {
        "name": "Frédéric Boulvain",
        "url": {}
      },
      {
        "name": "Xavier Devleeschouwer",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-039.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-040/",
    "title": "ROBustness In Network (robin): an R Package for Comparison and Validation of Communities ",
    "description": "In network analysis, many community detection algorithms have been developed. However, their implementation leaves unaddressed the question of the statistical validation of the results. Here, we present [robin](https://CRAN.R-project.org/package=robin) (ROBustness In Network), an R package to assess the robustness of the community structure of a network found by one or more methods to give indications about their reliability. The procedure initially detects if the community structure found by a set of algorithms is statistically significant and then compares two selected detection algorithms on the same graph to choose the one that better fits the network of interest. We demonstrate the use of our package on the American College Football benchmark dataset.",
    "author": [
      {
        "name": "Valeria Policastro (*package author and creator*)",
        "url": {}
      },
      {
        "name": "Dario Righelli",
        "url": {}
      },
      {
        "name": "Annamaria Carissimo",
        "url": {}
      },
      {
        "name": "Luisa Cutillo",
        "url": {}
      },
      {
        "name": "Italia De Feis",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-041/",
    "title": "Finding Optimal Normalizing Transformations via bestNormalize",
    "description": "The bestNormalize R package was designed to help users find a transformation that can effectively normalize a vector regardless of its actual distribution. Each of the many normalization techniques that have been developed has its own strengths and weaknesses, and deciding which to use until data are fully observed is difficult or impossible. This package facilitates choosing between a range of possible transformations and will automatically return the best one, i.e., the one that makes data look the *most* normal. To evaluate and compare the normalization efficacy across a suite of possible transformations, we developed a statistic based on a goodness of fit test divided by its degrees of freedom. Transformations can be seamlessly trained and applied to newly observed data and can be implemented in conjunction with caret and recipes for data preprocessing in machine learning workflows. Custom transformations and normalization statistics are supported.",
    "author": [
      {
        "name": "Ryan A. Peterson",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-041.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-042/",
    "title": "Package wsbackfit for Smooth Backfitting Estimation of Generalized Structured Models",
    "description": "A package is introduced that provides the weighted smooth backfitting estimator for a large family of popular semiparametric regression models. This family is known as *generalized structured models*, comprising, for example, generalized varying coefficient model, generalized additive models, mixtures, potentially including parametric parts. The kernel-based weighted smooth backfitting belongs to the statistically most efficient procedures for this model class. Its asymptotic properties are well-understood thanks to the large body of literature about this estimator. The introduced weights allow for the inclusion of sampling weights, trimming, and efficient estimation under heteroscedasticity. Further options facilitate easy handling of aggregated data, prediction, and the presentation of estimation results. Cross-validation methods are provided which can be used for model and bandwidth selection.[^1]",
    "author": [
      {
        "name": "Javier Roca-Pardiñas",
        "url": {}
      },
      {
        "name": "María Xosé Rodríguez-Álvarez",
        "url": {}
      },
      {
        "name": "Stefan Sperlich",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-042.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-043/",
    "title": "RLumCarlo: Simulating Cold Light using Monte Carlo Methods",
    "description": "The luminescence phenomena of insulators and semiconductors (e.g., natural minerals such as quartz) have various application domains. For instance, Earth Sciences and archaeology exploit luminescence as a dating method. Herein, we present the R package [RLumCarlo](https://CRAN.R-project.org/package=RLumCarlo) implementing sets of luminescence models to be simulated with Monte Carlo (MC) methods. MC methods make a powerful ally to all kinds of simulation attempts involving stochastic processes. Luminescence production is such a stochastic process in the form of charge (electron-hole pairs) interaction within insulators and semiconductors. To simulate luminescence-signal curves, we distribute single and independent MC processes to virtual MC clusters. [RLumCarlo](https://CRAN.R-project.org/package=RLumCarlo) comes with a modularized design and consistent user interface: (1) C++ functions represent the modeling core and implement models for specific stimulations modes. (2) R functions give access to combinations of models and stimulation modes, start the simulation and render terminal and graphical feedback. The combination of MC clusters supports the simulation of complex luminescence phenomena.",
    "author": [
      {
        "name": "Sebastian Kreutzer",
        "url": {}
      },
      {
        "name": "Johannes Friedrich",
        "url": {}
      },
      {
        "name": "Vasilis Pagonis",
        "url": {}
      },
      {
        "name": "Christian Laag",
        "url": {}
      },
      {
        "name": "Ena Rajovic",
        "url": {}
      },
      {
        "name": "Christoph Schmidt",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-043.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:35+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-025/",
    "title": "TULIP: A Toolbox for Linear Discriminant Analysis with Penalties",
    "description": "Linear discriminant analysis (LDA) is a powerful tool in building classifiers with easy computation and interpretation. Recent advancements in science technology have led to the popularity of datasets with high dimensions, high orders and complicated structure. Such datasetes motivate the generalization of LDA in various research directions. The R package TULIP integrates several popular high-dimensional LDA-based methods and provides a comprehensive and user-friendly toolbox for linear, semi-parametric and tensor-variate classification. Functions are included for model fitting, cross validation and prediction. In addition, motivated by datasets with diverse sources of predictors, we further include functions for covariate adjustment. Our package is carefully tailored for low storage and high computation efficiency. Moreover, our package is the first R package for many of these methods, providing great convenience to researchers in this area.",
    "author": [
      {
        "name": "Yuqing Pan",
        "url": {}
      },
      {
        "name": "Qing Mai",
        "url": {}
      },
      {
        "name": "Xin Zhang",
        "url": {}
      }
    ],
    "date": "2021-01-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-025.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-015/",
    "title": "Kuhn-Tucker and Multiple Discrete-Continuous Extreme Value Model Estimation and Simulation in R: The rmdcev Package",
    "description": "This paper introduces the package rmdcev in R for estimation and simulation of Kuhn-Tucker demand models with individual heterogeneity. The models supported by rmdcev are the multiple-discrete continuous extreme value (MDCEV) model and Kuhn-Tucker specification common in the environmental economics literature on recreation demand. Latent class and random parameters specifications can be implemented and the models are fit using maximum likelihood estimation or Bayesian estimation. The rmdcev package also implements demand forecasting and welfare calculation for policy simulation. The purpose of this paper is to describe the model estimation and simulation framework and to demonstrate the functionalities of rmdcev using real datasets.",
    "author": [
      {
        "name": "Patrick Lloyd-Smith",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-015.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-016/",
    "title": "NTS: An R Package for Nonlinear Time Series Analysis",
    "description": "Linear time series models are commonly used in analyzing dependent data and in forecasting. On the other hand, real phenomena often exhibit nonlinear behavior and the observed data show nonlinear dynamics. This paper introduces the R package NTS that offers various computational tools and nonlinear models for analyzing nonlinear dependent data. The package fills the gaps of several outstanding R packages for nonlinear time series analysis. Specifically, the NTS package covers the implementation of threshold autoregressive (TAR) models, autoregressive conditional mean models with exogenous variables (ACMx), functional autoregressive models, and state-space models. Users can also evaluate and compare the performance of different models and select the best one for prediction. Furthermore, the package implements flexible and comprehensive sequential Monte Carlo methods (also known as particle filters) for modeling non-Gaussian or nonlinear processes. Several examples are used to demonstrate the capabilities of the NTS package.",
    "author": [
      {
        "name": "Xialu Liu",
        "url": {}
      },
      {
        "name": "Rong Chen",
        "url": {}
      },
      {
        "name": "Ruey Tsay",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-016.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-017/",
    "title": "Species Distribution Modeling using Spatial Point Processes: a Case Study of Sloth Occurrence in Costa Rica",
    "description": "Species distribution models are widely used in ecology for conservation management of species and their environments. This paper demonstrates how to fit a log-Gaussian Cox process model to predict the intensity of sloth occurrence in Costa Rica, and assess the effect of climatic factors on spatial patterns using the R-INLA package. Species occurrence data are retrieved using spocc, and spatial climatic variables are obtained with raster. Spatial data and results are manipulated and visualized by means of several packages such as raster and tmap. This paper provides an accessible illustration of spatial point process modeling that can be used to analyze data that arise in a wide range of fields including ecology, epidemiology and the environment.",
    "author": [
      {
        "name": "Paula Moraga",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-017.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-018/",
    "title": "A Graphical EDA Tool with ggplot2: brinton",
    "description": "We present [brinton](https://CRAN.R-project.org/package=brinton) package, which we developed for graphical exploratory data analysis in R. Based on [ggplot2](https://CRAN.R-project.org/package=ggplot2), [gridExtra](https://CRAN.R-project.org/package=gridExtra) and [rmarkdown](https://CRAN.R-project.org/package=rmarkdown), brinton package introduces wideplot() graphics for exploring the structure of a dataset through a grid of variables and graphic types. It also introduces longplot() graphics, which present the entire catalog of available graphics for representing a particular variable using a grid of graphic types and variations on these types. Finally, it introduces the plotup() function, which complements the previous two functions in that it presents a particular graphic for a specific variable of a dataset. This set of functions is useful for understanding the structure of a data set, discovering unexpected properties in the data, evaluating different graphic representations of these properties, and selecting a particular graphic for display on the screen.",
    "author": [
      {
        "name": "Pere Millán-Martínez",
        "url": {}
      },
      {
        "name": "Ramon Oller",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-019/",
    "title": "MoTBFs: An R Package for Learning Hybrid Bayesian Networks Using Mixtures of Truncated Basis Functions",
    "description": "This paper introduces [MoTBFs](https://CRAN.R-project.org/package=MoTBFs), an R package for manipulating mixtures of truncated basis functions. This class of functions allows the representation of joint probability distributions involving discrete and continuous variables simultaneously, and includes mixtures of truncated exponentials and mixtures of polynomials as special cases. The package implements functions for learning the parameters of univariate, multivariate, and conditional distributions, and provides support for parameter learning in Bayesian networks with both discrete and continuous variables. Probabilistic inference using forward sampling is also implemented. Part of the functionality of the MoTBFs package relies on the [bnlearn](https://CRAN.R-project.org/package=bnlearn) package, which includes functions for learning the structure of a Bayesian network from a data set. Leveraging this functionality, the MoTBFs package supports learning of MoTBF-based Bayesian networks over hybrid domains. We give a brief introduction to the methodological context and algorithms implemented in the package. An extensive illustrative example is used to describe the package, its functionality, and its usage.",
    "author": [
      {
        "name": "Inmaculada Pérez-Bernabé",
        "url": {}
      },
      {
        "name": "Ana D. Maldonado",
        "url": {}
      },
      {
        "name": "Thomas D. Nielsen",
        "url": {}
      },
      {
        "name": "Antonio Salmerón",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-019.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-020/",
    "title": "Analyzing Basket Trials under Multisource Exchangeability Assumptions",
    "description": "Basket designs are prospective clinical trials that are devised with the hypothesis that the presence of selected molecular features determine a patient's subsequent response to a particular \"targeted\" treatment strategy. Basket trials are designed to enroll multiple clinical subpopulations to which it is assumed that the therapy in question offers beneficial efficacy in the presence of the targeted molecular profile. The treatment, however, may not offer acceptable efficacy to all subpopulations enrolled. Moreover, for rare disease settings, such as oncology wherein these trials have become popular, marginal measures of statistical evidence are difficult to interpret for sparsely enrolled subpopulations. Consequently, basket trials pose challenges to the traditional paradigm for trial design, which assumes inter-patient exchangeability. The package [basket](https://CRAN.R-project.org/package=basket) for the R programmming environment facilitates the analysis of basket trials by implementing multi-source exchangeability models. By evaluating all possible pairwise exchangeability relationships, this hierarchical modeling framework facilitates Bayesian posterior shrinkage among a collection of discrete and pre-specified subpopulations. Analysis functions are provided to implement posterior inference of the response rates and all possible exchangeability relationships between subpopulations. In addition, the package can identify \"poolable\" subsets of and report their response characteristics. The functionality of the package is demonstrated using data from an oncology study with subpopulations defined by tumor histology.",
    "author": [
      {
        "name": "Michael J. Kane",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-020.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-021/",
    "title": "OpenLand: Software for Quantitative Analysis and Visualization of Land Use and Cover Change",
    "description": "There is an increasing availability of spatially explicit, freely available land use and cover (LUC) time series worldwide. Because of the enormous amount of data this represents, the continuous updates and improvements in spatial and temporal resolution and category differentiation, as well as increasingly dynamic and complex changes made, manual data extraction and analysis is highly time consuming, and making software tools available to automatize LUC data assessment is becoming imperative. This paper presents a software developed in R, which combines LUC raster time series data and their transitions, calculates state-of-the-art LUC change indicators, and creates spatio-temporal visualizations, all in a coherent workflow. The functionality of the application developed is demonstrated using an LUC dataset of the Pantanal floodplain contribution area in Central Brazil.",
    "author": [
      {
        "name": "Reginal Exavier",
        "url": {}
      },
      {
        "name": "Peter Zeilhofer",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-021.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-023/",
    "title": "FarmTest: An R Package for Factor-Adjusted Robust Multiple Testing",
    "description": "We provide a publicly available library [FarmTest](https://CRAN.R-project.org/package=FarmTest) in the R programming system. This library implements a factor-adjusted robust multiple testing principle proposed by [@FKSZ2017] for large-scale simultaneous inference on mean effects. We use a multi-factor model to explicitly capture the dependence among a large pool of variables. Three types of factors are considered: observable, latent, and a mixture of observable and latent factors. The non-factor case, which corresponds to standard multiple mean testing under weak dependence, is also included. The library implements a series of adaptive Huber methods integrated with fast data-driven tuning schemes to estimate model parameters and to construct test statistics that are robust against heavy-tailed and asymmetric error distributions. Extensions to two-sample multiple mean testing problems are also discussed. The results of some simulation experiments and a real data analysis are reported.",
    "author": [
      {
        "name": "Koushiki Bose, Jianqing Fan",
        "url": {}
      },
      {
        "name": "Yuan Ke",
        "url": {}
      },
      {
        "name": "Xiaoou Pan, Wen-Xin Zhou",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-023.zip\n\n\nJ. Fan, Y. Ke, Q. Sun and W.-X. Zhou. FarmTest: Factor-adjusted robust multiple testing with approximate false discovery control. Journal of the American Statistical Association, 114(528): 1880–1893, 2019. URL https://doi.org/10.1080/01621459.2018.1527700.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-024/",
    "title": "User-Specified General-to-Specific and Indicator Saturation Methods",
    "description": "General-to-Specific (GETS) modelling provides a comprehensive, systematic and cumulative approach to modelling that is ideally suited for conditional forecasting and counterfactual analysis, whereas Indicator Saturation (ISAT) is a powerful and flexible approach to the detection and estimation of structural breaks (e.g. changes in parameters), and to the detection of outliers. To these ends, multi-path backwards elimination, single and multiple hypothesis tests on the coefficients, diagnostics tests and goodness-of-fit measures are combined to produce a parsimonious final model. In many situations a specific model or estimator is needed, a specific set of diagnostics tests may be required, or a specific fit criterion is preferred. In these situations, if the combination of estimator/model, diagnostics tests and fit criterion is not offered in a pre-programmed way by publicly available software, then the implementation of user-specified GETS and ISAT methods puts a large programming-burden on the user. Generic functions and procedures that facilitate the implementation of user-specified GETS and ISAT methods for specific problems can therefore be of great benefit. The R package gets is the first software -- both inside and outside the R universe -- to provide a complete set of facilities for user-specified GETS and ISAT methods: User-specified model/estimator, user-specified diagnostics and user-specified goodness-of-fit criteria. The aim of this article is to illustrate how user-specified GETS and ISAT methods can be implemented with the R package gets.",
    "author": [
      {
        "name": "Genaro Sucarrat",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-024.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-001/",
    "title": "The biglasso Package: A Memory- and Computation-Efficient Solver for Lasso Model Fitting with Big Data in R",
    "description": "Penalized regression models such as the lasso have been extensively applied to analyzing high-dimensional data sets. However, due to memory limitations, existing R packages like [glmnet](https://CRAN.R-project.org/package=glmnet) and [ncvreg](https://CRAN.R-project.org/package=ncvreg) are not capable of fitting lasso-type models for ultrahigh-dimensional, multi-gigabyte data sets that are increasingly seen in many areas such as genetics, genomics, biomedical imaging, and high-frequency finance. In this research, we implement an R package called [biglasso](https://CRAN.R-project.org/package=biglasso) that tackles this challenge. biglasso utilizes memory-mapped files to store the massive data on the disk, only reading data into memory when necessary during model fitting, and is thus able to handle out-of-core computation seamlessly. Moreover, it's equipped with newly proposed, more efficient feature screening rules, which substantially accelerate the computation. Benchmarking experiments show that our biglasso package, as compared to existing popular ones like glmnet, is much more memory- and computation-efficient. We further analyze a 36 GB simulated GWAS data set on a laptop with only 16 GB RAM to demonstrate the out-of-core computation capability of biglasso in analyzing massive data sets that cannot be accommodated by existing R packages.",
    "author": [
      {
        "name": "Yaohui Zeng",
        "url": {}
      },
      {
        "name": "Patrick Breheny",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-001.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-002/",
    "title": "Comparing Multiple Survival Functions with Crossing Hazards in R",
    "description": "It is frequently of interest in time-to-event analysis to compare multiple survival functions nonparametrically. However, when the hazard functions cross, tests in existing R packages do not perform well. To address the issue, we introduce the package survELtest, which provides tests for comparing multiple survival functions with possibly crossing hazards. Due to its powerful likelihood ratio formulation, this is the only R package to date that works when the hazard functions cross. We illustrate the use of the procedures in survELtest by applying them to data from randomized clinical trials and simulated datasets. We show that these methods lead to more significant results than those obtained by existing R packages.",
    "author": [
      {
        "name": "Hsin-wen Chang",
        "url": {}
      },
      {
        "name": "Pei-Yuan Tsai",
        "url": {}
      },
      {
        "name": "Jen-Tse Kao",
        "url": {}
      },
      {
        "name": "Guo-You Lan",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-002.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-003/",
    "title": "A Unified Algorithm for the Non-Convex Penalized Estimation: The ncpen Package",
    "description": "Various R packages have been developed for the non-convex penalized estimation but they can only be applied to the smoothly clipped absolute deviation (SCAD) or minimax concave penalty (MCP). We develop an R package, entitled ncpen, for the non-convex penalized estimation in order to make data analysts to experience other non-convex penalties. The package ncpen implements a unified algorithm based on the convex concave procedure and modified local quadratic approximation algorithm, which can be applied to a broader range of non-convex penalties, including the SCAD and MCP as special examples. Many user-friendly functionalities such as generalized information criteria, cross-validation and ridge regularization are provided also.",
    "author": [
      {
        "name": "Dongshin Kim",
        "url": {}
      },
      {
        "name": "Sangin Lee",
        "url": {}
      },
      {
        "name": "Sunghoon Kwon",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-003.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-004/",
    "title": "Six Years of Shiny in Research - Collaborative Development of Web Tools in R",
    "description": "The use of Shiny in research publications is investigated over the six and a half years since the appearance of this popular web application framework for R, which has been utilised in many varied research areas. While it is demonstrated that the complexity of Shiny applications is limited by the background architecture, and real security concerns exist for novice app developers, the collaborative benefits are worth attention from the wider research community. Shiny simplifies the use of complex methodologies for people of different specialities, at the level of proficiency appropriate for the end user. This enables a diverse community of users to interact efficiently, and utilise cutting edge methodologies. The literature reviewed demonstrates that complex methodologies can be put into practice without insisting on investment in professional training, for a comprehensive understanding from all participants. It appears that Shiny opens up concurrent benefits in communication between those who analyse data and other disciplines, that would enrich much of the peer-reviewed research.",
    "author": [
      {
        "name": "Peter Kasprzak",
        "url": {}
      },
      {
        "name": "Lachlan Mitchell",
        "url": {}
      },
      {
        "name": "Olena Kravchuk",
        "url": {}
      },
      {
        "name": "Andy Timmins",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-005/",
    "title": "fitzRoy - An R Package to Encourage Reproducible Sports Analysis",
    "description": "The importance of reproducibility, and the related issue of open access to data, has received a lot of recent attention. Momentum on these issues is gathering in the sports analytics community. While Australian Rules football (AFL) is the leading commercial sport in Australia, unlike popular international sports, there has been no mechanism for the public to access comprehensive statistics on players and teams. Expert commentary currently relies heavily on data that isn't made readily accessible and this produces an unnecessary barrier for the development of an inclusive sports analytics community. We present the R package fitzRoy to provide easy access to AFL statistics.",
    "author": [
      {
        "name": "Robert N. Nguyen",
        "url": {}
      },
      {
        "name": "James T. Day",
        "url": {}
      },
      {
        "name": "David I. Warton",
        "url": {}
      },
      {
        "name": "Oscar Lane",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-005.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-006/",
    "title": "Assembling Pharmacometric Datasets in R - The puzzle Package",
    "description": "Pharmacometric analyses are integral components of the drug development process. The core of each pharmacometric analysis is a dataset. The time required to construct a pharmacometrics dataset can sometimes be higher than the effort required for the modeling *per se*. To simplify the process, the puzzle R package has been developed aimed at simplifying and facilitating the time consuming and error prone task of assembling pharmacometrics datasets. Puzzle consist of a series of functions written in R. These functions create, from tabulated files, datasets that are compatible with the formatting requirements of the gold standard non-linear mixed effects modeling software. With only one function, puzzle(), complex pharmacometrics databases can easily be assembled. Users are able to select from different absorption processes such as zero- and first-order, or a combination of both. Furthermore, datasets containing data from one or more analytes, and/or one or more responses, and/or time dependent and/or independent covariates, and/or urine data can be simultaneously assembled. The puzzle package is a powerful and efficient tool that helps modelers, programmers and pharmacometricians through the challenging process of assembling pharmacometrics datasets.",
    "author": [
      {
        "name": "Mario González Sales\\*",
        "url": {}
      },
      {
        "name": "Olivier Barrière\\*",
        "url": {}
      },
      {
        "name": "Pierre Olivier Tremblay",
        "url": {}
      },
      {
        "name": "Guillaume Bonnefois",
        "url": {}
      },
      {
        "name": "Julie Desrochers",
        "url": {}
      },
      {
        "name": "Fahima Nekka",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-006.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-007/",
    "title": "RNGforGPD: An R Package for Generation of Univariate and Multivariate Generalized Poisson Data",
    "description": "This article describes the R package RNGforGPD, which is designed for the generation of univariate and multivariate generalized Poisson data. Some illustrative examples are given, the utility and functionality of the package are demonstrated; and its performance is assessed via simulations that are devised around both artificial and real data.",
    "author": [
      {
        "name": "Hesen Li",
        "url": {}
      },
      {
        "name": "Dr. Hakan Demirtas",
        "url": {}
      },
      {
        "name": "Ruizhe Chen",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-007.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-008/",
    "title": "Testing the Equality of Normal Distributed and Independent Groups' Means Under Unequal Variances by doex Package",
    "description": "In this paper, we present the [doex](https://CRAN.R-project.org/package=doex) package contains the tests for equality of normal distributed and independent group means under unequal variances such as Cochran F, Welch-Aspin, Welch, Box, Scott-Smith, Brown-Forsythe, Johansen F, Approximate F, Alexander-Govern, Generalized F, Modified Brown-Forsythe, Permutation F, Adjusted Welch, B2, Parametric Bootstrap, Fiducial Approach, and Alvandi Generalized F-test. Most of these tests are not available in any package. Thus, doex is easy to use for researchers in multidisciplinary studies. In this study, an extensive Monte-Carlo simulation study is conducted to investigate the performance of the the tests for equality of normal distributed group means under unequal variances",
    "author": [
      {
        "name": "Mustafa Cavus",
        "url": {}
      },
      {
        "name": "Berna Yazıcı",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-008.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-010/",
    "title": "A Fast and Scalable Implementation Method for Competing Risks Data with the R Package fastcmprsk",
    "description": "Advancements in medical informatics tools and high-throughput biological experimentation make large-scale biomedical data routinely accessible to researchers. Competing risks data are typical in biomedical studies where individuals are at risk to more than one cause (type of event) which can preclude the others from happening. The [@fine1999proportional] proportional subdistribution hazards model is a popular and well-appreciated model for competing risks data and is currently implemented in a number of statistical software packages. However, current implementations are not computationally scalable for large-scale competing risks data. We have developed an R package, [fastcmprsk](https://CRAN.R-project.org/package=fastcmprsk), that uses a novel forward-backward scan algorithm to significantly reduce the computational complexity for parameter estimation by exploiting the structure of the subject-specific risk sets. Numerical studies compare the speed and scalability of our implementation to current methods for unpenalized and penalized Fine-Gray regression and show impressive gains in computational efficiency.",
    "author": [
      {
        "name": "Eric S. Kawaguchi",
        "url": {}
      },
      {
        "name": "Jenny I. Shen",
        "url": {}
      },
      {
        "name": "Gang Li",
        "url": {}
      },
      {
        "name": "Marc A. Suchard",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-010.zip\n\n\nJ. P. Fine and R. J. Gray. A proportional hazards model for the subdistribution of a competing risk. Journal of the American Statistical Association, 94(446): 496–509, 1999. DOI 10.1080/01621459.1999.10474144.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-011/",
    "title": "ordinalClust: An R Package to Analyze Ordinal Data",
    "description": "Ordinal data are used in many domains, especially when measurements are collected from people through observations, tests, or questionnaires. [ordinalClust](https://CRAN.R-project.org/package=ordinalClust) is an innovative R package dedicated to ordinal data that provides tools for modeling, clustering, co-clustering and classifying such data. Ordinal data are modeled using the BOS distribution, which is a model with two meaningful parameters referred to as \\\"position\\\" and \\\"precision\\\". The former indicates the mode of the distribution and the latter describes how scattered the data are around the mode: the user is able to easily interpret the distribution of their data when given these two parameters. The package is based on the co-clustering framework (when rows and columns are simultaneously clustered). The co-clustering approach uses the Latent Block Model (LBM) and the SEM-Gibbs algorithm for parameter inference. On the other hand, the clustering and the classification methods follow on from simplified versions of the SEM-Gibbs algorithm. For the classification process, two approaches are proposed. In the first one, the BOS parameters are estimated from the training dataset in the conventional way. In the second approach, parsimony is introduced by estimating the parameters and column-clusters from the training dataset. We empirically show that this approach can yield better results. For the clustering and co-clustering processes, the ICL-BIC criterion is used for model selection purposes. An overview of these methods is given, and the way to use them with the ordinalClust package is described using real datasets. The latest stable package version is available on the Comprehensive R Archive Network (CRAN).",
    "author": [
      {
        "name": "Margot Selosse",
        "url": {}
      },
      {
        "name": "Julien Jacques",
        "url": {}
      },
      {
        "name": "Christophe Biernacki",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-011.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-012/",
    "title": "KSPM: A Package For Kernel Semi-Parametric Models",
    "description": "Kernel semi-parametric models and their equivalence with linear mixed models provide analysts with the flexibility of machine learning methods and a foundation for inference and tests of hypothesis. These models are not impacted by the number of predictor variables, since the kernel trick transforms them to a kernel matrix whose size only depends on the number of subjects. Hence, methods based on this model are appealing and numerous, however only a few R programs are available and none includes a complete set of features. Here, we present the KSPM package to fit the kernel semi-parametric model and its extensions in a unified framework. KSPM allows multiple kernels and unlimited interactions in the same model. It also includes predictions, statistical tests, variable selection procedure and graphical tools for diagnostics and interpretation of variable effects. Currently KSPM is implemented for continuous dependent variables but could be extended to binary or survival outcomes.",
    "author": [
      {
        "name": "Catherine Schramm",
        "url": {}
      },
      {
        "name": "Sébastien Jacquemont",
        "url": {}
      },
      {
        "name": "Karim Oualkacha",
        "url": {}
      },
      {
        "name": "Aurélie Labbe",
        "url": {}
      },
      {
        "name": "Celia M.T. Greenwood",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-012.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-013/",
    "title": "AQuadtree: an R Package for Quadtree Anonymization of Point Data",
    "description": "The demand for precise data for analytical purposes grows rapidly among the research community and decision makers as more geographic information is being collected. Laws protecting data privacy are being enforced to prevent data disclosure. Statistical institutes and agencies need methods to preserve confidentiality while maintaining accuracy when disclosing geographic data. In this paper we present the AQuadtree package, a software intended to produce and deal with official spatial data making data privacy and accuracy compatible. The lack of specific methods in *R* to anonymize spatial data motivated the development of this package, providing an automatic aggregation tool to anonymize point data. We propose a methodology based on hierarchical geographic data structures to create a varying size grid adapted to local area population densities. This article gives insights and hints for implementation and usage. We hope this new tool may be helpful for statistical offices and users of official spatial data.",
    "author": [
      {
        "name": "Raymond Lagonigro",
        "url": {}
      },
      {
        "name": "Ramon Oller",
        "url": {}
      },
      {
        "name": "Joan Carles Martori",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-013.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2021-014/",
    "title": "miWQS: Multiple Imputation Using Weighted Quantile Sum Regression",
    "description": "The miWQS package in the Comprehensive R Archive Network (CRAN) utilizes weighted quantile sum regression (WQS) in the multiple imputation (MI) framework. The data analyzed is a set/mixture of continuous and correlated components/chemicals that are reasonable to combine in an index and share a common outcome. These components are also interval-censored between zero and upper thresholds, or detection limits, which may differ among the components. This type of data is found in areas such as chemical epidemiological studies, sociology, and genomics. The miWQS package can be run using complete or incomplete data, which may be placed in the first quantile, or imputed using bootstrap or Bayesian approach. This article provides a stepwise and hands-on approach to handle uncertainty due to values below the detection limit in correlated component mixture problems.",
    "author": [
      {
        "name": "Paul M. Hargarten",
        "url": {}
      },
      {
        "name": "David C. Wheeler",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2021-014.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:34+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-027/",
    "title": "spinifex: An R Package for Creating a Manual Tour of Low-dimensional Projections of Multivariate Data",
    "description": "Dynamic low-dimensional linear projections of multivariate data collectively known as tours provide an important tool for exploring multivariate data and models. The R package tourr provides functions for several types of tours: grand, guided, little, local and frozen. Each of these can be viewed dynamically, or saved into a data object for animation. This paper describes a new package, spinifex, which provides a manual tour of multivariate data where the projection coefficient of a single variable is controlled. The variable is rotated fully into the projection, or completely out of the projection. The resulting sequence of projections can be displayed as an animation, with functions from either the plotly or gganimate packages. By varying the coefficient of a single variable, it is possible to explore the sensitivity of structure in the projection to that variable. This is particularly useful when used with a projection pursuit guided tour to simplify and understand the solution. The use of the manual tour is applied particle physics data to illustrate the sensitivity of structure in a projection to specific variable contributions.",
    "author": [
      {
        "name": "Nicholas Spyrison",
        "url": {}
      },
      {
        "name": "Dianne Cook",
        "url": {}
      }
    ],
    "date": "2020-09-13",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-027.zip\nCRAN packages used\ntourr, spinifex, plotly, gganimate, ggplot2, shiny, knitr, rmarkdown\nCRAN Task Views implied by cited packages\nTeachingStatistics, ReproducibleResearch, WebTechnologies, Graphics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-007/",
    "title": "The Rockerverse: Packages and Applications for Containerisation with R",
    "description": "The Rocker Project provides widely used Docker images for R across different application scenarios. This article surveys downstream projects that build upon the Rocker Project images and presents the current state of R packages for managing Docker images and controlling containers. These use cases cover diverse topics such as package development, reproducible research, collaborative work, cloud-based data processing, and production deployment of services. The variety of applications demonstrates the power of the Rocker Project specifically and containerisation in general. Across the diverse ways to use containers, we identified common themes: reproducible environments, scalability and efficiency, and portability across clouds. We conclude that the current growth and diversification of use cases is likely to continue its positive impact, but see the need for consolidating the Rockerverse ecosystem of packages, developing common practices for applications, and exploring alternative containerisation software.",
    "author": [
      {
        "name": "Daniel Nüst",
        "url": {}
      },
      {
        "name": "Dirk Eddelbuettel",
        "url": {}
      },
      {
        "name": "Dom Bennett",
        "url": {}
      },
      {
        "name": "Robrecht Cannoodt",
        "url": {}
      },
      {
        "name": "Dav Clark",
        "url": {}
      },
      {
        "name": "Gergely Daróczi",
        "url": {}
      },
      {
        "name": "Mark Edmondson",
        "url": {}
      },
      {
        "name": "Colin Fay",
        "url": {}
      },
      {
        "name": "Ellis Hughes",
        "url": {}
      },
      {
        "name": "Lars Kjeldgaard",
        "url": {}
      },
      {
        "name": "Sean Lopp",
        "url": {}
      },
      {
        "name": "Ben Marwick",
        "url": {}
      },
      {
        "name": "Heather Nolis",
        "url": {}
      },
      {
        "name": "Jacqueline Nolis",
        "url": {}
      },
      {
        "name": "Hong Ooi",
        "url": {}
      },
      {
        "name": "Karthik Ram",
        "url": {}
      },
      {
        "name": "Noam Ross",
        "url": {}
      },
      {
        "name": "Lori Shepherd",
        "url": {}
      },
      {
        "name": "Péter Sólymos",
        "url": {}
      },
      {
        "name": "Tyson Lee Swetnam",
        "url": {}
      },
      {
        "name": "Nitesh Turaga",
        "url": {}
      },
      {
        "name": "Charlotte Van Petegem",
        "url": {}
      },
      {
        "name": "Jason Williams",
        "url": {}
      },
      {
        "name": "Craig Willis",
        "url": {}
      },
      {
        "name": "Nan Xiao",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-007.zip\nCRAN packages used\nsys, stevedore, AzureContainers, googleCloudRunner, babelwhale, BiocManager, reticulate, Shiny, dockerfiler, dockr, liftr, checkpoint, renv, sf, rgdal, sanitizers, RSelenium, batchtools, googleComputeEngineR, future, plumber, drake, golem, analogsea, Rserve, svSocket, keras, DBI, dbplyr, dplyr, testthat, tinytest, ttdo, diffobj\nCRAN Task Views implied by cited packages\nModelDeployment, WebTechnologies, HighPerformanceComputing, ReproducibleResearch, Databases, NumericalMathematics, Spatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-008/",
    "title": "Linear Fractional Stable Motion with the rlfsm R Package",
    "description": "Linear fractional stable motion is a type of a stochastic integral driven by symmetric alpha-stable Lévy motion. The integral could be considered as a non-Gaussian analogue of the fractional Brownian motion. The present paper discusses R package rlfsm created for numerical procedures with the linear fractional stable motion. It is a set of tools for simulation of these processes as well as performing statistical inference and simulation studies on them. We introduce: tools that we developed to work with that type of motions as well as methods and ideas underlying them. Also we perform numerical experiments to show finite-sample behavior of certain estimators of the integral, and give an idea of how to envelope workflow related to the linear fractional stable motion in S4 classes and methods. Supplementary materials, including codes for numerical experiments, are available online. rlfsm could be found on CRAN and gitlab.",
    "author": [
      {
        "name": "Stepan Mazur",
        "url": {}
      },
      {
        "name": "Dmitry Otryakhin",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-008.zip\nCRAN packages used\nrlfsm, somebm, stabledist, stable, ggplot2\nCRAN Task Views implied by cited packages\nDistributions, Graphics, Phylogenetics, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-009/",
    "title": "ProjectManagement: an R Package for Managing Projects",
    "description": "Project management is an important body of knowledge and practices that comprises the planning, organisation and control of resources to achieve one or more pre-determined objectives. In this paper, we introduce ProjectManagement, a new R package that provides the necessary tools to manage projects in a broad sense, and illustrate its use by examples.",
    "author": [
      {
        "name": "Juan Carlos Gonçalves-Dosantos",
        "url": {}
      },
      {
        "name": "Ignacio García-Jurado",
        "url": {}
      },
      {
        "name": "Julián Costa",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-009.zip\nCRAN packages used\nPlotPrjNetworks, plan, ProjectManagement, triangle, plotly, igraph, kappalab, GameTheory, lpSolveAPI\nCRAN Task Views implied by cited packages\nOptimization, Distributions, gR, Graphics, Spatial, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-010/",
    "title": "gk: An R Package for the g-and-k and Generalised g-and-h Distributions",
    "description": "The g-and-k and (generalised) g-and-h distributions are flexible univariate distributions which can model highly skewed or heavy tailed data through only four parameters: location and scale, and two shape parameters influencing the skewness and kurtosis. These distributions have the unusual property that they are defined through their quantile function (inverse cumulative distribution function) and their density is unavailable in closed form, which makes parameter inference complicated. This paper presents the gk R package to work with these distributions. It provides the usual distribution functions and several algorithms for inference of independent identically distributed data, including the finite difference stochastic approximation method, which has not been used before for this problem.",
    "author": [
      {
        "name": "Dennis Prangle",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-010.zip\nCRAN packages used\ngk, microbenchmark, abc, EasyABC, Ecdat\nCRAN Task Views implied by cited packages\nBayesian, Distributions, Econometrics, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-011/",
    "title": "Tools for Analyzing R Code the Tidy Way",
    "description": "With the current emphasis on reproducibility and replicability, there is an increasing need to examine how data analyses are conducted. In order to analyze the between researcher variability in data analysis choices as well as the aspects within the data analysis pipeline that contribute to the variability in results, we have created two R packages: matahari and tidycode. These packages build on methods created for natural language processing; rather than allowing for the processing of natural language, we focus on R code as the substrate of interest. The matahari package facilitates the logging of everything that is typed in the R console or in an R script in a tidy data frame. The tidycode package contains tools to allow for analyzing R calls in a tidy manner. We demonstrate the utility of these packages as well as walk through two examples.",
    "author": [
      {
        "name": "Lucy D’Agostino McGowan",
        "url": {}
      },
      {
        "name": "Sean Kross",
        "url": {}
      },
      {
        "name": "Jeffrey Leek",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-011.zip\nCRAN packages used\nmatahari, tidycode, tidyverse, tidytext, dplyr, purrr, wordcloud, data.table, gh\nCRAN Task Views implied by cited packages\nNaturalLanguageProcessing, Databases, Finance, HighPerformanceComputing, ModelDeployment, TimeSeries, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-012/",
    "title": "rcosmo: R Package for Analysis of Spherical, HEALPix and Cosmological Data",
    "description": "The analysis of spatial observations on a sphere is important in areas such as geosciences, physics and embryo research, just to name a few. The purpose of the package rcosmo is to conduct efficient information processing, visualisation, manipulation and spatial statistical analysis of Cosmic Microwave Background (CMB) radiation and other spherical data. The package was developed for spherical data stored in the Hierarchical Equal Area isoLatitude Pixelation (Healpix) representation. rcosmo has more than 100 different functions. Most of them initially were developed for CMB, but also can be used for other spherical data as rcosmo contains tools for transforming spherical data in cartesian and geographic coordinates into the HEALPix representation. We give a general description of the package and illustrate some important functionalities and benchmarks.",
    "author": [
      {
        "name": "Daniel Fryer",
        "url": {}
      },
      {
        "name": "Ming Li",
        "url": {}
      },
      {
        "name": "Andriy Olenko",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-012.zip\nCRAN packages used\nsp, sphereplot, rgl, geosphere, SpherWave, SphericalCubature, RandomFields, geoR, Directional, gensphere, CircNNTSR, VecStatGraphs3D, sm, cosmoFns, CRAC, FITSio, spider, astro, rcosmo, microbenchmark\nCRAN Task Views implied by cited packages\nSpatial, ChemPhys, SpatioTemporal, Bayesian, Distributions, Graphics, Multivariate, NumericalMathematics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-013/",
    "title": "Variable Importance Plots—An Introduction to the vip Package",
    "description": "In the era of “big data”, it is becoming more of a challenge to not only build state-of-the-art by Brandon M. Greenwell, Bradley C. Boehmke Introduction to the vip Package Variable Importance Plots—An",
    "author": [
      {
        "name": "Brandon M. Greenwell",
        "url": {}
      },
      {
        "name": "Bradley C. Boehmke",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\niml, R6, foreach, ingredients, DALEX, mmpf, varImp, party, measures, vita, rfVarImpOOB, randomForestExplainer, tree.interpreter, pkgsearch, caret, mlr, ranger, vip, ggplot2, partykit, earth, nnet, vivo, pdp, microbenchmark, iBreakDown, fastshap, xgboost, ALEPlot, DT, mlr3, data.table, AmesHousing, SuperLearner, glmnet, kernlab, plyr, doParallel\nCRAN Task Views implied by cited packages\nMachineLearning, HighPerformanceComputing, Multivariate, Survival, Environmetrics, TeachingStatistics, Cluster, Econometrics, Finance, Graphics, ModelDeployment, NaturalLanguageProcessing, Optimization, Phylogenetics, ReproducibleResearch, SocialSciences, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-014/",
    "title": "difNLR: Generalized Logistic Regression Models for DIF and DDF Detection",
    "description": "Differential item functioning (DIF) and differential distractor functioning (DDF) are impor tant topics in psychometrics, pointing to potential unfairness in items with respect to minorities or different social groups. Various methods have been proposed to detect these issues. The difNLR R package extends DIF methods currently provided in other packages by offering approaches based on generalized logistic regression models that account for possible guessing or inattention, and by pro viding methods to detect DIF and DDF among ordinal and nominal data. In the current paper, we describe implementation of the main functions of the difNLR package, from data generation, through the model fitting and hypothesis testing, to graphical representation of the results. Finally, we provide a real data example to bring the concepts together.",
    "author": [
      {
        "name": "Adéla Hladká",
        "url": {}
      },
      {
        "name": "Patrícia Martinková",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-014.zip\nCRAN packages used\ndifR, DIFlasso, DIFboost, GDINA, mirt, lordif, psychotree, difNLR, ShinyItemAnalysis, ggplot2, stats, VGAM, nnet\nCRAN Task Views implied by cited packages\nPsychometrics, Econometrics, SocialSciences, Distributions, Environmetrics, ExtremeValue, Graphics, MachineLearning, MissingData, Multivariate, Phylogenetics, Survival, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-015/",
    "title": "The R package NonProbEst for estimation in non-probability surveys",
    "description": "Different inference procedures are proposed in the literature to correct selection bias that might be introduced with non-random sampling mechanisms. The R package NonProbEst enables the estimation of parameters using some of these techniques to correct selection bias in non-probability surveys. The mean and the total of the target variable are estimated using Propensity Score Adjustment, calibration, statistical matching, model-based, model-assisted and model-calibratated techniques. Confidence intervals can also obtained for each method. Machine learning algorithms can be used for estimating the propensities or for predicting the unknown values of the target variable for the non-sampled units. Variance of a given estimator is performed by two different Leave-One-Out jackknife procedures. The functionality of the package is illustrated with example data sets.",
    "author": [
      {
        "name": "M. Rueda",
        "url": {}
      },
      {
        "name": "R. Ferri-García",
        "url": {}
      },
      {
        "name": "L. Castro",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-015.zip\nCRAN packages used\nNonProbEst, caret, sampling, survey\nCRAN Task Views implied by cited packages\nOfficialStatistics, HighPerformanceComputing, MachineLearning, Multivariate, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-016/",
    "title": "NlinTS: An R Package For Causality Detection in Time Series",
    "description": "The causality is an important concept that is widely studied in the literature, and has several applications, especially when modelling dependencies within complex data, such as multivariate time series. In this article, we present a theoretical description of methods from the NlinTS package, and we focus on causality measures. The package contains the classical Granger causality test. To handle non-linear time series, we propose an extension of this test using an artificial neural network. The package includes an implementation of the Transfer entropy, which is also considered as a non linear causality measure based on information theory. For discrete variables, we use the classical Shannon Transfer entropy, while for continuous variables, we adopt the k-nearest neighbors approach to estimate it.",
    "author": [
      {
        "name": "Youssef Hmamouche",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-016.zip\nCRAN packages used\nNlinTS, vars, lmtest, RTransferEntropy, timeSeries, Rcpp\nCRAN Task Views implied by cited packages\nTimeSeries, Finance, Econometrics, HighPerformanceComputing, MissingData, NumericalMathematics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-017/",
    "title": "SimilaR: R Code Clone and Plagiarism Detection",
    "description": "Third-party software for assuring source code quality is becoming increasingly popular. Tools that evaluate the coverage of unit tests, perform static code analysis, or inspect run-time memory use are crucial in the software development life cycle. More sophisticated methods allow for performing meta-analyses of large software repositories, e.g., to discover abstract topics they relate to or common design patterns applied by their developers. They may be useful in gaining a better understanding of the component interdependencies, avoiding cloned code as well as detecting plagiarism in programming classes. A meaningful measure of similarity of computer programs often forms the basis of such tools. While there are a few noteworthy instruments for similarity assessment, none of them turns out particularly suitable for analysing R code chunks. Existing solutions rely on rather simple techniques and heuristics and fail to provide a user with the kind of sensitivity and specificity required for working with R scripts. In order to fill this gap, we propose a new algorithm based on a Program Dependence Graph, implemented in the SimilaR package. It can serve as a tool not only for improving R code quality but also for detecting plagiarism, even when it has been masked by applying some obfuscation techniques or imputing dead code. We demonstrate its accuracy and efficiency in a real-world case study.",
    "author": [
      {
        "name": "Maciej Bartoszuk",
        "url": {}
      },
      {
        "name": "Marek Gagolewski",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-017.zip\nCRAN packages used\nmagrittr, SimilaR, nortest, DescTools\nCRAN Task Views implied by cited packages\nMissingData, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-018/",
    "title": "SurvBoost: An R Package for High-Dimensional Variable Selection in the Stratified Proportional Hazards Model via Gradient Boosting",
    "description": "High-dimensional variable selection in the proportional hazards (PH) model has many successful applications in different areas. In practice, data may involve confounding variables that do not satisfy the PH assumption, in which case the stratified proportional hazards (SPH) model can be adopted to control the confounding effects by stratification without directly modeling the confounding effects. However, there is a lack of computationally efficient statistical software for high-dimensional variable selection in the SPH model. In this work an R package, SurvBoost, is developed to implement the gradient boosting algorithm for fitting the SPH model with high-dimensional covariate variables. Simulation studies demonstrate that in many scenarios SurvBoost can achieve better selection accuracy and reduce computational time substantially compared to the existing R package that implements boosting algorithms without stratification. The proposed R package is also illustrated by an analysis of gene expression data with survival outcome in The Cancer Genome Atlas study. In addition, a detailed hands-on tutorial for SurvBoost is provided.",
    "author": [
      {
        "name": "Emily Morris",
        "url": {}
      },
      {
        "name": "Kevin He",
        "url": {}
      },
      {
        "name": "Yanming Li",
        "url": {}
      },
      {
        "name": "Yi Li",
        "url": {}
      },
      {
        "name": "Jian Kang",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-018.zip\nCRAN packages used\nmboost, survival, Rcpp, RcppArmadillo, RcppParallel\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NumericalMathematics, Survival, ClinicalTrials, Econometrics, MachineLearning, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-019/",
    "title": "Skew-t Expected Information Matrix Evaluation and Use for Standard Error Calculations",
    "description": "Skew-t distributions derived from skew-normal distributions, as developed by Azzalini and several co-workers, are popular because of their theoretical foundation and the availability of computational methods in the R package sn. One difficulty with this skew-t family is that the elements of the expected information matrix do not have closed form analytic formulas. Thus, we developed a numerical integration method of computing the expected information matrix in the R package skewtInfo. The accuracy of our expected information matrix calculation method was confirmed by comparing the result with that obtained using an observed information matrix for a very large sample size. A Monte Carlo study to evaluate the accuracy of the standard errors obtained with our expected information matrix calculation method, for the case of three realistic skew-t parameter vectors, indicates that use of the expected information matrix results in standard errors as accurate as, and sometimes a little more accurate than, use of an observed information matrix.",
    "author": [
      {
        "name": "R. Douglas Martin",
        "url": {}
      },
      {
        "name": "Chindhanai Uthaisaad",
        "url": {}
      },
      {
        "name": "Daniel Z. Xia",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-019.zip\nCRAN packages used\nsn\nCRAN Task Views implied by cited packages\nDistributions, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-020/",
    "title": " Individual-Level Modelling of Infectious Disease Data: EpiILM",
    "description": "In this article we introduce the R package EpiILM, which provides tools for simulation from, and inference for, discrete-time individual-level models of infectious disease transmission proposed by Deardon et al. (2010). The inference is set in a Bayesian framework and is carried out via Metropolis Hastings Markov chain Monte Carlo (MCMC). For its fast implementation, key functions are coded in Fortran. Both spatial and contact network models are implemented in the package and can be set in either susceptible-infected (SI) or susceptible-infected-removed (SIR) compartmental frameworks. Use of the package is demonstrated through examples involving both simulated and real data.",
    "author": [
      {
        "name": "Vineetha Warriyar K. V.",
        "url": {}
      },
      {
        "name": "Waleed Almutiry",
        "url": {}
      },
      {
        "name": "Rob Deardon",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-020.zip\nCRAN packages used\nR0, EpiEstim, EpiModel, epinet, surveillance, EpiILM, igraph, ergm, adaptMCMC, coda\nCRAN Task Views implied by cited packages\ngR, Bayesian, Environmetrics, Graphics, Optimization, SocialSciences, Spatial, SpatioTemporal, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-021/",
    "title": "tsmp: An R Package for Time Series with Matrix Profile",
    "description": "This article describes tsmp, an R package that implements the MP concept for TS. The tsmp package is a toolkit that allows all-pairs similarity joins, motif, discords and chains discovery, semantic segmentation, etc. Here we describe how the tsmp package may be used by showing some of the use-cases from the original articles and evaluate the algorithm speed in the R environment. This package can be downloaded at https://CRAN.R-project.org/package=tsmp.",
    "author": [
      {
        "name": "Francisco Bischoff",
        "url": {}
      },
      {
        "name": "Pedro Pereira Rodrigues",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-021.zip\nCRAN packages used\ntsmp\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-022/",
    "title": "npordtests: An R Package of Nonparametric Tests for Equality of Location Against Ordered Alternatives",
    "description": "Ordered alternatives are an important statistical problem in many situation such as increased risk of congenital malformation caused by excessive alcohol consumption during pregnancy life test experiments, drug-screening studies, dose-finding studies, the dose-response studies, age-related response. There are numerous other examples of this nature. In this paper, we present the npordtests package to test the equality of locations for ordered alternatives. The package includes the Jonckheere Terpstra, Beier and Buning’s Adaptive, Modified Jonckheere-Terpstra, Terpstra-Magel, Ferdhiana Terpstra-Magel, KTP, S and Gaur’s Gc tests. A simulation study is conducted to determine which test is the most appropriate test for which scenario and to suggest it to the researchers.",
    "author": [
      {
        "name": "Bulent Altunkaynak",
        "url": {}
      },
      {
        "name": "Hamza Gamgam",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-022.zip\nCRAN packages used\nclinfun, jtGWAS, fastJT, kSamples, StatCharrms, PMCMRplus, npordtests\nCRAN Task Views implied by cited packages\nClinicalTrials, Environmetrics, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-023/",
    "title": "ari: The Automated R Instructor",
    "description": "We present the ari package for automatically generating technology-focused educational videos. The goal of the package is to create reproducible videos, with the ability to change and update video content seamlessly. We present several examples of generating videos including using R Markdown slide decks, PowerPoint slides, or simple images as source material. We also discuss how ari can help instructors reach new audiences through programmatically translating materials into other languages.",
    "author": [
      {
        "name": "Sean Kross",
        "url": {}
      },
      {
        "name": "Jeffrey T. Leek",
        "url": {}
      },
      {
        "name": "John Muschelli",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-023.zip\nCRAN packages used\nari, text2speech, aws.polly, tuneR, ariExtra, animation, aws.signature, rmarkdown, xaringan, webshot, rgoogleslides, readOffice, officer, pdftools, docxtractr, mscstts, googleLanguageR\nCRAN Task Views implied by cited packages\nReproducibleResearch, WebTechnologies, Graphics, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-024/",
    "title": "CoxPhLb: An R Package for Analyzing Length Biased Data under Cox Model",
    "description": "Data subject to length-biased sampling are frequently encountered in various applications including prevalent cohort studies and are considered as a special case of left-truncated data under the stationarity assumption. Many semiparametric regression methods have been proposed for length biased data to model the association between covariates and the survival outcome of interest. In this paper, we present a brief review of the statistical methodologies established for the analysis of length-biased data under the Cox model, which is the most commonly adopted semiparametric model, and introduce an R package CoxPhLb that implements these methods. Specifically, the package includes features such as fitting the Cox model to explore covariate effects on survival times and checking the proportional hazards model assumptions and the stationarity assumption. We illustrate usage of the package with a simulated data example and a real dataset, the Channing House data, which are publicly available.",
    "author": [
      {
        "name": "Chi Hyun Lee",
        "url": {}
      },
      {
        "name": "Heng Zhou",
        "url": {}
      },
      {
        "name": "Jing Ning",
        "url": {}
      },
      {
        "name": "Diane D. Liu",
        "url": {}
      },
      {
        "name": "Yu Shen",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-024.zip\nCRAN packages used\nCoxPhLb, survival, KMsurv, coxphw\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Econometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-025/",
    "title": "CopulaCenR: Copula based Regression Models for Bivariate Censored Data in R",
    "description": "Bivariate time-to-event data frequently arise in research areas such as clinical trials and epidemiological studies, where the occurrence of two events are correlated. In many cases, the exact event times are unknown due to censoring. The copula model is a popular approach for modeling correlated bivariate censored data, in which the two marginal distributions and the between margin dependence are modeled separately. This article presents the R package CopulaCenR, which is designed for modeling and testing bivariate data under right or (general) interval censoring in a regression setting. It provides a variety of Archimedean copula functions including a flexible two-parameter copula and different types of regression models (parametric and semiparametric) for marginal distributions. In particular, it implements a semiparametric transformation model for the margins with proportional hazards and proportional odds models being its special cases. The numerical optimization is based on a novel two-step algorithm. For the regression parameters, three likelihood-based tests (Wald, generalized score and likelihood ratio tests) are also provided. We use two real data examples to illustrate the key functions in CopulaCenR.",
    "author": [
      {
        "name": "Tao Sun",
        "url": {}
      },
      {
        "name": "Ying Ding",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-025.zip\nCRAN packages used\nCopulaCenR, survival, parfm, frailtypack, coxme, phmm, copula, VineCopula, CopulaRegression, gcmr, gamCopula, Copula.surv, Sunclarco, GJRM\nCRAN Task Views implied by cited packages\nSurvival, Distributions, ClinicalTrials, Econometrics, ExtremeValue, Finance, Multivariate, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-026/",
    "title": "BayesMallows: An R Package for the Bayesian Mallows Model",
    "description": "BayesMallows is an R package for analyzing preference data in the form of rankings with the Mallows rank model, and its finite mixture extension, in a Bayesian framework. The model is grounded on the idea that the probability density of an observed ranking decreases exponentially with the distance to the location parameter. It is the first Bayesian implementation that allows wide choices of distances, and it works well with a large amount of items to be ranked. BayesMallows handles non-standard data: partial rankings and pairwise comparisons, even in cases including non-transitive preference patterns. The Bayesian paradigm allows coherent quantification of posterior uncertainties of estimates of any quantity of interest. These posteriors are fully available to the user, and the package comes with convienient tools for summarizing and visualizing the posterior distributions.",
    "author": [
      {
        "name": "Øystein Sørensen",
        "url": {}
      },
      {
        "name": "Marta Crispino",
        "url": {}
      },
      {
        "name": "Qinghua Liu",
        "url": {}
      },
      {
        "name": "Valeria Vitelli",
        "url": {}
      }
    ],
    "date": "2020-09-10",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-026.zip\nCRAN packages used\nBayesMallows, PerMallows, pmr, rankdist, microbenchmark, dplyr, parallel, tidyr, label.switching\nCRAN Task Views implied by cited packages\nDatabases, ModelDeployment\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-028/",
    "title": "S, R, and Data Science",
    "description": "Data science is increasingly important and challenging. It requires computational tools and programming environments that handle big data and difficult computations, while supporting creative, high-quality analysis. The R language and related software play a major role in computing for data science. R is featured in most programs for training in the field. R packages provide tools for a wide range of purposes and users. The description of a new technique, particularly from research in statistics, is frequently accompanied by an R package, greatly increasing the usefulness of the description. The history of R makes clear its connection to data science. R was consciously designed to replicate in open-source software the contents of the S software. S in turn was written by data analysis researchers at Bell Labs as part of the computing environment for research in data analysis and collaborations to apply that research, rather than as a separate project to create a programming language. The features of S and the design decisions made for it need to be understood in this broader context of supporting effective data analysis (which would now be called data science). These characteristics were all transferred to R and remain central to its effectiveness. Thus, R can be viewed as based historically on a domain-specific language for the domain of data science. Note to R Journal readers: The following paper was published online in the History of Programming Languages (HOPL), Volume 4, in June 2020 (DOI 10.1145/3386334). The content seems likely to be of interest to many R Journal readers, and since HOPL is plausibly not typical reading for data scientists, the editors of the R Journal have kindly offered to republish the paper here. This is possible thanks also to the enlightened policy of the ACM, providing for open distribution through the chosen copyright declaration.",
    "author": [
      {
        "name": "John M. Chambers",
        "url": {}
      }
    ],
    "date": "2020-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-029/",
    "title": "Provenance of R’s Gradient Optimizers",
    "description": "Gradient optimization methods (function minimizers) are well-represented in both the base and package universe of R (R Core Team, 2019). However, some of the methods and the codes developed from them were published before standards for hardware and software were established, in particular the IEEE arithmetic (IEEE, 1985). There have been cases of unexpected behaviour or outright errors, and these are the focus of the histoRicalg project. A summary history of some of the tools in R for gradient optimization methods is presented to give perspective on such methods and the occasions where they could be used effectively.",
    "author": [
      {
        "name": "John C. Nash",
        "url": {}
      }
    ],
    "date": "2020-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-001/",
    "title": "Mapping Smoothed Spatial Effect Estimates from Individual-Level Data: MapGAM ",
    "description": "We introduce and illustrate the utility of MapGAM, a user-friendly R package that provides a unified framework for estimating, predicting and drawing inference on covariate-adjusted spatial effects using individual-level data. The package also facilitates visualization of spatial effects via automated mapping procedures. MapGAM estimates covariate-adjusted spatial associations with a univariate or survival outcome using generalized additive models that include a non-parametric bivariate smooth term of geolocation parameters. Estimation and mapping methods are implemented for continuous, discrete, and right-censored survival data. In the current manuscript, we summarize the methodology implemented in MapGAM and illustrate the package using two example simulated datasets: the first considering a case-control study design from the state of Massachusetts and the second considering right-censored survival data from California.",
    "author": [
      {
        "name": "Lu Bai",
        "url": {}
      },
      {
        "name": "Daniel L. Gillen",
        "url": {}
      },
      {
        "name": "Scott M. Bartell",
        "url": {}
      },
      {
        "name": "Verónica M. Vieira",
        "url": {}
      }
    ],
    "date": "2020-03-31",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-001.zip\nCRAN packages used\nMapGAM\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-002/",
    "title": "mudfold: An R Package for Nonparametric IRT Modelling of Unfolding Processes",
    "description": "Item response theory (IRT) models for unfolding processes use the responses of individuals to attitudinal tests or questionnaires in order to infer item and person parameters located on a latent continuum. Parametric models in this class use parametric functions to model the response process, which in practice can be restrictive. MUDFOLD (Multiple UniDimensional unFOLDing) can be used to obtain estimates of person and item ranks without imposing strict parametric assumptions on the item response functions (IRFs). This paper describes the implementation of the MUDFOLD method for binary preferential-choice data in the R package mudfold. The latter incorporates estimation, visualization, and simulation methods in order to provide R users with utilities for nonparametric analysis of attitudinal questionnaire data. After a brief introduction in IRT, we provide the method ological framework implemented in the package. A description of the available functions is followed by practical examples and suggestions on how this method can be used even outside the field of psychometrics.",
    "author": [
      {
        "name": "Spyros E. Balafas",
        "url": {}
      },
      {
        "name": "Wim P. Krijnen",
        "url": {}
      },
      {
        "name": "Wendy J. Post",
        "url": {}
      },
      {
        "name": "Ernst C. Wit",
        "url": {}
      }
    ],
    "date": "2020-03-31",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-002.zip\nCRAN packages used\nmudfold, GGUM, mirt, mokken, boot, mice, gtools, glmnet, mgcv, zoo, reshape2, ggplot2, smacof\nCRAN Task Views implied by cited packages\nPsychometrics, Econometrics, MissingData, SocialSciences, Environmetrics, Survival, TimeSeries, Bayesian, Finance, Graphics, MachineLearning, Multivariate, OfficialStatistics, Optimization, Phylogenetics, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-005/",
    "title": "lspartition: Partitioning-Based Least Squares Regression",
    "description": "Nonparametric partitioning-based least squares regression is an important tool in empirical work. Common examples include regressions based on splines, wavelets, and piecewise polynomials. This article discusses the main methodological and numerical features of the R software package lspartition, which implements results for partitioning-based least squares (series) regression estimation and inference from Cattaneo and Farrell (2013) and Cattaneo, Farrell, and Feng (2020). These results cover the multivariate regression function as well as its derivatives. First, the package provides data-driven methods to choose the number of partition knots optimally, according to integrated mean squared error, yielding optimal point estimation. Second, robust bias correction is implemented to combine this point estimator with valid inference. Third, the package provides estimates and inference for the unknown function both pointwise and uniformly in the conditioning variables. In particular, valid confidence bands are provided. Finally, an extension to two-sample analysis is developed, which can be used in treatment-control comparisons and related problems.",
    "author": [
      {
        "name": "Matias D. Cattaneo",
        "url": {}
      },
      {
        "name": "Max H. Farrell",
        "url": {}
      },
      {
        "name": "Yingjie Feng",
        "url": {}
      }
    ],
    "date": "2020-03-31",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-005.zip\nCRAN packages used\nggplot2\nCRAN Task Views implied by cited packages\nGraphics, Phylogenetics, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-006/",
    "title": "SortedEffects: Sorted Causal Effects in R",
    "description": "Chernozhukov et al. (2018) proposed the sorted effect method for nonlinear regression models. This method consists of reporting percentiles of the partial effects, the sorted effects, in addition to the average effect commonly used to summarize the heterogeneity in the partial effects. They also propose to use the sorted effects to carry out classification analysis where the observational units are classified as most and least affected if their partial effect are above or below some tail sorted effects. The R package SortedEffects implements the estimation and inference methods therein and provides tools to visualize the results. This vignette serves as an introduction to the package and displays basic functionality of the functions within.",
    "author": [
      {
        "name": "Shuowen Chen",
        "url": {}
      },
      {
        "name": "Victor Chernozhukov",
        "url": {}
      },
      {
        "name": "Iván Fernández-Val",
        "url": {}
      },
      {
        "name": "Ye Luo",
        "url": {}
      }
    ],
    "date": "2020-03-31",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-006.zip\nCRAN packages used\nSortedEffects, SortedEffect, quantreg, margins, parallel, boot\nCRAN Task Views implied by cited packages\nEconometrics, Optimization, SocialSciences, Survival, Environmetrics, ReproducibleResearch, Robust, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-054/",
    "title": "The R Package trafo for Transforming Linear Regression Models",
    "description": "Researchers and data-analysts often use the linear regression model for descriptive, predictive, and inferential purposes. This model relies on a set of assumptions that, when not satisfied, yields biased results and noisy estimates. A common problem that can be solved in many ways – use of less restrictive methods (e.g. generalized linear regression models or non-parametric methods ), variance corrections or transformations of the response variable just to name a few. We focus on the latter option as it allows to keep using the simple and well-known linear regression model. The list of transformations proposed in the literature is long and varies according to the problem they aim to solve. Such diversity can leave analysts lost and confused. We provide a framework implemented as an R-package, trafo, to help select suitable transformations depending on the user requirements and data being analyzed. The package trafo contains a collection of selected transformations and estimation methods that complement and increase the breadth of methods that exist in R.",
    "author": [
      {
        "name": "Lily Medina",
        "url": {}
      },
      {
        "name": "Ann-Kristin Kreutzmann",
        "url": {}
      },
      {
        "name": "Natalia Rojas-Perilla",
        "url": {}
      },
      {
        "name": "Piedad Castro",
        "url": {}
      }
    ],
    "date": "2020-01-06",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-054.zip\nCRAN packages used\ntrafo, car, rcompanio, bestNormalize, caret, Johnson, jtrans, MASS, AID, Ecdat\nCRAN Task Views implied by cited packages\nEconometrics, Multivariate, SocialSciences, TeachingStatistics, Distributions, Environmetrics, Finance, HighPerformanceComputing, MachineLearning, NumericalMathematics, Psychometrics, Robust, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-055/",
    "title": "BondValuation: An R Package for Fixed Coupon Bond Analysis",
    "description": "The purpose of this paper is to introduce the R package BondValuation for the analysis of large datasets of fixed coupon bonds. The conceptual heterogeneity of fixed coupon bonds traded in the global markets imposes a high degree of complexity on their comparative analysis. Contrary to baseline fixed income theory, in practice, most bonds feature coupon period irregularities. In addition, there are a multitude of day count methods that determine the interest accrual, the cash flows and the discount factors used in bond valuation. Several R packages, e.g., fBonds, RQuantLib, and YieldCurve, provide tools for fixed income analysis. Nevertheless, none of them is capable of evaluating bonds featuring irregular first and/or final coupon periods, and neither provides adequate coverage of day count conventions currently used in the global bond markets. The R package BondValuation closes this gap using the generalized valuation methodology presented in Djatschenko (2019).",
    "author": [
      {
        "name": "Wadim Djatschenko",
        "url": {}
      }
    ],
    "date": "2020-01-06",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-055.zip\nCRAN packages used\nBondValuation, fBonds, RQuantLib, YieldCurve\nCRAN Task Views implied by cited packages\nFinance\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-057/",
    "title": "HCmodelSets: An R Package for Specifying Sets of Well-fitting Models in High Dimensions",
    "description": "In the context of regression with a large number of explanatory variables, Cox and Battey (2017) emphasize that if there are alternative reasonable explanations of the data that are statistically indistinguishable, one should aim to specify as many of these explanations as is feasible. The standard practice, by contrast, is to report a single effective model for prediction. This paper illustrates the R implementation of the new ideas in the package HCmodelSets, using simple reproducible examples and real data. Results of some simulation experiments are also reported.",
    "author": [
      {
        "name": "Henrique Hoeltgebaum",
        "url": {}
      },
      {
        "name": "Heather Battey",
        "url": {}
      }
    ],
    "date": "2020-01-06",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-057.zip\nCRAN packages used\nHCmodelSets, glmnet, survival\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Econometrics, MachineLearning, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-053/",
    "title": "spGARCH: An R-Package for Spatial and Spatiotemporal ARCH and GARCH models",
    "description": "In this paper, a general overview on spatial and spatiotemporal ARCH models is provided. In particular, we distinguish between three different spatial ARCH-type models. In addition to the original definition of ?, we introduce an logarithmic spatial ARCH model in this paper. For this new model, maximum-likelihood estimators for the parameters are proposed. In addition, we consider a new complex-valued definition of the spatial ARCH process. Moreover, spatial GARCH models are briefly discussed. From a practical point of view, the use of the R-package spGARCH is demonstrated. To be precise, we show how the proposed spatial ARCH models can be simulated and summarize the variety of spatial models, which can be estimated by the estimation functions provided in the package. Eventually, we apply all procedures to a real-data example.",
    "author": [
      {
        "name": "Philipp Otto",
        "url": {}
      }
    ],
    "date": "2019-12-28",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspGARCH, spdep, gstat, Stem, rugarch, Rsolnp, Rcpp, RcppEigen\nCRAN Task Views implied by cited packages\nSpatial, NumericalMathematics, SpatioTemporal, Finance, HighPerformanceComputing, Optimization, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-044/",
    "title": "Fitting Tails by the Empirical Residual Coefficient of Variation: The ercv Package",
    "description": "This article is a self-contained introduction to the R package ercv and to the methodology on which it is based through the analysis of nine examples. The methodology is simple and trustworthy for the analysis of extreme values and relates the two main existing methodologies. The package contains R functions for visualizing, fitting and validating the distribution of tails. It also provides multiple threshold tests for a generalized Pareto distribution, together with an automatic threshold selection algorithm.",
    "author": [
      {
        "name": "Joan del Castillo",
        "url": {}
      },
      {
        "name": "Isabel Serra",
        "url": {}
      },
      {
        "name": "Maria Padilla",
        "url": {}
      },
      {
        "name": "David Moriña",
        "url": {}
      }
    ],
    "date": "2019-12-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-044.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-045/",
    "title": "biclustermd: An R Package for Biclustering with Missing Values",
    "description": "Biclustering is a statistical learning technique that attempts to find homogeneous partitions of rows and columns of a data matrix. For example, movie ratings might be biclustered to group both raters and movies. biclust is a current R package allowing users to implement a variety of biclustering algorithms. However, its algorithms do not allow the data matrix to have missing values. We provide a new R package, biclustermd, which allows users to perform biclustering on numeric data even in the presence of missing values.",
    "author": [
      {
        "name": "John Reisner",
        "url": {}
      },
      {
        "name": "Hieu Pham",
        "url": {}
      },
      {
        "name": "Sigurdur Olafsson",
        "url": {}
      },
      {
        "name": "Stephen Vardeman",
        "url": {}
      },
      {
        "name": "Jing Li",
        "url": {}
      }
    ],
    "date": "2019-12-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-045.zip\nCRAN packages used\nbiclust, superbiclust, s4vd, BiBitR, biclustermd, clues, nycflights13, tidyverse, ggplot2\nCRAN Task Views implied by cited packages\nGraphics, Cluster, Phylogenetics, TeachingStatistics\nBioconductor packages used\niBBiG, QUBIC\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-046/",
    "title": "PPCI: an R Package for Cluster Identification using Projection Pursuit",
    "description": "This paper presents the R package PPCI which implements three recently proposed projec tion pursuit methods for clustering. The methods are unified by the approach of defining an optimal hyperplane to separate clusters, and deriving a projection index whose optimiser is the vector normal to this separating hyperplane. Divisive hierarchical clustering algorithms that can detect clusters defined in different subspaces are readily obtained by recursively bi-partitioning the data through such hyperplanes. Projecting onto the vector normal to the optimal hyperplane enables visualisations of the data that can be used to validate the partition at each level of the cluster hierarchy. Clustering models can also be modified in an interactive manner to improve their solutions. Extensions to problems involving clusters which are not linearly separable, and to the problem of finding maximum hard margin hyperplanes for clustering are also discussed.",
    "author": [
      {
        "name": "David P. Hofmeyr",
        "url": {}
      },
      {
        "name": "Nicos G. Pavlidis",
        "url": {}
      }
    ],
    "date": "2019-12-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-046.zip\nCRAN packages used\nProjectionBasedClustering, subspace\nCRAN Task Views implied by cited packages\nCluster\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-048/",
    "title": "Associative Classification in R: arc, arulesCBA, and rCBA",
    "description": "Several methods for creating classifiers based on rules discovered via association rule mining have been proposed in the literature. These classifiers are called associative classifiers and the best-known algorithm is Classification Based on Associations (CBA). Interestingly, only very few implementations are available and, until recently, no implementation was available for R. Now, three packages provide CBA. This paper introduces associative classification, the CBA algorithm, and how it can be used in R. A comparison of the three packages is provided to give the potential user an idea about the advantages of each of the implementations. We also show how the packages are related to the existing infrastructure for association rule mining already available in R.",
    "author": [
      {
        "name": "Michael Hahsler",
        "url": {}
      },
      {
        "name": "Ian Johnson",
        "url": {}
      },
      {
        "name": "Tomáš Kliegr",
        "url": {}
      },
      {
        "name": "Jaroslav Kuchař",
        "url": {}
      }
    ],
    "date": "2019-12-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-048.zip\nCRAN packages used\narc, arulesCBA, rCBA, RWeka, arules, arulesViz, qcba, sbrl, RKEEL, discretization, Matrix, qCBA, matrix, rJAVA, mlbench, datasets\nCRAN Task Views implied by cited packages\nMachineLearning, ModelDeployment, Econometrics, Multivariate, NaturalLanguageProcessing, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-050/",
    "title": "Comparing namedCapture with other R packages for regular expressions",
    "description": "Regular expressions are powerful tools for manipulating non-tabular textual data. For many tasks (visualization, machine learning, etc), tables of numbers must be extracted from such data before processing by other R functions. We present the R package namedCapture, which facilitates such tasks by providing a new user-friendly syntax for defining regular expressions in R code. We begin by describing the history of regular expressions and their usage in R. We then describe the new features of the namedCapture package, and provide detailed comparisons with related R packages (rex, stringr, stringi, tidyr, rematch2, re2r).",
    "author": [
      {
        "name": "Toby Dylan Hocking",
        "url": {}
      }
    ],
    "date": "2019-12-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-050.zip\nCRAN packages used\nnamedCapture, rex, stringr, stringi, tidyr, rematch2, re2r, microbenchmark\nCRAN Task Views implied by cited packages\nNaturalLanguageProcessing\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-051/",
    "title": "Resampling-Based Analysis of Multivariate Data and Repeated Measures Designs with the R Package MANOVA.RM",
    "description": "Nonparametric statistical inference methods for a modern and robust analysis of longitudinal and multivariate data in factorial experiments are essential for research. While existing approaches that rely on specific distributional assumptions of the data (multivariate normality and/or equal covariance matrices) are implemented in statistical software packages, there is a need for user-friendly software that can be used for the analysis of data that do not fulfill the aforementioned assumptions and provide accurate p value and confidence interval estimates. Therefore, newly developed nonpara metric statistical methods based on bootstrapand permutation-approaches, which neither assume multivariate normality nor specific covariance matrices, have been implemented in the freely available R package MANOVA.RM. The package is equipped with a graphical user interface for plausible applications in academia and other educational purpose. Several motivating examples illustrate the application of the methods.",
    "author": [
      {
        "name": "Sarah Friedrich",
        "url": {}
      },
      {
        "name": "Frank Konietschke",
        "url": {}
      },
      {
        "name": "Markus Pauly",
        "url": {}
      }
    ],
    "date": "2019-12-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-051.zip\nCRAN packages used\nstats, npmv, nparLD, GFD, SCGLR, car, flip, ffmanova, MANOVA.RM, RGtk2, plotrix, HSAUR, ellipse, multcomp\nCRAN Task Views implied by cited packages\nGraphics, Multivariate, SocialSciences, ClinicalTrials, Econometrics, Finance, Survival, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-052/",
    "title": "lpirfs: An R Package to Estimate Impulse Response Functions by Local Projections",
    "description": "Impulse response analysis is a cornerstone in applied (macro-)econometrics. Estimating impulse response functions using local projections (LPs) has become an appealing alternative to the traditional structural vector autoregressive (SVAR) approach. Despite its growing popularity and applications, however, no R package yet exists that makes this method available. In this paper, I introduce lpirfs, a fast and flexible R package that provides a broad framework to compute and visualize impulse response functions using LPs for a variety of data sets.",
    "author": [
      {
        "name": "Philipp Adämmer",
        "url": {}
      }
    ],
    "date": "2019-12-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-052.zip\nCRAN packages used\nvars, lpirfs, Rcpp, plm\nCRAN Task Views implied by cited packages\nEconometrics, Finance, HighPerformanceComputing, NumericalMathematics, SpatioTemporal, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2020-003/",
    "title": "mistr: A Computational Framework for Mixture and Composite Distributions",
    "description": "Finite mixtures and composite distributions allow to model the probabilistic representation of data with more generality than simple distributions and are useful to consider in a wide range of applications. The R package mistr provides an extensible computational framework for creating, transforming, and evaluating these models, together with multiple methods for their visualization and description. In this paper we present the main computational framework of the package and illustrate its application. In addition, we provide and show functions for data modeling using two specific composite distributions as well as a numerical example where a composite distribution is estimated to describe the log-returns of selected stocks.",
    "author": [
      {
        "name": "Lukas Sablica",
        "url": {}
      },
      {
        "name": "Kurt Hornik",
        "url": {}
      }
    ],
    "date": "2019-12-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2020-003.zip\nCRAN packages used\nmistr, distr, CompLognormal, evmix, OpVar, ReIns, gendist, ggplot2, actuar, bbmle\nCRAN Task Views implied by cited packages\nDistributions, ExtremeValue, Finance, Graphics, Phylogenetics, Robust, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-042/",
    "title": "coxed: An R Package for Computing Duration-Based Quantities from the Cox Proportional Hazards Model",
    "description": "The Cox proportional hazards model is one of the most frequently used estimators in duration (survival) analysis. Because it is estimated using only the observed durations’ rank ordering, typical quantities of interest used to communicate results of the Cox model come from the hazard function (e.g., hazard ratios or percentage changes in the hazard rate). These quantities are substantively vague and difficult for many audiences of research to understand. We introduce a suite of methods in the R package coxed to address these problems. The package allows researchers to calculate duration-based quantities from Cox model results, such as the expected duration (or survival time) given covariate values and marginal changes in duration for a specified change in a covariate. These duration-based quantities often match better with researchers’ substantive interests and are easily understood by most readers. We describe the methods and illustrate use of the package.",
    "author": [
      {
        "name": "Jonathan Kropko",
        "url": {}
      },
      {
        "name": "Jeffrey J. Harden",
        "url": {}
      }
    ],
    "date": "2019-12-26",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-042.zip\nCRAN packages used\ncoxed\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-043/",
    "title": "The IDSpatialStats R Package: Quantifying Spatial Dependence of Infectious Disease Spread",
    "description": "Spatial statistics for infectious diseases are important because the spatial and temporal scale over which transmission operates determine the dynamics of disease spread. Many methods for quantifying the distribution and clustering of spatial point patterns have been developed (e.g. K function and pair correlation function) and are routinely applied to infectious disease case occurrence data. However, these methods do not explicitly account for overlapping chains of transmission and require knowledge of the underlying population distribution, which can be limiting when analyzing epidemic case occurrence data. Therefore, we developed two novel spatial statistics that account for these effects to estimate: 1) the mean of the spatial transmission kernel, and 2) the τ-statistic, a measure of global clustering based on pathogen subtype. We briefly introduce these statistics and show how to implement them using the IDSpatialStats R package.",
    "author": [
      {
        "name": "John R. Giles",
        "url": {}
      },
      {
        "name": "Henrik Salje",
        "url": {}
      },
      {
        "name": "Justin Lessler",
        "url": {}
      }
    ],
    "date": "2019-12-26",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-043.zip\nCRAN packages used\nlgcp, ppmlasso, spdep, ads, spatstat, splancs, IDSpatialStats, DCluster, SGCS, sparr\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-037/",
    "title": "Fixed Point Acceleration in R",
    "description": "A fixed point problem is one where we seek a vector, X, for a function, f, such that f(X) = X. The solution of many such problems can be accelerated by using a fixed point acceleration algorithm. With the release of the FixedPoint package there is now a number of algorithms available in R that can be used for accelerating the finding of a fixed point of a function. These algorithms include Newton acceleration, Aitken acceleration and Anderson acceleration as well as epsilon extrapolation methods and minimal polynomial methods. This paper demonstrates the use of fixed point accelerators in solving numerical mathematics problems using the algorithms of the FixedPoint package as well as the squarem method of the SQUAREM package.",
    "author": [
      {
        "name": "Stuart Baumann",
        "url": {}
      },
      {
        "name": "Margaryta Klymak",
        "url": {}
      }
    ],
    "date": "2019-08-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-037.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-038/",
    "title": "SemiCompRisks: An R Package for the Analysis of Independent and Cluster-correlated Semi-competing Risks Data",
    "description": "Semi-competing risks refer to the setting where primary scientific interest lies in estimation and inference with respect to a non-terminal event, the occurrence of which is subject to a terminal event. In this paper, we present the R package SemiCompRisks that provides functions to perform the analysis of independent/clustered semi-competing risks data under the illness-death multi-state model. The package allows the user to choose the specification for model components from a range of options giving users substantial flexibility, including: accelerated failure time or proportional hazards regression models; parametric or non-parametric specifications for baseline survival functions; parametric or non-parametric specifications for random effects distributions when the data are cluster correlated; and, a Markov or semi-Markov specification for terminal event following non-terminal event. While estimation is mainly performed within the Bayesian paradigm, the package also provides the maximum likelihood estimation for select parametric models. The package also includes functions for univariate survival analysis as complementary analysis tools.",
    "author": [
      {
        "name": "Danilo Alvares",
        "url": {}
      },
      {
        "name": "Sebastien Haneuse",
        "url": {}
      },
      {
        "name": "Catherine Lee",
        "url": {}
      },
      {
        "name": "Kyu Ha Lee",
        "url": {}
      }
    ],
    "date": "2019-08-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-038.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-039/",
    "title": "RSSampling: A Pioneering Package for Ranked Set Sampling ",
    "description": "Ranked set sampling (RSS) is an advanced data collection method when the exact mea surement of an observation is difficult and/or expensive used in a number of research areas, e.g., environment, bioinformatics, ecology, etc. In this method, random sets are drawn from a population and the units in sets are ranked with a ranking mechanism which is based on a visual inspection or a concomitant variable. Because of the importance of working with a good design and easy analysis, there is a need for a software tool which provides sampling designs and statistical inferences based on RSS and its modifications. This paper introduces an R package as a free and easy-to-use analysis tool for both sampling processes and statistical inferences based on RSS and its modified versions. For researchers, the RSSampling package provides a sample with RSS, extreme RSS, median RSS, percentile RSS, balanced groups RSS, double versions of RSS, L-RSS, truncation-based RSS, and robust extreme RSS when the judgment rankings are both perfect and imperfect. Researchers can also use this new package to make parametric inferences for the population mean and the variance where the sample is obtained via classical RSS. Moreover, this package includes applications of the nonparametric methods which are one sample sign test, Mann-Whitney-Wilcoxon test, and Wilcoxon signed-rank test procedures. The package is available as RSSampling on CRAN.",
    "author": [
      {
        "name": "Busra Sevinc",
        "url": {}
      },
      {
        "name": "Bekir Cetintav",
        "url": {}
      },
      {
        "name": "Melek Esemen",
        "url": {}
      },
      {
        "name": "Selma Gurler",
        "url": {}
      }
    ],
    "date": "2019-08-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-039.zip\nCRAN packages used\nNSM3, RSSampling, stats, LearnBayes\nCRAN Task Views implied by cited packages\nBayesian, Distributions, Survival, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-040/",
    "title": "unival: An FA-based R Package For Assessing Essential Unidimensionality Using External Validity Information",
    "description": "The unival package is designed to help researchers decide between unidimensional and correlated factors solutions in the factor analysis of psychometric measures. The novelty of the approach is its use of external information, in which multiple factor scores and general factor scores are related to relevant external variables or criteria. The unival package’s implementation comes from a series of procedures put forward by Ferrando and Lorenzo-Seva (2019) and new methodological developments proposed in this article. We assess models fitted using unival by means of a simulation study extending the results obtained in the original proposal. Its usefulness is also assessed through a real-world data example. Based on these results, we conclude unival is a valuable tool for use in applications in which the dimensionality of an item set is to be assessed.",
    "author": [
      {
        "name": "Pere J. Ferrando",
        "url": {}
      },
      {
        "name": "Urbano Lorenzo-Seva",
        "url": {}
      },
      {
        "name": "David Navarro-Gonzalez",
        "url": {}
      }
    ],
    "date": "2019-08-20",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-040.zip\nCRAN packages used\nunival, stats, optimbase, psych, mirt\nCRAN Task Views implied by cited packages\nPsychometrics, MissingData\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-035/",
    "title": "BINCOR: An R package for Estimating the Correlation between Two Unevenly Spaced Time Series",
    "description": "This paper presents a computational program named BINCOR (BINned CORrelation) for estimating the correlation between two unevenly spaced time series. This program is also applicable to the situation of two evenly spaced time series not on the same time grid. BINCOR is based on a novel estimation approach proposed by Mudelsee (2010) for estimating the correlation between two climate time series with different timescales. The idea is that autocorrelation (e.g. an AR1 process) means that memory enables values obtained on different time points to be correlated. Binned correlation is performed by resampling the time series under study into time bins on a regular grid, assigning the mean values of the variable under scrutiny within those bins. We present two examples of our BINCOR package with real data: instrumental and paleoclimatic time series. In both applications BINCOR works properly in detecting well-established relationships between the climate records compared.",
    "author": [
      {
        "name": "Josue M. Polanco-Martinez",
        "url": {}
      },
      {
        "name": "Martin A. Medina-Elizalde",
        "url": {}
      },
      {
        "name": "Maria Fernanda Sanchez Goni",
        "url": {}
      },
      {
        "name": "Manfred            Mudelsee",
        "url": {}
      }
    ],
    "date": "2019-08-18",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-035.zip\nCRAN packages used\nBINCOR, dplR, pracma, TSdist\nCRAN Task Views implied by cited packages\nDifferentialEquations, NumericalMathematics, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-036/",
    "title": "auditor: an R Package for Model-Agnostic Visual Validation and Diagnostics",
    "description": "Machine learning models have successfully been applied to challenges in applied in biology, medicine, finance, physics, and other fields. With modern software it is easy to train even a complex model that fits the training data and results in high accuracy on test set. However, problems often arise when models are confronted with the real-world data. This paper describes methodology and tools for model-agnostic auditing. It provides functinos for assessing and comparing the goodness of fit and performance of models. In addition, the package may be used for analysis of the similarity of residuals and for identification of outliers and influential observations. The examination is carried out by diagnostic scores and visual verification. The code presented in this paper are implemented in the auditor package. Its flexible and consistent grammar facilitates the validation models of a large class of models.",
    "author": [
      {
        "name": "Alicja Gosiewska",
        "url": {}
      },
      {
        "name": "Przemysław Biecek",
        "url": {}
      }
    ],
    "date": "2019-08-18",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-036.zip\nCRAN packages used\nauditor\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-024/",
    "title": "shadow: R Package for Geometric Shadow Calculations in an Urban Environment",
    "description": "This paper introduces the shadow package for R. The package provides functions for shadow related calculations in the urban environment, namely shadow height, shadow footprint and Sky View Factor (SVF) calculations, as well as a wrapper function to estimate solar radiation while taking shadow effects into account. All functions operate on a layer of polygons with a height attribute, also known as “extruded polygons” or 2.5D vector data. Such data are associated with accuracy limitations in representing urban environments. However, unlike 3D models, polygonal layers of building outlines along with their height are abundantly available and their processing does not require specialized closed-source 3D software. The present package thus brings spatio-temporal shadow, SVF and solar radiation calculation capabilities to the open-source spatial analysis workflow in R. Package functionality is demonstrated using small reproducible examples for each function. Wider potential use cases include urban environment applications such as evaluation of micro-climatic influence for urban planning, studying urban climatic comfort and estimating photovoltaic energy production potential.",
    "author": [
      {
        "name": "Michael Dorman",
        "url": {}
      },
      {
        "name": "Evyatar Erell",
        "url": {}
      },
      {
        "name": "Adi Vulkan",
        "url": {}
      },
      {
        "name": "Itai Kloog",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-024.zip\nCRAN packages used\ninsol, shadow, sp, threejs, raster, rgeos, maptools, parallel, plot3D, sf\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-025/",
    "title": "Integration of networks and pathways with StarBioTrek package",
    "description": "High-throughput genomic technologies bring to light a comprehensive hallmark of molecular changes of a disease. It is increasingly evident that genes are not isolated from each other and the identification of a gene signature can only partially elucidate the de-regulated biological functions in a disease. The comprehension of how groups of genes (pathways) are related to each other (pathway cross talk) could explain biological mechanisms causing diseases. Biological pathways are important tools to identify gene interactions and decrease the large number of genes to be studied by partitioning them into smaller groups. Furthermore, recent scientific studies have demonstrated that an integration of pathways and networks, instead of a single component of the pathway or a single network, could lead to a deeper understanding of the pathology. StarBioTrek is an R package for the integration of biological pathways and networks which provides a series of functions to support the user in their analyses. In particular, it implements algorithms to identify pathways cross-talk networks and gene network drivers in pathways. It is available as open source and open development software in the Bioconductor platform.",
    "author": [
      {
        "name": "Claudia Cava",
        "url": {}
      },
      {
        "name": "Isabella Castiglioni",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-025.zip\nCRAN packages used\nXGR, qgraph, GOplot\nCRAN Task Views implied by cited packages\nPsychometrics\nBioconductor packages used\nNetPathMiner, ToPASeq, StarBioTrek, graphite\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-026/",
    "title": "ciuupi: An R package for Computing Confidence Intervals that Utilize Uncertain Prior Information",
    "description": "We have created the R package ciuupi to compute confidence intervals that utilize uncertain prior information in linear regression. Unlike post-model-selection confidence intervals, the confidence interval that utilizes uncertain prior information (CIUUPI) implemented in this package has, to an excellent approximation, coverage probability throughout the parameter space that is very close to the desired minimum coverage probability. Furthermore, when the uncertain prior information is correct, the CIUUPI is, on average, shorter than the standard confidence interval constructed using the full linear regression model. In this paper we provide motivating examples of scenarios where the CIUUPI may be used. We then give a detailed description of this interval and the numerical constrained optimization method implemented in R to obtain it. Lastly, using a real data set as an illustrative example, we show how to use the functions in ciuupi.",
    "author": [
      {
        "name": "Rheanna Mainzer",
        "url": {}
      },
      {
        "name": "Paul Kabaila",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-026.zip\nCRAN packages used\nciuupi, nloptr, statmod\nCRAN Task Views implied by cited packages\nDistributions, NumericalMathematics, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-027/",
    "title": "cvcrand: A Package for Covariate-constrained Randomization and the Clustered Permutation Test for Cluster Randomized Trials",
    "description": "The cluster randomized trial (CRT) is a randomized controlled trial in which randomization is conducted at the cluster level (e.g., school or hospital) and outcomes are measured for each individual within a cluster. Often, the number of clusters available to randomize is small (≤ 20), which increases the chance of baseline covariate imbalance between comparison arms. Such imbalance is particularly problematic when the covariates are predictive of the outcome because it can threaten the internal validity of the CRT. Pair-matching and stratification are two restricted randomization approaches that are frequently used to ensure balance at the design stage. An alternative, less commonly-used restricted randomization approach is covariate-constrained randomization. Covariate-constrained randomization quantifies baseline imbalance of cluster-level covariates using a balance metric and randomly selects a randomization scheme from those with acceptable balance by the balance metric. It is able to accommodate multiple covariates, both categorical and continuous. To facilitate imple mentation of covariate-constrained randomization for the design of two-arm parallel CRTs, we have developed the cvcrand R package. In addition, cvcrand also implements the clustered permutation test for analyzing continuous and binary outcomes collected from a CRT designed with covariate constrained randomization. We used a real cluster randomized trial to illustrate the functions included in the package.",
    "author": [
      {
        "name": "Hengshi Yu",
        "url": {}
      },
      {
        "name": "Fan Li",
        "url": {}
      },
      {
        "name": "John A. Gallis",
        "url": {}
      },
      {
        "name": "Elizabeth L. Turner",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-027.zip\nCRAN packages used\ncvcrand\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-028/",
    "title": "jomo: A Flexible Package for Two-level Joint Modelling Multiple Imputation",
    "description": "Multiple imputation is a tool for parameter estimation and inference with partially observed data, which is used increasingly widely in medical and social research. When the data to be imputed are correlated or have a multilevel structure — repeated observations on patients, school children nested in classes within schools within educational districts — the imputation model needs to include this structure. Here we introduce our joint modelling package for multiple imputation of multilevel data, jomo, which uses a multivariate normal model fitted by Markov Chain Monte Carlo (MCMC). Compared to previous packages for multilevel imputation, e.g. pan, jomo adds the facility to (i) handle and impute categorical variables using a latent normal structure, (ii) impute level-2 variables, and (iii) allow for cluster-specific covariance matrices, including the option to give them an inverse-Wishart distribution at level 2. The package uses C routines to speed up the computations and has been extensively validated in simulation studies both by ourselves and others.",
    "author": [
      {
        "name": "Matteo Quartagno",
        "url": {}
      },
      {
        "name": "Simon Grund",
        "url": {}
      },
      {
        "name": "James Carpenter",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-028.zip\nCRAN packages used\njomo, pan, norm, cat, mix, R2MLwiN, mitools, mice, semTools, BaBooN, mitml, Amelia, mi, lavaan.survey, dummies, nlme\nCRAN Task Views implied by cited packages\nMissingData, OfficialStatistics, SocialSciences, Multivariate, Psychometrics, Bayesian, ChemPhys, Econometrics, Environmetrics, Finance, Spatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-029/",
    "title": "ipwErrorY: An R Package for Estimation of Average Treatment Effect with Misclassified Binary Outcome",
    "description": "It has been well documented that ignoring measurement error may result in severely biased inference results. In recent years, there has been limited but increasing research on causal inference with measurement error. In the presence of misclassified binary outcome variable, Shu and Yi (2017) considered the inverse probability weighted estimation of the average treatment effect and proposed valid estimation methods to correct for misclassification effects for various settings. To expedite the application of those methods for situations where misclassification in the binary outcome variable is a real concern, we implement correction methods proposed by Shu and Yi (2017) and develop an R package ipwErrorY for general users. Simulated datasets are used to illustrate the use of the developed package.",
    "author": [
      {
        "name": "Di Shu",
        "url": {}
      },
      {
        "name": "Grace Y. Yi",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-029.zip\nCRAN packages used\nipwErrorY, nleqslv\nCRAN Task Views implied by cited packages\nNumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-030/",
    "title": "optimParallel: An R Package Providing a Parallel Version of the L-BFGS-B Optimization Method",
    "description": "The R package optimParallel provides a parallel version of the L-BFGS-B optimization method of optim(). The main function of the package is optimParallel(), which has the same usage and output as optim(). Using optimParallel() can significantly reduce the optimization time, especially when the evaluation time of the objective function is large and no analytical gradient is available. We introduce the R package and illustrate its implementation, which takes advantage of the lexical scoping mechanism of R.",
    "author": [
      {
        "name": "Florian Gerber",
        "url": {}
      },
      {
        "name": "Reinhard Furrer",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-030.zip\nCRAN packages used\nlbfgsb3, lbfgsb3c, optimParallel, testthat, microbenchmark\nCRAN Task Views implied by cited packages\nOptimization\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-031/",
    "title": "swgee: An R Package for Analyzing Longitudinal Data with Response Missingness and Covariate Measurement Error",
    "description": "Though longitudinal data often contain missing responses and error-prone covariates, relatively little work has been available to simultaneously correct for the effects of response missingness and covariate measurement error on analysis of longitudinal data. Yi (2008) proposed a simulation based marginal method to adjust for the bias induced by measurement error in covariates as well as by missingness in response. The proposed method focuses on modeling the marginal mean and variance structures, and the missing at random mechanism is assumed. Furthermore, the distribution of covariates are left unspecified. These features make the proposed method applicable to a broad settings. In this paper, we develop an R package, called swgee, which implements the method proposed by Yi (2008). Moreover, our package includes additional implementation steps which extend the setting considered by Yi (2008). To describe the use of the package and its main features, we report simulation studies and analyses of a data set arising from the Framingham Heart Study.",
    "author": [
      {
        "name": "Juan Xiong",
        "url": {}
      },
      {
        "name": "Grace Y. Yi",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-031.zip\nCRAN packages used\ngee, yags, wgeesel, geepack, mvtnorm\nCRAN Task Views implied by cited packages\nSocialSciences, Distributions, Econometrics, Finance, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-032/",
    "title": "roahd Package: Robust Analysis of High Dimensional Data",
    "description": "The focus of this paper is on the open-source R package roahd (RObust Analysis of High dimensional Data), see Tarabelloni et al. (2017). roahd has been developed to gather recently proposed statistical methods that deal with the robust inferential analysis of univariate and multivariate functional data. In particular, efficient methods for outlier detection and related graphical tools, methods to represent and simulate functional data, as well as inferential tools for testing differences and dependency among families of curves will be discussed, and the associated functions of the package will be described in details.",
    "author": [
      {
        "name": "Francesca Ieva",
        "url": {}
      },
      {
        "name": "Anna Maria Paganoni",
        "url": {}
      },
      {
        "name": "Juan Romo",
        "url": {}
      },
      {
        "name": "Nicholas Tarabelloni",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-032.zip\nCRAN packages used\nR\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-033/",
    "title": "The Landscape of R Packages for Automated Exploratory Data Analysis",
    "description": "The increasing availability of large but noisy data sets with a large number of heterogeneous variables leads to the increasing interest in the automation of common tasks for data analysis. The most time-consuming part of this process is the Exploratory Data Analysis, crucial for better domain understanding, data cleaning, data validation, and feature engineering. There is a growing number of libraries that attempt to automate some of the typical Exploratory Data Analysis tasks to make the search for new insights easier and faster. In this paper, we present a systematic review of existing tools for Automated Exploratory Data Analysis (autoEDA). We explore the features of fifteen popular R packages to identify the parts of analysis that can be effectively automated with the current tools and to point out new directions for further autoEDA development.",
    "author": [
      {
        "name": "Mateusz Staniak",
        "url": {}
      },
      {
        "name": "Przemysław Biecek",
        "url": {}
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-033.zip\nCRAN packages used\ncranlogs, radiant, visdat, archivist, xtable, arsenal, DataExplorer, dataMaid, dlookr, ExPanDaR, explore, shiny, exploreR, funModeling, inspectdf, RtutoR, SmartEDA, data.table, summarytools, knitr, ggplot2, xray, tableone, describer, skimr, prettyR, Hmisc, ggfortify, autoplotly, gpairs, GGally, survminer, cr17, DALEX, iml\nCRAN Task Views implied by cited packages\nReproducibleResearch, TeachingStatistics, MissingData, WebTechnologies, Bayesian, ClinicalTrials, Econometrics, Finance, Graphics, HighPerformanceComputing, Multivariate, OfficialStatistics, Phylogenetics, SocialSciences, Survival, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-019/",
    "title": "MDFS: MultiDimensional Feature Selection in R",
    "description": "Identification of informative variables in an information system is often performed using simple one-dimensional filtering procedures that discard information about interactions between variables. Such an approach may result in removing some relevant variables from consideration. Here we present an R package MDFS (MultiDimensional Feature Selection) that performs identification of informative variables taking into account synergistic interactions between multiple descriptors and the decision variable. MDFS is an implementation of an algorithm based on information theory (Mnich and Rudnicki, 2017). The computational kernel of the package is implemented in C++. A high-performance version implemented in CUDA C is also available. The application of MDFS is demonstrated using the well-known Madelon dataset, in which a decision variable is generated from synergistic interactions between descriptor variables. It is shown that the application of multidimen sional analysis results in better sensitivity and ranking of importance.",
    "author": [
      {
        "name": "Radosław Piliszek",
        "url": {}
      },
      {
        "name": "Krzysztof Mnich",
        "url": {}
      },
      {
        "name": "Szymon Migacz",
        "url": {}
      },
      {
        "name": "Paweł Tabaszewski",
        "url": {}
      },
      {
        "name": "Andrzej Sułecki",
        "url": {}
      },
      {
        "name": "Aneta            Polewko-Klim",
        "url": {}
      },
      {
        "name": "Witold Rudnicki",
        "url": {}
      }
    ],
    "date": "2019-08-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-019.zip\nCRAN packages used\nMDFS, Rfast\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-020/",
    "title": "Nowcasting: An R Package for Predicting Economic Variables Using Dynamic Factor Models",
    "description": "The nowcasting package provides the tools to make forecasts of monthly or quarterly economic variables using dynamic factor models. The objective is to help the user at each step of the forecasting process, starting with the construction of a database, all the way to the interpretation of the forecasts. The dynamic factor model adopted in this package is based on the articles from Giannone et al. (2008) and Banbura et al. (2011). Although there exist several other dynamic factor model packages available for R, ours provides an environment to easily forecast economic variables and interpret results.",
    "author": [
      {
        "name": "Serge de Valk",
        "url": {}
      },
      {
        "name": "Daiane de Mattos",
        "url": {}
      },
      {
        "name": "Pedro Ferreira",
        "url": {}
      }
    ],
    "date": "2019-08-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-020.zip\nCRAN packages used\nnowcasting\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-021/",
    "title": "ConvergenceClubs: A Package for Performing the Phillips and Sul's Club Convergence Clustering Procedure",
    "description": "This paper introduces package ConvergenceClubs, which implements functions to perform the Phillips and Sul (2007, 2009) club convergence clustering procedure in a simple and reproducible manner. The approach proposed by Phillips and Sul to analyse the convergence patterns of groups of economies is formulated as a nonlinear time varying factor model that allows for different time paths as well as individual heterogeneity. Unlike other approaches in which economies are grouped a priori, it also allows the endogenous determination of convergence clubs. The algorithm, usage, and implementation details are discussed.",
    "author": [
      {
        "name": "Roberto Sichera",
        "url": {}
      },
      {
        "name": "Pietro Pizzuto",
        "url": {}
      }
    ],
    "date": "2019-08-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-021.zip\nCRAN packages used\nConvergenceClubs, mFilter\nCRAN Task Views implied by cited packages\nTimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-022/",
    "title": "SimCorrMix: Simulation of Correlated Data with Multiple Variable Types Including Continuous and Count Mixture Distributions",
    "description": "The SimCorrMix package generates correlated continuous (normal, non-normal, and mix ture), binary, ordinal, and count (regular and zero-inflated, Poisson and Negative Binomial) variables that mimic real-world data sets. Continuous variables are simulated using either Fleishman’s third order or Headrick’s fifth-order power method transformation. Simulation occurs at the component level for continuous mixture distributions, and the target correlation matrix is specified in terms of correlations with components. However, the package contains functions to approximate expected correlations with continuous mixture variables. There are two simulation pathways which calculate intermediate correlations involving count variables differently, increasing accuracy under a wide range of parameters. The package also provides functions to calculate cumulants of continuous mixture distributions, check parameter inputs, calculate feasible correlation boundaries, and summarize and plot simulated variables. SimCorrMix is an important addition to existing R simulation packages because it is the first to include continuous mixture and zero-inflated count variables in correlated data sets.",
    "author": [
      {
        "name": "Allison Fialkowski",
        "url": {}
      },
      {
        "name": "Hemant Tiwari",
        "url": {}
      }
    ],
    "date": "2019-08-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-022.zip\nCRAN packages used\nAdaptGauss, DPP, bgmm, ClusterR, mclust, mixture, AdMit, bimixt, bmixture, CAMAN, flexmix, mixdist, mixtools, nspmix, MixtureInf, Rmixmod, hurdlr, zic, mixpack, distr, stats, rebmix, SimCorrMix, SimMultiCorrData, GenOrd, VGAM, Matrix, ggplot2, mvtnorm\nCRAN Task Views implied by cited packages\nCluster, Distributions, Multivariate, Bayesian, Environmetrics, Econometrics, Psychometrics, ExtremeValue, Finance, Graphics, MetaAnalysis, NumericalMathematics, Phylogenetics, Robust, SocialSciences, Survival, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-023/",
    "title": "Time-Series Clustering in R Using the dtwclust Package",
    "description": "Most clustering strategies have not changed considerably since their initial definition. The common improvements are either related to the distance measure used to assess dissimilarity, or the function used to calculate prototypes. Time-series clustering is no exception, with the Dynamic Time Warping distance being particularly popular in that context. This distance is computationally expensive, so many related optimizations have been developed over the years. Since no single clustering algorithm can be said to perform best on all datasets, different strategies must be tested and compared, so a common infrastructure can be advantageous. In this manuscript, a general overview of shape-based time-series clustering is given, including many specifics related to Dynamic Time Warping and associated techniques. At the same time, a description of the dtwclust package for the R statistical software is provided, showcasing how it can be used to evaluate many different time-series clustering procedures.",
    "author": [
      {
        "name": "Alexis Sardá-Espinosa",
        "url": {}
      }
    ],
    "date": "2019-08-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-023.zip\nCRAN packages used\ndtwclust, flexclust, cluster, TSdist, TSclust, pdc, dtw, proxy, clue, foreach, RcppParallel, doParallel\nCRAN Task Views implied by cited packages\nTimeSeries, Cluster, Multivariate, HighPerformanceComputing, Environmetrics, Optimization, Robust\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-009/",
    "title": "mixedsde: A Package to Fit Mixed Stochastic Differential Equations",
    "description": "Stochastic differential equations (SDEs) are useful to model continuous stochastic processes. When (independent) repeated temporal data are available, variability between the trajectories can be modeled by introducing random effects in the drift of the SDEs. These models are useful to analyze neuronal data, crack length data, pharmacokinetics, financial data, to cite some applications among other. The R package focuses on the estimation of SDEs with linear random effects in the drift. The goal is to estimate the common density of the random effects from repeated discrete observations of the SDE. The package mixedsde proposes three estimation methods: a Bayesian parametric, a frequentist parametric and a frequentist nonparametric method. The three procedures are described as well as the main functions of the package. Illustrations are presented on simulated and real data.",
    "author": [
      {
        "name": "Charlotte Dion",
        "url": {}
      },
      {
        "name": "Simone Hermann",
        "url": {}
      },
      {
        "name": "Adeline Samson",
        "url": {}
      }
    ],
    "date": "2019-08-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-009.zip\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-010/",
    "title": "Indoor Positioning and Fingerprinting: The R Package ipft",
    "description": "Methods based on Received Signal Strength Indicator (RSSI) fingerprinting are in the forefront among several techniques being proposed for indoor positioning. This paper introduces the R package ipft, which provides algorithms and utility functions for indoor positioning using fingerprinting techniques. These functions are designed for manipulation of RSSI fingerprint data sets, estimation of positions, comparison of the performance of different positioning models, and graphical visualization of data. Well-known machine learning algorithms are implemented in this package to perform analysis and estimations over RSSI data sets. The paper provides a description of these algorithms and functions, as well as examples of its use with real data. The ipft package provides a base that we hope to grow into a comprehensive library of fingerprinting-based indoor positioning methodologies.",
    "author": [
      {
        "name": "Emilio Sansano",
        "url": {}
      },
      {
        "name": "Raúl Montoliu",
        "url": {}
      },
      {
        "name": "Óscar Belmonte",
        "url": {}
      },
      {
        "name": "Joaquín Torres-Sospedra",
        "url": {}
      }
    ],
    "date": "2019-08-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-010.zip\nCRAN packages used\nipft, ggplot2\nCRAN Task Views implied by cited packages\nGraphics, Phylogenetics, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-011/",
    "title": "RobustGaSP: Robust Gaussian Stochastic Process Emulation in R",
    "description": "Gaussian stochastic process (GaSP) emulation is a powerful tool for approximating compu tationally intensive computer models. However, estimation of parameters in the GaSP emulator is a challenging task. No closed-form estimator is available and many numerical problems arise with standard estimates, e.g., the maximum likelihood estimator. In this package, we implement a marginal posterior mode estimator, for special priors and parameterizations. This estimation method that meets the robust parameter estimation criteria was discussed in Gu et al. (2018); mathematical reasons are provided therein to explain why robust parameter estimation can greatly improve predictive performance of the emulator. In addition, inert inputs (inputs that almost have no effect on the variability of a function) can be identified from the marginal posterior mode estimation at no extra computational cost. The package also implements the parallel partial Gaussian stochastic process (PP GaSP) emulator (Gu and Berger (2016)) for the scenario where the computer model has multiple outputs on, for example, spatial-temporal coordinates. The package can be operated in a default mode, but also allows numerous user specifications, such as the capability of specifying trend functions and noise terms. Examples are studied herein to highlight the performance of the package in terms of out-of-sample prediction.",
    "author": [
      {
        "name": "Mengyang Gu",
        "url": {}
      },
      {
        "name": "Jesus Palomo",
        "url": {}
      },
      {
        "name": "James O. Berger",
        "url": {}
      }
    ],
    "date": "2019-08-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-011.zip\nCRAN packages used\nDiceKriging, GPfit, mleGP, spatial, fields, RobustGaSP, lhs, sensitivity, nloptr\nCRAN Task Views implied by cited packages\nExperimentalDesign, Spatial, Distributions, Environmetrics, Optimization, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-012/",
    "title": "Whats for dynr: A Package for Linear and Nonlinear Dynamic Modeling in R",
    "description": "Intensive longitudinal data in the behavioral sciences are often noisy, multivariate in nature, and may involve multiple units undergoing regime switches by showing discontinuities interspersed with continuous dynamics. Despite increasing interest in using linear and nonlinear differential/difference equation models with regime switches, there has been a scarcity of software packages that are fast and freely accessible. We have created an R package called dynr that can handle a broad class of linear and nonlinear discreteand continuous-time models, with regime-switching properties and linear Gaussian measurement functions, in C, while maintaining simple and easy-to learn model specification functions in R. We present the mathematical and computational bases used by the dynr R package, and present two illustrative examples to demonstrate the unique features of dynr.",
    "author": [
      {
        "name": "Lu Ou",
        "url": {}
      },
      {
        "name": "Michael D. Hunter",
        "url": {}
      },
      {
        "name": "Sy-Miin Chow",
        "url": {}
      }
    ],
    "date": "2019-08-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-012.zip\nCRAN packages used\ndynr, dlm, KFAS, dse, OpenMx, ctsem, depmixS4, RHmm, MSwM, MSBVAR, MSGARCH, pomp, stats, Rcpp, RcppGSL, mice\nCRAN Task Views implied by cited packages\nTimeSeries, Finance, MissingData, Psychometrics, Bayesian, Cluster, DifferentialEquations, Environmetrics, HighPerformanceComputing, Multivariate, NumericalMathematics, OfficialStatistics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-016/",
    "title": "Identifying and Testing Recursive vs. Interdependent Links in Simultaneous Equation Models via the SIRE Package",
    "description": "Simultaneous equation models (SEMs) are composed of relations which either represent by Gianmarco Vacca and Maria Grazia Zoia Package Equation Models via the SIRE Interdependent Links in Simultaneous Identifying and Testing Recursive vs. Contributed research article 1",
    "author": [
      {
        "name": "Gianmarco Vacca",
        "url": {}
      },
      {
        "name": "Maria Grazia Zoia",
        "url": {}
      }
    ],
    "date": "2019-08-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-016.zip\nCRAN packages used\nSIRE, igraph, systemfit, Rsolnp\nCRAN Task Views implied by cited packages\nOptimization, Econometrics, gR, Graphics, Psychometrics, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-017/",
    "title": "fclust: An R Package for Fuzzy Clustering",
    "description": "Fuzzy clustering methods discover fuzzy partitions where observations can be softly assigned to more than one cluster. The package fclust is a toolbox for fuzzy clustering in the R programming language. It not only implements the widely used fuzzy k-means (FkM) algorithm, but also many FkM variants. Fuzzy cluster similarity measures, cluster validity indices and cluster visualization tools are also offered. In the current version, all the functions are rewritten in the C++ language allowing their application in large-size problems. Moreover, new fuzzy relational clustering algorithms for partitioning qualitative/mixed data are provided together with an improved version of the so-called Gustafson-Kessel algorithm to avoid singularity in the cluster covariance matrices. Finally, it is now possible to automatically select the number of clusters by means of the available fuzzy cluster validity indices.",
    "author": [
      {
        "name": "Maria Brigida Ferraro",
        "url": {}
      },
      {
        "name": "Paolo Giordani",
        "url": {}
      },
      {
        "name": "Alessio Serafini",
        "url": {}
      }
    ],
    "date": "2019-08-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-017.zip\nCRAN packages used\nfclust, cluster, clue, e1071, skmeans, vegclust, ppclust, Rcpp, RcppArmadillo, smacof, MASS\nCRAN Task Views implied by cited packages\nCluster, Multivariate, Environmetrics, NumericalMathematics, Psychometrics, Distributions, Robust, Econometrics, HighPerformanceComputing, MachineLearning, NaturalLanguageProcessing, Optimization, SocialSciences, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-018/",
    "title": "Matching with Clustered Data: the CMatching Package in R",
    "description": "Matching is a well known technique to balance covariates distribution between treated and control units in non-experimental studies. In many fields, clustered data are a very common occurrence in the analysis of observational data and the clustering can add potentially interesting information. Matching algorithms should be adapted to properly exploit the hierarchical structure. In this article we present the CMatching package implementing matching algorithms for clustered data. The package provides functions for obtaining a matched dataset along with estimates of most common parameters of interest and model-based standard errors. A propensity score matching analysis, relating math proficiency with homework completion for students belonging to different schools (based on the NELS-88 data), illustrates in detail the use of the algorithms.",
    "author": [
      {
        "name": "Massimo Cannas",
        "url": {}
      },
      {
        "name": "Bruno Arpino",
        "url": {}
      }
    ],
    "date": "2019-08-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-018.zip\nCRAN packages used\nCMatching, Matching, designmatch, optmatch, MatchIT, quickmatch, multiwayvcov\nCRAN Task Views implied by cited packages\nSocialSciences, Econometrics, ExperimentalDesign, HighPerformanceComputing, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-002/",
    "title": "Connecting R with D3 for dynamic graphics, to explore multivariate data with tours",
    "description": "The tourr package in R has several algorithms and displays for showing multivariate data as a sequence of low-dimensional projections. It can display as a movie but has no capacity for interaction, such as stop/go, change tour type, drop/add variables. The tourrGui package provides these sorts of controls, but the interface is programmed with the dated RGtk2 package. This work explores using custom messages to pass data from R to D3 for viewing, using the Shiny framework. This is an approach that can be generally used for creating all sorts of interactive graphics.",
    "author": [
      {
        "name": "Michael Kipp",
        "url": {}
      },
      {
        "name": "Ursula Laa",
        "url": {}
      },
      {
        "name": "Dianne Cook",
        "url": {}
      }
    ],
    "date": "2019-07-30",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntourr, tourrGui, RGtk2\nCRAN Task Views implied by cited packages\nGraphics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-003/",
    "title": "dr4pl: A Stable Convergence Algorithm for the 4 Parameter Logistic Model",
    "description": "The 4 Parameter Logistic (4PL) model has been recognized as a major tool to analyze the relationship between doses and responses in pharmacological experiments. A main strength of this model is that each parameter contributes an intuitive meaning enhancing interpretability of a fitted model. However, implementing the 4PL model using conventional statistical software often encounters numerical errors. This paper highlights the issue of convergence failure and presents several causes with solutions. These causes include outliers and a non-logistic data shape, so useful remedies such as robust estimation, outlier diagnostics and constrained optimization are proposed. These features are implemented in a new R package dr4pl (Dose-Response analysis using the 4 Parameter Logistic model) whose code examples are presented as a separate section. Our R package dr4pl is shown to work well for data sets where the traditional dose-response modelling packages drc and nplr fail.",
    "author": [
      {
        "name": "Hyowon An",
        "url": {}
      },
      {
        "name": "Justin T. Landis",
        "url": {}
      },
      {
        "name": "Aubrey G. Bailey",
        "url": {}
      },
      {
        "name": "James S. Marron",
        "url": {}
      },
      {
        "name": "Dirk P. Dittmer",
        "url": {}
      }
    ],
    "date": "2019-07-30",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-003.zip\nCRAN packages used\ndr4pl, drc, nplr\nCRAN Task Views implied by cited packages\nChemPhys\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-004/",
    "title": "Time Series Forecasting with KNN in R: the tsfknn Package",
    "description": "In this paper the tsfknn package for time series forecasting using k-nearest neighbor regres sion is described. This package allows users to specify a KNN model and to generate its forecasts. The user can choose among different multi-step ahead strategies and among different functions to aggregate the targets of the nearest neighbors. It is also possible to assess the forecast accuracy of the KNN model.",
    "author": [
      {
        "name": "Francisco Martínez",
        "url": {}
      },
      {
        "name": "María P. Frías",
        "url": {}
      },
      {
        "name": "Francisco Charte",
        "url": {}
      },
      {
        "name": "Antonio J. Rivera",
        "url": {}
      }
    ],
    "date": "2019-07-30",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-004.zip\nCRAN packages used\nforecast, caret, nnfor, tsfknn, forecastHybrid, GMDH, NTS, tsDyn, nnet, neuralnet\nCRAN Task Views implied by cited packages\nTimeSeries, Econometrics, Finance, MachineLearning, Environmetrics, HighPerformanceComputing, MissingData, Multivariate, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-005/",
    "title": "rollmatch: An R Package for Rolling Entry Matching",
    "description": "The gold standard of experimental research is the randomized control trial. However, interventions are often implemented without a randomized control group for practical or ethical reasons. Propensity score matching (PSM) is a popular method for minimizing the effects of a randomized experiment from observational data by matching members of a treatment group to similar candidates that did not receive the intervention. Traditional PSM is not designed for studies that enroll participants on a rolling basis and does not provide a solution for interventions in which the baseline and intervention period are undefined in the comparison group. Rolling Entry Matching (REM) is a new matching method that addresses both issues. REM selects comparison members who are similar to intervention members with respect to both static (e.g., race) and dynamic (e.g., health conditions) characteristics. This paper will discuss the key components of REM and introduce the rollmatch R package.",
    "author": [
      {
        "name": "Kasey Jones",
        "url": {}
      },
      {
        "name": "Rob Chew",
        "url": {}
      },
      {
        "name": "Allison Witman",
        "url": {}
      },
      {
        "name": "Yiyan Liu",
        "url": {}
      }
    ],
    "date": "2019-07-30",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-005.zip\nCRAN packages used\nrollmatch, CBPS, ipw, MatchIt, Matching, optmatch\nCRAN Task Views implied by cited packages\nSocialSciences, HighPerformanceComputing, MissingData, OfficialStatistics, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-006/",
    "title": "orthoDr: Semiparametric Dimension Reduction via Orthogonality Constrained Optimization",
    "description": "orthoDr is a package in R that solves dimension reduction problems using orthogonality constrained optimization approach. The package serves as a unified framework for many regression and survival analysis dimension reduction models that utilize semiparametric estimating equations. The main computational machinery of orthoDr is a first-order algorithm developed by Wen and Yin (2012) for optimization within the Stiefel manifold. We implement the algorithm through Rcpp and OpenMP for fast computation. In addition, we developed a general-purpose solver for such constrained problems with user-specified objective functions, which works as a drop-in version of optim(). The package also serves as a platform for future methodology developments along this line of work.",
    "author": [
      {
        "name": "Ruoqing Zhu",
        "url": {}
      },
      {
        "name": "Jiyang Zhang",
        "url": {}
      },
      {
        "name": "Ruilin Zhao",
        "url": {}
      },
      {
        "name": "Peng Xu",
        "url": {}
      },
      {
        "name": "Wenzhuo Zhou",
        "url": {}
      },
      {
        "name": "Xin Zhang",
        "url": {}
      }
    ],
    "date": "2019-07-30",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-006.zip\nCRAN packages used\northoDr, Rcpp, RcppArmadillo, ManifoldOpthm\nCRAN Task Views implied by cited packages\nNumericalMathematics, HighPerformanceComputing\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-007/",
    "title": "Modeling regimes with extremes: the bayesdfa package for identifying and forecasting common trends and anomalies in multivariate time-series data",
    "description": "The bayesdfa package provides a flexible Bayesian modeling framework for applying dy namic factor analysis (DFA) to multivariate time-series data as a dimension reduction tool. The core estimation is done with the Stan probabilistic programming language. In addition to being one of the few Bayesian implementations of DFA, novel features of this model include (1) optionally modeling latent process deviations as drawn from a Student-t distribution to better model extremes, and (2) optionally including autoregressive and moving-average components in the latent trends. Besides estimation, we provide a series of plotting functions to visualize trends, loadings, and model pre dicted values. A secondary analysis for some applications is to identify regimes in latent trends. We provide a flexible Bayesian implementation of a Hidden Markov Model — also written with Stan — to characterize regime shifts in latent processes. We provide simulation testing and details on parameter sensitivities in supplementary information.",
    "author": [
      {
        "name": "Eric J. Ward",
        "url": {}
      },
      {
        "name": "Sean C. Anderson",
        "url": {}
      },
      {
        "name": "Luis A. Damiano",
        "url": {}
      },
      {
        "name": "Mary E. Hunsicker",
        "url": {}
      },
      {
        "name": "Michael A. Litzow",
        "url": {}
      }
    ],
    "date": "2019-07-30",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndlm, KFAS, MARSS, tsfa, rstan, heavy, bsts, stochvol, loo, depmixS4, HMM, msm\nCRAN Task Views implied by cited packages\nTimeSeries, Bayesian, Finance, Cluster, Distributions, Econometrics, Multivariate, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-008/",
    "title": "Optimization Routines for Enforcing One-to-One Matches in Record Linkage Problems",
    "description": "Record linkage aims at quickly and accurately identifying if two records represent the same real world entity. In many applications, we are interested in restricting the linkage results to \"1 to 1\" links, that is a single record does not appear more than once in the output. This can be dealt with the transport algorithm. The optimization problem, however, grows quadratically in the size of the input, quickly becoming untreatable for cases with a few thousand records. This paper compares different solutions, provided by some R packages for linear programming solvers. The comparison is done in terms of memory usage and execution time. The aim is to overcome the current implementation in the toolkit RELAIS, specifically developed for record linkage problems. The results highlight improvements beyond expectations. In fact the tested solutions allow successfully executing the \"1 to 1\" reduction for large size datasets up to the largest sample surveys at National Statistical Institutes.",
    "author": [
      {
        "name": "Diego Moretti",
        "url": {}
      },
      {
        "name": "Luca Valentino",
        "url": {}
      },
      {
        "name": "Tiziana Tuoto",
        "url": {}
      }
    ],
    "date": "2019-07-30",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-008.zip\nCRAN packages used\nlpSolve, Rglpk, ROI.plugin.clp, intpoint, slam\nCRAN Task Views implied by cited packages\nOptimization\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-041/",
    "title": "Using Web Services to Work with Geodata in R",
    "description": "Through collaborative mapping, a massive amount of data is accessible. Many individuals contribute information each day. The growing amount of geodata is gathered by volunteers or obtained via crowd-sourcing. One outstanding example of this is the OpenStreetMap (OSM) Project which provides access to big data in geography. Another online mapping service that enables the integration of geodata into the analysis is Google Maps. The expanding content and the availability of geographic information radically changes the perspective on geodata (Chilton 2009). Recently many application programming interfaces (APIs) have been built on OSM and Google Maps. That leads to a point where it is possible to access sections of geographical information without the usage of a complex database solution, especially if one only requires a small data section for a visualization. First tools for spatial analysis have been included in the R language very early (Bivand and Gebhardt, 2000) and this development will continue to accelerate, underpinning a continual change. Notably, in recent years many tools have been developed to enable the usage of R as a geographic information system (GIS). With a GIS it is possible to process spatial data. QuantumGIS (QGIS) is a free software solution for these tasks, and a user interface is available for this purpose. R is, therefore, an alternative to geographic information systems like QGIS (QGIS Development Team 2009). Besides, add-ins for QGIS and R-packages (RQGIS) are available, that enables the combination of R and QGIS (Muenchow and Schratz 2017). It is the target of this article to present some of the most important R-functionalities to download and process geodata from OSM and the Google Maps API. The focus of this paper is on functions that enable the natural usage of these APIs.",
    "author": [
      {
        "name": "Jan-Philipp Kolb",
        "url": {}
      }
    ],
    "date": "2019-07-30",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-041.zip\nCRAN packages used\nRQGIS, osmdata, OpenStreetMap, ggplot2, ggmap, RgoogleMaps, leaflet, magrittr, tmap, mapview, mapdeck, lawn, googleway, RDSTK, RJSONIO, jsonlite, tmaptools, sf, OpenStreeetMap, mapmisc, opencage, curl, XML, osmplotr, sp, igraph\nCRAN Task Views implied by cited packages\nSpatial, WebTechnologies, Graphics, SpatioTemporal, gR, OfficialStatistics, Optimization, Phylogenetics, TeachingStatistics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2019-001/",
    "title": "atable: Create Tables for Clinical Trial Reports",
    "description": "Examining distributions of variables is the first step in the analysis of a clinical trial before more specific modelling can begin. Reporting these results to stakeholders of the trial is an essential part of a statistician’s work. The atable package facilitates these steps by offering easy-to-use but still flexible functions.",
    "author": [
      {
        "name": "Armin Ströbel",
        "url": {}
      }
    ],
    "date": "2019-07-22",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2019-001.zip\nCRAN packages used\nmultgee, Hmisc, knitr, xtable, flextable, settings, survival, furniture, tableone, stargazer, DescTools, margrittr, dplyr\nCRAN Task Views implied by cited packages\nReproducibleResearch, SocialSciences, ClinicalTrials, Econometrics, MissingData, Bayesian, ModelDeployment, Multivariate, OfficialStatistics, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-081/",
    "title": "idmTPreg: Regression Model for Progressive Illness Death Data",
    "description": "The progressive illness-death model is frequently used in medical applications. For example, the model may be used to describe the disease process in cancer studies. We have developed a new R package called idmTPreg to estimate regression coefficients in datasets that can be described by the progressive illness-death model. The motivation for the development of the package is a recent contribution that enables the estimation of possibly time-varying covariate effects on the transition probabilities for a progressive illness-death data. The main feature of the package is that it befits both non-Markov and Markov progressive illness-death data. The package implements the introduced estimators obtained using a direct binomial regression approach. Also, variance estimates and confidence bands are implemented in the package. This article presents guidelines for the use of the package.",
    "author": [
      {
        "name": "Leyla Azarang",
        "url": {}
      },
      {
        "name": "Manuel Oviedo de la Fuente",
        "url": {}
      }
    ],
    "date": "2019-02-11",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-081.zip\nCRAN packages used\nidmTPreg, mstate, msm, p3state.msm, doParallel, foreach, survival\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Distributions, Econometrics, HighPerformanceComputing, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-076/",
    "title": "Dynamic Simulation and Testing for Single-Equation Cointegrating and Stationary Autoregressive Distributed Lag Models",
    "description": "While autoregressive distributed lag models allow for extremely flexible dynamics, interpret ing the substantive significance of complex lag structures remains difficult. In this paper we discuss dynamac (dynamic autoregressive and cointegrating models), an R package designed to assist users in estimating, dynamically simulating, and plotting the results of a variety of autoregressive distributed lag models. It also contains a number of post-estimation diagnostics, including a test for cointegration for when researchers are estimating the error-correction variant of the autoregressive distributed lag model.",
    "author": [
      {
        "name": "Soren Jordan",
        "url": {}
      },
      {
        "name": "Andrew Q. Philips",
        "url": {}
      }
    ],
    "date": "2018-12-31",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-076.zip\nCRAN packages used\ndynsim, Zelig, urca, MASS\nCRAN Task Views implied by cited packages\nEconometrics, Finance, SocialSciences, Distributions, Environmetrics, Multivariate, NumericalMathematics, Psychometrics, Robust, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-074/",
    "title": "ShinyItemAnalysis for Teaching Psychometrics and to Enforce Routine Analysis of Educational Tests",
    "description": "This work introduces ShinyItemAnalysis, an R package and an online shiny application for psychometric analysis of educational tests and items. ShinyItemAnalysis covers a broad range of psy chometric methods and offers data examples, model equations, parameter estimates, interpretation of results, together with a selected R code, and is therefore suitable for teaching psychometric concepts with R. Furthermore, the application aspires to be an easy-to-use tool for analysis of educational tests by allowing the users to upload and analyze their own data and to automatically generate analysis reports in PDF or HTML. We argue that psychometric analysis should be a routine part of test development in order to gather proofs of reliability and validity of the measurement, and we demonstrate how ShinyItemAnalysis may help enforce this goal.",
    "author": [
      {
        "name": "Patrícia Martinková",
        "url": {}
      },
      {
        "name": "Adéla Drabinová",
        "url": {}
      }
    ],
    "date": "2018-12-13",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-074.zip\nCRAN packages used\npsych, ltm, difR, lavaan, ShinyItemAnalysis, shiny, mirt, corrplot, cowplot, CTT, data.table, deltaPlotR, difNLR, DT, ggdendro, ggplot2, gridExtra, knitr, latticeExtra, moments, msm, nnet, plotly, psychometric, reshape2, rmarkdown, shinyBS, shinydashboard, shinyjs, stringr, xtable\nCRAN Task Views implied by cited packages\nPsychometrics, ReproducibleResearch, MissingData, Distributions, Econometrics, Graphics, WebTechnologies, Finance, HighPerformanceComputing, MachineLearning, MetaAnalysis, Multivariate, OfficialStatistics, Phylogenetics, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-075/",
    "title": "Measurement Errors in R",
    "description": "This paper presents an R package to handle and represent measurements with errors in a very simple way. We briefly introduce the main concepts of metrology and propagation of uncertainty, and discuss related R packages. Building upon this, we introduce the errors package, which provides a class for associating uncertainty metadata, automated propagation and reporting. Working with errors enables transparent, lightweight, less error-prone handling and convenient representation of measurements with errors. Finally, we discuss the advantages, limitations and future work of computing with errors.",
    "author": [
      {
        "name": "Iñaki Ucar",
        "url": {}
      },
      {
        "name": "Edzer Pebesma",
        "url": {}
      },
      {
        "name": "Arturo Azcorra",
        "url": {}
      }
    ],
    "date": "2018-12-13",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-075.zip\nCRAN packages used\nunits, errors, car, msm, metRology, propagate, spup, distr, distrEllipse, distrEx, distrMod, distrRmetrics, distrSim, distrTeach, magrittr, ggplot2, tibble\nCRAN Task Views implied by cited packages\nDistributions, ChemPhys, Econometrics, Finance, Graphics, Multivariate, Phylogenetics, Robust, SocialSciences, Survival, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-070/",
    "title": "SARIMA Analysis and Automated Model Reports with BETS, an R Package",
    "description": "This article aims to demonstrate how the powerful features of the R package BETS can be applied to SARIMA time series analysis. BETS provides not only thousands of Brazilian economic time series from different institutions, but also a range of analytical tools, and educational resources. In particular, BETS is capable of generating automated model reports for any given time series. These reports rely on a single function call and are able to build three types of models (SARIMA being one of them). The functions need few inputs and output rich content. The output varies according to the inputs and usually consists of a summary of the series properties, step-by-step explanations on how the model was developed, predictions made by the model, and a file containing these predictions. This work focuses on this feature and several other BETS functions that are designed to help in modeling time series. We present them in a thorough case study: the SARIMA approach to model and forecast the Brazilian production of intermediate goods index series.",
    "author": [
      {
        "name": "Talitha F. Speranza",
        "url": {}
      },
      {
        "name": "Pedro C. Ferreira",
        "url": {}
      },
      {
        "name": "Jonatha A. da Costa",
        "url": {}
      }
    ],
    "date": "2018-12-11",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nBETS, forecast, mFilter, urca, seasonal, httr, rvest, RMySQL, rmarkdown, stats, dygraphs\nCRAN Task Views implied by cited packages\nTimeSeries, Econometrics, Finance, WebTechnologies, Databases, Environmetrics, MissingData, OfficialStatistics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-072/",
    "title": "Explanations of Model Predictions with live and breakDown Packages ",
    "description": "Complex models are commonly used in predictive modeling. In this paper we present R packages that can be used for explaining predictions from complex black box models and attributing parts of these predictions to input features. We introduce two new approaches and corresponding packages for such attribution, namely live and breakDown. We also compare their results with existing implementations of state-of-the-art solutions, namely, lime (Pedersen and Benesty, 2018) which implements Locally Interpretable Model-agnostic Explanations and iml (Molnar et al., 2018) which implements Shapley values.",
    "author": [
      {
        "name": "Mateusz Staniak",
        "url": {}
      },
      {
        "name": "Przemysław Biecek",
        "url": {}
      }
    ],
    "date": "2018-12-11",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-072.zip\nCRAN packages used\npdp, lime, caret, mlr, DALEX, iml, live, breakDown, archivist, xgboost, party, data.table, e1071, glmnet, randomForest\nCRAN Task Views implied by cited packages\nMachineLearning, Environmetrics, HighPerformanceComputing, Multivariate, Survival, Cluster, Distributions, Finance, MissingData, ModelDeployment, Psychometrics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-073/",
    "title": "bnclassify: Learning Bayesian Network Classifiers",
    "description": "The bnclassify package provides state-of-the art algorithms for learning Bayesian network classifiers from data. For structure learning it provides variants of the greedy hill-climbing search, a well-known adaptation of the Chow-Liu algorithm and averaged one-dependence estimators. It provides Bayesian and maximum likelihood parameter estimation, as well as three naive-Bayes specific methods based on discriminative score optimization and Bayesian model averaging. The implementation is efficient enough to allow for time-consuming discriminative scores on medium sized data sets. The bnclassify package provides utilities for model evaluation, such as cross-validated accuracy and penalized log-likelihood scores, and analysis of the underlying networks, including network plotting via the Rgraphviz package. It is extensively tested, with over 200 automated tests that give a code coverage of 94%. Here we present the main functionalities, illustrate them with a number of data sets, and comment on related software.",
    "author": [
      {
        "name": "Bojan Mihaljević",
        "url": {}
      },
      {
        "name": "Concha Bielza",
        "url": {}
      },
      {
        "name": "Pedro Larrañaga",
        "url": {}
      }
    ],
    "date": "2018-12-11",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-073.zip\nCRAN packages used\nbnlearn, bnclassify, caret, mlr, gRain, deal\nCRAN Task Views implied by cited packages\nBayesian, gR, HighPerformanceComputing, MachineLearning, Multivariate\nBioconductor packages used\nRgraphviz\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-053/",
    "title": "stplanr: A Package for Transport Planning",
    "description": "Tools for transport planning should be flexible, scalable, and transparent. The stplanr package demonstrates and provides a home for such tools, with an emphasis on spatial transport data and non-motorized modes. The stplanr package facilitates common transport planning tasks including: downloading and cleaning transport datasets; creating geographic “desire lines” from origin-destination (OD) data; route assignment, locally and interfaces to routing services such as CycleStreets.net; calculation of route segment attributes such as bearing and aggregate flow; and ‘travel watershed’ analysis. This paper demonstrates this functionality using reproducible examples on real transport datasets. More broadly, the experience of developing and using R functions for transport applications shows that open source software can form the basis of a reproducible transport planning workflow. The stplanr package, alongside other packages and open source projects, could provide a more transparent and democratically accountable alternative to the current approach, which is heavily reliant on proprietary and relatively inaccessible software.",
    "author": [
      {
        "name": "Robin Lovelace",
        "url": {}
      },
      {
        "name": "Richard Ellison",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-053.zip\nCRAN packages used\nsp, rgeos, rgdal, sf, SpatialEpi, diseasemapping, leaflet, tmap, mapview, mapmisc, XML, twitteR, ggplot2, muStat, mgcv, shiny, haven, rio, dplyr, osmdata, stats19, bikedata, stplanr, nycflights, nycflights13, cyclestreets, igraph, Rcpp, aspace, MCI\nCRAN Task Views implied by cited packages\nSpatial, WebTechnologies, Graphics, OfficialStatistics, SpatioTemporal, Bayesian, Databases, Econometrics, Environmetrics, gR, HighPerformanceComputing, ModelDeployment, NumericalMathematics, Optimization, Phylogenetics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-054/",
    "title": "rcss: R package for optimal convex stochastic switching",
    "description": "The R package rcss provides users with a tool to approximate the value functions in the Bellman recursion under certain assumptions that guarantee desirable convergence properties. This R package represents the first software implementation of these methods using matrices and nearest neighbors. This package also employs a pathwise dynamic method to gauge the quality of these value function approximations. Statistical analysis can be performed on the results to obtain other useful practical insights. This paper describes rcss version 1.6.",
    "author": [
      {
        "name": "Juri Hinz",
        "url": {}
      },
      {
        "name": "Jeremy Yee",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-054.zip\nCRAN packages used\nrcss, Rcpp, OpenMp\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-055/",
    "title": "addhaz: Contribution of Chronic Diseases to the Disability Burden Using R",
    "description": "The increase in life expectancy followed by the burden of chronic diseases contributes to disability at older ages. The estimation of how much chronic conditions contribute to disability can be useful to develop public health strategies to reduce the burden. This paper introduces the R package addhaz, which is based on the attribution method (Nusselder and Looman, 2004) to partition disability into the additive contributions of diseases using cross-sectional data. The R package includes tools to fit the additive hazard model, the core of the attribution method, to binary and multinomial outcomes. The models are fitted by maximizing the binomial and multinomial log-likelihood functions using constrained optimization. Wald and bootstrap confidence intervals can be obtained for the parameter estimates. Also, the contribution of diseases to the disability prevalence and their bootstrap confidence intervals can be estimated. An additional feature is the possibility to use parallel computing to obtain the bootstrap confidence intervals. In this manuscript, we illustrate the use of addhaz with several examples for the binomial and multinomial models, using the data from the Brazilian National Health Survey, 2013.",
    "author": [
      {
        "name": "Renata Tiene de Carvalho Yokota",
        "url": {}
      },
      {
        "name": "Caspar WN Looman",
        "url": {}
      },
      {
        "name": "Wilma Johanna Nusselder",
        "url": {}
      },
      {
        "name": "Herman Van            Oyen",
        "url": {}
      },
      {
        "name": "Geert Molenberghs",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-055.zip\nCRAN packages used\naddhaz, boot, stats, logbin, VGAM\nCRAN Task Views implied by cited packages\nEconometrics, SocialSciences, Survival, Distributions, Environmetrics, ExtremeValue, Multivariate, Optimization, Psychometrics, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-056/",
    "title": "Snowboot: Bootstrap Methods for Network Inference",
    "description": "Complex networks are used to describe a broad range of disparate social systems and natural phenomena, from power grids to customer segmentation to human brain connectome. Challenges of parametric model specification and validation inspire a search for more data-driven and flexible nonparametric approaches for inference of complex networks. In this paper we discuss methodology and R implementation of two bootstrap procedures on random networks, that is, patchwork bootstrap of Thompson et al. (2016) and Gel et al. (2017) and vertex bootstrap of Snijders and Borgatti (1999). To our knowledge, the new R package snowboot is the first implementation of the vertex and patchwork bootstrap inference on networks in R. Our new package is accompanied with a detailed user’s manual, and is compatible with the popular R package on network studies igraph. We evaluate the patchwork bootstrap and vertex bootstrap with extensive simulation studies and illustrate their utility in an application to analysis of real world networks.",
    "author": [
      {
        "name": "Yuzhou Chen",
        "url": {}
      },
      {
        "name": "Yulia R. Gel",
        "url": {}
      },
      {
        "name": "Vyacheslav Lyubchich",
        "url": {}
      },
      {
        "name": "Kusha Nezafati",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-056.zip\nCRAN packages used\nsnowboot, bootnet, sna, graphics, igraph, parallel, Rcpp, Rdpack, stats, VGAM\nCRAN Task Views implied by cited packages\nOptimization, SocialSciences, Bayesian, Distributions, Econometrics, Environmetrics, ExtremeValue, gR, Graphics, HighPerformanceComputing, Multivariate, NumericalMathematics, Psychometrics, Spatial, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-057/",
    "title": "testforDEP: An R Package for Modern Distribution-free Tests and Visualization Tools for Independence",
    "description": "This article introduces testforDEP, a portmanteau R package implementing for the first time several modern tests and visualization tools for independence between two variables. While classical tests for independence are in the base R packages, there have been several recently developed tests for independence that are not available in R. This new package combines the classical tests including Pearson’s product moment correlation coefficient method, Kendall’s τ rank correlation coefficient method and Spearman’s ρ rank correlation coefficient method with modern tests consisting of an empirical likelihood based test, a density-based empirical likelihood ratio test, Kallenberg data driven test, maximal information coefficient test, Hoeffding’s independence test and the continuous analysis of variance test. For two input vectors of observations, the function testforDEP provides a common interface for each of the tests and returns test statistics, corresponding p values and bootstrap confidence intervals as output. The function AUK provides an interface to visualize Kendall plots and computes the area under the Kendall plot similar to computing the area under a receiver operating characteristic (ROC) curve.",
    "author": [
      {
        "name": "Jeffrey C. Miecznikowski",
        "url": {}
      },
      {
        "name": "En-shuo Hsu",
        "url": {}
      },
      {
        "name": "Yanhua Chen",
        "url": {}
      },
      {
        "name": "Albert Vexler",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-057.zip\nCRAN packages used\ntestforDEP, Hmisc, minerva\nCRAN Task Views implied by cited packages\nBayesian, ClinicalTrials, Econometrics, MissingData, Multivariate, OfficialStatistics, ReproducibleResearch, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-058/",
    "title": "Navigating the R Package Universe",
    "description": "Today, the enormous number of contributed packages available to R users outstrips any given user’s ability to understand how these packages work, their relative merits, or how they are related to each other. We organized a plenary session at useR!2017 in Brussels for the R community to think through these issues and ways forward. This session considered three key points of discussion. Users can navigate the universe of R packages with (1) capabilities for directly searching for R packages, (2) guidance for which packages to use, e.g., from CRAN Task Views and other sources, and (3) access to common interfaces for alternative approaches to essentially the same problem.",
    "author": [
      {
        "name": "Julia Silge",
        "url": {}
      },
      {
        "name": "John C. Nash",
        "url": {}
      },
      {
        "name": "Spencer Graves",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-058.zip\nCRAN packages used\nsos, CRANsearcher, utils, pkgdown, lfe, optimx\nCRAN Task Views implied by cited packages\nEconometrics, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-059/",
    "title": "rFSA: An R Package for Finding Best Subsets and Interactions",
    "description": "Herein we present the R package rFSA, which implements an algorithm for improved variable selection. The algorithm searches a data space for models of a user-specified form that are statistically optimal under a measure of model quality. Many iterations afford a set of feasible solutions (or candidate models) that the researcher can evaluate for relevance to his or her questions of interest. The algorithm can be used to formulate new or to improve upon existing models in bioinformatics, health care, and myriad other fields in which the volume of available data has outstripped researchers’ practical and computational ability to explore larger subsets or higher-order interaction terms. The package accommodates linear and generalized linear models, as well as a variety of criterion functions such as Allen’s PRESS and AIC. New modeling strategies and criterion functions can be adapted easily to work with rFSA.",
    "author": [
      {
        "name": "Joshua Lambert",
        "url": {}
      },
      {
        "name": "Liyu Gong",
        "url": {}
      },
      {
        "name": "Corrine F. Elliott",
        "url": {}
      },
      {
        "name": "Katherine Thompson",
        "url": {}
      },
      {
        "name": "Arnold Stromberg",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-059.zip\nCRAN packages used\nrFSA, leaps, glmulti, glmnet, hierNet, hashmap, geepack, devtools\nCRAN Task Views implied by cited packages\nSocialSciences, ChemPhys, Econometrics, MachineLearning, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-060/",
    "title": "lmridge: A Comprehensive R Package for Ridge Regression",
    "description": "The ridge regression estimator, one of the commonly used alternatives to the conventional ordinary least squares estimator, avoids the adverse effects in the situations when there exists some considerable degree of multicollinearity among the regressors. There are many software packages available for estimation of ridge regression coefficients. However, most of them display limited methods to estimate the ridge biasing parameters without testing procedures. Our developed package, lmridge can be used to estimate ridge coefficients considering a range of different existing biasing parameters, to test these coefficients with more than 25 ridge related statistics, and to present different graphical displays of these statistics.",
    "author": [
      {
        "name": "Muhammad Imdad Ullah",
        "url": {}
      },
      {
        "name": "Muhammad Aslam",
        "url": {}
      },
      {
        "name": "Saima Altaf",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-060.zip\nCRAN packages used\nlmridge, ridge, MASS, lrmest, ltsbase, penalized, glmnet, RXshrink, rrBLUP, RidgeFusion, bigRR, lpridge, genridge, CoxRidge\nCRAN Task Views implied by cited packages\nMachineLearning, Survival, Distributions, Econometrics, Environmetrics, Multivariate, NumericalMathematics, Psychometrics, Robust, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-061/",
    "title": "Geospatial Point Density",
    "description": "This paper introduces a spatial point density algorithm designed to be explainable, meaning ful, and efficient. Originally designed for military applications, this technique applies to any spatial point process where there is a desire to clearly understand the measurement of density and maintain fidelity of the point locations. Typical spatial density plotting algorithms, such as kernel density estimation, implement some type of smoothing function that often results in a density value that is difficult to interpret. The purpose of the visualization method in this paper is to understand spatial point activity density with precision and meaning. The temporal tendency of the point process as an extension of the point density methodology is also discussed and displayed. Applications include visualization and measurement of any type of spatial point process. Visualization techniques integrate ggmap with examples from San Diego crime data.",
    "author": [
      {
        "name": "Paul F. Evangelista",
        "url": {}
      },
      {
        "name": "David Beskow",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-061.zip\nCRAN packages used\npointdensityP, spatstat, kde2d, bkde2D, ggplot2, ggmap, data.table\nCRAN Task Views implied by cited packages\nSpatial, Finance, Graphics, HighPerformanceComputing, Phylogenetics, SpatioTemporal, Survival, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-063/",
    "title": "sdpt3r: Semidefinite Quadratic Linear Programming in R",
    "description": "We present the package sdpt3r, an R implementation of the Matlab package SDPT3 (Toh et al., 1999). The purpose of the software is to solve semidefinite quadratic linear programming (SQLP) problems, which encompasses problems such as D-optimal experimental design, the nearest correlation matrix problem, and distance weighted discrimination, as well as problems in graph theory",
    "author": [
      {
        "name": "Adam Rahman",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-063.zip\nCRAN packages used\nsdpt3r, Rdsdp, Rcsdp, cccp, scs, Rmosek, quantmod\nCRAN Task Views implied by cited packages\nOptimization, Finance\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-064/",
    "title": "Downside Risk Evaluation with the R Package GAS",
    "description": "Financial risk managers routinely use non–linear time series models to predict the downside risk of the capital under management. They also need to evaluate the adequacy of their model using so–called backtesting procedures. The latter involve hypothesis testing and evaluation of loss functions. This paper shows how the R package GAS can be used for both the dynamic prediction and the evaluation of downside risk. Emphasis is given to the two key financial downside risk measures: Value-at-Risk (VaR) and Expected Shortfall (ES). High-level functions for: (i) prediction, (ii) backtesting, and (iii) model comparison are discussed, and code examples are provided. An illustration using the series of log–returns of the Dow Jones Industrial Average constituents is reported.",
    "author": [
      {
        "name": "David Ardia",
        "url": {}
      },
      {
        "name": "Kris Boudt",
        "url": {}
      },
      {
        "name": "Leopoldo Catania",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-064.zip\nCRAN packages used\nGAS, cubature\nCRAN Task Views implied by cited packages\nNumericalMathematics, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-065/",
    "title": "NetworkToolbox: Methods and Measures for Brain, Cognitive, and Psychometric Network Analysis in R",
    "description": "This article introduces the NetworkToolbox package for R. Network analysis offers an intuitive perspective on complex phenomena via models depicted by nodes (variables) and edges (correlations). The ability of networks to model complexity has made them the standard approach for modeling the intricate interactions in the brain. Similarly, networks have become an increasingly attractive model for studying the complexity of psychological and psychopathological phenomena. NetworkToolbox aims to provide researchers with state-of-the-art methods and measures for es timating and analyzing brain, cognitive, and psychometric networks. In this article, I introduce NetworkToolbox and provide a tutorial for applying some the package’s functions to personality data.",
    "author": [
      {
        "name": "Alexander P. Christensen",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-065.zip\nCRAN packages used\nNetworkToolbox, statnet, igraph, sna, brainGraph, qgraph, IsingFit, bootnet, glasso, psych, MVN, EGA, lavaan\nCRAN Task Views implied by cited packages\nPsychometrics, Optimization, SocialSciences, Bayesian, Econometrics, gR, Graphics, MissingData, OfficialStatistics, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-066/",
    "title": "jsr223: A Java Platform Integration for R with Programming Languages Groovy, JavaScript, JRuby, Jython, and Kotlin",
    "description": "The R package jsr223 is a high-level integration for five programming languages in the Java platform: Groovy, JavaScript, JRuby, Jython, and Kotlin. Each of these languages can use Java objects in their own syntax. Hence, jsr223 is also an integration for R and the Java platform. It enables developers to leverage Java solutions from within R by embedding code snippets or evaluating script files. This approach is generally easier than rJava’s low-level approach that employs the Java Native Interface. jsr223’s multi-language support is dependent on the Java Scripting API: an implementation of “JSR-223: Scripting for the Java Platform” that defines a framework to embed scripts in Java applications. The jsr223 package also features extensive data exchange capabilities and a callback interface that allows embedded scripts to access the current R session. In all, jsr223 makes solutions developed in Java or any of the jsr223-supported languages easier to use in R.",
    "author": [
      {
        "name": "Floid R. Gilbert",
        "url": {}
      },
      {
        "name": "David B. Dahl",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-066.zip\nCRAN packages used\nrJava, cranlogs, jsr223, rscala, jdx, V8, R6, Rserve, opencpu, rGroovy, jsonlite, reticulate, rJython, PythonInR, rjson\nCRAN Task Views implied by cited packages\nWebTechnologies, ModelDeployment, NumericalMathematics, HighPerformanceComputing\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-068/",
    "title": "RcppMsgPack: MessagePack Headers and Interface Functions for R",
    "description": "MessagePack, or MsgPack for short, or when referring to the implementation, is an efficient binary serialization format for exchanging data between different programming languages. The RcppMsgPack package provides R with both the MessagePack C++ header files, and the ability to access, create and alter MessagePack objects directly from R. The main driver functions of the R interface are two functions msgpack_pack and msgpack_unpack. The function msgpack_pack serializes R objects to a raw MessagePack message. The function msgpack_unpack de-serializes MessagePack messages back into R objects. Several helper functions are available to aid in processing and formatting data including msgpack_simplify, msgpack_format and msgpack_map.",
    "author": [
      {
        "name": "Travers Ching",
        "url": {}
      },
      {
        "name": "Dirk Eddelbuettel",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-068.zip\nCRAN packages used\nmongolite, RProtoBuf, RcppRedis, RcppMsgPack, Rcpp, nanotime, httr, feather, data.table\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, Databases, NumericalMathematics, Finance, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-069/",
    "title": "BNSP: an R Package for Fitting Bayesian Semiparametric Regression Models and Variable Selection",
    "description": "The R package BNSP provides a unified framework for semiparametric location-scale regression and stochastic search variable selection. The statistical methodology that the package is built upon utilizes basis function expansions to represent semiparametric covariate effects in the mean and variance functions, and spike-slab priors to perform selection and regularization of the estimated effects. In addition to the main function that performs posterior sampling, the package includes functions for assessing convergence of the sampler, summarizing model fits, visualizing covariate effects and obtaining predictions for new responses or their means given feature/covariate vectors.",
    "author": [
      {
        "name": "Georgios Papageorgiou",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-069.zip\nCRAN packages used\nBNSP, bamlss, spikeSlabGAM, brms, gamboostLSS, mgcv, coda, ggplot2, plot3D, threejs, colorspace, np, gamair, lattice\nCRAN Task Views implied by cited packages\nBayesian, Graphics, Econometrics, Environmetrics, Phylogenetics, SocialSciences, gR, MachineLearning, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-079/",
    "title": "The politeness Package: Detecting Politeness in Natural Language",
    "description": "This package provides tools to extract politeness markers in English natural language. It also allows researchers to easily visualize and quantify politeness between groups of documents. This package combines and extends prior research on the linguistic markers of politeness (Brown and Levinson, 1987; Danescu-Niculescu-Mizil et al., 2013; Voigt et al., 2017). We demonstrate two applications for detecting politeness in natural language during consequential social interactions— distributive negotiations, and speed dating.",
    "author": [
      {
        "name": "Michael Yeomans",
        "url": {}
      },
      {
        "name": "Alejandro Kantor",
        "url": {}
      },
      {
        "name": "Dustin Tingley",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-079.zip\nCRAN packages used\ntidytext, tm, quanteda, coreNLP, spacyR, SentimentAnalysis, syuzhet, topicmodeling, stm, glmnet, textir, hunspell, data.table, politeness\nCRAN Task Views implied by cited packages\nNaturalLanguageProcessing, HighPerformanceComputing, Finance, MachineLearning, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-080/",
    "title": "Consistency Cubes: a fast, efficient method for exact Boolean minimization.",
    "description": "A lot of effort has been spent over the past few decades in the QCA methodology field, to develop efficient Boolean minimization algorithms to derive an exact, and more importantly complete list of minimal prime implicants that explain the initial, observed positive configurations. As the complexity grows exponentially with every new condition, the required computer memory goes past the current computer resources and the polynomial time required to solve this problem quickly grows towards infinity. This paper introduces a new alternative to the existing non-polynomial attempts. It completely solves the memory problem, and preliminary tests show it is exponentially hundreds of time faster than eQMC, the current “best” algorithm for QCA in R, and probes into a territory where it competes and even outperforms engineering algorithms such as Espresso, for exact minimizations. While speed is not much of an issue now (eQMC is fast enough for simple data), it might prove to be essential when further developing towards all possible temporal orders, or searching for configurations in panel data over time, combined with / or automatic detection of difficult counterfactuals etc.",
    "author": [
      {
        "name": "Adrian Dusa",
        "url": {}
      }
    ],
    "date": "2018-12-08",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-080.zip\nCRAN packages used\nQCA, lpSolve, venn, LogicOpt\nCRAN Task Views implied by cited packages\nOptimization\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:32+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-044/",
    "title": "revengc: An R package to Reverse Engineer Summarized Data",
    "description": "Decoupled (e.g. separate averages) and censored (e.g. > 100 species) variables are continually reported by many well-established organizations, such as the World Health Organization (WHO), Centers for Disease Control and Prevention (CDC), and World Bank. The challenge therefore is to infer what the original data could have been given summarized information. We present an R package that reverse engineers censored and/or decoupled data with two main functions. The cnbinom.pars() function estimates the average and dispersion parameter of a censored univariate frequency table. The rec() function reverse engineers summarized data into an uncensored bivariate table of probabilities.",
    "author": [
      {
        "name": "Samantha Duchscherer",
        "url": {}
      },
      {
        "name": "Robert Stewart",
        "url": {}
      },
      {
        "name": "Marie Urban",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-044.zip\nCRAN packages used\nrevengc, truncdist, mipfp\nCRAN Task Views implied by cited packages\nOfficialStatistics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-045/",
    "title": "Basis-Adaptive Selection Algorithm in dr-package",
    "description": "Sufficient dimension reduction (SDR) turns out to be a useful dimension reduction tool in high-dimensional regression analysis. Weisberg (2002) developed the dr-package to implement the four most popular SDR methods. However, the package does not provide any clear guidelines as to which method should be used given a data. Since the four methods may provide dramatically different dimension reduction results, the selection in the dr-package is problematic for statistical practitioners. In this paper, a basis-adaptive selection algorithm is developed in order to relieve this issue. The basic idea is to select an SDR method that provides the highest correlation between the basis estimates obtained by the four classical SDR methods. A real data example and numerical studies confirm the practical usefulness of the developed algorithm.",
    "author": [
      {
        "name": "Jae Keun Yoo",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-045.zip\nCRAN packages used\ndr\nCRAN Task Views implied by cited packages\nMultivariate, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-046/",
    "title": "fICA: FastICA Algorithms and Their Improved Variants",
    "description": "In independent component analysis (ICA) one searches for mutually independent non gaussian latent variables when the components of the multivariate data are assumed to be linear combinations of them. Arguably, the most popular method to perform ICA is FastICA. There are two classical versions, the deflation-based FastICA where the components are found one by one, and the symmetric FastICA where the components are found simultaneously. These methods have been implemented previously in two R packages, fastICA and ica. We present the R package fICA and compare it to the other packages. Additional features in fICA include optimization of the extraction order in the deflation-based version, possibility to use any nonlinearity function, and improvement to convergence of the deflation-based algorithm. The usage of the package is demonstrated by applying it to the real ECG data of a pregnant woman.",
    "author": [
      {
        "name": "Jari Miettinen",
        "url": {}
      },
      {
        "name": "Klaus Nordhausen",
        "url": {}
      },
      {
        "name": "Sara Taskinen",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-046.zip\nCRAN packages used\nfastICA, ica, fICA, BSSasymp\nCRAN Task Views implied by cited packages\nPsychometrics, ChemPhys, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-047/",
    "title": "Spatial Uncertainty Propagation Analysis with the spup R Package",
    "description": "Many environmental and geographical models, such as those used in land degradation, agro ecological and climate studies, make use of spatially distributed inputs that are known imperfectly. The R package spup provides functions for examining the uncertainty propagation from input data and model parameters onto model outputs via the environmental model. The functions include uncertainty model specification, stochastic simulation and propagation of uncertainty using Monte Carlo (MC) techniques. Uncertain variables are described by probability distributions. Both numerical and categorical data types are handled. The package also accommodates spatial auto-correlation within a variable and cross-correlation between variables. The MC realizations may be used as input to the environmental models written in or called from R. This article provides theoretical background and three worked examples that guide users through the application of spup.",
    "author": [
      {
        "name": "Kasia Sawicka",
        "url": {}
      },
      {
        "name": "Gerard B.M. Heuvelink",
        "url": {}
      },
      {
        "name": "Dennis J.J. Walvoort",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-047.zip\nCRAN packages used\npropagate, errors, metRology, spup, gstat, stats, mvtnorm, whisker, shiny\nCRAN Task Views implied by cited packages\nChemPhys, WebTechnologies, Distributions, Finance, Multivariate, Spatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-048/",
    "title": "clustMixType: User-Friendly Clustering of Mixed-Type Data in R",
    "description": "Clustering algorithms are designed to identify groups in data where the traditional emphasis has been on numeric data. In consequence, many existing algorithms are devoted to this kind of data even though a combination of numeric and categorical data is more common in most business applications. Recently, new algorithms for clustering mixed-type data have been proposed based on Huang’s k-prototypes algorithm. This paper describes the R package clustMixType which provides an implementation of k-prototypes in R.",
    "author": [
      {
        "name": "Gero Szepannek",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-048.zip\nCRAN packages used\ngower, cluster, CluMix, flexclust, fpc, clustMD, kamila, clustMixType, klaR, wesanderson, clusteval\nCRAN Task Views implied by cited packages\nCluster, Multivariate, Environmetrics, Graphics, MachineLearning, Robust\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-049/",
    "title": "Stilt: Easy Emulation of Time Series AR(1) Computer Model Output in Multidimensional Parameter Space",
    "description": "Statistically approximating or “emulating” time series model output in parameter space is a common problem in climate science and other fields. There are many packages for spatio-temporal modeling. However, they often lack focus on time series, and exhibit statistical complexity. Here, we present the R package stilt designed for simplified AR(1) time series Gaussian process emulation, and provide examples relevant to climate modelling. Notably absent is Markov chain Monte Carlo estimation – a challenging concept to many scientists. We keep the number of user choices to a minimum. Hence, the package can be useful pedagogically, while still applicable to real life emulation problems. We provide functions for emulator cross-validation, empirical coverage, prediction, as well as response surface plotting. While the examples focus on climate model emulation, the emulator is general and can be also used for kriging spatio-temporal data.",
    "author": [
      {
        "name": "Roman Olson",
        "url": {}
      },
      {
        "name": "Kelsey L. Ruckert",
        "url": {}
      },
      {
        "name": "Won Chang",
        "url": {}
      },
      {
        "name": "Klaus Keller",
        "url": {}
      },
      {
        "name": "Murali Haran",
        "url": {}
      },
      {
        "name": "Soon-Il An",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-049.zip\nCRAN packages used\ngstat, mlegp, spBayes, ramps, spTimer, RandomFields, stilt, fields, maps, spam, dotCall64\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, Bayesian, Multivariate, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-050/",
    "title": "SMM: An R Package for Estimation and Simulation of Discrete-time semi-Markov Models",
    "description": "Semi-Markov models, independently introduced by Lévy (1954), Smith (1955) and Takacs (1954), are a generalization of the well-known Markov models. For semi-Markov models, sojourn times can be arbitrarily distributed, while sojourn times of Markov models are constrained to be exponentially distributed (in continuous time) or geometrically distributed (in discrete time). The aim of this paper is to present the R package SMM, devoted to the simulation and estimation of discrete time multi-state semi-Markov and Markov models. For the semi-Markov case we have considered: parametric and non-parametric estimation; with and without censoring at the beginning and/or at the end of sample paths; one or several independent sample paths. Several discrete-time distributions are considered for the parametric estimation of sojourn time distributions of semi-Markov chains: Uniform, Geometric, Poisson, Discrete Weibull and Binomial Negative.",
    "author": [
      {
        "name": "Vlad Stefan Barbu",
        "url": {}
      },
      {
        "name": "Caroline Bérard",
        "url": {}
      },
      {
        "name": "Dominique Cellier",
        "url": {}
      },
      {
        "name": "Mathilde Sautreuil",
        "url": {}
      },
      {
        "name": "Nicolas Vergne",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nSMM, semiMarkov, hsmm, mhsmm\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-051/",
    "title": "ggplot2 Compatible Quantile-Quantile Plots in R",
    "description": "Q-Q plots allow us to assess univariate distributional assumptions by comparing a set of quantiles from the empirical and the theoretical distributions in the form of a scatterplot. To aid in the interpretation of Q-Q plots, reference lines and confidence bands are often added. We can also detrend the Q-Q plot so the vertical comparisons of interest come into focus. Various implementations of Q-Q plots exist in R, but none implements all of these features. qqplotr extends ggplot2 to provide a complete implementation of Q-Q plots. This paper introduces the plotting framework provided by qqplotr and provides multiple examples of how it can be used.",
    "author": [
      {
        "name": "Alexandre Almeida",
        "url": {}
      },
      {
        "name": "Adam Loy",
        "url": {}
      },
      {
        "name": "Heike Hofmann",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-051.zip\nCRAN packages used\nbase, lattice, car, ggplot2, qqplotr, stats, robustbase, boot\nCRAN Task Views implied by cited packages\nMultivariate, Econometrics, Graphics, Robust, SocialSciences, Finance, Optimization, Phylogenetics, Survival, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-052/",
    "title": "Forecast Combinations in R using the ForecastComb Package",
    "description": "This paper introduces the R package ForecastComb. The aim is to provide researchers and practitioners with a comprehensive implementation of the most common ways in which forecasts can be combined. The package in its current version covers 15 popular estimation methods for creating a combined forecasts – including simple methods, regression-based methods, and eigenvector-based methods. It also includes useful tools to deal with common challenges of forecast combination (e.g., missing values in component forecasts, or multicollinearity), and to rationalize and visualize the combination results.",
    "author": [
      {
        "name": "Christoph E. Weiss",
        "url": {}
      },
      {
        "name": "Eran Raviv",
        "url": {}
      },
      {
        "name": "Gernot Roetzer",
        "url": {}
      }
    ],
    "date": "2018-12-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-052.zip\nCRAN packages used\nBMA, opera, forecastHybrid, ForecastCombinations, GeomComb, quadprog, mtsdi, forecTheta\nCRAN Task Views implied by cited packages\nTimeSeries, Bayesian, Econometrics, OfficialStatistics, Optimization, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-040/",
    "title": "Profile Likelihood Estimation of the Correlation Coefficient in the Presence of Left, Right or Interval Censoring and Missing Data",
    "description": "We discuss implementation of a profile likelihood method for estimating a Pearson correla tion coefficient from bivariate data with censoring and/or missing values. The method is implemented in an R package clikcorr which calculates maximum likelihood estimates of the correlation coefficient when the data are modeled with either a Gaussian or a Student t-distribution, in the presence of left, right, or interval censored and/or missing data. The R package includes functions for conducting inference and also provides graphical functions for visualizing the censored data scatter plot and profile log likelihood function. The performance of clikcorr in a variety of circumstances is evaluated through extensive simulation studies. We illustrate the package using two dioxin exposure datasets.",
    "author": [
      {
        "name": "Yanming Li",
        "url": {}
      },
      {
        "name": "Brenda W. Gillespie",
        "url": {}
      },
      {
        "name": "Kerby Shedden",
        "url": {}
      },
      {
        "name": "John A. Gillespie",
        "url": {}
      }
    ],
    "date": "2018-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-040.zip\nCRAN packages used\nclikcorr, survival, mvtnorm\nCRAN Task Views implied by cited packages\nClinicalTrials, Distributions, Econometrics, Finance, Multivariate, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-041/",
    "title": "The utiml Package: Multi-label Classification in R",
    "description": "Learning classification tasks in which each instance is associated with one or more labels are known as multi-label learning. The implementation of multi-label algorithms, performed by different researchers, have several specificities, like input/output format, different internal functions, distinct programming language, to mention just some of them. As a result, current machine learning tools include only a small subset of multi-label decomposition strategies. The utiml package is a framework for the application of classification algorithms to multi-label data. Like the well known MULAN used with Weka, it provides a set of multi-label procedures such as sampling methods, transformation strategies, threshold functions, pre-processing techniques and evaluation metrics. The package was designed to allow users to easily perform complete multi-label classification experiments in the R environment. This paper describes the utiml API and illustrates its use in different multi-label classification scenarios.",
    "author": [
      {
        "name": "Adriano Rivolli",
        "url": {}
      },
      {
        "name": "Andre C. P. L. F. de Carvalho",
        "url": {}
      }
    ],
    "date": "2018-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-041.zip\nCRAN packages used\nmldr, mlr, MLPUGS, utiml, randomForest, C50, e1071, parallel\nCRAN Task Views implied by cited packages\nMachineLearning, Environmetrics, Cluster, Distributions, MissingData, Multivariate, Psychometrics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-042/",
    "title": "Dot-Pipe: an S3 Extensible Pipe for R",
    "description": "Pipe notation is popular with a large league of R users, with magrittr being the dominant realization. However, this should not be enough to consider piping in R as a settled topic that is not subject to further discussion, experimentation, or possibility for improvement. To promote innovation opportunities, we describe the wrapr R package and “dot-pipe” notation, a well behaved sequencing operator with S3 extensibility. We include a number of examples of using this pipe to interact with and extend other R packages.",
    "author": [
      {
        "name": "John Mount",
        "url": {}
      },
      {
        "name": "Nina Zumel",
        "url": {}
      }
    ],
    "date": "2018-08-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndata.table, magrittr, dplyr, future, rmonad, pipeR, backpipe, drake, wrapr, ggplot2, rquery\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, Databases, Finance, Graphics, ModelDeployment, Phylogenetics, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-043/",
    "title": "nsROC: An R package for Non-Standard ROC Curve Analysis",
    "description": "The receiver operating characteristic (ROC) curve is a graphical method which has become standard in the analysis of diagnostic markers, that is, in the study of the classification ability of a numerical variable. Most of the commercial statistical software provide routines for the standard ROC curve analysis. Of course, there are also many R packages dealing with the ROC estimation as well as other related problems. In this work we introduce the nsROC package which incorporates some new ROC curve procedures. Particularly: ROC curve comparison based on general distances among functions for both paired and unpaired designs; efficient confidence bands construction; a generalization of the curve considering different classification subsets than the one involved in the classical defini tion of the ROC curve; a procedure to deal with censored data in cumulative-dynamic ROC curve estimation for time-to-event outcomes; and a non-parametric ROC curve method for meta-analysis. This is the only R package which implements these particular procedures.",
    "author": [
      {
        "name": "Sonia Pérez-Fernández",
        "url": {}
      },
      {
        "name": "Pablo Martínez-Camblor",
        "url": {}
      },
      {
        "name": "Peter Filzmoser",
        "url": {}
      },
      {
        "name": "Norberto Corral",
        "url": {}
      }
    ],
    "date": "2018-08-17",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-043.zip\nCRAN packages used\npROC, ROCR, plotROC, fbroc, OptimalCutpoints, timeROC, survivalROC, HSROC, nsROC, sde, tdROC, survival\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, DifferentialEquations, Econometrics, Finance, MachineLearning, Multivariate, SocialSciences, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-038/",
    "title": "mmpf: Monte-Carlo Methods for Prediction Functions",
    "description": "Machine learning methods can often learn high-dimensional functions which generalize well but are not human interpretable. The mmpf package marginalizes prediction functions using Monte-Carlo methods, allowing users to investigate the behavior of these learned functions, as on a lower dimensional subset of input features: partial dependence and variations thereof. This makes machine learning methods more useful in situations where accurate prediction is not the only goal, such as in the social sciences where linear models are commonly used because of their interpretability. Many methods for estimating prediction functions produce estimated functions which are not directly human-interpretable because of their complexity: for example, they may include high dimensional interactions and/or complex nonlinearities. While a learning method’s capacity to automatically learn interactions and nonlinearities is attractive when the goal is prediction, there are many cases where users want good predictions and the ability to understand how predictions depend on the features. mmpf implements general methods for interpreting prediction functions using Monte-Carlo methods. These methods allow any function which generates predictions to be be interpreted. mmpf is currently used in other packages for machine learning like edarf and mlr (Jones and Linder, 2016; Bischl et al., 2016).",
    "author": [
      {
        "name": "Zachary M. Jones",
        "url": {}
      }
    ],
    "date": "2018-06-29",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmmpf, edarf\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-039/",
    "title": "dimRed and coRanking - Unifying Dimensionality Reduction in R",
    "description": "“Dimensionality reduction” (DR) is a widely used approach to find low dimensional and interpretable representations of data that are natively embedded in high-dimensional spaces. DR can be realized by a plethora of methods with different properties, objectives, and, hence, (dis)advantages. The resulting low-dimensional data embeddings are often difficult to compare with objective criteria. Here, we introduce the dimRed and coRanking packages for the R language. These open source software packages enable users to easily access multiple classical and advanced DR methods using a common interface. The packages also provide quality indicators for the embeddings and easy visualization of high dimensional data. The coRanking package provides the functionality for assessing DR methods in the co-ranking matrix framework. In tandem, these packages allow for uncovering complex structures high dimensional data. Currently 15 DR methods are available in the package, some of which were not previously available to R users. Here, we outline the dimRed and coRanking packages and make the implemented methods understandable to the interested reader.",
    "author": [
      {
        "name": "Guido Kraemer",
        "url": {}
      },
      {
        "name": "Markus Reichstein",
        "url": {}
      },
      {
        "name": "Miguel D. Mahecha",
        "url": {}
      }
    ],
    "date": "2018-06-29",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndimRed, coRanking, kernlab, vegan, RANN, igraph, lle, diffusionMap, MASS, igraph, Rtsne, fastICA, DRR\nCRAN Task Views implied by cited packages\nMultivariate, Optimization, Psychometrics, Spatial, Environmetrics, gR, Graphics, ChemPhys, Cluster, Distributions, Econometrics, MachineLearning, NaturalLanguageProcessing, NumericalMathematics, Phylogenetics, Robust, SocialSciences\nBioconductor packages used\npcaMethods\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-037/",
    "title": "Collections in R: Review and Proposal",
    "description": "R is a powerful tool for data processing, visualization, and modeling. However, R is slower than other languages used for similar purposes, such as Python. One reason for this is that R lacks base support for collections, abstract data types that store, manipulate, and return data (e.g., sets, maps, stacks). An exciting recent trend in the R extension ecosystem is the development of collection packages, packages that provide classes that implement common collections. At least 12 collection packages are available across the two major R extension repositories, the Comprehensive R Archive Network (CRAN) and Bioconductor. In this article, we compare collection packages in terms of their features, design philosophy, ease of use, and performance on benchmark tests. We demonstrate that, when used well, the data structures provided by collection packages are in many cases significantly faster than the data structures provided by base R. We also highlight current deficiencies among R collection packages and propose avenues of possible improvement. This article provides useful recommendations to R programmers seeking to speed up their programs and aims to inform the development of future collection-oriented software for R.",
    "author": [
      {
        "name": "Timothy Barry",
        "url": {}
      }
    ],
    "date": "2018-06-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRcpp, hashr, hashFunction, filehashSQLite, tictoc, DSL, bit64, bit, Oarray, sets, filehash, hash, hashmap, rstackdeque, rstack, liqueueR, dequer, flifo, listenv, stdvectors, microbenchmark, neuroim, FindMinIC\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, MedicalImaging, NumericalMathematics\nBioconductor packages used\nS4Vectors\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-036/",
    "title": "Small Area Disease Risk Estimation and Visualization Using R",
    "description": "Small area disease risk estimation is essential for disease prevention and control. In this paper, we demonstrate how R can be used to obtain disease risk estimates and quantify risk factors using areal data. We explain how to define disease risk models and how to perform Bayesian inference using the INLA package. We also show how to make interactive maps of estimates using the leaflet package to better understand the disease spatial patterns and communicate the results. We show an example of lung cancer risk in Pennsylvania, United States, in year 2002, and demonstrate that R represents an excellent tool for disease surveillance by enabling reproducible health data analysis.",
    "author": [
      {
        "name": "Paula Moraga",
        "url": {}
      }
    ],
    "date": "2018-06-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-036.zip\nCRAN packages used\nleaflet, SpatialEpi, spdep, ggplot2, flexdashboard, shiny, SpatialEpiApp, dygraphs, DT, rmarkdown\nCRAN Task Views implied by cited packages\nReproducibleResearch, Spatial, Econometrics, Graphics, Phylogenetics, TimeSeries, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-035/",
    "title": "RatingScaleReduction package: stepwise rating scale item reduction without predictability loss",
    "description": "This study presents an innovative method for reducing the number of rating scale items without predictability loss. The “area under the receiver operator curve” method (AUC ROC) is used for the stepwise method of reducing items of a rating scale. RatingScaleReduction R package contains the presented implementation. Differential evolution (a metaheuristic for optimization) was applied to one of the analyzed datasets to illustrate that the presented stepwise method can be used with other classifiers to reduce the number of rating scale items (variables). The targeted areas of application are decision making, data mining, machine learning, and psychometrics. Keywords: rating scale, receiver operator characteristic, ROC, AUC, scale reduction.",
    "author": [
      {
        "name": "Waldemar W. Koczkodaj",
        "url": {}
      },
      {
        "name": "Feng Li",
        "url": {}
      },
      {
        "name": "Alicja Wolny–Dominiak",
        "url": {}
      }
    ],
    "date": "2018-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\npROC, ROCR, RatingScaleReduction, DEoptim\nCRAN Task Views implied by cited packages\nMachineLearning, Multivariate, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-034/",
    "title": "ICSOutlier: Unsupervised Outlier Detection for Low-Dimensional Contamination Structure",
    "description": "Detecting outliers in a multivariate and unsupervised context is an important and ongoing problem notably for quality control. Many statistical methods are already implemented in R and are briefly surveyed in the present paper. But only a few lead to the accurate identification of potential outliers in the case of a small level of contamination. In this particular context, the Invariant Coordinate Selection (ICS) method shows remarkable properties for identifying outliers that lie on a low-dimensional subspace in its first invariant components. It is implemented in the ICSOutlier package. The main function of the package, ics.outlier, offers the possibility of labelling potential outliers in a completely automated way. Four examples, including two real examples in quality control, illustrate the use of the function. Comparing with several other approaches, it appears that ICS is generally as efficient as its competitors and shows an advantage in the context of a small proportion of outliers lying in a low-dimensional subspace. In quality control, the method may help in properly identifying some defective products while not detecting too many false positives.",
    "author": [
      {
        "name": "Aurore Archimbaud",
        "url": {}
      },
      {
        "name": "Klaus Nordhausen",
        "url": {}
      },
      {
        "name": "Anne Ruiz-Gazen",
        "url": {}
      }
    ],
    "date": "2018-05-30",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-034.zip\nCRAN packages used\nmvoutlier, CerioliOutlierDetection, rrcovHD, faoutlier, abodOutlier, HighDimOut, alphaOutlier, extremevalues, HDoutliers, outliers, DMwR2, ldbod, Rlof, depth, REPPlab, OutlierDC, pcadapt, rrcov, ICSOutlier, ICS, robustbase\nCRAN Task Views implied by cited packages\nRobust, Multivariate, OfficialStatistics, Psychometrics, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-033/",
    "title": "RealVAMS: An R Package for Fitting a Multivariate Value-added Model (VAM)",
    "description": "We present RealVAMS, an R package for fitting a generalized linear mixed model to multimembership data with partially crossed and partially nested random effects. RealVAMS utilizes a multivariate generalized linear mixed model with pseudo-likelihood approximation for fitting normally distributed continuous response(s) jointly with a binary outcome. In an educational context, the model is referred to as a multidimensional value-added model, which extends previous theory to estimate the relationships between potential teacher contributions toward different student outcomes and to allow the consideration of a binary, real-world outcome such as graduation. The simultaneous joint modeling of continuous and binary outcomes was not available prior to RealVAMS due to computational difficulties. In this paper, we discuss the multidimensional model, describe RealVAMS, and demonstrate the use of this package and its modeling options with an educational data set.",
    "author": [
      {
        "name": "Jennifer Broatch",
        "url": {}
      },
      {
        "name": "Jennifer Green",
        "url": {}
      },
      {
        "name": "Andrew Karl",
        "url": {}
      }
    ],
    "date": "2018-05-22",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-033.zip\nCRAN packages used\nRealVAMS, lme4\nCRAN Task Views implied by cited packages\nBayesian, Econometrics, Environmetrics, OfficialStatistics, Psychometrics, SocialSciences, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-018/",
    "title": "PanJen: An R package for Ranking Transformations in a Linear Regression",
    "description": "PanJen is an R-package for ranking transformations in linear regressions. It provides users with the ability to explore the relationship between a dependent variable and its independent variables. The package offers an easy and data-driven way to choose a functional form in multiple linear regression models by comparing a range of parametric transformations. The parametric functional forms are benchmarked against each other and a non-parametric transformation. The package allows users to generate plots that show the relation between a covariate and the dependent variable. Furthermore, PanJen will enable users to specify specific functional transformations, driven by a priori and theory-based hypotheses. The package supplies both model fits and plots that allow users to make informed choices on the functional forms in their regression. We show that the ranking in PanJen outperforms the Box-Tidwell transformation, especially in the presence of inefficiency, heteroscedasticity or endogeneity.",
    "author": [
      {
        "name": "Cathrine Ulla Jensen",
        "url": {}
      },
      {
        "name": "Toke Emil Panduro",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-018.zip\nCRAN packages used\nPanJen, mgcv\nCRAN Task Views implied by cited packages\nBayesian, Econometrics, Environmetrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-019/",
    "title": "Tackling Uncertainties of Species Distribution Model Projections with Package mopa",
    "description": "Species Distribution Models (SDMs) constitute an important tool to assist decision-making in environmental conservation and planning in the context of climate change. Nevertheless, SDM pro jections are affected by a wide range of uncertainty factors (related to training data, climate projections and SDM techniques), which limit their potential value and credibility. The new package mopa pro vides tools for designing comprehensive multi-factor SDM ensemble experiments, combining multiple sources of uncertainty (e.g. baseline climate, pseudo-absence realizations, SDM techniques, future projections) and allowing to assess their contribution to the overall spread of the ensemble projection. In addition, mopa is seamlessly integrated with the climate4R bundle and allows straightforward retrieval and post-processing of state-of-the-art climate datasets (including observations and climate change projections), thus facilitating the proper analysis of key uncertainty factors related to climate data.",
    "author": [
      {
        "name": "M. Iturbide",
        "url": {}
      },
      {
        "name": "J. Bedia",
        "url": {}
      },
      {
        "name": "J.M. Gutiérrez",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-019.zip\nCRAN packages used\nmopa, sdm, biomod2, dismo, SDMTools, raster, sp, e1071, stats, ranger, earth, tree, rpart, caret\nCRAN Task Views implied by cited packages\nMachineLearning, Multivariate, Environmetrics, Spatial, SpatioTemporal, Survival, Cluster, Distributions, HighPerformanceComputing, Psychometrics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-020/",
    "title": "FHDI: An R Package for Fractional Hot Deck Imputation",
    "description": "Fractional hot deck imputation (FHDI), proposed by Kalton and Kish (1984) and investigated by Kim and Fuller (2004), is a tool for handling item nonresponse in survey sampling. In FHDI, each missing item is filled with multiple observed values yielding a single completed data set for subsequent analyses. An R package FHDI is developed to perform FHDI and also the fully efficient fractional imputation (FEFI) method of (Fuller and Kim, 2005) to impute multivariate missing data with arbitrary missing patterns. FHDI substitutes missing items with a few observed values jointly obtained from a set of donors whereas the FEFI uses all the possible donors. This paper introduces FHDI as a tool for implementing the multivariate version of fractional hot deck imputation discussed in Im et al. (2015) as well as FEFI. For variance estimation of FHDI and FEFI, the Jackknife method is implemented, and replicated weights are provided as a part of the output.",
    "author": [
      {
        "name": "Jongho Im",
        "url": {}
      },
      {
        "name": "In Ho Cho",
        "url": {}
      },
      {
        "name": "Jae Kwang Kim",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-020.zip\nCRAN packages used\nmice, mi, Amelia, VIM, FHDI\nCRAN Task Views implied by cited packages\nOfficialStatistics, SocialSciences, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-021/",
    "title": "Bayesian Testing, Variable Selection and Model Averaging in Linear Models using R with BayesVarSel",
    "description": "In this paper, objective Bayesian methods for hypothesis testing and variable selection in linear models are considered. The focus is on BayesVarSel, an R package that computes posterior probabilities of hypotheses/models and provides a suite of tools to properly summarize the results. We introduce the usage of specific functions to compute several types of model averaging estimations and predictions weighted by posterior probabilities. BayesVarSel contains exact algorithms to perform fast computations in problems of small to moderate size and heuristic sampling methods to solve large problems. We illustrate the functionalities of the package with several data examples.",
    "author": [
      {
        "name": "Gonzalo Garcia-Donato",
        "url": {}
      },
      {
        "name": "Anabel Forte",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-021.zip\nCRAN packages used\nBayesVarSel, faraway, BayesFactor, BMS, mombf, BAS, BMA\nCRAN Task Views implied by cited packages\nBayesian, Econometrics, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-022/",
    "title": "onewaytests: An R Package for One-Way Tests in Independent Groups Designs",
    "description": "One-way tests in independent groups designs are the most commonly utilized statistical methods with applications on the experiments in medical sciences, pharmaceutical research, agri culture, biology, engineering, social sciences and so on. In this paper, we present the onewaytests package to investigate treatment effects on the dependent variable. The package offers the one-way tests in independent groups designs, which include ANOVA, Welch’s heteroscedastic F test, Welch’s heteroscedastic F test with trimmed means and Winsorized variances, Brown-Forsythe test, Alexander Govern test, James second order test and Kruskal-Wallis test. The package also provides pairwise comparisons, graphical approaches, and assesses variance homogeneity and normality of data in each group via tests and plots. A simulation study is also conducted to give recommendations for applied researchers on the selection of appropriate one-way tests under assumption violations. Furthermore, especially for non-R users, a user-friendly web application of the package is provided. This application is available at http://www.softmed.hacettepe.edu.tr/onewaytests.",
    "author": [
      {
        "name": "Osman Dag",
        "url": {}
      },
      {
        "name": "Anil Dolgun",
        "url": {}
      },
      {
        "name": "Naime Meric Konar",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-022.zip\nCRAN packages used\nonewaytests, onewaytests, stats\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-023/",
    "title": "Inventorymodel: an R Package for Centralized Inventory Problems",
    "description": "Inventory management of goods is an integral part of logistics systems; common to various economic sectors such as industry, agriculture and trade; and independent of production volume. In general, as companies seek to minimize economic losses, studies on problems of multi-agent inventory have increased in recent years. A multi-agent inventory problem is a situation in which several agents face individual inventory problems and agree to coordinate their orders with the objective of reducing their costs. The R package Inventorymodel allows the determination of both the optimal policy for some inventory situations with deterministic demands and the allocation of costs from a game-theoretic perspective. The required calculations may be computed for any number of agents although the computational complexity of this class of problems when the involved agents enlarge is not reduced. In this work, the different possibilities that the package offers are described and some examples of usage are also demonstrated.",
    "author": [
      {
        "name": "Alejandro Saavedra-Nieves",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-023.zip\nCRAN packages used\nInventorymodel, e1071, GameTheoryAllocation\nCRAN Task Views implied by cited packages\nCluster, Distributions, Environmetrics, MachineLearning, Multivariate, Psychometrics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-024/",
    "title": "R Package imputeTestbench to Compare Imputation Methods for Univariate Time Series",
    "description": "Missing observations are common in time series data and several methods are available to impute these values prior to analysis. Variation in statistical characteristics of univariate time series can have a profound effect on characteristics of missing observations and, therefore, the accuracy of different imputation methods. The imputeTestbench package can be used to compare the prediction accuracy of different methods as related to the amount and type of missing data for a user-supplied dataset. Missing data are simulated by removing observations completely at random or in blocks of different sizes depending on characteristics of the data. Several imputation algorithms are included with the package that vary from simple replacement with means to more complex interpolation methods. The testbench is not limited to the default functions and users can add or remove methods as needed. Plotting functions also allow comparative visualization of the behavior and effectiveness of different algorithms. We present example applications that demonstrate how the package can be used to understand differences in prediction accuracy between methods as affected by characteristics of a dataset and the nature of missing data.",
    "author": [
      {
        "name": "Marcus W Beck",
        "url": {}
      },
      {
        "name": "Neeraj Bokde",
        "url": {}
      },
      {
        "name": "Gualberto Asencio-Cortés",
        "url": {}
      },
      {
        "name": "Kishore Kulat",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-024.zip\nCRAN packages used\nimputeTestbench, dplyr, reshape2, tidyr, ggplot2, forecast, imputeTS, zoo, stats, datasets, Rcpp, matlabr\nCRAN Task Views implied by cited packages\nTimeSeries, Econometrics, Environmetrics, Finance, Graphics, HighPerformanceComputing, ModelDeployment, NumericalMathematics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-025/",
    "title": "rpostgis: Linking R with a PostGIS Spatial Database",
    "description": "With the proliferation of sensors and the ease of data collection from online sources, large datasets have become the norm in many scientific disciplines, and efficient data storage, management, and retrival is imperative for large research projects. Relational databases provide a solution, but in order to be useful, must be able to be linked to analysis and visualization tools, such as R. Here, we present a package intended to facilitate integration of R with the open-source database software PostgreSQL, with a focus on its spatial extension, PostGIS. The package rpostgis (version 1.4.1) provides methods for spatial data handling (vector and raster) between PostGIS-enabled databases and R, methods for R \"data.frame\"s storage in PostgreSQL, and a set of convenient wrappers for common database procedures. We thus expect rpostgis to be useful for both (1) existing users of spatial data in R and/or PostGIS, and (2) R users who have yet to adopt relational databases for their projects.",
    "author": [
      {
        "name": "David Bucklin",
        "url": {}
      },
      {
        "name": "Mathieu Basille",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-025.zip\nCRAN packages used\nrgdal, maptools, raster, RPostgreSQL, DBI, rpostgis, sp, rgeos, wkb, sf, rpostgisLT, adehabitatLT\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-026/",
    "title": "lba: An R Package for Latent Budget Analysis",
    "description": "The latent budget model is a mixture model for compositional data sets in which the entries, a contingency table, may be either realizations from a product multinomial distribution or distribution free. Based on this model, the latent budget analysis considers the interactions of two variables; the ex planatory (row) and the response (column) variables. The package lba uses expectation-maximization and active constraints method (ACM) to carry out, respectively, the maximum likelihood and the least squares estimation of the model parameters. It contains three main functions, lba which performs the analysis, goodnessfit for model selection and goodness of fit and the plotting functions plotcorr and plotlba used as a help in the interpretation of the results.",
    "author": [
      {
        "name": "Enio G. Jelihovschi",
        "url": {}
      },
      {
        "name": "Ivan Bezerra Allaman",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-026.zip\nCRAN packages used\nlba, alabama, plotrix, scatterplot3d, rgl, MASS\nCRAN Task Views implied by cited packages\nGraphics, Multivariate, Psychometrics, Distributions, Econometrics, Environmetrics, NumericalMathematics, Optimization, Robust, SocialSciences, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-027/",
    "title": "Semiparametric Generalized Linear Models with the gldrm Package",
    "description": "This paper introduces a new algorithm to estimate and perform inferences on a recently proposed and developed semiparametric generalized linear model (glm). Rather than selecting a particular parametric exponential family model, such as the Poisson distribution, this semiparametric glm assumes that the response is drawn from the more general exponential tilt family. The regression coefficients and unspecified reference distribution are estimated by maximizing a semiparametric like lihood. The new algorithm incorporates several computational stability and efficiency improvements over the algorithm originally proposed. In particular, the new algorithm performs well for either small or large support for the nonparametric response distribution. The algorithm is implemented in a new R package called gldrm.",
    "author": [
      {
        "name": "Michael J. Wurm",
        "url": {}
      },
      {
        "name": "Paul J. Rathouz",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-027.zip\nCRAN packages used\ngldrm\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-028/",
    "title": "LP Algorithms for Portfolio Optimization: The PortfolioOptim Package",
    "description": "The paper describes two algorithms for financial portfolio optimization with the following risk measures: CVaR, MAD, LSAD and dispersion CVaR. These algorithms can be applied to discrete distributions of asset returns since then the optimization problems can be reduced to linear programs. The first algorithm solves a simple recourse problem as described by Haneveld using Benders de composition method. The second algorithm finds an optimal portfolio with the smallest distance to a given benchmark portfolio and is an adaptation of the least norm solution (called also normal solution) of linear programs due to Zhao and Li. The algorithms are implemented in R in the package PortfolioOptim.",
    "author": [
      {
        "name": "Andrzej Palczewski",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-028.zip\nCRAN packages used\nfPortfolio, PortfolioAnalytics, Rglpk, quadprog, DEoptim, GenSA, psoptim, parma, nloptr, PortfolioOptim\nCRAN Task Views implied by cited packages\nOptimization, Finance\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-029/",
    "title": "Welfare, Inequality and Poverty Analysis with rtip: An Approach Based on Stochastic Dominance",
    "description": "Disparities in economic welfare, inequality and poverty across and within countries are of great interest to sociologists, economists, researchers, social organizations and political scientists. Information about these topics is commonly based on surveys. We present a package called rtip that implements techniques based on stochastic dominance to make unambiguous comparisons, in terms of welfare, poverty and inequality, among income distributions. Besides providing point estimates and confidence intervals for the most commonly used indicators of these characteristics, the package rtip estimates the usual Lorenz curve, the generalized Lorenz curve, the TIP (Three I’s of Poverty) curve and allows to test statistically whether one curve is dominated by another.",
    "author": [
      {
        "name": "Angel Berihuete",
        "url": {}
      },
      {
        "name": "Carmen D. Ramos",
        "url": {}
      },
      {
        "name": "Miguel A. Sordo",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-029.zip\nCRAN packages used\nrtip, IC2, ineq, laeken, boot\nCRAN Task Views implied by cited packages\nOfficialStatistics, Econometrics, Optimization, SocialSciences, Survival, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-031/",
    "title": "SetMethods: an Add-on R Package for Advanced QCA",
    "description": "This article presents the functionalities of the R package SetMethods, aimed at performing advanced set-theoretic analyses. This includes functions for performing set-theoretic multi-method research, set-theoretic theory evaluation, Enhanced Standard Analysis, diagnosing the impact of temporal, spatial, or substantive clusterings of the data on the results obtained via Qualitative Com parative Analysis (QCA), indirect calibration, and visualising QCA results via XY plots or radar charts. Each functionality is presented in turn, the conceptual idea and the logic behind the procedure being first summarized, and afterwards illustrated with data from Schneider et al. (2010).",
    "author": [
      {
        "name": "Ioana-Elena Oana",
        "url": {}
      },
      {
        "name": "Carsten Q. Schneider",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-031.zip\nCRAN packages used\nQCA, SetMethods\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-032/",
    "title": "HRM: An R Package for Analysing High-dimensional Multi-factor Repeated Measures",
    "description": "High-dimensional longitudinal data pose a serious challenge for statistical inference as many test statistics cannot be computed for high-dimensional data, or they do not maintain the nominal type-I error rate, or have very low power. Therefore, it is necessary to derive new inference methods capable of dealing with high dimensionality, and to make them available to statistics practitioners. One such method is implemented in the package HRM described in this article. This new method uses a similar approach as the Welch-Satterthwaite t-test approximation and works very well for high-dimensional data as long as the data distribution is not too skewed or heavy-tailed. The package also provides a GUI to offer an easy way to apply the methods.",
    "author": [
      {
        "name": "Martin Happ",
        "url": {}
      },
      {
        "name": "Solomon W. Harrar",
        "url": {}
      },
      {
        "name": "Arne C. Bathke",
        "url": {}
      }
    ],
    "date": "2018-05-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nHRM, ggplot2, data.table, RGtk2, RGtk2Extras, cairoDevice, xtable, longitudinal, MANOVA.RM\nCRAN Task Views implied by cited packages\nGraphics, Finance, HighPerformanceComputing, Phylogenetics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-017/",
    "title": "Advanced Bayesian Multilevel Modeling with the R Package brms",
    "description": "The brms package allows R users to easily specify a wide range of Bayesian single-level and multilevel models which are fit with the probabilistic programming language Stan behind the scenes. Several response distributions are supported, of which all parameters (e.g., location, scale, and shape) can be predicted. Non-linear relationships may be specified using non-linear predictor terms or semi-parametric approaches such as splines or Gaussian processes. Multivariate models can be fit as well. To make all of these modeling options possible in a multilevel framework, brms provides an intuitive and powerful formula syntax, which extends the well known formula syntax of lme4. The purpose of the present paper is to introduce this syntax in detail and to demonstrate its usefulness with four examples, each showing relevant aspects of the syntax.",
    "author": [
      {
        "name": "Paul-Christian Bürkner",
        "url": {}
      }
    ],
    "date": "2018-05-18",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nbrms, lme4, rstanarm, MCMCglmm, mgcv, nlme, afex, loo, gamlss.data, bridgesampling\nCRAN Task Views implied by cited packages\nBayesian, SocialSciences, Econometrics, Environmetrics, Psychometrics, OfficialStatistics, SpatioTemporal, ChemPhys, Finance, Phylogenetics, Spatial, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-005/",
    "title": "Support Vector Machines for Survival Analysis with R",
    "description": "This article introduces the R package survivalsvm, implementing support vector machines for survival analysis. Three approaches are available in the package: The regression approach takes censoring into account when formulating the inequality constraints of the support vector problem. In the ranking approach, the inequality constraints set the objective to maximize the concordance index for comparable pairs of observations. The hybrid approach combines the regression and ranking constraints in a single model. We describe survival support vector machines and their implementation, provide examples and compare the prediction performance with the Cox proportional hazards model, random survival forests and gradient boosting using several real datasets. On these datasets, survival support vector machines perform on par with the reference methods.",
    "author": [
      {
        "name": "Césaire J. K. Fouodo",
        "url": {}
      },
      {
        "name": "Inke R. König",
        "url": {}
      },
      {
        "name": "Claus Weihs",
        "url": {}
      },
      {
        "name": "Andreas Ziegler",
        "url": {}
      },
      {
        "name": "Marvin N. Wright",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-005.zip\nCRAN packages used\nsurvivalsvm, kernlab, pracma, quadprog, Matrix, randomForestSRC, mboost, mlr, ggplot2, tikzDevice\nCRAN Task Views implied by cited packages\nMachineLearning, Multivariate, NumericalMathematics, Optimization, Survival, Cluster, DifferentialEquations, Econometrics, Graphics, HighPerformanceComputing, NaturalLanguageProcessing, Phylogenetics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-008/",
    "title": "Nonparametric Independence Tests and k-sample Tests for Large Sample Sizes Using Package HHG",
    "description": "Nonparametric tests of independence and k-sample tests are ubiquitous in modern applica tions, but they are typically computationally expensive. We present a family of nonparametric tests that are computationally efficient and powerful for detecting any type of dependence between a pair of univariate random variables. The computational complexity of the suggested tests is sub-quadratic in sample size, allowing calculation of test statistics for millions of observations. We survey both algorithms and the HHG package in which they are implemented, with usage examples showing the implementation of the proposed tests for both the independence case and the k-sample problem. The tests are compared to existing nonparametric tests via several simulation studies comparing both runtime and power. Special focus is given to the design of data structures used in implementation of the tests. These data structures can be useful for developers of nonparametric distribution-free tests.",
    "author": [
      {
        "name": "Barak Brill",
        "url": {}
      },
      {
        "name": "Yair Heller",
        "url": {}
      },
      {
        "name": "Ruth Heller",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-008.zip\nCRAN packages used\nHmisc, infotheo, entropy, minerva, dHSIC, energy, HHG, kernlab, dslice, rbenchmark, doRNG\nCRAN Task Views implied by cited packages\nMultivariate, Bayesian, ClinicalTrials, Cluster, Econometrics, HighPerformanceComputing, MachineLearning, NaturalLanguageProcessing, OfficialStatistics, Optimization, ReproducibleResearch, SocialSciences\nBioconductor packages used\nminet\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-009/",
    "title": "Simple Features for R: Standardized Support for Spatial Vector Data",
    "description": "Simple features are a standardized way of encoding spatial vector data (points, lines, polygons) in computers. The sf package implements simple features in R, and has roughly the same capacity for spatial vector data as packages sp, rgeos, and rgdal. We describe the need for this package, its place in the R package ecosystem, and its potential to connect R to other computer systems. We illustrate this with examples of its use.",
    "author": [
      {
        "name": "Edzer Pebesma",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-009.zip\nCRAN packages used\nsf, sp, rgdal, rgeos, tidyverse, dplyr, ggplot2, lwgeom, geosphere, s2, raster, Rcpp\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, Graphics, HighPerformanceComputing, ModelDeployment, NumericalMathematics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-010/",
    "title": "Pstat: An R Package to Assess Population Differentiation in Phenotypic Traits",
    "description": "The package Pstat calculates PST values to assess differentiation among populations from a set of quantitative traits and provides bootstrapped distributions and confidence intervals for PST . Variations of PST as a function of the parameter c/h2 are studied as well. The package implements different transformations of the measured phenotypic traits to eliminate variation resulting from allometric growth, including calculation of residuals from linear regression, Reist standardization, and the Aitchison transformation.",
    "author": [
      {
        "name": "Stéphane Blondeau Da Silva",
        "url": {}
      },
      {
        "name": "Anne Da Silva",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-010.zip\nCRAN packages used\nPstat, diveRsity, hierfstat\nCRAN Task Views implied by cited packages\nGenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-011/",
    "title": "Approximating the Sum of Independent Non-Identical Binomial Random Variables",
    "description": "The distribution of the sum of independent non-identical binomial random variables is frequently encountered in areas such as genomics, healthcare, and operations research. Analytical solutions for the density and distribution are usually cumbersome to find and difficult to compute. Several methods have been developed to approximate the distribution, among which is the saddlepoint approximation. However, implementation of the saddlepoint approximation is non-trivial. In this paper, we implement the saddlepoint approximation in the sinib package and provide two examples to illustrate its usage. One example uses simulated data while the other uses real-world healthcare data. The sinib package addresses the gap between the theory and the implementation of approximating the sum of independent non-identical binomials.",
    "author": [
      {
        "name": "Boxiang Liu",
        "url": {}
      },
      {
        "name": "Thomas Quertermous",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-011.zip\nCRAN packages used\nstats, EQL, sinib\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-012/",
    "title": "cchs: An R Package for Stratified Case-Cohort Studies",
    "description": "The cchs package contains a function, also called cchs, for analyzing data from a stratified case-cohort study, as used in epidemiology. For data from this type of study, cchs calculates Estimator III of Borgan et al. (2000), which is a score-unbiased estimator for the regression coefficients in the Cox proportional hazards model. From the user’s point of view, the function is similar to coxph (in the survival package) and other widely used model-fitting functions. Convenient software has not previously been available for Estimator III since it is complicated to calculate. SAS and S-Plus code-fragments for the calculation have been published, but cchs is easier to use and more efficient in terms of time and memory, and can cope with much larger datasets. It also avoids several minor approximations and simplifications.",
    "author": [
      {
        "name": "Edmund Jones",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-012.zip\nCRAN packages used\ncchs, survival, cchs, survival, survey, NestedCohort\nCRAN Task Views implied by cited packages\nSurvival, SocialSciences, ClinicalTrials, Econometrics, OfficialStatistics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-013/",
    "title": "InfoTrad: An R package for estimating the probability of informed trading",
    "description": "The purpose of this paper is to introduce the R package InfoTrad for estimating the proba bility of informed trading (PIN) initially proposed by Easley et al. (1996). PIN is a popular information asymmetry measure that proxies the proportion of informed traders in the market. This study provides a short survey on alternative estimation techniques for the PIN. There are many problems documented in the existing literature in estimating PIN. InfoTrad package aims to address two problems. First, the sequential trading structure proposed by Easley et al. (1996) and later extended by Easley et al. (2002) is prone to sample selection bias for stocks with large trading volumes, due to floating point exception. This problem is solved by different factorizations provided by Easley et al. (2010) (EHO factorization) and Lin and Ke (2011) (LK factorization). Second, the estimates are prone to bias due to boundary solutions. A grid-search algorithm (YZ algorithm) is proposed by Yan and Zhang (2012) to overcome the bias introduced due to boundary estimates. In recent years, clustering algorithms have become popular due to their flexibility in quickly handling large data sets. Gan et al. (2015) propose an algorithm (GAN algorithm) to estimate PIN using hierarchical agglomerative clustering which is later extended by Ersan and Alici (2016) (EA algorithm). The package InfoTrad offers LK and EHO factorizations given an input matrix and initial parameter vector. In addition, these factorizations can be used to estimate PIN through YZ algorithm, GAN algorithm and EA algorithm.",
    "author": [
      {
        "name": "Duygu Çelik",
        "url": {}
      },
      {
        "name": "Murat Tiniç",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-013.zip\nCRAN packages used\nInfoTrad, FinAsym, PIN, nloptr\nCRAN Task Views implied by cited packages\nFinance, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-014/",
    "title": "Generalized Additive Model Multiple Imputation by Chained Equations With Package ImputeRobust",
    "description": "Data analysis, common to all empirical sciences, often requires complete data sets. Unfortu nately, real world data collection will usually result in data values not being observed. We present a package for robust multiple imputation (the ImputeRobust package) that allows the use of generalized additive models for location, scale, and shape in the context of chained equations. The paper describes the basics of the imputation technique which builds on a semi-parametric regression model (GAMLSS) and the algorithms and functions provided with the corresponding package. Furthermore, some illustrative examples are provided.",
    "author": [
      {
        "name": "Daniel Salfran",
        "url": {}
      },
      {
        "name": "Martin Spiess",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-014.zip\nCRAN packages used\nImputeRobust, mice, gamlss\nCRAN Task Views implied by cited packages\nEconometrics, Multivariate, OfficialStatistics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-015/",
    "title": "MGLM: An R Package for Multivariate Categorical Data Analysis",
    "description": "Data with multiple responses is ubiquitous in modern applications. However, few tools are available for regression analysis of multivariate counts. The most popular multinomial-logit model has a very restrictive mean-variance structure, limiting its applicability to many data sets. This article introduces an R package MGLM, short for multivariate response generalized linear models, that expands the current tools for regression analysis of polytomous data. Distribution fitting, random number generation, regression, and sparse regression are treated in a unifying framework. The algorithm, usage, and implementation details are discussed.",
    "author": [
      {
        "name": "Juhyun Kim",
        "url": {}
      },
      {
        "name": "Yiwen Zhang",
        "url": {}
      },
      {
        "name": "Joshua Day",
        "url": {}
      },
      {
        "name": "Hua Zhou",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-015.zip\nCRAN packages used\nMGLM, VGAM, glmnet, dirmult, parallel, isoform, glmc\nCRAN Task Views implied by cited packages\nDistributions, Survival, Econometrics, Environmetrics, ExtremeValue, MachineLearning, Multivariate, Psychometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-016/",
    "title": "ArCo: An R package to Estimate Artificial Counterfactuals",
    "description": "In this paper we introduce the ArCo package for R which consists of a set of functions to implement the the Artificial Counterfactual (ArCo) methodology to estimate causal effects of an intervention (treatment) on aggregated data and when a control group is not necessarily available. The ArCo method is a two-step procedure, where in the first stage a counterfactual is estimated from a large panel of time series from a pool of untreated peers. In the second-stage, the average treatment effect over the post-intervention sample is computed. Standard inferential procedures are available. The package is illustrated with both simulated and real datasets.",
    "author": [
      {
        "name": "Yuri R. Fonseca",
        "url": {}
      },
      {
        "name": "Ricardo P. Masini",
        "url": {}
      },
      {
        "name": "Marcelo C. Medeiros",
        "url": {}
      },
      {
        "name": "Gabriel F. R. Vasconcelos",
        "url": {}
      }
    ],
    "date": "2018-05-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-016.zip\nCRAN packages used\nArCo, boot, glmnet, Synth\nCRAN Task Views implied by cited packages\nSurvival, Econometrics, MachineLearning, Optimization, SocialSciences, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-001/",
    "title": "A System for an Accountable Data Analysis Process in R",
    "description": "Efficiently producing transparent analyses may be difficult for beginners or tedious for the experienced. This implies a need for computing systems and environments that can efficiently satisfy reproducibility and accountability standards. To this end, we have developed a system, R package, and R Shiny application called adapr (Accountable Data Analysis Process in R) that is built on the principle of accountable units. An accountable unit is a data file (statistic, table or graphic) that can be associated with a provenance, meaning how it was created, when it was created and who created it, and this is similar to the ’verifiable computational results’ (VCR) concept proposed by Gavish and Donoho. Both accountable units and VCRs are version controlled, sharable, and can be incorporated into a collaborative project. However, accountable units use file hashes and do not involve watermarking or public repositories like VCRs. Reproducing collaborative work may be highly complex, requiring repeating computations on multiple systems from multiple authors; however, determining the provenance of each unit is simpler, requiring only a search using file hashes and version control systems.",
    "author": [
      {
        "name": "Jonathan Gelfond",
        "url": {}
      },
      {
        "name": "Martin Goros",
        "url": {}
      },
      {
        "name": "Brian Hernandez",
        "url": {}
      },
      {
        "name": "Alex Bokov",
        "url": {}
      }
    ],
    "date": "2018-05-15",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nknitr, rmarkdown, cacher, archivist, adapr, packrat\nCRAN Task Views implied by cited packages\nReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-002/",
    "title": "GrpString: An R Package for Analysis of Groups of Strings",
    "description": "The R package GrpString was developed as a comprehensive toolkit for quantitatively analyzing and comparing groups of strings. It offers functions for researchers and data analysts to prepare strings from event sequences, extract common patterns from strings, and compare patterns be tween string vectors. The package also finds transition matrices and complexity of strings, determines clusters in a string vector, and examines the statistical difference between two groups of strings.",
    "author": [
      {
        "name": "Hui Tang",
        "url": {}
      },
      {
        "name": "Elizabeth L. Day",
        "url": {}
      },
      {
        "name": "Molly B. Atkinson",
        "url": {}
      },
      {
        "name": "Norbert J. Pienta",
        "url": {}
      }
    ],
    "date": "2018-05-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-002.zip\nCRAN packages used\nstringr, stringb, stringi, gsubfn, uniqtag, stringdist, TraMineR, informR, GrpString, entropy\nCRAN Task Views implied by cited packages\nNaturalLanguageProcessing, OfficialStatistics, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-003/",
    "title": "Epistemic Game Theory: Putting Algorithms to Work",
    "description": "The aim of this study is to construct an epistemic model in which each rational choice under common belief in rationality is supplemented by a type which expresses such a belief. In practice, the finding of type depends on manual solution approach with some mathematical operations in scope of the theory. This approach becomes less convenient with the growth of the size of the game. To solve this difficulty, a linear programming model is constructed for two-player, static and non-cooperative games to find the type that is supporting that player’s rational choice is optimal under common belief in rationality and maximizing the utility of the game. Since the optimal choice would only be made from rational choices, it is first necessary to eliminate all strictly dominated choices. In real life, the games are usually large sized. Therefore, the elimination process should be performed in a computer environment. Since software related to game theory was mostly prepared with a result-oriented approach for some types of games, it was necessary to develop software to execute the iterated elimination method. With this regard, a program has been developed that determines the choices that are strictly dominated by pure and randomized choices in two-player games. Two functions named “esdc” and “type” are created by using R statistical programming language for the operations performed in both parts, and these functions are added to the content of an R package after its creation with the name EpistemicGameTheory.",
    "author": [
      {
        "name": "Bilge Başer",
        "url": {}
      },
      {
        "name": "Nalan Cinemre",
        "url": {}
      }
    ],
    "date": "2018-05-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-003.zip\nCRAN packages used\nEpistemicGameTheory, roxygen2, lpSolve\nCRAN Task Views implied by cited packages\nOptimization\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2018-004/",
    "title": "Residuals and Diagnostics for Binary and Ordinal Regression Models: An Introduction to the sure Package",
    "description": "Residual diagnostics is an important topic in the classroom, but it is less often used in practice by Brandon M. Greenwell, Andrew J. McCarthy, Bradley C. Boehmke, and Dungang Liu Introduction to the sure Package Ordinal Regression Models: An",
    "author": [
      {
        "name": "Brandon M. Greenwell",
        "url": {}
      },
      {
        "name": "Andrew J. McCarthy",
        "url": {}
      },
      {
        "name": "Bradley C. Boehmke",
        "url": {}
      },
      {
        "name": "Dungang Liu",
        "url": {}
      }
    ],
    "date": "2018-05-15",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2018-004.zip\nCRAN packages used\nMASS, VGAM, ordinal, rms, PResiduals, sure, ggplot2\nCRAN Task Views implied by cited packages\nEconometrics, Psychometrics, SocialSciences, Distributions, Environmetrics, Multivariate, Survival, ExtremeValue, Graphics, NumericalMathematics, Phylogenetics, ReproducibleResearch, Robust\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-067/",
    "title": "RQGIS: Integrating R with QGIS for Statistical Geocomputing",
    "description": "Integrating R with Geographic Information Systems (GIS) extends R’s statistical capabilities with numerous geoprocessing and data handling tools available in a GIS. QGIS is one of the most popular open-source GIS, and it furthermore integrates other GIS programs such as the System for Automated Geoscientific Analyses (SAGA) GIS and the Geographic Resources Analysis Support System (GRASS) GIS within a single software environment. This and its QGIS Python API makes it a perfect candidate for console-based geoprocessing. By establishing an interface, the R package RQGIS makes it possible to use QGIS as a geoprocessing workhorse from within R. Compared to other packages building a bridge to GIS (e.g., rgrass7, RSAGA, RPyGeo), RQGIS offers a wider range of geoalgorithms, and is often easier to use due to various convenience functions. Finally, RQGIS supports the seamless integration of Python code using reticulate from within R for improved extendability.",
    "author": [
      {
        "name": "Jannes Muenchow",
        "url": {}
      },
      {
        "name": "Patrick Schratz",
        "url": {}
      },
      {
        "name": "Alexander Brenning",
        "url": {}
      }
    ],
    "date": "2017-12-04",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-067.zip\nCRAN packages used\nmaptools, raster, sp, sf, mapview, mapmisc, osmar, dodgr, RArcInfo, rgrass7, mapedit, rgdal, rgeos, RSAGA, RPyGeo, RQGIS, reticulate, rPython, sperrorest, nlme, mgcv, spgrass6, leaflet\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, Econometrics, Environmetrics, NumericalMathematics, SocialSciences, Bayesian, ChemPhys, Finance, HighPerformanceComputing, OfficialStatistics, Psychometrics, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-068/",
    "title": "rpsftm: An R Package for Rank Preserving Structural Failure Time Models",
    "description": "Treatment switching in a randomised controlled trial occurs when participants change from their randomised treatment to the other trial treatment during the study. Failure to account for treatment switching in the analysis (i.e. by performing a standard intention-to-treat analysis) can lead to biased estimates of treatment efficacy. The rank preserving structural failure time model (RPSFTM) is a method used to adjust for treatment switching in trials with survival outcomes. The RPSFTM is due to Robins and Tsiatis (1991) and has been developed by White et al. (1997, 1999). The method is randomisation based and uses only the randomised treatment group, observed event times, and treatment history in order to estimate a causal treatment effect. The treatment effect, ψ, is estimated by balancing counter-factual event times (that would be observed if no treatment were received) between treatment groups. G-estimation is used to find the value of ψ such that a test statistic Z (ψ) = 0. This is usually the test statistic used in the intention-to-treat analysis, for example, the log rank test statistic. We present an R package, rpsftm, that implements the method.",
    "author": [
      {
        "name": "Annabel Allison",
        "url": {}
      },
      {
        "name": "Ian R White",
        "url": {}
      },
      {
        "name": "Simon Bond",
        "url": {}
      }
    ],
    "date": "2017-12-04",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-068.zip\nCRAN packages used\nipw, rpsftm, eha\nCRAN Task Views implied by cited packages\nSurvival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-066/",
    "title": "glmmTMB Balances Speed and Flexibility Among Packages for Zero-inflated Generalized Linear Mixed Modeling",
    "description": "Count data can be analyzed using generalized linear mixed models when observations are correlated in ways that require random effects. However, count data are often zero-inflated, containing more zeros than would be expected from the typical error distributions. We present a new package, glmmTMB, and compare it to other R packages that fit zero-inflated mixed models. The glmmTMB package fits many types of GLMMs and extensions, including models with continuously distributed responses, but here we focus on count responses. glmmTMB is faster than glmmADMB, MCMCglmm, and brms, and more flexible than INLA and mgcv for zero-inflated modeling. One unique feature of glmmTMB (among packages that fit zero-inflated mixed models) is its ability to estimate the Conway-Maxwell-Poisson distribution parameterized by the mean. Overall, its most appealing features for new users may be the combination of speed, flexibility, and its interface’s similarity to lme4.",
    "author": [
      {
        "name": "Mollie E. Brooks",
        "url": {}
      },
      {
        "name": "Kasper Kristensen",
        "url": {}
      },
      {
        "name": "Koen J. van Benthem",
        "url": {}
      },
      {
        "name": "Arni Magnusson",
        "url": {}
      },
      {
        "name": "Casper W. Berg",
        "url": {}
      },
      {
        "name": "Anders Nielsen",
        "url": {}
      },
      {
        "name": "Hans J. Skaug",
        "url": {}
      },
      {
        "name": "Martin Mächler",
        "url": {}
      },
      {
        "name": "Benjamin M. Bolker",
        "url": {}
      }
    ],
    "date": "2017-12-01",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-066.zip\nCRAN packages used\nglmmTMB, pscl, MCMCglmm, mgcv, brms, gamlss, flexmix, MXM, VGAM, mgcv, TMB, devtools\nCRAN Task Views implied by cited packages\nEconometrics, Environmetrics, SocialSciences, Bayesian, Psychometrics, Survival, Cluster, Distributions, ExtremeValue, gR, MachineLearning, Multivariate, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-064/",
    "title": "carx: an R Package to Estimate Censored Autoregressive Time Series with Exogenous Covariates ",
    "description": "We implement in the R package carx a novel and computationally efficient quasi-likelihood method for estimating a censored autoregressive model with exogenous covariates. The proposed quasi-likelihood method reduces to maximum likelihood estimation in absence of censoring. The carx package contains many useful functions for practical data analysis with censored stochastic regression, including functions for outlier detection, model diagnostics, and prediction with censored time series data. We illustrate the capabilities of the carx package with simulations and an elaborate real data analysis.",
    "author": [
      {
        "name": "Chao Wang",
        "url": {}
      },
      {
        "name": "Kung-Sik Chan",
        "url": {}
      }
    ],
    "date": "2017-11-27",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-064.zip\nCRAN packages used\ncensReg, AER, NADA, VGAM, MCMCpack, cents, ARCensReg, carx, xts, TSA\nCRAN Task Views implied by cited packages\nSurvival, TimeSeries, Econometrics, Distributions, Multivariate, Psychometrics, Bayesian, Environmetrics, ExtremeValue, Finance, SocialSciences, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-065/",
    "title": "An Introduction to Rocker: Docker Containers for R",
    "description": "We describe the Rocker project, which provides a widely-used suite of Docker images with customized R environments for particular tasks. We discuss how this suite is organized, and how these tools can increase portability, scaling, reproducibility, and convenience of R users and developers.",
    "author": [
      {
        "name": "Carl Boettiger",
        "url": {}
      },
      {
        "name": "Dirk Eddelbuettel",
        "url": {}
      }
    ],
    "date": "2017-11-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\npackrat, rhub, tidyverse\nCRAN Task Views implied by cited packages\nReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-059/",
    "title": "Simulating Probabilistic Long-Term Effects in Models with Temporal Dependence",
    "description": "The R package pltesim calculates and depicts probabilistic long-term effects in binary models with temporal dependence variables. The package performs two tasks. First, it calculates the change in the probability of the event occurring given a change in a theoretical variable. Second, it calculates the rolling difference in the future probability of the event for two scenarios: one where the event occurred at a given time and one where the event does not occur. The package is consistent with the recent movement to depict meaningful and easy-to-interpret quantities of interest with the requisite measures of uncertainty. It is the first to make it easy for researchers to interpret shortand long-term effects of explanatory variables in binary autoregressive models, which can have important implications for the correct interpretation of these models.",
    "author": [
      {
        "name": "Christopher Gandrud",
        "url": {}
      },
      {
        "name": "Laron K. Williams",
        "url": {}
      }
    ],
    "date": "2017-11-22",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-059.zip\nCRAN packages used\npltesim, ggplot2, DAMisc\nCRAN Task Views implied by cited packages\nGraphics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-060/",
    "title": "ManlyMix: An R Package for Manly Mixture Modeling",
    "description": "Model-based clustering is a popular technique for grouping objects based on a finite mixture model. It has countless applications in different fields of study. The R package ManlyMix implements the Manly mixture model that allows modeling skewness within data groups and performs cluster analysis. ManlyMix is a powerful diagnostics tool that is capable of conducting investigation con cerning the normality of variables upon fitting of a Manly forward or backward model. Theoretical foundations as well as description of functions are provided. All features of the package are illus trated with examples in great detail. The analysis of real-life datasets demonstrates the flexibility and usefulness of the package.",
    "author": [
      {
        "name": "Xuwen Zhu",
        "url": {}
      },
      {
        "name": "Volodymyr Melnykov",
        "url": {}
      }
    ],
    "date": "2017-11-22",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nflowClust, mixsmsn, EMMIXskew, EMMIXuskew, mixsmsn, EMMIXskew, EMMIXuskew, flowClust, ManlyMix, ManlyMix\nCRAN Task Views implied by cited packages\nCluster\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-061/",
    "title": "Partial Rank Data with the hyper2 Package: Likelihood Functions for Generalized Bradley-Terry Models",
    "description": "Here I present the hyper2 package for generalized Bradley-Terry models and give examples from two competitive situations: single scull rowing, and the competitive cooking game show Mas terChef Australia. A number of natural statistical hypotheses may be tested straightforwardly using the software.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2017-11-22",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-061.zip\nCRAN packages used\nhyper2, aylmer\nCRAN Task Views implied by cited packages\nDistributions\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-062/",
    "title": "riskRegression: Predicting the Risk of an Event using Cox Regression Models",
    "description": "In the presence of competing risks a prediction of the time-dynamic absolute risk of an event can be based on cause-specific Cox regression models for the event and the competing risks (Benichou and Gail, 1990). We present computationally fast and memory optimized C++ functions with an R inter face for predicting the covariate specific absolute risks, their confidence intervals, and their confidence bands based on right censored time to event data. We provide explicit formulas for our implementation of the estimator of the (stratified) baseline hazard function in the presence of tied event times. As a by-product we obtain fast access to the baseline hazards (compared to survival::basehaz()) and predictions of survival probabilities, their confidence intervals and confidence bands. Confidence intervals and confidence bands are based on point-wise asymptotic expansions of the corresponding statistical functionals. The software presented here is implemented in the riskRegression package.",
    "author": [
      {
        "name": "Brice Ozenne",
        "url": {}
      },
      {
        "name": "Anne Lyngholm Sørensen",
        "url": {}
      },
      {
        "name": "Thomas Scheike",
        "url": {}
      },
      {
        "name": "Christian Torp-Pedersen",
        "url": {}
      },
      {
        "name": "Thomas            Alexander Gerds",
        "url": {}
      }
    ],
    "date": "2017-11-22",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-062.zip\nCRAN packages used\nsurvival, rms, riskRegression, mstate, rbenchmark, profvis, mets\nCRAN Task Views implied by cited packages\nSurvival, Econometrics, SocialSciences, ClinicalTrials, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-063/",
    "title": "openEBGM: An R Implementation of the Gamma-Poisson Shrinker Data Mining Model",
    "description": "We introduce the R package openEBGM, an implementation of the Gamma-Poisson Shrinker (GPS) model for identifying unexpected counts in large contingency tables using an empirical Bayes approach. The Empirical Bayes Geometric Mean (EBGM) and quantile scores are obtained from the GPS model estimates. openEBGM provides for the evaluation of counts using a number of different methods, including the model-based disproportionality scores, the relative reporting ratio (RR), and the proportional reporting ratio (PRR). Data squashing for computational efficiency and stratification for confounding variable adjustment are included. Application to adverse event detection is discussed.",
    "author": [
      {
        "name": "Travis Canida",
        "url": {}
      },
      {
        "name": "John Ihrie",
        "url": {}
      }
    ],
    "date": "2017-11-22",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-063.zip\nCRAN packages used\nopenEBGM, PhViD, mederrRank, tidyr, ggplot2, data.table\nCRAN Task Views implied by cited packages\nBayesian, Finance, Graphics, HighPerformanceComputing, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-057/",
    "title": "Allele Imputation and Haplotype Determination from Databases Composed of Nuclear Families",
    "description": "The alleHap package is designed for imputing genetic missing data and reconstruct non recombinant haplotypes from pedigree databases in a deterministic way. When genotypes of related individuals are available in a number of linked genetic markers, the program starts by identifying haplotypes compatible with the observed genotypes in those markers without missing values. If haplotypes are identified in parents or offspring, missing alleles can be imputed in subjects containing missing values. Several scenarios are analyzed: family completely genotyped, children partially genotyped and parents completely genotyped, children fully genotyped and parents containing entirely or partially missing genotypes, and founders and their offspring both only partially genotyped. The alleHap package also has a function to simulate pedigrees including all these scenarios. This article describes in detail how our package works for the desired applications, including illustrated explanations and easily reproducible examples.",
    "author": [
      {
        "name": "Nathan Medina-Rodríguez",
        "url": {}
      },
      {
        "name": "Ángelo Santana",
        "url": {}
      }
    ],
    "date": "2017-11-16",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nhaplo.ccs, haplo.stats, hsphase, linkim, rrBLUP, synbreed, alleHap\nCRAN Task Views implied by cited packages\nGenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-058/",
    "title": "rentrez: An R package for the NCBI eUtils API",
    "description": "The USA National Center for Biotechnology Information (NCBI) is one of the world’s most important sources of biological information. NCBI databases like PubMed and GenBank contain mil lions of records describing bibliographic, genetic, genomic, and medical data. Here I present rentrez, a package which provides an R interface to 50 NCBI databases. The package is well-documented, contains an extensive suite of unit tests and has an active user base. The programmatic interface to the NCBI provided by rentrez allows researchers to query databases and download or import particular records into R sessions for subsequent analysis. The complete nature of the package, its extensive test-suite and the fact the package implements the NCBI’s usage policies all make rentrez a powerful aid to developers of new packages that perform more specific tasks.",
    "author": [
      {
        "name": "David J. Winter",
        "url": {}
      }
    ],
    "date": "2017-11-16",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-058.zip\nCRAN packages used\nape, RISmed, pubmed.mineR, rentrez, reutils, rotl, fulltext, treemap\nCRAN Task Views implied by cited packages\nPhylogenetics, Environmetrics, Genetics, Graphics, OfficialStatistics, WebTechnologies\nBioconductor packages used\ngenomes, RMassBank, MeSHSim, genbankr\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-056/",
    "title": "Splitting It Up: The spduration Split-Population Duration Regression Package for Time-Varying Covariates",
    "description": "We present an implementation of split-population duration regression in the spduration (Beger et al., 2017) package for R that allows for time-varying covariates. The statistical model accounts for units that are immune to a certain outcome and are not part of the duration process the researcher is primarily interested in. We provide insights for when immune units exist, that can significantly increase the predictive performance compared to standard duration models. The package includes estimation and several post-estimation methods for split-population Weibull and log-logistic models. We provide an empirical application to data on military coups.",
    "author": [
      {
        "name": "Andreas Beger",
        "url": {}
      },
      {
        "name": "Daniel W. Hill",
        "url": {}
      },
      {
        "name": "Jr.",
        "url": {}
      },
      {
        "name": "Nils. W. Metternich",
        "url": {}
      },
      {
        "name": "Shahryar Minhas",
        "url": {}
      },
      {
        "name": "Michael D. Ward",
        "url": {}
      }
    ],
    "date": "2017-11-05",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspduration, survival, smcure\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Econometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-055/",
    "title": "mle.tools: An R Package for Maximum Likelihood Bias Correction",
    "description": "Recently, Mazucheli (2017) uploaded the package mle.tools to CRAN. It can be used for bias corrections of maximum likelihood estimates through the methodology proposed by Cox and Snell (1968). The main function of the package, coxsnell.bc(), computes the bias corrected maximum likelihood estimates. Although in general, the bias corrected estimators may be expected to have better sampling properties than the uncorrected estimators, analytical expressions from the formula proposed by Cox and Snell (1968) are either tedious or impossible to obtain. The purpose of this paper is twofolded: to introduce the mle.tools package, especially the coxsnell.bc() function; secondly, to compare, for thirty one continuous distributions, the bias estimates from the coxsnell.bc() function and the bias estimates from analytical expressions available in the literature. We also compare, for five distributions, the observed and expected Fisher information. Our numerical experiments show that the functions are efficient to estimate the biases by the Cox-Snell formula and for calculating the observed and expected Fisher information.",
    "author": [
      {
        "name": "Josmar Mazucheli",
        "url": {}
      },
      {
        "name": "André Felipe B. Menezes",
        "url": {}
      },
      {
        "name": "Saralees Nadarajah",
        "url": {}
      }
    ],
    "date": "2017-11-01",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-055.zip\nCRAN packages used\nmle.tools, fitdistrplus\nCRAN Task Views implied by cited packages\nDistributions, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-054/",
    "title": "ider: Intrinsic Dimension Estimation with R",
    "description": "In many data analyses, the dimensionality of the observed data is high while its intrinsic dimension remains quite low. Estimating the intrinsic dimension of an observed dataset is an essential preliminary step for dimensionality reduction, manifold learning, and visualization. This paper introduces an R package, named ider, that implements eight intrinsic dimension estimation methods, including a recently proposed method based on a second-order expansion of a probability mass function and a generalized linear model. The usage of each function in the package is explained with datasets generated using a function that is also included in the package.",
    "author": [
      {
        "name": "Hideitsu Hino",
        "url": {}
      }
    ],
    "date": "2017-10-31",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-054.zip\nCRAN packages used\nider, ider, ider, fractal, nonlinearTseries, tseriesChaos, fractal, nonlinearTseries, tseriesChaos, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, ider, Rcpp\nCRAN Task Views implied by cited packages\nTimeSeries, Finance, HighPerformanceComputing, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-052/",
    "title": "BayesBD: An R Package for Bayesian Inference on Image Boundaries",
    "description": "We present the BayesBD package providing Bayesian inference for boundaries of noisy images. The BayesBD package implements flexible Gaussian process priors indexed by the circle to recover the boundary in a binary or Gaussian noised image. The boundary recovered by BayesBD has the practical advantages of guaranteed geometric restrictions and convenient joint inferences under certain assumptions, in addition to its desirable theoretical property of achieving (nearly) minimax optimal rate in a way that is adaptive to the unknown smoothness. The core sampling tasks for our model have linear complexity, and are implemented in C++ for computational efficiency using packages Rcpp and RcppArmadillo. Users can access the full functionality of the package in both the command line and the corresponding shiny application. Additionally, the package includes numerous utility functions to aid users in data preparation and analysis of results. We compare BayesBD with selected existing packages using both simulations and real data applications, demonstrating the excellent performance and flexibility of BayesBD even when the observation contains complicated structural information that may violate its assumptions.",
    "author": [
      {
        "name": "Nicholas Syring",
        "url": {}
      },
      {
        "name": "Meng Li",
        "url": {}
      }
    ],
    "date": "2017-10-25",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nBayesBD, RcppArmadillo, shiny\nCRAN Task Views implied by cited packages\nNumericalMathematics, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-053/",
    "title": "Simulating Noisy, Nonparametric, and Multivariate Discrete Patterns",
    "description": "Requiring no analytical forms, nonparametric discrete patterns are flexible in representing complex relationships among random variables. This makes them increasingly useful for data-driven applications. However, there appears to be no software tools for simulating nonparametric discrete patterns, which prevents objective evaluation of statistical methods that discover discrete relationships from data. We present a simulator to generate nonparametric discrete functions as contingency tables. User can request strictly many-to-one functional patterns. The simulator can also produce contingency tables representing dependent non-functional and independent relationships. An option is provided to apply random noise to contingency tables. We demonstrate the utility of the simulator by showing the advantage of the FunChisq test over Pearson’s chi-square test in detecting functional patterns. This simulator, implemented in the function simulate_tables in the R package FunChisq (version 2.4.0 or greater), offers an important means to evaluate the performance of nonparametric statistical pattern discovery methods.",
    "author": [
      {
        "name": "Ruby Sharma",
        "url": {}
      },
      {
        "name": "Sajal Kumar",
        "url": {}
      },
      {
        "name": "Hua Zhong",
        "url": {}
      },
      {
        "name": "Mingzhou Song",
        "url": {}
      }
    ],
    "date": "2017-10-25",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-053.zip\nCRAN packages used\nrTableICC, FunChisq\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-046/",
    "title": "Visualization of Regression Models Using visreg",
    "description": "Regression models allow one to isolate the relationship between the outcome and an ex planatory variable while the other variables are held constant. Here, we introduce an R package, visreg, for the convenient visualization of this relationship via short, simple function calls. In addition to estimates of this relationship, the package also provides pointwise confidence bands and partial residuals to allow assessment of variability as well as outliers and other deviations from modeling assumptions. The package provides several options for visualizing models with interactions, including lattice plots, contour plots, and both static and interactive perspective plots. The implementation of the package is designed to be fully object-oriented and interface seamlessly with R’s rich collection of model classes, allowing a consistent interface for visualizing not only linear models, but generalized linear models, proportional hazards models, generalized additive models, robust regression models, and many more.",
    "author": [
      {
        "name": "Patrick Breheny",
        "url": {}
      },
      {
        "name": "Woodrow Burchett",
        "url": {}
      }
    ],
    "date": "2017-10-24",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nvisreg, rms, rockchalk, car, effects, plotmo, lattice, ggplot2, splines, rgl, MASS, mgcv, locfit, randomForest, e1071, gbm, lme4\nCRAN Task Views implied by cited packages\nSocialSciences, Econometrics, Environmetrics, MachineLearning, Multivariate, Graphics, Psychometrics, Survival, Bayesian, Distributions, SpatioTemporal, Cluster, Finance, NumericalMathematics, OfficialStatistics, Phylogenetics, ReproducibleResearch, Robust\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-047/",
    "title": "arulesViz: Interactive Visualization of Association Rules with R",
    "description": "Association rule mining is a popular data mining method to discover interesting relation ships between variables in large databases. An extensive toolbox is available in the R-extension package arules. However, mining association rules often results in a vast number of found rules, leaving the analyst with the task to go through a large set of rules to identify interesting ones. Sifting manually through extensive sets of rules is time-consuming and strenuous. Visualization and espe cially interactive visualization has a long history of making large amounts of data better accessible. The R-extension package arulesViz provides most popular visualization techniques for association rules. In this paper, we discuss recently added interactive visualizations to explore association rules and demonstrate how easily they can be used in arulesViz via a unified interface. With examples, we help to guide the user in selecting appropriate visualizations and interpreting the results.",
    "author": [
      {
        "name": "Michael Hahsler",
        "url": {}
      }
    ],
    "date": "2017-10-24",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\narulesViz, arules, DT, plotly, grid, visNetwork\nCRAN Task Views implied by cited packages\nMachineLearning, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-048/",
    "title": "liureg: A Comprehensive R Package for the Liu Estimation of Linear Regression Model with Collinear Regressors",
    "description": "The Liu regression estimator is now a commonly used alternative to the conventional ordinary least squares estimator that avoids the adverse effects in the situations when there exists a considerable degree of multicollinearity among the regressors. There are only a few software packages available for estimation of the Liu regression coefficients, though with limited methods to estimate the Liu biasing parameter without addressing testing procedures. Our liureg package can be used to estimate the Liu regression coefficients utilizing a range of different existing biasing parameters, to test these coefficients with more than 15 Liu related statistics, and to present different graphical displays of these statistics.",
    "author": [
      {
        "name": "Muhammad Imdadullah",
        "url": {}
      },
      {
        "name": "Muhammad Aslam",
        "url": {}
      },
      {
        "name": "Saima Altaf",
        "url": {}
      }
    ],
    "date": "2017-10-24",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-048.zip\nCRAN packages used\nlrmest, ltsbase, liureg, lmridge, MASS, mctest\nCRAN Task Views implied by cited packages\nDistributions, Econometrics, Environmetrics, Multivariate, NumericalMathematics, Psychometrics, Robust, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-049/",
    "title": "The welchADF Package for Robust Hypothesis Testing in Unbalanced Multivariate Mixed Models with Heteroscedastic and Non-normal Data",
    "description": "A new R package is presented for dealing with non-normality and variance heterogeneity of sample data when conducting hypothesis tests of main effects and interactions in mixed models. The proposal departs from an existing SAS program which implements Johansen’s general formulation of Welch-James’s statistic with approximate degrees of freedom, which makes it suitable for testing any linear hypothesis concerning cell means in univariate and multivariate mixed model designs when the data pose non-normality and non-homogeneous variance. Improved type I error rate control is obtained using bootstrapping for calculating an empirical critical value, whereas robustness against non-normality is achieved through trimmed means and Winsorized variances. A wrapper function eases the application of the test in common situations, such as performing omnibus tests on all effects and interactions, pairwise contrasts, and tetrad contrasts of two-way interactions. The package is demonstrated in several problems including unbalanced univariate and multivariate designs.",
    "author": [
      {
        "name": "Pablo J. Villacorta",
        "url": {}
      }
    ],
    "date": "2017-10-24",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-049.zip\nCRAN packages used\nART, WRS2, robustbase, robust, robustlmm, nlme, lme4, welchADF, gamm4, mgcv\nCRAN Task Views implied by cited packages\nRobust, Econometrics, Environmetrics, SocialSciences, Bayesian, OfficialStatistics, Psychometrics, SpatioTemporal, ChemPhys, Finance, Multivariate, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-050/",
    "title": "Bayesian Regression Models for Interval-censored Data in R",
    "description": "The package icenReg provides classic survival regression models for interval-censored data. We present an update to the package that extends the parametric models into the Bayesian framework. Core additions include functionality to define the regression model with the standard regression syntax while providing a custom prior function. Several other utility functions are presented that allow for simplified examination of the posterior distribution.",
    "author": [
      {
        "name": "Clifford Anderson-Bergman",
        "url": {}
      }
    ],
    "date": "2017-10-24",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-050.zip\nCRAN packages used\nicenReg, foreach, doParallel, coda, Rcpp, RcppEigen\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NumericalMathematics, Bayesian, gR, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-051/",
    "title": "queueing: A Package For Analysis Of Queueing Networks and Models in R",
    "description": "queueing is a package that solves and provides the main performance measures for both basic Markovian queueing models and single and multiclass product-form queueing networks. It can be used both in education and for professional purposes. It provides an intuitive, straightforward way to build queueing models using S3 methods. The package solves Markovian models of the form M/M/c/K/M/FCFS, open and closed single class Jackson networks, open and closed multiclass networks and mixed networks. Markovian models are used when both the customer inter-arrival time and the server processing time are exponentially distributed. Queueing network solvers are useful for modelling situations in which more than one station must be visited.",
    "author": [
      {
        "name": "Pedro Cañadilla Jiménez",
        "url": {}
      },
      {
        "name": "Yolanda Román Montoya",
        "url": {}
      }
    ],
    "date": "2017-10-24",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-051.zip\nCRAN packages used\nsimmer, queuecomputer, queueing\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-044/",
    "title": "fourierin: An R package to compute Fourier integrals",
    "description": "We present the R package fourierin (Basulto-Elias, 2017) for evaluating functions defined as Fourier-type integrals over a collection of argument values. The integrals are finitely supported with integrands involving continuous functions of one or two variables. As an important application, such Fourier integrals arise in so-called “inversion formulas”, where one seeks to evaluate a probability density at a series of points from a given characteristic function (or vice versa) through Fourier transforms. This paper intends to fill a gap in current R software, where tools for repeated evaluation of functions as Fourier integrals are not directly available. We implement two approaches for such computations with numerical integration. In particular, if the argument collection for evaluation corresponds to a regular grid, then an algorithm from Inverarity (2002) may be employed based on a fast Fourier transform, which creates significant improvements in the speed over a second approach to numerical Fourier integration (where the latter also applies to cases where the points for evaluation are not on a grid). We illustrate the package with the computation of probability densities and characteristic functions through Fourier integrals/transforms, for both univariate and bivariate examples.",
    "author": [
      {
        "name": "Guillermo Basulto-Elias",
        "url": {}
      },
      {
        "name": "Alicia Carriquiry",
        "url": {}
      },
      {
        "name": "Kris De Brabanter",
        "url": {}
      },
      {
        "name": "Daniel J. Nordman",
        "url": {}
      }
    ],
    "date": "2017-10-12",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nfourierin, RcppArmadillo\nCRAN Task Views implied by cited packages\nNumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-045/",
    "title": "afmToolkit: an R Package for Automated AFM Force-Distance Curves Analysis",
    "description": "Atomic force microscopy (AFM) is widely used to measure molecular and colloidal inter actions as well as mechanical properties of biomaterials. In this paper the afmToolkit R package is introduced. This package allows the user to automatically batch process AFM force-distance and force-time curves. afmToolkit capabilities range from importing ASCII files and preprocessing the curves (contact point detection, baseline correction. . . ) for finding relevant physical information, such as Young’s modulus, adhesion energies and exponential decay for force relaxation and creep experiments. This package also contains plotting, summary and feature extraction functions. The package also comes with several data sets so the user can test the aforementioned features with ease. The package afmToolkit eases the basic processing of large amount of AFM F-d/t curves at once. It is also flexible enough to easily incorporate new functions as they are needed and can be seen as a programming infrastructure for further algorithm development.",
    "author": [
      {
        "name": "Rafael Benítez",
        "url": {}
      },
      {
        "name": "Vicente J. Bolós",
        "url": {}
      },
      {
        "name": "José-Luis Toca-Herrera",
        "url": {}
      }
    ],
    "date": "2017-10-12",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-045.zip\nCRAN packages used\nafmToolkit, devtools, ggplot2, minpack.lm, gridExtra, scales, dplyr\nCRAN Task Views implied by cited packages\nChemPhys, Graphics, Optimization, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-042/",
    "title": "adegraphics: An S4 Lattice-Based Package for the Representation of Multivariate Data",
    "description": "The ade4 package provides tools for multivariate analyses. Whereas new statistical methods have been added regularly in the package since its first release in 2002, the graphical functions, that are used to display the main outputs of an analysis, have not benefited from such enhancements. In this context, the adegraphics package, available on CRAN since 2015, is a complete reimplementation of the ade4 graphical functionalities but with large improvements. The package uses the S4 object system (each graph is an object) and is based on the graphical framework provided by lattice and grid. We give a brief description of the package and illustrate some important functionalities to build elegant graphs.",
    "author": [
      {
        "name": "Aurélie Siberchicot",
        "url": {}
      },
      {
        "name": "Alice Julien-Laferrière",
        "url": {}
      },
      {
        "name": "Anne-Béatrice Dufour",
        "url": {}
      },
      {
        "name": "Jean Thioulouse",
        "url": {}
      },
      {
        "name": "Stéphane            Dray",
        "url": {}
      }
    ],
    "date": "2017-10-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-042.zip\nCRAN packages used\nvegan, MASS, FactoMineR, ade4, lattice, adegraphics, sp, spdep, RColorBrewer, ggplot2\nCRAN Task Views implied by cited packages\nMultivariate, Spatial, Graphics, Psychometrics, Environmetrics, Econometrics, Phylogenetics, Distributions, NumericalMathematics, Robust, SocialSciences, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-043/",
    "title": "LeArEst: Length and Area Estimation from Data Measured with Additive Error",
    "description": "This paper describes an R package LeArEst that can be used for estimating object dimensions from a noisy image. The package is based on a simple parametric model for data that are drawn from uniform distribution contaminated by an additive error. Our package is able to estimate the length of the object of interest on a given straight line that intersects it, as well as to estimate the object area when it is elliptically shaped. The input data may be a numerical vector or an image in JPEG format. In this paper, background statistical models and methods for the package are summarized, and the algorithms and key functions implemented are described. Also, examples that demonstrate its usage are provided. Availability: LeArEst is available on CRAN.",
    "author": [
      {
        "name": "Mirta Benšić",
        "url": {}
      },
      {
        "name": "Petar Taler",
        "url": {}
      },
      {
        "name": "Safet Hamedović",
        "url": {}
      },
      {
        "name": "Emmanuel Karlo Nyarko",
        "url": {}
      },
      {
        "name": "Kristian Sabo",
        "url": {}
      }
    ],
    "date": "2017-10-07",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-043.zip\nCRAN packages used\nLeArEst, decon, deamer, conicfit, jpeg, opencpu, shiny\nCRAN Task Views implied by cited packages\nWebTechnologies, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-040/",
    "title": "dGAselID: An R Package for Selecting a Variable Number of Features in High Dimensional Data",
    "description": "The dGAselID package proposes an original approach to feature selection in high dimen sional data. The method is built upon a diploid genetic algorithm. The genotype to phenotype mapping is modeled after the Incomplete Dominance Inheritance, overpassing the necessity to define a dominance scheme. The fitness evaluation is done by user selectable supervised classifiers, from a broad range of options. Cross validation options are also accessible. A new approach to crossover, inspired from the random assortment of chromosomes during meiosis is included. Several mutation operators, inspired from genetics, are also proposed. The package is fully compatible with the data formats used in Bioconductor and MLInterfaces package, readily applicable to microarray studies, but is flexible to other feature selection applications from high dimensional data. Several options for the visualization of evolution and outcomes are implemented to facilitate the interpretation of results. The package’s functionality is illustrated by examples.",
    "author": [
      {
        "name": "Nicolae Teodor Melita",
        "url": {}
      },
      {
        "name": "Stefan Holban",
        "url": {}
      }
    ],
    "date": "2017-08-25",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndGAselID, genalg, GA, nsga2R, gaoptim, STPGA, kofnGA, mogavs, gaselect, scales\nCRAN Task Views implied by cited packages\nOptimization\nBioconductor packages used\nMLInterfaces, MLInterfaces, ALL, genefilter, hgu95av2.db\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-041/",
    "title": "CRTgeeDR: an R Package for Doubly Robust Generalized Estimating Equations Estimations in Cluster Randomized Trials with Missing Data",
    "description": "Semi-parametric approaches based on generalized estimating equations (GEE) are widely used to analyze correlated outcomes in longitudinal settings. In this paper, we present a package CRTgeeDR developed for cluster randomized trials with missing data (CRTs). For use of inverse probability weighting to adjust for missing data in cluster randomized trials, we show that other software lead to biased estimation for non-independence working correlation structure. CRTgeeDR solves this problem. We also extend the ability of existing packages to allow augmented Doubly Robust GEE estimation (DR). Simulation studies demonstrate the consistency of estimators implemented in CRTgeeDR compared to packages such as geepack and the gains associated with the use of the DR for analyzing a binary outcome using a logistic regression. Finally, we illustrate the method on data from a sanitation CRT in developing countries.",
    "author": [
      {
        "name": "Melanie Prague",
        "url": {}
      },
      {
        "name": "Rui Wang",
        "url": {}
      },
      {
        "name": "Victor De Gruttola",
        "url": {}
      }
    ],
    "date": "2017-08-25",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nCRTgeeDR, gee, geepack, geeM, ipw, drgee, CausalGAM, tmle, tmlenet, numDeriv, geesmv\nCRAN Task Views implied by cited packages\nSocialSciences, Econometrics, NumericalMathematics, Robust\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-039/",
    "title": "anomalyDetection: Implementation of Augmented Network Log Anomaly Detection Procedures",
    "description": "As the number of cyber-attacks continues to grow on a daily basis, so does the delay in threat detection. For instance, in 2015, the Office of Personnel Management discovered that approximately 21.5 million individual records of Federal employees and contractors had been stolen. On average, the time between an attack and its discovery is more than 200 days. In the case of the OPM breach, the attack had been going on for almost a year. Currently, cyber analysts inspect numerous potential incidents on a daily basis, but have neither the time nor the resources available to perform such a task. anomalyDetection aims to curtail the time frame in which anomalous cyber activities go unnoticed and to aid in the efficient discovery of these anomalous transactions among the millions of daily logged events by i) providing an efficient means for pre-processing and aggregating cyber data for analysis by employing a tabular vector transformation and handling multicollinearity concerns; ii) offering numerous built-in multivariate statistical functions such as Mahalanobis distance, factor analysis, principal components analysis to identify anomalous activity, iii) incorporating the pipe operator (%>%) to allow it to work well in the tidyverse workflow. Combined, anomalyDetection offers cyber analysts an efficient and simplified approach to break up network events into time-segment blocks and identify periods associated with suspected anomalies for further evaluation.",
    "author": [
      {
        "name": "Robert J. Gutierrez",
        "url": {}
      },
      {
        "name": "Bradley C. Boehmke",
        "url": {}
      },
      {
        "name": "Kenneth W. Bauer",
        "url": {}
      },
      {
        "name": "Cade M. Saie",
        "url": {}
      },
      {
        "name": "Trevor J. Bihl",
        "url": {}
      }
    ],
    "date": "2017-08-04",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-039.zip\nCRAN packages used\nanomalyDetection, magrittr, tidyverse\nCRAN Task Views implied by cited packages\nWebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-038/",
    "title": "ctmcd: An R Package for Estimating the Parameters of a Continuous-Time Markov Chain from Discrete-Time Data",
    "description": "This article introduces the R package ctmcd, which provides an implementation of methods for the estimation of the parameters of a continuous-time Markov chain given that data are only available on a discrete-time basis. This data consists of partial observations of the state of the chain, which are made without error at discrete times, an issue also known as the embedding problem for Markov chains. The functions provided comprise matrix logarithm based approximations as described in Israel et al. (2001), as well as Kreinin and Sidelnikova (2001), an expectation-maximization algorithm and a Gibbs sampling approach, both introduced by Bladt and Sørensen (2005). For the expectation maximization algorithm Wald confidence intervals based on the Fisher information estimation method of Oakes (1999) are provided. For the Gibbs sampling approach, equal-tailed credibility intervals can be obtained. In order to visualize the parameter estimates, a matrix plot function is provided. The methods described are illustrated by Standard and Poor’s discrete-time corporate credit rating transition data.",
    "author": [
      {
        "name": "Marius Pfeuffer",
        "url": {}
      }
    ],
    "date": "2017-07-28",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-038.zip\nCRAN packages used\nmsm, ctmcd, coda, foreach, doParallel\nCRAN Task Views implied by cited packages\nBayesian, Distributions, gR, HighPerformanceComputing, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-036/",
    "title": "Discrete Time Markov Chains with R",
    "description": "The markovchain package aims to provide S4 classes and methods to easily handle Discrete Time Markov Chains (DTMCs), filling the gap with what is currently available in the CRAN repository. In this work, I provide an exhaustive description of the main functions included in the package, as well as hands-on examples.",
    "author": [
      {
        "name": "Giorgio Alfredo Spedicato",
        "url": {}
      }
    ],
    "date": "2017-07-24",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmarkovchain, clickstream, DTMCPack, MTCM, FuzzyStatProb, depmixS4, HMM, msm, heemod, TPmsm, Rcpp, igraph, matlab, Matrix, expm, method, expm, RcppParallel\nCRAN Task Views implied by cited packages\nNumericalMathematics, HighPerformanceComputing, Survival, Cluster, Distributions, Econometrics, Finance, gR, Graphics, Multivariate, Optimization, Spatial, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-037/",
    "title": "Furniture for Quantitative Scientists",
    "description": "A basic understanding of the distributions of study variables and the relationships among them is essential to inform statistical modeling. This understanding is achieved through the com putation of summary statistics and exploratory data analysis. Unfortunately, this step tends to be under-emphasized in the research process, in part because of the often tedious nature of thorough exploratory data analysis. The table1() function in the furniture package streamlines much of the exploratory data analysis process, making the computation and communication of summary statistics simple and beautiful while offering significant time-savings to the researcher.",
    "author": [
      {
        "name": "Tyson S. Barrett",
        "url": {}
      },
      {
        "name": "Emily Brignone",
        "url": {}
      }
    ],
    "date": "2017-07-24",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nfurniture\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-034/",
    "title": "anchoredDistr: a Package for the Bayesian Inversion of Geostatistical Parameters with Multi-type and Multi-scale Data",
    "description": "The Method of Anchored Distributions (MAD) is a method for Bayesian inversion designed for inferring both local (e.g. point values) and global properties (e.g. mean and variogram parameters) of spatially heterogenous fields using multi-type and multi-scale data. Software implementations of MAD exist in C++ and C# to import data, execute an ensemble of forward model simulations, and perform basic post-processing of calculating likelihood and posterior distributions for a given application. This article describes the R package anchoredDistr that has been built to provide an R based environment for this method. In particular, anchoredDistr provides a range of post-processing capabilities for MAD software by taking advantage of the statistical capabilities and wide use of the R language. Two examples from stochastic hydrogeology are provided to highlight the features of the package for MAD applications in inferring anchored distributions of local parameters (e.g. point values of transmissivity) as well as global parameters (e.g. the mean of the spatial random function for hydraulic conductivity).",
    "author": [
      {
        "name": "Heather Savoy",
        "url": {}
      },
      {
        "name": "Falk Heße",
        "url": {}
      },
      {
        "name": "Yoram Rubin",
        "url": {}
      }
    ],
    "date": "2017-06-28",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-034.zip\nCRAN packages used\ngstat, spBayes, spTimer, anchoredDistr, devtools, RSQLite, np, plyr, dplyr, ggplot2\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, Bayesian, Econometrics, Graphics, Phylogenetics, SocialSciences, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-035/",
    "title": "A Tidy Data Model for Natural Language Processing using cleanNLP",
    "description": "Recent advances in natural language processing have produced libraries that extract low level features from a collection of raw texts. These features, known as annotations, are usually stored internally in hierarchical, tree-based data structures. This paper proposes a data model to represent annotations as a collection of normalized relational data tables optimized for exploratory data analysis and predictive modeling. The R package cleanNLP, which calls one of two state of the art NLP libraries (CoreNLP or spaCy), is presented as an implementation of this data model. It takes raw text as an input and returns a list of normalized tables. Specific annotations provided include tokenization, part of speech tagging, named entity recognition, sentiment analysis, dependency parsing, coreference resolution, and word embeddings. The package currently supports input text in English, German, French, and Spanish.",
    "author": [
      {
        "name": "Taylor Arnold",
        "url": {}
      }
    ],
    "date": "2017-06-28",
    "categories": [],
    "contents": "\n\n\n\nSupplementary materials\nSupplementary materials are available in addition to this article. It can be downloaded at\nRJ-2017-035.zip\nCRAN packages used\ndplyr, ggplot2, magrittr, broom, janitor, tidyr, cleanNLP, tidytext, StanfordCoreNLP, coreNLP, XML, spacyr, NLP, cleanNLP, lda, lsa, topicmodels, sqliter, rJava, sotu, glmnet\nCRAN Task Views implied by cited packages\nNaturalLanguageProcessing, WebTechnologies, Graphics, HighPerformanceComputing, MachineLearning, Phylogenetics, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-030/",
    "title": "PGEE: An R Package for Analysis of Longitudinal Data with High-Dimensional Covariates",
    "description": "We introduce an R package PGEE that implements the penalized generalized estimating equations (GEE) procedure proposed by Wang et al. (2012) to analyze longitudinal data with a large number of covariates. The PGEE package includes three main functions: CVfit, PGEE, and MGEE. The CVfit function computes the cross-validated tuning parameter for penalized generalized estimating equations. The function PGEE performs simultaneous estimation and variable selection for longitudinal data with high-dimensional covariates; whereas the function MGEE fits unpenalized GEE to the data for comparison. The R package PGEE is illustrated using a yeast cell-cycle gene expression data set.",
    "author": [
      {
        "name": "Gul Inan",
        "url": {}
      },
      {
        "name": "Lan Wang",
        "url": {}
      }
    ],
    "date": "2017-06-08",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngee, geepack, PGEE, MASS, mvtnorm, ncvreg, penalized, glmnet, rqPen\nCRAN Task Views implied by cited packages\nMachineLearning, SocialSciences, Distributions, Econometrics, Multivariate, Survival, Environmetrics, Finance, NumericalMathematics, Psychometrics, Robust\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-031/",
    "title": "minval: An R package for MINimal VALidation of Stoichiometric Reactions",
    "description": "A genome-scale metabolic reconstruction is a compilation of all stoichiometric reactions that can describe the entire cellular metabolism of an organism, and they have become an indispensable tool for our understanding of biological phenomena, covering fields that range from systems biology to bioengineering. Interrogation of metabolic reconstructions are generally carried through Flux Balance Analysis, an optimization method in which the biological sense of the optimal solution is highly sensitive to thermodynamic unbalance caused by the presence of stoichiometric reactions whose compounds are not produced or consumed in any other reaction (orphan metabolites) and by mass unbalance. The minval package was designed as a tool to identify orphan metabolites and evaluate the mass and charge balance of stoichiometric reactions. The package also includes functions to characterize and write models in TSV and SBML formats, extract all reactants, products, metabolite names and compartments from a metabolic reconstruction.",
    "author": [
      {
        "name": "Daniel Osorio",
        "url": {}
      },
      {
        "name": "Janneth González",
        "url": {}
      },
      {
        "name": "Andrés Pinzón",
        "url": {}
      }
    ],
    "date": "2017-06-08",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsybil, abcdeFBA, minval, gdata, readxl, xlsx, sybilSBML, sybil\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-032/",
    "title": "Working with Daily Climate Model Output Data in R and the futureheatwaves Package",
    "description": "Research on climate change impacts can require extensive processing of climate model output, especially when using ensemble techniques to incorporate output from multiple climate models and multiple simulations of each model. This processing can be particularly extensive when identifying and characterizing multi-day extreme events like heat waves and frost day spells, as these must be processed from model output with daily time steps. Further, climate model output is in a format and follows standards that may be unfamiliar to most R users. Here, we provide an overview of working with daily climate model output data in R. We then present the futureheatwaves package, which we developed to ease the process of identifying, characterizing, and exploring multi-day extreme events in climate model output. This package can input a directory of climate model output files, identify all extreme events using customizable event definitions, and summarize the output using user-specified functions.",
    "author": [
      {
        "name": "G. Brooke Anderson",
        "url": {}
      },
      {
        "name": "Colin Eason",
        "url": {}
      },
      {
        "name": "Elizabeth A. Barnes",
        "url": {}
      }
    ],
    "date": "2017-06-08",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nfutureheatwaves, ggplot2, ncdf4, RNetCDF, ncdf4.helpers, PCICt, ncdf4.helpers, RCMIP5, wux, ggplot2, Rcpp, leaflet\nCRAN Task Views implied by cited packages\nGraphics, Phylogenetics, Spatial, SpatioTemporal, HighPerformanceComputing, NumericalMathematics\nBioconductor packages used\nncdfFlow\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-033/",
    "title": "Counterfactual: An R Package for Counterfactual Analysis",
    "description": "The Counterfactual package implements the estimation and inference methods of Cher nozhukov et al. (2013) for counterfactual analysis. The counterfactual distributions considered are the result of changing either the marginal distribution of covariates related to the outcome variable of interest, or the conditional distribution of the outcome given the covariates. They can be applied to estimate quantile treatment effects and wage decompositions. This paper serves as an introduction to the package and displays basic functionality of the commands contained within.",
    "author": [
      {
        "name": "Mingli Chen",
        "url": {}
      },
      {
        "name": "Victor Chernozhukov",
        "url": {}
      },
      {
        "name": "Iván Fernández-Val",
        "url": {}
      },
      {
        "name": "Blaise Melly",
        "url": {}
      }
    ],
    "date": "2017-06-08",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nCounterfactual, quantreg, survival\nCRAN Task Views implied by cited packages\nEconometrics, SocialSciences, Survival, ClinicalTrials, Environmetrics, Optimization, ReproducibleResearch, Robust\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-029/",
    "title": "flan: An R Package for Inference on Mutation Models.",
    "description": "This paper describes flan, a package providing tools for fluctuation analysis of mutant cell counts. It includes functions dedicated to the distribution of final numbers of mutant cells. Parametric estimation and hypothesis testing are also implemented, enabling inference on different sorts of data with several possible methods. An overview of the subject is proposed. The general form of mutation models is described, including the classical models as particular cases. Estimating from a model, when the data have been generated by another, induces different possible biases, which are identified and discussed. The three estimation methods available in the package are described, and their mean squared errors are compared. Finally, implementation is discussed, and a few examples of usage on real data sets are given.",
    "author": [
      {
        "name": "Adrien Mazoyer",
        "url": {}
      },
      {
        "name": "Rémy Drouilhet",
        "url": {}
      },
      {
        "name": "Stéphane Despréaux",
        "url": {}
      },
      {
        "name": "Bernard Ycart",
        "url": {}
      }
    ],
    "date": "2017-05-18",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nflan, Rcpp, ggplot2, RcppGSL, polynom, RcppArmadillo, lbfgsb3\nCRAN Task Views implied by cited packages\nNumericalMathematics, Graphics, HighPerformanceComputing, Optimization, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-028/",
    "title": "checkmate: Fast Argument Checks for Defensive R Programming",
    "description": "Dynamically typed programming languages like R allow programmers to write generic, flexible and concise code and to interact with the language using an interactive Read-eval-print-loop (REPL). However, this flexibility has its price: As the R interpreter has no information about the expected variable type, many base functions automatically convert the input instead of raising an exception. Unfortunately, this frequently leads to runtime errors deeper down the call stack which obfuscates the original problem and renders debugging challenging. Even worse, unwanted conver sions can remain undetected and skew or invalidate the results of a statistical analysis. As a resort, assertions can be employed to detect unexpected input during runtime and to signal understandable and traceable errors. The package checkmate provides a plethora of functions to check the type and related properties of the most frequently used R objects and variable types. The package is mostly written in C to avoid any unnecessary performance overhead. Thus, the programmer can conveniently write concise, well-tested assertions which outperforms custom R code for many applications. Fur thermore, checkmate simplifies writing unit tests using the framework testthat (Wickham, 2011) by extending it with plenty of additional expectation functions, and registered C routines are available for package developers to perform assertions on arbitrary SEXPs (internal data structure for R objects implemented as struct in C) in compiled code.",
    "author": [
      {
        "name": "Michel Lang",
        "url": {}
      }
    ],
    "date": "2017-05-12",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncheckmate, assertthat, assertive, assertive.numbers, assertive.sets, assertr, magrittr, dplyr, testthat, data.table, tibble, microbenchmark, mlr, BatchJobs, batchtools\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, Finance, MachineLearning, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-001/",
    "title": "iotools: High-Performance I/O Tools for R",
    "description": "The iotools package provides a set of tools for input and output intensive data processing in R. The functions chunk.apply and read.chunk are supplied to allow for iteratively loading contiguous blocks of data into memory as raw vectors. These raw vectors can then be efficiently converted into matrices and data frames with the iotools functions mstrsplit and dstrsplit. These functions minimize copying of data and avoid the use of intermediate strings in order to drastically improve performance. Finally, we also provide read.csv.raw to allow users to read an entire dataset into memory with the same efficient parsing code. In this paper, we present these functions through a set of examples with an emphasis on the flexibility provided by chunk-wise operations. We provide benchmarks comparing the speed of read.csv.raw to data loading functions provided in base R and other contributed packages.",
    "author": [
      {
        "name": "Taylor Arnold",
        "url": {}
      },
      {
        "name": "Michael J. Kane",
        "url": {}
      },
      {
        "name": "Simon Urbanek",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nbigmemory, ff, readr, foreach, iterators, iotools, Matrix\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, Econometrics, Multivariate, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-002/",
    "title": "IsoGeneGUI: Multiple Approaches for Dose-Response Analysis of Microarray Data Using R",
    "description": "The analysis of transcriptomic experiments with ordered covariates, such as dose-response data, has become a central topic in bioinformatics, in particular in omics studies. Consequently, multiple R packages on CRAN and Bioconductor are designed to analyse microarray data from various perspectives under the assumption of order restriction. We introduce the new R package IsoGene Graphical User Interface (IsoGeneGUI), an extension of the original IsoGene package that includes methods from most of available R packages designed for the analysis of order restricted microarray data, namely orQA, ORIClust, goric and ORCME. The methods included in the new IsoGeneGUI range from inference and estimation to model selection and clustering tools. The IsoGeneGUI is not only the most complete tool for the analysis of order restricted microarray experiments available in R but also it can be used to analyse other types of dose-response data. The package provides all the methods in a user friendly fashion, so analyses can be implemented by users with limited knowledge of R programming.",
    "author": [
      {
        "name": "Martin Otava",
        "url": {}
      },
      {
        "name": "Rudradev Sengupta",
        "url": {}
      },
      {
        "name": "Ziv Shkedy",
        "url": {}
      },
      {
        "name": "Dan Lin",
        "url": {}
      },
      {
        "name": "Setia Pramana",
        "url": {}
      },
      {
        "name": "Tobias Verbeke",
        "url": {}
      },
      {
        "name": "Philippe            Haldermans",
        "url": {}
      },
      {
        "name": "Ludwig A. Hothorn",
        "url": {}
      },
      {
        "name": "Daniel Gerhard",
        "url": {}
      },
      {
        "name": "Rebecca M. Kuiper",
        "url": {}
      },
      {
        "name": "Florian Klinglmueller",
        "url": {}
      },
      {
        "name": "           Adetayo Kasim",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\ntitle: ‘IsoGeneGUI: Multiple Approaches for Dose-Response Analysis of Microarray Data\nUsing R’\ndescription: The analysis of transcriptomic experiments with ordered covariates, such\nas dose-response data, has become a central topic in bioinformatics, in particular\nin omics studies. Consequently, multiple R packages on CRAN and Bioconductor are\ndesigned to analyse microarray data from various perspectives under the assumption\nof order restriction. We introduce the new R package IsoGene Graphical User Interface\n(IsoGeneGUI), an extension of the original IsoGene package that includes methods\nfrom most of available R packages designed for the analysis of order restricted\nmicroarray data, namely orQA, ORIClust, goric and ORCME. The methods included in\nthe new IsoGeneGUI range from inference and estimation to model selection and clustering\ntools. The IsoGeneGUI is not only the most complete tool for the analysis of order\nrestricted microarray experiments available in R but also it can be used to analyse\nother types of dose-response data. The package provides all the methods in a user\nfriendly fashion, so analyses can be implemented by users with limited knowledge\nof R programming.\nauthor:\n- Martin Otava\n- Rudradev Sengupta\n- Ziv Shkedy\n- Dan Lin\n- Setia Pramana\n- Tobias Verbeke\n- Philippe Haldermans\n- Ludwig A. Hothorn\n- Daniel Gerhard\n- Rebecca M. Kuiper\n- Florian Klinglmueller\n- ’ Adetayo Kasim’\ndate: ‘2017-05-10’\ndate_received: ‘2015-07-22’\njournal:\ntitle: The R Journal\nissn: 2073-4859\nfirstpage: 14\nlastpage: 26\nvolume: 9\nissue: 1\nslug: RJ-2017-002\npackages:\ncran:\n- IsoGene\n- orQA\n- goric\n- ORCME\n- ORIClust\n- limma\n- mratios\nbioc: ~\npreview: preview.png\nCTV: Cluster\noutput:\ndistill::distill_article:\nself_contained: no\ntoc: no\nlegacy_pdf: yes\npdf_url: RJ-2017-002.pdf\ncitation_url: https://doi.org/10.32614/RJ-2017-002\ndoi: 10.32614/RJ-2017-002\ncreative_commons: CC BY\ncsl: /home/mitchell/R/x86_64-pc-linux-gnu-library/4.1/rjtools/rjournal.csl\n\n\n\nCRAN packages used\nIsoGene, orQA, goric, ORCME, ORIClust, limma, mratios\nCRAN Task Views implied by cited packages\nCluster\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-003/",
    "title": "OrthoPanels: An R Package for Estimating a Dynamic Panel Model with Fixed Effects Using the Orthogonal Reparameterization Approach",
    "description": "This article describes the R package OrthoPanels, which includes the function opm(). This function implements the orthogonal reparameterization approach recommended by Lancaster (2002) to estimate dynamic panel models with fixed effects (and optionally: wave specific intercepts). This article provides a statistical description of the orthogonal reparameterization approach, a demonstration of the package using real-world data, and simulations comparing the estimator to the known-to-be-biased OLS estimator and the commonly used GMM estimator.",
    "author": [
      {
        "name": "Mark Pickup",
        "url": {}
      },
      {
        "name": "Paul Gustafson",
        "url": {}
      },
      {
        "name": "Davor Cubranic",
        "url": {}
      },
      {
        "name": "Geoffrey Evans",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nOrthoPanels, plm\nCRAN Task Views implied by cited packages\nEconometrics, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-004/",
    "title": "smoof: Single- and Multi-Objective Optimization Test Functions",
    "description": "Benchmarking algorithms for optimization problems usually is carried out by running the algorithms under consideration on a diverse set of benchmark or test functions. A vast variety of test functions was proposed by researchers and is being used for investigations in the literature. The smoof package implements a large set of test functions and test function generators for both the single and multi-objective case in continuous optimization and provides functions to easily create own test functions. Moreover, the package offers some additional helper methods, which can be used in the context of optimization.",
    "author": [
      {
        "name": "Jakob Bossek",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nemoa, mco, ecr, cec2005benchmark, cec2013, globalOptTests, soobench, smoof, ParamHelpers, ggplot2\nCRAN Task Views implied by cited packages\nOptimization, Graphics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-005/",
    "title": "alineR: an R Package for Optimizing Feature-Weighted Alignments and Linguistic Distances",
    "description": "Linguistic distance measurements are commonly used in anthropology and biology when quantitative and statistical comparisons between words are needed. This is common, for example, when analyzing linguistic and genetic data. Such comparisons can provide insight into historical population patterns and evolutionary processes. However, the most commonly used linguistic distances are derived from edit distances, which do not weight phonetic features that may, for example, represent smaller-scale patterns in linguistic evolution. Thus, computational methods for calculating feature-weighted linguistic distances are needed for linguistic, biological, and evolutionary applications; additionally, the linguistic distances presented here are generic and may have broader applications in fields such as text mining and search, as well as applications in psycholinguistics and morphology. To facilitate this research, we are making available an open-source R software package that performs feature-weighted linguistic distance calculations. The package also includes a supervised learning methodology that uses a genetic algorithm and manually determined alignments to estimate 13 linguistic parameters including feature weights and a skip penalty. Here we present the package and use it to demonstrate the supervised learning methodology by estimating the optimal linguistic parameters for both simulated data and for a sample of Austronesian languages. Our results show that the methodology can estimate these parameters for both simulated and real language data, that optimizing feature weights improves alignment accuracy by approximately 29%, and that optimization significantly affects the resulting distance measurements. Availability: alineR is available on CRAN.",
    "author": [
      {
        "name": "Sean S. Downey",
        "url": {}
      },
      {
        "name": "Guowei Sun",
        "url": {}
      },
      {
        "name": "Peter Norquest",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nalineR, stringdist, RecordLinkage, doMC\nCRAN Task Views implied by cited packages\nOfficialStatistics, HighPerformanceComputing\nBioconductor packages used\nBiostrings\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-006/",
    "title": "Implementing a Metapopulation Bass Diffusion Model using the R Package deSolve",
    "description": "Diffusion is a fundamental process in physical, biological, social and economic settings. Consumer products often go viral, with sales driven by the word of mouth effect, as their adoption spreads through a population. The classic diffusion model used for product adoption is the Bass diffusion model, and this divides a population into two groups of people: potential adopters who are likely to adopt a product, and adopters who have purchased the product, and influence others to adopt. The Bass diffusion model is normally captured in an aggregate form, where no significant consumer differences are modeled. This paper extends the Bass model to capture a spatial perspective, using metapopulation equations from the field of infectious disease modeling. The paper’s focus is on simulation of deterministic models by solving ordinary differential equations, and does not encompass parameter estimation. The metapopulation model in implemented in R using the deSolve package, and shows the potential of using the R framework to implement large-scale integral equation models, with applications in the field of marketing and consumer behaviour.",
    "author": [
      {
        "name": "Jim Duggan",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndeSolve, EpiModel, ggplot2, scales\nCRAN Task Views implied by cited packages\nDifferentialEquations, Graphics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-007/",
    "title": "MDplot: Visualise Molecular Dynamics",
    "description": "The MDplot package provides plotting functions to allow for automated visualisation of molecular dynamics simulation output. It is especially useful in cases where the plot generation is rather tedious due to complex file formats or when a large number of plots are generated. The graphs that are supported range from those which are standard, such as RMSD/RMSF (root-mean-square deviation and root-mean-square fluctuation, respectively) to less standard, such as thermodynamic integration analysis and hydrogen bond monitoring over time. All told, they address many com monly used analyses. In this article, we set out the MDplot package’s functions, give examples of the function calls, and show the associated plots. Plotting and data parsing is separated in all cases, i.e. the respective functions can be used independently. Thus, data manipulation and the integration of additional file formats is fairly easy. Currently, the loading functions support GROMOS, GROMACS, and AMBER file formats. Moreover, we also provide a Bash interface that allows simple embedding of MDplot into Bash scripts as the final analysis step. Availability: The package can be obtained in the latest major version from CRAN (https://cran.r project.org/package=MDplot) or in the most recent version from the project’s GitHub page at https://github.com/MDplot/MDplot, where feedback is also most welcome. MDplot is published under the GPL-3 license.",
    "author": [
      {
        "name": "Christian Margreitter",
        "url": {}
      },
      {
        "name": "Chris Oostenbrink",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMDplot, bio3d, Rknots\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-008/",
    "title": "On Some Extensions to GA Package: Hybrid Optimisation, Parallelisation and Islands EvolutionOn some extensions to GA package: hybrid optimisation, parallelisation and islands evolution",
    "description": "Genetic algorithms are stochastic iterative algorithms in which a population of individuals evolve by emulating the process of biological evolution and natural selection. The R package GA provides a collection of general purpose functions for optimisation using genetic algorithms. This paper describes some enhancements recently introduced in version 3 of the package. In particular, hybrid GAs have been implemented by including the option to perform local searches during the evolution. This allows to combine the power of genetic algorithms with the speed of a local optimiser. Another major improvement is the provision of facilities for parallel computing. Parallelisation has been implemented using both the master-slave approach and the islands evolution model. Several examples of usage are presented, with both real-world data examples and benchmark functions, showing that often high-quality solutions can be obtained more efficiently.",
    "author": [
      {
        "name": "Luca Scrucca",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrgenoud, Rmalschains, DEoptim, GenSA, pso, cmaes, tabuSearch, GA, quantmod, doParallel, foreach, iterators, doRNG, forecast, astsa, globalOptTests, Rcpp, memoise\nCRAN Task Views implied by cited packages\nOptimization, HighPerformanceComputing, Finance, MachineLearning, TimeSeries, Econometrics, Environmetrics, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-009/",
    "title": "imputeTS: Time Series Missing Value Imputation in R",
    "description": "The imputeTS package specializes on univariate time series imputation. It offers multiple state-of-the-art imputation algorithm implementations along with plotting functions for time series missing data statistics. While imputation in general is a well-known problem and widely covered by R packages, finding packages able to fill missing values in univariate time series is more complicated. The reason for this lies in the fact, that most imputation algorithms rely on inter-attribute correlations, while univariate time series imputation instead needs to employ time dependencies. This paper provides an introduction to the imputeTS package and its provided algorithms and tools. Furthermore, it gives a short overview about univariate time series imputation in R.",
    "author": [
      {
        "name": "Steffen Moritz",
        "url": {}
      },
      {
        "name": "Thomas Bartz-Beielstein",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nAMELIA, mice, VIM, missMDA, imputeTS, zoo, forecast, spacetime, timeSeries, xts\nCRAN Task Views implied by cited packages\nTimeSeries, Finance, Econometrics, OfficialStatistics, Environmetrics, Multivariate, SocialSciences, SpatioTemporal, Psychometrics, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-010/",
    "title": "Update of the nlme Package to Allow a Fixed Standard Deviation of the Residual Error",
    "description": "The use of linear and non-linear mixed models in the life sciences and pharmacometrics is common practice. Estimation of the parameters of models not involving a system of differential equations is often done by the R or S-Plus software with the nonlinear mixed effects nlme package. The estimated residual error may be used for diagnosis of the fitted model, but not whether the model correctly describes the relation between response and included variables including the true covariance structure. The latter is only true if the residual error is known in advance. Therefore, it may be necessary or more appropriate to fix the residual error a priori instead of estimate its value. This can be the case if one wants to include evidence from past studies or a theoretical derivation; e.g., when using a binomial model. S-Plus has an option to fix this residual error to a constant, in contrast to R. For convenience, the nlme package was customized to offer this option as well. In this paper, we derived the log-likelihoods for the mixed models using a fixed residual error. By using some well-known examples from mixed models, we demonstrated the equivalence of R and S-Plus with respect to the estimates. The updated package has been accepted by the Comprehensive R Archive Network (CRAN) team and will be available at the CRAN website.",
    "author": [
      {
        "name": "Simon H. Heisterkamp",
        "url": {}
      },
      {
        "name": "Engelbertus van Willigen",
        "url": {}
      },
      {
        "name": "Paul-Matthias Diderichsen",
        "url": {}
      },
      {
        "name": "John Maringwa",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nnlme\nCRAN Task Views implied by cited packages\nChemPhys, Econometrics, Environmetrics, Finance, OfficialStatistics, Psychometrics, SocialSciences, Spatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-011/",
    "title": "EMSaov: An R Package for the Analysis of Variance with the Expected Mean Squares and its Shiny Application",
    "description": "EMSaov is a new R package that we developed to provide users with an analysis of variance table including the expected mean squares (EMS) for various types of experimental design. It is not easy to find the appropriate test, particularly the denominator for the F statistic that depends on the EMS, when some variables exhibit random effects or when we use a special experimental design such as nested design, repeated measures design, or split-plot design. With EMSaov, a user can easily find the F statistic denominator and can determine how to analyze the data when using a special experimental design. We also develop a web application with a GUI interface using the shiny package in R . We expect that our application can contribute to the efficient and easy analysis of experimental data.",
    "author": [
      {
        "name": "Hye-Min Choe",
        "url": {}
      },
      {
        "name": "Mijeong Kim",
        "url": {}
      },
      {
        "name": "Eun-Kyung Lee",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nnlme, afex, EMSaov, shiny\nCRAN Task Views implied by cited packages\nChemPhys, Econometrics, Environmetrics, Finance, OfficialStatistics, Psychometrics, SocialSciences, Spatial, SpatioTemporal, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-012/",
    "title": "Multilabel Classification with R Package mlr",
    "description": "We implemented several multilabel classification algorithms in the machine learning package mlr. The implemented methods are binary relevance, classifier chains, nested stacking, dependent binary relevance and stacking, which can be used with any base learner that is accessible in mlr. Moreover, there is access to the multilabel classification versions of randomForestSRC and rFerns. All these methods can be easily compared by different implemented multilabel performance measures and resampling methods in the standardized mlr framework. In a benchmark experiment with several multilabel datasets, the performance of the different methods is evaluated.",
    "author": [
      {
        "name": "Philipp Probst",
        "url": {}
      },
      {
        "name": "Quay Au",
        "url": {}
      },
      {
        "name": "Giuseppe Casalicchio",
        "url": {}
      },
      {
        "name": "Clemens Stachl",
        "url": {}
      },
      {
        "name": "Bernd Bischl",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmldr, rFerns, randomForestSRC, randomForestSRC, ada, batchtools\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, MachineLearning, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-013/",
    "title": "milr: Multiple-Instance Logistic Regression with Lasso Penalty",
    "description": "The purpose of the milr package is to analyze multiple-instance data. Ordinary multiple instance data consists of many independent bags, and each bag is composed of several instances. The statuses of bags and instances are binary. Moreover, the statuses of instances are not observed, whereas the statuses of bags are observed. The functions in this package are applicable for analyzing multiple-instance data, simulating data via logistic regression, and selecting important covariates in the regression model. To this end, maximum likelihood estimation with an expectation-maximization algorithm is implemented for model estimation, and a lasso penalty added to the likelihood function is applied for variable selection. Additionally, an \"milr\" object is applicable to generic functions fitted, predict and summary. Simulated data and a real example are given to demonstrate the features of this package.",
    "author": [
      {
        "name": "Ping-Yang Chen",
        "url": {}
      },
      {
        "name": "Ching-Chuan Chen",
        "url": {}
      },
      {
        "name": "Chun-Hao Yang",
        "url": {}
      },
      {
        "name": "Sheng-Mao Chang",
        "url": {}
      },
      {
        "name": "Kuo-Jung Lee",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmilr\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-014/",
    "title": "spcadjust: An R Package for Adjusting for Estimation Error in Control Charts",
    "description": "In practical applications of control charts the in-control state and the corresponding chart parameters are usually estimated based on some past in-control data. The estimation error then needs to be accounted for. In this paper we present an R package, spcadjust, which implements a bootstrap based method for adjusting monitoring schemes to take into account the estimation error. By bootstrapping the past data this method guarantees, with a certain probability, a conditional performance of the chart. In spcadjust the method is implement for various types of Shewhart, CUSUM and EWMA charts, various performance criteria, and both parametric and non-parametric bootstrap schemes. In addition to the basic charts, charts based on linear and logistic regression models for risk adjusted monitoring are included, and it is easy for the user to add further charts. Use of the package is demonstrated by examples.",
    "author": [
      {
        "name": "Axel Gandy",
        "url": {}
      },
      {
        "name": "Jan Terje Kvaløy",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspcadjust, surveillance, spc, qcc, IQCC, qcr, edcc, MSQC\nCRAN Task Views implied by cited packages\nEnvironmetrics, SpatioTemporal, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-015/",
    "title": "GsymPoint: An R Package to Estimate the Generalized Symmetry Point, an Optimal Cut-off Point for Binary Classification in Continuous Diagnostic Tests",
    "description": "In clinical practice, it is very useful to select an optimal cutpoint in the scale of a continuous biomarker or diagnostic test for classifying individuals as healthy or diseased. Several methods for choosing optimal cutpoints have been presented in the literature, depending on the ultimate goal. One of these methods, the generalized symmetry point, recently introduced, generalizes the symmetry point by incorporating the misclassification costs. Two statistical approaches have been proposed in the literature for estimating this optimal cutpoint and its associated sensitivity and specificity measures, a parametric method based on the generalized pivotal quantity and a nonparametric method based on empirical likelihood. In this paper, we introduce GsymPoint, an R package that implements these methods in a user-friendly environment, allowing the end-user to calculate the generalized symmetry point depending on the levels of certain categorical covariates. The practical use of this package is illustrated using three real biomedical datasets.",
    "author": [
      {
        "name": "Mónica López-Ratón",
        "url": {}
      },
      {
        "name": "Elisa M. Molanes-López",
        "url": {}
      },
      {
        "name": "Emilio Letón",
        "url": {}
      },
      {
        "name": "Carmen Cadarso-Suárez",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nGsymPoint, PresenceAbsence, DiagnosisMed, pROC, OptimalCutpoints, GsymPoint\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-016/",
    "title": "pdp: An R Package for Constructing Partial Dependence Plots",
    "description": "Complex nonparametric models—like neural networks, random forests, and support vector machines—are more common than ever in predictive analytics, especially when dealing with large observational databases that don’t adhere to the strict assumptions imposed by traditional statistical techniques (e.g., multiple linear regression which assumes linearity, homoscedasticity, and normality). Unfortunately, it can be challenging to understand the results of such models and explain them to management. Partial dependence plots offer a simple solution. Partial dependence plots are low dimensional graphical renderings of the prediction function so that the relationship between the outcome and predictors of interest can be more easily understood. These plots are especially useful in explaining the output from black box models. In this paper, we introduce pdp, a general R package for constructing partial dependence plots.",
    "author": [
      {
        "name": "Brandon M. Greenwell",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrandomForest, gbm, party, partykit, pdp, plotmo, lattice, ICEbox, car, effects, ggplot2, grid, latticeExtra, gridExtra, nnet, C50, rpart, adabag, ipred, adabag, xgboost, Cubist, MASS, earth, mda, ranger, e1071, kernlab, caret, magrittr, foreach, viridis, plyr, doMC, doParallel, dplyr\nCRAN Task Views implied by cited packages\nMachineLearning, Multivariate, Environmetrics, Survival, Econometrics, SocialSciences, Graphics, HighPerformanceComputing, Cluster, Distributions, Psychometrics, Finance, NaturalLanguageProcessing, NumericalMathematics, Optimization, Phylogenetics, Robust, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-017/",
    "title": "Weighted Effect Coding for Observational Data with wec",
    "description": "Weighted effect coding refers to a specific coding matrix to include factor variables in generalised linear regression models. With weighted effect coding, the effect for each category represents the deviation of that category from the weighted mean (which corresponds to the sample mean). This technique has particularly attractive properties when analysing observational data, that commonly are unbalanced. The wec package is introduced, that provides functions to apply weighted effect coding to factor variables, and to interactions between (a.) a factor variable and a continuous variable and between (b.) two factor variables.",
    "author": [
      {
        "name": "Rense Nieuwenhuis",
        "url": {}
      },
      {
        "name": "Manfred te Grotenhuis",
        "url": {}
      },
      {
        "name": "Ben Pelzer",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nwec\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-018/",
    "title": "coxphMIC: An R Package for Sparse Estimation of Cox Proportional Hazards Models via Approximated Information Criteria",
    "description": "In this paper, we describe an R package named coxphMIC, which implements the sparse estimation method for Cox proportional hazards models via approximated information criterion (Su et al., 2016). The developed methodology is named MIC which stands for “Minimizing approximated Information Criteria\". A reparameterization step is introduced to enforce sparsity while at the same time keeping the objective function smooth. As a result, MIC is computationally fast with a superior performance in sparse estimation. Furthermore, the reparameterization tactic yields an additional advantage in terms of circumventing post-selection inference (Leeb and Pötscher, 2005). The MIC method and its R implementation are introduced and illustrated with the PBC data.",
    "author": [
      {
        "name": "Razieh Nabi",
        "url": {}
      },
      {
        "name": "Xiaogang Su",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-019/",
    "title": "Retrieval and Analysis of Eurostat Open Data with the eurostat Package",
    "description": "The increasing availability of open statistical data resources is providing novel opportunities for research and citizen science. Efficient algorithmic tools are needed to realize the full potential of the new information resources. We introduce the eurostat R package that provides a collection of custom tools for the Eurostat open data service, including functions to query, download, manipulate, and visualize these data sets in a smooth, automated and reproducible manner. The online documentation provides detailed examples on the analysis of these spatio-temporal data collections. This work provides substantial improvements over the previously available tools, and has been extensively tested by an active user community. The eurostat R package contributes to the growing open source ecosystem dedicated to reproducible research in computational social science and digital humanities.",
    "author": [
      {
        "name": "Leo Lahti",
        "url": {}
      },
      {
        "name": "Janne Huovari",
        "url": {}
      },
      {
        "name": "Markus Kainu",
        "url": {}
      },
      {
        "name": "Przemysław Biecek",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nFAOSTAT, WDI, pxweb, osmar, eurostat, smarterpoland, rsdmx, datamart, quandl, pdfetch, classInt, httr, jsonlite, readr, sp, stringi, tibble, plotrix, maptools, rgdal, rgeos, scales, stringr, countrycode\nCRAN Task Views implied by cited packages\nSpatial, WebTechnologies, Graphics, NaturalLanguageProcessing, SpatioTemporal, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-020/",
    "title": "Market Area Analysis for Retail and Service Locations with MCI",
    "description": "In retail location analysis, marketing research and spatial planning, the market areas of stores and/or locations are a frequent subject. Market area analyses consist of empirical observations and modeling via theoretical and/or econometric models such as the Huff Model or the Multiplicative Competitive Interaction Model. The authors’ package MCI implements the steps of market area analysis into R with a focus on fitting the models and data preparation and processing.",
    "author": [
      {
        "name": "Thomas Wieland",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMCI, SpatialPosition, ggmap, osmar, osrm, car, spgwr\nCRAN Task Views implied by cited packages\nSpatial, WebTechnologies, Econometrics, Finance, Multivariate, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-021/",
    "title": "PSF: Introduction to R Package for Pattern Sequence Based Forecasting Algorithm",
    "description": "This paper introduces the R package that implements the Pattern Sequence based Forecasting (PSF) algorithm, which was developed for univariate time series forecasting. This algorithm has been successfully applied to many different fields. The PSF algorithm consists of two major parts: clustering and prediction. The clustering part includes selection of the optimum number of clusters. It labels time series data with reference to such clusters. The prediction part includes functions like optimum window size selection for specific patterns and prediction of future values with reference to past pattern sequences. The PSF package consists of various functions to implement the PSF algorithm. It also contains a function which automates all other functions to obtain optimized prediction results. The aim of this package is to promote the PSF algorithm and to ease its usage with minimum efforts. This paper describes all the functions in the PSF package with their syntax. It also provides a simple example. Finally, the usefulness of this package is discussed by comparing it to auto.arima and ets, well-known time series forecasting functions available on CRAN repository.",
    "author": [
      {
        "name": "Neeraj Bokde",
        "url": {}
      },
      {
        "name": "Gualberto Asencio-Cortés",
        "url": {}
      },
      {
        "name": "Francisco Martínez-Álvarez",
        "url": {}
      },
      {
        "name": "Kishore Kulat",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nPSF, cluster, data.table, forecast\nCRAN Task Views implied by cited packages\nEnvironmetrics, Finance, Cluster, Econometrics, HighPerformanceComputing, Multivariate, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-022/",
    "title": "BayesBinMix: an R Package for Model Based Clustering of Multivariate Binary Data",
    "description": "The BayesBinMix package offers a Bayesian framework for clustering binary data with or without missing values by fitting mixtures of multivariate Bernoulli distributions with an unknown number of components. It allows the joint estimation of the number of clusters and model parameters using Markov chain Monte Carlo sampling. Heated chains are run in parallel and accelerate the convergence to the target posterior distribution. Identifiability issues are addressed by implementing label switching algorithms. The package is demonstrated and benchmarked against the Expectation Maximization algorithm using a simulation study as well as a real dataset.",
    "author": [
      {
        "name": "Panagiotis Papastamoulis",
        "url": {}
      },
      {
        "name": "Magnus Rattray",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nBayesBinMix, label.switching, foreach, doParallel, coda, FlexMix, flexclust\nCRAN Task Views implied by cited packages\nBayesian, Cluster, gR, HighPerformanceComputing\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-023/",
    "title": "Network Visualization with ggplot2",
    "description": "This paper explores three different approaches to visualize networks by building on the grammar of graphics framework implemented in the ggplot2 package. The goal of each approach is to provide the user with the ability to apply the flexibility of ggplot2 to the visualization of network data, including through the mapping of network attributes to specific plot aesthetics. By incorporating networks in the ggplot2 framework, these approaches (1) allow users to enhance networks with additional information on edges and nodes, (2) give access to the strengths of ggplot2, such as layers and facets, and (3) convert network data objects to the more familiar data frames.",
    "author": [
      {
        "name": "Sam Tyner",
        "url": {}
      },
      {
        "name": "François Briatte",
        "url": {}
      },
      {
        "name": "Heike Hofmann",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nigraph, sna, network, statnet, ggplot2, ggnetwork, geomnet, ggmap, ggfortify, GGally, gcookbook, intergraph, grid, ggrepel, ndtv, gridExtra, tnet, ggCompNet, tidyverse, plyr, dplyr\nCRAN Task Views implied by cited packages\ngR, SocialSciences, Graphics, Optimization, Spatial, Bayesian, Phylogenetics, WebTechnologies\nBioconductor packages used\nggbio, ggtree\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-024/",
    "title": "The mosaic Package: Helping Students to Think with Data Using R",
    "description": "The mosaic package provides a simplified and systematic introduction to the core functional ity related to descriptive statistics, visualization, modeling, and simulation-based inference required in first and second courses in statistics. This introduction to the package describes some of the guiding principles behind the design of the package and provides illustrative examples of several of the most important functions it implements. These can be combined to help students “think with data\" using R in their early course work, starting with simple, yet powerful, declarative commands.",
    "author": [
      {
        "name": "Randall Pruim",
        "url": {}
      },
      {
        "name": "Daniel T Kaplan",
        "url": {}
      },
      {
        "name": "Nicholas J Horton",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmosaic, lattice, mosaic, mosaicData, ggplot2, ggplot2, dplyr, parallel, MASS\nCRAN Task Views implied by cited packages\nGraphics, Multivariate, Phylogenetics, Distributions, Econometrics, Environmetrics, NumericalMathematics, Psychometrics, Robust, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-025/",
    "title": "autoimage: Multiple Heat Maps for Projected Coordinates",
    "description": "Heat maps are commonly used to display the spatial distribution of a response observed on a two-dimensional grid. The autoimage package provides convenient functions for constructing multiple heat maps in unified, seamless way, particularly when working with projected coordinates. The autoimage package natively supports: 1. automatic inclusion of a color scale with the plotted image, 2. construction of heat maps for responses observed on regular or irregular grids, as well as non-gridded data, 3. construction of a matrix of heat maps with a common color scale, 4. construction of a matrix of heat maps with individual color scales, 5. projecting coordinates before plotting, 6. easily adding geographic borders, points, and other features to the heat maps. After comparing the autoimage package’s capabilities for constructing heat maps to those of existing tools, a carefully selected set of examples is used to highlight the capabilities of the autoimage package.",
    "author": [
      {
        "name": "Joshua P. French",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nautoimage, fields, lattice, sp, ggplot2, spatstat, gridExtra, cowplot, akima, mapproj, gear, viridisLite, maps\nCRAN Task Views implied by cited packages\nSpatial, Graphics, SpatioTemporal, Multivariate, NumericalMathematics, Phylogenetics, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-026/",
    "title": "Hosting Data Packages via drat: A Case Study with Hurricane Exposure Data",
    "description": "Data-only packages offer a way to provide extended functionality for other R users. However, such packages can be large enough to exceed the package size limit (5 megabytes) for the Comprehen sive R Archive Network (CRAN). As an alternative, large data packages can be posted to additional repostiories beyond CRAN itself in a way that allows smaller code packages on CRAN to access and use the data. The drat package facilitates creation and use of such alternative repositories and makes it particularly simple to host them via GitHub. CRAN packages can draw on packages posted to drat repositories through the use of the ‘Additonal_repositories’ field in the DESCRIPTION file. This paper describes how R users can create a suite of coordinated packages, in which larger data packages are hosted in an alternative repository created with drat, while a smaller code package that interacts with this data is created that can be submitted to CRAN.",
    "author": [
      {
        "name": "G. Brooke Anderson",
        "url": {}
      },
      {
        "name": "Dirk Eddelbuettel",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nNMMAPSlite, stashR, rnoaa, tigris, UScensus2000, drat, grattan, hurricaneexposure, devtools, rcmdcheck, git2r, littler, knitr, roxygen2\nCRAN Task Views implied by cited packages\nReproducibleResearch, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2017-027/",
    "title": "The NoiseFiltersR Package: Label Noise Preprocessing in R",
    "description": "In Data Mining, the value of extracted knowledge is directly related to the quality of the used data. This makes data preprocessing one of the most important steps in the knowledge discovery process. A common problem affecting data quality is the presence of noise. A training set with label noise can reduce the predictive performance of classification learning techniques and increase the overfitting of classification models. In this work we present the NoiseFiltersR package. It contains the first extensive R implementation of classical and state-of-the-art label noise filters, which are the most common techniques for preprocessing label noise. The algorithms used for the implementation of the label noise filters are appropriately documented and referenced. They can be called in a R-user-friendly manner, and their results are unified by means of the \"filter\" class, which also benefits from adapted print and summary methods.",
    "author": [
      {
        "name": "Pablo Morales",
        "url": {}
      },
      {
        "name": "Julián Luengo",
        "url": {}
      },
      {
        "name": "Luís P.F. Garcia",
        "url": {}
      },
      {
        "name": "Ana C. Lorena",
        "url": {}
      },
      {
        "name": "André C.P.L.F. de Carvalho",
        "url": {}
      },
      {
        "name": "           Francisco Herrera",
        "url": {}
      }
    ],
    "date": "2017-05-10",
    "categories": [],
    "contents": "\ntitle: ‘The NoiseFiltersR Package: Label Noise Preprocessing in R’\ndescription: In Data Mining, the value of extracted knowledge is directly related\nto the quality of the used data. This makes data preprocessing one of the most important\nsteps in the knowledge discovery process. A common problem affecting data quality\nis the presence of noise. A training set with label noise can reduce the predictive\nperformance of classification learning techniques and increase the overfitting of\nclassification models. In this work we present the NoiseFiltersR package. It contains\nthe first extensive R implementation of classical and state-of-the-art label noise\nfilters, which are the most common techniques for preprocessing label noise. The\nalgorithms used for the implementation of the label noise filters are appropriately\ndocumented and referenced. They can be called in a R-user-friendly manner, and their\nresults are unified by means of the “filter” class, which also benefits from adapted\nprint and summary methods.\nauthor:\n- Pablo Morales\n- Julián Luengo\n- Luís P.F. Garcia\n- Ana C. Lorena\n- André C.P.L.F. de Carvalho\n- ’ Francisco Herrera’\ndate: ‘2017-05-10’\ndate_received: ‘2016-07-12’\njournal:\ntitle: The R Journal\nissn: 2073-4859\nfirstpage: 219\nlastpage: 228\nvolume: 9\nissue: 1\nslug: RJ-2017-027\npackages:\ncran:\n- MICE\n- Amelia\n- caret\n- FSelector\n- mvoutlier\n- robustDA\n- probFDA\n- NoiseFiltersR\n- unbalanced\n- RWeka\nbioc: ~\npreview: preview.png\nCTV:\n- MachineLearning\n- Multivariate\n- Robust\n- HighPerformanceComputing\n- NaturalLanguageProcessing\n- OfficialStatistics\n- SocialSciences\noutput:\ndistill::distill_article:\nself_contained: no\ntoc: no\nlegacy_pdf: yes\npdf_url: RJ-2017-027.pdf\ncitation_url: https://doi.org/10.32614/RJ-2017-027\ndoi: 10.32614/RJ-2017-027\ncreative_commons: CC BY\ncsl: /home/mitchell/R/x86_64-pc-linux-gnu-library/4.1/rjtools/rjournal.csl\n\n\n\nCRAN packages used\nMICE, Amelia, caret, FSelector, mvoutlier, robustDA, probFDA, NoiseFiltersR, unbalanced, RWeka\nCRAN Task Views implied by cited packages\nMachineLearning, Multivariate, Robust, HighPerformanceComputing, NaturalLanguageProcessing, OfficialStatistics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:31+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-036/",
    "title": "rnrfa: An R package to Retrieve, Filter and Visualize Data from the UK National River Flow Archive",
    "description": "The UK National River Flow Archive (NRFA) stores several types of hydrological data and metadata: daily river flow and catchment rainfall time series, gauging station and catchment informa tion. Data are served through the NRFA web services via experimental RESTful APIs. Obtaining NRFA data can be unwieldy due to complexities in handling HTTP GET requests and parsing responses in JSON and XML formats. The rnrfa package provides a set of functions to programmatically access, filter, and visualize NRFA data using simple R syntax. This paper describes the structure of the rnrfa package, including examples using the main functions gdf() and cmr() for flow and rainfall data, respectively. Visualization examples are also provided with a shiny web application and functions provided in the package. Although this package is regional specific, the general framework and structure could be applied to similar databases.",
    "author": [
      {
        "name": "Claudia Vitolo",
        "url": {}
      },
      {
        "name": "Matthew Fry",
        "url": {}
      },
      {
        "name": "Wouter Buytaert",
        "url": {}
      }
    ],
    "date": "2017-01-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrnrfa, rnoaa, waterData, RNCEP, shiny, leaflet, rmarkdown, DT, dplyr, cowplot, plyr, httr, xml2, stringr, xts, rjson, ggmap, ggplot2, rgdal, sp, ggrepel, devtools, microbenchmark, cranlogs, evd, outliers, spacetime, sos4R\nCRAN Task Views implied by cited packages\nWebTechnologies, Spatial, SpatioTemporal, ReproducibleResearch, Distributions, Econometrics, Environmetrics, ExtremeValue, Finance, Graphics, Phylogenetics, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-041/",
    "title": "Normal Tolerance Interval Procedures in the tolerance Package",
    "description": "Statistical tolerance intervals are used for a broad range of applications, such as quality control, engineering design tests, environmental monitoring, and bioequivalence testing. tolerance is the only R package devoted to procedures for tolerance intervals and regions. Perhaps the most commonly-employed functions of the package involve normal tolerance intervals. A number of new procedures for this setting have been included in recent versions of tolerance. In this paper, we discuss and illustrate the functions that implement these normal tolerance interval procedures, one of which is a new, novel type of operating characteristic curve.",
    "author": [
      {
        "name": "Derek S. Young",
        "url": {}
      }
    ],
    "date": "2017-01-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntolerance, cranlogs, rgl\nCRAN Task Views implied by cited packages\nDistributions, Graphics, Multivariate, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-054/",
    "title": "Computing Pareto Frontiers and Database Preferences with the rPref Package",
    "description": "The concept of Pareto frontiers is well-known in economics. Within the database community there exist many different solutions for the specification and calculation of Pareto frontiers, also called Skyline queries in the database context. Slight generalizations like the combination of the Pareto operator with the lexicographical order have been established under the term database preferences. In this paper we present the rPref package which allows to efficiently deal with these concepts within R. With its help, database preferences can be specified in a very similar way as in a state-of-the-art database management system. Our package provides algorithms for an efficient calculation of the Pareto-optimal set and further functionalities for visualizing and analyzing the induced preference order.",
    "author": [
      {
        "name": "Patrick Roocks",
        "url": {}
      }
    ],
    "date": "2017-01-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrPref, emoa, mco, TunePareto, dplyr, lazyeval, RcppParallel, igraph, ggplot2\nCRAN Task Views implied by cited packages\nGraphics, Optimization, gR, HighPerformanceComputing, Phylogenetics, Spatial\nBioconductor packages used\nRgraphviz\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-057/",
    "title": "Weighted Distance Based Discriminant Analysis: The R Package WeDiBaDis",
    "description": "The WeDiBaDis package provides a user friendly environment to perform discriminant analysis (supervised classification). WeDiBaDis is an easy to use package addressed to the biological and medical communities, and in general, to researchers interested in applied studies. It can be suitable when the user is interested in the problem of constructing a discriminant rule on the basis of distances between a relatively small number of instances or units of known unbalanced-class membership measured on many (possibly thousands) features of any type. This is a current situation when analyzing genetic biomedical data. This discriminant rule can then be used both, as a means of explaining differences among classes, but also in the important task of assigning the class membership for new unlabeled units. Our package implements two discriminant analysis procedures in an R environment: the well-known distance-based discriminant analysis (DB-discriminant) and a weighted distance-based discriminant (WDB-discriminant), a novel classifier rule that we introduce. This new procedure is based on an improvement of the DB rule taking into account the statistical depth of the units. This article presents both classifying procedures and describes the implementation of each in detail. We illustrate the use of the package using an ecological and a genetic experimental example. Finally, we illustrate the effectiveness of the new proposed procedure (WDB), as compared with DB. This comparison is carried out using thirty-eight, high-dimensional, class-unbalanced, cancer data sets, three of which include clinical features.",
    "author": [
      {
        "name": "Itziar Irigoien",
        "url": {}
      },
      {
        "name": "Francesc Mestres",
        "url": {}
      },
      {
        "name": "Concepcion Arenas",
        "url": {}
      }
    ],
    "date": "2017-01-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncluster, ICGE, vegan\nCRAN Task Views implied by cited packages\nEnvironmetrics, Multivariate, Cluster, Phylogenetics, Psychometrics, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-059/",
    "title": "condSURV: An R Package for the Estimation of the Conditional Survival Function for Ordered Multivariate Failure Time Data",
    "description": "One major goal in clinical applications of time-to-event data is the estimation of survival with censored data. The usual nonparametric estimator of the survival function is the time-honored Kaplan-Meier product-limit estimator. Though this estimator has been implemented in several R packages, the development of the condSURV R package has been motivated by recent contributions that allow the estimation of the survival function for ordered multivariate failure time data. The condSURV package provides three different approaches all based on the Kaplan-Meier estimator. In one of these approaches these quantities are estimated conditionally on current or past covariate measures. Illustration of the software usage is included using real data.",
    "author": [
      {
        "name": "Luis Meira-Machado",
        "url": {}
      },
      {
        "name": "Marta Sestelo",
        "url": {}
      }
    ],
    "date": "2017-01-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsurvival, prodlim, condSURV\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Econometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-042/",
    "title": "easyROC: An Interactive Web-tool for ROC Curve Analysis Using R Language Environment",
    "description": "ROC curve analysis is a fundamental tool for evaluating the performance of a marker in a number of research areas, e.g., biomedicine, bioinformatics, engineering etc., and is frequently used for discriminating cases from controls. There are a number of analysis tools which are used to guide researchers through their analysis. Some of these tools are commercial and provide basic methods for ROC curve analysis while others offer advanced analysis techniques and a command-based user interface, such as the R environment. The R environmentg includes comprehensive tools for ROC curve analysis; however, using a command-based interface might be challenging and time consuming when a quick evaluation is desired; especially for non-R users, physicians etc. Hence, a quick, comprehensive, free and easy-to-use analysis tool is required. For this purpose, we developed a user-friendly web tool based on the R language. This tool provides ROC statistics, graphical tools, optimal cutpoint calculation, comparison of several markers, and sample size estimation to support researchers in their decisions without writing R codes. easyROC can be used via any device with an internet connection independently of the operating system. The web interface of easyROC is constructed with the R package shiny. This tool is freely available through www.biosoft.hacettepe.edu.tr/easyROC.",
    "author": [
      {
        "name": "Dincer Goksuluk",
        "url": {}
      },
      {
        "name": "Selcuk Korkmaz",
        "url": {}
      },
      {
        "name": "Gokmen Zararsiz",
        "url": {}
      },
      {
        "name": "A. Ergun Karaagaoglu",
        "url": {}
      }
    ],
    "date": "2016-12-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nROCR, pROC, OptimalCutpoints, shiny, plotROC, plyr\nCRAN Task Views implied by cited packages\nMachineLearning, Multivariate, WebTechnologies\nBioconductor packages used\nROC\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-033/",
    "title": "diverse: an R Package to Analyze Diversity in Complex Systems",
    "description": "The package diverse provides an easy-to-use interface to calculate and visualize different aspects of diversity in complex systems. In recent years, an increasing number of research projects in social and interdisciplinary sciences, including fields like innovation studies, scientometrics, economics, and network science have emphasized the role of diversification and sophistication of socioeconomic systems. However, so far no dedicated package exists that covers the needs of these emerging fields and interdisciplinary teams. Most packages about diversity tend to be created according to the demands and terminology of particular areas of natural and biological sciences. The package diverse uses interdisciplinary concepts of diversity—like variety, disparity and balance— as well as ubiquity and revealed comparative advantages, that are relevant to many fields of science, but are in particular useful for interdisciplinary research on diversity in socioeconomic systems. The package diverse provides a toolkit for social scientists, interdisciplinary researcher, and beginners in ecology to (i) import data, (ii) calculate different data transformations and normalization like revealed comparative advantages, (iii) calculate different diversity measures, and (iv) connect diverse to other specialized R packages on similarity measures, data visualization techniques, and statistical significance tests. The comprehensiveness of the package, from matrix import and transformations options, over similarity and diversity measures, to data visualization methods, makes it a useful package to explore different dimensions of diversity in complex systems.",
    "author": [
      {
        "name": "Miguel R. Guevara",
        "url": {}
      },
      {
        "name": "Dominik Hartmann",
        "url": {}
      },
      {
        "name": "Marcelo Mendoza",
        "url": {}
      }
    ],
    "date": "2016-12-12",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nentropart, vegan, biodiversityR, Blaunet, diveRsity, divo, FD, hierDiversity, simboot, treescape, SYNCSA, diverse, proxy, pheatmap, treemap, igraph, foreign, spadeR\nCRAN Task Views implied by cited packages\nEnvironmetrics, Multivariate, OfficialStatistics, Phylogenetics, Spatial, gR, Graphics, Optimization, Psychometrics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-037/",
    "title": "Qtools: A Collection of Models and Tools for Quantile Inference",
    "description": "Quantiles play a fundamental role in statistics. The quantile function defines the distribution of a random variable and, thus, provides a way to describe the data that is specular but equivalent to that given by the corresponding cumulative distribution function. There are many advantages in working with quantiles, starting from their properties. The renewed interest in their usage seen in the last years is due to the theoretical, methodological, and software contributions that have broadened their applicability. This paper presents the R package Qtools, a collection of utilities for unconditional and conditional quantiles.",
    "author": [
      {
        "name": "Marco Geraci",
        "url": {}
      }
    ],
    "date": "2016-12-12",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nquantreg, bayesQR, BSquare, lqmm, Qtools, boot, Rearrangement, mice\nCRAN Task Views implied by cited packages\nSocialSciences, Bayesian, Econometrics, Optimization, Robust, Survival, Environmetrics, Multivariate, OfficialStatistics, ReproducibleResearch, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-045/",
    "title": "Ake: An R Package for Discrete and Continuous Associated Kernel Estimations",
    "description": "Kernel estimation is an important technique in exploratory data analysis. Its utility relies on its ease of interpretation, especially based on graphical means. The Ake package is introduced for univariate density or probability mass function estimation and also for continuous and discrete regression functions using associated kernel estimators. These associated kernels have been proposed due to their specific features of variables of interest. The package focuses on associated kernel methods appropriate for continuous (bounded, positive) or discrete (count, categorical) data often found in applied settings. Furthermore, optimal bandwidths are selected by cross-validation for any associated kernel and by Bayesian methods for the binomial kernel. Other Bayesian methods for selecting bandwidths with other associated kernels will complete this package in its future versions; particularly, a Bayesian adaptive method for gamma kernel estimation of density functions is developed. Some practical and theoretical aspects of the normalizing constant in both density and probability mass functions estimations are given.",
    "author": [
      {
        "name": "Wanbitching E. Wansouwé",
        "url": {}
      },
      {
        "name": "Sobom M. Somé",
        "url": {}
      },
      {
        "name": "Célestin C. Kokonendji",
        "url": {}
      }
    ],
    "date": "2016-12-12",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nAke\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-051/",
    "title": "water: Tools and Functions to Estimate Actual Evapotranspiration Using Land Surface Energy Balance Models in R",
    "description": "The crop water requirement is a key factor in the agricultural process. It is usually estimated throughout actual evapotranspiration (ETa ). This parameter is the key to develop irrigation strategies, to improve water use efficiency and to understand hydrological, climatic, and ecosystem processes. Currently, it is calculated with classical methods, which are difficult to extrapolate, or with land surface energy balance models (LSEB), such as METRIC and SEBAL, which are based on remote sensing data. This paper describes water, an open implementation of LSEB. The package provides several functions to estimate the parameters of the LSEB equation from satellite data and proposes a new object class to handle weather station data. One of the critical steps in METRIC is the selection of “cold” and “hot” pixels, which water solves with an automatic method. The water package can process a batch of satellite images and integrates most of the already published sub-models for METRIC. Although water implements METRIC, it will be expandable to SEBAL and others in the near future. Finally, two different procedures are demonstrated using data that is included in water package.",
    "author": [
      {
        "name": "Guillermo Federico Olmedo",
        "url": {}
      },
      {
        "name": "Samuel Ortega-Farías",
        "url": {}
      },
      {
        "name": "Daniel de la Fuente-Sáiz",
        "url": {}
      },
      {
        "name": "David Fonseca-            Luego",
        "url": {}
      },
      {
        "name": "Fernando Fuentes-Peñailillo",
        "url": {}
      }
    ],
    "date": "2016-12-12",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nraster, raster\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-061/",
    "title": "Measurement Units in R",
    "description": "We briefly review SI units, and discuss R packages that deal with measurement units, their compatibility and conversion. Built upon udunits2 and the UNIDATA udunits library, we introduce the package units that provides a class for maintaining unit metadata. When used in expression, it automatically converts units, and simplifies units of results when possible; in case of incompatible units, errors are raised. The class flexibly allows expansion beyond predefined units. Using units may eliminate a whole class of potential scientific programming mistakes. We discuss the potential and limitations of computing with explicit units.",
    "author": [
      {
        "name": "Edzer Pebesma",
        "url": {}
      },
      {
        "name": "Thomas Mailund",
        "url": {}
      },
      {
        "name": "James Hiebert",
        "url": {}
      }
    ],
    "date": "2016-12-12",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nlubridate, sp, measurements, NISTunits, udunits2, units, ggplot2, spacetime, h5, RNetCDF, sos4R\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, Graphics, Phylogenetics, ReproducibleResearch, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-062/",
    "title": "mctest: An R Package for Detection of Collinearity among Regressors",
    "description": "It is common for linear regression models to be plagued with the problem of multicollinearity when two or more regressors are highly correlated. This problem results in unstable estimates of regression coefficients and causes some serious problems in validation and interpretation of the model. Different diagnostic measures are used to detect multicollinearity among regressors. Many statistical software and R packages provide few diagnostic measures for the judgment of multicollinearity. Most widely used diagnostic measures in these software are: coefficient of determination (R2 ), variance inflation factor/tolerance limit (VIF/TOL), eigenvalues, condition number (CN) and condition index (CI) etc. In this manuscript, we present an R package, mctest, that computes popular and widely used multicollinearity diagnostic measures. The package also indicates which regressors may be the reason of collinearity among regressors.",
    "author": [
      {
        "name": "Muhammad Imdadullah",
        "url": {}
      },
      {
        "name": "Muhammad Aslam",
        "url": {}
      },
      {
        "name": "Saima Altaf",
        "url": {}
      }
    ],
    "date": "2016-12-12",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmctest, perturb, HH, car, fmsb, rms, faraway, usdm, VIF, leaps, bestglm, glmulti, meifly\nCRAN Task Views implied by cited packages\nSocialSciences, Econometrics, ChemPhys, ClinicalTrials, Finance, Multivariate, ReproducibleResearch, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-044/",
    "title": "Escape from Boxland",
    "description": "A library of common geometric shapes can be used to train our brains for understanding data structure in high-dimensional Euclidean space. This article describes the methods for producing cubes, spheres, simplexes, and tori in multiple dimensions. It also describes new ways to define and generate high-dimensional tori. The algorithms are described, critical code chunks are given, and a large collection of generated data are provided. These are available in the R package geozoo, and selected movies and images, are available on the GeoZoo web site (http://schloerke.github.io/geozoo/).",
    "author": [
      {
        "name": "Barret Schloerke",
        "url": {}
      },
      {
        "name": "Hadley Wickham",
        "url": {}
      },
      {
        "name": "Dianne Cook",
        "url": {}
      },
      {
        "name": "Heike Hofmann",
        "url": {}
      }
    ],
    "date": "2016-11-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngeozoo, tourr, bitops, geozoo, geozoo\nCRAN Task Views implied by cited packages\nMultivariate\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-046/",
    "title": "An Introduction to Principal Surrogate Evaluation with the pseval Package",
    "description": "We describe a new package called pseval that implements the core methods for the evaluation of principal surrogates in a single clinical trial. It provides a flexible interface for defining models for the risk given treatment and the surrogate, the models for integration over the missing counterfactual surrogate responses, and the estimation methods. Estimated maximum likelihood and pseudo-score can be used for estimation, and the bootstrap for inference. A variety of post-estimation methods are provided, including print, summary, plot, and testing. We summarize the main statistical methods that are implemented in the package and illustrate its use from the perspective of a novice R user.",
    "author": [
      {
        "name": "Michael C. Sachs",
        "url": {}
      },
      {
        "name": "Erin E. Gabriel",
        "url": {}
      }
    ],
    "date": "2016-11-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\npseval, survival, survey, ggplot2, lattice, Surrogate\nCRAN Task Views implied by cited packages\nGraphics, SocialSciences, Survival, ClinicalTrials, Econometrics, Multivariate, OfficialStatistics, Pharmacokinetics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-052/",
    "title": "quantreg.nonpar: An R Package for Performing Nonparametric Series Quantile Regression",
    "description": "The R package quantreg.nonpar implements nonparametric quantile regression methods to estimate and make inference on partially linear quantile models. quantreg.nonpar obtains point estimates of the conditional quantile function and its derivatives based on series approximations to the nonparametric part of the model. It also provides pointwise and uniform confidence intervals over a region of covariate values and/or quantile indices for the same functions using analytical and resampling methods. This paper serves as an introduction to the package and displays basic functionality of the functions contained within.",
    "author": [
      {
        "name": "Michael Lipsitz",
        "url": {}
      },
      {
        "name": "Alexandre Belloni",
        "url": {}
      },
      {
        "name": "Victor Chernozhukov",
        "url": {}
      },
      {
        "name": "Iván Fernández-Val",
        "url": {}
      }
    ],
    "date": "2016-11-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nquantreg.nonpar, quantreg, QuantifQuantile, quantregGrowth, fda\nCRAN Task Views implied by cited packages\nEnvironmetrics, Econometrics, Optimization, ReproducibleResearch, Robust, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-053/",
    "title": "nmfgpu4R: GPU-Accelerated Computation of the Non-Negative Matrix Factorization (NMF) Using CUDA Capable Hardware",
    "description": "In this work, a novel package called nmfgpu4R is presented, which offers the computation of Non-negative Matrix Factorization (NMF) on Compute Unified Device Architecture (CUDA) platforms within the R environment. Benchmarks show a remarkable speed-up in terms of time per iteration by utilizing the parallelization capabilities of modern graphics cards. Therefore the application of NMF gets more attractive for real-world sized problems because the time to compute a factorization is reduced by an order of magnitude.",
    "author": [
      {
        "name": "Sven Koitka",
        "url": {}
      },
      {
        "name": "Christoph M. Friedrich",
        "url": {}
      }
    ],
    "date": "2016-11-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nNMF, NMFN, nmfgpu4R, Matrix, SparseM\nCRAN Task Views implied by cited packages\nEconometrics, Multivariate, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-055/",
    "title": "micompr: An R Package for Multivariate Independent Comparison of Observations",
    "description": "The R package micompr implements a procedure for assessing if two or more multivariate samples are drawn from the same distribution. The procedure uses principal component analysis to convert multivariate observations into a set of linearly uncorrelated statistical measures, which are then compared using a number of statistical methods. This technique is independent of the distributional properties of samples and automatically selects features that best explain their differences. The procedure is appropriate for comparing samples of time series, images, spectrometric measures or similar high-dimension multivariate observations.",
    "author": [
      {
        "name": "Nuno Fachada",
        "url": {}
      },
      {
        "name": "João Rodrigues",
        "url": {}
      },
      {
        "name": "Vitor V. Lopes",
        "url": {}
      },
      {
        "name": "Rui C. Martins",
        "url": {}
      },
      {
        "name": "Agostinho C. Rosa",
        "url": {}
      }
    ],
    "date": "2016-11-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmicompr, vegan, Blossom, energy, crossmatch, cramer, ks, ChemoSpec, biotools, MVN, testthat, knitr, roxygen2, deseasonalize\nCRAN Task Views implied by cited packages\nMultivariate, ChemPhys, Environmetrics, Phylogenetics, Psychometrics, ReproducibleResearch, Spatial, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-032/",
    "title": "Design of the TRONCO BioConductor Package for TRanslational ONCOlogy",
    "description": "Models of cancer progression provide insights on the order of accumulation of genetic alterations during cancer development. Algorithms to infer such models from the currently available mutational profiles collected from different cancer patients (cross-sectional data) have been defined in the literature since late the 90s. These algorithms differ in the way they extract a graphical model of the events modelling the progression, e.g., somatic mutations or copy-number alterations. TRONCO is an R package for TRanslational ONcology which provides a series of functions to assist the user in the analysis of cross-sectional genomic data and, in particular, it implements algorithms that aim to model cancer progression by means of the notion of selective advantage. These algorithms are proved to outperform the current state-of-the-art in the inference of cancer progression models. TRONCO also provides functionalities to load input cross-sectional data, set up the execution of the algorithms, assess the statistical confidence in the results, and visualize the models. Availability. Freely available at http://www.bioconductor.org/ under GPL license; project hosted at http://bimib.disco.unimib.it/ and https://github.com/BIMIB-DISCo/TRONCO. Contact. tronco@disco.unimib.it",
    "author": [
      {
        "name": "Marco Antoniotti",
        "url": {}
      },
      {
        "name": "Giulio Caravagna",
        "url": {}
      },
      {
        "name": "Luca De Sano",
        "url": {}
      },
      {
        "name": "Alex Graudenzi",
        "url": {}
      },
      {
        "name": "Giancarlo Mauri",
        "url": {}
      },
      {
        "name": "Bud            Mishra",
        "url": {}
      },
      {
        "name": "Daniele Ramazzotti",
        "url": {}
      }
    ],
    "date": "2016-10-21",
    "categories": [],
    "contents": "\n\n\n\nBioconductor packages used\nTRONCO\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-039/",
    "title": "Variants of Simple Correspondence Analysis",
    "description": "This paper presents the R package CAvariants (Lombardo and Beh, 2017). The package performs six variants of correspondence analysis on a two-way contingency table. The main function that shares the same name as the package – CAvariants – allows the user to choose (via a series of input parameters) from six different correspondence analysis procedures. These include the classical approach to (symmetrical) correspondence analysis, singly ordered correspondence analysis, doubly ordered correspondence analysis, non symmetrical correspondence analysis, singly ordered non symmetrical correspondence analysis and doubly ordered non symmetrical correspondence analysis. The code provides the flexibility for constructing either a classical correspondence plot or a biplot graphical display. It also allows the user to consider other important features that allow to assess the reliability of the graphical representations, such as the inclusion of algebraically derived elliptical confidence regions. This paper provides R functions that elaborates more fully on the code presented in Beh and Lombardo (2014).",
    "author": [
      {
        "name": "Rosaria Lombardo",
        "url": {}
      },
      {
        "name": "Eric J. Beh",
        "url": {}
      }
    ],
    "date": "2016-10-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMASS, ca, anacor, FactoMineR, cabootcrs, CAinterprTools, homals, dualScale, ExPosition, vegan, ade4, cncaGUI, PTAk, CAvariants\nCRAN Task Views implied by cited packages\nPsychometrics, Multivariate, Environmetrics, ChemPhys, Spatial, Distributions, Econometrics, Graphics, MedicalImaging, NumericalMathematics, Pharmacokinetics, Phylogenetics, Robust, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-047/",
    "title": "Calculating Biological Module Enrichment or Depletion and Visualizing Data on Large-scale Molecular Maps with ACSNMineR and RNaviCell Packages",
    "description": "Biological pathways or modules represent sets of interactions or functional relationships occurring at the molecular level in living cells. A large body of knowledge on pathways is organized in public databases such as the KEGG, Reactome, or in more specialized repositories, the Atlas of Cancer Signaling Network (ACSN) being an example. All these open biological databases facilitate analyses, improving our understanding of cellular systems. We hereby describe ACSNMineR for calculation of enrichment or depletion of lists of genes of interest in biological pathways. ACSNMineR integrates ACSN molecular pathways gene sets, but can use any gene set encoded as a GMT file, for instance sets of genes available in the Molecular Signatures Database (MSigDB). We also present RNaviCell, that can be used in conjunction with ACSNMineR to visualize different data types on web-based, interactive ACSN maps. We illustrate the functionalities of the two packages with biological data taken from large-scale cancer datasets.",
    "author": [
      {
        "name": "Paul Deveau",
        "url": {}
      },
      {
        "name": "Emmanuel Barillot",
        "url": {}
      },
      {
        "name": "Valentina Boeva",
        "url": {}
      },
      {
        "name": "Andrei Zinovyev",
        "url": {}
      },
      {
        "name": "Eric Bonnet",
        "url": {}
      }
    ],
    "date": "2016-10-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nACSNMineR, RNaviCell, ggplot2\nCRAN Task Views implied by cited packages\nGraphics, Phylogenetics\nBioconductor packages used\nGOstats, clusterProfiler\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-049/",
    "title": "dCovTS: Distance Covariance/Correlation for Time Series",
    "description": "The distance covariance function is a new measure of dependence between random vectors. We drop the assumption of iid data to introduce distance covariance for time series. The R package dCovTS provides functions that compute and plot distance covariance and correlation functions for both univariate and multivariate time series. Additionally it includes functions for testing serial independence based on distance covariance. This paper describes the theoretical background of distance covariance methodology in time series and discusses in detail the implementation of these methods with the R package dCovTS.",
    "author": [
      {
        "name": "Maria Pitsillou",
        "url": {}
      },
      {
        "name": "Konstantinos Fokianos",
        "url": {}
      }
    ],
    "date": "2016-10-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nenergy, doParallel, portes, MTS\nCRAN Task Views implied by cited packages\nTimeSeries, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-050/",
    "title": "comf: An R Package for Thermal Comfort Studies",
    "description": "The field of thermal comfort generated a number of thermal comfort indices. Their code implementation needs to be done by individual researchers. This paper presents the R package, comf, which includes functions for common and new thermal comfort indices. Additional functions allow comparisons between the predictive performance of these indices. This paper reviews existing thermal comfort indices and available code implementations. This is followed by the description of the R package and an example how to use the R package for the comparison of different thermal comfort indices on data from a thermal comfort study.",
    "author": [
      {
        "name": "Marcel Schweiker",
        "url": {}
      }
    ],
    "date": "2016-10-21",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncomf\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-056/",
    "title": "mixtox: An R Package for Mixture Toxicity Assessment",
    "description": "Mixture toxicity assessment is indeed necessary for humans and ecosystems that are contin ually exposed to a variety of chemical mixtures. This paper describes an R package, called mixtox, which offers a general framework of curve fitting, mixture experimental design, and mixture toxicity prediction for practitioners in toxicology. The unique features of mixtox include: (1) constructing a uniform table for mixture experimental design; and (2) predicting toxicity of a mixture with multiple components based on reference models such as concentration addition, independent action, and generalized concentration addition. We describe the various functions of the package and provide examples to illustrate their use and show the collaboration of mixtox with other existing packages (e.g., drc) in predicting toxicity of chemical mixtures.",
    "author": [
      {
        "name": "Xiang-Wei Zhu",
        "url": {}
      },
      {
        "name": "Jian-Yi Chen",
        "url": {}
      }
    ],
    "date": "2016-10-21",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-060/",
    "title": "ggfortify: Unified Interface to Visualize Statistical Results of Popular R Packages",
    "description": "The ggfortify package provides a unified interface that enables users to use one line of code to visualize statistical results of many R packages using ggplot2 idioms. With the help of ggfortify, statisticians, data scientists, and researchers can avoid the sometimes repetitive work of using the ggplot2 syntax to achieve what they need.",
    "author": [
      {
        "name": "Yuan Tang",
        "url": {}
      },
      {
        "name": "Masaaki Horikoshi",
        "url": {}
      },
      {
        "name": "Wenxuan Li",
        "url": {}
      }
    ],
    "date": "2016-09-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nlattice, ggplot2, ggfortify, cluster, lfda, zoo, xts, timeSeries, forecast, changepoint, strucchange, dlm, dplyr, tidyr, gridExtra, scales\nCRAN Task Views implied by cited packages\nTimeSeries, Finance, Econometrics, Environmetrics, Graphics, Multivariate, Bayesian, Cluster, Pharmacokinetics, Phylogenetics, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-031/",
    "title": "QPot: An R Package for Stochastic Differential Equation Quasi-Potential Analysis",
    "description": "QPot (pronounced kyoo + p ät) is an R package for analyzing two-dimensional systems of stochastic differential equations. It provides users with a wide range of tools to simulate, analyze, and visualize the dynamics of these systems. One of QPot’s key features is the computation of the quasi-potential, an important tool for studying stochastic systems. Quasi-potentials are particularly useful for comparing the relative stabilities of equilibria in systems with alternative stable states. This paper describes QPot’s primary functions, and explains how quasi-potentials can yield insights about the dynamics of stochastic systems. Three worked examples guide users through the application of QPot’s functions.",
    "author": [
      {
        "name": "Christopher M. Moore",
        "url": {}
      },
      {
        "name": "Christopher R. Stieha",
        "url": {}
      },
      {
        "name": "Ben C. Nolting",
        "url": {}
      },
      {
        "name": "Maria K. Cameron",
        "url": {}
      },
      {
        "name": "Karen C.            Abbott",
        "url": {}
      }
    ],
    "date": "2016-09-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nQPot, rootSolve, deSolve, phaseR, Sim.DiffProc, yuima, viridis, plot3D, rgl\nCRAN Task Views implied by cited packages\nDifferentialEquations, TimeSeries, Finance, Graphics, Multivariate, Pharmacokinetics, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-034/",
    "title": "Simulating Correlated Binary and Multinomial Responses under Marginal Model Specification: The SimCorMultRes Package",
    "description": "We developed the R package SimCorMultRes to facilitate simulation of correlated categori cal (binary and multinomial) responses under a desired marginal model specification. The simulated correlated categorical responses are obtained by applying threshold approaches to correlated contin uous responses of underlying regression models and the dependence structure is parametrized in terms of the correlation matrix of the latent continuous responses. This article provides an elaborate introduction to the SimCorMultRes package demonstrating its design and usage via three examples. The package can be obtained via CRAN.",
    "author": [
      {
        "name": "Anestis Touloumis",
        "url": {}
      }
    ],
    "date": "2016-09-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nSimCorMultRes, GenOrd, MultiOrd, mvtBinaryEP, multgee\nCRAN Task Views implied by cited packages\nDistributions, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-035/",
    "title": "eiCompare: Comparing Ecological Inference Estimates across EI and EI:RC",
    "description": "Social scientists and statisticians often use aggregate data to predict individual-level behavior because the latter are not always available. Various statistical techniques have been developed to make inferences from one level (e.g., precinct) to another level (e.g., individual voter) that minimize errors associated with ecological inference. While ecological inference has been shown to be highly problematic in a wide array of scientific fields, many political scientists and analysis employ the techniques when studying voting patterns. Indeed, federal voting rights lawsuits now require such an analysis, yet expert reports are not consistent in which type of ecological inference is used. This is especially the case in the analysis of racially polarized voting when there are multiple candidates and multiple racial groups. The eiCompare package was developed to easily assess two of the more common ecological inference methods: EI and EI:R×C. The package facilitates a seamless comparison between these methods so that scholars and legal practitioners can easily assess the two methods and whether they produce similar or disparate findings.",
    "author": [
      {
        "name": "Loren Collingwood",
        "url": {}
      },
      {
        "name": "Kassra Oskooii",
        "url": {}
      },
      {
        "name": "Sergio Garcia-Rios",
        "url": {}
      },
      {
        "name": "Matt Barreto",
        "url": {}
      }
    ],
    "date": "2016-09-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nei, eiPack, eiCompare\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-038/",
    "title": "Two-Tier Latent Class IRT Models in R",
    "description": "In analyzing data deriving from the administration of a questionnaire to a group of individu als, Item Response Theory (IRT) models provide a flexible framework to account for several aspects involved in the response process, such as the existence of multiple latent traits. In this paper, we focus on a class of semi-parametric multidimensional IRT models, in which these traits are represented through one or more discrete latent variables; these models allow us to cluster individuals into homo geneous latent classes and, at the same time, to properly study item characteristics. In particular, we follow a within-item multidimensional formulation similar to that adopted in the two-tier models, with each item measuring one or two latent traits. The proposed class of models may be estimated through the package MLCIRTwithin, whose functioning is illustrated in this paper with examples based on data about quality-of-life measurement and about the propensity to commit a crime.",
    "author": [
      {
        "name": "Silvia Bacci",
        "url": {}
      },
      {
        "name": "Francesco Bartolucci",
        "url": {}
      }
    ],
    "date": "2016-09-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMLCIRTwithin, MultiLCIRT, CDM, mirt, flirt, covLCA, lavaan, OpenMx, LMest\nCRAN Task Views implied by cited packages\nPsychometrics, Econometrics, OfficialStatistics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-040/",
    "title": "hdm: High-Dimensional Metrics",
    "description": "In this article the package High-dimensional Metrics hdm is introduced. It is a collection of statistical methods for estimation and quantification of uncertainty in high-dimensional approximately sparse models. It focuses on providing confidence intervals and significance testing for (possibly many) low-dimensional subcomponents of the high-dimensional parameter vector. Efficient estimators and uniformly valid confidence intervals for regression coefficients on target variables (e.g., treatment or policy variable) in a high-dimensional approximately sparse regression model, for average treatment effect (ATE) and average treatment effect for the treated (ATET), as well for extensions of these param eters to the endogenous setting are provided. Theory grounded, data-driven methods for selecting the penalization parameter in Lasso regressions under heteroscedastic and non-Gaussian errors are implemented. Moreover, joint/ simultaneous confidence intervals for regression coefficients of a high-dimensional sparse regression are implemented. Data sets which have been used in the literature and might be useful for classroom demonstration and for testing new estimators are included.",
    "author": [
      {
        "name": "Victor Chernozhukov",
        "url": {}
      },
      {
        "name": "Chris Hansen",
        "url": {}
      },
      {
        "name": "Martin Spindler",
        "url": {}
      }
    ],
    "date": "2016-09-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nglmnet, lars, hdm\nCRAN Task Views implied by cited packages\nMachineLearning, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-048/",
    "title": "Subgroup Discovery with Evolutionary Fuzzy Systems in R: The SDEFSR Package",
    "description": "Subgroup discovery is a data mining task halfway between descriptive and predictive data mining. Nowadays it is very relevant for researchers due to the fact that the knowledge extracted is simple and interesting. For this task, evolutionary fuzzy systems are well suited algorithms because they can find a good trade-off between multiple objectives in large search spaces. In fact, this paper presents the SDEFSR package, which contains all the evolutionary fuzzy systems for subgroup discovery presented throughout the literature. It is a package without dependencies on other software, providing functions with recommended default parameters. In addition, it brings a graphical user interface to avoid the user having to know all the parameters of the algorithms.",
    "author": [
      {
        "name": "Ángel M. García",
        "url": {}
      },
      {
        "name": "Francisco Charte",
        "url": {}
      },
      {
        "name": "Pedro González",
        "url": {}
      },
      {
        "name": "Cristóbal J. Carmona",
        "url": {}
      },
      {
        "name": "María J. del Jesus",
        "url": {}
      }
    ],
    "date": "2016-09-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrsubgroup, SDEFSR, devtools, mldr\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-058/",
    "title": "Distance Measures for Time Series in R: The TSdist Package",
    "description": "The definition of a distance measure between time series is crucial for many time series data mining tasks, such as clustering and classification. For this reason, a vast portfolio of time series distance measures has been published in the past few years. In this paper, the TSdist package is presented, a complete tool which provides a unified framework to calculate the largest variety of time series dissimilarity measures available in R at the moment, to the best of our knowledge. The package implements some popular distance measures which were not previously available in R, and moreover, it also provides wrappers for measures already included in other R packages. Additionally, the application of these distance measures to clustering and classification tasks is also supported in TSdist, directly enabling the evaluation and comparison of their performance within these two frameworks.",
    "author": [
      {
        "name": "Usue Mori",
        "url": {}
      },
      {
        "name": "Alexander Mendiburu",
        "url": {}
      },
      {
        "name": "Jose A. Lozano",
        "url": {}
      }
    ],
    "date": "2016-09-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nTSdist, dtw, pdc, proxy, longitudinalData, TSclust, zoo, xts\nCRAN Task Views implied by cited packages\nTimeSeries, Econometrics, Finance, Environmetrics, Multivariate, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-030/",
    "title": "multipleNCC: Inverse Probability Weighting of Nested Case-Control Data",
    "description": "Reuse of controls from nested case-control designs can increase efficiency in many situations, for instance with competing risks or in other multiple endpoints situations. The matching between cases and controls must be broken when controls are to be used for other endpoints. A weighted analysis can then be performed to take care of the biased sampling from the cohort. We present the R package multipleNCC for reuse of controls in nested case-control studies by inverse probability weighting of the partial likelihood. The package handles right-censored, left-truncated and additionally matched data, and varying numbers of sampled controls and the whole analysis is carried out using one simple command. Four weight estimators are presented and variance estimation is explained. The package is illustrated by analyzing health survey data from three counties in Norway for two causes of death: cardiovascular disease and death from alcohol abuse, liver disease, and accidents and violence. The data set is included in the package.",
    "author": [
      {
        "name": "Nathalie C. Støer",
        "url": {}
      },
      {
        "name": "Sven Ove Samuelsen",
        "url": {}
      }
    ],
    "date": "2016-08-11",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmultipleNCC, survival, mgcv, ipw, MatchIt, NestedCohort, survey, Epi, gam\nCRAN Task Views implied by cited packages\nSocialSciences, Survival, Econometrics, Environmetrics, OfficialStatistics, Bayesian, ClinicalTrials\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-043/",
    "title": "tigris: An R Package to Access and Work with Geographic Data from the US Census Bureau",
    "description": "TIGER/Line shapefiles from the United States Census Bureau are commonly used for the mapping and analysis of US demographic trends. The tigris package provides a uniform interface for R users to download and work with these shapefiles. Functions in tigris allow R users to request Census geographic datasets using familiar geographic identifiers and return those datasets as objects of class \"Spatial*DataFrame\". In turn, tigris ensures consistent and high-quality spatial data for R users’ cartographic and spatial analysis projects that involve US Census data. This article provides an overview of the functionality of the tigris package, and concludes with an applied example of a geospatial workflow using data retrieved with tigris.",
    "author": [
      {
        "name": "Kyle Walker",
        "url": {}
      }
    ],
    "date": "2016-08-11",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntigris, rgdal, sp, UScensus2010, USABoundaries, choroplethr, ggplot2, sp, rappdirs, dplyr, tmap, shiny, leaflet, devtools\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, Graphics, OfficialStatistics, Phylogenetics, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-005/",
    "title": "Maps, Coordinate Reference Systems and Visualising Geographic Data with mapmisc",
    "description": "The mapmisc package provides functions for visualising geospatial data, including fetching background map layers, producing colour scales and legends, and adding scale bars and orientation arrows to plots. Background maps are returned in the coordinate reference system of the dataset supplied, and inset maps and direction arrows reflect the map projection being plotted. This is a “light weight” package having an emphasis on simplicity and ease of use.",
    "author": [
      {
        "name": "Patrick E. Brown",
        "url": {}
      }
    ],
    "date": "2016-07-28",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsp, raster, mapmisc, rgdal, RColorBrewer, classInt, rgeos, geosphere, dismo, maptools, R.utils, geostatsp, knitr, ggplot2, leaflet\nCRAN Task Views implied by cited packages\nSpatial, Graphics, SpatioTemporal, Phylogenetics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-024/",
    "title": "statmod: Probability Calculations for the Inverse Gaussian Distribution",
    "description": "The inverse Gaussian distribution (IGD) is a well known and often used probability dis tribution for which fully reliable numerical algorithms have not been available. We develop fast, reliable basic probability functions (dinvgauss, pinvgauss, qinvgauss and rinvgauss) for the IGD that work for all possible parameter values and which achieve close to full machine accuracy. The most challenging task is to compute quantiles for given cumulative probabilities and we develop a simple but elegant mathematical solution to this problem. We show that Newton’s method for finding the quantiles of a IGD always converges monotonically when started from the mode of the distribution. Simple Taylor series expansions are used to improve accuracy on the log-scale. The IGD probability functions provide the same options and obey the same conventions as do probability functions provided in the stats package.",
    "author": [
      {
        "name": "Göknur Giner",
        "url": {}
      },
      {
        "name": "Gordon K. Smyth",
        "url": {}
      }
    ],
    "date": "2016-07-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nSuppDists, STAR, statmod\nCRAN Task Views implied by cited packages\nDistributions, HighPerformanceComputing, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-027/",
    "title": "Nonparametric Tests for the Interaction in Two-way Factorial Designs Using R",
    "description": "An increasing number of R packages include nonparametric tests for the interaction in two-way factorial designs. This paper briefly describes the different methods of testing and reports the resulting p-values of such tests on datasets for four types of designs: between, within, mixed, and pretest-posttest designs. Potential users are advised only to apply tests they are quite familiar with and not be guided by p-values for selecting packages and tests.",
    "author": [
      {
        "name": "Jos Feys",
        "url": {}
      }
    ],
    "date": "2016-07-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nWRS2, nparLD, coin, lmPerm, perm, ez, boot, ART, ARTool, npIntFactRep, Rfit, StatMethRank, outliers, npsm, cocor\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Econometrics, ExperimentalDesign, Optimization, Psychometrics, Robust, SocialSciences, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-002/",
    "title": "Gender Prediction Methods Based on First Names with genderizeR",
    "description": "In recent years, there has been increased interest in methods for gender prediction based on first names that employ various open data sources. These methods have applications from bibliometric studies to customizing commercial offers for web users. Analysis of gender disparities in science based on such methods are published in the most prestigious journals, although they could be improved by choosing the most suited prediction method with optimal parameters and performing validation studies using the best data source for a given purpose. There is also a need to monitor and report how well a given prediction method works in comparison to others. In this paper, the author recommends a set of tools (including one dedicated to gender prediction, the R package called genderizeR), data sources (including the genderize.io API), and metrics that could be fully reproduced and tested in order to choose the optimal approach suitable for different gender analyses.",
    "author": [
      {
        "name": "Kamil Wais",
        "url": {}
      }
    ],
    "date": "2016-07-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngenderizeR, qdap, gender, babynames, sortinghat, stringr, tm, ROCR, verification, data.table, dplyr\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NaturalLanguageProcessing, Finance, MachineLearning, Multivariate, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-029/",
    "title": "sbtools: A Package Connecting R to Cloud-based Data for Collaborative Online Research",
    "description": "The adoption of high-quality tools for collaboration and reproducibile research such as R and Github is becoming more common in many research fields. While Github and other version management systems are excellent resources, they were originally designed to handle code and scale poorly to large text-based or binary datasets. A number of scientific data repositories are coming online and are often focused on dataset archival and publication. To handle collaborative workflows using large scientific datasets, there is increasing need to connect cloud-based online data storage to R. In this article, we describe how the new R package sbtools enables direct access to the advanced online data functionality provided by ScienceBase, the U.S. Geological Survey’s online scientific data storage platform.",
    "author": [
      {
        "name": "Luke A Winslow",
        "url": {}
      },
      {
        "name": "Scott Chamberlain",
        "url": {}
      },
      {
        "name": "Alison P Appling",
        "url": {}
      },
      {
        "name": "Jordan S Read",
        "url": {}
      }
    ],
    "date": "2016-07-23",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-001/",
    "title": "metaplus: An R Package for the Analysis of Robust Meta-Analysis and Meta-Regression",
    "description": "The metaplus package is described with examples of its use for fitting meta-analysis and meta-regression. For either meta-analysis or meta-regression it is possible to fit one of three models: standard normal random effect, t-distribution random effect or mixture of normal random effects. The latter two models allow for robustness by allowing for a random effect distribution with heavier tails than the normal distribution, and for both robust models the presence of outliers may be tested using the parametric bootstrap. For the mixture of normal random effects model the outlier studies may be identified through their posterior probability of membership in the outlier component of the mixture. Plots allow the results of the different models to be compared. The package is demonstrated on three examples: a meta-analysis with no outliers, a meta-analysis with an outlier and a meta-regression with an outlier.",
    "author": [
      {
        "name": "Ken J. Beath",
        "url": {}
      }
    ],
    "date": "2016-06-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmetaplus, metafor, bbmle, forestplot, extrafont\nCRAN Task Views implied by cited packages\nMetaAnalysis, ClinicalTrials, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-014/",
    "title": "Spatio-Temporal Interpolation using gstat",
    "description": "We present new spatio-temporal geostatistical modelling and interpolation capabilities of the R package gstat. Various spatio-temporal covariance models have been implemented, such as the separable, product-sum, metric and sum-metric models. In a real-world application we compare spatio temporal interpolations using these models with a purely spatial kriging approach. The target variable of the application is the daily mean PM10 concentration measured at rural air quality monitoring stations across Germany in 2005. R code for variogram fitting and interpolation is presented in this paper to illustrate the workflow of spatio-temporal interpolation using gstat. We conclude that the system works properly and that the extension of gstat facilitates and eases spatio-temporal geostatistical modelling and prediction for R users.",
    "author": [
      {
        "name": "Benedikt Gräler",
        "url": {}
      },
      {
        "name": "Edzer Pebesma",
        "url": {}
      },
      {
        "name": "Gerard Heuvelink",
        "url": {}
      }
    ],
    "date": "2016-06-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspacetime, gstat, RandomFields, spTimer, spBayes, spate, FNN\nCRAN Task Views implied by cited packages\nSpatioTemporal, Spatial, Bayesian, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-020/",
    "title": "Crowdsourced Data Preprocessing with R and Amazon Mechanical Turk",
    "description": "This article introduces the use of the Amazon Mechanical Turk (MTurk) crowdsourcing platform as a resource for R users to leverage crowdsourced human intelligence for preprocessing “messy” data into a form easily analyzed within R. The article first describes MTurk and the MTurkR package, then outlines how to use MTurkR to gather and manage crowdsourced data with MTurk using some of the package’s core functionality. Potential applications of MTurkR include construction of manually coded training sets, human transcription and translation, manual data scraping from scanned documents, content analysis, image classification, and the completion of online survey questionnaires, among others. As an example of massive data preprocessing, the article describes an image rating task involving 225 crowdsourced workers and more than 5500 images using just three MTurkR function calls.",
    "author": [
      {
        "name": "Thomas J. Leeper",
        "url": {}
      }
    ],
    "date": "2016-06-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMTurkR, MTurkRGUI, tcltk, curl, XML\nCRAN Task Views implied by cited packages\nWebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-021/",
    "title": "mclust 5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models",
    "description": "Finite mixture models are being used increasingly to model a wide variety of random phenomena for clustering, classification and density estimation. mclust is a powerful and popular package which allows modelling of data as a Gaussian finite mixture with different covariance structures and different numbers of mixture components, for a variety of purposes of analysis. Recently, version 5 of the package has been made available on CRAN. This updated version adds new covariance structures, dimension reduction capabilities for visualisation, model selection criteria, initialisation strategies for the EM algorithm, and bootstrap-based inference, making it a full-featured R package for data analysis via finite mixture modelling.",
    "author": [
      {
        "name": "Luca Scrucca",
        "url": {}
      },
      {
        "name": "Michael Fop",
        "url": {}
      },
      {
        "name": "T. Brendan Murphy",
        "url": {}
      },
      {
        "name": "Adrian E. Raftery",
        "url": {}
      }
    ],
    "date": "2016-06-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmclust, cranlogs, Rmixmod, mixture, EMCluster, mixtools, bgmm, flexmix, igraph, gclus, rrcov, tourr, fpc\nCRAN Task Views implied by cited packages\nCluster, Multivariate, Distributions, Environmetrics, Graphics, gR, Optimization, Psychometrics, Robust, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-026/",
    "title": "R Packages to Aid in Handling Web Access Logs",
    "description": "Web access logs contain information on HTTP(S) requests and form a key part of both industry and academic explorations of human behaviour on the internet. But the preparation (reading, parsing and manipulation) of that data is just unique enough to make generalized tools unfit for the task, both in programming time and processing time which are compounded when dealing with large data sets common with web access logs. In this paper we explain and demonstrate a series of packages designed to efficiently read in, parse and munge access log data, allowing researchers to handle URLs and IP addresses easily. These packages are substantially faster than existing R methods from a 3-500% speedup for file reading to a 57,000% speedup in URL parsing.",
    "author": [
      {
        "name": "Oliver Keyes",
        "url": {}
      },
      {
        "name": "Bob Rudis",
        "url": {}
      },
      {
        "name": "Jay Jacobs",
        "url": {}
      }
    ],
    "date": "2016-06-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nhttr, ApacheLogProcessor, webreadr, readr, microbenchmark, urltools, httr, XML, lubridate, iptools, rgeolocate, Rcpp\nCRAN Task Views implied by cited packages\nWebTechnologies, HighPerformanceComputing, NumericalMathematics, ReproducibleResearch, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-028/",
    "title": "GMDH: An R Package for Short Term Forecasting via GMDH-Type Neural Network Algorithms",
    "description": "Group Method of Data Handling (GMDH)-type neural network algorithms are the heuristic self organization method for the modelling of complex systems. GMDH algorithms are utilized for a variety of purposes, examples include identification of physical laws, the extrapolation of physical fields, pattern recognition, clustering, the approximation of multidimensional processes, forecasting without models, etc. In this study, the R package GMDH is presented to make short term forecasting through GMDH-type neural network algorithms. The GMDH package has options to use different transfer functions (sigmoid, radial basis, polynomial, and tangent functions) simultaneously or separately. Data on cancer death rate of Pennsylvania from 1930 to 2000 are used to illustrate the features of the GMDH package. The results based on ARIMA models and exponential smoothing methods are included for comparison.",
    "author": [
      {
        "name": "Osman Dag",
        "url": {}
      },
      {
        "name": "Ceylan Yozgatligil",
        "url": {}
      }
    ],
    "date": "2016-06-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nglarma, ftsa, MARSS, ensembleBMA, ProbForecastGOP, forecast\nCRAN Task Views implied by cited packages\nTimeSeries, Bayesian, Econometrics, Environmetrics, Finance\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-018/",
    "title": "keyplayer: An R Package for Locating Key Players in Social Networks",
    "description": "Interest in social network analysis has exploded in the past few years, partly thanks to the advancements in statistical methods and computing for network analysis. A wide range of the methods for network analysis is already covered by existent R packages. However, no comprehensive packages are available to calculate group centrality scores and to identify key players (i.e., those players who constitute the most central group) in a network. These functionalities are important because, for example, many social and health interventions rely on key players to facilitate the intervention. Identifying key players is challenging because players who are individually the most central are not necessarily the most central as a group due to redundancy in their connections. In this paper we develop methods and tools for computing group centrality scores and for identifying key players in social networks. We illustrate the methods using both simulated and empirical examples. The package keyplayer providing the presented methods is available from Comprehensive R Archive Network (CRAN).",
    "author": [
      {
        "name": "Weihua An",
        "url": {}
      },
      {
        "name": "Yu-Hsin Liu",
        "url": {}
      }
    ],
    "date": "2016-05-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nnetwork, sna, igraph, statnet, RSiena, keyplayer, influenceR\nCRAN Task Views implied by cited packages\nSocialSciences, gR, Optimization, Bayesian, Graphics, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-022/",
    "title": "clustering.sc.dp: Optimal Clustering with Sequential Constraint by Using Dynamic Programming",
    "description": "The general clustering algorithms do not guarantee optimality because of the hardness of the problem. Polynomial-time methods can find the clustering corresponding to the exact optimum only in special cases. For example, the dynamic programming algorithm can solve the one-dimensional clustering problem, i.e., when the items to be clustered can be characterised by only one scalar number. Optimal one-dimensional clustering is provided by package Ckmeans.1d.dp in R. The paper shows a possible generalisation of the method implemented in this package to multidimensional data: the dynamic programming method can be applied to find the optimum clustering of vectors when only subsequent items may form a cluster. Sequential data are common in various fields including telecommunication, bioinformatics, marketing, transportation etc. The proposed algorithm can determine the optima for a range of cluster numbers in order to support the case when the number of clusters is not known in advance.",
    "author": [
      {
        "name": "Tibor Szkaliczki",
        "url": {}
      }
    ],
    "date": "2016-05-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nCkmeans.1d.dp, clustering.sc.dp\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-023/",
    "title": "progenyClust: an R package for Progeny Clustering",
    "description": "Identifying the optimal number of clusters is a common problem faced by data scientists in various research fields and industry applications. Though many clustering evaluation techniques have been developed to solve this problem, the recently developed algorithm Progeny Clustering is a much faster alternative and one that is relevant to biomedical applications. In this paper, we introduce an R package progenyClust that implements and extends the original Progeny Clustering algorithm for evaluating clustering stability and identifying the optimal cluster number. We illustrate its applicability using two examples: a simulated test dataset for proof-of-concept, and a cell imaging dataset for demonstrating its application potential in biomedical research. The progenyClust package is versatile in that it offers great flexibility for picking methods and tuning parameters. In addition, the default parameter setting as well as the plot and summary methods offered in the package make the application of Progeny Clustering straightforward and coherent.",
    "author": [
      {
        "name": "Chenyue W. Hu",
        "url": {}
      },
      {
        "name": "Amina A. Qutub",
        "url": {}
      }
    ],
    "date": "2016-05-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncclust, clusterSim, cluster, Nbclust, fpc, progenyClust, stat, Hmisc\nCRAN Task Views implied by cited packages\nCluster, Multivariate, Bayesian, ClinicalTrials, Econometrics, Environmetrics, OfficialStatistics, ReproducibleResearch, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-025/",
    "title": "Using DECIPHER v2.0 to Analyze Big Biological Sequence Data in R",
    "description": "In recent years, the cost of DNA sequencing has decreased at a rate that has outpaced improvements in memory capacity. It is now common to collect or have access to many gigabytes of biological sequences. This has created an urgent need for approaches that analyze sequences in subsets without requiring all of the sequences to be loaded into memory at one time. It has also opened opportunities to improve the organization and accessibility of information acquired in sequencing projects. The DECIPHER package offers solutions to these problems by assisting in the curation of large sets of biological sequences stored in compressed format inside a database. This approach has many practical advantages over standard bioinformatics workflows, and enables large analyses that would otherwise be prohibitively time consuming.",
    "author": [
      {
        "name": "Erik S. Wright",
        "url": {}
      }
    ],
    "date": "2016-05-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRSQLite\nCRAN Task Views implied by cited packages\nDatabases\nBioconductor packages used\nBiostrings, DECIPHER\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-009/",
    "title": "FWDselect: An R Package for Variable Selection in Regression Models",
    "description": "In multiple regression models, when there are a large number (p) of explanatory variables which may or may not be relevant for predicting the response, it is useful to be able to reduce the model. To this end, it is necessary to determine the best subset of q (q ≤ p) predictors which will establish the model with the best prediction capacity. FWDselect package introduces a new forward stepwise based selection procedure to select the best model in different regression frameworks (parametric or nonparametric). The developed methodology, which can be equally applied to linear models, generalized linear models or generalized additive models, aims to introduce solutions to the following two topics: i) selection of the best combination of q variables by using a step-by-step method; and, perhaps, most importantly, ii) search for the number of covariates to be included in the model based on bootstrap resampling techniques. The software is illustrated using real and simulated data.",
    "author": [
      {
        "name": "Marta Sestelo",
        "url": {}
      },
      {
        "name": "Nora M. Villanueva",
        "url": {}
      },
      {
        "name": "Luis Meira-Machado",
        "url": {}
      },
      {
        "name": "Javier Roca-Pardiñas",
        "url": {}
      }
    ],
    "date": "2016-04-20",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmeifly, leaps, subselect, leaps, subselect, lars, glmnet, glmulti, bestglm, mgcv, FWDselect\nCRAN Task Views implied by cited packages\nChemPhys, SocialSciences, MachineLearning, Bayesian, Econometrics, Environmetrics, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-003/",
    "title": "Conditional Fractional Gaussian Fields with the Package FieldSim",
    "description": "We propose an effective and fast method to simulate multidimensional conditional fractional Gaussian fields with the package FieldSim. Our method is valid not only for conditional simulations associated to fractional Brownian fields, but to any Gaussian field and on any (non regular) grid of points.",
    "author": [
      {
        "name": "Alexandre Brouste",
        "url": {}
      },
      {
        "name": "Jacques Istas",
        "url": {}
      },
      {
        "name": "Sophie Lambert-Lacroix",
        "url": {}
      }
    ],
    "date": "2016-04-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nFieldSim, RandomFields\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-006/",
    "title": "Variable Clustering in High-Dimensional Linear Regression: The R Package clere",
    "description": "Dimension reduction is one of the biggest challenges in high-dimensional regression models. We recently introduced a new methodology based on variable clustering as a means to reduce dimen sionality. We present here the R package clere that implements some refinements of this methodology. An overview of the package functionalities as well as examples to run an analysis are described. Numerical experiments on real data were performed to illustrate the good predictive performance of our parsimonious method compared to standard dimension reduction approaches.",
    "author": [
      {
        "name": "Loïc Yengo",
        "url": {}
      },
      {
        "name": "Julien Jacques",
        "url": {}
      },
      {
        "name": "Christophe Biernacki",
        "url": {}
      },
      {
        "name": "Mickael Canouil",
        "url": {}
      }
    ],
    "date": "2016-04-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nglmnet, spikeslab, clere, Rcpp, RcppEigen, lasso2, flare\nCRAN Task Views implied by cited packages\nMachineLearning, NumericalMathematics, Bayesian, HighPerformanceComputing, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-011/",
    "title": "Exploring Interaction Effects in Two-Factor Studies using the hiddenf Package in R.",
    "description": "In crossed, two-factor studies with one observation per factor-level combination, interaction effects between factors can be hard to detect and can make the choice of a suitable statistical model difficult. This article describes hiddenf, an R package that enables users to quantify and characterize a certain form of interaction in two-factor layouts. When effects of one factor (a) fall into two groups depending on the level of another factor, and (b) are constant within these groups, the interaction pattern is deemed \"hidden additivity\" because within groups, the effects of the two factors are additive, while between groups the factors are allowed to interact. The hiddenf software can be used to estimate, test, and report an appropriate factorial effects model corresponding to hidden additivity, which is intermediate between the unavailable full factorial model and the overly-simplistic additive model. Further, the software also conducts five statistical tests for interaction proposed between 1949 and 2014. A collection of 17 datasets is used for illustration.",
    "author": [
      {
        "name": "Christopher T. Franck",
        "url": {}
      },
      {
        "name": "Jason A. Osborne",
        "url": {}
      }
    ],
    "date": "2016-04-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nhiddenf, additivityTests\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-013/",
    "title": "Model Builder for Item Factor Analysis with OpenMx",
    "description": "We introduce a shiny web application to facilitate the construction of Item Factor Analysis (a.k.a. Item Response Theory) models using the OpenMx package. The web application assists with importing data, outcome recoding, and model specification. However, the app does not conduct any analysis but, rather, generates an analysis script. Generated Rmarkdown output serves dual purposes: to analyze a data set and demonstrate good programming practices. The app can be used as a teaching tool or as a starting point for custom analysis scripts.",
    "author": [
      {
        "name": "Joshua N. Pritikin",
        "url": {}
      },
      {
        "name": "Karen M. Schmidt",
        "url": {}
      }
    ],
    "date": "2016-04-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nOpenMx, shiny, Rmarkdown, ifaTools, rpf\nCRAN Task Views implied by cited packages\nPsychometrics, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-015/",
    "title": "SWMPr: An R Package for Retrieving, Organizing, and Analyzing Environmental Data for Estuaries",
    "description": "The System-Wide Monitoring Program (SWMP) was implemented in 1995 by the US National Estuarine Research Reserve System. This program has provided two decades of continuous monitoring data at over 140 fixed stations in 28 estuaries. However, the increasing quantity of data provided by the monitoring network has complicated broad-scale comparisons between systems and, in some cases, prevented simple trend analysis of water quality parameters at individual sites. This article describes the SWMPr package that provides several functions that facilitate data retrieval, organization, and analysis of time series data in the reserve estuaries. Previously unavailable functions for estuaries are also provided to estimate rates of ecosystem metabolism using the open-water method. The SWMPr package has facilitated a cross-reserve comparison of water quality trends and links quantitative information with analysis tools that have use for more generic applications to environmental time series.",
    "author": [
      {
        "name": "Marcus W Beck",
        "url": {}
      }
    ],
    "date": "2016-04-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nSWMPr, shiny, cents, wq, ggmap, StreamMetabolism\nCRAN Task Views implied by cited packages\nWebTechnologies, Environmetrics, Spatial, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-016/",
    "title": "CryptRndTest: An R Package for Testing the Cryptographic Randomness",
    "description": "In this article, we introduce the R package CryptRndTest that performs eight statistical randomness tests on cryptographic random number sequences. The purpose of the package is to provide software implementing recently proposed cryptographic randomness tests utilizing goodness of-fit tests superior to the usual chi-square test in terms of statistical performance. Most of the tests included in package CryptRndTest are not available in other software packages such as the R package RDieHarder or the C library TestU01. Chi-square, Anderson-Darling, Kolmogorov-Smirnov, and Jarque-Bera goodness-of-fit procedures are provided along with cryptographic randomness tests. CryptRndTest utilizes multiple precision floating numbers for sequences longer than 64-bit based on the package Rmpfr. By this way, included tests are applied precisely for higher bit-lengths. In addition CryptRndTest provides a user friendly interface to these cryptographic randomness tests. As an illustrative application, CryptRndTest is used to test available random number generators in R.",
    "author": [
      {
        "name": "Haydar Demirhan",
        "url": {}
      },
      {
        "name": "Nihan Bitirim",
        "url": {}
      }
    ],
    "date": "2016-04-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRDieHarder, randtests, DescTools, CryptRndTest, Rmpfr, kSamples, tseries, copula, gmp\nCRAN Task Views implied by cited packages\nDistributions, Finance, NumericalMathematics, Econometrics, Environmetrics, ExtremeValue, Multivariate, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-019/",
    "title": "SchemaOnRead: A Package for Schema-on-Read in R",
    "description": "SchemaOnRead is a CRAN package that provides an extensible mechanism for importing a wide range of file types into R as well as support for the emerging schema-on-read paradigm in R. The schema-on-read tools within the package include a single function call that recursively reads folders with text, comma separated value, raster image, R data, HDF5, NetCDF, spreadsheet, Weka, Epi Info, Pajek network, R network, HTML, SPSS, Systat, and Stata files. It also recursively reads folders (e.g., schemaOnRead(\"folder\")), returning a nested list of the contained elements. The provided tools can be used as-is or easily customized to implement tool chains in R. This paper’s contribution is that it introduces and describes the SchemaOnRead package and compares it to related R packages.",
    "author": [
      {
        "name": "Michael J. North",
        "url": {}
      }
    ],
    "date": "2016-04-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nSchemaOnRead, rio, readbitmap, foreign, testthat\nCRAN Task Views implied by cited packages\nOfficialStatistics, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-004/",
    "title": "rTableICC: An R Package for Random Generation of 22K and RC Contingency Tables",
    "description": "In this paper, we describe the R package rTableICC that provides an interface for random generation of 2×2×K and R×C contingency tables constructed over either intraclass-correlated or uncorrelated individuals. Intraclass correlations arise in studies where sampling units include more than one individual and these individuals are correlated. The package implements random generation of contingency tables over individuals with or without intraclass correlations under various sampling plans. The package include two functions for the generation of K 2×2 tables over product-multinomial sampling schemes and that of 2×2×K tables under Poisson or multinomial sampling plans. It also contains two functions that generate R×C tables under product-multinomial, multinomial or Poisson sampling plans with or without intraclass correlations. The package also includes a function for random number generation from a given probability distribution. In addition to the contingency table format, the package also provides raw data required for further estimation purposes.",
    "author": [
      {
        "name": "Haydar Demirhan",
        "url": {}
      }
    ],
    "date": "2016-04-02",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrTableICC, partitions\nCRAN Task Views implied by cited packages\nNumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-007/",
    "title": "Stylometry with R: A Package for Computational Text Analysis",
    "description": "This software paper describes ‘Stylometry with R’ (stylo), a flexible R package for the high level analysis of writing style in stylometry. Stylometry (computational stylistics) is concerned with the quantitative study of writing style, e.g. authorship verification, an application which has considerable potential in forensic contexts, as well as historical research. In this paper we introduce the possibilities of stylo for computational text analysis, via a number of dummy case studies from English and French literature. We demonstrate how the package is particularly useful in the exploratory statistical analysis of texts, e.g. with respect to authorial writing style. Because stylo provides an attractive graphical user interface for high-level exploratory analyses, it is especially suited for an audience of novices, without programming skills (e.g. from the Digital Humanities). More experienced users can benefit from our implementation of a series of standard pipelines for text processing, as well as a number of similarity metrics.",
    "author": [
      {
        "name": "Maciej Eder",
        "url": {}
      },
      {
        "name": "Jan Rybicki",
        "url": {}
      },
      {
        "name": "Mike Kestemont",
        "url": {}
      }
    ],
    "date": "2015-12-22",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nstylo\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-035/",
    "title": "Generalized Hermite Distribution Modelling with the R Package hermite",
    "description": "The Generalized Hermite distribution (and the Hermite distribution as a particular case) is often used for fitting count data in the presence of overdispersion or multimodality. Despite this, to our knowledge, no standard software packages have implemented specific functions to compute basic probabilities and make simple statistical inference based on these distributions. We present here a set of computational tools that allows the user to face these difficulties by modelling with the Generalized Hermite distribution using the R package hermite. The package can also be used to generate random deviates from a Generalized Hermite distribution and to use basic functions to compute probabilities (density, cumulative density and quantile functions are available), to estimate parameters using the maximum likelihood method and to perform the likelihood ratio test for Poisson assumption against a Generalized Hermite alternative. In order to improve the density and quantile functions performance when the parameters are large, Edgeworth and Cornish-Fisher expansions have been used. Hermite regression is also a useful tool for modeling inflated count data, so its inclusion to a commonly used software like R will make this tool available to a wide range of potential users. Some examples of usage in several fields of application are also given.",
    "author": [
      {
        "name": "David Moriña",
        "url": {}
      },
      {
        "name": "Manuel Higueras",
        "url": {}
      },
      {
        "name": "Pedro Puig",
        "url": {}
      },
      {
        "name": "María Oliveira",
        "url": {}
      }
    ],
    "date": "2015-12-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmaxLik, radir\nCRAN Task Views implied by cited packages\nOptimization\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-034/",
    "title": "Open-Channel Computation with R",
    "description": "The rivr package provides a computational toolset for simulating steady and unsteady one dimensional flows in open channels. It is designed primarily for use by instructors of undergraduate and graduate-level open-channel hydrodynamics courses in such diverse fields as river engineering, physical geography and geophysics. The governing equations used to describe open-channel flows are briefly presented, followed by example applications. These include the computation of gradually varied flows and two examples of unsteady flows in channels—namely, the tracking of the evolution of a flood wave in a channel and the prediction of extreme variation in the water-surface profile that results when a sluice gate is abruptly closed. Model results for the unsteady flow examples are validated against standard benchmarks. The article concludes with a discussion of potential modifications and extensions to the package.",
    "author": [
      {
        "name": "Michael C. Koohafkan",
        "url": {}
      },
      {
        "name": "Bassam A. Younis",
        "url": {}
      }
    ],
    "date": "2015-11-28",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrivr, knitr, shiny, Rcpp\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NumericalMathematics, ReproducibleResearch, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-017/",
    "title": "scmamp: Statistical Comparison of Multiple Algorithms in Multiple Problems",
    "description": "Comparing the results obtained by two or more algorithms in a set of problems is a central task in areas such as machine learning or optimization. Drawing conclusions from these comparisons may require the use of statistical tools such as hypothesis testing. There are some interesting papers that cover this topic. In this manuscript we present scmamp, an R package aimed at being a tool that simplifies the whole process of analyzing the results obtained when comparing algorithms, from loading the data to the production of plots and tables. Comparing the performance of different algorithms is an essential step in many research and practical computational works. When new algorithms are proposed, they have to be compared with the state of the art. Similarly, when an algorithm is used for a particular problem, its performance with different sets of parameters has to be compared, in order to tune them for the best results. When the differences are very clear (e.g., when an algorithm is the best in all the problems used in the comparison), the direct comparison of the results may be enough. However, this is an unusual situation and, thus, in most situations a direct comparison may be misleading and not enough to draw sound conclusions; in those cases, the statistical assessment of the results is advisable. The statistical comparison of algorithms in the context of machine learning has been covered in several papers. In particular, the tools implemented in this package are those presented in Demšar (2006); García and Herrera (2008); García et al. (2010). Another good review that covers, among other aspects, the statistical assessment of the results in the context of supervised classification can be found in Santafé et al. (2015).",
    "author": [
      {
        "name": "Borja Calvo",
        "url": {}
      },
      {
        "name": "Guzmán Santafé",
        "url": {}
      }
    ],
    "date": "2015-11-26",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nscmamp\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-022/",
    "title": "Numerical Evaluation of the Gauss Hypergeometric Function with the hypergeo Package",
    "description": "This paper introduces the hypergeo package of R routines for numerical calculation of hypergeometric functions. The package is focussed on efficient and accurate evaluation of the Gauss hypergeometric function over the whole of the complex plane within the constraints of fixed-precision arithmetic. The hypergeometric series is convergent only within the unit circle, so analytic continuation must be used to define the function outside the unit circle. This short document outlines the numerical and conceptual methods used in the package; and justifies the package philosophy, which is to maintain transparent and verifiable links between the software and Abramowitz and Stegun (1965). Most of the package functionality is accessed via the single function hypergeo(), which dispatches to one of several methods depending on the value of its arguments. The package is demonstrated in the context of game theory.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2015-11-18",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngsl, appell, hypergeo\nCRAN Task Views implied by cited packages\nNumericalMathematics, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-036/",
    "title": "Code Profiling in R: A Review of Existing Methods and an Introduction to Package GUIProfiler",
    "description": "Code analysis tools are crucial to understand program behavior. Profile tools use the results of time measurements in the execution of a program to gain this understanding and thus help in the optimization of the code. In this paper, we review the different available packages to profile R code and show the advantages and disadvantages of each of them. In additon, we present GUIProfiler, a package that fulfills some unmet needs. Package GUIProfiler generates an HTML report with the timing for each code line and the relationships between different functions. This package mimics the behavior of the MATLAB profiler. The HTML report includes information on the time spent on each of the lines of the profiled code (the slowest code is highlighted). If the package is used within the RStudio environment, the user can navigate across the bottlenecks in the code and open the editor to modify the lines of code where more time is spent. It is also possible to edit the code using Notepad++ (a free editor for Windows) by simply clicking on the corresponding line. The graphical user interface makes it easy to identify the specific lines which slow down the code. The integration in RStudio and the generation of an HTML report makes GUIProfiler a very convenient tool to perform code optimization.",
    "author": [
      {
        "name": "Angel Rubio",
        "url": {}
      },
      {
        "name": "Fernando de Villar",
        "url": {}
      }
    ],
    "date": "2015-11-18",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\naprof, proftools, profr, microbenchmark, Nozzle.R1, knitr, GUIProfiler, stringr, plyr, devtools, shiny\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, ReproducibleResearch, WebTechnologies\nBioconductor packages used\nRgraphviz, graph\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-024/",
    "title": "An R Package for the Panel Approach Method for Program Evaluation: pampe",
    "description": "The pampe package for R implements the panel data approach method for program evalua tion designed to estimate the causal effects of political interventions or treatments. This procedure exploits the dependence among cross-sectional units to construct a counterfactual of the treated unit(s), and it is an appropriate method for research events that occur at an aggregate level like countries or regions and that affect only one or a small number of units. The implementation of the pampe package is illustrated using data from Hong Kong and 24 other units, by examining the economic impact of the political and economic integration of Hong Kong with mainland China in 1997 and 2004 respectively.",
    "author": [
      {
        "name": "Ainhoa Vega-Bayo",
        "url": {}
      }
    ],
    "date": "2015-11-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\npampe, leaps, xtable\nCRAN Task Views implied by cited packages\nChemPhys, Econometrics, ReproducibleResearch, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-018/",
    "title": "VSURF: An R Package for Variable Selection Using Random Forests",
    "description": "This paper describes the R package VSURF. Based on random forests, and for both regression and classification problems, it returns two subsets of variables. The first is a subset of important variables including some redundancy which can be relevant for interpretation, and the second one is a smaller subset corresponding to a model trying to avoid redundancy focusing more closely on the prediction objective. The two-stage strategy is based on a preliminary ranking of the explanatory variables using the random forests permutation-based score of importance and proceeds using a stepwise forward strategy for variable introduction. The two proposals can be obtained automatically using data-driven default values, good enough to provide interesting results, but strategy can also be tuned by the user. The algorithm is illustrated on a simulated example and its applications to real datasets are presented.",
    "author": [
      {
        "name": "Robin Genuer",
        "url": {}
      },
      {
        "name": "Jean-Michel Poggi",
        "url": {}
      },
      {
        "name": "Christine Tuleau-Malot",
        "url": {}
      }
    ],
    "date": "2015-11-08",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nVSURF, rpart, randomForest, party, ipred, Boruta, varSelRF, spikeSlabGAM, BioMark, mlbench, mixOmics\nCRAN Task Views implied by cited packages\nMachineLearning, Environmetrics, Survival, ChemPhys, Multivariate, Bayesian, HighPerformanceComputing\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-008/",
    "title": "quickpsy: An R Package to Fit Psychometric Functions for Multiple Groups",
    "description": "quickpsy is a package to parametrically fit psychometric functions. In comparison with previous R packages, quickpsy was built to easily fit and plot data for multiple groups. Here, we describe the standard parametric model used to fit psychometric functions and the standard estimation of its parameters using maximum likelihood. We also provide examples of usage of quickpsy, including how allowing the lapse rate to vary can sometimes eliminate the bias in parameter estimation, but not in general. Finally, we describe some implementation details, such as how to avoid the problems associated to round-off errors in the maximisation of the likelihood or the use of closures and non-standard evaluation functions.",
    "author": [
      {
        "name": "Daniel Linares",
        "url": {}
      },
      {
        "name": "Joan López-Moliner",
        "url": {}
      }
    ],
    "date": "2015-11-08",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nquickpsy, psyphy, modelfree, MPDiR, gridExtra, dplyr, ggplot2\nCRAN Task Views implied by cited packages\nPsychometrics, Graphics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-010/",
    "title": "An Interactive Survey Application for Validating Social Network Analysis Techniques",
    "description": "Social network analysis is extremely well supported by the R community and is routinely used for studying the relationships between people engaged in collaborative activities. While there has been rapid development of new approaches and metrics in this field, the challenging question of validity (how well insights derived from social networks agree with reality) is often difficult to address. We propose the use of several R packages to generate interactive surveys that are specifically well suited for validating social network analyses. Using our web-based survey application, we were able to validate the results of applying community-detection algorithms to infer the organizational structure of software developers contributing to open-source projects.",
    "author": [
      {
        "name": "Mitchell Joblin",
        "url": {}
      },
      {
        "name": "Wolfgang Mauerer",
        "url": {}
      }
    ],
    "date": "2015-11-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nigraph, sna, twitteR, Rfacebook, shiny\nCRAN Task Views implied by cited packages\nWebTechnologies, Optimization, Bayesian, gR, Graphics, SocialSciences, Spatial\nBioconductor packages used\ngraph\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-021/",
    "title": "QuantifQuantile: An R Package for Performing Quantile Regression Through Optimal Quantization",
    "description": "In quantile regression, various quantiles of a response variable Y are modelled as func tions of covariates (rather than its mean). An important application is the construction of reference curves/surfaces and conditional prediction intervals for Y. Recently, a nonparametric quantile regres sion method based on the concept of optimal quantization was proposed. This method competes very well with k-nearest neighbor, kernel, and spline methods. In this paper, we describe an R package, called QuantifQuantile, that allows to perform quantization-based quantile regression. We describe the various functions of the package and provide examples.",
    "author": [
      {
        "name": "Isabelle Charlier",
        "url": {}
      },
      {
        "name": "Davy Paindaveine",
        "url": {}
      },
      {
        "name": "Jérôme Saracco",
        "url": {}
      }
    ],
    "date": "2015-10-30",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nquantreg, quantregGrowth, QuantifQuantile, rgl, quantregGrowth\nCRAN Task Views implied by cited packages\nEnvironmetrics, Econometrics, Graphics, Multivariate, Optimization, ReproducibleResearch, Robust, SocialSciences, SpatioTemporal, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-026/",
    "title": "ClustVarLV: An R Package for the Clustering of Variables Around Latent Variables",
    "description": "The clustering of variables is a strategy for deciphering the underlying structure of a data set. Adopting an exploratory data analysis point of view, the Clustering of Variables around Latent Variables (CLV) approach has been proposed by Vigneau and Qannari (2003). Based on a family of optimization criteria, the CLV approach is adaptable to many situations. In particular, constraints may be introduced in order to take account of additional information about the observations and/or the variables. In this paper, the CLV method is depicted and the R package ClustVarLV including a set of functions developed so far within this framework is introduced. Considering successively different types of situations, the underlying CLV criteria are detailed and the various functions of the package are illustrated using real case studies.",
    "author": [
      {
        "name": "Evelyne Vigneau",
        "url": {}
      },
      {
        "name": "Mingkun Chen",
        "url": {}
      },
      {
        "name": "El Mostafa Qannari",
        "url": {}
      }
    ],
    "date": "2015-10-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncluster, ClustVarLV, ClustOfVar, clere, biclust, pvclust, Hmisc, FactoMineR, plsgenomics, Rcpp, ClustVarLV\nCRAN Task Views implied by cited packages\nMultivariate, Cluster, Psychometrics, Environmetrics, HighPerformanceComputing, Bayesian, ClinicalTrials, Econometrics, Graphics, NumericalMathematics, OfficialStatistics, ReproducibleResearch, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2016-012/",
    "title": "Heteroscedastic Censored and Truncated Regression with crch",
    "description": "The crch package provides functions for maximum likelihood estimation of censored or truncated regression models with conditional heteroscedasticity along with suitable standard methods to summarize the fitted models and compute predictions, residuals, etc. The supported distributions include leftor right-censored or truncated Gaussian, logistic, or student-t distributions with potentially different sets of regressors for modeling the conditional location and scale. The models and their R implementation are introduced and illustrated by numerical weather prediction tasks using precipitation data for Innsbruck (Austria).",
    "author": [
      {
        "name": "Jakob W. Messner",
        "url": {}
      },
      {
        "name": "Georg J. Mayr",
        "url": {}
      },
      {
        "name": "Achim Zeileis",
        "url": {}
      }
    ],
    "date": "2015-10-14",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndglm, glmx, gamlss, betareg, crch, Formula, gamlss.cens, gamlss.tr, sampleSelection, mhurdle\nCRAN Task Views implied by cited packages\nEconometrics, Psychometrics, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-031/",
    "title": "mtk: A General-Purpose and Extensible R Environment for Uncertainty and Sensitivity Analyses of Numerical Experiments",
    "description": "Along with increased complexity of the models used for scientific activities and engineering come diverse and greater uncertainties. Today, effectively quantifying the uncertainties contained in a model appears to be more important than ever. Scientific fellows know how serious it is to calibrate their model in a robust way, and decision-makers describe how critical it is to keep the best effort to reduce the uncertainties about the model. Effectively accessing the uncertainties about the model requires mastering all the tasks involved in the numerical experiments, from optimizing the experimental design to managing the very time consuming aspect of model simulation and choosing the adequate indicators and analysis methods. In this paper, we present an open framework for organizing the complexity associated with numerical model simulation and analyses. Named mtk (Mexico Toolkit), the developed system aims at providing practitioners from different disciplines with a systematic and easy way to compare and to find the best method to effectively uncover and quantify the uncertainties contained in the model and further to evaluate their impact on the performance of the model. Such requirements imply that the system must be generic, universal, homogeneous, and extensible. This paper discusses such an implementation using the R scientific computing platform and demonstrates its functionalities with examples from agricultural modeling. The package mtk is of general purpose and easy to extend. Numerous methods are already available in the actual release version, including Fast, Sobol, Morris, Basic Monte-Carlo, Regression, LHS (Latin Hypercube Sampling), PLMM (Polynomial Linear metamodel). Most of them are compiled from available R packages with extension tools delivered by package mtk.",
    "author": [
      {
        "name": "Juhui Wang",
        "url": {}
      },
      {
        "name": "Robert Faivre",
        "url": {}
      },
      {
        "name": "Hervé Richard",
        "url": {}
      },
      {
        "name": "Hervé Monod",
        "url": {}
      }
    ],
    "date": "2015-10-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsensitivity, spartan, diceDesign, planor, mtk, ff\nCRAN Task Views implied by cited packages\nEnvironmetrics, ExperimentalDesign, HighPerformanceComputing\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-029/",
    "title": "ALTopt: An R Package for Optimal Experimental Design of Accelerated Life Testing",
    "description": "The R package ALTopt has been developed with the aim of creating and evaluating optimal experimental designs of censored accelerated life tests (ALTs). This package takes the generalized linear model approach to ALT planning, because this approach can easily handle censoring plans and derive information matrices for evaluating designs. Three types of optimality criteria are considered: D-optimality for model parameter estimation, U-optimality for reliability prediction at a single use condition, and I-optimality for reliability prediction over a region of use conditions. The Weibull distribution is assumed for failure time data and more than one stress factor can be specified in the package. Several graphical evaluation tools are also provided for the comparison of different ALT test plans.",
    "author": [
      {
        "name": "Kangwon Seo",
        "url": {}
      },
      {
        "name": "Rong Pan",
        "url": {}
      }
    ],
    "date": "2015-09-29",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nALTopt\nCRAN Task Views implied by cited packages\nExperimentalDesign\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-033/",
    "title": "mmpp: A Package for Calculating Similarity and Distance Metrics for Simple and Marked Temporal Point Processes",
    "description": "A simple temporal point process (SPP) is an important class of time series, where the sample realization of the process is solely composed of the times at which events occur. Particular examples of point process data are neuronal spike patterns or spike trains, and a large number of distance and similarity metrics for those data have been proposed. A marked point process (MPP) is an extension of a simple temporal point process, in which a certain vector valued mark is associated with each of the temporal points in the SPP. Analyses of MPPs are of practical importance because instances of MPPs include recordings of natural disasters such as earthquakes and tornadoes. In this paper, we introduce the R package mmpp, which implements a number of distance and similarity metrics for SPPs, and also extends those metrics for dealing with MPPs.",
    "author": [
      {
        "name": "Hideitsu Hino",
        "url": {}
      },
      {
        "name": "Ken Takano",
        "url": {}
      },
      {
        "name": "Noboru Murata",
        "url": {}
      }
    ],
    "date": "2015-09-29",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsplancs, spatstat, PtProcess, stpp, mmpp, SAPP, etasFLP\nCRAN Task Views implied by cited packages\nSpatioTemporal, Spatial, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-027/",
    "title": "Working with Multilabel Datasets in R: The mldr Package",
    "description": "Most classification algorithms deal with datasets which have a set of input features, the variables to be used as predictors, and only one output class, the variable to be predicted. However, in late years many scenarios in which the classifier has to work with several outputs have come to life. Automatic labeling of text documents, image annotation or protein classification are among them. Multilabel datasets are the product of these new needs, and they have many specific traits. The mldr package allows the user to load datasets of this kind, obtain their characteristics, produce specialized plots, and manipulate them. The goal is to provide the exploratory tools needed to analyze multilabel datasets, as well as the transformation and manipulation functions that will make possible to apply binary and multiclass classification models to this data or the development of new multilabel classifiers. Thanks to its integrated user interface, the exploratory functions will be available even to non-specialized R users.",
    "author": [
      {
        "name": "Francisco Charte",
        "url": {}
      },
      {
        "name": "David Charte",
        "url": {}
      }
    ],
    "date": "2015-09-16",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRWeka, mldr, shiny, Rcmdr, rattle, XML, circlize, devtools, pROC, shiny\nCRAN Task Views implied by cited packages\nWebTechnologies, MachineLearning, Finance, NaturalLanguageProcessing\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-032/",
    "title": "treeClust: An R Package for Tree-Based Clustering Dissimilarities",
    "description": "This paper describes treeClust, an R package that produces dissimilarities useful for cluster ing. These dissimilarities arise from a set of classification or regression trees, one with each variable in the data acting in turn as a the response, and all others as predictors. This use of trees produces dissim ilarities that are insensitive to scaling, benefit from automatic variable selection, and appear to perform well. The software allows a number of options to be set, affecting the set of objects returned in the call; the user can also specify a clustering algorithm and, optionally, return only the clustering vector. The package can also generate a numeric data set whose inter-point distances relate to the treeClust ones; such a numeric data set can be much smaller than the vector of inter-point dissimilarities, a useful feature in big data sets.",
    "author": [
      {
        "name": "Samuel E. Buttrey",
        "url": {}
      },
      {
        "name": "Lyn R. Whitaker",
        "url": {}
      }
    ],
    "date": "2015-09-16",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntreeClust, cluster, rpart, tree\nCRAN Task Views implied by cited packages\nCluster, Environmetrics, MachineLearning, Multivariate, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-017/",
    "title": "Fitting Conditional and Simultaneous Autoregressive Spatial Models in hglm",
    "description": "We present a new version (> 2.0) of the hglm package for fitting hierarchical generalized linear models (HGLMs) with spatially correlated random effects. CAR() and SAR() families for con ditional and simultaneous autoregressive random effects were implemented. Eigen decomposition of the matrix describing the spatial structure (e.g., the neighborhood matrix) was used to transform the CAR/SAR random effects into an independent, but heteroscedastic, Gaussian random effect. A linear predictor is fitted for the random effect variance to estimate the parameters in the CAR and SAR models. This gives a computationally efficient algorithm for moderately sized problems.",
    "author": [
      {
        "name": "Moudud Alam",
        "url": {}
      },
      {
        "name": "Lars Rönnegård",
        "url": {}
      },
      {
        "name": "Xia Shen",
        "url": {}
      }
    ],
    "date": "2015-09-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nhglm, spaMM, HGLMMM\nCRAN Task Views implied by cited packages\nSpatial\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-020/",
    "title": "apc: An R Package for Age-Period-Cohort Analysis",
    "description": "The apc package includes functions for age-period-cohort analysis based on the canonical parametrisation of Kuang et al. (2008a). The package includes functions for organizing the data, descriptive plots, a deviance table, estimation of (sub-models of) the age-period-cohort model, a plot for specification testing, plots of estimated parameters, and sub-sample analysis.",
    "author": [
      {
        "name": "Bent Nielsen",
        "url": {}
      }
    ],
    "date": "2015-08-05",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\napc, Epi\nCRAN Task Views implied by cited packages\nSurvival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-025/",
    "title": "BSGS: Bayesian Sparse Group Selection",
    "description": "An R package BSGS is provided for the integration of Bayesian variable and sparse group selection separately proposed by Chen et al. (2011) and Chen et al. (in press) for variable selection problems, even in the cases of large p and small n. This package is designed for variable selection problems including the identification of the important groups of variables and the active variables within the important groups. This article introduces the functions in the BSGS package that can be used to perform sparse group selection as well as variable selection through simulation studies and real data.",
    "author": [
      {
        "name": "Kuo-Jung Lee",
        "url": {}
      },
      {
        "name": "Ray-Bing Chen",
        "url": {}
      }
    ],
    "date": "2015-08-05",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nBSGS\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-023/",
    "title": "SRCS: Statistical Ranking Color Scheme for Visualizing Parameterized Multiple Pairwise Comparisons with R",
    "description": "The problem of comparing a new solution method against existing ones to find statistically significant differences arises very often in sciences and engineering. When the problem instance being solved is defined by several parameters, assessing a number of methods with respect to many problem configurations simultaneously becomes a hard task. Some visualization technique is required for presenting a large number of statistical significance results in an easily interpretable way. Here we review an existing color-based approach called Statistical Ranking Color Scheme (SRCS) for displaying the results of multiple pairwise statistical comparisons between several methods assessed separately on a number of problem configurations. We introduce an R package implementing SRCS, which performs all the pairwise statistical tests from user data and generates customizable plots. We demonstrate its applicability on two examples from the areas of dynamic optimization and machine learning, in which several algorithms are compared on many problem instances, each defined by a combination of parameters.",
    "author": [
      {
        "name": "Pablo J. Villacorta",
        "url": {}
      },
      {
        "name": "José A. Sáez",
        "url": {}
      }
    ],
    "date": "2015-07-29",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nfactorplot, SRCS, e1071, RWeka\nCRAN Task Views implied by cited packages\nMachineLearning, Cluster, Distributions, Environmetrics, Multivariate, NaturalLanguageProcessing, Psychometrics\nBioconductor packages used\npaircompviz\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-030/",
    "title": "abctools: An R Package for Tuning Approximate Bayesian Computation Analyses",
    "description": "Approximate Bayesian computation (ABC) is a popular family of algorithms which perform approximate parameter inference when numerical evaluation of the likelihood function is not possible but data can be simulated from the model. They return a sample of parameter values which produce simulations close to the observed dataset. A standard approach is to reduce the simulated and observed datasets to vectors of summary statistics and accept when the difference between these is below a specified threshold. ABC can also be adapted to perform model choice. In this article, we present a new software package for R, abctools which provides methods for tuning ABC algorithms. This includes recent dimension reduction algorithms to tune the choice of summary statistics, and coverage methods to tune the choice of threshold. We provide several illustrations of these routines on applications taken from the ABC literature.",
    "author": [
      {
        "name": "Matthew A. Nunes",
        "url": {}
      },
      {
        "name": "Dennis Prangle",
        "url": {}
      }
    ],
    "date": "2015-07-29",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nabctools, abc, easyABC, MASS\nCRAN Task Views implied by cited packages\nBayesian, Distributions, Econometrics, Environmetrics, Multivariate, NumericalMathematics, Pharmacokinetics, Psychometrics, Robust, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-019/",
    "title": "zoib: An R Package for Bayesian Inference for Beta Regression and Zero/One Inflated Beta Regression",
    "description": "The beta distribution is a versatile function that accommodates a broad range of probability distribution shapes. Beta regression based on the beta distribution can be used to model a response variable y that takes values in open unit interval (0, 1). Zero/one inflated beta (ZOIB) regression models can be applied when y takes values from closed unit interval [0, 1]. The ZOIB model is based a piecewise distribution that accounts for the probability mass at 0 and 1, in addition to the probability density within (0, 1). This paper introduces an R package – zoib that provides Bayesian inferences for a class of ZOIB models. The statistical methodology underlying the zoib package is discussed, the functions covered by the package are outlined, and the usage of the package is illustrated with three examples of different data and model types. The package is comprehensive and versatile in that it can model data with or without inflation at 0 or 1, accommodate clustered and correlated data via latent variables, perform penalized regression as needed, and allow for model comparison via the computation of the DIC criterion.",
    "author": [
      {
        "name": "Fang Liu",
        "url": {}
      },
      {
        "name": "Yunchuan Kong",
        "url": {}
      }
    ],
    "date": "2015-07-18",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nbetareg, Bayesianbetareg, zoib, coda, rjags\nCRAN Task Views implied by cited packages\nBayesian, gR, Cluster, Econometrics, Psychometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-028/",
    "title": "PracTools: Computations for Design of Finite Population Samples",
    "description": "PracTools is an R package with functions that compute sample sizes for various types of finite population sampling designs when totals or means are estimated. One-, two-, and three-stage designs are covered as well as allocations for stratified sampling and probability proportional to size sampling. Sample allocations can be computed that minimize the variance of an estimator subject to a budget constraint or that minimize cost subject to a precision constraint. The package also contains some specialized functions for estimating variance components and design effects. Several finite populations are included that are useful for classroom instruction.",
    "author": [
      {
        "name": "Richard Valliant",
        "url": {}
      },
      {
        "name": "Jill A. Dever",
        "url": {}
      },
      {
        "name": "Frauke Kreuter",
        "url": {}
      }
    ],
    "date": "2015-06-30",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\npps, sampling, samplingbook, simFrame, survey, PracTools, stratification, alabama, Rsolnp, SamplingStrata\nCRAN Task Views implied by cited packages\nOfficialStatistics, Optimization, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-011/",
    "title": "R as an Environment for Reproducible Analysis of DNA Amplification Experiments",
    "description": "There is an ever-increasing number of applications, which use quantitative PCR (qPCR) or digital PCR (dPCR) to elicit fundamentals of biological processes. Moreover, quantitative isother mal amplification (qIA) methods have become more prominent in life sciences and point-of-care diagnostics. Additionally, the analysis of melting data is essential during many experiments. Several software packages have been developed for the analysis of such datasets. In most cases, the software is either distributed as closed source software or as monolithic block with little freedom to perform highly customized analysis procedures. We argue, among others, that R is an excellent foundation for reproducible and transparent data analysis in a highly customizable cross-platform environment. However, for novices it is often challenging to master R or learn capabilities of the vast number of packages available. In the paper, we describe exemplary workflows for the analysis of qPCR, qIA or dPCR experiments including the analysis of melting curve data. Our analysis relies entirely on R packages available from public repositories. Additionally, we provide information related to standardized and reproducible research.",
    "author": [
      {
        "name": "Stefan Rödiger",
        "url": {}
      },
      {
        "name": "Michał Burdukiewicz",
        "url": {}
      },
      {
        "name": "Konstantin Blagodatskikh",
        "url": {}
      },
      {
        "name": "Michael Jahn",
        "url": {}
      },
      {
        "name": "Peter Schierack",
        "url": {}
      }
    ],
    "date": "2015-06-25",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndpcR, kulife, MCMC.qpcr, qPCR.CT, DivMelt, qpcR, chipPCR, MBmca, RDML, RNetCDF, archivist, settings, shiny, rateratio.test\nCRAN Task Views implied by cited packages\nReproducibleResearch, Spatial, SpatioTemporal, WebTechnologies\nBioconductor packages used\nnondetects, qpcrNorm, HTqPCR, SLqPCR, ddCt, EasyqpcR, unifiedWMWqPCR, ReadqPCR, NormqPCR\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-009/",
    "title": "Implementing Persistent O(1) Stacks and Queues in R",
    "description": "True to their functional roots, most R functions are side-effect-free, and users expect datatypes to be persistent. However, these semantics complicate the creation of efficient and dynamic data structures. Here, we describe the implementation of stack and queue data structures satisfying these conditions in R, available in the CRAN package rstackdeque. Guided by important work in purely functional languages, we look at both partiallyand fully-persistent versions of queues, comparing their performance characteristics. Finally, we illustrate the usefulness of such dynamic structures with examples of generating and solving mazes.",
    "author": [
      {
        "name": "Shawn T. O’Neil",
        "url": {}
      }
    ],
    "date": "2015-06-24",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrstackdeque, dplyr, microbenchmark, ggplot2, hash, Rcpp\nCRAN Task Views implied by cited packages\nGraphics, HighPerformanceComputing, NumericalMathematics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-007/",
    "title": "sae: An R Package for Small Area Estimation",
    "description": "We describe the R package sae for small area estimation. This package can be used to obtain model-based estimates for small areas based on a variety of models at the area and unit levels, along with basic direct and indirect estimates. Mean squared errors are estimated by analytical approximations in simple models and applying bootstrap procedures in more complex models. We describe the package functions and show how to use them through examples.",
    "author": [
      {
        "name": "Isabel Molina",
        "url": {}
      },
      {
        "name": "Yolanda Marhuenda",
        "url": {}
      }
    ],
    "date": "2015-06-02",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsae, nlme, MASS, survey, sampling, rsae, JoSae, hbsae, mme, saery, sae2\nCRAN Task Views implied by cited packages\nOfficialStatistics, SocialSciences, Econometrics, Environmetrics, Pharmacokinetics, Psychometrics, Bayesian, ChemPhys, Distributions, Finance, Multivariate, NumericalMathematics, Robust, Spatial, SpatioTemporal, Survival, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-010/",
    "title": "Correspondence Analysis on Generalised Aggregated Lexical Tables (CA-GALT) in the FactoMineR Package",
    "description": "Correspondence analysis on generalised aggregated lexical tables (CA-GALT) is a method that generalizes classical CA-ALT to the case of several quantitative, categorical and mixed variables. It aims to establish a typology of the external variables and a typology of the events from their mutual relationships. In order to do so, the influence of external variables on the lexical choices is untangled cancelling the associations among them, and to avoid the instability issued from multicollinearity, they are substituted by their principal components. The CaGalt function, implemented in the FactoMineR package, provides numerous numerical and graphical outputs. Confidence ellipses are also provided to validate and improve the representation of words and variables. Although this methodology was developed mainly to give an answer to the problem of analyzing open-ended questions, it can be applied to any kind of frequency/contingency table with external variables.",
    "author": [
      {
        "name": "Belchin Kostov",
        "url": {}
      },
      {
        "name": "Mónica Bécue-Bertaut",
        "url": {}
      },
      {
        "name": "François Husson",
        "url": {}
      }
    ],
    "date": "2015-06-02",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nFactoMineR\nCRAN Task Views implied by cited packages\nMultivariate, Psychometrics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-013/",
    "title": "fslr: Connecting the FSL Software with R",
    "description": "We present the package fslr, a set of R functions that interface with FSL (FMRIB Software Library), a commonly-used open-source software package for processing and analyzing neuroimaging data. The fslr package performs operations on ‘nifti’ image objects in R using command-line functions from FSL, and returns R objects back to the user. fslr allows users to develop image processing and analysis pipelines based on FSL functionality while interfacing with the functionality provided by R. We present an example of the analysis of structural magnetic resonance images, which demonstrates how R users can leverage the functionality of FSL without switching to shell commands. Glossary of acronyms MRI Magnetic Resonance Imaging/Image FSL FMRIB Software Library PD Proton Density FAST FMRIB’s Automated Segmentation Tool FLAIR Fluid-Attenuated Inversion Recovery FLIRT FMRIB’s Linear Image Registration Tool MS Multiple Sclerosis BET Brain Extraction Tool FMRIB Functional MRI of the Brain Group FNIRT FMRIB’s Nonlinear Image Registration Tool MNI Montreal Neurological Institute",
    "author": [
      {
        "name": "John Muschelli",
        "url": {}
      },
      {
        "name": "Elizabeth Sweeney",
        "url": {}
      },
      {
        "name": "Martin Lindquist",
        "url": {}
      },
      {
        "name": "Ciprian Crainiceanu",
        "url": {}
      }
    ],
    "date": "2015-06-02",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nAnalyzeFMRI, RNiftyReg, fmri, fslr, oro.nifti, ggplot2, ggplot2, mgcv\nCRAN Task Views implied by cited packages\nMedicalImaging, ChemPhys, Graphics, Phylogenetics, Bayesian, Econometrics, Environmetrics, SocialSciences\nBioconductor packages used\nEBImage\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-003/",
    "title": "sparkTable: Generating Graphical Tables for Websites and Documents with R",
    "description": "Visual analysis of data is important to understand the main characteristics, main trends and relationships in data sets and it can be used to assess the data quality. Using the R package sparkTable, statistical tables holding quantitative information can be enhanced by including spark-type graphs such as sparklines and sparkbars . These kind of graphics are well-known in literature and are considered as simple, intense and illustrative graphs that are small enough to fit in a single line. Thus, they can easily enrich tables and texts with additional information in a comprehensive visual way. The R package sparkTable uses a clean S4 class design and provides methods to create different types of sparkgraphs that can be used in websites, presentations and documents. We also implemented an easy way for non-experts to create highly complex tables. In this case, graphical parameters can be interactively changed, variables can be sorted, graphs can be added and removed in an interactive manner. Thereby it is possible to produce custom-tailored graphical tables – standard tables that are enriched with graphs – that can be displayed in a browser and exported to various formats.",
    "author": [
      {
        "name": "Alexander Kowarik",
        "url": {}
      },
      {
        "name": "Bernhard Meindl",
        "url": {}
      },
      {
        "name": "Matthias Templ",
        "url": {}
      }
    ],
    "date": "2015-05-29",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsparkTable, knitr, brew, xtable, shiny\nCRAN Task Views implied by cited packages\nReproducibleResearch, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-008/",
    "title": "showtext: Using System Fonts in R Graphics",
    "description": "This article introduces the showtext package that makes it easy to use system fonts in R graphics. Unlike other methods to embed fonts into graphics, showtext converts text into raster images or polygons, and then adds them to the plot canvas. This method produces platform-independent image files that do not rely on the fonts that create them. It supports a large number of font formats and R graphics devices, and meanwhile provides convenient features such as using web fonts and integrating with knitr. This article provides an elaborate introduction to the showtext package, including its design, usage, and examples.",
    "author": [
      {
        "name": "Yixuan Qiu",
        "url": {}
      }
    ],
    "date": "2015-05-29",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nextrafont, showtext, knitr, Cairo, Rttf2pt1, sysfonts, RCurl, jsonlite, ggplot2, xkcd, RSvgDevice\nCRAN Task Views implied by cited packages\nGraphics, WebTechnologies, Phylogenetics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-016/",
    "title": "Estimability Tools for Package Developers",
    "description": "When a linear model is rank-deficient, then predictions based on that model become questionable because not all predictions are uniquely estimable. However, some of them are, and the estimability package provides tools that package developers can use to tell which is which. With the use of these tools, a model object’s predict method could return estimable predictions as-is while flagging non-estimable ones in some way, so that the user can know which predictions to believe. The estimability package also provides, as a demonstration, an estimability-enhanced epredict method to use in place of predict for models fitted using the stats package.",
    "author": [
      {
        "name": "Russell V. Lenth",
        "url": {}
      }
    ],
    "date": "2015-05-11",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nestimability\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-015/",
    "title": "Manipulation of Discrete Random Variables with discreteRV",
    "description": "A prominent issue in statistics education is the sometimes large disparity between the theoretical and the computational coursework. discreteRV is an R package for manipulation of discrete random variables which uses clean and familiar syntax similar to the mathematical notation in introductory probability courses. The package offers functions that are simple enough for users with little experience with statistical programming, but has more advanced features which are suitable for a large number of more complex applications. In this paper, we introduce and motivate discreteRV, describe its functionality, and provide reproducible examples illustrating its use.",
    "author": [
      {
        "name": "Eric Hare",
        "url": {}
      },
      {
        "name": "Andreas Buja",
        "url": {}
      },
      {
        "name": "Heike Hofmann",
        "url": {}
      }
    ],
    "date": "2015-05-06",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndiscreteRV, devtools\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-004/",
    "title": "rdrobust: An R Package for Robust Nonparametric Inference in Regression-Discontinuity Designs",
    "description": "This article describes the R package rdrobust, which provides data-driven graphical and in ference procedures for RD designs. The package includes three main functions: rdrobust, rdbwselect and rdplot. The first function (rdrobust) implements conventional local-polynomial RD treatment effect point estimators and confidence intervals, as well as robust bias-corrected confidence intervals, for average treatment effects at the cutoff. This function covers sharp RD, sharp kink RD, fuzzy RD and fuzzy kink RD designs, among other possibilities. The second function (rdbwselect) implements several bandwidth selectors proposed in the RD literature. The third function (rdplot) provides data-driven optimal choices of evenly-spaced and quantile-spaced partition sizes, which are used to implement several data-driven RD plots.",
    "author": [
      {
        "name": "Sebastian Calonico",
        "url": {}
      },
      {
        "name": "Matias D. Cattaneo",
        "url": {}
      },
      {
        "name": "Rocío Titiunik",
        "url": {}
      }
    ],
    "date": "2015-04-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrdrobust\nCRAN Task Views implied by cited packages\nEconometrics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-005/",
    "title": "Frames2: A Package for Estimation in Dual Frame Surveys",
    "description": "Data from complex survey designs require special consideration with regard to estimation of finite population parameters and corresponding variance estimation procedures, as a consequence of significant departures from the simple random sampling assumption. In the past decade a number of statistical software packages have been developed to facilitate the analysis of complex survey data. All these statistical software packages are able to treat samples selected from one sampling frame containing all population units. Dual frame surveys are very useful when it is not possible to guarantee a complete coverage of the target population and may result in considerable cost savings over a single frame design with comparable precision. There are several estimators available in the statistical literature but no existing software covers dual frame estimation procedures. This gap is now filled by package Frames2. In this paper we highlight the main features of the package. The package includes the main estimators in dual frame surveys and also provides interval confidence estimation.",
    "author": [
      {
        "name": "Antonio Arcos",
        "url": {}
      },
      {
        "name": "David Molina",
        "url": {}
      },
      {
        "name": "Maria Giovanna Ranalli",
        "url": {}
      },
      {
        "name": "María del Mar Rueda",
        "url": {}
      }
    ],
    "date": "2015-04-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsurvey, sampling, laeken, TeachingSampling, Frames2\nCRAN Task Views implied by cited packages\nOfficialStatistics, ReproducibleResearch, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-006/",
    "title": "The Complex Multivariate Gaussian Distribution",
    "description": "Here I introduce package cmvnorm, a complex generalization of the mvtnorm package. A complex generalization of the Gaussian process is suggested and numerical results presented using the package. An application in the context of approximating the Weierstrass σ-function using a complex Gaussian process is given.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2015-04-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncmvnorm, mvtnorm, emulator\nCRAN Task Views implied by cited packages\nDistributions, Finance, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-012/",
    "title": "The gridGraphics Package",
    "description": "The gridGraphics package provides a function, grid.echo(), that can be used to convert a plot drawn with the graphics package to a visually identical plot drawn using grid. This conversion provides access to a variety of grid tools for making customisations and additions to the plot that are not possible with the graphics package.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2015-04-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngridGraphics, lattice, ggplot2, plotrix, maps, gridBase, gridSVG\nCRAN Task Views implied by cited packages\nGraphics, Multivariate, Pharmacokinetics, Phylogenetics, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-002/",
    "title": "fanplot: An R Package for Visualising Sequential Distributions",
    "description": "Fan charts, first developed by the Bank of England in 1996, have become a standard method for visualising forecasts with uncertainty. Using shading fan charts focus the attention towards the whole distribution away from a single central measure. This article describes the basics of plotting fan charts using an R add-on package alongside some additional methods for displaying sequential distributions. Examples are based on distributions of both estimated parameters from a time series model and future values with uncertainty.",
    "author": [
      {
        "name": "Guy J. Abel",
        "url": {}
      }
    ],
    "date": "2015-04-07",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nvars, forecast, fanplot, R2OpenBUGS, zoo, tsbugs, RColorBrewer, shiny\nCRAN Task Views implied by cited packages\nTimeSeries, Econometrics, Finance, Environmetrics, gR, Graphics, Spatial, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-014/",
    "title": "Identifying Complex Causal Dependencies in Configurational Data with Coincidence Analysis",
    "description": "We present cna, a package for performing Coincidence Analysis (CNA). CNA is a config urational comparative method for the identification of complex causal dependencies—in particular, causal chains and common cause structures—in configurational data. After a brief introduction to the method’s theoretical background and main algorithmic ideas, we demonstrate the use of the package by means of an artificial and a real-life data set. Moreover, we outline planned enhancements of the package that will further increase its applicability.",
    "author": [
      {
        "name": "Michael Baumgartner",
        "url": {}
      },
      {
        "name": "Alrik Thiem",
        "url": {}
      }
    ],
    "date": "2015-03-30",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nQCA, SetMethods, cna\nCRAN Task Views implied by cited packages\nCausalInference\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:30+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2015-001/",
    "title": "Peptides: A Package for Data Mining of Antimicrobial Peptides",
    "description": "Antimicrobial peptides (AMP) are a promising source of antibiotics with a broad spectrum activity against bacteria and low incidence of developing resistance. The mechanism by which an AMP executes its function depends on a set of computable physicochemical properties from the amino acid sequence. The Peptides package was designed to allow the quick and easy computation of ten structural characteristics own of the antimicrobial peptides, with the aim of generating data to increase the accuracy in classification and design of new amino acid sequences. Moreover, the options to read and plot XVG output files from GROMACS molecular dynamics package are included.",
    "author": [
      {
        "name": "Daniel Osorio",
        "url": {}
      },
      {
        "name": "Paola Rondón-Villarreal",
        "url": {}
      },
      {
        "name": "Rodrigo Torres",
        "url": {}
      }
    ],
    "date": "2015-02-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nPeptides, caret\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, MachineLearning, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-028/",
    "title": "bshazard: A Flexible Tool for Nonparametric Smoothing of the Hazard Function",
    "description": "The hazard function is a key component in the inferential process in survival analysis and relevant for describing the pattern of failures. However, it is rarely shown in research papers due to the difficulties in nonparametric estimation. We developed the bshazard package to facilitate the computation of a nonparametric estimate of the hazard function, with data-driven smoothing. The method accounts for left truncation, right censoring and possible covariates. B-splines are used to estimate the shape of the hazard within the generalized linear mixed models framework. Smoothness is controlled by imposing an autoregressive structure on the baseline hazard coefficients. This perspective allows an ’automatic’ smoothing by avoiding the need to choose the smoothing parameter, which is estimated from the data as a dispersion parameter. A simulation study demonstrates the capability of our software and an application to estimate the hazard of Non-Hodgkin’s lymphoma in Swedish population data shows its potential.",
    "author": [
      {
        "name": "Paola Rebora",
        "url": {}
      },
      {
        "name": "Agus Salim",
        "url": {}
      },
      {
        "name": "Marie Reilly",
        "url": {}
      }
    ],
    "date": "2015-01-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmuhaz, flexsurv, bshazard, Epi, survival, splines\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Distributions, Econometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-030/",
    "title": "Farewell's Linear Increments Model for Missing Data: The FLIM package",
    "description": "Missing data is common in longitudinal studies. We present a package for Farewell’s Linear Increments Model for Missing Data (the FLIM package), which can be used to fit linear models for observed increments of longitudinal processes and impute missing data. The method is valid for data with regular observation patterns. The end result is a list of fitted models and a hypothetical complete dataset corresponding to the data we might have observed had individuals not been missing. The FLIM package may also be applied to longitudinal studies for causal analysis, by considering counterfactual data as missing data for instance to compare the effect of different treatments when only data from observational studies are available. The aim of this article is to give an introduction to the FLIM package and to demonstrate how the package can be applied.",
    "author": [
      {
        "name": "Rune Hoff",
        "url": {}
      },
      {
        "name": "Jon Michael Gran",
        "url": {}
      },
      {
        "name": "Daniel Farewell",
        "url": {}
      }
    ],
    "date": "2015-01-09",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nzoo\nCRAN Task Views implied by cited packages\nEconometrics, Environmetrics, Finance, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-024/",
    "title": "Flexible R Functions for Processing Accelerometer Data, with Emphasis on NHANES 2003-2006",
    "description": "Accelerometers are a valuable tool for measuring physical activity (PA) in epidemiological studies. However, considerable processing is needed to convert time-series accelerometer data into meaningful variables for statistical analysis. This article describes two recently developed R packages for processing accelerometer data. The package accelerometry contains functions for performing various data processing procedures, such as identifying periods of non-wear time and bouts of activity. The functions are flexible, computationally efficient, and compatible with uniaxial or triaxial data. The package nhanesaccel is specifically for processing data from the National Health and Nutrition Examination Survey (NHANES), years 2003–2006. Its primary function generates measures of PA volume, intensity, frequency, and patterns according to user-specified data processing methods. This function can process the NHANES 2003-2006 dataset in under one minute, which is a drastic improve ment over existing software. This article highlights important features of packages accelerometry and nhanesaccel and demonstrates typical usage for PA researchers.",
    "author": [
      {
        "name": "Dane R. Van Domelen",
        "url": {}
      },
      {
        "name": "W. Stephen Pittard",
        "url": {}
      }
    ],
    "date": "2015-01-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\naccelerometry, Rcpp, pawacc, PhysicalActivity, survey, GGIR\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NumericalMathematics, OfficialStatistics, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-026/",
    "title": "ngspatial: A Package for Fitting the Centered Autologistic and Sparse Spatial Generalized Linear Mixed Models for Areal Data",
    "description": "Two important recent advances in areal modeling are the centered autologistic model and the sparse spatial generalized linear mixed model (SGLMM), both of which are reparameterizations of traditional models. The reparameterizations improve regression inference by alleviating spatial confounding, and the sparse SGLMM also greatly speeds computing by reducing the dimension of the spatial random effects. Package ngspatial (’ng’ = non-Gaussian) provides routines for fitting these new models. The package supports composite likelihood and Bayesian inference for the centered autologistic model, and Bayesian inference for the sparse SGLMM.",
    "author": [
      {
        "name": "John Hughes",
        "url": {}
      }
    ],
    "date": "2015-01-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nngspatial, CARBayes, spdep, Rcpp, RcppArmadillo, batchmeans\nCRAN Task Views implied by cited packages\nSpatial, NumericalMathematics, Econometrics, HighPerformanceComputing\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-029/",
    "title": "SMR: An R package for computing the externally studentized normal midrange distribution",
    "description": "The main purpose of this paper is to present the main algorithms underlining the con struction and implementation of the SMR package, whose aim to compute studentized normal midrange distribution. Details on the externally studentized normal midrange and standardized normal midrange distributions are also given. The package follows the same structure as the prob ability functions implemented in R. That is: the probability density function (dSMR), the cumulative distribution function (pSMR), the quantile function (qSMR) and the random number generating function (rSMR). The pseudocodes and illustrative examples of how to use the package are presented.",
    "author": [
      {
        "name": "Ben Dêivide Oliveira Batista",
        "url": {}
      },
      {
        "name": "Daniel Furtado Ferreira",
        "url": {}
      }
    ],
    "date": "2015-01-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nSMR\nCRAN Task Views implied by cited packages\nDistributions\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-031/",
    "title": "MVN: An R Package for Assessing Multivariate Normality",
    "description": "Assessing the assumption of multivariate normality is required by many parametric mul tivariate statistical methods, such as MANOVA, linear discriminant analysis, principal component analysis, canonical correlation, etc. It is important to assess multivariate normality in order to proceed with such statistical methods. There are many analytical methods proposed for checking multivariate normality. However, deciding which method to use is a challenging process, since each method may give different results under certain conditions. Hence, we may say that there is no best method, which is valid under any condition, for normality checking. In addition to numerical results, it is very useful to use graphical methods to decide on multivariate normality. Combining the numerical results from several methods with graphical approaches can be useful and provide more reliable decisions. Here, we present an R package, MVN, to assess multivariate normality. It contains the three most widely used multivariate normality tests, including Mardia’s, Henze-Zirkler’s and Royston’s, and graphical approaches, including chi-square Q-Q, perspective and contour plots. It also includes two multivariate outlier detection methods, which are based on robust Mahalanobis distances. Moreover, this package offers functions to check the univariate normality of marginal distributions through both tests and plots. Furthermore, especially for non-R users, we provide a user-friendly web application of the package. This application is available at http://www.biosoft.hacettepe.edu.tr/MVN/.",
    "author": [
      {
        "name": "Selcuk Korkmaz",
        "url": {}
      },
      {
        "name": "Dincer Goksuluk",
        "url": {}
      },
      {
        "name": "Gokmen Zararsiz",
        "url": {}
      }
    ],
    "date": "2015-01-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMASS, FactoMineR, psych, CCA, MVN, shiny\nCRAN Task Views implied by cited packages\nPsychometrics, Multivariate, Distributions, Econometrics, Environmetrics, NumericalMathematics, Pharmacokinetics, Robust, SocialSciences, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-032/",
    "title": "qmethod: A Package to Explore Human Perspectives Using Q Methodology",
    "description": "Q is a methodology to explore the distinct subjective perspectives that exist within a group. It is used increasingly across disciplines. The methodology is semi-qualitative and the data are analysed using data reduction methods to discern the existing patterns of thought. This package is the first to perform Q analysis in R, and it provides many advantages to the existing software: namely, it is fully cross-platform, the algorithms can be transparently examined, it provides results in a clearly structured and tabulated form ready for further exploration and modelling, it produces a graphical summary of the results, and it generates a more concise report of the distinguishing and consensus statements. This paper introduces the methodology and explains how to use the package, its advantages as well as its limitations. I illustrate the main functions with a dataset on value patterns about democracy.",
    "author": [
      {
        "name": "Aiora Zabala",
        "url": {}
      }
    ],
    "date": "2015-01-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nqmethod, psych, GPArotation, FactoMineR\nCRAN Task Views implied by cited packages\nPsychometrics, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-033/",
    "title": "gset: An R Package for Exact Sequential Test of Equivalence Hypothesis Based on Bivariate Non-Central t-Statistics",
    "description": "The R package gset calculates equivalence and futility boundaries based on the exact bivariate non-central t test statistics. It is the first R package that targets specifically at the group sequential test of equivalence hypotheses. The exact test approach adopted by gset neither assumes the large-sample normality of the test statistics nor ignores the contribution to the overall Type I error rate from rejecting one out of the two one-sided hypotheses under a null value. The features of gset include: error spending functions, computation of equivalence boundaries and futility boundaries, either binding or nonbinding, depiction of stagewise boundary plots, and operating characteristics of a given group sequential design including empirical Type I error rate, empirical power, expected sample size, and probability of stopping at an interim look due to equivalence or futility.",
    "author": [
      {
        "name": "Fang Liu",
        "url": {}
      }
    ],
    "date": "2015-01-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngsDesign, GroupSeq, Hmisc, PwrGSD, AGSDest, clinfun\nCRAN Task Views implied by cited packages\nClinicalTrials, ExperimentalDesign, Bayesian, Econometrics, Multivariate, OfficialStatistics, ReproducibleResearch, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-027/",
    "title": "sgof: An R Package for Multiple Testing Problems",
    "description": "In this paper we present a new R package called sgof for multiple hypothesis testing. The principal aim of this package is to implement SGoF-type multiple testing methods, known to be more powerful than the classical false discovery rate (FDR) and family-wise error rate (FWER) based methods in certain situations, particularly when the number of tests is large. This package includes Bi nomial and Conservative SGoF and the Bayesian and Beta-Binomial SGoF multiple testing procedures, which are adaptations of the original SGoF method to the Bayesian setting and to possibly correlated tests, respectively. The sgof package also implements the Benjamini-Hochberg and Benjamini-Yekutieli FDR controlling procedures. For each method the package provides (among other things) the number of rejected null hypotheses, estimation of the corresponding FDR, and the set of adjusted p values. Some automatic plots of interest are implemented too. Two real data examples are used to illustrate how sgof works.",
    "author": [
      {
        "name": "Irene Castro-Conde",
        "url": {}
      },
      {
        "name": "Jacobo de Uña-Álvarez",
        "url": {}
      }
    ],
    "date": "2014-11-24",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsgof, mutoss, multcomp\nCRAN Task Views implied by cited packages\nClinicalTrials, SocialSciences, Survival\nBioconductor packages used\nqvalue, HybridMTest, multtest\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-020/",
    "title": "Coordinate-Based Meta-Analysis of fMRI Studies with R ",
    "description": "This paper outlines how to conduct a simple meta-analysis of neuroimaging foci of activation in R. In particular, the first part of this paper reviews the nature of fMRI data, and briefly overviews the existing packages that can be used to analyze fMRI data in R. The second part illustrates how to handle fMRI data by showing how to visualize the results of different neuroimaging studies in a so-called orthographic view, where the spatial distribution of the foci of activation from different fMRI studies can be inspected visually. Functional MRI (fMRI) is one of the most important and powerful tools of neuroscientific research. Although not as commonly used for fMRI analysis as some specific applications such as SPM (Friston et al., 2006), AFNI (Cox and Hyde, 1997), or FSL (Smith et al., 2004), R does provide several packages that can be employed in neuroimaging research. These packages deal with a variety of topics, ranging from reading and manipulating fMRI datasets, to implementing sophisticated statistical models. The goal of this paper is to provide a brief introduction to fMRI analysis, and the various R packages that can be used to carry it out. As an example, it will show how to use simple R commands to read fMRI images and plot results from previous studies, which can then be visually compared. This is a special form of meta-analysis, and a common way to compare results from the existing literature.",
    "author": [
      {
        "name": "Andrea Stocco",
        "url": {}
      }
    ],
    "date": "2014-11-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nfmri, RNiftyReg, AnalyzeFMRI, RfmriVC, arf3DS4, BHMSMAfMRI, neuRosim, oro.nifti, spatstat\nCRAN Task Views implied by cited packages\nMedicalImaging, ChemPhys, Spatial, SpatioTemporal, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-023/",
    "title": "phaseR: An R Package for Phase Plane Analysis of Autonomous ODE Systems",
    "description": "When modelling physical systems, analysts will frequently be confronted by differential equations which cannot be solved analytically. In this instance, numerical integration will usually be the only way forward. However, for autonomous systems of ordinary differential equations (ODEs) in one or two dimensions, it is possible to employ an instructive qualitative analysis foregoing this requirement, using so-called phase plane methods. Moreover, this qualitative analysis can even prove to be highly useful for systems that can be solved analytically, or will be solved numerically anyway. The package phaseR allows the user to perform such phase plane analyses: determining the stability of any equilibrium points easily, and producing informative plots.",
    "author": [
      {
        "name": "Michael J. Grayling",
        "url": {}
      }
    ],
    "date": "2014-09-30",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndeSolve, ReacTran, rootSolve, bvpSolve, sde, phaseR\nCRAN Task Views implied by cited packages\nDifferentialEquations, Finance, Pharmacokinetics, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-025/",
    "title": "Applying spartan to Understand Parameter Uncertainty in Simulations",
    "description": "In attempts to further understand the dynamics of complex systems, the application of computer simulation is becoming increasingly prevalent. Whereas a great deal of focus has been placed in the development of software tools that aid researchers develop simulations, similar focus has not been applied in the creation of tools that perform a rigorous statistical analysis of results generated through simulation: vital in understanding how these results offer an insight into the captured system. This encouraged us to develop spartan, a package of statistical techniques designed to assist researchers in understanding the relationship between their simulation and the real system. Previously we have described each technique within spartan in detail, with an accompanying immunology case study examining the development of lymphoid tissue. Here we provide a practical introduction to the package, demonstrating how each technique is run in R, to assist researchers in integrating this package alongside their chosen simulation platform.",
    "author": [
      {
        "name": "Kieran Alden",
        "url": {}
      },
      {
        "name": "Mark Read",
        "url": {}
      },
      {
        "name": "Paul S Andrews",
        "url": {}
      },
      {
        "name": "Jon Timmis",
        "url": {}
      },
      {
        "name": "Mark Coles",
        "url": {}
      }
    ],
    "date": "2014-09-30",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspartan, lhs, gplots, XML\nCRAN Task Views implied by cited packages\nDistributions, ExperimentalDesign, Graphics, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-022/",
    "title": "Prinsimp",
    "description": "Principal Components Analysis (PCA) is a common way to study the sources of variation in a high-dimensional data set. Typically, the leading principal components are used to understand the variation in the data or to reduce the dimension of the data for subsequent analysis. The remaining principal components are ignored since they explain little of the variation in the data. However, the space spanned by the low variation principal components may contain interesting structure, structure that PCA cannot find. Prinsimp is an R package that looks for interesting structure of low variability. “Interesting” is defined in terms of a simplicity measure. Looking for interpretable structure in a low variability space has particular importance in evolutionary biology, where such structure can signify the existence of a genetic constraint.",
    "author": [
      {
        "name": "Jonathan Zhang",
        "url": {}
      },
      {
        "name": "Nancy Heckman",
        "url": {}
      },
      {
        "name": "Davor Cubranic",
        "url": {}
      },
      {
        "name": "Joel G. Kingsolver",
        "url": {}
      },
      {
        "name": "Travis Gaydos",
        "url": {}
      },
      {
        "name": "J.S.            Marron",
        "url": {}
      }
    ],
    "date": "2014-09-28",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nprinsimp\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-021/",
    "title": "Automatic Conversion of Tables to LongForm Dataframes",
    "description": "TableToLongForm automatically converts hierarchical Tables intended for a human reader into a simple LongForm dataframe that is machine readable, making it easier to access and use the data for analysis. It does this by recognising positional cues present in the hierarchical Table (which would normally be interpreted visually by the human brain) to decompose, then reconstruct the data into a LongForm dataframe. The article motivates the benefit of such a conversion with an example Table, followed by a short user manual, which includes a comparison between the simple one argument call to TableToLongForm, with code for an equivalent manual conversion. The article then explores the types of Tables the package can convert by providing a gallery of all recognised patterns. It finishes with a discussion of available diagnostic methods and future work.",
    "author": [
      {
        "name": "Jimmy Oh",
        "url": {}
      }
    ],
    "date": "2014-09-26",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nTableToLongForm, reshape2, plyr\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-002/",
    "title": "A Multiscale Test of Spatial Stationarity for Textured Images in R",
    "description": "The ability to automatically identify areas of homogeneous texture present within a greyscale image is an important feature of image processing algorithms. This article describes the R package LS2Wstat which employs a recent wavelet-based test of stationarity for locally stationary random fields to assess such spatial homogeneity. By embedding this test within a quadtree image segmentation procedure we are also able to identify texture regions within an image.",
    "author": [
      {
        "name": "Matthew A. Nunes",
        "url": {}
      },
      {
        "name": "Sarah L. Taylor",
        "url": {}
      },
      {
        "name": "Idris A. Eckley",
        "url": {}
      }
    ],
    "date": "2014-06-16",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nLS2Wstat, LS2W, urca, CADFtest, locits\nCRAN Task Views implied by cited packages\nTimeSeries, Econometrics, Finance\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-008/",
    "title": "ROSE: a Package for Binary Imbalanced Learning",
    "description": "The ROSE package provides functions to deal with binary classification problems in the presence of imbalanced classes. Artificial balanced samples are generated according to a smoothed bootstrap approach and allow for aiding both the phases of estimation and accuracy evaluation of a binary classifier in the presence of a rare class. Functions that implement more traditional remedies for the class imbalance and different metrics to evaluate accuracy are also provided. These are estimated by holdout, bootstrap or cross-validation methods.",
    "author": [
      {
        "name": "Nicola Lunardon",
        "url": {}
      },
      {
        "name": "Giovanna Menardi",
        "url": {}
      },
      {
        "name": "Nicola Torelli",
        "url": {}
      }
    ],
    "date": "2014-06-16",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nDMwR, caret, ROSE, ROSE, ROSE, class\nCRAN Task Views implied by cited packages\nMultivariate, HighPerformanceComputing, MachineLearning, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-016/",
    "title": "Web Technologies Task View",
    "description": "This article presents the CRAN Task View on Web Technologies. We describe the most important aspects of Web Technologies and Web Scraping and list some of the packages that are currently available on CRAN. Finally, we plot the network of Web Technology related package dependencies.",
    "author": [
      {
        "name": "Patrick Mair",
        "url": {}
      },
      {
        "name": "Scott Chamberlain",
        "url": {}
      }
    ],
    "date": "2014-06-10",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nXML, RCurl, rjson, RJSONIO, jsonlite, httr, ROAuth, shiny, rgbif, rfishbase, rfisheries, rsnps, rentrez, crn, RNCEP, WDI, TFX, anametrix, rpubchem, cimis, nhlscrapr, tm, translate, scholar, RgoogleMap, Rfacebook, twitteR, streamR, AWS.tools, MTurkR, GuardianR, igraph\nCRAN Task Views implied by cited packages\nWebTechnologies, Spatial, ChemPhys, Finance, gR, Graphics, HighPerformanceComputing, NaturalLanguageProcessing, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-004/",
    "title": "brainR: Interactive 3 and 4D Images of High Resolution Neuroimage Data",
    "description": "We provide software tools for displaying and publishing interactive 3-dimensional (3D) and 4-dimensional (4D) figures to html webpages, with examples of high-resolution brain imaging. Our framework is based in the R statistical software using the rgl package, a 3D graphics library. We build on this package to allow manipulation of figures including rotation and translation, zooming, coloring of brain substructures, adjusting transparency levels, and addition/or removal of brain structures. The need for better visualization tools of ultra high dimensional data is ever present; we are providing a clean, simple, web-based option. We also provide a package (brainR) for users to readily implement these tools.",
    "author": [
      {
        "name": "John Muschelli",
        "url": {}
      },
      {
        "name": "Elizabeth Sweeney",
        "url": {}
      },
      {
        "name": "Ciprian Crainiceanu",
        "url": {}
      }
    ],
    "date": "2014-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrgl, knitr, Sweave, slidify, misc3d, brainR\nCRAN Task Views implied by cited packages\nGraphics, Multivariate, MedicalImaging, ReproducibleResearch, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-013/",
    "title": "The gridSVG Package",
    "description": "The gridSVG package can be used to generate a grid-based R plot in an SVG format, with the ability to add special effects to the plot. The special effects include animation, interactivity, and advanced graphical features, such as masks and filters. This article provides a basic introduction to important functions in the gridSVG package and discusses the advantages and disadvantages of gridSVG compared to similar R packages.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      },
      {
        "name": "Simon Potter",
        "url": {}
      }
    ],
    "date": "2014-06-02",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngridSVG, lattice, ggplot2, animation, RSVGTipsDevice, googleVis, shiny\nCRAN Task Views implied by cited packages\nGraphics, WebTechnologies, Multivariate, Pharmacokinetics, Phylogenetics, ReproducibleResearch, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-006/",
    "title": "PivotalR: A Package for Machine Learning on Big Data",
    "description": "PivotalR is an R package that provides a front-end to PostgreSQL and all PostgreSQL-like databases such as Pivotal Inc.’s Greenplum Database (GPDB) (Pivotal Inc., 2013a), HAWQ (Pivotal Inc., 2013b). When running on the products of Pivotal Inc., PivotalR utilizes the full power of parallel computation and distributive storage, and thus gives the normal R user access to big data. PivotalR also provides the R wrapper for MADlib. MADlib is an open-source library for scalable in-database analytics. It provides data-parallel implementations of mathematical, statistical and machine-learning algorithms for structured and unstructured data. Thus PivotalR also enables the user to apply machine learning algorithms onto big data.",
    "author": [
      {
        "name": "Hai Qian",
        "url": {}
      }
    ],
    "date": "2014-05-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nPivotalR, RPostgreSQL, shiny\nCRAN Task Views implied by cited packages\nWebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-009/",
    "title": "investr: An R Package for Inverse Estimation",
    "description": "Inverse estimation is a classical and well-known problem in regression. In simple terms, it involves the use of an observed value of the response to make inference on the corresponding unknown value of the explanatory variable. To our knowledge, however, statistical software is somewhat lacking the capabilities for analyzing these types of problems. In this paper1 , we introduce investr (which stands for inverse estimation in R), a package for solving inverse estimation problems in both linear and nonlinear regression models.",
    "author": [
      {
        "name": "Brandon M. Greenwell",
        "url": {}
      },
      {
        "name": "Christine M. Schubert Kabban",
        "url": {}
      }
    ],
    "date": "2014-05-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ninvestr, MASS, drc, car, boot\nCRAN Task Views implied by cited packages\nEconometrics, SocialSciences, ChemPhys, Multivariate, Pharmacokinetics, Distributions, Environmetrics, Finance, NumericalMathematics, Optimization, Psychometrics, Robust, Survival, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-018/",
    "title": "oligoMask: A Framework for Assessing and Removing the Effect of Genetic Variants on Microarray Probes",
    "description": "As expression microarrays are typically designed relative to a reference genome, any individual genetic variant that overlaps a probe’s genomic position can possibly cause a reduction in hybridization due to the probe no longer being a perfect match to a given sample’s mRNA at that locus. If the samples or groups used in a microarray study differ in terms of genetic variants, the results of the microarray experiment can be negatively impacted. The oligoMask package is an R/SQLite framework which can utilize publicly available genetic variants and works in conjunction with the oligo package to read in the expression data and remove microarray probes which are likely to impact a given microarray experiment prior to analysis. Tools are provided for creating an SQLite database containing the probe and variant annotations and for performing the commonly used RMA preprocessing procedure for Affymetrix microarrays. The oligoMask package is freely available at https://github.com/dbottomly/oligoMask.",
    "author": [
      {
        "name": "Daniel Bottomly",
        "url": {}
      },
      {
        "name": "Beth Wilmot",
        "url": {}
      },
      {
        "name": "Shannon K. McWeeney",
        "url": {}
      }
    ],
    "date": "2014-05-27",
    "categories": [],
    "contents": "\n\n\n\nBioconductor packages used\noligo, xps, maskBAD, VariantAnnotation, BSgenome, Biostrings\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-011/",
    "title": "The stringdist Package for Approximate String Matching",
    "description": "Comparing text strings in terms of distance functions is a common and fundamental task in many statistical text-processing applications. Thus far, string distance functionality has been somewhat scattered around R and its extension packages, leaving users with inconistent interfaces and encoding handling. The stringdist package was designed to offer a low-level interface to several popular string distance algorithms which have been re-implemented in C for this purpose. The package offers distances based on counting q-grams, edit-based distances, and some lesser known heuristic distance functions. Based on this functionality, the package also offers inexact matching equivalents of R’s native exact matching functions match and %in%.",
    "author": [
      {
        "name": "Mark P.J. van der Loo",
        "url": {}
      }
    ],
    "date": "2014-04-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nkernlab, RecordLinkage, MiscPsycho, cba, Mkmisc, deducorrect, vwr, stringdist, textcat, TraMineR\nCRAN Task Views implied by cited packages\nOfficialStatistics, Cluster, NaturalLanguageProcessing, Graphics, MachineLearning, Multivariate, Optimization, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-005/",
    "title": "The RWiener Package: an R Package Providing Distribution Functions for the Wiener Diffusion Model",
    "description": "We present the RWiener package that provides R functions for the Wiener diffusion model. The core of the package are the four distribution functions dwiener, pwiener, qwiener and rwiener, which use up-to-date methods, implemented in C, and provide fast and accurate computation of the density, distribution, and quantile function, as well as a random number generator for the Wiener diffusion model. We used the typical Wiener diffusion model with four parameters: boundary separation, non-decision time, initial bias and drift rate parameter. Beyond the distribution functions, we provide extended likelihood-based functions that can be used for parameter estimation and model selection. The package can be obtained via CRAN.",
    "author": [
      {
        "name": "Dominik Wabersich",
        "url": {}
      },
      {
        "name": "Joachim Vandekerckhove",
        "url": {}
      }
    ],
    "date": "2014-04-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRWiener\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-007/",
    "title": "rotations: An R Package for SO(3) Data",
    "description": "In this article we introduce the rotations package which provides users with the ability to simulate, analyze and visualize three-dimensional rotation data. More specifically it includes four commonly used distributions from which to simulate data, four estimators of the central orientation, six confidence region estimation procedures and two approaches to visualizing rotation data. All of these features are available for two different parameterizations of rotations: three-by-three matrices and quaternions. In addition, two datasets are included that illustrate the use of rotation data in practice.",
    "author": [
      {
        "name": "Bryan Stanfill",
        "url": {}
      },
      {
        "name": "Heike Hofmann",
        "url": {}
      },
      {
        "name": "Ulrike Genschel",
        "url": {}
      }
    ],
    "date": "2014-04-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\norientlib, onion, circular, SpherWave, rotations, ggplot2, sphereplot, Rcpp, RcppArmadillo\nCRAN Task Views implied by cited packages\nNumericalMathematics, Graphics, Environmetrics, HighPerformanceComputing, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-014/",
    "title": "MRCV: A Package for Analyzing Categorical Variables with Multiple Response Options",
    "description": "Multiple response categorical variables (MRCVs), also known as “pick any” or “choose all that apply” variables, summarize survey questions for which respondents are allowed to select more than one category response option. Traditional methods for analyzing the association between categorical variables are not appropriate with MRCVs due to the within-subject dependence among responses. We have developed the MRCV package as the first R package available to correctly analyze MRCV data. Statistical methods offered by our package include counterparts to traditional Pearson chi-square tests for independence and loglinear models, where bootstrap methods and Rao-Scott adjustments are relied on to obtain valid inferences. We demonstrate the primary functions within the package by analyzing data from a survey assessing the swine waste management practices of Kansas farmers.",
    "author": [
      {
        "name": "Natalie A. Koziol",
        "url": {}
      },
      {
        "name": "Christopher R. Bilder",
        "url": {}
      }
    ],
    "date": "2014-04-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMRCV, geepack\nCRAN Task Views implied by cited packages\nEconometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-019/",
    "title": "sgr: A Package for Simulating Conditional Fake Ordinal Data",
    "description": "Many self-report measures of attitudes, beliefs, personality, and pathology include items that can be easily manipulated by respondents. For example, an individual may deliberately attempt to manipulate or distort responses to simulate grossly exaggerated physical or psychological symptoms in order to reach specific goals such as, for example, obtaining financial compensation, avoiding being charged with a crime, avoiding military duty, or obtaining drugs. This article introduces the package sgr that can be used to perform fake data analysis according to the sample generation by replacement approach. The package includes functions for making simple inferences about discrete/ordinal fake data. The package allows to quantify uncertainty in inferences based on possible fake data as well as to study the implications of fake data for empirical results.",
    "author": [
      {
        "name": "Luigi Lombardi",
        "url": {}
      },
      {
        "name": "Massimiliano Pastore",
        "url": {}
      }
    ],
    "date": "2014-04-19",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsgr, polycor, MASS\nCRAN Task Views implied by cited packages\nMultivariate, Psychometrics, Distributions, Econometrics, Environmetrics, NumericalMathematics, Pharmacokinetics, Robust, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-012/",
    "title": "RStorm: Developing and Testing Streaming Algorithms in R",
    "description": "Streaming data, consisting of indefinitely evolving sequences, are becoming ubiquitous in many branches of science and in various applications. Computer scientists have developed streaming applications such as Storm and the S4 distributed stream computing platform1 to deal with data streams. However, in current production packages testing and evaluating streaming algorithms is cumbersome. This paper presents RStorm for the development and evaluation of streaming algorithms analogous to these production packages, but implemented fully in R. RStorm allows developers of streaming algorithms to quickly test, iterate, and evaluate various implementations of streaming algorithms. The paper provides both a canonical computer science example, the streaming word count, and examples of several statistical applications of RStorm.",
    "author": [
      {
        "name": "Maurits Kaptein",
        "url": {}
      }
    ],
    "date": "2014-03-18",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRStorm, stream\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-001/",
    "title": "Taming PITCHf/x Data with XML2R and pitchRx",
    "description": "XML2R is a framework that reduces the effort required to transform XML content into tables in a way that preserves parent to child relationships. pitchRx applies XML2R’s grammar for XML manipulation to Major League Baseball Advanced Media (MLBAM)’s Gameday data. With pitchRx, one can easily obtain and store Gameday data in a remote database. The Gameday website hosts a wealth of XML data, but perhaps most interesting is PITCHf/x. Among other things, PITCHf/x data can be used to recreate a baseball’s flight path from a pitcher’s hand to home plate. With pitchRx, one can easily create animations and interactive 3D scatterplots of the baseball’s flight path. PITCHf/x data is also commonly used to generate a static plot of baseball locations at the moment they cross home plate. These plots, sometimes called strike-zone plots, can also refer to a plot of event probabilities over the same region. pitchRx provides an easy and robust way to generate strike-zone plots using the ggplot2 package.",
    "author": [
      {
        "name": "Carson Sievert",
        "url": {}
      }
    ],
    "date": "2014-03-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\npitchRx, XML2R, ggplot2, rgl, dplyr, mgcv, knitr\nCRAN Task Views implied by cited packages\nGraphics, Bayesian, Econometrics, Environmetrics, Multivariate, Phylogenetics, ReproducibleResearch, SocialSciences, SpatioTemporal, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-003/",
    "title": "Stratified Weibull Regression Model for Interval-Censored Data",
    "description": "Interval censored outcomes arise when a silent event of interest is known to have occurred within a specific time period determined by the times of the last negative and first positive diagnostic tests. There is a rich literature on parametric and non-parametric approaches for the analysis of interval-censored outcomes. A commonly used strategy is to use a proportional hazards (PH) model with the baseline hazard function parameterized. The proportional hazards assumption can be relaxed in stratified models by allowing the baseline hazard function to vary across strata defined by a subset of explanatory variables. In this paper, we describe and implement a new R package straweib, for fitting a stratified Weibull model appropriate for interval censored outcomes. We illustrate the R package straweib by analyzing data from a longitudinal oral health study on the timing of the emergence of permanent teeth in 4430 children.",
    "author": [
      {
        "name": "Xiangdong Gu",
        "url": {}
      },
      {
        "name": "David Shapiro",
        "url": {}
      },
      {
        "name": "Michael D. Hughes",
        "url": {}
      },
      {
        "name": "Raji Balasubramanian",
        "url": {}
      }
    ],
    "date": "2014-03-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsurvival, straweib\nCRAN Task Views implied by cited packages\nClinicalTrials, Econometrics, SocialSciences, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-010/",
    "title": "Rankcluster: An R Package for Clustering Multivariate Partial Rankings",
    "description": "The Rankcluster package is the first R package proposing both modeling and clustering tools for ranking data, potentially multivariate and partial. Ranking data are modeled by the Insertion Sorting Rank (ISR) model, which is a meaningful model parametrized by a central ranking and a dispersion parameter. A conditional independence assumption allows multivariate rankings to be taken into account, and clustering is performed by means of mixtures of multivariate ISR models. The parameters of the cluster (central rankings and dispersion parameters) help the practitioners to interpret the clustering. Moreover, the Rankcluster package provides an estimate of the missing ranking positions when rankings are partial. After an overview of the mixture of multivariate ISR models, the Rankcluster package is described and its use is illustrated through the analysis of two real datasets.",
    "author": [
      {
        "name": "Julien Jacques",
        "url": {}
      },
      {
        "name": "Quentin Grimonprez",
        "url": {}
      },
      {
        "name": "Christophe Biernacki",
        "url": {}
      }
    ],
    "date": "2014-03-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRankcluster, pmr, RMallow\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-015/",
    "title": "Archiving Reproducible Research with R and Dataverse",
    "description": "Reproducible research and data archiving are increasingly important issues in research involving statistical analyses of quantitative data. This article introduces the dvn package, which allows R users to publicly archive datasets, analysis files, codebooks, and associated metadata in Dataverse Network online repositories, an open-source data archiving project sponsored by Harvard University. In this article I review the importance of data archiving in the context of reproducible research, introduces the Dataverse Network, explain the implementation of the dvn package, and provide example code for archiving and releasing data using the package.",
    "author": [
      {
        "name": "Thomas J. Leeper",
        "url": {}
      }
    ],
    "date": "2014-03-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndvn, knitr, rfigshare, RCurl, XML, rfigshare, rdryad, OAIHarvester\nCRAN Task Views implied by cited packages\nWebTechnologies, Phylogenetics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2014-017/",
    "title": "Addendum to ``Statistical Software from a Blind Person's Perspective''",
    "description": "This short note explains a solution to a problem for blind users when using the R terminal under Windows Vista or Windows 7, as identified in Godfrey (2013). We note the way the solution was discovered and subsequent confirmatory experiments. As part of his preparations for teaching a blind student in a statistics course, the second author",
    "author": [
      {
        "name": "A. Jonathan R. Godfrey",
        "url": {}
      },
      {
        "name": "Robert Erhardt",
        "url": {}
      }
    ],
    "date": "2014-03-03",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-029/",
    "title": "Dynamic Parallelization of R Functions",
    "description": "R offers several extension packages that allow it to perform parallel computations. These operate on fixed points in the program flow and make it difficult to deal with nested parallelism and to organize parallelism in complex computations in general. In this article we discuss, first, of how to detect parallelism in functions, and second, how to minimize user intervention in that process. We present a solution that requires minimal code changes and enables to flexibly and dynamically choose the degree of parallelization in the resulting computation. An implementation is provided by the R package parallelize.dynamic and practical issues are discussed with the help of examples.",
    "author": [
      {
        "name": "Stefan Böhringer",
        "url": {}
      }
    ],
    "date": "2013-12-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRsge, foreach, boot, snow, parallelize.dynamic\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, Econometrics, Optimization, SocialSciences, Survival, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-034/",
    "title": "betategarch: Simulation, Estimation and Forecasting of Beta-Skew-t-EGARCH Models",
    "description": "This paper illustrates the usage of the betategarch package, a package for the simulation, estimation and forecasting of Beta-Skew-t-EGARCH models. The Beta-Skew-t-EGARCH model is a dynamic model of the scale or volatility of financial returns. The model is characterised by its robustness to jumps or outliers, and by its exponential specification of volatility. The latter enables richer dynamics, since parameters need not be restricted to be positive to ensure positivity of volatility. In addition, the model also allows for heavy tails and skewness in the conditional return (i.e. scaled return), and for leverage and a time-varying long-term component in the volatility specification. More generally, the model can be viewed as a model of the scale of the error in a dynamic regression.",
    "author": [
      {
        "name": "Genaro Sucarrat",
        "url": {}
      }
    ],
    "date": "2013-12-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntseries, fGarch, rugarch, AutoSEARCH, zoo\nCRAN Task Views implied by cited packages\nFinance, TimeSeries, Econometrics, Environmetrics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-032/",
    "title": "The R in Robotics",
    "description": "The aim of this contribution is to connect two previously separated worlds: robotic application development with the Robot Operating System (ROS) and statistical programming with R. This fruitful combination becomes apparent especially in the analysis and visualization of sensory data. We therefore introduce a new language extension for ROS that allows to implement nodes in pure R. All relevant aspects are described in a step-by-step development of a common sensor data transformation node. This includes the reception of raw sensory data via the ROS network, message interpretation, bag-file analysis, transformation and visualization, as well as the transmission of newly generated messages back into the ROS network.",
    "author": [
      {
        "name": "André Dietrich",
        "url": {}
      },
      {
        "name": "Sebastian Zug",
        "url": {}
      },
      {
        "name": "Jörg Kaiser",
        "url": {}
      }
    ],
    "date": "2013-12-13",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRcpp\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-021/",
    "title": "factorplot: Improving Presentation of Simple Contrasts in Generalized Linear Models",
    "description": "Recent statistical literature has paid attention to the presentation of pairwise comparisons either from the point of view of the reference category problem in generalized linear models (GLMs) or in terms of multiple comparisons. Both schools of thought are interested in the parsimonious presentation of sufficient information to enable readers to evaluate the significance of contrasts resulting from the inclusion of qualitative variables in GLMs. These comparisons also arise when trying to interpret multinomial models where one category of the dependent variable is omitted as a reference. While considerable advances have been made, opportunities remain to improve the presentation of this information, especially in graphical form. The factorplot package provides new functions for graphically and numerically presenting results of hypothesis tests related to pairwise comparisons resulting from qualitative covariates in GLMs or coefficients in multinomial logistic regression models.",
    "author": [
      {
        "name": "David A. Armstrong II",
        "url": {}
      }
    ],
    "date": "2013-11-22",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmultcomp, qvcalc, Epi, car, multcompView, factorplot\nCRAN Task Views implied by cited packages\nSocialSciences, Survival, ClinicalTrials, Econometrics, Finance, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-030/",
    "title": "CompLognormal: An R Package for Composite Lognormal Distributions",
    "description": "In recent years, composite models based on the lognormal distribution have become popular in actuarial sciences and related areas. In this short note, we present a new R package for computing the probability density function, cumulative density function, and quantile function, and for generating random numbers of any composite model based on the lognormal distribution. The use of the package is illustrated using a real data set.",
    "author": [
      {
        "name": "S. Nadarajah",
        "url": {}
      },
      {
        "name": "S. A. A. Bakar",
        "url": {}
      }
    ],
    "date": "2013-11-18",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nCompLognormal, CRAN, poweRlaw, SMPracticals, MASS, fitdistrplus, distrMod\nCRAN Task Views implied by cited packages\nDistributions, Survival, Econometrics, Environmetrics, Multivariate, NumericalMathematics, Pharmacokinetics, Psychometrics, Robust, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-031/",
    "title": "lfe: Linear Group Fixed Effects",
    "description": "Linear models with fixed effects and many dummy variables are common in some fields. Such models are straightforward to estimate unless the factors have too many levels. The R package lfe solves this problem by implementing a generalization of the within transformation to multiple factors, tailored for large problems.",
    "author": [
      {
        "name": "Simen Gaure",
        "url": {}
      }
    ],
    "date": "2013-11-18",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMatrix, plm, lfe, igraph, multicore\nCRAN Task Views implied by cited packages\nEconometrics, gR, Graphics, Multivariate, NumericalMathematics, Optimization, Spatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-033/",
    "title": "On Sampling from the Multivariate t Distribution",
    "description": "The multivariate normal and the multivariate t distributions belong to the most widely used multivariate distributions in statistics, quantitative risk management, and insurance. In contrast to the multivariate normal distribution, the parameterization of the multivariate t distribution does not correspond to its moments. This, paired with a non-standard implementation in the R package mvtnorm, provides traps for working with the multivariate t distribution. In this paper, common traps are clarified and corresponding recent changes to mvtnorm are presented.",
    "author": [
      {
        "name": "Marius Hofert",
        "url": {}
      }
    ],
    "date": "2013-11-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmvtnorm, MASS, evir, mnormt, QRM\nCRAN Task Views implied by cited packages\nDistributions, Multivariate, Environmetrics, ExtremeValue, Econometrics, Finance, NumericalMathematics, Pharmacokinetics, Psychometrics, Robust, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-027/",
    "title": "rlme: An R Package for Rank-Based Estimation and Prediction in Random Effects Nested Models",
    "description": "There is a lack of robust statistical analyses for random effects linear models. In practice, statistical analyses, including estimation, prediction and inference, are not reliable when data are unbalanced, of small size, contain outliers, or not normally distributed. It is fortunate that rank-based regression analysis is a robust nonparametric alternative to likelihood and least squares analysis. We propose an R package that calculates rank-based statistical analyses for twoand three-level random effects nested designs. In this package, a new algorithm which recursively obtains robust predictions for both scale and random effects is used, along with three rank-based fitting methods.",
    "author": [
      {
        "name": "Yusuf K. Bilgic",
        "url": {}
      },
      {
        "name": "Herbert Susmann",
        "url": {}
      }
    ],
    "date": "2013-10-25",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\naa, Rfit, rlme, lme4\nCRAN Task Views implied by cited packages\nBayesian, Econometrics, Environmetrics, OfficialStatistics, Psychometrics, SocialSciences, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-023/",
    "title": "RNetCDF - A Package for Reading and Writing NetCDF Datasets",
    "description": "This paper describes the RNetCDF package (version 1.6), an interface for reading and writing files in Unidata NetCDF format, and gives an introduction to the NetCDF file format. NetCDF is a machine independent binary file format which allows storage of different types of array based data, along with short metadata descriptions. The package presented here allows access to the most important functions of the NetCDF C-interface for reading, writing, and modifying NetCDF datasets. In this paper, we present a short overview on the NetCDF file format and show usage examples of the package.",
    "author": [
      {
        "name": "Pavel Michna",
        "url": {}
      },
      {
        "name": "Milton Woods",
        "url": {}
      }
    ],
    "date": "2013-10-14",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRNetCDF, ncdf, ncdf4\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-022/",
    "title": "spMC: Modelling Spatial Random Fields with Continuous Lag Markov Chains",
    "description": "Currently, a part of the R statistical software is developed in order to deal with spatial models. More specifically, some available packages allow the user to analyse categorical spatial random patterns. However, only the spMC package considers a viewpoint based on transition probabilities between locations. Through the use of this package it is possible to analyse the spatial variability of data, make inference, predict and simulate the categorical classes in unobserved sites. An example is presented by analysing the well-known Swiss Jura data set.",
    "author": [
      {
        "name": "Luca Sartore",
        "url": {}
      }
    ],
    "date": "2013-09-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspMC, gstat, geoRglm, RandomFields\nCRAN Task Views implied by cited packages\nSpatial, SpatioTemporal, Bayesian\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-035/",
    "title": "Changes to grid for R 3.0.0",
    "description": "From R 3.0.0, there is a new recommended way to develop new grob classes in grid. In a nutshell, two new “hook” functions, makeContext() and makeContent() have been added to grid to provide an alternative to the existing hook functions preDrawDetails(), drawDetails(), and postDrawDetails(). There is also a new function called grid.force(). This article discusses why these changes have been made, provides a simple demonstration of the use of the new functions, and discusses some of the implications for packages that build on grid.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2013-09-27",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nlattice, ggplot2, gtable, gridSVG, grImport, gridGraphviz, gridExtra\nCRAN Task Views implied by cited packages\nGraphics, Multivariate, Pharmacokinetics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-025/",
    "title": "Performance Attribution for Equity Portfolios",
    "description": "The pa package provides tools for conducting performance attribution for long-only, single currency equity portfolios. The package uses two methods: the Brinson-Hood-Beebower model (hereafter referred to as the Brinson model) and a regression-based analysis. The Brinson model takes an ANOVA-type approach and decomposes the active return of any portfolio into asset allocation, stock selection, and interaction effect. The regression-based analysis utilizes estimated coefficients, based on a regression model, to attribute active return to different factors.",
    "author": [
      {
        "name": "Yang Lu",
        "url": {}
      },
      {
        "name": "David Kane",
        "url": {}
      }
    ],
    "date": "2013-09-23",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\npa, portfolio, PerformanceAnalytics, portfolio\nCRAN Task Views implied by cited packages\nFinance\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-024/",
    "title": "Surface Melting Curve Analysis with R",
    "description": "Nucleic acid Melting Curve Analysis is a powerful method to investigate the interaction of double stranded nucleic acids. Many researchers rely on closed source software which is not ubiquitously available, and gives only little control over the computation and data presentation. R in contrast, is open source, highly adaptable and provides numerous utilities for data import, sophisticated statistical analysis and presentation in publication quality. This article covers methods, implemented in the MBmca package, for DNA Melting Curve Analysis on microbead surfaces. Particularly, the use of the second derivative melting peaks is suggested as an additional parameter to characterize the melting behavior of DNA duplexes. Examples of microbead surface Melting Curve Analysis on fragments of human genes are presented.",
    "author": [
      {
        "name": "Stefan Rödiger",
        "url": {}
      },
      {
        "name": "Alexander Böhm",
        "url": {}
      },
      {
        "name": "Ingolf Schimke",
        "url": {}
      }
    ],
    "date": "2013-08-26",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nqpcR, MBmca, robustbase, stats, signal, zoo, delftfews, Hmisc, base, fda\nCRAN Task Views implied by cited packages\nEconometrics, Multivariate, Bayesian, ClinicalTrials, Environmetrics, Finance, NumericalMathematics, OfficialStatistics, ReproducibleResearch, Robust, SocialSciences, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-028/",
    "title": "Temporal Disaggregation of Time Series",
    "description": "Temporal disaggregation methods are used to disaggregate low frequency time series to higher frequency series, where either the sum, the average, the first or the last value of the resulting high frequency series is consistent with the low frequency series. Temporal disaggregation can be performed with or without one or more high frequency indicator series. The package tempdisagg is a collection of several methods for temporal disaggregation.",
    "author": [
      {
        "name": "Christoph Sax",
        "url": {}
      },
      {
        "name": "Peter Steiner",
        "url": {}
      }
    ],
    "date": "2013-08-26",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntempdisagg\nCRAN Task Views implied by cited packages\nTimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-026/",
    "title": "ExactCIdiff: An R Package for Computing Exact Confidence Intervals for the Difference of Two Proportions",
    "description": "Comparing two proportions through the difference is a basic problem in statistics and has applications in many fields. More than twenty confidence intervals (Newcombe, 1998a,b) have been proposed. Most of them are approximate intervals with an asymptotic infimum coverage probability much less than the nominal level. In addition, large sample may be costly in practice. So exact optimal confidence intervals become critical for drawing valid statistical inference with accuracy and precision. Recently, Wang (2010, 2012) derived the exact smallest (optimal) one-sided 1 − α confidence intervals for the difference of two paired or independent proportions. His intervals, however, are computer-intensive by nature. In this article, we provide an R package ExactCIdiff to implement the intervals when the sample size is not large. This would be the first available package in R to calculate the exact confidence intervals for the difference of proportions. Exact two-sided 1 − α interval can be easily obtained by taking the intersection of the lower and upper one-sided 1 − α/2 intervals. Readers may jump to Examples 1 and 2 to obtain these intervals.",
    "author": [
      {
        "name": "Guogen Shan",
        "url": {}
      },
      {
        "name": "Weizhen Wang",
        "url": {}
      }
    ],
    "date": "2013-08-16",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nExactCIdiff, Epi, PropCIs, exactci\nCRAN Task Views implied by cited packages\nSurvival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-020/",
    "title": "Translating Probability Density Functions: From R to BUGS and Back Again",
    "description": "The ability to implement statistical models in the BUGS language facilitates Bayesian in ference by automating MCMC algorithms. Software packages that interpret the BUGS language include OpenBUGS, WinBUGS, and JAGS. R packages that link BUGS software to the R environment, including rjags and R2WinBUGS, are widely used in Bayesian analysis. Indeed, many packages in the Bayesian task view on CRAN (http://cran.r-project.org/web/views/Bayesian.html) depend on this integration. However, the R and BUGS languages use different representations of common probability density functions, creating a potential for errors to occur in the implementation or interpre tation of analyses that use both languages. Here we review different parameterizations used by the R and BUGS languages, describe how to translate between the languages, and provide an R function, r2bugs.distributions, that transforms parameterizations from R to BUGS and back again.",
    "author": [
      {
        "name": "David S. LeBauer",
        "url": {}
      },
      {
        "name": "Michael C. Dietze",
        "url": {}
      },
      {
        "name": "Benjamin M. Bolker",
        "url": {}
      }
    ],
    "date": "2013-06-17",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrjags, R2WinBUGS\nCRAN Task Views implied by cited packages\nBayesian, gR, Cluster\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-008/",
    "title": "PIN: Measuring Asymmetric Information in Financial Markets with R",
    "description": "The package PIN computes a measure of asymmetric information in financial markets, the so-called probability of informed trading. This is obtained from a sequential trade model and is used to study the determinants of an asset price. Since the probability of informed trading depends on the number of buyand sell-initiated trades during a trading day, this paper discusses the entire modelling cycle, from data handling to the computation of the probability of informed trading and the estimation of parameters for the underlying theoretical model.",
    "author": [
      {
        "name": "Paolo Zagaglia",
        "url": {}
      }
    ],
    "date": "2013-06-04",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nPIN, highfrequency, IBrokers, orderbook\nCRAN Task Views implied by cited packages\nFinance\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-001/",
    "title": "RTextTools: A Supervised Learning Package for Text Classification",
    "description": "Social scientists have long hand-labeled texts to create datasets useful for studying topics from congressional policymaking to media reporting. Many social scientists have begun to incorporate machine learning into their toolkits. RTextTools was designed to make machine learning accessible by providing a start-to-finish product in less than 10 steps. After installing RTextTools, the initial step is to generate a document term matrix. Second, a container object is created, which holds all the objects needed for further analysis. Third, users can use up to nine algorithms to train their data. Fourth, the data are classified. Fifth, the classification is summarized. Sixth, functions are available for performance evaluation. Seventh, ensemble agreement is conducted. Eighth, users can cross-validate their data. Finally, users write their data to a spreadsheet, allowing for further manual coding if required.",
    "author": [
      {
        "name": "Timothy P. Jurka",
        "url": {}
      },
      {
        "name": "Loren Collingwood",
        "url": {}
      },
      {
        "name": "Amber E. Boydstun",
        "url": {}
      },
      {
        "name": "Emiliano Grossman",
        "url": {}
      },
      {
        "name": "Wouter van            Atteveldt",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRTextTools, glmnet, maxent, e1071, tm, ipred, caTools, randomForest, nnet, tree\nCRAN Task Views implied by cited packages\nMachineLearning, Environmetrics, NaturalLanguageProcessing, Survival, Cluster, Distributions, Econometrics, HighPerformanceComputing, Multivariate, Psychometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-002/",
    "title": "Generalized Simulated Annealing for Global Optimization: The GenSA Package",
    "description": "Many problems in statistics, finance, biology, pharmacology, physics, mathematics, eco nomics, and chemistry involve determination of the global minimum of multidimensional functions. R packages for different stochastic methods such as genetic algorithms and differential evolution have been developed and successfully used in the R community. Based on Tsallis statistics, the R package GenSA was developed for generalized simulated annealing to process complicated non-linear objective functions with a large number of local minima. In this paper we provide a brief introduction to the R package and demonstrate its utility by solving a non-convex portfolio optimization problem in finance and the Thomson problem in physics. GenSA is useful and can serve as a complementary tool to, rather than a replacement for, other widely used R packages for optimization.",
    "author": [
      {
        "name": "Yang Xiang",
        "url": {}
      },
      {
        "name": "Sylvain Gubian",
        "url": {}
      },
      {
        "name": "Brian Suomela",
        "url": {}
      },
      {
        "name": "Julia Hoeng",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nDEoptim, rgenoud, likelihood, dclone, subselect, GenSA\nCRAN Task Views implied by cited packages\nOptimization, HighPerformanceComputing, Bayesian, ChemPhys, gR, MachineLearning\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-003/",
    "title": "Multiple Factor Analysis for Contingency Tables in the FactoMineR Package",
    "description": "We present multiple factor analysis for contingency tables (MFACT) and its implementation in the FactoMineR package. This method, through an option of the MFA function, allows us to deal with multiple contingency or frequency tables, in addition to the categorical and quantitative multiple tables already considered in previous versions of the package. Thanks to this revised function, either a multiple contingency table or a mixed multiple table integrating quantitative, categorical and frequency data can be tackled. The FactoMineR package (Lê et al., 2008; Husson et al., 2011) offers the most commonly used principal component methods: principal component analysis (PCA), correspondence analysis (CA; Benzécri, 1973), multiple correspondence analysis (MCA; Lebart et al., 2006) and multiple factor analysis (MFA; Escofier and Pagès, 2008). Detailed presentations of these methods enriched by numerous examples can be consulted at the website http://factominer.free.fr/. An extension of the MFA function that considers contingency or frequency tables as proposed by Bécue-Bertaut and Pagès (2004, 2008) is detailed in this article. First, an example is presented in order to motivate the approach. Next, the mortality data used to illustrate the method are introduced. Then we briefly describe multiple factor analysis (MFA) and present the principles of its extension to contingency tables. A real example on mortality data illustrates the handling of the MFA function to analyse these multiple tables and, finally, conclusions are presented.",
    "author": [
      {
        "name": "Belchin Kostov",
        "url": {}
      },
      {
        "name": "Mónica Bécue-Bertaut",
        "url": {}
      },
      {
        "name": "François Husson",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nFactoMineR\nCRAN Task Views implied by cited packages\nMultivariate, Psychometrics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-004/",
    "title": "Hypothesis Tests for Multivariate Linear Models Using the car Package",
    "description": "The multivariate linear model is Y = X B + E (n×m) (n× p)( p×m) (n×m) The multivariate linear model can be fit with the lm function in R, where the left-hand side of the model comprises a matrix of response variables, and the right-hand side is specified exactly as for a univariate linear model (i.e., with a single response variable). This paper explains how to use the Anova and linearHypothesis functions in the car package to perform convenient hypothesis tests for parameters in multivariate linear models, including models for repeated-measures data.",
    "author": [
      {
        "name": "John Fox",
        "url": {}
      },
      {
        "name": "Michael Friendly",
        "url": {}
      },
      {
        "name": "Sanford Weisberg",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncar, lme4, nlme, survival, nnet, MASS, survey, heplots\nCRAN Task Views implied by cited packages\nSocialSciences, Econometrics, Environmetrics, OfficialStatistics, Psychometrics, Finance, Multivariate, Pharmacokinetics, SpatioTemporal, Survival, Bayesian, ChemPhys, ClinicalTrials, Distributions, MachineLearning, NumericalMathematics, Robust, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-005/",
    "title": "osmar: OpenStreetMap and R",
    "description": "OpenStreetMap provides freely accessible and editable geographic data. The osmar package smoothly integrates the OpenStreetMap project into the R ecosystem. The osmar package provides infrastructure to access OpenStreetMap data from different sources, to enable working with the OSM data in the familiar R idiom, and to convert the data into objects based on classes provided by existing R packages. This paper explains the package’s concept and shows how to use it. As an application we present a simple navigation device.",
    "author": [
      {
        "name": "Manuel J. A. Eugster",
        "url": {}
      },
      {
        "name": "Thomas Schlesinger",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nosmar, osmar, OpenStreetMap, RgoogleMaps, ggmap, sp, igraph, geosphere, Rcpp\nCRAN Task Views implied by cited packages\nSpatial, WebTechnologies, gR, Graphics, HighPerformanceComputing, NumericalMathematics, Optimization, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-006/",
    "title": "ftsa: An R Package for Analyzing Functional Time Series",
    "description": "Recent advances in computer recording and storing technology have tremendously increased the presence of functional data, whose graphical representation can be infinite-dimensional curve, image, or shape. When the same functional object is observed over a period of time, such data are known as functional time series. This article makes first attempt to describe several techniques (centered around functional principal component analysis) for modeling and forecasting functional time series from a computational aspect, using a readily-available R addon package. These methods are demonstrated using age-specific Australian fertility rate data from 1921 to 2006, and monthly sea surface temperature data from January 1950 to December 2011.",
    "author": [
      {
        "name": "Han Lin Shang",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nftsa\nCRAN Task Views implied by cited packages\nFunctionalData, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-007/",
    "title": "Statistical Software from a Blind Person's Perspective",
    "description": "Blind people have experienced access issues to many software applications since the advent of the Windows operating system; statistical software has proven to follow the rule and not be an exception. The ability to use R within minutes of download with next to no adaptation has opened doors for accessible production of statistical analyses for this author (himself blind) and blind students around the world. This article shows how little is required to make R the most accessible statistical software available today. There is any number of ramifications that this opportunity creates for blind students, especially in terms of their future research and employment prospects. There is potential for making R even better for blind users. The extensibility of R makes this possible through added functionality being made available in an add-on package called BrailleR. Functions in this package are intended to make graphical information available in text form.",
    "author": [
      {
        "name": "A. Jonathan R. Godfrey",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRcmdr, TeachingDemos, BrailleR, R2HTML\nCRAN Task Views implied by cited packages\nFinance, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-009/",
    "title": "QCA: A Package for Qualitative Comparative Analysis",
    "description": "We present QCA, a package for performing Qualitative Comparative Analysis (QCA). QCA is becoming increasingly popular with social scientists, but none of the existing software alternatives covers the full range of core procedures. This gap is now filled by QCA. After a mapping of the method’s diffusion, we introduce some of the package’s main capabilities, including the calibration of crisp and fuzzy sets, the analysis of necessity relations, the construction of truth tables and the derivation of complex, parsimonious and intermediate solutions.",
    "author": [
      {
        "name": "Alrik Thiem",
        "url": {}
      },
      {
        "name": "Adrian Duşa",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nQCA, QCA3, VennDiagram\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-010/",
    "title": "An Introduction to the EcoTroph R Package: Analyzing Aquatic Ecosystem Trophic Networks",
    "description": "Recent advances in aquatic ecosystem modelling have particularly focused on trophic network analysis through trophodynamic models. We present here a R package devoted to a recently developed model, EcoTroph. This model enables the analysis of aquatic ecological networks and the related impacts of fisheries. It was available through a plug-in in the well-known Ecopath with Ecosim software or through implementations in Excel sheets. The R package we developed simplifies the access to the EcoTroph model and offers a new interfacing between two widely used software, Ecopath and R.",
    "author": [
      {
        "name": "Mathieu Colléter",
        "url": {}
      },
      {
        "name": "Jérôme Guitton",
        "url": {}
      },
      {
        "name": "Didier Gascuel",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nEcoTroph, XML\nCRAN Task Views implied by cited packages\nWebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-011/",
    "title": "stellaR: A Package to Manage Stellar Evolution Tracks and Isochrones",
    "description": "We present the R package stellaR, which is designed to access and manipulate publicly available stellar evolutionary tracks and isochrones from the Pisa low-mass database. The procedures for extracting important stages in the evolution of a star from the database, for constructing isochrones from stellar tracks and for interpolating among tracks are discussed and demonstrated. Due to the advance in the instrumentation, nowadays astronomers can deal with a huge amount of high-quality observational data. In the last decade impressive improvements of spectroscopic and photometric observational capabilities made available data which stimulated the research in the glob ular clusters field. The theoretical effort of recovering the evolutionary history of the clusters benefits from the computation of extensive databases of stellar tracks and isochrones, such as Pietrinferni et al. (2006); Dotter et al. (2008); Bertelli et al. (2008). We recently computed a large data set of stellar tracks and isochrones, “The Pisa low-mass database” (Dell’Omodarme et al., 2012), with up to date physical and chemical inputs, and made available all the calculations to the astrophysical community at the Centre de Données astronomiques de Strasbourg (CDS)1 , a data center dedicated to the collection and worldwide distribution of astronomical data. In most databases, the management of the information and the extraction of the relevant evolu tionary properties from libraries of tracks and/or isochrones is the responsibility of the end users. Due to its extensive capabilities of data manipulation and analysis, however, R is an ideal choice for these tasks. Nevertheless R is not yet well known in astrophysics; up to December 2012 only seven astronomical or astrophysical-oriented packages have been published on CRAN (see the CRAN Task View Chemometrics and Computational Physics). The package stellaR (Dell’Omodarme and Valle, 2012) is an effort to make available to the astro physical community a basic tool set with the following capabilities: retrieve the required calculations from CDS; plot the information in a suitable form; construct by interpolation tracks or isochrones of compositions different to the ones available in the database; construct isochrones for age not included in the database; extract relevant evolutionary points from tracks or isochrones.",
    "author": [
      {
        "name": "Matteo Dell’Omodarme",
        "url": {}
      },
      {
        "name": "Giada Valle",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nstellaR\nCRAN Task Views implied by cited packages\nChemPhys\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-012/",
    "title": "Let Graphics Tell the Story - Datasets in R",
    "description": "Graphics are good for showing the information in datasets and for complementing modelling. Sometimes graphics show information models miss, sometimes graphics help to make model results more understandable, and sometimes models show whether information from graphics has statistical support or not. It is the interplay of the two approaches that is valuable. Graphics could be used a lot more in R examples and we explore this idea with some datasets available in R packages.",
    "author": [
      {
        "name": "Antony Unwin",
        "url": {}
      },
      {
        "name": "Heike Hofmann",
        "url": {}
      },
      {
        "name": "Dianne Cook",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMASS, granova, ggplot2, vcd, knitr, HH\nCRAN Task Views implied by cited packages\nGraphics, Multivariate, SocialSciences, ClinicalTrials, Distributions, Econometrics, Environmetrics, ExperimentalDesign, NumericalMathematics, Pharmacokinetics, Phylogenetics, Psychometrics, ReproducibleResearch, Robust\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-013/",
    "title": "Estimating Spatial Probit Models in R",
    "description": "In this article we present the Bayesian estimation of spatial probit models in R and provide an implementation in the package spatialprobit. We show that large probit models can be estimated with sparse matrix representations and Gibbs sampling of a truncated multivariate normal distribution with the precision matrix. We present three examples and point to ways to achieve further performance gains through parallelization of the Markov Chain Monte Carlo approach.",
    "author": [
      {
        "name": "Stefan Wilhelm",
        "url": {}
      },
      {
        "name": "Miguel Godinho de Matos",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nspBayes, spatial, geoR, sgeostat, spdep, sphet, sna, network, Matrix, sparseM, spatialprobit, McSpatial, LearnBayes, tmvtnorm, mvtnorm, igraph\nCRAN Task Views implied by cited packages\nSpatial, Bayesian, Distributions, Econometrics, SocialSciences, gR, Multivariate, Optimization, SpatioTemporal, Finance, Graphics, NumericalMathematics, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-014/",
    "title": "ggmap: Spatial Visualization with ggplot2",
    "description": "In spatial statistics the ability to visualize data and models superimposed with their basic social landmarks and geographic context is invaluable. ggmap is a new tool which enables such visualization by combining the spatial information of static maps from Google Maps, OpenStreetMap, Stamen Maps or CloudMade Maps with the layered grammar of graphics implementation of ggplot2. In addition, several new utility functions are introduced which allow the user to access the Google Geocoding, Distance Matrix, and Directions APIs. The result is an easy, consistent and modular framework for spatial graphics with several convenient tools for spatial data analysis.",
    "author": [
      {
        "name": "David Kahle",
        "url": {}
      },
      {
        "name": "Hadley Wickham",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsp, RgoogleMaps, ggplot2, ggmap, maps, maptools, DeducerSpatial, plyr, rjson, osmar\nCRAN Task Views implied by cited packages\nSpatial, WebTechnologies, Graphics, Phylogenetics, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-015/",
    "title": "mpoly: Multivariate Polynomials in R",
    "description": "The mpoly package is a general purpose collection of tools for symbolic computing with multivariate polynomials in R. In addition to basic arithmetic, mpoly can take derivatives of polyno mials, compute Gröbner bases of collections of polynomials, and convert polynomials into a functional form to be evaluated. Among other things, it is hoped that mpoly will provide an R-based foundation for the computational needs of algebraic statisticians.",
    "author": [
      {
        "name": "David Kahle",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nmpoly, multipol, polynom, PolynomF, rSymPy\nCRAN Task Views implied by cited packages\nNumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-016/",
    "title": "beadarrayFilter: An R Package to Filter Beads",
    "description": "Microarrays enable the expression levels of thousands of genes to be measured simultane ously. However, only a small fraction of these genes are expected to be expressed under different experimental conditions. Nowadays, filtering has been introduced as a step in the microarray pre processing pipeline. Gene filtering aims at reducing the dimensionality of data by filtering redundant features prior to the actual statistical analysis. Previous filtering methods focus on the Affymetrix platform and can not be easily ported to the Illumina platform. As such, we developed a filtering method for Illumina bead arrays. We developed an R package, beadarrayFilter, to implement the latter method. In this paper, the main functions in the package are highlighted and using many examples, we illustrate how beadarrayFilter can be used to filter bead arrays.",
    "author": [
      {
        "name": "Anyiawung Chiara Forcheh",
        "url": {}
      },
      {
        "name": "Geert Verbeke",
        "url": {}
      },
      {
        "name": "Adetayo Kasim",
        "url": {}
      },
      {
        "name": "Dan Lin",
        "url": {}
      },
      {
        "name": "Ziv Shkedy",
        "url": {}
      },
      {
        "name": "Willem Talloen",
        "url": {}
      },
      {
        "name": "Hinrich W.H. Göhlmann",
        "url": {}
      },
      {
        "name": "Lieven Clement",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nbeadarrayFilter\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-017/",
    "title": "Fast Pure R Implementation of GEE: Application of the Matrix Package",
    "description": "Generalized estimating equation solvers in R only allow for a few pre-determined options for the link and variance functions. We provide a package, geeM, which is implemented entirely in R and allows for user specified link and variance functions. The sparse matrix representations provided in the Matrix package enable a fast implementation. To gain speed, we make use of analytic inverses of the working correlation when possible and a trick to find quick numeric inverses when an analytic inverse is not available. Through three examples, we demonstrate the speed of geeM, which is not much worse than C implementations like geepack and gee on small data sets and faster on large data sets.",
    "author": [
      {
        "name": "Lee S. McDaniel",
        "url": {}
      },
      {
        "name": "Nicholas C. Henderson",
        "url": {}
      },
      {
        "name": "Paul J. Rathouz",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngeepack, gee, geeM, Matrix\nCRAN Task Views implied by cited packages\nEconometrics, SocialSciences, Multivariate, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-018/",
    "title": "RcmdrPlugin.temis, a Graphical Integrated Text Mining Solution in R",
    "description": "We present the package RcmdrPlugin.temis, a graphical user interface for user-friendly text mining in R. Built as a plug-in to the R Commander provided by the Rcmdr package, it brings together several existing packages and provides new features streamlining the process of importing, managing and analyzing a corpus, in addition to saving results and plots to a report file. Beyond common file formats, automated import of corpora from the Dow Jones Factiva content provider and Twitter is supported. Featured analyses include vocabulary and dissimilarity tables, terms frequencies, terms specific of levels of a variable, term co-occurrences, time series, correspondence analysis and hierarchical clustering.",
    "author": [
      {
        "name": "Milan Bouchet-Valat",
        "url": {}
      },
      {
        "name": "Gilles Bastin",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntm, RcmdrPlugin.temis, Rcmdr, RODBC, tm.plugin.factiva, twitteR, SnowballC, zoo, lattice, ca, R2HTML\nCRAN Task Views implied by cited packages\nNaturalLanguageProcessing, Finance, Multivariate, Econometrics, Environmetrics, Graphics, HighPerformanceComputing, Pharmacokinetics, Psychometrics, ReproducibleResearch, TimeSeries, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2013-019/",
    "title": "Possible Directions for Improving Dependency Versioning in R",
    "description": "One of the most powerful features of R is its infrastructure for contributed code. The built-in package manager and complementary repositories provide a great system for development and exchange of code, and have played an important role in the growth of the platform towards the de-facto standard in statistical computing that it is today. However, the number of packages on CRAN and other repositories has increased beyond what might have been foreseen, and is revealing some limitations of the current design. One such problem is the general lack of dependency versioning in the infrastructure. This paper explores this problem in greater detail, and suggests approaches taken by other open source communities that might work for R as well. Three use cases are defined that exemplify the issue, and illustrate how improving this aspect of package management could increase reliability while supporting further growth of the R community.",
    "author": [
      {
        "name": "Jeroen Ooms",
        "url": {}
      }
    ],
    "date": "2013-06-03",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nCRAN\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-010/",
    "title": "frailtyHL: A Package for Fitting Frailty Models with H-likelihood",
    "description": "We present the frailtyHL package for fitting semi-parametric frailty models using h likelihood. This package allows lognormal or gamma frailties for random-effect distribution, and it fits shared or multilevel frailty models for correlated survival data. Functions are provided to format and summarize the frailtyHL results. The estimates of fixed effects and frailty parameters and their standard errors are calculated. We illustrate the use of our package with three well-known data sets and compare our results with various alternative R-procedures.",
    "author": [
      {
        "name": "Il Do Ha",
        "url": {}
      },
      {
        "name": "Maengseok Noh",
        "url": {}
      },
      {
        "name": "Youngjo Lee",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nfrailtyHL, survival, coxme, phmm, frailtypack, hglm, HGLMMM, dhglm\nCRAN Task Views implied by cited packages\nSurvival, ClinicalTrials, Econometrics, SocialSciences\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-011/",
    "title": "influence.ME: Tools for Detecting Influential Data in Mixed Effects Models",
    "description": "influence.ME provides tools for detecting influential data in mixed effects models. The application of these models has become common practice, but the development of diagnostic tools has lagged behind. influence.ME calculates standardized measures of influential data for the point estimates of generalized mixed effects models, such as DFBETAS, Cook’s distance, as well as percentile change and a test for changing levels of significance. influence.ME calculates these measures of influence while accounting for the nesting structure of the data. The package and measures of influential data are introduced, a practical example is given, and strategies for dealing with influential data are suggested. The application of mixed effects regression models has become common practice in the field of social sciences. As used in the social sciences, mixed effects regression models take into account that observations on individual respondents are nested within higher-level groups such as schools, classrooms, states, and countries (Snijders and Bosker, 1999), and are often referred to as multilevel regression models. Despite these models’ increasing popularity, diagnostic tools to evaluate fitted models lag behind. We introduce influence.ME (Nieuwenhuis, Pelzer, and te Grotenhuis, 2012), an R-package that provides tools for detecting influential cases in mixed effects regression models estimated with lme4 (Bates and Maechler, 2010). It is commonly accepted that tests for influential data should be performed on regression models, especially when estimates are based on a relatively small number of cases. However, most existing procedures do not account for the nesting structure of the data. As a result, these existing procedures fail to detect that higher-level cases may be influential on estimates of variables measured at specifically that level. In this paper, we outline the basic rationale on detecting influential data, describe standardized measures of influence, provide a practical example of the analysis of students in 23 schools, and discuss strategies for dealing with influential cases. Testing for influential cases in mixed effects regression models is important, because influential data negatively influence the statistical fit and generalizability of the model. In social science applications of mixed models the testing for influential data is especially important, since these models are frequently based on large numbers of observations at the individual level while the number of higher level groups is relatively small. For instance, Van der Meer, te Grotenhuis, and Pelzer (2010) were unable to find any country-level comparative studies involving more than 54 countries. With such a relatively low number of countries, a single country can easily be overly influential on the parameter estimates of one or more of the country-level variables.",
    "author": [
      {
        "name": "Rense Nieuwenhuis",
        "url": {}
      },
      {
        "name": "Manfred te Grotenhuis",
        "url": {}
      },
      {
        "name": "Ben Pelzer",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-012/",
    "title": "The crs Package: Nonparametric Regression Splines for Continuous and Categorical Predictors",
    "description": "A new package crs is introduced for computing nonparametric regression (and quantile) splines in the presence of both continuous and categorical predictors. B-splines are employed in the regression model for the continuous predictors and kernel weighting is employed for the categorical predictors. We also develop a simple R interface to NOMAD, which is a mixed integer optimization solver used to compute optimal regression spline solutions.",
    "author": [
      {
        "name": "Zhenghua Nie",
        "url": {}
      },
      {
        "name": "Jeffrey S. Racine",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncrs, SemiPar, mgcv, gss, gam, MASS, rgl\nCRAN Task Views implied by cited packages\nSocialSciences, Econometrics, Environmetrics, Multivariate, Bayesian, Distributions, Graphics, NumericalMathematics, Optimization, Pharmacokinetics, Psychometrics, Robust, SpatioTemporal, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-013/",
    "title": "Debugging grid Graphics",
    "description": "A graphical scene that has been produced using the grid graphics package consists of grobs (graphical objects) and viewports. This article describes functions that allow the exploration and inspection of the grobs and viewports in a grid scene, including several functions that are available in a new package called gridDebug. The ability to explore the grobs and viewports in a grid scene is useful for adding more drawing to a scene that was produced using grid and for understanding and debugging the grid code that produced a scene.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      },
      {
        "name": "Velvet Ly",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nggplot2, gridDebug, graph, Rgraphviz, gridGraphviz, gridSVG, playwith\nCRAN Task Views implied by cited packages\nGraphics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-014/",
    "title": "Rfit: Rank-based Estimation for Linear Models",
    "description": "In the nineteen seventies, Jurečková and Jaeckel proposed rank estimation for linear models. Since that time, several authors have developed inference and diagnostic methods for these estimators. These rank-based estimators and their associated inference are highly efficient and are robust to outliers in response space. The methods include estimation of standard errors, tests of general linear hypotheses, confidence intervals, diagnostic procedures including studentized residuals, and measures of influential cases. We have developed an R package, Rfit, for computing of these robust procedures. In this paper we highlight the main features of the package. The package uses standard linear model syntax and includes many of the main inference and diagnostic functions.",
    "author": [
      {
        "name": "John D. Kloke",
        "url": {}
      },
      {
        "name": "Joseph W. McKean",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRfit, Rfit, MASS, quantreg\nCRAN Task Views implied by cited packages\nEconometrics, Environmetrics, Robust, SocialSciences, Distributions, Multivariate, NumericalMathematics, Optimization, Pharmacokinetics, Psychometrics, ReproducibleResearch, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-015/",
    "title": "Graphical Markov Models with Mixed Graphs in R",
    "description": "In this paper we provide a short tutorial illustrating the new functions in the package ggm that deal with ancestral, summary and ribbonless graphs. These are mixed graphs (containing three types of edges) that are important because they capture the modified independence structure after marginalisation over, and conditioning on, nodes of directed acyclic graphs. We provide functions to verify whether a mixed graph implies that A is independent of B given C for any disjoint sets of nodes and to generate maximal graphs inducing the same independence structure of non-maximal graphs. Finally, we provide functions to decide on the Markov equivalence of two graphs with the same node set but different types of edges.",
    "author": [
      {
        "name": "Kayvan Sadeghi",
        "url": {}
      },
      {
        "name": "Giovanni M. Marchetti",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngRain, ggm, ggm, igraph, gRbase\nCRAN Task Views implied by cited packages\ngR, Graphics, Optimization, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-016/",
    "title": "What's in a Name?",
    "description": "Any shape that is drawn using the grid graphics package can have a name associated with it. If a name is provided, it is possible to access, query, and modify the shape after it has been drawn. These facilities allow for very detailed customisations of plots and also for very general transformations of plots that are drawn by packages based on grid.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-017/",
    "title": "It's Not What You Draw,It's What You Don't Draw",
    "description": "The R graphics engine has new support for drawing complex paths via the functions polypath() and grid.path(). This article explains what is meant by a complex path and demonstrates the usefulness of complex paths in drawing non-trivial shapes, logos, customised data symbols, and maps. One of the design goals of the R graphics system is to allow fine control over the small details of plots. One way that the R graphics system does this is by providing access to low-level generic graphics facilities, such as the ability to draw basic shapes and the ability to control apparently esoteric, but still useful, features of those shapes, such as the line end style used for drawing lines. In R version 2.12.0, another low-level graphics facility was added to R: the ability to draw complex paths (not just polygons). This article describes this new facility and presents some examples that show how complex paths might be useful.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngrImport, maptools, maps\nCRAN Task Views implied by cited packages\nSpatial\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-018/",
    "title": "The State of Naming Conventions in R",
    "description": "Most programming language communities have naming conventions that are generally agreed upon, that is, a set of rules that governs how functions and variables are named. This is not the case with R, and a review of unofficial style guides and naming convention usage on CRAN shows that a number of different naming conventions are currently in use. Some naming conventions are, however, more popular than others and as a newcomer to the R community or as a developer of a new package this could be useful to consider when choosing what naming convention to adopt.",
    "author": [
      {
        "name": "Rasmus Bååth",
        "url": {}
      }
    ],
    "date": "2012-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-001/",
    "title": "Analysing Seasonal Data",
    "description": "Many common diseases, such as the flu and cardiovascular disease, increase markedly in winter and dip in summer. These seasonal patterns have been part of life for millennia and were first noted in ancient Greece by both Hippocrates and Herodotus. Recent interest has focused on climate change, and the concern that seasons will become more extreme with harsher winter and summer weather. We describe a set of R functions designed to model seasonal patterns in disease. We illustrate some simple descriptive and graphical methods, a more complex method that is able to model non-stationary patterns, and the case-crossover to control for seasonal confounding. In this paper we illustrate some of the functions of the season package (Barnett et al., 2012), which contains a range of functions for analysing seasonal health data. We were motivated by the great interest in seasonality found in the health literature, and the relatively small number of seasonal tools in R (or other software packages). The existing seasonal tools in R are: • the baysea function of the timsac package and the decompose and stl functions of the stats package for decomposing a time series into a trend and season; • the dynlm function of the dynlm package and the ssm function of the sspir package for fitting dynamic linear models with optional seasonal components; • the arima function of the stats package and the Arima function of the forecast package for fitting seasonal components as part of an autoregressive integrated moving average (ARIMA) model; and • the bfast package for detecting breaks in a seasonal pattern. These tools are all useful, but most concern decomposing equally spaced time series data. Our package includes models that can be applied to seasonal patterns in unequally spaced data. Such data are common in observational studies when the timing of responses cannot be controlled (e.g. for a postal survey). In the health literature much of the analysis of seasonal data uses simple methods such as com paring rates of disease by month or using a cosinor regression model, which assumes a sinusoidal seasonal pattern. We have created functions for these simple, but often very effective analyses, as we describe below. More complex seasonal analyses examine non-stationary seasonal patterns that change over time. Changing seasonal patterns in health are currently of great interest as global warming is predicted to make seasonal changes in the weather more extreme. Hence there is a need for statistical tools that can estimate whether a seasonal pattern has become more extreme over time or whether its phase has changed. Ours is also the first R package that includes the case-crossover, a useful method for controlling for seasonality. This paper illustrates just some of the functions of the season package. We show some descriptive functions that give simple means or plots, and functions whose goal is inference based on generalised linear models. The package was written as a companion to a book on seasonal analysis by Barnett and Dobson (2010), which contains further details on the statistical methods and R code.",
    "author": [
      {
        "name": "Adrian G Barnett",
        "url": {}
      },
      {
        "name": "Peter Baker",
        "url": {}
      },
      {
        "name": "Annette J Dobson",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nseason, timsac, dynlm, sspir, forecast, bfast\nCRAN Task Views implied by cited packages\nTimeSeries, Finance, Econometrics, Environmetrics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-002/",
    "title": "MARSS: Multivariate Autoregressive State-space Models for Analyzing Time-series Data",
    "description": "MARSS is a package for fitting multivariate autoregressive state-space models to time-series data. The MARSS package implements state-space models in a maximum likelihood framework. The core functionality of MARSS is based on likelihood maximization using the Kalman filter/smoother, combined with an EM algorithm. To make comparisons with other packages available, parameter estimation is also permitted via direct search routines available in ’optim’. The MARSS package allows data to contain missing values and allows a wide variety of model structures and constraints to be specified (such as fixed or shared parameters). In addition to model-fitting, the package provides bootstrap routines for simulating data and generating confidence intervals, and multiple options for calculating model selection criteria (such as AIC).",
    "author": [
      {
        "name": "Elizabeth E. Holmes",
        "url": {}
      },
      {
        "name": "Eric J. Ward",
        "url": {}
      },
      {
        "name": "Kellie Wills",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nMARSS, sspir, dlm, dse, KFAS, FKF\nCRAN Task Views implied by cited packages\nTimeSeries, Finance, Bayesian, Environmetrics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-003/",
    "title": "openair - Data Analysis Tools for the Air Quality Community",
    "description": "The openair package contains data analysis tools for the air quality community. This paper provides an overview of data importers, main functions, and selected utilities and workhorse functions within the package and the function output class, as of package version 0.4-14. It is intended as an explanation of the rationale for the package and a technical description for those wishing to work more interactively with the main functions or develop additional functions to support ‘higher level’ use of openair and R. Large volumes of air quality data are routinely collected for regulatory purposes, but few of those in local authorities and government bodies tasked with this responsibility have the time, expertise or funds to comprehensively analyse this potential resource (Chow and Watson, 2008). Furthermore, few of these institutions can routinely access the more powerful statistical methods typically required to make the most effective use of such data without a suite of often expensive and niche-application proprietary software products. This in turn places large cost and time burdens on both these institutions and others (e.g. academic or commercial) wishing to contribute to this work. In addition, such collaborative working practices can also become highly restricted and polarised if data analysis undertaken by one partner cannot be validated or replicated by another because they lack access to the same licensed products. Being freely distributed under general licence, R has the obvious potential to act as a common platform for those routinely collecting and archiving data and the wider air quality community. This potential has already been proven in several other research areas, and commonly cited ex amples include the Bioconductor project (Gentleman et al, 2004) and the Epitools collaboration (http://www.medepi.com/epitools). However, what is perhaps most inspiring is the degree of trans parency that has been demonstrated by the recent public analysis of climate change data in R and as sociated open debate (http://chartsgraphs.wordpress.com/category/r-climate-data-analysis tool/). Anyone affected by a policy decision, could potentially have unlimited access to scrutinise both the tools and data used to shape that decision.",
    "author": [
      {
        "name": "Karl Ropkins",
        "url": {}
      },
      {
        "name": "David C. Carslaw",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nopenair, openair, lattice, latticeExtra, hexbin, grDevices, mgcv, stats, grDevices, RColorBrewer\nCRAN Task Views implied by cited packages\nGraphics, Environmetrics, SpatioTemporal, Bayesian, Econometrics, Multivariate, Pharmacokinetics, SocialSciences, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-004/",
    "title": "Foreign Library Interface",
    "description": "We present an improved Foreign Function Interface (FFI) for R to call arbitary native functions without the need for C wrapper code. Further we discuss a dynamic linkage framework for binding standard C libraries to R across platforms using a universal type information format. The package rdyncall comprises the framework and an initial repository of cross-platform bindings for standard libraries such as (legacy and modern) OpenGL, the family of SDL libraries and Expat. The package enables system-level programming using the R language; sample applications are given in the article. We outline the underlying automation tool-chain that extracts cross-platform bindings from C headers, making the repository extendable and open for library developers.",
    "author": [
      {
        "name": "Daniel Adler",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrdyncall, Rffi\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-005/",
    "title": "Vdgraph: A Package for Creating Variance Dispersion Graphs",
    "description": "This article introduces the package Vdgraph that is used for making variance dispersion graphs of response surface designs. The package includes functions that make the variance dispersion graph of one design or compare variance dispersion graphs of two designs, which are stored in data frames or matrices. The package also contains several minimum run response surface designs (stored as matrices) that are not available in other R packages.",
    "author": [
      {
        "name": "John Lawson",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nVdgraph, rsm\nCRAN Task Views implied by cited packages\nExperimentalDesign\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-006/",
    "title": "xgrid and R: Parallel Distributed Processing Using Heterogeneous Groups of Apple Computers",
    "description": "The Apple Xgrid system provides access to groups (or grids) of computers that can be used to facilitate parallel processing. We describe the xgrid package which facilitates access to this system to undertake independent simulations or other long-running jobs that can be divided into replicate runs within R. Detailed examples are provided to demonstrate the interface, along with results from a simulation study of the performance gains using a variety of grids. Use of the grid for “embarassingly parallel” independent jobs has the potential for major speedups in time to completion. Appendices provide guidance on setting up the workflow, utilizing add-on packages, and constructing grids using existing machines.",
    "author": [
      {
        "name": "Sarah C. Anoke",
        "url": {}
      },
      {
        "name": "Yuting Zhao",
        "url": {}
      },
      {
        "name": "Rafael Jaeger",
        "url": {}
      },
      {
        "name": "Nicholas J. Horton",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nGridR, Rmpi, snow, multicore, xgrid, runjags, poLCA\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, Bayesian, Cluster, Multivariate, Psychometrics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-007/",
    "title": "maxent: An R Package for Low-memory Multinomial Logistic Regression with Support for Semi-automated Text Classification",
    "description": "maxent is a package with tools for data classification using multinomial logistic regression, also known as maximum entropy. The focus of this maximum entropy classifier is to minimize memory consumption on very large datasets, particularly sparse document-term matrices represented by the tm text mining package.",
    "author": [
      {
        "name": "Timothy P. Jurka",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nnnet, mlogit, maxent, Rcpp, tm, Matrix, slam, SparseM\nCRAN Task Views implied by cited packages\nEconometrics, NumericalMathematics, HighPerformanceComputing, Multivariate, NaturalLanguageProcessing, SocialSciences, MachineLearning\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-008/",
    "title": "Sumo: An Authenticating Web Application with an Embedded R Session",
    "description": "Sumo is a web application intended as a template for developers. It is distributed as a Java ‘war’ file that deploys automatically when placed in a Servlet container’s ‘webapps’ directory. If a user supplies proper credentials, Sumo creates a session-specific Secure Shell connection to the host and a user-specific R session over that connection. Developers may write dynamic server pages that make use of the persistent R session and user-specific file space. The supplied example plots a data set conditional on preferences indicated by the user; it also displays some static text. A companion server page allows the user to interact directly with the R session. Sumo’s novel feature set complements previous efforts to supply R functionality over the internet.",
    "author": [
      {
        "name": "Timothy T. Bergsma",
        "url": {}
      },
      {
        "name": "Michael S. Smith",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRpad, brew, R.rsp, Rook, Rserve, R2HTML\nCRAN Task Views implied by cited packages\nReproducibleResearch, WebTechnologies, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2012-009/",
    "title": "Who Did What? The Roles of R Package Authors and How to Refer to Them",
    "description": "Computational infrastructure for representing persons and citations has been available in R for several years, but has been restructured through enhanced classes \"person\" and \"bibentry\" in recent versions of R. The new features include support for the specification of the roles of package authors (e.g. maintainer, author, contributor, translator, etc.) and more flexible formatting/printing tools among various other improvements. Here, we introduce the new classes and their methods and indicate how this functionality is employed in the management of R packages. Specifically, we show how the authors of R packages can be specified along with their roles in package ‘DESCRIPTION’ and/or ‘CITATION’ files and the citations produced from it. R packages are the result of scholarly activity and as such constitute scholarly resources which must be clearly identifiable for the respective scientific communities and, more generally, today’s information society. In particular, packages published by standard repositories can be regarded as reliable sources which can and should be referenced (i.e. cited) by scientific works such as articles or other packages. This requires conceptual frameworks and computational infrastructure for describing bibliographic resources, general enough to encompass the needs of communities with an interest in R. These needs include support for exporting bibliographic metadata in standardized formats such as BIBTEX (Berry and Patashnik, 2010), but also facilitating bibliometric analyses and investigations of the social fabric underlying the creation of scholarly knowledge. The latter requires a richer vocabulary than commonly employed by reference management software such as BIBTEX, identifying persons and their roles in relation to bibliographic resources. For example, a thesis typically has an author and advisors. Software can have an (original) author and a translator to another language (such as from S to R). The maintainer of an R package is not necessarily an author. In this paper, we introduce the base R infrastructure (as completely available in R since version 2.14.0) for representing and manipulating such scholarly data: objects of class \"person\" (hereafter, per son objects) hold information about persons, possibly including their roles; objects of class \"bibentry\" (hereafter, bibentry objects) hold bibliographic information in enhanced BIBTEX style, ideally using person objects when referring to persons (such as authors or editors). Furthermore, we indicate how this functionality is employed in the management of R packages, in particular in their ‘CITATION’ and ‘DESCRIPTION’ files.",
    "author": [
      {
        "name": "Kurt Hornik",
        "url": {}
      },
      {
        "name": "Duncan Murdoch",
        "url": {}
      },
      {
        "name": "Achim Zeileis",
        "url": {}
      }
    ],
    "date": "2012-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nboot, bibtex, XML\nCRAN Task Views implied by cited packages\nEconometrics, Optimization, ReproducibleResearch, SocialSciences, Survival, TimeSeries, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-011/",
    "title": "Creating and Deploying an Application with (R)Excel and R",
    "description": "We present some ways of using R in Excel and build an example application using the package rpart. Starting with simple interactive use of rpart in Excel, we eventually package the code into an Excel-based application, hiding all details (including R itself) from the end user. In the end, our application implements a service-oriented architecture (SOA) with a clean separation of presentation and computation layer.",
    "author": [
      {
        "name": "Thomas Baier",
        "url": {}
      },
      {
        "name": "Erich Neuwirth",
        "url": {}
      },
      {
        "name": "Michele De Meo",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrpart\nCRAN Task Views implied by cited packages\nEnvironmetrics, MachineLearning, Multivariate, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-012/",
    "title": "glm2: Fitting Generalized Linear Models with Convergence Problems",
    "description": "The R function glm uses step-halving to deal with certain types of convergence problems when using iteratively reweighted least squares to fit a generalized linear model. This works well in some circumstances but non-convergence remains a possibility, particularly with a non-standard link function. In some cases this is because step-halving is never invoked, despite a lack of convergence. In other cases step-halving is invoked but is unable to induce convergence. One remedy is to impose a stricter form of step-halving than is currently available in glm, so that the deviance is forced to decrease in every iteration. This has been implemented in the glm2 function available in the glm2 package. Aside from a modified computational algorithm, glm2 operates in exactly the same way as glm and provides improved convergence properties. These improvements are illustrated here with an identity link Poisson model, but are also relevant in other contexts.",
    "author": [
      {
        "name": "Ian C. Marschner",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nglm2\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-013/",
    "title": "Implementing the Compendium Concept with Sweave and DOCSTRIP",
    "description": "This article suggests an implementation of the compendium concept by combining Sweave and the LATEX literate programming environment DOCSTRIP.",
    "author": [
      {
        "name": "Michael Lundholm",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-014/",
    "title": "Watch Your Spelling!",
    "description": "We discuss the facilities in base R for spell checking via Aspell, Hunspell or Ispell, which are useful in particular for conveniently checking the spelling of natural language texts in package Rd files and vignettes. Spell checking performance is illustrated using the Rd files in package stats. This example clearly indicates the need for a domain-specific statistical dictionary. We analyze the results of spell checking all Rd files in all CRAN packages and show how these can be employed for building such a dictionary.",
    "author": [
      {
        "name": "Kurt Hornik",
        "url": {}
      },
      {
        "name": "Duncan Murdoch",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-015/",
    "title": "Ckmeans.1d.dp: Optimal k-means Clustering in One Dimension by Dynamic Programming",
    "description": "The heuristic k-means algorithm, widely used for cluster analysis, does not guarantee optimality. We developed a dynamic programming algorithm for optimal one-dimensional clustering. The algorithm is implemented as an R package called Ckmeans.1d.dp. We demonstrate its advantage in optimality and runtime over the standard iterative k-means algorithm.",
    "author": [
      {
        "name": "Haizhou Wang",
        "url": {}
      },
      {
        "name": "Mingzhou Song",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nCkmeans.1d.dp\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-016/",
    "title": "Nonparametric Goodness-of-Fit Tests for Discrete Null Distributions",
    "description": "Methodology extending nonparametric goodness-of-fit tests to discrete null distributions has existed for several decades. However, modern statistical software has generally failed to provide this methodology to users. We offer a revision of R’s ks.test() function and a new cvm.test() function that fill this need in the R language for two of the most popular nonparametric goodness-of-fit tests. This paper describes these contributions and provides examples of their usage. Particular attention is given to various numerical issues that arise in their implementation.",
    "author": [
      {
        "name": "Taylor B. Arnold",
        "url": {}
      },
      {
        "name": "John W. Emerson",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndgof, nortest, ADGofTest, CvM2SL1Test, CvM2SL2Test, cramer\nCRAN Task Views implied by cited packages\nMultivariate\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-017/",
    "title": "Using the Google Visualisation API with R",
    "description": "The googleVis package provides an interface between R and the Google Visualisation API to create interactive charts which can be embedded into web pages. The best known of these charts is probably the Motion Chart, popularised by Hans Rosling in his TED talks. With the googleVis package users can easily create web pages with interactive charts based on R data frames and display them either via the local R HTTP help server or within their own sites.",
    "author": [
      {
        "name": "Markus Gesmann",
        "url": {}
      },
      {
        "name": "Diego de Castillo",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nrsp, googleVis, rjsonio, brew\nCRAN Task Views implied by cited packages\nReproducibleResearch, SpatioTemporal, WebTechnologies\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-018/",
    "title": "GrapheR: a Multiplatform GUI for Drawing Customizable Graphs in R",
    "description": "This article presents GrapheR, a Graphical User Interface allowing the user to draw customiz able and high-quality graphs without knowing any R commands. Six kinds of graph are available: histograms, box-and-whisker plots, bar plots, pie charts, curves and scatter plots. The complete process is described with the examples of a bar plot and a scatter plot illustrating the legendary puzzle of African and European swallows’ migrations.",
    "author": [
      {
        "name": "Maxime Hervé",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nJGR, playwith, GrapheR\nCRAN Task Views implied by cited packages\nGraphics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-019/",
    "title": "rainbow: An R Package for Visualizing Functional Time Series",
    "description": "Recent advances in computer technology have tremendously increased the use of functional data, whose graphical representation can be infinite-dimensional curves, images or shapes. This article describes four methods for visualizing functional time series using an R add-on package. These methods are demonstrated using age-specific Australian fertility data from 1921 to 2006 and monthly sea surface temperatures from January 1950 to December 2006.",
    "author": [
      {
        "name": "Han Lin Shang",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-020/",
    "title": "Portable C++ for R Packages",
    "description": "Package checking errors are more common on Solaris than Linux. In many cases, these errors are due to non-portable C++ code. This article reviews some commonly recurring problems in C++ code found in R packages and suggests solutions.",
    "author": [
      {
        "name": "Martyn Plummer",
        "url": {}
      }
    ],
    "date": "2011-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nRcpp\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, NumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-001/",
    "title": "Rmetrics - timeDate Package",
    "description": "The management of time and holidays can prove crucial in applications that rely on historical data. A typical example is the aggregation of a data set recorded in different time zones and under dif ferent daylight saving time rules. Besides the time zone conversion function, which is well supported by default classes in R, one might need functions to handle special days or holidays. In this respect, the package timeDate enhances default date-time classes in R and brings new functionalities to time zone management and the creation of holiday calendars.",
    "author": [
      {
        "name": "Yohan Chalabi",
        "url": {}
      },
      {
        "name": "Martin Mächler",
        "url": {}
      },
      {
        "name": "Diethelm Würtz",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntimeDate, timeDate, timeDate, timeDate, timeDate, timeDate, timeDate, timeDate, timeDate\nCRAN Task Views implied by cited packages\nFinance, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-002/",
    "title": "testthat: Get Started with Testing",
    "description": "Software testing is important, but many of us don’t do it because it is frustrating and boring. testthat is a new testing framework for R that is easy learn and use, and integrates with your existing workflow. This paper shows how, with illustrations from existing packages.",
    "author": [
      {
        "name": "Hadley Wickham",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntestthat, RUnit, svUnit, stringr, lubridate\nCRAN Task Views implied by cited packages\nReproducibleResearch, TimeSeries\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-003/",
    "title": "Content-Based Social Network Analysis of Mailing Lists",
    "description": "Social Network Analysis (SNA) provides tools to examine relationships between people. Text Mining (TM) allows capturing the text they produce in Web 2.0 applications, for example, however it neglects their social structure. This paper applies an approach to combine the two methods named “content-based SNA”. Using the R mailing lists, R-help and R-devel, we show how this combination can be used to describe people’s interests and to find out if authors who have similar interests actually communicate. We find that the expected positive relationship between sharing interests and communicating gets stronger as the centrality scores of authors in the communication networks increase.",
    "author": [
      {
        "name": "Angela Bohn",
        "url": {}
      },
      {
        "name": "Ingo Feinerer",
        "url": {}
      },
      {
        "name": "Kurt Hornik",
        "url": {}
      },
      {
        "name": "Patrick Mair",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ntm.plugin.mail, car, tm, sna, igraph\nCRAN Task Views implied by cited packages\nNaturalLanguageProcessing, Optimization, SocialSciences, Bayesian, Econometrics, Finance, gR, Graphics, HighPerformanceComputing, Multivariate, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-004/",
    "title": "The digitize Package: Extracting Numerical Data from Scatterplots",
    "description": "I present the small R package digitize, designed to extract data from scatterplots with a simple method and suited to small datasets. I present an application of this method to the extraction of data from a graph whose source is not available.",
    "author": [
      {
        "name": "Timothée Poisot",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndigitize, ReadImages\nCRAN Task Views implied by cited packages\nMetaAnalysis\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-005/",
    "title": "Differential Evolution with DEoptim",
    "description": "The R package DEoptim implements the Differential Evolution algorithm. This algorithm is an evolutionary technique similar to classic genetic algorithms that is useful for the solution of global optimization problems. In this note we provide an introduction to the package and demonstrate its utility for financial applications by solving a non-convex portfolio optimization problem.",
    "author": [
      {
        "name": "David Ardia",
        "url": {}
      },
      {
        "name": "Kris Boudt",
        "url": {}
      },
      {
        "name": "Peter Carl",
        "url": {}
      },
      {
        "name": "Katharine M. Mullen",
        "url": {}
      },
      {
        "name": "Brian G. Peterson",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nDEoptim, PortfolioAnalytics, quantmod, PerformanceAnalytics\nCRAN Task Views implied by cited packages\nFinance, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-006/",
    "title": "rworldmap : a new R package for mapping global data",
    "description": "rworldmap is a new package available on CRAN for mapping and visualisation of global data. The vision is to make the display of global data easier, to facilitate understanding and com munication. The initial concentration is on data referenced by country or grid due to the frequency of use of such data in global assessments. Tools to link data referenced by country (either name or code) to a map, and then to display the map are provided as are functions to map global gridded data. Country and gridded functions accept the same arguments to specify the nature of categories and colour and how legends are formatted. This package builds on the functionality of existing packages, particularly sp, maptools and fields. Example code is provided to produce maps, to link with the packages classInt, RColorBrewer and ncdf, and to plot examples of publicly available country and gridded data.",
    "author": [
      {
        "name": "Andy South",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-007/",
    "title": "Cryptographic Boolean Functions with R",
    "description": "A new package called boolfun is available for R users. The package provides tools to handle Boolean functions, in particular for cryptographic purposes. This document guides the user through some (code) examples and gives a feel of what can be done with the package.",
    "author": [
      {
        "name": "Frédéric Lafitte",
        "url": {}
      },
      {
        "name": "Dirk Van Heule",
        "url": {}
      },
      {
        "name": "Julien Van hamme",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nboolfun, R.oo, multipol\nCRAN Task Views implied by cited packages\nNumericalMathematics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-008/",
    "title": "Raster Images in R Graphics",
    "description": "The R graphics engine has new support for rendering raster images via the functions rasterImage() and grid.raster(). This leads to better scaling of raster images, faster rendering to screen, and smaller graphics files. Several examples of possible applications of these new features are described.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-009/",
    "title": "Probabilistic Weather Forecasting in R",
    "description": "This article describes two R packages for probabilistic weather forecasting, ensembleBMA, which offers ensemble postprocessing via Bayesian model averaging (BMA), and ProbForecastGOP, which implements the geostatistical output perturbation (GOP) method. BMA forecasting models use mixture distributions, in which each component corresponds to an ensemble member, and the form of the component distribution depends on the weather parameter (temperature, quantitative precipitation or wind speed). The model parameters are estimated from training data. The GOP technique uses geostatistical methods to produce probabilistic forecasts of entire weather fields for temperature or pressure, based on a single numerical forecast on a spatial grid. Both packages include functions for evaluating predictive performance, in addition to model fitting and forecasting.",
    "author": [
      {
        "name": "Chris Fraley",
        "url": {}
      },
      {
        "name": "Adrian Raftery",
        "url": {}
      },
      {
        "name": "Tilmann Gneiting",
        "url": {}
      },
      {
        "name": "McLean Sloughter",
        "url": {}
      },
      {
        "name": "Veronica Berrocal",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nensembleBMA, chron, fields, maps, ProbForecastGOP, RandomFields, fields\nCRAN Task Views implied by cited packages\nSpatial, TimeSeries, Bayesian, SpatioTemporal\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2011-010/",
    "title": "Analyzing an Electronic Limit Order Book",
    "description": "The orderbook package provides facilities for exploring and visualizing the data associated with an order book: the electronic collection of the outstanding limit orders for a financial instrument. This article provides an overview of the orderbook package and examples of its use.",
    "author": [
      {
        "name": "David Kane",
        "url": {}
      },
      {
        "name": "Andrew Liu",
        "url": {}
      },
      {
        "name": "Khanh Nguyen",
        "url": {}
      }
    ],
    "date": "2011-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-009/",
    "title": "hglm: A Package for Fitting Hierarchical Generalized Linear Models",
    "description": "We present the hglm package for fitting hierarchical generalized linear models. It can be used for linear mixed models and generalized linear mixed models with random effects for a variety of links and a variety of distributions for both the outcomes and the random effects. Fixed effects can also be fitted in the dispersion part of the model.",
    "author": [
      {
        "name": "Lars Rönnegård",
        "url": {}
      },
      {
        "name": "Xia Shen",
        "url": {}
      },
      {
        "name": "Moudud Alam",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nhglm, lme4, MASS, dglm, HGLMMM, nlme\nCRAN Task Views implied by cited packages\nEconometrics, Environmetrics, Psychometrics, SocialSciences, OfficialStatistics, Pharmacokinetics, SpatioTemporal, Bayesian, ChemPhys, Distributions, Finance, Multivariate, NumericalMathematics, Robust, Spatial\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-010/",
    "title": "Source References",
    "description": "Since version 2.10.0, R includes expanded support for source references in R code and ‘.Rd’ files. This paper describes the origin and purposes of source references, and current and future support for them.",
    "author": [
      {
        "name": "Duncan Murdoch",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-011/",
    "title": "dclone: Data Cloning in R",
    "description": "The dclone R package contains low level functions for implementing maximum likelihood estimating procedures for complex models using data cloning and Bayesian Markov Chain Monte Carlo methods with support for JAGS, WinBUGS and OpenBUGS.",
    "author": [
      {
        "name": "Péter Sólymos",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ndclone, rjags, coda, R2WinBUGS, BRugs\nCRAN Task Views implied by cited packages\ngR, Bayesian, Cluster, HighPerformanceComputing, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-012/",
    "title": "stringr: modern, consistent string processing",
    "description": "String processing is not glamorous, but it is frequently used in data cleaning and preparation. The existing string functions in R are powerful, but not friendly. To remedy this, the stringr package provides string functions that are simpler and more consistent, and also fixes some functionality that R is missing compared to other programming languages.",
    "author": [
      {
        "name": "Hadley Wickham",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-013/",
    "title": "Solving Differential Equations in R",
    "description": "Although R is still predominantly applied for statistical analysis and graphical representation, it is rapidly becoming more suitable for mathematical computing. One of the fields where considerable progress has been made recently is the solution of differential equations. Here we give a brief overview of differential equations that can now be solved by R.",
    "author": [
      {
        "name": "Karline Soetaert",
        "url": {}
      },
      {
        "name": "Thomas Petzoldt",
        "url": {}
      },
      {
        "name": "R. Woodrow Setzer",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nlimSolve, rootSolve, deSolve, bvpSolve, ReacTran, PBSddesolve, sde, pomp, bvpSolve, ReacTran, deSolve, deSolve, ReacTran, deSolve, odesolve, odesolve, nlmeODE, FME, ccems, ReacTran\nCRAN Task Views implied by cited packages\nDifferentialEquations, Pharmacokinetics, TimeSeries, Bayesian, Finance, Optimization\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-014/",
    "title": "Bayesian Estimation of the GARCH(1,1) Model with Student-t Innovations",
    "description": "This note presents the R package bayesGARCH which provides functions for the Bayesian estimation of the parsimonious and effective GARCH(1,1) model with Student-t innovations. The estimation procedure is fully automatic and thus avoids the tedious task of tuning an MCMC sampling algorithm. The usage of the package is shown in an empirical application to exchange rate log-returns.",
    "author": [
      {
        "name": "David Ardia",
        "url": {}
      },
      {
        "name": "Lennart F. Hoogerheide",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nfGarch, rgarch, tseries, bayesGARCH, coda, foreach\nCRAN Task Views implied by cited packages\nFinance, Bayesian, TimeSeries, Econometrics, Environmetrics, gR, HighPerformanceComputing\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-015/",
    "title": "cudaBayesreg: Bayesian Computation in CUDA",
    "description": "Graphical processing units are rapidly gaining maturity as powerful general parallel comput ing devices. The package cudaBayesreg uses GPU–oriented procedures to improve the performance of Bayesian computations. The paper motivates the need for devising high-performance computing strategies in the context of fMRI data analysis. Some features of the package for Bayesian analysis of brain fMRI data are illustrated. Comparative computing performance figures between sequential and parallel implementations are presented as well.",
    "author": [
      {
        "name": "Adelino Ferreira da Silva",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncudaBayesreg, bayesm, cudaBayesregData, oro.nifti\nCRAN Task Views implied by cited packages\nMedicalImaging, Bayesian, HighPerformanceComputing, Cluster, Distributions, Econometrics, Multivariate\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-016/",
    "title": "binGroup: A Package for Group Testing",
    "description": "When the prevalence of a disease or of some other binary characteristic is small, group testing (also known as pooled testing) is frequently used to estimate the prevalence and/or to identify individuals as positive or negative. We have developed the binGroup package as the first package designed to address the estimation problem in group testing. We present functions to estimate an overall prevalence for a homogeneous population. Also, for this setting, we have functions to aid in the very important choice of the group size. When individuals come from a heterogeneous population, our group testing regression functions can be used to estimate an individual probability of disease positivity by using the group observations only. We illustrate our functions with data from a multiple vector transfer design experiment and a human infectious disease prevalence study.",
    "author": [
      {
        "name": "Christopher R. Bilder",
        "url": {}
      },
      {
        "name": "Boan Zhang",
        "url": {}
      },
      {
        "name": "Frank Schaarschmidt",
        "url": {}
      },
      {
        "name": "Joshua M. Tebbs",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nbinGroup, binom\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-017/",
    "title": "The RecordLinkage Package: Detecting Errors in Data",
    "description": "Record linkage deals with detecting homonyms and mainly synonyms in data. The package RecordLinkage provides means to perform and evaluate different record linkage methods. A stochas tic framework is implemented which calculates weights through an EM algorithm. The determination of the necessary thresholds in this model can be achieved by tools of extreme value theory. Further more, machine learning methods are utilized, including decision trees (rpart), bootstrap aggregating (bagging), ada boost (ada), neural nets (nnet) and support vector machines (svm). The generation of record pairs and comparison patterns from single data items are provided as well. Comparison patterns can be chosen to be binary or based on some string metrics. In order to reduce computation time and memory usage, blocking can be used. Future development will concentrate on additional and refined methods, performance improvements and input/output facilities needed for real-world application.",
    "author": [
      {
        "name": "Murat Sariyar",
        "url": {}
      },
      {
        "name": "Andreas Borg",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-018/",
    "title": "spikeslab: Prediction and Variable Selection Using Spike and Slab Regression",
    "description": "Weighted generalized ridge regression offers unique advantages in correlated high-dimensional problems. Such estimators can be efficiently computed using Bayesian spike and slab models and are effective for prediction. For sparse variable selection, a generalization of the elastic net can be used in tandem with these Bayesian estimates. In this article, we describe the R-software package spikeslab for implementing this new spike and slab prediction and variable selection methodology.",
    "author": [
      {
        "name": "Hemant Ishwaran",
        "url": {}
      },
      {
        "name": "Udaya B. Kogalur",
        "url": {}
      },
      {
        "name": "J. Sunil Rao",
        "url": {}
      }
    ],
    "date": "2010-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nlars, snow\nCRAN Task Views implied by cited packages\nHighPerformanceComputing, MachineLearning\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-001/",
    "title": "IsoGene: An R Package for Analyzing Dose-response Studies in Microarray Experiments",
    "description": "IsoGene is an R package for the analysis of dose-response microarray experiments to identify gene or subsets of genes with a monotone relationship between the gene expression and the doses. Several testing procedures (i.e., the likelihood ratio test, Williams, Marcus, the M, and Modified M), that take into account the order restriction of the means with respect to the increasing doses are implemented in the package. The inference is based on resampling methods, both permutations and the Significance Analysis of Microarrays (SAM).",
    "author": [
      {
        "name": "Setia Pramana",
        "url": {}
      },
      {
        "name": "Dan Lin",
        "url": {}
      },
      {
        "name": "Philippe Haldermans",
        "url": {}
      },
      {
        "name": "Ziv Shkedy",
        "url": {}
      },
      {
        "name": "Tobias Verbeke",
        "url": {}
      },
      {
        "name": "Hinrich Göhlmann",
        "url": {}
      },
      {
        "name": "An De Bondt",
        "url": {}
      },
      {
        "name": "Willem Talloen",
        "url": {}
      },
      {
        "name": "Luc Bijnens.",
        "url": {}
      }
    ],
    "date": "2010-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-002/",
    "title": "Online Reproducible Research: An Application to Multivariate Analysis of Bacterial DNA Fingerprint Data",
    "description": "This paper presents an example of online reproducible multivariate data analysis. This example is based on a web page providing an online computing facility on a server. HTML forms contain editable R code snippets that can be executed in any web browser thanks to the Rweb software. The example is based on the multivariate analysis of DNA fingerprints of the internal bacterial flora of the poultry red mite Dermanyssus gallinae. Several multivariate data analysis methods from the ade4 package are used to compare the fingerprints of mite pools coming from various poultry farms. All the computations and graphical displays can be redone interactively and further explored online, using only a web browser. Statistical methods are detailed in the duality diagram framework, and a discussion about online reproducibility is initiated.",
    "author": [
      {
        "name": "Jean Thioulouse",
        "url": {}
      },
      {
        "name": "Claire Valiente-Moro",
        "url": {}
      },
      {
        "name": "Lionel Zenner",
        "url": {}
      }
    ],
    "date": "2010-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nade4, seqinr, ade4, vegan, CGIwithR, R2HTML\nCRAN Task Views implied by cited packages\nEnvironmetrics, Multivariate, Psychometrics, Spatial, Graphics, Genetics, Phylogenetics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-003/",
    "title": "MCMC for Generalized Linear Mixed Models with glmmBUGS",
    "description": "The glmmBUGS package is a bridging tool between Generalized Linear Mixed Models (GLMMs) in R and the BUGS language. It provides a simple way of performing Bayesian inference using Markov Chain Monte Carlo (MCMC) methods, taking a model formula and data frame in R and writing a BUGS model file, data file, and initial values files. Functions are provided to reformat and summarize the BUGS results. A key aim of the package is to provide files and objects that can be modified prior to calling BUGS, giving users a platform for customizing and extending the models to accommodate a wide variety of analyses.",
    "author": [
      {
        "name": "Patrick Brown",
        "url": {}
      },
      {
        "name": "Lutong Zhou",
        "url": {}
      }
    ],
    "date": "2010-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-004/",
    "title": "Mapping and Measuring Country Shapes",
    "description": "The article introduces the cshapes R package, which includes our CShapes dataset of contemporary and historical country boundaries, as well as computational tools for computing geographical measures from these maps. We provide an overview of the need for considering spatial dependence in comparative research, how this requires appropriate historical maps, and detail how the cshapes associated R package cshapes can contribute to these ends. We illustrate the use of the package for drawing maps, computing spatial variables for countries, and generating weights matrices for spatial statistics.",
    "author": [
      {
        "name": "Nils B. Weidmann",
        "url": {}
      },
      {
        "name": "Kristian Skrede Gleditsch",
        "url": {}
      }
    ],
    "date": "2010-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-005/",
    "title": "tmvtnorm: A Package for the Truncated Multivariate Normal Distribution",
    "description": "In this article we present tmvtnorm, an R package implementation for the truncated mul tivariate normal distribution. We consider random number generation with rejection and Gibbs sampling, computation of marginal densities as well as computation of the mean and covariance of the truncated variables. This contribution brings together latest research in this field and provides useful methods for both scholars and practitioners when working with truncated normal variables.",
    "author": [
      {
        "name": "Stefan Wilhelm",
        "url": {}
      },
      {
        "name": "B. G. Manjunath",
        "url": {}
      }
    ],
    "date": "2010-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-006/",
    "title": "neuralnet: Training of Neural Networks",
    "description": "Artificial neural networks are applied in many situations. neuralnet is built to train multilayer perceptrons in the context of regression analyses, i.e. to approximate functional relationships between covariates and response variables. Thus, neural networks are used as extensions of generalized linear models. neuralnet is a very flexible package. The backpropagation algorithm and three versions of resilient backpropagation are implemented and it provides a custom-choice of activation and error function. An arbitrary number of covariates and response variables as well as of hidden layers can theoretically be included. The paper gives a brief introduction to multi-layer perceptrons and resilient backpropagation and demonstrates the application of neuralnet using the data set infert, which is contained in the R distribution.",
    "author": [
      {
        "name": "Frauke Günther",
        "url": {}
      },
      {
        "name": "Stefan Fritsch",
        "url": {}
      }
    ],
    "date": "2010-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-007/",
    "title": "glmperm: A Permutation of Regressor Residuals Test for Inference in Generalized Linear Models",
    "description": "We introduce a new R package called glmperm for inference in generalized linear models especially for small and moderate-sized data sets. The inference is based on the permutation of regressor residuals test introduced by Potter (2005). The implementation of glmperm outperforms currently available permutation test software as glmperm can be applied in situations where more than one covariate is involved.",
    "author": [
      {
        "name": "Wiebke Werft",
        "url": {}
      },
      {
        "name": "Axel Benner",
        "url": {}
      }
    ],
    "date": "2010-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2010-008/",
    "title": "Two-sided Exact Tests and Matching Confidence Intervals for Discrete Data",
    "description": "There is an inherent relationship between two-sided hypothesis tests and confidence intervals. A series of two-sided hypothesis tests may be inverted to obtain the matching 100(1-α)% confidence interval defined as the smallest interval that contains all point null parameter values that would not be rejected at the α level. Unfortunately, for discrete data there are several different ways of defining two-sided exact tests and the most commonly used two-sided exact tests are defined one way, while the most commonly used exact confidence intervals are inversions of tests defined another way. This can lead to inconsistencies where the exact test rejects but the exact confidence interval contains the null parameter value. The packages exactci and exact2x2 provide several exact tests with the matching confidence intervals avoiding these inconsistencies as much as possible. Examples are given for binomial and Poisson parameters and both paired and unpaired 2 × 2 tables. Applied statisticians are increasingly being encouraged to report confidence intervals (CI) and parameter estimates along with p-values from hypothesis tests. The htest class of the stats package is ideally suited to these kinds of analyses, because all the related statistics may be presented when the results are printed. For exact two-sided tests applied to discrete data, a test-CI inconsistency may occur: the p-value may indicate a significant result at level α while the associated 100(1-α)% confidence interval may cover the null value of the parameter. Ideally, we would like to present a unified report (Hirji, 2006), whereby the p-value and the confidence interval match as much as possible.",
    "author": [
      {
        "name": "Michael P. Fay",
        "url": {}
      }
    ],
    "date": "2010-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nexactci, exact2x2, exactci, exact2x2, exactci, exact2x2, PropCIs, rateratio.test, coin, perm\nCRAN Task Views implied by cited packages\nClinicalTrials, Survival\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-011/",
    "title": "ConvergenceConcepts: An R Package to Investigate Various Modes of Convergence",
    "description": "ConvergenceConcepts is an R package, built upon the tkrplot, tcltk and lattice packages, designed to investigate the convergence of simulated sequences of random variables. Four classical modes of convergence may be studied, namely: almost sure convergence (a.s.), convergence in probability (P), convergence in law (L) and convergence in r-th mean (r). This investigation is performed through accurate graphical representations. This package may be used as a pedagogical tool. It may give students a better understanding of these notions and help them to visualize these difficult theoretical concepts. Moreover, some scholars could gain some insight into the behaviour of some random sequences they are interested in.",
    "author": [
      {
        "name": "Pierre Lafaye de Micheaux",
        "url": {}
      },
      {
        "name": "Benoit Liquet",
        "url": {}
      }
    ],
    "date": "2009-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-012/",
    "title": "copas: An R package for Fitting the Copas Selection Model",
    "description": "This article describes the R package copas which is an add-on package to the R package meta. The R package copas can be used to fit the Copas selection model to adjust for bias in meta-analysis. A clinical example is used to illustrate fitting and interpreting the Copas selection model.",
    "author": [
      {
        "name": "J. Carpenter",
        "url": {}
      },
      {
        "name": "G. Rücker",
        "url": {}
      },
      {
        "name": "G. Schwarzer",
        "url": {}
      }
    ],
    "date": "2009-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ncopas, meta\nCRAN Task Views implied by cited packages\nClinicalTrials, MetaAnalysis\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-013/",
    "title": "Party on!",
    "description": "Recursive partitioning methods are amongst the most popular and widely used statistical learning tools for nonparametric regression and classification. Especially random forests, that can deal with large numbers of predictor variables even in the presence of complex interactions, are being applied successfully in many scientific fields (see, e.g., ??, and the references therein for applications in genetics and social sciences). Thus, it is not surprising that there is a variety of recursive partitioning tools available in R (see http://CRAN.R-project.org/view=MachineLearning for an overview).",
    "author": [
      {
        "name": "Carolin Strobl",
        "url": {}
      },
      {
        "name": "Torsten Hothorn",
        "url": {}
      },
      {
        "name": "Achim Zeileis",
        "url": {}
      }
    ],
    "date": "2009-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-014/",
    "title": "Aspects of the Social Organization and Trajectory of the R Project",
    "description": "Based partly on interviews with members of the R Core team, this paper considers the development of the R Project in the context of open-source software development and, more generally, voluntary activities. The paper describes aspects of the social organization of the R Project, including the organization of the R Core team; describes the trajectory of the R Project; seeks to identify factors crucial to the success of R; and speculates about the prospects for R.",
    "author": [
      {
        "name": "John Fox",
        "url": {}
      }
    ],
    "date": "2009-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-015/",
    "title": "asympTest: A Simple R Package for Classical Parametric Statistical Tests and Confidence Intervals in Large Samples",
    "description": "asympTest is an R package implementing large sample tests and confidence intervals. One and two sample mean and variance tests (differences and ratios) are considered. The test statistics are all expressed in the same form as the Student t-test, which facilitates their presentation in the classroom. This contribution also fills the gap of a robust (to non-normality) alternative to the chi square single variance test for large samples, since no such procedure is implemented in standard statistical software.",
    "author": [
      {
        "name": "J.-F. Coeurjolly",
        "url": {}
      },
      {
        "name": "R. Drouilhet",
        "url": {}
      },
      {
        "name": "P. Lafaye de Micheaux",
        "url": {}
      },
      {
        "name": "J.-F. Robineau",
        "url": {}
      }
    ],
    "date": "2009-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nasympTest, asympTest, asympTest\nCRAN Task Views implied by cited packages\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-016/",
    "title": "Rattle: A Data Mining GUI for R",
    "description": "Data mining delivers insights, patterns, and descriptive and predictive models from the large amounts of data available today in many organisations. The data miner draws heavily on methodologies, techniques and algorithms from statistics, machine learning, and computer science. R increasingly provides a powerful platform for data mining. However, scripting and programming is sometimes a challenge for data analysts moving into data mining. The Rattle package provides a graphical user interface specifically for data mining using R. It also provides a stepping stone toward using R as a programming language for data analysis.",
    "author": [
      {
        "name": "Graham J Williams",
        "url": {}
      }
    ],
    "date": "2009-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\narules, RGtk2, RGtk2, rattle, rattle, rattle, rattle, Hmisc, fBasics, mice, rggobi, rggobi, latticist, playwith, lattice, reshape, randomForest, Amelia, rpart, party, rpart, randomForest, ROCR, pmml, rattle, pmml, RGtk2\nCRAN Task Views implied by cited packages\nMachineLearning, Multivariate, Graphics, Environmetrics, OfficialStatistics, SocialSciences, Survival, Bayesian, ClinicalTrials, Distributions, Econometrics, Finance, Pharmacokinetics, ReproducibleResearch\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-017/",
    "title": "sos: Searching Help Pages of R Packages",
    "description": "The sos package provides a means to quickly and flexibly search the help pages of contributed packages, finding functions and datasets in seconds or minutes that could not be found in hours or days by any other means we know. Its findFn function accesses Jonathan Baron’s R Site Search database and returns the matches in a data frame of class \"findFn\", which can be further manipulated by other sos functions to produce, for example, an Excel file that starts with a summary sheet that makes it relatively easy to prioritize alternative packages for further study. As such, it provides a very powerful way to do a literature search for functions and packages relevant to a particular topic of interest and could become virtually mandatory for authors of new packages or papers in publications such as The R Journal and the Journal of Statistical Software.",
    "author": [
      {
        "name": "Spencer Graves",
        "url": {}
      },
      {
        "name": "Sundar Dorai-Raj",
        "url": {}
      },
      {
        "name": "Romain François",
        "url": {}
      }
    ],
    "date": "2009-12-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\nsos, sos, sos, sos, WriteXLS, RODBC, sos, fda, deSolve, PKfit, sos, sos, sos, sos\nCRAN Task Views implied by cited packages\nDifferentialEquations, Pharmacokinetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-018/",
    "title": "Transitioning to R: Replicating SAS, Stata, and SUDAAN Analysis Techniques in Health Policy Data",
    "description": "Statistical, data manipulation, and presentation tools make R an ideal integrated package for research in the fields of health policy and healthcare management and evaluation. However, the technical documentation accompanying most data sets used by researchers in these fields does not include syntax examples for analysts to make the transition from another statistical package to R. This paper describes the steps required to import health policy data into R, to prepare that data for analysis using the two most common complex survey variance calculation techniques, and to produce the principal set of statistical estimates sought by health policy researchers. Using data from the Medical Expenditure Panel Survey Household Component (MEPS-HC), this paper outlines complex survey data analysis techniques in R, with side-by-side comparisons to the SAS, Stata, and SUDAAN statistical software packages.",
    "author": [
      {
        "name": "Anthony Damico",
        "url": {}
      }
    ],
    "date": "2009-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-001/",
    "title": "New Numerical Algorithm for Multivariate Normal Probabilities in Package mvtnorm",
    "description": "? proposed a numerical algorithm for evaluating multivariate normal probabilities. Starting with version 0.9-0 of the mvtnorm package (??), this algorithm is available to the R community. We give a brief introduction to Miwa’s procedure and compare it to a quasi-randomized Monte-Carlo procedure proposed by ?, which has been available through mvtnorm for some years now, both with respect to computing time and accuracy.",
    "author": [
      {
        "name": "Xuefei Mi",
        "url": {}
      },
      {
        "name": "Tetsuhisa Miwa",
        "url": {}
      },
      {
        "name": "Torsten Hothorn",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:28+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-002/",
    "title": "EMD: A Package for Empirical Mode Decomposition and Hilbert Spectrum",
    "description": "The concept of empirical mode decomposition (EMD) and the Hilbert spectrum (HS) has been developed rapidly in many disciplines of science and engineering since Huang et al. (1998) invented EMD. The key feature of EMD is to decompose a signal into so-called intrinsic mode function (IMF). Furthermore, the Hilbert spectral analysis of intrinsic mode functions provides frequency information evolving with time and quantifies the amount of variation due to oscillation at different time scales and time locations. In this article, we introduce an R package called EMD (Kim and Oh, 2008) that performs oneand twodimensional EMD and HS.",
    "author": [
      {
        "name": "Donghoh Kim",
        "url": {}
      },
      {
        "name": "Hee-Seok Oh",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:28+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-003/",
    "title": "AdMit",
    "description": "A package for constructing and using an adaptive mixture of Student-t distributions as a flexible candidate distribution for efficient simulation.",
    "author": [
      {
        "name": "David Ardia",
        "url": {}
      },
      {
        "name": "Lennart F. Hoogerheide",
        "url": {}
      },
      {
        "name": "Herman K. van Dijk",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:28+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-004/",
    "title": "Easier parallel computing in R with snowfall and sfCluster",
    "description": "Many statistical analysis task in areas such as bioinformatics are computationally very intensive, while lots of them rely on embarrasingly parallel computations (Ananth Grama, 2003). Multiple computers or even multiple processor cores on standard desktop computers, which are widespread available nowadays, can easily contribute to faster analyses.",
    "author": [
      {
        "name": "Jochen Knaus",
        "url": {}
      },
      {
        "name": "Christine Porzelius",
        "url": {}
      },
      {
        "name": "Harald Binder",
        "url": {}
      },
      {
        "name": "Guido Schwarzer",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:28+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-005/",
    "title": "expert: Modeling Without Data Using Expert Opinion",
    "description": "The expert package provides tools to create and manipulate empirical statistical models using expert opinion (or judgment). Here, the latter expression refers to a specific body of techniques to elicit the distribution of a random variable when data is scarce or unavailable. Opinions on the quantiles of the distribution are sought from experts in the field and aggregated into a final estimate. The package supports aggregation by means of the Cooke, Mendel–Sheridan and predefined weights models.",
    "author": [
      {
        "name": "Vincent Goulet",
        "url": {}
      },
      {
        "name": "Michel Jacques",
        "url": {}
      },
      {
        "name": "Mathieu Pigeon",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:28+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-006/",
    "title": "Drawing Diagrams with R",
    "description": "R provides a number of well-known high-level facilities for producing sophisticated statistical plots, including the “traditional” plots in the graphics package (R Development Core Team, 2008), the Trellis-style plots provided by lattice (Sarkar, 2008), and the grammar-of-graphics-inspired approach of ggplot2 (Wickham, 2009).",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\nCRAN packages used\ngraphics, lattice, ggplot2\nCRAN Task Views implied by cited packages\nGraphics, Multivariate, Pharmacokinetics, Phylogenetics\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:28+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-007/",
    "title": "Collaborative Software Development Using R-Forge",
    "description": "Open source software (OSS) is typically created in a decentralized self-organizing process by a community of developers having the same or similar interests (see the famous essay by ?). A key factor for the success of OSS over the last two decades is the Internet: Developers who rarely meet face-to-face can employ new means of communication, both for rapidly writing and deploying software (in the spirit of Linus Torvald’s “release early, release often paradigm”). Therefore, many tools emerged that assist a collaborative software development process, including in particular tools for source code management (SCM) and version control.",
    "author": [
      {
        "name": "Stefan Theußl",
        "url": {}
      },
      {
        "name": "Achim Zeileis",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:28+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-008/",
    "title": "Facets of R",
    "description": "We are seeing today a widespread, and welcome, tendency for non-computer-specialists among statisticians and others to write collections of R functions that organize and communicate their work. Along with the flood of software sometimes comes an attitude that one need only learn, or teach, a sort of basic how-to-write-the-function level of R programming, beyond which most of the detail is unimportant or can be absorbed without much discussion. As delusions go, this one is not very objectionable if it encourages participation. Nevertheless, a delusion it is. In fact, functions are only one of a variety of important facets that R has acquired by intent or circumstance during the three-plus decades of the history of the software and of its predecessor S. To create valuable and trustworthy software using R often requires an understanding of some of these facets and their interrelations. This paper identifies six facets, discussing where they came from, how they support or conflict with each other, and what implications they have for the future of programming with R.",
    "author": [
      {
        "name": "John M. Chambers",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-009/",
    "title": "The hwriter package: Composing HTML documents with R objects",
    "description": "HTML documents are structured documents made of diverse elements such as paragraphs, sections, columns, figures and tables organized in a hierarchical layout. Combination of HTML documents and hyperlinking is useful to report analysis results; for example, in the package arrayQualityMetrics, estimating the quality of microarray data sets and cellHTS2, performing the analysis of cell-based screens.",
    "author": [
      {
        "name": "Gregoire Pau",
        "url": {}
      },
      {
        "name": "Wolfgang Huber",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-010/",
    "title": "PMML: An Open Standard for Sharing Models",
    "description": "The PMML package exports a variety of predictive and descriptive models from R to the Predictive Model Markup Language (Data Mining Group, 2008). PMML is an XML-based language and has become the de-facto standard to represent not only predictive and descriptive models, but also data preand post-processing. In so doing, it allows for the interchange of models among different tools and environments, mostly avoiding proprietary issues and incompatibilities.",
    "author": [
      {
        "name": "Alex Guazzelli",
        "url": {}
      },
      {
        "name": "Michael Zeller",
        "url": {}
      },
      {
        "name": "Wen-Ching Lin",
        "url": {}
      },
      {
        "name": "Graham Williams",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RJ-2009-019/",
    "title": "Sample Size Estimation while Controlling False Discovery Rate for Microarray Experiments Using the ssize.fdr Package",
    "description": "Microarray experiments are becoming more and more popular and critical in many biological disciplines. As in any statistical experiment, appropriate experimental design is essential for reliable statistical inference, and sample size has a crucial role in experimental design. Because microarray experiments are rather costly, it is important to have an adequate sample size that will achieve a desired power without wasting resources.",
    "author": [
      {
        "name": "Megan Orr",
        "url": {}
      },
      {
        "name": "Peng Liu",
        "url": {}
      }
    ],
    "date": "2009-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:29+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-009/",
    "title": "An introduction to rggobi",
    "description": "\"An introduction to rggobi\" published in R News.",
    "author": [
      {
        "name": "Hadley Wickham",
        "url": {}
      },
      {
        "name": "Michael Lawrence",
        "url": {}
      },
      {
        "name": "Duncan Temple Lang",
        "url": {}
      },
      {
        "name": "Deborah F. Swayne",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2008-009/preview.png",
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {},
    "preview_width": 370,
    "preview_height": 370
  },
  {
    "path": "articles/RN-2008-010/",
    "title": "Introducing the bipartite package: Analysing ecological networks",
    "description": "\"Introducing the bipartite package: Analysing ecological networks\" published in R News.",
    "author": [
      {
        "name": "Carsten F. Dormann",
        "url": {}
      },
      {
        "name": "Bernd Gruber",
        "url": {}
      },
      {
        "name": "Jochen Fründ",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-011/",
    "title": "The profileModel R package: Profiling objectives for models with linear predictors",
    "description": "\"The profileModel R package: Profiling objectives for models with linear predictors\" published in R News.",
    "author": [
      {
        "name": "Ioannis Kosmidis",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-012/",
    "title": "An introduction to text mining in R",
    "description": "\"An introduction to text mining in R\" published in R News.",
    "author": [
      {
        "name": "Ingo Feinerer",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-013/",
    "title": "Animation: A package for statistical animations",
    "description": "\"Animation: A package for statistical animations\" published in R News.",
    "author": [
      {
        "name": "Yihui Xie",
        "url": {}
      },
      {
        "name": "Xiaoyue Cheng",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-014/",
    "title": "The VGAM package",
    "description": "\"The VGAM package\" published in R News.",
    "author": [
      {
        "name": "Thomas W. Yee",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-015/",
    "title": "Comparing non-identical objects",
    "description": "\"Comparing non-identical objects\" published in R News.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-016/",
    "title": "Mvna: An R package for the Nelson-Aalen estimator in multistate models",
    "description": "\"Mvna: An R package for the Nelson-Aalen estimator in multistate models\" published in R News.",
    "author": [
      {
        "name": "A. Allignol",
        "url": {}
      },
      {
        "name": "J. Beyersmann",
        "url": {}
      },
      {
        "name": "M. Schumacher",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-017/",
    "title": "Programmers’ Niche: The Y of R",
    "description": "\"Programmers’ Niche: The Y of R\" published in R News.",
    "author": [
      {
        "name": "Vince Carey",
        "url": {}
      }
    ],
    "date": "2008-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-001/",
    "title": "Using Sweave with LyX",
    "description": "\"Using Sweave with LyX\" published in R News.",
    "author": [
      {
        "name": "Gregor Gorjanc",
        "url": {}
      }
    ],
    "date": "2008-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-002/",
    "title": "Trade costs",
    "description": "\"Trade costs\" published in R News.",
    "author": [
      {
        "name": "Jeff Enos",
        "url": {}
      },
      {
        "name": "David Kane",
        "url": {}
      },
      {
        "name": "Arjun Ravi Narayan",
        "url": {}
      },
      {
        "name": "Aaron Schwartz",
        "url": {}
      },
      {
        "name": "Daniel Suo",
        "url": {}
      },
      {
        "name": "Luyi Zhao",
        "url": {}
      }
    ],
    "date": "2008-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-003/",
    "title": "Survival analysis for cohorts with missing covariate information",
    "description": "\"Survival analysis for cohorts with missing covariate information\" published in R News.",
    "author": [
      {
        "name": "Hormuzd A. Katki",
        "url": {}
      },
      {
        "name": "Steven D. Mark",
        "url": {}
      }
    ],
    "date": "2008-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-004/",
    "title": "Segmented: An R package to fit regression models with broken-line relationships",
    "description": "\"Segmented: An R package to fit regression models with broken-line relationships\" published in R News.",
    "author": [
      {
        "name": "Vito M. R. Muggeo",
        "url": {}
      }
    ],
    "date": "2008-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-005/",
    "title": "Bayesian estimation for parsimonious threshold autoregressive models in R",
    "description": "\"Bayesian estimation for parsimonious threshold autoregressive models in R\" published in R News.",
    "author": [
      {
        "name": "Cathy W. S. Chen",
        "url": {}
      },
      {
        "name": "Edward M. H. Lin",
        "url": {}
      },
      {
        "name": "F. C. Liu",
        "url": {}
      },
      {
        "name": "Richard Gerlach",
        "url": {}
      }
    ],
    "date": "2008-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2008-005/preview.png",
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {},
    "preview_width": 1198,
    "preview_height": 1195
  },
  {
    "path": "articles/RN-2008-006/",
    "title": "Statistical modeling of loss distributions using actuar",
    "description": "\"Statistical modeling of loss distributions using actuar\" published in R News.",
    "author": [
      {
        "name": "Vincent Goulet",
        "url": {}
      },
      {
        "name": "Mathieu Pigeon",
        "url": {}
      }
    ],
    "date": "2008-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-007/",
    "title": "Programmers’ Niche: Multivariate polynomials in r",
    "description": "\"Programmers’ Niche: Multivariate polynomials in r\" published in R News.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2008-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2008-008/",
    "title": "R Help Desk: How can I avoid this loop or make it faster?",
    "description": "\"R Help Desk: How can I avoid this loop or make it faster?\" published in R News.",
    "author": [
      {
        "name": "Uwe Ligges",
        "url": {}
      },
      {
        "name": "John Fox",
        "url": {}
      }
    ],
    "date": "2008-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-021/",
    "title": "SpherWave: An R package for analyzing scattered spherical data by spherical wavelets",
    "description": "\"SpherWave: An R package for analyzing scattered spherical data by spherical wavelets\" published in R News.",
    "author": [
      {
        "name": "Hee-Seok Oh",
        "url": {}
      },
      {
        "name": "Donghoh Kim",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2007-021/preview.png",
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {},
    "preview_width": 688,
    "preview_height": 352
  },
  {
    "path": "articles/RN-2007-022/",
    "title": "Diving behaviour analysis in R",
    "description": "\"Diving behaviour analysis in R\" published in R News.",
    "author": [
      {
        "name": "Sebastián P. Luque",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2007-022/preview.png",
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {},
    "preview_width": 480,
    "preview_height": 480
  },
  {
    "path": "articles/RN-2007-023/",
    "title": "Very large numbers in R: Introducing package Brobdingnag",
    "description": "\"Very large numbers in R: Introducing package Brobdingnag\" published in R News.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-024/",
    "title": "Applied bayesian non- and semi-parametric inference using DPpackage",
    "description": "\"Applied bayesian non- and semi-parametric inference using DPpackage\" published in R News.",
    "author": [
      {
        "name": "Alejandro Jara",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-025/",
    "title": "An introduction to gWidgets",
    "description": "\"An introduction to gWidgets\" published in R News.",
    "author": [
      {
        "name": "John Verzani",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2007-025/preview.png",
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {},
    "preview_width": 216,
    "preview_height": 147
  },
  {
    "path": "articles/RN-2007-026/",
    "title": "Financial journalism with R",
    "description": "\"Financial journalism with R\" published in R News.",
    "author": [
      {
        "name": "Bill Alpert",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-027/",
    "title": "Need a hint?",
    "description": "\"Need a hint?\" published in R News.",
    "author": [
      {
        "name": "Sanford Weisberg",
        "url": {}
      },
      {
        "name": "Hadley Wickham",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-028/",
    "title": "Psychometrics task view",
    "description": "\"Psychometrics task view\" published in R News.",
    "author": [
      {
        "name": "Patrick Mair",
        "url": {}
      },
      {
        "name": "Reinhold Hatzinger",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-029/",
    "title": "Meta: An R package for meta-analysis",
    "description": "\"Meta: An R package for meta-analysis\" published in R News.",
    "author": [
      {
        "name": "Guido Schwarzer",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-030/",
    "title": "Extending the R Commander by “plug-in” packages",
    "description": "\"Extending the R Commander by “plug-in” packages\" published in R News.",
    "author": [
      {
        "name": "John Fox",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2007-030/preview.png",
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {},
    "preview_width": 691,
    "preview_height": 745
  },
  {
    "path": "articles/RN-2007-031/",
    "title": "Improvements to the multiple testing package multtest",
    "description": "\"Improvements to the multiple testing package multtest\" published in R News.",
    "author": [
      {
        "name": "Sandra L. Taylor",
        "url": {}
      },
      {
        "name": "Duncan Temple Lang",
        "url": {}
      },
      {
        "name": "Katherine S. Pollard",
        "url": {}
      }
    ],
    "date": "2007-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-011/",
    "title": "New functions for multivariate analysis",
    "description": "\"New functions for multivariate analysis\" published in R News.",
    "author": [
      {
        "name": "Peter Dalgaard",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-012/",
    "title": "Gnm: A package for generalized nonlinear models",
    "description": "\"Gnm: A package for generalized nonlinear models\" published in R News.",
    "author": [
      {
        "name": "Heather Turner",
        "url": {}
      },
      {
        "name": "David Firth",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-013/",
    "title": "Fmri: A package for analyzing fmri data",
    "description": "\"Fmri: A package for analyzing fmri data\" published in R News.",
    "author": [
      {
        "name": "Jörg Polzehl",
        "url": {}
      },
      {
        "name": "Karten Tabelow",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2007-013/preview.png",
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {},
    "preview_width": 530,
    "preview_height": 427
  },
  {
    "path": "articles/RN-2007-014/",
    "title": "Optmatch: Flexible, optimal matching for observational studies",
    "description": "\"Optmatch: Flexible, optimal matching for observational studies\" published in R News.",
    "author": [
      {
        "name": "Ben B. Hansen",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-015/",
    "title": "Random survival forests for R",
    "description": "\"Random survival forests for R\" published in R News.",
    "author": [
      {
        "name": "Hemant Ishwaran",
        "url": {}
      },
      {
        "name": "Udaya B. Kogalur",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-016/",
    "title": "Rwui: A web application to create user friendly web interfaces for R scripts",
    "description": "\"Rwui: A web application to create user friendly web interfaces for R scripts\" published in R News.",
    "author": [
      {
        "name": "Richard Newton",
        "url": {}
      },
      {
        "name": "Lorenz Wernisch",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2007-016/preview.png",
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {},
    "preview_width": 969,
    "preview_height": 1001
  },
  {
    "path": "articles/RN-2007-017/",
    "title": "The np package: Kernel methods for categorical and continuous data",
    "description": "\"The np package: Kernel methods for categorical and continuous data\" published in R News.",
    "author": [
      {
        "name": "Tristen Hayfield",
        "url": {}
      },
      {
        "name": "Jeffrey S. Racine",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-018/",
    "title": "eiPack: {R} \\times {C} ecological inferences and higher-dimension data management",
    "description": "\"eiPack: {R} \\times {C} ecological inferences and higher-dimension data management\" published in R News.",
    "author": [
      {
        "name": "Olivia Lau",
        "url": {}
      },
      {
        "name": "Ryan T. Moore",
        "url": {}
      },
      {
        "name": "Michael Kellermann",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-019/",
    "title": "The ade4 package–II: Two-table and {K}-table methods",
    "description": "\"The ade4 package–II: Two-table and {K}-table methods\" published in R News.",
    "author": [
      {
        "name": "Stéphane Dray",
        "url": {}
      },
      {
        "name": "Anne B. Dufour",
        "url": {}
      },
      {
        "name": "Daniel Chessel",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-020/",
    "title": "Review of “The R Book”",
    "description": "\"Review of “The R Book”\" published in R News.",
    "author": [
      {
        "name": "Friedrich Leisch",
        "url": {}
      }
    ],
    "date": "2007-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-001/",
    "title": "Viewing binary files with the hexView package",
    "description": "\"Viewing binary files with the hexView package\" published in R News.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-002/",
    "title": "FlexMix: An R package for finite mixture modelling",
    "description": "\"FlexMix: An R package for finite mixture modelling\" published in R News.",
    "author": [
      {
        "name": "Bettina Grün",
        "url": {}
      },
      {
        "name": "Friedrich Leisch",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-003/",
    "title": "Using R to perform the AMMI analysis on agriculture variety trials",
    "description": "\"Using R to perform the AMMI analysis on agriculture variety trials\" published in R News.",
    "author": [
      {
        "name": "Andrea Onofri",
        "url": {}
      },
      {
        "name": "Egidio Ciriciofolo",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-004/",
    "title": "Inferences for ratios of normal means",
    "description": "\"Inferences for ratios of normal means\" published in R News.",
    "author": [
      {
        "name": "Gemechis Dilba",
        "url": {}
      },
      {
        "name": "Frank Schaarschmidt",
        "url": {}
      },
      {
        "name": "Ludwig A. Hothorn",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-005/",
    "title": "Working with unknown values",
    "description": "\"Working with unknown values\" published in R News.",
    "author": [
      {
        "name": "Gregor Gorjanc",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-006/",
    "title": "A new package for fitting random effect models",
    "description": "\"A new package for fitting random effect models\" published in R News.",
    "author": [
      {
        "name": "Jochen Einbeck",
        "url": {}
      },
      {
        "name": "John Hinde",
        "url": {}
      },
      {
        "name": "Ross Darnell",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-007/",
    "title": "Augmenting R with Unix tools",
    "description": "\"Augmenting R with Unix tools\" published in R News.",
    "author": [
      {
        "name": "Andrew Robinson",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-008/",
    "title": "POT: Modelling peaks over a threshold",
    "description": "\"POT: Modelling peaks over a threshold\" published in R News.",
    "author": [
      {
        "name": "Mathieu Ribatet",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-009/",
    "title": "Backtests",
    "description": "\"Backtests\" published in R News.",
    "author": [
      {
        "name": "Kyle Campbell",
        "url": {}
      },
      {
        "name": "Jeff Enos",
        "url": {}
      },
      {
        "name": "Daniel Gerlanc",
        "url": {}
      },
      {
        "name": "David Kane",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2007-010/",
    "title": "Review of John Verzani’s book: Using R for Introductory Statistics",
    "description": "\"Review of John Verzani’s book: Using R for Introductory Statistics\" published in R News.",
    "author": [
      {
        "name": "Andy Liaw",
        "url": {}
      }
    ],
    "date": "2007-04-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-036/",
    "title": "Graphs and networks: Tools in Bioconductor",
    "description": "\"Graphs and networks: Tools in Bioconductor\" published in R News.",
    "author": [
      {
        "name": "Li Long",
        "url": {}
      },
      {
        "name": "Vince Carey",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-037/",
    "title": "Modeling package dependencies using graphs",
    "description": "\"Modeling package dependencies using graphs\" published in R News.",
    "author": [
      {
        "name": "Seth Falcon",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-038/",
    "title": "Image analysis for microscopy screens",
    "description": "\"Image analysis for microscopy screens\" published in R News.",
    "author": [
      {
        "name": "Oleg Sklyar",
        "url": {}
      },
      {
        "name": "Wolfgang Huber",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2006-038/preview.png",
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {},
    "preview_width": 1220,
    "preview_height": 401
  },
  {
    "path": "articles/RN-2006-039/",
    "title": "Beadarray: An R package to analyse Illumina BeadArrays",
    "description": "\"Beadarray: An R package to analyse Illumina BeadArrays\" published in R News.",
    "author": [
      {
        "name": "Mark Dunning",
        "url": {}
      },
      {
        "name": "Mike Smith",
        "url": {}
      },
      {
        "name": "Natalie Thorne",
        "url": {}
      },
      {
        "name": "Simon Tavaré",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-040/",
    "title": "Transcript mapping with high-density tiling arrays",
    "description": "\"Transcript mapping with high-density tiling arrays\" published in R News.",
    "author": [
      {
        "name": "Matthew Ritchie",
        "url": {}
      },
      {
        "name": "Wolfgang Huber",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-041/",
    "title": "Analyzing flow cytometry data with Bioconductor",
    "description": "\"Analyzing flow cytometry data with Bioconductor\" published in R News.",
    "author": [
      {
        "name": "Nolwenn Le Meur",
        "url": {}
      },
      {
        "name": "Florian Hahne",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-042/",
    "title": "Protein complex membership estimation using apComplex",
    "description": "\"Protein complex membership estimation using apComplex\" published in R News.",
    "author": [
      {
        "name": "Denise Scholtens",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-043/",
    "title": "SNP metadata access and use with Bioconductor",
    "description": "\"SNP metadata access and use with Bioconductor\" published in R News.",
    "author": [
      {
        "name": "Vince Carey",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-044/",
    "title": "Integrating biological data resources into R with biomaRt",
    "description": "\"Integrating biological data resources into R with biomaRt\" published in R News.",
    "author": [
      {
        "name": "Steffen Durinck",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-045/",
    "title": "Identifying interesting genes with siggenes",
    "description": "\"Identifying interesting genes with siggenes\" published in R News.",
    "author": [
      {
        "name": "Holger Schwender",
        "url": {}
      },
      {
        "name": "Andreas Krause",
        "url": {}
      },
      {
        "name": "Katja Ickstadt",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-046/",
    "title": "Reverse engineering genetic networks using the GeneNet package",
    "description": "\"Reverse engineering genetic networks using the GeneNet package\" published in R News.",
    "author": [
      {
        "name": "Juliane Schäfer",
        "url": {}
      },
      {
        "name": "Rainer Opgen-Rhein",
        "url": {}
      },
      {
        "name": "Korbinian Strimmer",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-047/",
    "title": "A multivariate approach to integrating datasets using made4 and ade4",
    "description": "\"A multivariate approach to integrating datasets using made4 and ade4\" published in R News.",
    "author": [
      {
        "name": "Aedín C. Culhane",
        "url": {}
      },
      {
        "name": "Jean Thioulouse",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-048/",
    "title": "Using amap and ctc packages for huge clustering",
    "description": "\"Using amap and ctc packages for huge clustering\" published in R News.",
    "author": [
      {
        "name": "Antoine Lucas",
        "url": {}
      },
      {
        "name": "Sylvain Jasson",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-049/",
    "title": "Model-based microarray image analysis",
    "description": "\"Model-based microarray image analysis\" published in R News.",
    "author": [
      {
        "name": "Chris Fraley",
        "url": {}
      },
      {
        "name": "Adrian E. Raftery",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2006-049/preview.png",
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {},
    "preview_width": 652,
    "preview_height": 652
  },
  {
    "path": "articles/RN-2006-050/",
    "title": "Sample size estimation for microarray experiments using the ssize package",
    "description": "\"Sample size estimation for microarray experiments using the ssize package\" published in R News.",
    "author": [
      {
        "name": "Gregory R. Warnes",
        "url": {}
      }
    ],
    "date": "2006-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-025/",
    "title": "Sweave and the open document format – the odfWeave package",
    "description": "\"Sweave and the open document format – the odfWeave package\" published in R News.",
    "author": [
      {
        "name": "Max Kuhn",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2006-025/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 480,
    "preview_height": 480
  },
  {
    "path": "articles/RN-2006-026/",
    "title": "Plotrix",
    "description": "\"Plotrix\" published in R News.",
    "author": [
      {
        "name": "Jim Lemon",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-027/",
    "title": "Rpanel: Making graphs move with tcltk",
    "description": "\"Rpanel: Making graphs move with tcltk\" published in R News.",
    "author": [
      {
        "name": "Adrian Bowman",
        "url": {}
      },
      {
        "name": "Ewan Crawford",
        "url": {}
      },
      {
        "name": "Richard Bowman",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2006-027/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 114,
    "preview_height": 75
  },
  {
    "path": "articles/RN-2006-028/",
    "title": "R’s role in the climate change debate.",
    "description": "\"R’s role in the climate change debate.\" published in R News.",
    "author": [
      {
        "name": "Matthew Pocernich",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-029/",
    "title": "Interacting with data using the filehash package",
    "description": "\"Interacting with data using the filehash package\" published in R News.",
    "author": [
      {
        "name": "Roger D. Peng",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-030/",
    "title": "Special functions in R: Introducing the gsl package",
    "description": "\"Special functions in R: Introducing the gsl package\" published in R News.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-031/",
    "title": "A short introduction to the SIMEX and MCSIMEX",
    "description": "\"A short introduction to the SIMEX and MCSIMEX\" published in R News.",
    "author": [
      {
        "name": "Wolfgang Lederer",
        "url": {}
      },
      {
        "name": "Helmut Küchenhoff",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-032/",
    "title": "Parametric links for binary response",
    "description": "\"Parametric links for binary response\" published in R News.",
    "author": [
      {
        "name": "Roger Koenker",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-033/",
    "title": "A new package for the Birnbaum-Saunders distribution",
    "description": "\"A new package for the Birnbaum-Saunders distribution\" published in R News.",
    "author": [
      {
        "name": "Víctor Leiva",
        "url": {}
      },
      {
        "name": "Hugo Hernández",
        "url": {}
      },
      {
        "name": "Marco Riquelme",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-034/",
    "title": "Review of Fionn Murtagh’s book:\ncorrespondence analysis and data coding with Java and R",
    "description": "\"Review of Fionn Murtagh’s book:\ncorrespondence analysis and data coding with Java and R\" published in R News.",
    "author": [
      {
        "name": "Susan Holmes",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-035/",
    "title": "R Help Desk: Accessing the sources",
    "description": "\"R Help Desk: Accessing the sources\" published in R News.",
    "author": [
      {
        "name": "Uwe Ligges",
        "url": {}
      }
    ],
    "date": "2006-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:43+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-021/",
    "title": "Fitting dose-response curves from bioassays and toxicity testing",
    "description": "\"Fitting dose-response curves from bioassays and toxicity testing\" published in R News.",
    "author": [
      {
        "name": "Johannes Ranke",
        "url": {}
      }
    ],
    "date": "2006-08-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2003-021/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 10,
    "preview_height": 13
  },
  {
    "path": "articles/RN-2006-020/",
    "title": "Non-linear regression for optimising the separation of carboxylic acids",
    "description": "\"Non-linear regression for optimising the separation of carboxylic acids\" published in R News.",
    "author": [
      {
        "name": "Peter Watkins",
        "url": {}
      },
      {
        "name": "Bill Venables",
        "url": {}
      }
    ],
    "date": "2006-08-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-021/",
    "title": "The pls package",
    "description": "\"The pls package\" published in R News.",
    "author": [
      {
        "name": "Bjørn-Helge Mevik",
        "url": {}
      }
    ],
    "date": "2006-08-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-022/",
    "title": "Some applications of model-based clustering in chemistry",
    "description": "\"Some applications of model-based clustering in chemistry\" published in R News.",
    "author": [
      {
        "name": "Chris Fraley",
        "url": {}
      },
      {
        "name": "Adrian E. Raftery",
        "url": {}
      }
    ],
    "date": "2006-08-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-023/",
    "title": "Mapping databases of X-ray powder patterns",
    "description": "\"Mapping databases of X-ray powder patterns\" published in R News.",
    "author": [
      {
        "name": "Ron Wehrens",
        "url": {}
      },
      {
        "name": "Egon Willighagen",
        "url": {}
      }
    ],
    "date": "2006-08-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-024/",
    "title": "Generating, using and visualizing molecular information in R",
    "description": "\"Generating, using and visualizing molecular information in R\" published in R News.",
    "author": [
      {
        "name": "Rajarshi Guha",
        "url": {}
      }
    ],
    "date": "2006-08-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2006-024/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 300,
    "preview_height": 300
  },
  {
    "path": "articles/RN-2006-007/",
    "title": "S4 classes for distributions",
    "description": "\"S4 classes for distributions\" published in R News.",
    "author": [
      {
        "name": "Peter Ruckdeschel",
        "url": {}
      },
      {
        "name": "Matthias Kohl",
        "url": {}
      },
      {
        "name": "Thomas Stabla",
        "url": {}
      },
      {
        "name": "Florian Camphausen",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-008/",
    "title": "The regress function",
    "description": "\"The regress function\" published in R News.",
    "author": [
      {
        "name": "David Clifford",
        "url": {}
      },
      {
        "name": "Peter McCullagh",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-009/",
    "title": "Processing data for outliers",
    "description": "\"Processing data for outliers\" published in R News.",
    "author": [
      {
        "name": "Lukasz Komsta",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-010/",
    "title": "Analysing equity portfolios in R",
    "description": "\"Analysing equity portfolios in R\" published in R News.",
    "author": [
      {
        "name": "David Kane",
        "url": {}
      },
      {
        "name": "Jeff Enos",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-011/",
    "title": "GroupSeq: Designing clinical trials using group sequential designs",
    "description": "\"GroupSeq: Designing clinical trials using group sequential designs\" published in R News.",
    "author": [
      {
        "name": "Roman Pahl",
        "url": {}
      },
      {
        "name": "Andreas Ziegler",
        "url": {}
      },
      {
        "name": "Inke R. König",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2006-011/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 254,
    "preview_height": 192
  },
  {
    "path": "articles/RN-2006-012/",
    "title": "Using R/Sweave in everyday clinical practice",
    "description": "\"Using R/Sweave in everyday clinical practice\" published in R News.",
    "author": [
      {
        "name": "Sven Garbade",
        "url": {}
      },
      {
        "name": "Peter Burgard",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2006-012/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 304,
    "preview_height": 278
  },
  {
    "path": "articles/RN-2006-013/",
    "title": "changeLOS: An R-package for change in length of hospital stay based on the Aalen-Johansen estimator",
    "description": "\"changeLOS: An R-package for change in length of hospital stay based on the Aalen-Johansen estimator\" published in R News.",
    "author": [
      {
        "name": "M. Wangler",
        "url": {}
      },
      {
        "name": "J. Beyersmann",
        "url": {}
      },
      {
        "name": "M. Schumacher",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-014/",
    "title": "Balloon plot",
    "description": "\"Balloon plot\" published in R News.",
    "author": [
      {
        "name": "Nitin Jain",
        "url": {}
      },
      {
        "name": "Gregory R. Warnes",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-015/",
    "title": "Drawing pedigree diagrams with R and graphviz",
    "description": "\"Drawing pedigree diagrams with R and graphviz\" published in R News.",
    "author": [
      {
        "name": "Jing Hua Zhao",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-016/",
    "title": "Non-standard fonts in PostScript and PDF graphics",
    "description": "\"Non-standard fonts in PostScript and PDF graphics\" published in R News.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      },
      {
        "name": "Brian Ripley",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2006-016/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 94,
    "preview_height": 92
  },
  {
    "path": "articles/RN-2006-017/",
    "title": "The doBy package",
    "description": "\"The doBy package\" published in R News.",
    "author": [
      {
        "name": "Søren Højsgaard",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-018/",
    "title": "Normed division algebras with R: Introducing the onion package",
    "description": "\"Normed division algebras with R: Introducing the onion package\" published in R News.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-019/",
    "title": "Electrical properties of resistor networks",
    "description": "\"Electrical properties of resistor networks\" published in R News.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2006-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-001/",
    "title": "Applied Bayesian inference in R using MCMCpack",
    "description": "\"Applied Bayesian inference in R using MCMCpack\" published in R News.",
    "author": [
      {
        "name": "Andrew D. Martin",
        "url": {}
      },
      {
        "name": "Kevin M. Quinn",
        "url": {}
      }
    ],
    "date": "2006-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-002/",
    "title": "CODA: Convergence diagnosis and output analysis for MCMC",
    "description": "\"CODA: Convergence diagnosis and output analysis for MCMC\" published in R News.",
    "author": [
      {
        "name": "Martyn Plummer",
        "url": {}
      },
      {
        "name": "Nicky Best",
        "url": {}
      },
      {
        "name": "Kate Cowles",
        "url": {}
      },
      {
        "name": "Karen Vines",
        "url": {}
      }
    ],
    "date": "2006-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-003/",
    "title": "Bayesian software validation",
    "description": "\"Bayesian software validation\" published in R News.",
    "author": [
      {
        "name": "Samantha Cook",
        "url": {}
      },
      {
        "name": "Andrew Gelman",
        "url": {}
      }
    ],
    "date": "2006-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-004/",
    "title": "Making BUGS open",
    "description": "\"Making BUGS open\" published in R News.",
    "author": [
      {
        "name": "Andrew Thomas",
        "url": {}
      },
      {
        "name": "Bob OH́ara",
        "url": {}
      },
      {
        "name": "Uwe Ligges",
        "url": {}
      },
      {
        "name": "Sibylle Sturtz",
        "url": {}
      }
    ],
    "date": "2006-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-005/",
    "title": "The BUGS language",
    "description": "\"The BUGS language\" published in R News.",
    "author": [
      {
        "name": "Andrew Thomas",
        "url": {}
      }
    ],
    "date": "2006-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2006-006/",
    "title": "Bayesian data analysis using R",
    "description": "\"Bayesian data analysis using R\" published in R News.",
    "author": [
      {
        "name": "Jouni Kerman",
        "url": {}
      },
      {
        "name": "Andrew Gelman",
        "url": {}
      }
    ],
    "date": "2006-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-013/",
    "title": "BMA: An R package for Bayesian model averaging",
    "description": "\"BMA: An R package for Bayesian model averaging\" published in R News.",
    "author": [
      {
        "name": "Adrian E. Raftery",
        "url": {}
      },
      {
        "name": "Ian S. Painter",
        "url": {}
      },
      {
        "name": "Christopher T. Volinsky",
        "url": {}
      }
    ],
    "date": "2005-11-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-014/",
    "title": "Classes and methods for spatial data in R",
    "description": "\"Classes and methods for spatial data in R\" published in R News.",
    "author": [
      {
        "name": "Edzer J. Pebesma",
        "url": {}
      },
      {
        "name": "Roger S. Bivand",
        "url": {}
      }
    ],
    "date": "2005-11-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-015/",
    "title": "Running long R jobs with Condor DAG",
    "description": "\"Running long R jobs with Condor DAG\" published in R News.",
    "author": [
      {
        "name": "Xianhong Xie",
        "url": {}
      }
    ],
    "date": "2005-11-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-016/",
    "title": "Rstream: Streams of random numbers for stochastic simulation",
    "description": "\"Rstream: Streams of random numbers for stochastic simulation\" published in R News.",
    "author": [
      {
        "name": "Pierre L’Ecuyer",
        "url": {}
      },
      {
        "name": "Josef Leydold",
        "url": {}
      }
    ],
    "date": "2005-11-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-017/",
    "title": "Mfp: Multivariable fractional polynomials",
    "description": "\"Mfp: Multivariable fractional polynomials\" published in R News.",
    "author": [
      {
        "name": "Axel Benner",
        "url": {}
      }
    ],
    "date": "2005-11-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-018/",
    "title": "Crossdes: A package for design and randomization in crossover studies",
    "description": "\"Crossdes: A package for design and randomization in crossover studies\" published in R News.",
    "author": [
      {
        "name": "Oliver Sailer",
        "url": {}
      }
    ],
    "date": "2005-11-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-019/",
    "title": "R Help Desk: Make “R CMD” work under Windows – an example",
    "description": "\"R Help Desk: Make “R CMD” work under Windows – an example\" published in R News.",
    "author": [
      {
        "name": "Uwe Ligges",
        "url": {}
      },
      {
        "name": "Duncan Murdoch",
        "url": {}
      }
    ],
    "date": "2005-11-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-001/",
    "title": "Internationalization features of R 2.1.0",
    "description": "\"Internationalization features of R 2.1.0\" published in R News.",
    "author": [
      {
        "name": "Brian D. Ripley",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2005-001/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 665,
    "preview_height": 356
  },
  {
    "path": "articles/RN-2005-002/",
    "title": "Packages and their management in R 2.1.0",
    "description": "\"Packages and their management in R 2.1.0\" published in R News.",
    "author": [
      {
        "name": "Brian D. Ripley",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2005-002/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 172,
    "preview_height": 159
  },
  {
    "path": "articles/RN-2005-003/",
    "title": "Recent changes in grid graphics",
    "description": "\"Recent changes in grid graphics\" published in R News.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-004/",
    "title": "Hoa: An R package bundle for higher order likelihood inference",
    "description": "\"Hoa: An R package bundle for higher order likelihood inference\" published in R News.",
    "author": [
      {
        "name": "Alessandra Brazzale",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-005/",
    "title": "Fitting linear mixed models in R",
    "description": "\"Fitting linear mixed models in R\" published in R News.",
    "author": [
      {
        "name": "Douglas Bates",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-006/",
    "title": "Using R for statistical seismology",
    "description": "\"Using R for statistical seismology\" published in R News.",
    "author": [
      {
        "name": "Ray Brownrigg",
        "url": {}
      },
      {
        "name": "David Harte",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-007/",
    "title": "Literate programming for creating and maintaining packages",
    "description": "\"Literate programming for creating and maintaining packages\" published in R News.",
    "author": [
      {
        "name": "Jonathan Rougier",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-008/",
    "title": "CRAN task views",
    "description": "\"CRAN task views\" published in R News.",
    "author": [
      {
        "name": "Achim Zeileis",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-009/",
    "title": "Using control structures with Sweave",
    "description": "\"Using control structures with Sweave\" published in R News.",
    "author": [
      {
        "name": "Damian Betebenner",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-010/",
    "title": "The value of R for preclinical statisticians",
    "description": "\"The value of R for preclinical statisticians\" published in R News.",
    "author": [
      {
        "name": "Bill Pikounis",
        "url": {}
      },
      {
        "name": "Andy Liaw",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-011/",
    "title": "Recreational mathematics with R: Introducing the “magic” package",
    "description": "\"Recreational mathematics with R: Introducing the “magic” package\" published in R News.",
    "author": [
      {
        "name": "Robin K. S. Hankin",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2005-012/",
    "title": "Programmer’s Niche: How do you spell that number?",
    "description": "\"Programmer’s Niche: How do you spell that number?\" published in R News.",
    "author": [
      {
        "name": "John Fox",
        "url": {}
      }
    ],
    "date": "2005-05-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-009/",
    "title": "Lazy loading and packages in R 2.0.0",
    "description": "\"Lazy loading and packages in R 2.0.0\" published in R News.",
    "author": [
      {
        "name": "Brian D. Ripley",
        "url": {}
      }
    ],
    "date": "2004-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-010/",
    "title": "Fonts, lines, and transparency in R graphics",
    "description": "\"Fonts, lines, and transparency in R graphics\" published in R News.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2004-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-011/",
    "title": "The NMMAPSdata package",
    "description": "\"The NMMAPSdata package\" published in R News.",
    "author": [
      {
        "name": "Roger D. Peng",
        "url": {}
      },
      {
        "name": "Leah J. Welty",
        "url": {}
      }
    ],
    "date": "2004-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-012/",
    "title": "Laying out pathways with Rgraphviz",
    "description": "\"Laying out pathways with Rgraphviz\" published in R News.",
    "author": [
      {
        "name": "Jeff Gentry",
        "url": {}
      },
      {
        "name": "Vincent Carey",
        "url": {}
      },
      {
        "name": "Emden Gansner",
        "url": {}
      },
      {
        "name": "Robert Gentleman",
        "url": {}
      }
    ],
    "date": "2004-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-013/",
    "title": "Fusing R and BUGS through Wine",
    "description": "\"Fusing R and BUGS through Wine\" published in R News.",
    "author": [
      {
        "name": "Jun Yan",
        "url": {}
      }
    ],
    "date": "2004-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-014/",
    "title": "R package maintenance",
    "description": "\"R package maintenance\" published in R News.",
    "author": [
      {
        "name": "Paul Gilbert",
        "url": {}
      }
    ],
    "date": "2004-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-001/",
    "title": "The decision to use R",
    "description": "\"The decision to use R\" published in R News.",
    "author": [
      {
        "name": "Marc Schwartz",
        "url": {}
      }
    ],
    "date": "2004-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-002/",
    "title": "The ade4 package — I: One-table methods",
    "description": "\"The ade4 package — I: One-table methods\" published in R News.",
    "author": [
      {
        "name": "Daniel Chessel",
        "url": {}
      },
      {
        "name": "Anne B. Dufour",
        "url": {}
      },
      {
        "name": "Jean Thioulouse",
        "url": {}
      }
    ],
    "date": "2004-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-003/",
    "title": "Qcc: An R package for quality control charting and statistical process control",
    "description": "\"Qcc: An R package for quality control charting and statistical process control\" published in R News.",
    "author": [
      {
        "name": "Luca Scrucca",
        "url": {}
      }
    ],
    "date": "2004-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-004/",
    "title": "Least squares calculations in R",
    "description": "\"Least squares calculations in R\" published in R News.",
    "author": [
      {
        "name": "Douglas Bates",
        "url": {}
      }
    ],
    "date": "2004-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-005/",
    "title": "Tools for interactively exploring R packages",
    "description": "\"Tools for interactively exploring R packages\" published in R News.",
    "author": [
      {
        "name": "Jianhua Zhang",
        "url": {}
      },
      {
        "name": "Robert Gentleman",
        "url": {}
      }
    ],
    "date": "2004-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2004-005/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 531,
    "preview_height": 377
  },
  {
    "path": "articles/RN-2004-006/",
    "title": "The survival package",
    "description": "\"The survival package\" published in R News.",
    "author": [
      {
        "name": "Thomas Lumley",
        "url": {}
      }
    ],
    "date": "2004-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-007/",
    "title": "R Help Desk: Date and time classes in R",
    "description": "\"R Help Desk: Date and time classes in R\" published in R News.",
    "author": [
      {
        "name": "Gabor Grothendieck",
        "url": {}
      },
      {
        "name": "Thomas Petzoldt",
        "url": {}
      }
    ],
    "date": "2004-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2004-008/",
    "title": "Programmers’ Niche: A simple class, in S3 and S4",
    "description": "\"Programmers’ Niche: A simple class, in S3 and S4\" published in R News.",
    "author": [
      {
        "name": "Thomas Lumley",
        "url": {}
      }
    ],
    "date": "2004-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-014/",
    "title": "Dimensional reduction for data mapping",
    "description": "\"Dimensional reduction for data mapping\" published in R News.",
    "author": [
      {
        "name": "Jonathan Edwards",
        "url": {}
      },
      {
        "name": "Paul Oman",
        "url": {}
      }
    ],
    "date": "2003-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-015/",
    "title": "R as a simulation platform in ecological modelling",
    "description": "\"R as a simulation platform in ecological modelling\" published in R News.",
    "author": [
      {
        "name": "Thomas Petzoldt",
        "url": {}
      }
    ],
    "date": "2003-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2003-015/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 10,
    "preview_height": 13
  },
  {
    "path": "articles/RN-2003-016/",
    "title": "Using R for estimating longitudinal student achievement models",
    "description": "\"Using R for estimating longitudinal student achievement models\" published in R News.",
    "author": [
      {
        "name": "J. R. Lockwood",
        "url": {}
      },
      {
        "name": "Harold Doran",
        "url": {}
      },
      {
        "name": "Daniel F. McCaffrey",
        "url": {}
      }
    ],
    "date": "2003-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-017/",
    "title": "lmeSplines",
    "description": "\"lmeSplines\" published in R News.",
    "author": [
      {
        "name": "Rod Ball",
        "url": {}
      }
    ],
    "date": "2003-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-018/",
    "title": "Debugging without (too many) tears",
    "description": "\"Debugging without (too many) tears\" published in R News.",
    "author": [
      {
        "name": "Mark Bravington",
        "url": {}
      }
    ],
    "date": "2003-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-019/",
    "title": "The R2HTML package",
    "description": "\"The R2HTML package\" published in R News.",
    "author": [
      {
        "name": "Eric Lecoutre",
        "url": {}
      }
    ],
    "date": "2003-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-020/",
    "title": "R Help Desk: Package management",
    "description": "\"R Help Desk: Package management\" published in R News.",
    "author": [
      {
        "name": "Uwe Ligges",
        "url": {}
      }
    ],
    "date": "2003-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-009/",
    "title": "R Help Desk: An introduction to using R’s base graphics",
    "description": "\"R Help Desk: An introduction to using R’s base graphics\" published in R News.",
    "author": [
      {
        "name": "Marc Schwartz",
        "url": {}
      }
    ],
    "date": "2003-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-010/",
    "title": "Integrating grid graphics output with base graphics output",
    "description": "\"Integrating grid graphics output with base graphics output\" published in R News.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2003-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-011/",
    "title": "A new package for the general error distribution",
    "description": "\"A new package for the general error distribution\" published in R News.",
    "author": [
      {
        "name": "Angelo Mineo",
        "url": {}
      }
    ],
    "date": "2003-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-012/",
    "title": "Web-based microarray analysis using Bioconductor",
    "description": "\"Web-based microarray analysis using Bioconductor\" published in R News.",
    "author": [
      {
        "name": "Colin A. Smith",
        "url": {}
      }
    ],
    "date": "2003-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2003-012/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 448,
    "preview_height": 510
  },
  {
    "path": "articles/RN-2003-013/",
    "title": "Sweave, part II: Package vignettes",
    "description": "\"Sweave, part II: Package vignettes\" published in R News.",
    "author": [
      {
        "name": "Friedrich Leisch",
        "url": {}
      }
    ],
    "date": "2003-10-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-001/",
    "title": "Name space management for R",
    "description": "\"Name space management for R\" published in R News.",
    "author": [
      {
        "name": "Luke Tierney",
        "url": {}
      }
    ],
    "date": "2003-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-002/",
    "title": "Converting packages to S4",
    "description": "\"Converting packages to S4\" published in R News.",
    "author": [
      {
        "name": "Douglas Bates",
        "url": {}
      }
    ],
    "date": "2003-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-003/",
    "title": "The genetics package",
    "description": "\"The genetics package\" published in R News.",
    "author": [
      {
        "name": "Gregory R. Warnes",
        "url": {}
      }
    ],
    "date": "2003-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-004/",
    "title": "Variance inflation factors",
    "description": "\"Variance inflation factors\" published in R News.",
    "author": [
      {
        "name": "Jürgen Groß",
        "url": {}
      }
    ],
    "date": "2003-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-005/",
    "title": "Building Microsoft Windows versions of R and R packages under Intel Linux",
    "description": "\"Building Microsoft Windows versions of R and R packages under Intel Linux\" published in R News.",
    "author": [
      {
        "name": "Jun Yan",
        "url": {}
      },
      {
        "name": "A. J. Rossini",
        "url": {}
      }
    ],
    "date": "2003-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-006/",
    "title": "Analysing survey data in R",
    "description": "\"Analysing survey data in R\" published in R News.",
    "author": [
      {
        "name": "Thomas Lumley",
        "url": {}
      }
    ],
    "date": "2003-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-007/",
    "title": "Computational gains using RPVM on a Beowulf cluster",
    "description": "\"Computational gains using RPVM on a Beowulf cluster\" published in R News.",
    "author": [
      {
        "name": "Brett Carson",
        "url": {}
      },
      {
        "name": "Robert Murison",
        "url": {}
      },
      {
        "name": "Ian A. Mason",
        "url": {}
      }
    ],
    "date": "2003-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2003-008/",
    "title": "R Help Desk: Getting help – R’s help facilities and manuals",
    "description": "\"R Help Desk: Getting help – R’s help facilities and manuals\" published in R News.",
    "author": [
      {
        "name": "Uwe Ligges",
        "url": {}
      }
    ],
    "date": "2003-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-017/",
    "title": "Resampling methods in R: The boot package",
    "description": "\"Resampling methods in R: The boot package\" published in R News.",
    "author": [
      {
        "name": "Angelo J. Canty",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2002-017/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 85,
    "preview_height": 102
  },
  {
    "path": "articles/RN-2002-018/",
    "title": "Diagnostic checking in regression relationships",
    "description": "\"Diagnostic checking in regression relationships\" published in R News.",
    "author": [
      {
        "name": "Achim Zeileis",
        "url": {}
      },
      {
        "name": "Torsten Hothorn",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-019/",
    "title": "Delayed data packages",
    "description": "\"Delayed data packages\" published in R News.",
    "author": [
      {
        "name": "David E. Brahm",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-020/",
    "title": "Geepack: Yet another package for generalized estimating equations",
    "description": "\"Geepack: Yet another package for generalized estimating equations\" published in R News.",
    "author": [
      {
        "name": "Jun Yan",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-021/",
    "title": "On multiple comparisons in R",
    "description": "\"On multiple comparisons in R\" published in R News.",
    "author": [
      {
        "name": "Frank Bretz",
        "url": {}
      },
      {
        "name": "Torsten Hothorn",
        "url": {}
      },
      {
        "name": "Peter Westfall",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-022/",
    "title": "Classification and regression by randomForest",
    "description": "\"Classification and regression by randomForest\" published in R News.",
    "author": [
      {
        "name": "Andy Liaw",
        "url": {}
      },
      {
        "name": "Matthew Wiener",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-023/",
    "title": "Some strategies for dealing with genomic data",
    "description": "\"Some strategies for dealing with genomic data\" published in R News.",
    "author": [
      {
        "name": "Robert Gentleman",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-024/",
    "title": "Changes to the R-Tcl/Tk package",
    "description": "\"Changes to the R-Tcl/Tk package\" published in R News.",
    "author": [
      {
        "name": "Peter Dalgaard",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-025/",
    "title": "Sweave, part I: Mixing R and LaTeX",
    "description": "\"Sweave, part I: Mixing R and LaTeX\" published in R News.",
    "author": [
      {
        "name": "Friedrich Leisch",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2002-025/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 1,
    "preview_height": 1
  },
  {
    "path": "articles/RN-2002-026/",
    "title": "R Help Desk: Automation of mathematical annotation in plots",
    "description": "\"R Help Desk: Automation of mathematical annotation in plots\" published in R News.",
    "author": [
      {
        "name": "Uwe Ligges",
        "url": {}
      }
    ],
    "date": "2002-12-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-007/",
    "title": "Time series in R 1.5.0",
    "description": "\"Time series in R 1.5.0\" published in R News.",
    "author": [
      {
        "name": "Brian D. Ripley",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-008/",
    "title": "Naive time series forecasting methods",
    "description": "\"Naive time series forecasting methods\" published in R News.",
    "author": [
      {
        "name": "David Meyer",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-009/",
    "title": "Rmpi: Parallel statistical computing in R",
    "description": "\"Rmpi: Parallel statistical computing in R\" published in R News.",
    "author": [
      {
        "name": "Hao Yu",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-010/",
    "title": "The grid graphics package",
    "description": "\"The grid graphics package\" published in R News.",
    "author": [
      {
        "name": "Paul Murrell",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-011/",
    "title": "Lattice",
    "description": "\"Lattice\" published in R News.",
    "author": [
      {
        "name": "Deepayan Sarkar",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-012/",
    "title": "Programmer’s Niche",
    "description": "\"Programmer’s Niche\" published in R News.",
    "author": [
      {
        "name": "Bill Venables",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-013/",
    "title": "geoRglm: A package for generalised linear spatial models",
    "description": "\"geoRglm: A package for generalised linear spatial models\" published in R News.",
    "author": [
      {
        "name": "Ole F. Christensen",
        "url": {}
      },
      {
        "name": "Paulo J. Ribeiro",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-014/",
    "title": "Querying PubMed",
    "description": "\"Querying PubMed\" published in R News.",
    "author": [
      {
        "name": "Robert Gentleman",
        "url": {}
      },
      {
        "name": "Jeff Gentry",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-015/",
    "title": "Evd: Extreme value distributions",
    "description": "\"Evd: Extreme value distributions\" published in R News.",
    "author": [
      {
        "name": "Alec Stephenson",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-016/",
    "title": "Ipred: Improved predictors",
    "description": "\"Ipred: Improved predictors\" published in R News.",
    "author": [
      {
        "name": "Andrea Peters",
        "url": {}
      },
      {
        "name": "Torsten Hothorn",
        "url": {}
      },
      {
        "name": "Berthold Lausen",
        "url": {}
      }
    ],
    "date": "2002-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-001/",
    "title": "Reading foreign files",
    "description": "\"Reading foreign files\" published in R News.",
    "author": [
      {
        "name": "Duncan Murdoch",
        "url": {}
      }
    ],
    "date": "2002-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-002/",
    "title": "Maximally selected rank statistics in R",
    "description": "\"Maximally selected rank statistics in R\" published in R News.",
    "author": [
      {
        "name": "Torsten Hothorn",
        "url": {}
      },
      {
        "name": "Berthold Lausen",
        "url": {}
      }
    ],
    "date": "2002-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-003/",
    "title": "Quality control and early diagnostics for cDNA microarrays",
    "description": "\"Quality control and early diagnostics for cDNA microarrays\" published in R News.",
    "author": [
      {
        "name": "Günther Sawitzki",
        "url": {}
      }
    ],
    "date": "2002-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-004/",
    "title": "Bioconductor",
    "description": "\"Bioconductor\" published in R News.",
    "author": [
      {
        "name": "Robert Gentleman",
        "url": {}
      },
      {
        "name": "Vincent Carey",
        "url": {}
      }
    ],
    "date": "2002-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-005/",
    "title": "AnalyzeFMRI: An R package for the exploration and analysis of MRI and fMRI datasets",
    "description": "\"AnalyzeFMRI: An R package for the exploration and analysis of MRI and fMRI datasets\" published in R News.",
    "author": [
      {
        "name": "Jonathan Marchini",
        "url": {}
      }
    ],
    "date": "2002-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2002-006/",
    "title": "Using R for the analysis of DNA microarray data",
    "description": "\"Using R for the analysis of DNA microarray data\" published in R News.",
    "author": [
      {
        "name": "Sandrine Dudoit",
        "url": {}
      },
      {
        "name": "Yee Hwa Yang",
        "url": {}
      },
      {
        "name": "Ben Bolstad",
        "url": {}
      }
    ],
    "date": "2002-03-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2002-006/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 1441,
    "preview_height": 1080
  },
  {
    "path": "articles/RN-2001-018/",
    "title": "Porting R to Darwin/X11 and Mac OS X",
    "description": "\"Porting R to Darwin/X11 and Mac OS X\" published in R News.",
    "author": [
      {
        "name": "Jan Leeuw",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-019/",
    "title": "RPVM: Cluster statistical computing in R",
    "description": "\"RPVM: Cluster statistical computing in R\" published in R News.",
    "author": [
      {
        "name": "Michael Na Li",
        "url": {}
      },
      {
        "name": "A. J. Rossini",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-020/",
    "title": "Strucchange: Testing for structural change in linear regression relationships",
    "description": "\"Strucchange: Testing for structural change in linear regression relationships\" published in R News.",
    "author": [
      {
        "name": "Achim Zeileis",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-021/",
    "title": "Programmer’s Niche: Macros in R",
    "description": "\"Programmer’s Niche: Macros in R\" published in R News.",
    "author": [
      {
        "name": "Thomas Lumley",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-022/",
    "title": "More on spatial data",
    "description": "\"More on spatial data\" published in R News.",
    "author": [
      {
        "name": "Roger Bivand",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-023/",
    "title": "Object-oriented programming in R",
    "description": "\"Object-oriented programming in R\" published in R News.",
    "author": [
      {
        "name": "John M. Chambers",
        "url": {}
      },
      {
        "name": "Duncan Temple Lang",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-024/",
    "title": "In search of C/C++ & FORTRAN routines",
    "description": "\"In search of C/C++ & FORTRAN routines\" published in R News.",
    "author": [
      {
        "name": "Duncan Temple Lang",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-025/",
    "title": "Support vector machines",
    "description": "\"Support vector machines\" published in R News.",
    "author": [
      {
        "name": "David Meyer",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-026/",
    "title": "A primer on the R-Tcl/Tk package",
    "description": "\"A primer on the R-Tcl/Tk package\" published in R News.",
    "author": [
      {
        "name": "Peter Dalgaard",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2001-026/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 92,
    "preview_height": 73
  },
  {
    "path": "articles/RN-2001-027/",
    "title": "Wle: A package for robust statistics using weighted likelihood",
    "description": "\"Wle: A package for robust statistics using weighted likelihood\" published in R News.",
    "author": [
      {
        "name": "Claudio Agostinelli",
        "url": {}
      }
    ],
    "date": "2001-09-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-011/",
    "title": "Date-time classes",
    "description": "\"Date-time classes\" published in R News.",
    "author": [
      {
        "name": "Brian D. Ripley",
        "url": {}
      },
      {
        "name": "Kurt Hornik",
        "url": {}
      }
    ],
    "date": "2001-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-012/",
    "title": "Installing R under Windows",
    "description": "\"Installing R under Windows\" published in R News.",
    "author": [
      {
        "name": "Brian D. Ripley",
        "url": {}
      }
    ],
    "date": "2001-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": "articles/RN-2001-012/preview.png",
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {},
    "preview_width": 501,
    "preview_height": 383
  },
  {
    "path": "articles/RN-2001-013/",
    "title": "geoR: A package for geostatistical analysis",
    "description": "\"geoR: A package for geostatistical analysis\" published in R News.",
    "author": [
      {
        "name": "Paulo J. Ribeiro, Jr.",
        "url": {}
      },
      {
        "name": "Peter J. Diggle",
        "url": {}
      }
    ],
    "date": "2001-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-014/",
    "title": "Simulation and analysis of random fields",
    "description": "\"Simulation and analysis of random fields\" published in R News.",
    "author": [
      {
        "name": "Martin Schlather",
        "url": {}
      }
    ],
    "date": "2001-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-015/",
    "title": "Mgcv: GAMs and generalized ridge regression for R",
    "description": "\"Mgcv: GAMs and generalized ridge regression for R\" published in R News.",
    "author": [
      {
        "name": "Simon N. Wood",
        "url": {}
      }
    ],
    "date": "2001-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-016/",
    "title": "What’s the point of “tensor”?",
    "description": "\"What’s the point of “tensor”?\" published in R News.",
    "author": [
      {
        "name": "Jonathan Rougier",
        "url": {}
      }
    ],
    "date": "2001-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-017/",
    "title": "On multivariate t and Gauß probabilities in R",
    "description": "\"On multivariate t and Gauß probabilities in R\" published in R News.",
    "author": [
      {
        "name": "Torsten Hothorn",
        "url": {}
      },
      {
        "name": "Frank Bretz",
        "url": {}
      },
      {
        "name": "Alan Genz",
        "url": {}
      }
    ],
    "date": "2001-06-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-001/",
    "title": "Under new memory management",
    "description": "\"Under new memory management\" published in R News.",
    "author": [
      {
        "name": "Luke Tierney",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-002/",
    "title": "On exact rank tests in R",
    "description": "\"On exact rank tests in R\" published in R News.",
    "author": [
      {
        "name": "Torsten Hothorn",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-003/",
    "title": "Porting R to the Macintosh",
    "description": "\"Porting R to the Macintosh\" published in R News.",
    "author": [
      {
        "name": "Stefano M. Iacus",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-004/",
    "title": "The density of the non-central chi-squared distribution for large values of the noncentrality parameter",
    "description": "\"The density of the non-central chi-squared distribution for large values of the noncentrality parameter\" published in R News.",
    "author": [
      {
        "name": "Peter Dalgaard",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-005/",
    "title": "Connections",
    "description": "\"Connections\" published in R News.",
    "author": [
      {
        "name": "Brian D. Ripley",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-006/",
    "title": "Using databases with R",
    "description": "\"Using databases with R\" published in R News.",
    "author": [
      {
        "name": "Brian D. Ripley",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-007/",
    "title": "Rcgi 4: Making web statistics even easier",
    "description": "\"Rcgi 4: Making web statistics even easier\" published in R News.",
    "author": [
      {
        "name": "M. J. Ray",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-008/",
    "title": "Omegahat packages for R",
    "description": "\"Omegahat packages for R\" published in R News.",
    "author": [
      {
        "name": "John M. Chambers",
        "url": {}
      },
      {
        "name": "Duncan Temple Lang",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-009/",
    "title": "Using XML for statistics: The XML package",
    "description": "\"Using XML for statistics: The XML package\" published in R News.",
    "author": [
      {
        "name": "Duncan Temple Lang",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  },
  {
    "path": "articles/RN-2001-010/",
    "title": "Programmer’s Niche",
    "description": "\"Programmer’s Niche\" published in R News.",
    "author": [
      {
        "name": "Bill Venables",
        "url": {}
      }
    ],
    "date": "2001-01-01",
    "categories": [],
    "contents": "\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-07T21:31:42+00:00",
    "input_file": {}
  }
]
