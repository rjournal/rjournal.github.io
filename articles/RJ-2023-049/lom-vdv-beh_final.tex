% !TeX root = RJwrapper.tex
\title{Three-Way Correspondence Analysis in R} 
\author{by Rosaria Lombardo, Michel van de Velden and Eric J. Beh}

\maketitle

\abstract{
Three-way correspondence analysis is a suitable multivariate method for visualising the association in three-way categorical data,  modelling the global dependence, or reducing dimensionality.
This paper provides a description of an R package for performing three-way correspondence analysis: \pkg{CA3variants}. The functions in this package allow the analyst to perform several variations of this analysis, depending on the research question being posed and/or the properties underlying the data. Users can opt for the classical (symmetrical) approach or the non-symmetric variant - the latter is particularly useful if one of the three categorical variables is treated as a response variable. In addition, to perform the necessary three-way decompositions, a Tucker3 and a trivariate moment decomposition (using orthogonal polynomials) can be utilized. The Tucker3 method of decomposition can be used when one or more of the categorical variables is nominal while for ordinal variables the trivariate moment decomposition can be used. The package also provides a function that can be used to choose the model dimensionality.
}

\section{Introduction}
In many applications, one encounters problems where detecting and describing the association between three categorical variables is of interest. For example, one may wish to analyse animal counts stratified by species-by-site-by-time, treatment success stratified by cure-by-therapy-by-hospital, customer satisfaction-by-service's quality-by-country, or two interacting genes in expression under the genotypes of another gene. One method specifically designed for analysing such data is three-way correspondence analysis \cite[]{carkro96}. For this method of analysis, a three-way contingency table is decomposed in such a way that the maximum amount of association is reflected in a low-dimensional display. Depending on the underlying data, and the research questions being asked, there are various ways to quantify and decompose the association in the table, generate a visual display of the association and calculate the accompanying numerical summaries. Hence, several variants of three-way correspondence analysis exist. Common among all the variants that we describe below is the emphasis that is placed on data exploration through the visualization of the associations. 

There exists a sizable body of literature that examines the various theoretical properties and extensions of three-way correspondence analysis. For example, \citet{kro89}, \citet[]{carkro96}, \citet[Chap. 17]{kro08}, \citet[Chap. 11]{behlom14} and \citet{lombehkro21} discuss a wide range of issues concerned with this technique. However, there also appears to be only a few applications that use these techniques \cite[]{carkro98, vel07,lombehgue19}. One reason for the lack of applications could be the absence of R software packages  to perform three-way correspondence analysis. 

In this paper, we introduce \pkg{CA3variants}, a comprehensive R package that allows researchers to apply variants of three-way correspondence analysis. In Section~\ref{s.2}, we introduce the notation that we adopt as well as two key measures of association - Pearson's three-way phi-squared statistic and Marcotorchino's three-way index. These measures lie at the core of the three-way correspondence analysis variants that we describe below. In Section~\ref{s.3} we present three methods for decomposing a three-way contingency table,  with a particular focus on the appropriateness of the different variants. In Section~\ref{s.4} we show how the two association measures above,  can be partitioned in bivariate and trivariate association terms, and how can be used to define variants of three-way correspondence analysis, and we consider specific issues concerned with the visualization and selection of the dimensionality of the three-way correspondence analysis solution. 
In Section~\ref{s.5}, we briefly review the software that is currently available for three-way analyses. In Section~\ref{s.6}, we introduce our three-way correspondence analysis package, \pkg{CA3variants}, and illustrate its features and application through some illustrative examples. Some final comments are left for Section~\ref{s.7}.

 


%----------------------------------------------------------------------
\section{Measures of three-way association}
\label{s.2}

%----------------------------------------------------------------------

Three-way correspondence analysis provides a numerical and graphical summary of how categories and variables are related to one another. Rather than only considering the bivariate associations between pairs of variables, three-way correspondence analysis also considers the trivariate associations \citep{lombehkro21}.  

When performing three way correspondence analysis, the dependence structure of the three categorical variables that are cross-classified to form a contingency table is analysed by considering an appropriate measure of association. This measure can then be partitioned to reveal more detail about the nature of the association that exists between the variables. Two measures of association are implemented in the \pkg{CA3variants} package: Pearson's phi-squared statistic and Marcotorchino's index. Pearson's three-way statistic is appropriate when studying deviations from three-way independence and when the variables are symmetrically associated, while Marcotorchino's three-way index is a more suitable choice when the variables are not symmetrically associated. 
Depending on the choice of the measure of association used, the appropriately scaled three-way table can be decomposed into (low-dimensional) components for each of the variables. Before discussing these three-way decomposition methods, we first introduce the notation used throughout this paper. We then provide a brief description of Pearson's phi-squared statistic and Marcotorchino's index. 
%----------------------------
\subsection{Notation}
\label{s.notation}

Suppose we have data from a sample of {\it n} subjects on three categorical variables. Such data can be represented by a three-way contingency table consisting of $I$ rows, $J$ columns and $K$ tubes, where each cell value represents the count within an intersection of the levels of each of the three variables. 

Denote $\mathbf{\underline{N}}$ to be the contingency table of order ${I \times J \times K}$ belonging to the space $\Re^{ I\times J \times K}$, subscripted by $i$, $j$ and $k$ for $i=1,\, \ldots \,,\, I$, $j = 1,\, \ldots \,,\, J$, $k = 1,\, \ldots \,,\, K$,  whose $\left(i,\,j,\,k \right)$th term is $n_{ijk}$, while $\mathbf{ \underline{P}}$ is the table of joint relative frequencies of $\mathbf{\underline{N}}$ whose $\left(i,\,j,\,k \right)$th term is $p_{ijk} = n_{ijk} /n$, such that $\sum_{i=1}^I \sum_{j=1}^J \sum_{k=1}^K p_{ijk} = 1$. Define $p_{i\bullet \bullet} = \sum_{j=1}^J \sum_{k=1}^K p_{ijk}$, $p_{\bullet j  \bullet} = \sum_{i=1}^I \sum_{k=1}^K p_{ijk}$, $p_{\bullet \bullet k} = \sum_{i=1}^I \sum_{j=1}^J p_{ijk}$, $p_{ij \bullet} = \sum_{k=1}^K p_{ijk}$, $p_{i \bullet k} = \sum_{j=1}^J p_{ijk}$ and $p_{\bullet jk} = \sum_{i=1}^I p_{ijk}$ to be the univariate and bivariate marginal relative frequencies of the three-way contingency table. In addition, define $\mathbf{I}_I$ to be the identity matrix of order ${I\times I}$ in the space $\Re^I$, and let $\mathbf{ D}_I$, $\mathbf{D}_J$, $\mathbf{D}_K$ be the diagonal matrices containing the univariate marginal relative frequencies in $\Re^I$, $\Re^{ J}$ and $\Re^{ K}$ whose general term is $p_{i \bullet \bullet}$, $p_{\bullet j  \bullet}$ and $p_{\bullet \bullet k}$, respectively.

%========================================================================
\subsection{Pearson's three-way statistic}
%========================================================================

When the association between the categorical variables of a three-way contingency table, $\mathbf{\underline{N}}$, is considered to be symmetric, we can analyse the strength of this association using  Pearson's three-way phi-squared statistic 
\begin{eqnarray}
\label{phi}
\Phi^2 & = &\sum_{i=1}^I\sum_{j=1}^J\sum_{k=1}^K p_{i \bullet \bullet }p_{\bullet j\bullet }p_{\bullet \bullet k}\left ( \frac{p_{ijk }-p_{i\bullet \bullet }p_{\bullet j\bullet }p_{\bullet \bullet k}}{p_{i \bullet \bullet }p_{\bullet j\bullet }p_{\bullet \bullet k}} \right )^2\\
& = & \sum_{i=1}^I\sum_{j=1}^J\sum_{k=1}^K p_{i \bullet \bullet }p_{\bullet j\bullet }p_{\bullet \bullet k}\left( \frac{p_{ijk}} {p_{i \bullet \bullet }p_{\bullet j\bullet }p_{\bullet \bullet k}} -1 \right)^2  \nonumber \\
& = &\sum_{i=1}^I\sum_{j=1}^J\sum_{k=1}^K p_{i \bullet \bullet }p_{\bullet j\bullet }p_{\bullet \bullet k} \left(\pi_{P_{ijk}}\right)^2 . \nonumber
\end{eqnarray}

The symmetric nature of this measure implies that the three variables are all treated as predictor variables. That is, none are deemed to be dependent on the outcome of any other variable being studied. It can be shown that, under the independence assumption,  $\Phi^2$ can be partitioned as 
\begin{eqnarray}
\label{AnovaChi}
\Phi^2  &=& \sum_{i=1}^I\sum_{j=1}^J p_{i \bullet \bullet }p_{\bullet j\bullet }  \left (\frac{p_{ij\bullet }-p_{i\bullet \bullet }p_{\bullet j\bullet }}{p_{i \bullet \bullet }p_{\bullet j\bullet }} \right )^2
 + \sum_{i=1}^I\sum_{k=1}^K p_{i \bullet \bullet }p_{\bullet \bullet k} \left (\frac{p_{i\bullet k}-p_{i \bullet \bullet }p_{\bullet \bullet k}}{p_{i \bullet \bullet }p_{\bullet \bullet k}} \right )^2 \\
& + & \sum_{j=1}^J\sum_{k=1}^Kp_{\bullet j\bullet}p_{\bullet \bullet k}\left (\frac{p_{\bullet jk}-p_{\bullet j\bullet}p_{\bullet \bullet k}}{p_{\bullet j\bullet}p_{\bullet \bullet k}} \right )^2
 + \sum_{i=1}^I\sum_{j=1}^J\sum_{k=1}^K p_{i \bullet \bullet }p_{\bullet j\bullet }p_{\bullet
\bullet k} \left (\frac{p_{ijk}-_{\alpha}p_{ijk}}{p_{i \bullet \bullet }p_{\bullet j\bullet }p_{\bullet
\bullet k}} \right )^2 \,,  \nonumber
\end{eqnarray}
{
\noindent where 
\begin{eqnarray}
{_{\alpha}\hat{p}_{ijk}}=\hat{p}_{ij\bullet}\hat{p}_{\bullet \bullet k}+\hat{p}_{i\bullet k}\hat{p}_{\bullet j \bullet }+ \hat{p}_{\bullet j k}\hat{p}_{i\bullet  \bullet}-2\hat{p}_{i\bullet  \bullet}\hat{p}_{\bullet j \bullet}\hat{p}_{\bullet  \bullet k}\, .
\end{eqnarray}
}
\noindent {For further details see  \cite{carkro96} and \cite{lomtakbeh20}.  Briefly, we get} 
\begin{eqnarray}
\label{phi2_decomp}
\Phi^2 = \Phi_{IJ}^2 + \Phi_{IK}^2 + \Phi_{JK}^2 + \Phi_{IJK}^2 \,.
\end{eqnarray}

\noindent Observe that this partition also concerns Pearson’s chi-squared statistic, $X^2$, \cite[]{lan51,lomtakbeh20} obtained by multiplying each
of the terms of phi-squared  in equation (\ref{phi2_decomp}) by the sample size, $n$. Indeed, Pearson’s chi-squared statistic is well established for testing association between variables in contingency tables. Hence, deviations from three-way independence can be  orthogonally partitioned into three deviations from independence (for each of the two-way tables formed by summing over each variable of the three-way contingency table) and a three-way association term, as it  will be shown in Section \ref{s.nominalexample}.
This partition has been extensively discussed by \citet{carkro96} and more recently by \citet[Chap. 17]{kro08}, \citet{loi16} and \citet{lomtakbeh20}. \\


%-------------------------------------------------------------------------------
\subsection{Marcotorchino's three-way index}
\label{s:marco}
%-------------------------------------------------------------------------------
If the three categorical variables are non-symmetrically associated, or if one is interested in exploring an non-symmetric association between the variables, 
a more appropriate measure is the three-way Marcotorchino index. This index is defined by
\begin{eqnarray}
\label{indexMarco}
\tau_{M} & = & \frac{\sum_{i=1}^I \sum_{j=1}^J \sum_{k=1}^K p_{\bullet j \bullet }p_{\bullet \bullet k} \left(\frac{p_{ijk}}{p_{\bullet j \bullet}p_{\bullet \bullet k}} - p_{i \bullet\bullet} \right)^2}{1-\sum_{i=1}^I p_{i\bullet\bullet}^2} \,.
\end{eqnarray}

See, for example,  \citet{mar84a, mar84b}, \citet{lom96}, \citet{beh07}, \citet[Section 11.4.2]{behlom14} and \citet[Section 7.5]{behlom21b}. Since the denominator of equation~(\ref{indexMarco}) is independent on the cell values of $\mathbf{\underline{ N}}$, the numerator of the Marcotorchino index suffices as a measure of association when performing three-way correspondence analysis. 
This numerator measures the absolute increase in predictability of the response variable, given the predictor variables \cite[]{mar85,lom96}. Like Pearson's three-way phi-squared statistic, Marcortorchino's index is based on deviations from the three-way independence model. 
Without loss of generality,  assume that the row variable is considered to be dependent on the column and tube variables. In doing so, the numerator of equation (\ref{indexMarco}), which we shall simply refer to as Marcotorchino's  $\tau_{M_{num}}$ statistic, is equal to
\begin{eqnarray}
\label{Marco}
\tau_{M_{num}} & = & \sum_{i=1}^I \sum_{j=1}^J \sum_{k=1}^K p_{\bullet j \bullet }p_{\bullet \bullet k} \left(\frac{p_{ijk}}{p_{\bullet j \bullet}p_{\bullet \bullet k}} - p_{i \bullet\bullet} \right)^2\\
&=&\sum_{i=1}^I \sum_{j=1}^J \sum_{k=1}^K p_{\bullet j \bullet }p_{\bullet \bullet k} \left(\pi_{M_{ijk}}\right)^2. \nonumber
\end{eqnarray}

As in the symmetric case, an additive orthogonal partition of  $\tau_{M_{num}}$  exists and is given by
\begin{eqnarray}
\label{Mp}
\tau_{M_{num}}  & = & \sum_{i=1}^I \sum_{j=1}^J p_{\bullet j \bullet} \left(\frac{p_{ij \bullet}}{p_{\bullet j \bullet}} - p_{i \bullet \bullet} \right)^2 + \sum_{i=1}^I \sum_{k=1}^K p_{\bullet \bullet k} \left(\frac{p_{i \bullet k}}{p_{\bullet \bullet k}} - p_{i \bullet\bullet}\right)^2 \nonumber \\
& & \hspace{10mm} +\frac{1}{I}\sum_{j=1}^J \sum_{k=1}^K p_{\bullet j \bullet} p_{\bullet \bullet k} \left(\frac{p_{\bullet jk} - p_{\bullet j\bullet} p_{\bullet \bullet k}} {p_{\bullet j \bullet} p_{\bullet \bullet k}}\right)^2 \nonumber \\
& & \hspace{20mm} + \sum_{i=1}^I \sum_{j=1}^J \sum_{k=1}^K p_{\bullet j\bullet}p_{\bullet \bullet k} \left(\frac{p_{ijk} - _\alpha p_{ijk}}{p_{\bullet j\bullet}p_{\bullet \bullet k}}\right)^2 \,,
\end{eqnarray}
{
\noindent where
\begin{eqnarray*}
{_{\alpha}p_{ijk}}=\hat{p}_{ij\bullet} \hat{p}_{\bullet \bullet k} + \hat{p}_{i \bullet k} \hat{p}_{\bullet j \bullet} +\frac{\hat{p}_{\bullet jk}}{I} - \hat{p}_{i\bullet \bullet } \hat{p}_{\bullet j\bullet} \hat{p}_{\bullet \bullet k} - \hat{p}_{\bullet j \bullet} \frac{\hat{p}_{\bullet \bullet k}}{I}. \nonumber
\end{eqnarray*}
}
\noindent {The partition of $\tau_{M_{num}}$  may be more simply expressed as }
\begin{eqnarray}
\label{taupartition3}
\tau_{M_{num}} = \tau_{IJ} + \tau_{IK} + \tau_{JK} + \tau_{IJK}.
\end{eqnarray}

\noindent Hence, like Pearson's three-way phi-squared statistic, $\tau_{M_{num}}$ (and hence the total predictability measure $\tau_{M}$) is partitioned into four additive terms. The first three of these terms reflect the two-way associations and the fourth term reflects the three-way association. The first two bivariate terms of equation (\ref{taupartition3}) are equal to the numerators of the Goodman-Kruskal indices \citep{goo54} between the response (row) variable and each of the two predictor (column and tube) variables, respectively. These terms are also equal to the inertias of the marginal two-way tables in classical two-way non-symmetric correspondence analysis \citep{lau84,dam89,kro99,tak08}. The third bivariate term of (\ref{taupartition3}) is (up to the constant $1/I$) equal to $\Phi^2_{JK}$, which is Pearson's phi-squared statistic for the $J\times K$ contingency table formed by aggregating over the row categories. This term can be seen as a measure of the symmetric association between the two predictor variables. Finally, the last term of equation (\ref{taupartition3}) is a  measure of the trivariate association between the variables. %See also \citet[p.459]{behlom14}.
\citet{beh07} showed that the test statistic associated with Marcotorchino's three way index is the generalization of the $C$-statistic \citep{lig71}, referred to here as the $C_M$-statistic, and is defined by
\begin{eqnarray}
\label{eq:CM}
      C_M = \left(n-1\right) \left(I-1 \right) \tau_{M}\sim \chi^{2}_{\alpha, df} .
\end{eqnarray}

%\noindent For further details of the $C_M$-statistic see Beh and Lombardo (2014, Section 11.5.2).
\noindent Therefore, for both Pearson's three-way chi-squared statistic and Marcotorchino's three-way $\tau_M$ statistic, under the null hypothesis of complete independence, each term of the partition is a chi-squared random variable. For further details see \citet[]{lig71}, \citet[]{beh07}, \citet[Section 11.5.2]{behlom14} and \citet[Section 7.5.2]{behlom21b}.


%===========================================================================
\section{Decomposing three-way tables}
\label{s.3}
%=====================================================================
The choice of which measure of association to use should be made based on the data at hand and the research question under investigation. Depending on the choice, an appropriately scaled matrix can be constructed. Three-way correspondence  analysis can then be performed and involves fitting a model to the data. In particular, low-dimensional component matrices as well as a core matrix that links the different components, are fitted to the data in such a way that the sum-of-squares of the deviations between the low-dimensional approximation and the original table is as small as possible.  

Several decomposition models have been proposed in the literature for three-way contingency tables. In the \pkg{CA3variants} package three types of decomposition are implemented. They are the Tucker3 model \cite[]{tuc63,kro83,kro08,kie92} for when all three variables are nominal,  the trivariate moment decomposition \cite[]{lomkrobeh16,lombehkro21} for when all three variables are ordinal, and a hybrid decomposition for a mix of nominal and ordinal categorical variables \cite[]{lombeh17}.  In the following subsections, we briefly review these decomposition methods and how they apply to the different variants of three-way correspondence analysis. 

%======================================================================
\subsection{Tucker3 decomposition for three-way tables}
\label{s.31}
%======================================================================

For the Tucker3 decomposition, a three-way matrix $\underline{\mathbf{X}}$ with elements $x_{ijk}$ is decomposed such that
\begin{eqnarray*}
x_{ijk} & = &\sum_{p=1}^{P}\sum_{q=1}^{Q}\sum_{r=1}^{R}g_{pqr}a_{ip}b_{jq}
c_{kr} + e_{ijk} \,,
\end{eqnarray*}

\noindent where ${P, Q}$ and $R$ ($P \leq I$, $Q \leq J$, $R \leq K$) are the fixed number of the components corresponding to the row, column and tube variables,  respectively. The $a_{ip}$, $b_{jp}$ and $c_{kp}$ values are elements of the column matrices $\mathbf{A}$, $\mathbf{B}$ and $\mathbf{C}$, respectively, and give component loadings for the row, column and tube variables, while $g_{pqr}$ is an element of the $P \times Q \times R$  core array. 
 The term $e_{ijk}$ is the error of approximation. By ``flattening'' the three-way matrix $\underline{\mathbf{X}}$ -- for example, by concatenating the $K$ tubes of $\underline{\mathbf{X}}$ -- we can write the Tucker3 decomposition in matrix form by
\begin{eqnarray}
	\label{eq:Tucker3}
	\textrm{Tucker3}\left(\mathbf{X}\right) = \mathbf{A G}\left(\mathbf{ B}^{T} \otimes \mathbf{C}^{T}\right) + \mathbf{E} \,,
\end{eqnarray}

\noindent where $\mathbf{X}$ and ${\bf G}$ are, respectively, the $I\times JK$ matrix of (flattened) data values and the $P\times QR$ matrix of core elements. 

The solution to ${\mathbf{A}}$, ${\mathbf{B}}$, ${\mathbf{C}}$ and ${\mathbf{G}}$ is obtained by minimizing the sum-of-squares of the elements of $\mathbf{E}$ (matrix of the errors of approximation) using an alternating least-squares algorithm. The general framework of the algorithm that \pkg{CA3variants} uses is based on the Tuckals3 alternating least squares algorithm discusssed by \citet{krodel80} and \citet{kro83,kro94}.


\subsubsection{Symmetric three-way correspondence analysis}

For symmetric three-way correspondence analysis, the elements of Pearson's three-way phi-squared statistic are decomposed using a Tucker3 decomposition. In particular, the Tucker3 decomposition is applied to the appropriately scaled three-way array $\underline{\bm{\Pi}_{P}}$ with elements 
\begin{eqnarray}
	\label{eq:PiP}
	\pi_{P_{ijk}}  = \frac{p_{ijk}}{p_{i \bullet \bullet }p_{\bullet j\bullet }p_{\bullet \bullet k}}-1 \,,
\end{eqnarray}
where the component matrices, $\mathbf{A}$, $\mathbf{B}$ and $\mathbf{C}$ are constrained to be orthonormal with respect to the diagonal matrices of univariate marginal relative frequencies such that

\begin{eqnarray}
 \label{constraints}
 \mathbf{A}^T\mathbf{D}_I \mathbf{A} = \mathbf{I}_P\,, \hskip1em \mathbf{B}^T\mathbf{D}_J  \mathbf{B} = \mathbf{I}_Q\,, \hskip1em \text{and} \hskip1em \mathbf{C}^T\mathbf{D}_K  \mathbf{C} = \mathbf{I}_R\,.
\end{eqnarray}

Note that the weighted sum-of-squares of the elements of $\pi_{P_{ijk}}$ is equal to Pearson's three-way phi-squared statistic; see equation (\ref{phi}). In other words, the symmetric variant of three-way correspondence analysis amounts to minimizing the weighted squared differences between the standardized deviations of independence in the three-way table with the approximated values using the Tucker3 model. That is:
\[
\sum_{i=1}^I \sum_{j=1}^J \sum_{k=1}^K p_{i \bullet \bullet} p_{\bullet j \bullet} p_{\bullet \bullet k} \left(\pi_{P_{ijk}} - \widehat{\pi}_{P_{ijk}} \right)^2 \,,
\]
is minimized where, for some value of $P$, $Q$ and $R$
\[
\widehat{\pi}_{P_{ijk}}= \sum_{p=1}^{P}\sum_{q=1}^{Q}\sum_{r=1}^{R}
g_{pqr}a_{ip}b_{jq}c_{kr} \,.
\]
The constraints of equation (\ref{constraints}) are similar to the constraints used in  the traditional approach to simple (two-way) correspondence analysis. Consequently, symmetric three-way correspondence analysis can be seen as a direct extension of the traditional two-way correspondence analysis approach. For more details see, for example, \citet{carkro96}.


\subsubsection{Non-symmetric three-way correspondence analysis}
For non-symmetric three-way correspondence analysis, one variable needs to be selected as the response variable. In the following discussion we choose, without loss of generality, the first (row) variable to serve as the response variable. When performing  non-symmetric three-way correspondence analysis, we use the Tucker3 decomposition to decompose  Marcotorchino's three-way $\tau_{M_{num}}$ statistic defined by equation (\ref{Marco}).  Let $\underline{\bm{\Pi}_{M}}$ represents the three-way matrix with elements 
\begin{eqnarray}
	\label{eq:PiM}
	\pi_{M_{ijk}} = \frac{p_{ijk }}{p_{\bullet j\bullet }p_{\bullet \bullet k}}-p_{i \bullet \bullet } \,.
\end{eqnarray}

Non-symmetric three-way correspondence analysis is then performed by applying the Tucker3 decomposition to $\underline{\bm{\Pi}_{M}}$ where the components contained in the row, column and tube matrices ${\bf A}$, ${\bf B}$ and ${\bf C}$, are constrained to be orthornormal with respect to the weight matrices ${\bf I}_I$, ${\bf D}_J$, ${\bf D}_K$. That is, 
 \[
\mathbf{A}^T \mathbf{A} = \mathbf{I}_P\,, \hskip1em \mathbf{B}^T\mathbf{D}_J  \mathbf{B} = \mathbf{I}_Q \,, \hskip1em \text{and}\hskip1em \mathbf{C}^T\mathbf{D}_K  \mathbf{C} = \mathbf{I}_R \,.
\]

Note that, for the decomposition of $\underline{\bm{\Pi}_{M}}$, these constraints 
ensure that the weighted quadratic norm of the low-dimensional approximation $\underline{\widehat{\bm \Pi}_M}$, can be written as
\[
\| {\underline{\widehat{\bm \Pi}_M}}\|^{2} = \tau_{M_{num}} =  \sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R g^2_{pqr} \,.
\]



%=====================================================================
\subsection{Trivariate moment decomposition}
\label{s.32}
%=====================================================================
Rather than considering component matrices as the Tucker3 decomposition does, the trivariate moment decomposition is based on column matrices consisting of orthogonal polynomials. The decomposition was first proposed by \citet[Chap. 7]{beh98phd} and has since been described by, for example, \citet{behdav98}, \citet{lomkrobeh16} and \citet[eq.~10]{lombehkro21}, as an alternative method of three-way decomposition. It is particularly useful when a variable consists of ordered categories, either increasing or decreasing. The decomposition can be applied to either $\underline{\bm{\Pi}_{P}}$ or $\underline{\bm{\Pi}_{M}}$ and allows the researcher to incorporate the ordinality by replacing the Tucker3 components with the orthogonal polynomials for the ordinal variable. These polynomials are typically generated using the three-term recurrence formulae of \citet{eme68} who demonstrated their computational efficiency when compared with the Gram-Schmidt orthogonalization process. Refer to \cite{beh97, beh98, beh98phd} and \cite{behlom21a} for a definition and properties of these polynomials when performing correspondence analysis.

To form the polynomial basis space we generate as many orthogonal polynomials as there are ordered categories. 
The matrix of row, column and tube orthogonal polynomials is denoted by  $\bm{\mathcal{A}}=\{\alpha_{iu}\}$, (for $i = 1, \, \ldots \,,\, I$ and $u = 0,\, \ldots \,,\,I - 1$), $\bm{\mathcal{B}}=\{\beta_{jv}\}$ (for $j = 1, \, \ldots \,,\, J$ and $v = 0, \, \ldots \,,\, J - 1$) and $\bm{\mathcal{C}}=\{\gamma_{kw}\}$ (for $k = 1, \, \ldots \,,\, K$ and $w = 0, \, \ldots \,,\, K - 1$), respectively. Like the Tucker3 components, when a symmetric variant of three-way correspondence analysis is performed, the row polynomials are orthogonal with respect to the marginal relative frequencies $p_{i\bullet \bullet}$, while the column and tube polynomials are orthogonal with respect to $p_{\bullet j \bullet}$ and $p_{\bullet \bullet k}$, respectively.
In general, the first polynomial that is computed for each ordered variable of the three-way table represents the \textit{zeroth-order} polynomial and is equal to $1$ when in its normalized state.
The second polynomial is the \textit{first-order} polynomial and reflects the variation in the linearity of the categories.
The third polynomial is the  \textit{second-order} orthogonal polynomial and reflects the variation in the dispersion of the categories.
Higher-order polynomials represent higher-order moments of the ordered categories. These polynomials have been used extensively in the correspondence analysis literature. For more information, see, for example, \citet{beh97,beh98}, \citet[p. 94]{behlom14}, \citet{lombehkro16} and \citet[Chap. 4]{behlom21b}.

When using the trivariate moment decomposition for symmetric and non-symmetric three-way correspondence analysis, the decomposition of the arrays $\underline{\bm{\Pi}_{P}}$ and $\underline{\bm{\Pi}_{M}}$ is defined by replacing the matrices of components $\mathbf{A}$, $\mathbf{B}$ and  $\mathbf{C}$  (see equation ~(\ref{eq:Tucker3})) with their orthogonal polynomial equivalents. In particular, for the non-symmetric case and given the different row weights, we consider $\bm{\alpha}^*_{u} = p^{1/2}_{i\bullet \bullet} \bm{\alpha}_{u}$, $\bm{\beta}_{v}$ and $\bm{\gamma}_{w}$ such that
\begin{eqnarray}
 {\pi_{M_{ijk}}} & = &  \sum_{u=0}^{U} \sum_{v=0}^{V} \sum_{w=0}^{W}  \tilde{z}_{uvw} \alpha^*_{iu} \beta_{jv} \gamma_{kw}  \, .
\label{eq:deco2}
\end{eqnarray}

\noindent For the decomposition given by equation (\ref{eq:deco2}), the row polynomials are weighted such that $\sum_{i=1}^{I} \alpha^{*2}_{iu}=1$ while the column and tube polynomials are weighted so that $\sum_{j=1}^{J} p_{\bullet j \bullet}\beta^{2}_{jv}=1$ and $\sum_{k=1}^{K} p_{\bullet \bullet k}\gamma^{2}_{kw}=1$, respectively. Note that the indices $u,v,w$ are from 0 to $U$, $V$ and $W$ (where $U\leq I-1$, $V\leq J-1$, $W\leq K-1$), respectively, and correspond to the orders of the polynomials. The $\tilde{z}_{uvw}$ value in equation (\ref{eq:deco2}) is analogous to the core element $g_{pqr}$ in the nominal case and is therefore referred to as the \textit{polynomial core element} and is defined by

\[
\tilde{z}_{uvw}=\sum_{i=1}^I\sum_{j=1}^J\sum_{k=1}^K  {\pi}_{M_{ijk}}p_{\bullet j\bullet}p_{\bullet\bullet k}\alpha^*_{iu}\beta_{jv}\gamma_{kw} \,,
\] 

\noindent and is of order $\left(u,\,v,\,w\right)$. Such a term has also been referred to as a {\it generalized correlation}. See, for example, \cite{raybeh09}, \citet[Chap. 6]{behlom14} and \citet[Chap. 5]{behlom21b}. Observe that, unlike the Tucker3 decomposition given by equation (\ref{eq:Tucker3}), the trivariate moment decomposition has a closed form that justifies the absence of the error of approximation in equation (\ref{eq:deco2}).
%--------------------------------------------------------------------------------------------
\subsection{Hybrid decomposition for nominal and ordinal variables}
\label{hybrid}
%--------------------------------------------------------------

The hybrid decomposition involves computing  Tucker3 components for the nominal variables, and  orthogonal polynomials for the ordinal variables \citep{lombeh17,lombehkro21}. Generally for the analysis of three-way contingency tables, we distinguish the following two cases: 1) there are two ordinal variables and one nominal variable, and 2) there are two nominal variables and only one ordinal variable. Suppose we consider the case where we have a three-way contingency table in which the row and column variables are ordinal and the tube variable is nominal. Then the \textit{hybrid decomposition}, for case 1, involves calculating the polynomials for the row and column variables and the Tucker3 components for the nominal tube variable. When the row variable is treated as a response variable, three-way non-symmetric correspondence analysis can be performed using the hybrid decomposition of $\pi_{M_{ijk}}$ such that
\begin{eqnarray}
 \pi_{M_{ijk}} & =  & {\widehat{\pi}_{M_{ijk}}}+ e_{ijk} \nonumber \\ & = & \sum_{u=0}^{U} \sum_{v=0}^{V} \sum_{r=1}^{R}  {z}_{uvr} \alpha^*_{iu} \beta_{jv} c_{kr} + e_{ijk} \,.
\label{eq:deco3}
\end{eqnarray}
\noindent Here $\bm{\alpha}^*_u$ and $\bm{\beta}_v$ are the $u$th order row and $v$th order column polynomials, respectively, while $\bm{c}_r$ is the $r$th tube (Tucker3) component. The value of ${z}_{uvr}$ in equation (\ref{eq:deco3}) is defined by
\[
z_{uvr}=\sum_{i=1}^I\sum_{j=1}^J\sum_{k=1}^K  {\pi}_{M_{ijk}}p_{\bullet j\bullet}p_{\bullet\bullet k}\alpha^*_{iu}\beta_{jv} c_{kw} \,,
\]
and is referred to as the {\it hybrid core element} of order $\left(u, \, v, \, r\right)$. While the number of orthogonal polynomials for the rows and columns should always be equal to the number of categories that define the variable (see Section~\ref{s.32}),  the number of Tucker3 components for the tube variable can be smaller ($R\leq K$). 
\noindent A complete orthogonal decomposition is always used when all the three variables are ordered, as it is for equation~(\ref{eq:deco2}), but is seldom used in practice when  the variables are not all ordered. 
Like the Tucker3 decomposition (see equation (\ref{eq:Tucker3})) and unlike the trivariate moment decomposition (see equation (\ref{eq:deco2})), the hybrid decomposition given by equation (\ref{eq:deco3}) includes the error of approximation, $e_{ijk}$, because the decomposition no longer has a closed form solution because of the presence of the Tucker3 components.

%--------------------------------------------------------------------------
\section{Three-way correspondence analysis variants}
\label{s.4}

%\subsection {Three-way variants}
%\label{s.4.1}
%--------------------------------------------------------------------------

Combining the two measures of three-way association described in Section~\ref{s.2} with the three methods for decomposing three-way tables outlined in Section~\ref{s.3} gives four variants of three-way correspondence analysis: 
\begin{itemize}
	\item Symmetric three-way correspondence analysis: this analysis is based on the partition of Pearson's three-way phi-squared statistic and the Tucker3 decomposition of $\underline{\bm{\Pi}_{P}}$. It executes three-way correspondence analysis by treating all variables symmetrically and corresponds to the analysis described by \cite{carkro96}. 
	\item Non-symmetric three-way correspondence analysis: this corresponds to partitioning Marcotorchino's three-way statistic and applies a Tucker3 decomposition to $\underline{\bm{\Pi}_{M}}$. In this analysis, one of the three variables is treated as a response variable and the other two are treated as predictor variables \citep{lom96}. 
	\item Ordered symmetric three-way correspondence analysis: for this analysis, either the trivariate moment decomposition (if all variables are ordinal) or the hybrid decomposition (if one or two of the three variables are ordinal) is applied to $\underline{\bm{\Pi}_{P}}$ leading to the partition of Pearson's three-way phi-squared statistic \citep{lombehkro21}. 
	\item Ordered non-symmetric three-way correspondence analysis: this analysis is based on the trivariate moment decomposition (if all variables are ordinal) or the hybrid decomposition (if one or two of the three variables are ordinal) of $\underline{\bm{\Pi}_{M}}$ and leads to the partition of Marcotorchino's three-way index. Hence, in this analysis one variable is treated as the response variable and the other two are treated as predictor variables.  
\end{itemize}

These four variants of three-way correspondence analysis are incorporated into the \pkg{CA3variants} package. 


%--------------------------------------------------------------------------
\subsection {Visualizing three-way correspondence analysis solutions}
\label{s.4.2}
%--------------------------------------------------------------------------

Like the traditional approach to two-way correspondence analysis, visualization in three-way correspondence analysis is an important feature and helps to provide a descriptive analysis of the data. To visually display the (symmetric or non-symmetric) association that exists among the variables we consider the {\it interactive biplot} \cite[]{carkro96}, also called as {\it nested biplot} by \citet[p. 441]{kro08}. In the interactive biplot, the categories of one variable, referred to as a {\it reference variable}, are jointly visualized with all pair-wise combinations of the categories of the other two variables. Hence, depending on the choice of reference variable, we can distinguish three different {\it interactive coordinates}: row-column, row-tube and column-tube interactive coordinates. 

To see how this works, and why the resulting visualizations are indeed biplots, note that all four three-way correspondence analysis variants described in Section~\ref{s.4} yield three sets of ``coordinates'' (one for each variable), as well as an array of core elements that describe the strength of association between these values. Differences between variants can be described in terms of the different measures of association under consideration (i.e., Pearson's three-way phi-squared statistic or Marcotorchino's index), the orthogonalization constraints adopted, or the type of decomposition (i.e., Tucker3, trivariate moment decomposition or hybrid decomposition) used. 

Recall that the general form of the Tucker3 decomposition is given by equation~(\ref{eq:Tucker3}). As we described above, this matrix formulation is based on a ``flattened'' version of the three-way matrices that involves the concatenation of the categories of a variable. In fact, the concatenation of a variable that leads to the $P \times Q \times R$  approximation can be seen in the following three ways
\begin{align}
\label{eq:3biplots}
\mathbf{X}_{JK,I}=\left(\mathbf{ B} \otimes \mathbf{C} \right) \mathbf{G}_1 \mathbf{A}^{T} \nonumber \\
\mathbf{X}_{IK,J}=\left(\mathbf{ A} \otimes \mathbf{C} \right) \mathbf{G}_2 \mathbf{B}^{T}  \\
\mathbf{X}_{IJ,K}=\left(\mathbf{ A} \otimes \mathbf{B} \right) \mathbf{G}_3 \mathbf{C}^{T} \,. \nonumber
%\hspace{.1cm}
\end{align}

Note that these arrangements have no influence on the approximated values of $\underline{\bf X}$. The subscripted and flattened $\mathbf{G}$'s indicate that, although their elements are the same, the organization differs between them. Each of the formulations in equation (\ref{eq:3biplots})  constitutes a biplot. To show this, suppose we consider the decomposition of $\mathbf{X}_{JK,I}$. Then the rows of $\left(\mathbf{B} \otimes \mathbf{C} \right) \mathbf{G}_1$ are the principal coordinates of the pair-wise combinations of columns and tubes categories of $\underline{\bf X}$. Hence, plotting these jointly with the row standard coordinates contained in the rows of ${\bf A}$ provides the analyst with a  biplot interpretation of the association. For an extensive discussion of biplot interpretations in the context of correspondence analysis see, for example, \citet{gre10} and \citet[Chapters 7 and 8]{gow11}. For a more general treatment of data visualizations in dimension reduction methods, see \cite{gow14}.

For each approximation in equation (\ref{eq:3biplots}), the interactive coordinates can be expressed in either their standard or principal form (whose features are the same of those derived for biplots in the classical approach to correspondence analysis) and so leads to two types of {\it interactive biplots}:
\begin{itemize}
	\item For the first type of interactive biplot, we can factorize each equation in such a way that the categories for the non-interactive variable are displayed using standard coordinates so that they are orthonormal with respect to the appropriate metric. Therefore, observing the combination of categories from the other two variables (which constitutes the ``interactive'' structure of the categories) are defined using principal coordinates \cite[][p. 273]{kro08}. Algebraically, this choice simply means that the interactive coordinates are a form of principal coordinates. They are calculated from the Kronecker product of two component matrices (for example, ${\bf B}$ and ${\bf C}$) multiplied by the appropriate $\mathbf{G}$ matrix (for example, $\mathbf{G}_1$). When displaying the standard coordinates of the non-interactive variable, the points are often displayed as a projection from the origin to their position defined by their standard coordinate.
	\item For the second type of interactive biplot, the $\mathbf{G}$ matrix is applied to the non-interactive variable. Hence, the categories for this variable are displayed in terms of their principal coordinates  while the coordinates corresponding to the combination of categories from the other two variables (i.e., the interactive coordinates) are depicted as standard coordinates \cite[]{lombehkro21}.
\end{itemize}

%--------------------------------------------------------------------------------------------------------------------------
\subsection{Selecting the number of components}
%--------------------------------------------------------------------------------------------------------------------------
The three-way decompositions described in Section~\ref{s.3} require a chosen number of components ($P$, $Q$ and $R$) for each of the variables of $\mathbf{\underline{N}}$.
A common approach is to consider various solutions for the components, resulting in different values of dimensionality (i.e., values of $P$, $Q$ and $R$) and then inspect their appropriateness using a goodness-of-fit (or a lack-of-fit) measure with respect to the degrees-of-freedom of the approximation obtained from these solutions. By increasing the number of components the model becomes more complex but the goodness-of-fit of the model improves. Hence, by considering a goodness- (or lack-) of-fit measure for different model complexities, the trade-off between model fit and model complexity can be assessed. 

Unfortunately, there is no ``best'' way to determine the optimal trade-off between model fit and model complexity. Often, the choice of what dimensionality to select is made by visually inspecting a plot of the goodness-of-fit against the degrees-of-freedom of the model. One such plot is a scree-like plot and selecting the desired dimensionality is made by using a variety of strategies including simply looking for an ``elbow''. One may also select the dimensionality by observing where the ``elbow'' lies in the lower boundary of the convex hull \cite[]{krooor03,murkro03,kro08}. Scree-like plots can also be considered by using a measure of goodness- (or lack-) of-fit on the y-axis and the degrees of freedom (or the number of free parameters) on the x-axis. In this case, the analyst selects a model on or close to the ``elbow'' near the upper boundary of the convex hull \cite[]{timkie00,ceukie06}.

To aid in the visual detection of an ``elbow'' in the convex hull, \citet[]{ceukie06} introduce the $st$-criterion which looks at the smallest angle on the convex hull and allows one to choose a model on the higher (lower) boundary of the convex hull, with the best balance of goodness-(or lack-) of-fit and df (or free parameters). Given the goodness-fit-value, $f$, and the model complexity-value, $df$, the $st$ criterion for a model of dimensionality $l$ can be written as 
\begin{eqnarray}
st(l)=\left(\frac{f(l)-f(l-1)}{df(l)-df(l-1)}\right)/\left(\frac{f(l+1)-f(l)}{df(l+1)-df(l)}\right) \, .
\label{eq:st}
\end{eqnarray}
\noindent The number of models to consider when constructing the convex hull depends on the choice of dimensionality, $l$, made, since there are as many models available to consider as there are combinations of the three dimensions. 

In addition to evaluating the goodness-of-fit for the different models, it may also be insightful to asses how stable models of certain dimensionalities are. This can be done by first applying re-sampling procedures to the three-way tables and then considering the resulting convex hulls. Several ways to facilitate such an assessment have been implemented in the \pkg{CA3variants} package. More details of the relevant functions and options can be found in Section \ref{s.6}.


%---------------------------------------------------------------------------
\section{Related software}
\label{s.5}
%--------------------------------------------------------------------------

Currently, there are no packages available in R devoted to three-way correspondence analysis. However,  the R packages \pkg{PTAk}  \citep{lei10},  \pkg{ThreeWay}  \citep{gio14}, \pkg{rTensor}  \citep{li18}, \pkg{multiway} \citep{eil19}, \pkg{psych} \citep{rev18}, \pkg{tensorA} \citep{sta18}, \pkg{mvoutlier} \citep{zho19} and \pkg{irlba} \citep{hof17} can be used to perform several different three-way decompositions, including the Tucker3 decomposition. An overview of the areas of data analysis that these packages cover is summarised in Table \ref{tab:pack}.

A complete three-way methods program is also available in Pieter Kroonenberg's Fortran package 3WayPack and includes functionality to perform a multi-way correspondence analysis; see  \url{http://three-mode.leidenuniv.nl/} of the  {\it The Three-Mode Company}. Similarly, an extensive collection of three-way methods and decomposition tools are available for MATLAB through the N-Way Toolbox \cite[]{bro2020}. However, while these packages can be used to calculate solutions for the three-way correspondence analysis variants based on the Tucker3 decomposition, doing so requires some non-trivial data preparation and output processing steps.

\begin{table}[h!]
\setlength{\tabcolsep}{2pt}
\small{
\caption{R packages for three-way data analysis. CA3: symmetric three-way correspondence analysis; NSCA3: non-symmetric three-way correspondence analysis; OCA3: ordered symmetric three-way correspondence analysis; ONSCA3: ordered non-symmetric three-way correspondence analysis; PCA3: three-way principal component analysis} \label{tab:pack}
\begin{center}
\begin{tabular}{rrrrrr}
\toprule
\multicolumn{1}{r}{} & \multicolumn{5}{c}{\centering{\textit{Three-way Data Analysis}}}  \\
\midrule
\multicolumn{1}{r}{\textit{package}}&\multicolumn{1}{r}{CA3}&\multicolumn{1}{r}{ NSCA3}&\multicolumn{1}{r}{OCA3}&\multicolumn{1}{r}{ONSCA3}& \multicolumn{1}{r}{PCA3}\\ \midrule
\pkg{CA3variants} & x &  x & x &x  &  \\
\pkg{ThreeWay} &   &    &  &  &x \\
\pkg{PTAk} & x  &    &   &  &x\\
\pkg{rTensor} &   &    &  &  &x \\
\pkg{multiway} &   &    &  &  &x \\
\pkg{psych} &   &    &  &  &x \\
\pkg{tensorA} &   &    &  &  &x \\
\pkg{mvoutlier} &   &    &  &  &x \\
\pkg{irlba} &   &    &  &  &x \\
\bottomrule
\end{tabular}
\end{center}
}
\end{table}
The R package \pkg{CA3variants} provides a straightforward way to perform the different variants of three-way correspondence analysis described above on a three-way contingency table. Moreover, in addition to the Tucker3 variants of three-way correspondence analysis, the package also allows for the {application of trivariate moment decomposition and hybrid decomposition methods, suitable when variable categories are ordered.}


%----------------------------------------------------------------------------
\section{CA3variants: Package description and examples}
\label{s.6}
%----------------------------------------------------------------------------
%\subsection{A description of the CA3variants package}

In this section, we introduce the main functions, arguments and options available in the \pkg{CA3variants} package. These functions are  \code{tunelocal()} and \code{CA3variants()}.

The \code{tunelocal()} function can be used to determine an appropriate number of dimensions in the approximation of $\underline{\bm{\Pi}_{P}}$ or $\underline{\bm{\Pi}_{M}}$, while the  function \code{CA3variants()}   can be used to perform all four methods described in Section~\ref{s.4}. Some similarities and differences  of these four methods are summarized in Table~\ref{ca3variants}.

The \code{CA3variants()} and \code{tunelocal()} functions return S3 objects from which the \code{plot()}, \code{print()} and \code{summary()} functions are available.  Note that both functions require the input arguments \code{Xdata}, \code{ca3type}, \code{resp} and \code{norder}. Respectively, these arguments specify the three-way data, the type of analysis being performed (which can be chosen from those outlined in Section~\ref{s.4}), the response variable (in the case of a non-symmetric variant) and the number of ordinal variables (when an ordered variant is performed). \code{Xdata} can be a three-way table, or an ($n\times 3$) data matrix where the rows represent the $n$ observations/objects and the $3$ columns correspond to three categorical variables, i.e. the row, column and tube variables (the levels/categories of each variable are given by integer numbers).

The \code{tunelocal()} function can help the user to choose an appropriate number of dimensions  for any variant of three-way correspondence analysis. A list detailing the fit of all of the models considered can be obtained using \code{print(tune.out)}; here \code{tune.out} is the output object produced using the \code{tunelocal()} function. This function  considers the decompositions of the original data for all triplets of  dimensions. The stability of the fit of the solutions for different dimensionalities can also be assessed by adding arguments related to the implementation of three resampling schemes (when \samp{boots = TRUE} and  \samp{nboots = 100}). The available schemes are a non-parametric bootstrap resampling method or a parametric bootstrap method using one of two  distributions (multinomial or Poisson). The parametric bootstrap can be considered as a {\it simple} parametric bootstrap  (\samp{boottype = "bootpsimple"} ) when the row, column and tube marginals are fixed to equal those of the original three-way table. Alternatively, it can be performed using a  stratified parametric bootstrap method (\samp{boottype = "bootpstrat"}) where the row and column marginals are fixed for each  tube (for $k= 1,...,K$) of the original three-way table. 

Differently from \code{tunelocal()},  another important argument of \code{CA3variants()} is  \code{dims}.  The argument \code{dims} defines the dimensionality of the solution which can be driven by first using \code{tunelocal()}. 
The available variants for \code{ca3type} are: 

\begin{itemize}
	\item \code{ca3type = "CA3"} for symmetric three-way correspondence analysis. This option is appropriate when all variables are assumed, or known, to be nominal and symmetrically associated. This is also the default analysis that is performed.
	\item \code{ca3type = "NSCA3"} for non-symmetric three-way correspondence analysis. This option is appropriate when one of the variables is defined as the response variable which can be chosen by specifying  \code{resp = "row"} (the default choice), \code{resp = "column"} or \code{resp = "tube"}. All three variables are treated as being nominal.
	\item \code{ca3type = "OCA3"} for three-way ordered symmetric correspondence analysis. This option is appropriate when at least one of the three variables consists of ordered categories.
	\item \code{ca3type = "ONSCA3"} for three-way ordered non-symmetric correspondence analysis. This option is appropriate when at least one of three variables consists of ordered categories and one of the variables is defined as the response variable. The analyst can specify the response variable in the same way that the response variable is defined for non-symmetric three-way correspondence analysis (see \code{ca3type = "NSCA3"}).
\end{itemize}

\begin{table}[h!] %[h!]
%\setlength{\tabcolsep}{2pt}
\small{
	\raggedleft
	\begin{tabularx}{\textwidth}{l X X X}
		\hline
		{\bf Method}         &    {\bf Variables}& {\bf Association} &{\bf Decomposition method}\\ 
		\hline
		\code{ca3type = "CA3"}            &    nominal  & symmetric & Tucker3 \\                                     
		\code{ca3type = "NSCA3"}           &   nominal & non-symmetric & Tucker3 \\
		\code{ca3type = "OCA3"}             &   ordinal  & symmetric & Trivariate moment  \\
		\code{ca3type = "ONSCA3"}        &     ordinal  & non-symmetric & Trivariate moment  \\ 
\code{ca3type = "OCA3"}             &   one or two variables are ordinal & symmetric & Hybrid \\
		\code{ca3type = "ONSCA3"}        &  one or two variables are ordinal  & non-symmetric & Hybrid \\ 
								\hline
	\end{tabularx} 
	\caption {Similarities and differences of three-way correspondence analysis methods in \code{CA3variants()}. 
		\label{ca3variants}}
}
\end{table} 


Finally, the package contains four example data sets that can be used to test and benchmark the different methods, all with varying features and variable structures. They are: \code{happy} - a $4\times6\times4$ contingency table with $n = 40323$ -  \cite[]{dav77}, \code{happyNL} - a $4\times 5 \times 4$ contingency table with $n = 1669$ - (from the European Social Survey of 2016, http://www.europeansocialsurvey.org/), \code{museum} - a $253\times 3$ data matrix with $n = 253$ - (from a 2019 survery promoted by the University  ``Luigi Vanvitelli'', Italy), and \code{ratrank}  - a $9\times 9\times 5$ contingency table with $n = 44568$ - \cite[]{vel07}. In Section~\ref{s.62} we illustrate the package by performing a NSCA3 on the data set \code{happyNL}, while Sections \ref{s.nominalexample} and \ref{s.hybridexample} consider two symmetric analyses of the \code{ratrank} data set (a nominal three-way correspondence analysis and a hybrid three-way analysis).


%--------------------------------------------------------------------------------------------------------------------
\subsection{Three-way symmetric correspondence analysis: Ranking and rating data}
\label{s.nominalexample}
%--------------------------------------------------------------------------------------------------------------------
%\subsubsection{Overview of the data}

The dataset \code{ratrank} is one of the four datasets included in the \pkg{CA3variants} package. It is a data array of size $9\times 9 \times 5$ that is formed from the cross-classification of the {\it Rating} (row), {\it Ranking} (column), and {\it Country} (tube) variables, and was analyzed by \cite{vel07}. 

Participants from five European countries were asked to rate and rank the same nine values taken from the list of values (LOV) described by \citet{kah83}. For each of these European countries, a contingency table was constructed with counts of co-occurrences of rating numbers and rankings. The ranking task required participants to provide a strict ranking of the items. In the rating task, participants are asked to provide ratings (on a 9 point scale) to the same items. It gives the participants the freedom to rank the items in any way they desire, however,  it is also open to response tendencies.  Such tendencies can be referred to as response styles. For example,  some individuals may be more inclined to use extreme ratings (lowest or highest) where others only use middle ratings to express their preferences. The observed correspondence between the ratings and rankings could then be used to inspect response tendencies and, in this study, to relate such tendencies to nationalities. For more details on the data and the theory underlying the response tendencies, we refer to the \cite{vel07}. Our objective here is to illustrate the application of the \pkg{CA3variants} package by reproducing some of the results published in their paper.  
After downloading and installing \pkg{CA3variants} from the Comprehensive R Archive Network (CRAN), we load the package:
\begin{example}
 library("CA3variants")
\end{example}

\subsubsection{Dimensionality of the solution}

We use the \code{tunelocal()} function to determine an appropriate triplet of dimensions:

\begin{example}
 tune.ca3.out <- tunelocal(ratrank, ca3type = "CA3")
 print(tune.ca3.out)
 plot(tune.ca3.out)
\end{example}
The function \code{tunelocal()} yields an object containing goodness-of-fit measures, model complexity and, when \code{boots = TRUE}, the bootstrap samples used. However, using \code{print(tune.ca3.out)} we show the following numerical results  for the models  on the boundary:

\begin{example}
#> # Convex hull (upper bound)

#> # Selected model(s):
#>           complexity      fit
#> c(2, 2, 1)          1 17726.27


#> All models on upper bound:
#>            complexity      fit        st
#> c(1, 6, 1)          0 14265.56        NA
#> c(2, 2, 1)          1 17726.27 16.097440
#> c(3, 3, 1)          4 18371.23  3.116993
#> c(3, 3, 2)         12 18923.00  1.233506
#> c(3, 4, 2)         17 19202.58  1.846595
#> c(3, 4, 3)         28 19535.67  1.650064
#> c(4, 4, 3)         39 19737.53  1.291062
#> c(4, 4, 4)         54 19950.73  1.538958
#> c(5, 5, 4)         88 20264.76  1.368834
#> c(6, 6, 4)        130 20548.15  1.497295
#> c(7, 7, 4)        180 20773.47        NA
\end{example}


\begin{figure}[h]
	\begin{center}
{\includegraphics*[width = 1 \textwidth]{ca3tune.pdf}}
		\caption{\label{fig:ca3tune} Model fit versus complexity for three-way nominal CA of the \code{ratrank} data.}
	\end{center}
\end{figure}

The numerical and graphical output of \code{tune.ca3.out} show that an appropriate triplet of dimensions is $\left(2,\, 2,\, 1\right)$. 
Note that Figure \ref{fig:ca3tune} is generated when using \code{plot(tune.ca3.out)}. While cluttered, as the result of the large number of triplets that can be considered in the analysis of \code{ratrank},  Figure \ref{fig:ca3tune} also shows that an appropriate dimensionality of the solution is  $\left(2,\,2,\,1\right)$.
Each point in Figure \ref{fig:ca3tune} corresponds to a combination of dimensions. The y-axis gives the goodness-of-fit measure for each model which we use as the criterion for choosing the most appropriate dimensionality for the solution. More complex models that involve higher dimensionalities (or, equivalently, higher degrees of freedom) have a better fit. The red line outlines the convex hull where models on this line are superior to higher dimensional options with a similar fit. For example, in Figure~\ref{fig:ca3tune}, for the models that lie below the red line there is typically an alternative, less complex, model that achieves the same fit. Alternatively, there is also an equally complex model having a better fit. In three-way correspondence analysis, like other dimension reduction methods, users can factor in subjective criteria (such as interpretability) when selecting the dimensionality of a model. Here, in accordance with  \cite{ceukie06}, we follow the {\it st} criterion and select the model, marked green in Figure \ref{fig:ca3tune}, with two dimensions for the row ({\it Rating}) and column ({\it Ranking}) variables and one for tube ({\it Country}) variable.

\subsubsection{Numerical summary of the association}

By following the analysis described in \cite{vel07} we perform a symmetric three-way correspondence analysis, the default method, on the \code{ratrank} data.  Following the output of the tunelocal function and in accordance with \cite{vel07}, we specify two dimensions for the row ({\it Rating}) and column ({\it Ranking}) categories, and a single dimension for the tube ({\it Country}) categories. This can be achieved by:  
\begin{example}
 ca3.out <- CA3variants(ratrank, dims = c(2, 2, 1))
\end{example}

The \code{print()} function returns several key measures of association that are included in this output. These include the percentage of explained inertia along each dimension, the partition of Pearson's three-way chi-squared and phi-squared statistics into four terms (see equation (\ref{phi2_decomp})),  the corresponding degrees of freedom, the p-value, and the relative sizes of each term of the partition (allowing for comparisons between chi-squared values from different, asymptotic chi-squared distributions): 
\begin{example}
 print(ca3.out)

#> # Percentage contributions of the components to the total inertia for column-tube
biplots

#>     p1     p2 
#> 67.008 16.347 

#> # Percentage contributions of the components to the total inertia for row-tube 
biplots

#>     q1     q2 
#> 67.008 16.347 

#> # Percentage contributions of the components to the total inertia for row-column 
biplots

#>     r1 
#> 83.355 

#> #  Index partition

#>                Term-IJ Term-IK Term-JK Term-IJK Term-total
#> Chi-squared  18359.272 589.605 254.629 2062.404  21265.910
#> Phi-squared      0.412   0.013   0.006    0.046      0.477
#> % of Inertia    86.332   2.773   1.197    9.698    100.000
#> df              64.000  32.000  32.000  256.000    384.000
#> p-value          0.000   0.000   0.000    0.000      0.000
#> X2/df          286.864  18.425   7.957    8.056     55.380

\end{example}

This output shows that the Pearson chi-squared statistic of \code{ratrank} is 21265.91 and, with a p-value that is less than 0.0001,  there is a statistically significant association between at least two of the variables of the data set. Further insight into the nature of the association can be obtained from the terms of the partition of the overall chi-squared. The output shows that the most dominant source of association exists between the {\it Rating} and {\it Ranking} variables (\code{Term-IJ}), and contributes to 18359.27, or 86.33\%, of the total association among the three variables. The association between the {\it Rating-Country} variables (\code{Term-IK}) and {\it Ranking-Country} variables (\code{Term-JK}) accounts for relatively little in comparison (2.77\% and 1.20\%, respectively),  but are still statistically significant sources of association. The association among all three variables (\code{Term-IJK}) contributes to the remainder (or nearly 10\%) of the association between the variables. Further information about the nature of the association can be obtained visually by performing a correspondence analysis. 

\subsubsection{Visual summary of the association}

To reproduce the results from \cite{vel07}, we consider here the row-tube ({\it Rating - Country}) interactive biplot, so that the interactive row-tube points are plotted using principal coordinates. This biplot is given by Figure~\ref{fig_ratrank} and is produced from the command:

\begin{example}
 plot(ca3.out, biptype = "row-tube", addlines = F)
\end{example}


\begin{figure}[b!]
	\begin{center}
		{\includegraphics*[width=1.1\textwidth]{ratrank.pdf}}
		%\vspace{-30mm}
		\caption{\label{fig_ratrank} Interactive row-tube biplot for \code{ratrank}. The column points ({\it Ranking} categories) are depicted using standard coordinates and are labeled as ``rank1'' to ``rank9''. The interactive row-tube points ({\it Rating-Country} categories) are depicted using principal coordinates and are labelled by the rating number (1 to 9) followed by the first letter of the country: France (F), Germany (G), Italy (I), Spain (S) and the United Kingdom (U).}
	\end{center}
\end{figure}

By default, the \code{plot()} function uses a straight line  from the origin to each standard coordinate to depict the non-interactive variable.
However, with so many points in Figure~\ref{fig_ratrank}, adding  projection lines for each of the nine {\it Ranking} categories leads to a cluttered plot. Hence, and in accordance with \cite{vel07}, we use \code{addlines = F} to remove the lines. 
Furthermore, to control  the size of the points and their labels,  the \code{plot()} function uses two arguments \code{size1} and \code{size2} (for the points and labels, respectively); by default \code{size1 = 1} and \code{size2 = 3}. 

Finally, to avoid any further clutter of points close to the origin, a scaling argument can be used that helps to reveal important features of the association without impacting the approximation. The default for this scaling argument, which was applied here, is set such that the average sum of squares for the two sets of points is the same, and is thus in accordance with the recommendations given by \cite{goweretal2010} and \cite{vel17}. 
This default can be overwritten by specifying a value for the \code{scaleplot} argument in the \code{plot()} function. Note that, except for this scaling, the biplot given by Figure~\ref{fig_ratrank} is identical to Figure~1 in \cite{vel07}.

Figure~\ref{fig_ratrank} shows that the highest value rank (``rank9'') generally receives the highest possible rating (``9'') across all five countires. However, for the second highest value rank (``rank8'') the ratings tend to vary from 4 to 8, showing some heterogeneity in how ``rank8'' is perceived in terms of the {\it Rating} categories. For the lowest valued rank (``rank1''), we see a clear association with the lowest rating (``1''). However, this level of rating is also often linked to items that received a rank of ``2''. Moreover, for the items that receive a rank of ``1'' up to ``7'', we see that individuals tend to assign to them a rating of between ``1'' and ``3'' (inclusive). Finally, each of the ratings appear rather homogeneous across all five countries. However, with ratings from Germany being consistently furthest from the origin, and those from the United Kingdom being closest to the origin, these {\it Country} categories provide, relatively speaking, the strongest and weakest (respectively), contribution to the association. See \cite{vel07} for a more in-depth analysis and explanation of this analysis. 
%================================================================
\subsection{Three-way non-symmetric correspondence analysis: Happiness data}
\label{s.62}
%================================================================

%\subsubsection{Overview of the data}

Since 1977, the study of the relationship between happiness, household characteristics and education using data obtained from social survey data has received a great deal of attention. For an analysis of this data set, see, for example,  \cite{dav77}, \cite{clog82}, \cite{behdav98} and \citet[Chap.17]{kro08}. Davis' data set  \citet{dav77} examines the association between happiness, number of siblings and years of schooling completed of 1517 individuals  and is included in \pkg{CA3variants} as \code{happy}. \citet[Chap.17]{kro08} studied Davis' data by performing a symmetric three-way correspondence analysis. 

Following on from those studies mentioned above,  we analyze a three-way contingency table, obtained from the 2016 European Social Survey (http://www.europeansocialsurvey.org/). It involves a sample of  $1669$ respondents from the Netherlands and investigates the association between their reported level of {\it Happiness}, the  level of {\it Education} and the number of people in their {\it Household}. As an illustration of one of the three-way variants implemented in the \pkg{CA3variants} package, we now turn our attention to performing the non-symmetric variant of three-way correspondence analysis.  

\subsubsection{Defining the variables}

To assess the level of {\it Happiness} of a respondent, people were asked to reply to the question:

\begin{quote}
\textit{``Taking all things together, how happy would you say you are?"}	
\end{quote}

Responses were made on a scale from 1 (``extremely unhappy'') to 10 (``extremely happy''). After observing the distribution of counts in the data, we re-coded these scales into the following four categories of {\it Happiness}: {``low''} (for ratings  $<6$), {``middle''} (for ratings between $6-7$), {``high''} (for a rating equal to $8$),  and {``very-high''} (for ratings $>8$). 

The {\it Education} variable is defined using four categories. These are {``Less than lower secondary education''} (coded {``ED1''}), {``Lower secondary education completed''} (coded {``ED2''}), {``Upper secondary education completed''} (coded {``ED3''}) and {``Post-secondary and/or  tertiary  education completed''} (coded {``ED45''}). 

Finally, for the {\it Household} variable, the respondents were asked to reply to the question:
\begin{quote}
\textit{``Including yourself, how many people - including children - live here regularly as members of this household?"}
\end{quote}

The four categories from this question were defined as follows: a one person household is coded {``HS1''}, a two person household is coded {``HS2''}, a three person household is coded {``HS3''}, a four person household is coded {``HS4''}, a five person household is coded {``HS5'} and a household containing more than five people is coded {``>HS5''}. 

The cross-classification of the {\it Happiness}, {\it Education} and {\it Household} variables forms a three-way contingency table which has been included in the package with the object name \code{happyNL}.

For our analysis of this contingency table, we consider the non-symmetric three-way correspondence analysis variant with the row variable ({\it Happiness}) treated as the response variable, and the column ({\it Education}) and tube ({\it Household}) variables defined as the predictor variables. 
 

\subsubsection{Dimensionality of the solution}
\label{s:dimhappy}
Before performing a three-way NSCA on \code{happyNL} we first need to determine the dimensionality of the solution. This can be done by comparing the fit and complexity of models of different dimensionality using the \code{tunelocal()} function.  For this example, we consider decompositions applied to 100 resampled data tables (using the parametric bootstrap; the default), and calculate, for each triplet of dimensions, the mean goodness of fit over the bootstrap samples.  Note that, by doing so, the overall number of estimated models equals $I\times J\times K \times nboots = 80\times 100=8000$. All resampled data tables are collected in the object named `XG'  of the output of the \code{tunelocal()} function:
\begin{example}
tune.nsca3.out <- tunelocal(happyNL, ca3type = "NSCA3", resp = "row", boots = T)
plot(tune.nsca3.out)
\end{example}

The resulting plot is given as Figure~\ref{fig.tunelocal-nsca3}. Each point in this plot corresponds to a combination of dimensions. The y-axis gives the goodness-of-fit measure for each model. More complex models, that is those involving higher dimensionalities have a better fit. The red line denotes the convex hull where models on this line are superior to higher dimensional options with a similar fit. For example, in Figure \ref{fig.tunelocal-nsca3}, for the models which are below the red line there is an alternative, less complex, model achieving the same (or similar) fit, or there is an equally complex model having a better fit. In three-way correspondence analysis, as with other dimension reduction techniques, users can factor in subjective criteria such as interpretability when selecting the dimensionality of a model. Here, in accordance with  \cite{ceukie06}, we follow the {\it st} criterion and select the model, marked green in Figure~\ref{fig.tunelocal-nsca3}, with two dimensions for the row ({\it Education}) column ({\it Household}) and tube ({\it Happiness}) variables.

\begin{figure}[h]
	\begin{center}
{\includegraphics*[width=1\textwidth]{figtunelocalnsca3n2.pdf}}
		\caption{\label{fig.tunelocal-nsca3} Model fit versus complexity for three-way NSCA of the happiness bootstrapped data.}
	\end{center}
\end{figure}

Using \code{print(tune.nsca3.out)} we obtain the numerical results (i.e. fit) for the models that lie along the red line in Figure~\ref{fig.tunelocal-nsca3}. The output from this command is:
\begin{example}
#> # Note that when boots = T,  the data samples generated 
#> # are given in the object named 'XG' 

#> # Results for choosing the optimal model dimension

#> # Convex hull (upper bound)

#> # Selected model(s):
#>           complexity      fit
#> c(2, 2, 2)          4 187.6015


#> # All models on upper bound:
#>           complexity      fit       st
#> c(1, 1, 4)          0 111.6055       NA
#> c(1, 2, 2)          1 145.6444 2.433839
#> c(2, 2, 2)          4 187.6015 2.965930
#> c(2, 3, 3)         12 225.3251 1.766711
#> c(3, 3, 3)         20 246.6776 1.382302
#> c(3, 4, 3)         28 262.1246 1.246718
#> c(3, 4, 4)         39 279.1611 1.734889
#> c(3, 5, 4)         50 288.9810 1.190973
#> c(4, 5, 4)         69 303.2229       NA
\end{example}


\subsubsection{Numerical summary of the association}
The \code{CA3variants()} function can be used to perform a three-way non-symmetric correspondence analysis on \code{happyNL} by specifying the arguments of the function so that they define the data table, the dimensionality of the solution, the type of analysis and the response variable. Here, using the suggested dimensions, the analysis is performed so that: 
\begin{example}
 nsca3.out <- CA3variants(happyNL, ca3type = "NSCA3", resp = "row", 
 dims = c(2, 2, 2))
\end{example}
The numerical output from this analysis is obtained using \code{print(nsca3.out)}: 
\begin{example}
 print(nsca3.out)

#> # Percentage contributions of the components to the total inertia for pred biplots

#>   p1     p2 
#> 47.407 22.696 

#> #  Index partition

#>                Term-IJ Term-IK Term-JK Term-IJK Term-total
#> Tau Numerator     0.014   0.005   0.008    0.006      0.032
#> Tau               0.021   0.007   0.012    0.008      0.047
#> % of Inertia     43.162  14.719  24.749   17.371    100.000
#> CM-Statistic    102.583  34.981  58.819   41.285    237.669
#> df               12.000   9.000  12.000   36.000     69.000
#> p-value           0.000   0.000   0.000    0.251      0.000
#> CM-Statistic/df   8.549   3.887   4.902    1.147      3.444
\end{example}

By reducing the dimensionality of the solution to $\left(2, \, 2, \, 2\right)$, the first set of values (\code{p1} and \code{p2}) are the percentages of the total association which is explained by the two axes of a biplot; we will speak more on these two values shortly. The summary of values that follow \code{Index partition} gives the four terms of the partition of the Marcotorchino index, its numerator and its associated test statistic, $C_M$-statistic (see Section \ref{s:marco}). Note that the last column, labeled \code{Term-total} corresponds to the three-way index  being partitioned. Consequently, the seven rows of this output summarize the elements of each term of this partition, including their p-value and their relative sizes (allowing for comparisons between $C_M$-statistic values from different, asymptotic chi-squared distributions).  

The data set \code{happyNL} has a $C_M$-statistic of $237.669$. Its small p-value ($<0.0001$, df = 69) confirms that there is very strong evidence to conclude that the {\it Household} and {\it Education} variables are statistically significant predictors of {\it Happiness}. By partitioning the $C_M$-statistic associated with the Marcotorchino index, we can examine the sources of non-symmetric association that exists in the three-way table. We see that all the bivariate association terms are statistically significant, but not the trivariate association term  (p-value $<0.251$, df = 36) which assesses the increase in predictability of {\it Happiness} given the number of people in a {\it Household} and the highest level of {\it Education} of the participants. 

\subsubsection{Visual summary of the association}

While the trivariate  term from the partition of the $C_M$-statistic is not statistically significant, we shall nonetheless visually explore how people's level of {\it Happiness} is influenced by the number of people in their {\it Household}  and their highest level of {\it Education}. This shall be done by generating an interactive biplot with the interaction of each combination of categories of the predictor variables depicted using principal coordinates and, therefore, setting \code{biptype = "pred"}. Note that there is indeed an ``interaction'' (via a symmetric association) between the two predictor variables since the \code{Term-JK} p-value is less than $<0.0001$. The categories of the response variable are depicted in the biplot using standard coordinates when \code{biptype = "pred"}.

Applying the \code{plot()} function to the \pkg{CA3variants} object  can be used to generate different biplots. A description of some of all available plotting arguments can be found in Table \ref{tab.plot}. However, when a non-symmetric variant is applied to the \pkg{CA3variants} object, a suitable interactive biplot that portrays the non-symmetric association can be obtained using the command:
		
\begin{example}
plot(nsca3.out, biptype = "pred")
\end{example}
which produces the interactive biplot of Figure~\ref{fig.3way.happy-biplot-nsca}. 
Figure~\ref{fig.3way.happy-biplot-nsca} displays  straight lines from the origin to each standard coordinate to depict the non-interactive variable  for the four levels of the {\it Happiness} variable. Such lines are convenient for visualizing how the interactive points relate to the non-interactive points. This is because the proximity of the points from the origin reflect deviations from independence.

\begin{table}[h!]
\small{
	\raggedleft
\begin{tabularx}{\textwidth}{l X}
		\hline
		{\bf Arguments}         &    {\bf Description} \\ 
		\hline
		\code{Xout}            &   The output of \code{CA3variants()}.\\                                     
		\code{biptype}             & Specifies the type of interactive biplot being produced. When \code{ca3type = "CA3"} or \code{= "OCA3"} there are six options: \code{biptype = "row"}, \code{"column"}, \code{"tube"}, \code{"row-column"}, \code{"row-tube"} and \code{"column-tube"}. Each option refers to what is depicted using principal coordinates. For instance, \code{"row"} specifies that the row points are depicted using principal coordinates and, consequently, the interactive column-tube points are depicted using standard coordinates. When \code{ca3type = "NSCA3"} or \code{"ONSCA3"}, there are only two biplot options: \code{biptype = "resp"} or \code{"pred"}. The option  \code{"resp"} specifies that the response categories are depicted using principal coordinates, while the option \code{"pred"} indicates that the interactive predictor points are in principal coordinates.  \\ 
		\code{scaleplot}             &  A biplot scaling argument used to avoid spatial cluttering by pulling points away from the origin. See the  description of the ``gamma scaling'' in \citet[Section 2.3.1]{gow11}. By default, \code{scaleplot} is the overall average of the sum-of-squares of the two sets of coordinates (principal and standard ones), so that the average sum-of-squares for the two sets of points is the same \citep{vel17}.\\ 
%		\code{size1} & 	Specifies the size of the symbols/points.  By default, \code{size1 = 1}.\\
%		\code{size2}  &  Specifies the label size. By default, \code{size2 = 3}. \\
		\code{addlines} & 	Specifies whether the points in standard coordinates are represented using axes. By default, \code{addlines = TRUE}.\\
				\hline
	\end{tabularx} 
	\caption {Summary of important plotting options available in \code{plot.CA3variants()}. For all options use \code{?plot.CA3variants} 
		\label{tab.plot}}
}
\end{table} 


\begin{figure}[h!]
	\begin{center}
		{\includegraphics*[width=1.1\textwidth]{3wayca-biplot-nsca-row.pdf}}
		\caption{\label{fig.3way.happy-biplot-nsca} Interactive biplot from the NSCA3 of \code{happyNL} with {\it Happiness} and {\it Education} the interactive variables}
	\end{center}
\end{figure}


In Figure~\ref{fig.3way.happy-biplot-nsca}, we see that the first dimension accounts for 47\% (rounded to the nearest integer) of the association  between the variables while the second dimension accounts for 23\%. Thus, Figure~\ref{fig.3way.happy-biplot-nsca} captures approximately 70\% of the association  between the three categorical variables (when treated non-symmetrically) of \code{happyNL}. These two percentages are also included as \code{p1} and \code{p2}, respectively, from the numerical summaries included in \code{print(nsca3.out)}. Since Figure~\ref{fig.3way.happy-biplot-nsca} provides a good visual summary of the  non-symmetric association of the variables of \code{happyNL}, we now turn our attention to describing the nature of this association. 
The left side of Figure \ref{fig.3way.happy-biplot-nsca} shows a group of points corresponding to HS1 (a single person household) combined with all levels of education. It shows that respondents tend to exhibit lower levels of happiness when they live alone, regardless of education level. Due to the non-symmetric nature of the association we can also infer that for these single households, the groups with lower levels of education ({HS1ED1} and {HS1ED2}) lead (or help predict) a low, or middle, level of happiness. For those with a higher education, Figure~\ref{fig.3way.happy-biplot-nsca} also suggests that having a higher level of education does not necessarily lead to (or help to predict) a very-high happiness level. Furthermore, respondents in a two person household ({HS2}) tend to be very happy (HS2 is a good predictor of {very-high} levels of happiness), especially for those with a lower level of education ({HS2ED1} and {HS2ED2}). The interactive biplot shows that those with higher levels of education in a two person household are still more associated with a {very-high} level of happiness ({HS2ED3} and {HS2ED45}) but less  than those with less of an education. 

For the large households ({HS4} and {>HS5}), we observe that the effect of education level on happiness appears to be stronger. That is, for these larger households, respondents with a higher ({ED45}) or a middle-high ({ED3}) level of education tend to be more happy ({high} and {very-high}) than people with a lower level of education  ({ED1} and {ED2}). Respondents that live in large households and have a low level of education ({HS4ED2}, {HS4ED1}, {>HS5ED2} and {>HS5ED1}) are not highly happy individuals. Indeed, these interative points are on the opposite side of Figure~\ref{fig.3way.happy-biplot-nsca} to the {high} level of happiness.

%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
\subsection{Ordered three-way correspondence analysis: Ranking and rating data}
\label{s.hybridexample}
%-----------------------------------------------------------------------------------------------------------------------------------------------------------------
In the analysis of \code{ratrank} in Section~\ref{s.nominalexample} we treated the {\it Ranking} and {\it Rating} variables as nominal when they are, in fact, ordinal variables. Using the \code{CA3variants} package we can incorporate this ordinality in the decomposition. In particular, we perform the analysis by treating {\it Country} as a nominal variable and {\it Ranking} and {\it Rating} as ordinal by using the hybrid decomposition described in Section~\ref{hybrid}.

\subsubsection{Dimensionality of the solution}

Before we construct a low-dimensional display of the association between the ordinal variables ({\it Rating} and {\it Ranking}) and the nominal variable ({\it Country}), we determine the appropriate dimension of the solution. The $9\times 9 \times 5$ data set \code{ratrank} has a $8\times 8\times 4$ sized matrix of hybrid core elements that reflect the trivariate sources of association between the three variables. Not all these sources are important for describing the analysis, or are even practically relevant. In most practical cases the linear and quadratic sources of association are sufficient and provide a meaningful description of the association. \\
We use the \code{tunelocal()} function to determine the appropriate number of hybrid core elements to define the dimensionality of the solution. When using the \code{tunelocal()} function for analysing ordinal variables, one needs to specify the number of them; this is done by setting  the argument \code{norder = 2}. The numerical and graphical summaries from using this function are obtained using the commands:

\begin{example}
tune.oca3.out <- tunelocal(ratrank, ca3type = "OCA3", norder = 2)
print(tune.oca3.out)
\end{example}

The numerical and graphical output of a similar form to those seen in the previous examples. 
The visual and numerical outputs (not given here) show that the highest order hybrid core element is of order $\left(2,\, 2,\, 1\right)$ (i.e. the quadratic-by-quadratic-by-first order component association) so that all terms, up to the (2, 2, 1) term, together account for most of the association between the three variables.

\subsubsection{Numerical summary of the association}

We perform OCA3 on \code{ratrank} using the dimensionalities as suggested by the output of the \code{tunelocal()} function. Hence, we confine our attention to sources of association no higher than the quadratic-by-quadratic-by-first hybrid core so that:

\begin{example}
oca3.out <- CA3variants(ratrank, ca3type = "OCA3", dims = c(2, 2, 1), norder = 2)
\end{example}

We note that from such an analysis, only four of the $256$ hybrid core elements are required to account for most of the association that exists between the variables. We can gain more insight into the structure of the association by inspecting the core elements from the hybrid decomposition. When using the function \code{summary()},
the elements of the core and squared core arrays \cite[]{lombehkro21}, respectively, can be obtained:  	
\begin{example}
summary(oca3.out)

#> Core table 
#> , , r1

#>       q1     q2
#> p1 -0.527 -0.143
#> p2  0.198 -0.246

#> Squared core table
#> , , r1

#>      q1    q2
#> p1 0.278 0.020
#> p2 0.039 0.061

#> Explained inertia (reduced dimensions)
#> [1] 0.398

#> Total inertia (complete dimensions)
#> [1] 0.477

#> Proportion of explained inertia (when reducing dimensions)
#> [1] 0.834
\end{example}
Note that by confining the solution to include terms no higher than (2, 2, 1), the sum of squares of these four squared core elements is $83.4\%$  of the association that exists between the three variables of the contingency table. 

The four terms from this output are all adequately described using the linear and quadratic polynomials for {\it Rating} and {\it Ranking} and just one Tucker3 component for {\it Country},  and are:
\begin{itemize}
	\item the linear-by-linear polynomial component term (\code{0.278}) which describes the association between the ordered variables in terms of any differences that exist in the linearity of each ordered set of categories that form the {\it Rating} and {\it Ranking} variables,
	\item the linear-by-quadratic polynomial component term (\code{0.020}) which describes the association between the ordered variables in terms of any linear differences in the {\it Rating} variable and dispersion differences in the {\it Ranking} variable,
	\item the quadratic-by-linear polynomial component (\code{0.039}) which describes the association between the ordered variables in terms of any dispersion differences in the {\it Rating} variable and any linear differences that exist in the {\it Ranking} variable, and 
	\item the quadratic-by-quadratic polynomial component (\code{0.061}) which describes the association between the ordered variables in terms of any dispersion differences that exist in the {\it Rating} and {\it Ranking} variables.
\end{itemize}

Using the \code{print()} function we obtain the percentage contributions of the components to the total inertia for different biplots, the overall decomposition information, as well as the partitionings of the four terms of the Pearson three-way chi-squared statistic into their polynomial components. For example, suppose we focus on the pair-wise association between the {\it Rating} and {\it Ranking} variables. The partial output  corresponding to the row and column (linear and non-linear) components of  the \code{Chi2-IJ} term  can be shown: 

\begin{example}
print(oca3.out)
#> # ...
#> # Partition of the Term-IJ using polynomials 

#>          Term-IJ-poly %inertia df p-value
#> poly-row1    12701.275   69.182  8       0
#> poly-row2     3882.996   21.150  8       0
#> poly-row3      833.867    4.542  8       0
#> poly-row4      346.473    1.887  8       0
#> poly-row5      177.262    0.966  8       0
#> poly-row6      167.495    0.912  8       0
#> poly-row7      122.977    0.670  8       0
#> poly-row8      126.927    0.691  8       0
#> Chi2-IJ      18359.272  100.000 64       0
#> poly-col1    13819.761   75.274  8       0
#> poly-col2     3177.816   17.309  8       0
#> poly-col3      605.183    3.296  8       0
#> poly-col4      189.747    1.034  8       0
#> poly-col5      149.127    0.812  8       0
#> poly-col6      160.638    0.875  8       0
#> poly-col7      124.849    0.680  8       0
#> poly-col8      132.150    0.720  8       0
#> Chi2-IJ      18359.272  100.000 64       0
#> # ...
\end{example}

This output shows that all components are statistically significant (with p-values smaller than 0.0001). It also shows that the variation in the {\it Rating} variable (row variable) is dominated by the difference in the linearity of its categories - the linear component accounts for $100 \times 12701.28/18359.27 = 69.18\%$ of the variation in this variable. The linear component also accounts for the largest source of variation in the {\it Ranking} variable (column variable), contributing to $100 \times 13819.76/18359.27 = 75.27\%$ of the variables' variation. Thus, if we were to confine our attention to just exploring further the association between the {\it Rating} and {\it Ranking} variables by generating a visual summary of the association this can be done using the correspondence analysis approach introduced in \cite{beh97} and described by \citet[Chap. 6]{behlom14} and \citet[Chap. 4]{behlom21b}. Since both variables are dominated by differences in the linearity of their categories, such an analysis will produce a correspondence plot that is dominated more by the first  axis than any of the other axis in the optimal plot. 

\begin{figure}[h]
	\begin{center}
		%{\includegraphics*[width=1.2 \textwidth]{row-biplot-ca3.eps}}
\hspace*{-2cm}{\includegraphics*[width=1.1 \textwidth]{fig5-scale15.pdf}}		
\caption{\label{row-biplot-ca3} The row biplot from the classical three-way correspondence analysis of \code{ratrank}.}
	\end{center}
\end{figure}
\subsubsection{Visual summary of the association}

Since the three-way association term is statistically significant ($X^2 = 21265.91$, p-value $< 0.0001$), we can examine the nature of this association term more closely. Visually summarizing this three-way association can be done by considering the coordinate systems that generate the biplots described in Section \ref{s.4.2}. Recall that in Section~\ref{s.nominalexample}, we performed the classical approach to three-way correspondence analysis and visualized the results using the row-tube (interactive) biplot. In doing so, the {\it Ranking-Country} association – which is comparatively weak (contributing to 1.2\% of the association) but is statistically significant (p-value $< 0.0001$) – is depicted using standard coordinates while the {\it Rating} categories are depicted using principal coordinates that are akin to $\mathbf{X}_{IK, J}$ in (\ref{eq:3biplots}). 

To highlight  differences between the classical and ordered three-way correspondence analysis, we construct the row biplot of Figure~\ref{row-biplot-ca3}  using the command:

\begin{example}
plot(ca3.out, biptype = "row",  addlines = F,  scaleplot = 15)
\end{example}

note that \code{ca3.out} is the output from the classical analysis performed in Section~\ref{s.nominalexample}.  When {\it Rating} and {\it Ranking} are treated as ordinal variables, Figure~\ref{row-biplot-oca3} gives the row biplot that can be obtained from the command:

\begin{example}
plot(oca3.out, biptype = "row",  scaleplot = 15)
\end{example}


\begin{figure}[h]
	\begin{center}
%		{\includegraphics*[width=1.2 \textwidth]{row-biplot-oca3.eps}}
\hspace*{-2cm}{\includegraphics*[width=1.1 \textwidth]{fig6-scale15.pdf}}

		\caption{\label{row-biplot-oca3} The row biplot from the ordered (hybrid) three-way correspondence analysis of \code{ratrank}.}
	\end{center}
\end{figure}
\noindent where the value for \verb|scaleplot = 15| was chosen by trial and error to ensure a reasonable separation of the points in the biplot, without afffecting the approximation of the association between the variables.
While Figure~\ref{row-biplot-ca3} and Figure~\ref{row-biplot-oca3} both give parabolic configurations of the points, these configurations are quite different since the former treats the variables as  nominal and uses the components from a Tucker3 decomposition while the latter is constructed using orthogonal polynomials for the ordered row and column ({\it Ratings and Rankings}) variables and a Tucker3 component for the tube ({\it Country}) variable. Observe that the parabolic shape of {\it Rating}  in Figure~\ref{row-biplot-ca3}  is more pronounced than  the parabolic configuration of {\it Rating}  in Figure~\ref{row-biplot-oca3}. 

In addition to the visual differences between the two configurations of points, there are also some  features that make Figures~\ref{row-biplot-ca3} and \ref{row-biplot-oca3} distinct. 
Indeed, the first axis of Figure ~\ref{row-biplot-oca3} is constructed using the linear orthogonal polynomial while the second axis is constructed using the quadratic orthogonal polynomial. The linear and dispersion components contribute to 66\% and 17\%, respectively, of the total inertia of the data; these percentages can be obtained  from the output of the \code{print(oca3.out)} command.
		
When considering all variables as nominal, as was done in our analysis in Section \ref{s.nominalexample}, the ratings appear closely associated with the  rankings  across the five countries. However, treating the two variables ({\it Rating} and {\it Ranking}) as ordinal  provides additional information on some aspects of the variable distribution (mean and variability). 
For example, Figure~\ref{row-biplot-oca3}  shows that the configuration of the {\it Rating} categories along the first (linear polynomial) axis is different to the configuration along the first axis of  Figure~\ref{row-biplot-ca3}. This is because the variation of the {\it Rating} variabe is dominated more by differences in the linearity of its categories than by its dispersion differences.  This dominant linear component affects the variable association and is captured by the configuration of points in Figure~\ref{row-biplot-oca3}.

%-----------------------------------------------------------------------------
\section{Conclusion}
\label{s.7}
%-----------------------------------------------------------------------------
The \pkg{CA3variants} package described in this paper is, to the best of our knowledge, the only package that allows practitioners and researchers to directly perform  four variants of three-way correspondence analysis, including the classical three-way correspondence analysis \cite[]{carkro96},  the non-symmetric variant and the two ordered versions of three-way correspondence analysis \cite[]{lombehkro21}. 
Subsequent versions of the package may allow for additional flexibility by providing the user more tools to numerically and visually explore the association structure between categorical variables. These include, but are not confined to, the  decomposition
of the generalised Cressie-Read family of divergence statistics \citep{par96}.
Indeed, Pearson's statistic is one of many measures of symmetric association that can be considered. Alternatives include the Freeman-Tukey statistic, log-likelihood ratio statistic, Neyman's chi-squared statistic, and the Cressie-Read statistic, which were originally developed to study two variables  \citep{cre84,behlom23}. These measures are all special cases of the Cressie-Read family of divergence statistics and have been adapted for three-way and multi-way contingency tables \citep{par96,par03,lombeh22}. 
Thus, this family of statistics may be incorporated into the \pkg{CA3variants} package, thereby providing the user with greater flexibility for the choice of symmetric association they wish to consider.
Furthermore, next version of the package might consider  the construction of confidence regions that determine those categories (and interactions) that provide a statistically significant contribution to the association between the variables \citep{beh10, rin96, rin12}. Numerical summaries that accompany such regions, including p-values \citep{behlom15} can certainly be incorporated and would provide similar functionality that is available in the \pkg{CAvariants} package used for the correspondence analysis of two cross-classified categorical variables \citep{behlom16}. 

%\bibliographystyle{plainnat}
\bibliography{lom-vdv-beh}


\address{Rosaria Lombardo\\
Department of Economics, University of Campania  ``Luigi Vanvitelli''\\
Capua (CE), Italy\\
\email{rosaria.lombardo@unicampania.it}}

\address{Michel van de Velden \\
Econometric Institute, Erasmus University\\
Rotterdam, The Netherlands\\
\email{vandevelden@ese.eur.nl}}

\address{Eric J. Beh \\
National Institute for Applied Statistics Research Australia (NIASRA), University of Wollongong \\
Wollongong, Australia\\ and\\
Centre for Multi-Dimensional Data Visualisation (MuViSU) \\
Stellenbosch University \\
Stellenbosch, South Africa\\
 \email{ericb@uow.edu.au}}
