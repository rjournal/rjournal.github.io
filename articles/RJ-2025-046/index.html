<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { color: #00769e; background-color: #f1f3f5; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #00769e; } /* Normal */
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #657422; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #00769e; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #00769e; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #00769e; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #5e5e5e; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

<style>
  div.csl-bib-body { }
  div.csl-entry {
    clear: both;
    }
  .hanging div.csl-entry {
    margin-left:2em;
    text-indent:-2em;
  }
  div.csl-left-margin {
    min-width:2em;
    float:left;
  }
  div.csl-right-inline {
    margin-left:2em;
    padding-left:1em;
  }
  div.csl-indent {
    margin-left: 2em;
  }
</style>

  <!--radix_placeholder_meta_tags-->
<title>drclust: An R Package for Simultaneous Clustering and Dimensionality Reduction</title>

<meta property="description" itemprop="description" content="The primary objective of simultaneous methodologies for clustering and&#10;variable reduction is to identify both the optimal partition of units&#10;and the optimal subspace of variables, all at once. The optimality is&#10;typically determined using least squares or maximum likelihood&#10;estimation methods. These simultaneous techniques are particularly&#10;useful when working with Big Data, where the reduction (synthesis) is&#10;essential for both units and variables. Furthermore, a secondary&#10;objective of reducing variables through a subspace is to enhance the&#10;interpretability of the latent variables identified by the subspace&#10;using specific methodologies. The drclust package implements double&#10;K-means (KM), reduced KM, and factorial KM to address the primary&#10;objective. KM with disjoint principal components addresses both the&#10;primary and secondary objectives, while disjoint principal component&#10;analysis and disjoint factor analysis address the latter, producing&#10;the sparsest loading matrix. The models are implemented in C++ for&#10;faster execution, processing large data matrices in a reasonable&#10;amount of time."/>

<link rel="canonical" href="https://doi.org/10.32614/RJ-2025-046/"/>
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>
<link rel="icon" type="image/vnd.microsoft.icon" href="../../resources/favicon.ico"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2026-02-04"/>
<meta property="article:created" itemprop="dateCreated" content="2026-02-04"/>
<meta name="article:author" content="Ionel Prunila"/>
<meta name="article:author" content="Maurizio Vichi"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="drclust: An R Package for Simultaneous Clustering and Dimensionality Reduction"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="The primary objective of simultaneous methodologies for clustering and&#10;variable reduction is to identify both the optimal partition of units&#10;and the optimal subspace of variables, all at once. The optimality is&#10;typically determined using least squares or maximum likelihood&#10;estimation methods. These simultaneous techniques are particularly&#10;useful when working with Big Data, where the reduction (synthesis) is&#10;essential for both units and variables. Furthermore, a secondary&#10;objective of reducing variables through a subspace is to enhance the&#10;interpretability of the latent variables identified by the subspace&#10;using specific methodologies. The drclust package implements double&#10;K-means (KM), reduced KM, and factorial KM to address the primary&#10;objective. KM with disjoint principal components addresses both the&#10;primary and secondary objectives, while disjoint principal component&#10;analysis and disjoint factor analysis address the latter, producing&#10;the sparsest loading matrix. The models are implemented in C++ for&#10;faster execution, processing large data matrices in a reasonable&#10;amount of time."/>
<meta property="og:url" content="https://doi.org/10.32614/RJ-2025-046/"/>
<meta property="og:image" content="https://journal.r-project.org/articles/RJ-2025-046/figures/1iriscomparison.png"/>
<meta property="og:image:width" content="4152"/>
<meta property="og:image:height" content="2475"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="The R Journal"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary_large_image"/>
<meta property="twitter:title" content="drclust: An R Package for Simultaneous Clustering and Dimensionality Reduction"/>
<meta property="twitter:description" content="The primary objective of simultaneous methodologies for clustering and&#10;variable reduction is to identify both the optimal partition of units&#10;and the optimal subspace of variables, all at once. The optimality is&#10;typically determined using least squares or maximum likelihood&#10;estimation methods. These simultaneous techniques are particularly&#10;useful when working with Big Data, where the reduction (synthesis) is&#10;essential for both units and variables. Furthermore, a secondary&#10;objective of reducing variables through a subspace is to enhance the&#10;interpretability of the latent variables identified by the subspace&#10;using specific methodologies. The drclust package implements double&#10;K-means (KM), reduced KM, and factorial KM to address the primary&#10;objective. KM with disjoint principal components addresses both the&#10;primary and secondary objectives, while disjoint principal component&#10;analysis and disjoint factor analysis address the latter, producing&#10;the sparsest loading matrix. The models are implemented in C++ for&#10;faster execution, processing large data matrices in a reasonable&#10;amount of time."/>
<meta property="twitter:url" content="https://doi.org/10.32614/RJ-2025-046/"/>
<meta property="twitter:image" content="https://journal.r-project.org/articles/RJ-2025-046/figures/1iriscomparison.png"/>
<meta property="twitter:image:width" content="4152"/>
<meta property="twitter:image:height" content="2475"/>

<!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
<meta name="citation_title" content="drclust: An R Package for Simultaneous Clustering and Dimensionality Reduction"/>
<meta name="citation_fulltext_html_url" content="https://doi.org/10.32614/RJ-2025-046"/>
<meta name="citation_pdf_url" content="RJ-2025-046.pdf"/>
<meta name="citation_volume" content="17"/>
<meta name="citation_issue" content="4"/>
<meta name="citation_doi" content="10.32614/RJ-2025-046"/>
<meta name="citation_journal_title" content="The R Journal"/>
<meta name="citation_issn" content="2073-4859"/>
<meta name="citation_firstpage" content="103"/>
<meta name="citation_lastpage" content="132"/>
<meta name="citation_fulltext_world_readable" content=""/>
<meta name="citation_online_date" content="2026/02/04"/>
<meta name="citation_publication_date" content="2026/02/04"/>
<meta name="citation_author" content="Ionel Prunila"/>
<meta name="citation_author_institution" content="Department of Statistical Sciences, Sapienza University of Rome"/>
<meta name="citation_author" content="Maurizio Vichi"/>
<meta name="citation_author_institution" content="Department of Statistical Sciences, Sapienza University of Rome"/>
<!--/radix_placeholder_meta_tags-->
  
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","date_received","journal","volume","issue","slug","citation_url","packages","preview","bibliography","CTV","legacy_pdf","legacy_converted","output","draft","pdf_url","doi","creative_commons","csl","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["drclust: An R Package for Simultaneous Clustering and Dimensionality Reduction"]},{"type":"character","attributes":{},"value":["The primary objective of simultaneous methodologies for clustering and\nvariable reduction is to identify both the optimal partition of units\nand the optimal subspace of variables, all at once. The optimality is\ntypically determined using least squares or maximum likelihood\nestimation methods. These simultaneous techniques are particularly\nuseful when working with Big Data, where the reduction (synthesis) is\nessential for both units and variables. Furthermore, a secondary\nobjective of reducing variables through a subspace is to enhance the\ninterpretability of the latent variables identified by the subspace\nusing specific methodologies. The drclust package implements double\nK-means (KM), reduced KM, and factorial KM to address the primary\nobjective. KM with disjoint principal components addresses both the\nprimary and secondary objectives, while disjoint principal component\nanalysis and disjoint factor analysis address the latter, producing\nthe sparsest loading matrix. The models are implemented in C++ for\nfaster execution, processing large data matrices in a reasonable\namount of time."]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","orcid_id","address"]}},"value":[{"type":"character","attributes":{},"value":["Ionel Prunila"]},{"type":"character","attributes":{},"value":["Department of Statistical Sciences, Sapienza University of Rome"]},{"type":"character","attributes":{},"value":["0009-0009-3773-0481"]},{"type":"character","attributes":{},"value":["P.le Aldo Moro 5, 00185 Rome","Italy","[ionel.prunila@uniroma1.it](ionel.prunila@uniroma1.it){.uri}\n"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","orcid_id","address"]}},"value":[{"type":"character","attributes":{},"value":["Maurizio Vichi"]},{"type":"character","attributes":{},"value":["Department of Statistical Sciences, Sapienza University of Rome"]},{"type":"character","attributes":{},"value":["0000-0002-3876-444X"]},{"type":"character","attributes":{},"value":["P.le Aldo Moro 5, 00185 Rome","Italy","[maurizio.vichi@uniroma1.it](maurizio.vichi@uniroma1.it){.uri}\n"]}]}]},{"type":"character","attributes":{},"value":["2026-02-04"]},{"type":"character","attributes":{},"value":["2024-07-12"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","issn","firstpage","lastpage"]}},"value":[{"type":"character","attributes":{},"value":["The R Journal"]},{"type":"character","attributes":{},"value":["2073-4859"]},{"type":"integer","attributes":{},"value":[103]},{"type":"integer","attributes":{},"value":[132]}]},{"type":"integer","attributes":{},"value":[17]},{"type":"integer","attributes":{},"value":[4]},{"type":"character","attributes":{},"value":["RJ-2025-046"]},{"type":"character","attributes":{},"value":["https://doi.org/10.32614/RJ-2025-046"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["cran","bioc"]}},"value":[{"type":"character","attributes":{},"value":["psych","ade4","FactoMineR","FactoClass","factoextra","NbClust","drclust","clustrd","biplotbootGUI","Rcpp","RcppArmadillo","cluster","pheatmap","ggplot2","dplyr","GGally"]},{"type":"list","attributes":{},"value":[]}]},{"type":"character","attributes":{},"value":["preview.png"]},{"type":"character","attributes":{},"value":["prunila-vichi.bib"]},{"type":"character","attributes":{},"value":["ChemPhys","Cluster","Databases","Environmetrics","HighPerformanceComputing","MissingData","ModelDeployment","NetworkAnalysis","NumericalMathematics","Phylogenetics","Psychometrics","Robust","Spatial","TeachingStatistics"]},{"type":"logical","attributes":{},"value":[true]},{"type":"logical","attributes":{},"value":[true]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained","toc","mathjax","md_extension"]}},"value":[{"type":"logical","attributes":{},"value":[true]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"]},{"type":"character","attributes":{},"value":["-tex_math_single_backslash"]}]}]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["RJ-2025-046.pdf"]},{"type":"character","attributes":{},"value":["10.32614/RJ-2025-046"]},{"type":"character","attributes":{},"value":["CC BY"]},{"type":"character","attributes":{},"value":["/home/mitchell/R/x86_64-pc-linux-gnu-library/4.5/rjtools/rjournal.csl"]},{"type":"character","attributes":{},"value":["https://doi.org/10.32614/RJ-2025-046/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["figures/10centreedpca10m.png","figures/11silhouettek5q3.png","figures/12dkmk5q3heatm.png","figures/13biplot.png","figures/14parcoord.png","figures/1iriscomparison.png","figures/2sim123.png","figures/3Fit.png","figures/4ARI.png","figures/5fsf.png","figures/6AsA.png","figures/7runtime_RKM.png","figures/8runtime_others.png","figures/9pFfactkm.png","figures/Rlogo.png","prunila-vichi_files/figure-html5/unnamed-chunk-1-1.png","prunila-vichi.bib","prunila-vichi.tex","RJ-2025-046.zip","RJournal.sty","RJwrapper.log","RJwrapper.md.bak","RJwrapper.Rmd.bak","RJwrapper.tex"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      tolerance: 5,
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

/* adjust viewport for navbar height */
/* helps vertically center bootstrap (non-distill) content */
.min-vh-100 {
  min-height: calc(100vh - 100px) !important;
}

</style>

<script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="../../site_libs/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
  font-size: 100%;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

hr.section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  margin: 0px;
}


d-byline {
  border-top: none;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
  border-top: none;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: auto;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

d-article details {
  grid-column: text;
  margin-bottom: 0.8em;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

/* tweak for Pandoc numbered line within distill */
d-article pre.numberSource code > span {
    left: -2em;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}

/* Citation hover box */

.tippy-box[data-theme~=light-border] {
  background-color: rgba(250, 250, 250, 0.95);
}

.tippy-content > p {
  margin-bottom: 0;
  padding: 2px;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}

/* Styles for listing layout (hide title) */
.layout-listing d-title, .layout-listing .d-title {
  display: none;
}

/* Styles for posts lists (not auto-injected) */


.posts-with-sidebar {
  padding-left: 45px;
  padding-right: 45px;
}

.posts-list .description h2,
.posts-list .description p {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

.posts-list .description h2 {
  font-weight: 700;
  border-bottom: none;
  padding-bottom: 0;
}

.posts-list h2.post-tag {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  padding-bottom: 12px;
}
.posts-list {
  margin-top: 60px;
  margin-bottom: 24px;
}

.posts-list .post-preview {
  text-decoration: none;
  overflow: hidden;
  display: block;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding: 24px 0;
}

.post-preview-last {
  border-bottom: none !important;
}

.posts-list .posts-list-caption {
  grid-column: screen;
  font-weight: 400;
}

.posts-list .post-preview h2 {
  margin: 0 0 6px 0;
  line-height: 1.2em;
  font-style: normal;
  font-size: 24px;
}

.posts-list .post-preview p {
  margin: 0 0 12px 0;
  line-height: 1.4em;
  font-size: 16px;
}

.posts-list .post-preview .thumbnail {
  box-sizing: border-box;
  margin-bottom: 24px;
  position: relative;
  max-width: 500px;
}
.posts-list .post-preview img {
  width: 100%;
  display: block;
}

.posts-list .metadata {
  font-size: 12px;
  line-height: 1.4em;
  margin-bottom: 18px;
}

.posts-list .metadata > * {
  display: inline-block;
}

.posts-list .metadata .publishedDate {
  margin-right: 2em;
}

.posts-list .metadata .dt-authors {
  display: block;
  margin-top: 0.3em;
  margin-right: 2em;
}

.posts-list .dt-tags {
  display: block;
  line-height: 1em;
}

.posts-list .dt-tags .dt-tag {
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0.3em 0.4em;
  margin-right: 0.2em;
  margin-bottom: 0.4em;
  font-size: 60%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

.posts-list img {
  opacity: 1;
}

.posts-list img[data-src] {
  opacity: 0;
}

.posts-more {
  clear: both;
}


.posts-sidebar {
  font-size: 16px;
}

.posts-sidebar h3 {
  font-size: 16px;
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 400;
  text-transform: uppercase;
}

.sidebar-section {
  margin-bottom: 30px;
}

.categories ul {
  list-style-type: none;
  margin: 0;
  padding: 0;
}

.categories li {
  color: rgba(0, 0, 0, 0.8);
  margin-bottom: 0;
}

.categories li>a {
  border-bottom: none;
}

.categories li>a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
}

.categories .active {
  font-weight: 600;
}

.categories .category-count {
  color: rgba(0, 0, 0, 0.4);
}


@media(min-width: 768px) {
  .posts-list .post-preview h2 {
    font-size: 26px;
  }
  .posts-list .post-preview .thumbnail {
    float: right;
    width: 30%;
    margin-bottom: 0;
  }
  .posts-list .post-preview .description {
    float: left;
    width: 45%;
  }
  .posts-list .post-preview .metadata {
    float: left;
    width: 20%;
    margin-top: 8px;
  }
  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.5em;
    font-size: 16px;
  }
  .posts-with-sidebar .posts-list {
    float: left;
    width: 75%;
  }
  .posts-with-sidebar .posts-sidebar {
    float: right;
    width: 20%;
    margin-top: 60px;
    padding-top: 24px;
    padding-bottom: 24px;
  }
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

.downlevel .posts-list .post-preview {
  color: inherit;
}



</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // separator
  var separator = '<hr class="section-separator" style="clear: both"/>';
  // prepend separator above appendix
  $('.d-byline').before(separator);
  $('.d-article').before(separator);

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('details, div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme, except when numbering line
  // in code chunk
  $('pre:not(.numberLines) code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      var author_name = front_matter.authors[i].author
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', author_name ? 'ORCID ID for ' + author_name : 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // hoverable references
    $('span.citation[data-cites]').each(function() {
      const citeChild = $(this).children()[0]
      // Do not process if @xyz has been used without escaping and without bibliography activated
      // https://github.com/rstudio/distill/issues/466
      if (citeChild === undefined) return true

      if (citeChild.nodeName == "D-FOOTNOTE") {
        var fn = citeChild
        $(this).html(fn.shadowRoot.querySelector("sup"))
        $(this).id = fn.id
        fn.remove()
      }
      var refs = $(this).attr('data-cites').split(" ");
      var refHtml = refs.map(function(ref) {
        // Could use CSS.escape too here, we insure backward compatibility in navigator
        return "<p>" + $('div[id="ref-' + ref + '"]').html() + "</p>";
      }).join("\n");
      window.tippy(this, {
        allowHTML: true,
        content: refHtml,
        maxWidth: 500,
        interactive: true,
        interactiveBorder: 10,
        theme: 'light-border',
        placement: 'bottom-start'
      });
    });

    // fix footnotes in tables (#411)
    // replacing broken distill.pub feature
    $('table d-footnote').each(function() {
      // we replace internal showAtNode methode which is triggered when hovering a footnote
      this.hoverBox.showAtNode = function(node) {
        // ported from https://github.com/distillpub/template/pull/105/files
        calcOffset = function(elem) {
            let x = elem.offsetLeft;
            let y = elem.offsetTop;
            // Traverse upwards until an `absolute` element is found or `elem`
            // becomes null.
            while (elem = elem.offsetParent && elem.style.position != 'absolute') {
                x += elem.offsetLeft;
                y += elem.offsetTop;
            }

            return { left: x, top: y };
        }
        // https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetTop
        const bbox = node.getBoundingClientRect();
        const offset = calcOffset(node);
        this.show([offset.left + bbox.width, offset.top + bbox.height]);
      }
    })

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    // ignore leaflet img layers (#106)
    figures = figures.filter(':not(img[class*="leaflet"])')
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace(/^index[.]html/, "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<style type="text/css">
/* base variables */

/* Edit the CSS properties in this file to create a custom
   Distill theme. Only edit values in the right column
   for each row; values shown are the CSS defaults.
   To return any property to the default,
   you may set its value to: unset
   All rows must end with a semi-colon.                      */

/* Optional: embed custom fonts here with `@import`          */
/* This must remain at the top of this file.                 */



html {
  /*-- Main font sizes --*/
  --title-size:      50px;
  --body-size:       1.06rem;
  --code-size:       14px;
  --aside-size:      12px;
  --fig-cap-size:    13px;
  /*-- Main font colors --*/
  --title-color:     #000000;
  --header-color:    rgba(0, 0, 0, 0.8);
  --body-color:      rgba(0, 0, 0, 0.8);
  --aside-color:     rgba(0, 0, 0, 0.6);
  --fig-cap-color:   rgba(0, 0, 0, 0.6);
  /*-- Specify custom fonts ~~~ must be imported above   --*/
  --heading-font:    sans-serif;
  --mono-font:       monospace;
  --body-font:       sans-serif;
  --navbar-font:     sans-serif;  /* websites + blogs only */
}

/*-- ARTICLE METADATA --*/
d-byline {
  --heading-size:    0.6rem;
  --heading-color:   rgba(0, 0, 0, 0.5);
  --body-size:       0.8rem;
  --body-color:      rgba(0, 0, 0, 0.8);
}

/*-- ARTICLE TABLE OF CONTENTS --*/
.d-contents {
  --heading-size:    18px;
  --contents-size:   13px;
}

/*-- ARTICLE APPENDIX --*/
d-appendix {
  --heading-size:    15px;
  --heading-color:   rgba(0, 0, 0, 0.65);
  --text-size:       0.8em;
  --text-color:      rgba(0, 0, 0, 0.5);
}

/*-- WEBSITE HEADER + FOOTER --*/
/* These properties only apply to Distill sites and blogs  */

.distill-site-header {
  --title-size:       18px;
  --text-color:       rgba(255, 255, 255, 0.8);
  --text-size:        15px;
  --hover-color:      white;
  --bkgd-color:       #0F2E3D;
}

.distill-site-footer {
  --text-color:       rgba(255, 255, 255, 0.8);
  --text-size:        15px;
  --hover-color:      white;
  --bkgd-color:       #0F2E3D;
}

/*-- Additional custom styles --*/
/* Add any additional CSS rules below                      */
</style>
<style type="text/css">
/* base variables */

/* Edit the CSS properties in this file to create a custom
   Distill theme. Only edit values in the right column
   for each row; values shown are the CSS defaults.
   To return any property to the default,
   you may set its value to: unset
   All rows must end with a semi-colon.                      */

/* Optional: embed custom fonts here with `@import`          */
/* This must remain at the top of this file.                 */



html {
  /*-- Main font sizes --*/
  --title-size:      50px;
  --body-size:       1.06rem;
  --code-size:       14px;
  --aside-size:      12px;
  --fig-cap-size:    13px;
  /*-- Main font colors --*/
  --title-color:     #000000;
  --header-color:    rgba(0, 0, 0, 0.8);
  --body-color:      rgba(0, 0, 0, 0.8);
  --aside-color:     rgba(0, 0, 0, 0.6);
  --fig-cap-color:   rgba(0, 0, 0, 0.6);
  /*-- Specify custom fonts ~~~ must be imported above   --*/
  --heading-font:    sans-serif;
  --mono-font:       monospace;
  --body-font:       sans-serif;
  --navbar-font:     sans-serif;  /* websites + blogs only */
}

/*-- ARTICLE METADATA --*/
d-byline {
  --heading-size:    0.6rem;
  --heading-color:   rgba(0, 0, 0, 0.5);
  --body-size:       0.8rem;
  --body-color:      rgba(0, 0, 0, 0.8);
}

/*-- ARTICLE TABLE OF CONTENTS --*/
.d-contents {
  --heading-size:    18px;
  --contents-size:   13px;
}

/*-- ARTICLE APPENDIX --*/
d-appendix {
  --heading-size:    15px;
  --heading-color:   rgba(0, 0, 0, 0.65);
  --text-size:       0.8em;
  --text-color:      rgba(0, 0, 0, 0.7);
}

/*-- WEBSITE HEADER + FOOTER --*/
/* These properties only apply to Distill sites and blogs  */

.distill-site-header {
  --title-size:       18px;
  --text-color:       rgba(0, 0, 0, 0.8);
  --text-size:        15px;
  --hover-color:      black;
  --bkgd-color:       #ffffff;
}

.distill-site-footer {
  --text-color:       rgba(0, 0, 0, 0.8);
  --text-size:        15px;
  --hover-color:      black;
  --bkgd-color:       #fafafa;
}

/*-- Additional custom styles --*/
/* Add any additional CSS rules below                      */

.nav-right > a {
  text-transform: uppercase;
}

d-title h1, d-title p, d-title figure,
d-abstract p, d-abstract b {
  grid-column: page;
}

.rj-blue {
  color: #2467bb;
}

ul li {
  line-height: 1.6;
  margin-top: 0em;
  margin-bottom: 0em;
}</style>
<style type="text/css">
/* base style */

/* FONT FAMILIES */

:root {
  --heading-default: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  --mono-default: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  --body-default: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

body,
.posts-list .post-preview p,
.posts-list .description p {
  font-family: var(--body-font), var(--body-default);
}

h1, h2, h3, h4, h5, h6,
.posts-list .post-preview h2,
.posts-list .description h2 {
  font-family: var(--heading-font), var(--heading-default);
}

d-article div.sourceCode code,
d-article pre code {
  font-family: var(--mono-font), var(--mono-default);
}


/*-- TITLE --*/
d-title h1,
.posts-list > h1 {
  color: var(--title-color, black);
}

d-title h1 {
  font-size: var(--title-size, 50px);
}

/*-- HEADERS --*/
d-article h1,
d-article h2,
d-article h3,
d-article h4,
d-article h5,
d-article h6 {
  color: var(--header-color, rgba(0, 0, 0, 0.8));
}

/*-- BODY --*/
d-article > p,  /* only text inside of <p> tags */
d-article > ul, /* lists */
d-article > ol {
  color: var(--body-color, rgba(0, 0, 0, 0.8));
  font-size: var(--body-size, 1.06rem);
}


/*-- CODE --*/
d-article div.sourceCode code,
d-article pre code {
  font-size: var(--code-size, 14px);
}

/*-- ASIDE --*/
d-article aside {
  font-size: var(--aside-size, 12px);
  color: var(--aside-color, rgba(0, 0, 0, 0.6));
}

/*-- FIGURE CAPTIONS --*/
figure .caption,
figure figcaption,
.figure .caption {
  font-size: var(--fig-cap-size, 13px);
  color: var(--fig-cap-color, rgba(0, 0, 0, 0.6));
}

/*-- METADATA --*/
d-byline h3 {
  font-size: var(--heading-size, 0.6rem);
  color: var(--heading-color, rgba(0, 0, 0, 0.5));
}

d-byline {
  font-size: var(--body-size, 0.8rem);
  color: var(--body-color, rgba(0, 0, 0, 0.8));
}

d-byline a,
d-article d-byline a {
  color: var(--body-color, rgba(0, 0, 0, 0.8));
}

/*-- TABLE OF CONTENTS --*/
.d-contents nav h3 {
  font-size: var(--heading-size, 18px);
}

.d-contents nav a {
  font-size: var(--contents-size, 13px);
}

/*-- APPENDIX --*/
d-appendix h3 {
  font-size: var(--heading-size, 15px);
  color: var(--heading-color, rgba(0, 0, 0, 0.65));
}

d-appendix {
  font-size: var(--text-size, 0.8em);
  color: var(--text-color, rgba(0, 0, 0, 0.5));
}

d-appendix d-footnote-list a.footnote-backlink {
  color: var(--text-color, rgba(0, 0, 0, 0.5));
}

/*-- WEBSITE HEADER + FOOTER --*/
.distill-site-header .title {
  font-size: var(--title-size, 18px);
  font-family: var(--navbar-font), var(--heading-default);
}

.distill-site-header a,
.nav-dropdown .nav-dropbtn {
  font-family: var(--navbar-font), var(--heading-default);
}

.nav-dropdown .nav-dropbtn {
  color: var(--text-color, rgba(255, 255, 255, 0.8));
  font-size: var(--text-size, 15px);
}

.distill-site-header a:hover,
.nav-dropdown:hover .nav-dropbtn {
  color: var(--hover-color, white);
}

.distill-site-header {
  font-size: var(--text-size, 15px);
  color: var(--text-color, rgba(255, 255, 255, 0.8));
  background-color: var(--bkgd-color, #0F2E3D);
}

.distill-site-footer {
  font-size: var(--text-size, 15px);
  color: var(--text-color, rgba(255, 255, 255, 0.8));
  background-color: var(--bkgd-color, #0F2E3D);
}

.distill-site-footer a:hover {
  color: var(--hover-color, white);
}</style>
<!--/radix_placeholder_distill-->
  <script src="../../site_libs/header-attrs-2.30/header-attrs.js"></script>
  <script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="../../site_libs/popper-2.6.0/popper.min.js"></script>
  <link href="../../site_libs/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="../../site_libs/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="../../site_libs/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="../../site_libs/anchor-4.2.2/anchor.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<!--/radix_placeholder_site_in_header-->
  <script>
    $(function() {
      console.log("Starting...")

      // Mathjax config (add automatic linebreaks when supported)
      // MathJax = {
      //    tex: {
      //        inlineMath: [['$', '$'], ['\\(', '\\)']],
      //        displayMath: [['$$', '$$'], ['\\[', '\\]']],
      //        tags: 'ams',
      //        multline: true,
      //    },
      //    options: {
      //        linebreaks: { automatic: true },
      //    },
      // };

      // Always show Published - distill hides it if not set
      function show_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'visible');
      }

      show_byline_column('Published')

      // tweak function
      var rmd_meta = JSON.parse($("#radix-rmarkdown-metadata").html());
      function get_meta(name, meta) {
        var ind = meta.attributes.names.value.findIndex((e) => e == name)
        var val = meta.value[ind]
        if (val.type != 'list') {
          return val.value.toString()
        }
        return val
      }

      // tweak description
      // Add clickable tags
      const slug = get_meta('slug', rmd_meta)
      const cite_url = get_meta('citation_url', rmd_meta)

      var title = $("d-title").text

      const buttons = $('<div class="dt-tags" style="grid-column: page;">')
      buttons.append('<a href="#citation" class="dt-tag"><i class="fas fa-quote-left"></i> Cite</a>')
      buttons.append('<a href="' + slug + '.pdf" class="dt-tag"><i class="fas fa-file-pdf"></i> PDF</a>')
      
      // Conditionally add supplementary files button
      if (document.getElementById('supplementary-materials')) {
        // create element safely
        const btn_suppl = document.createElement('a');
        btn_suppl.href = slug + '.zip';
        btn_suppl.className = 'dt-tag';
        btn_suppl.innerHTML = '<i class="fas fa-file-zipper"></i> Supplement';
        buttons.append(btn_suppl);
      }

      // adds Abstract: in front of the first <p> in the title section --
      // unless it happens to be the subtitle (FIXME: this is a bad hack - can't distill do this?)
      var tpar = $("d-title p:not(:empty)").filter(function() {
        return !$(this).hasClass("subtitle");
      }).first();
      if (tpar) {
        const abstract = $('<d-abstract>')
        abstract.append('<b>Abstract:</b><br>')
        abstract.append(tpar) // Move description to d-abstract
        $("d-title p:empty").remove() // Remove empty paragraphs after title
        abstract.append(buttons)
        abstract.insertAfter($('d-title')) // Add abstract section after title */
      }

      // tweak by-line
      var byline = $("d-byline div.byline")
      ind = rmd_meta.attributes.names.value.findIndex((e) => e == "journal")
      const journal = get_meta('journal', rmd_meta)
      const volume = get_meta('volume', rmd_meta)
      const issue = get_meta('issue', rmd_meta)
      const jrtitle = get_meta('title', journal)
      const year = ((jrtitle == "R News") ? 2000 : 2008) + parseInt(volume)
      const firstpage = get_meta('firstpage', journal)
      const lastpage = get_meta('lastpage', journal)
      byline.append('<div class="rjournal grid">')
      $('div.rjournal').append('<h3>Volume</h3>')
      $('div.rjournal').append('<h3>Pages</h3>')
      $('div.rjournal').append('<a class="volume" href="../../issues/'+year+'-'+issue+'">'+volume+'/'+issue+'</a>')
      $('div.rjournal').append('<p class="pages">'+firstpage+' - '+lastpage+'</p>')

      const received_date = new Date(get_meta('date_received', rmd_meta))
      byline.find('h3:contains("Published")').parent().append('<h3>Received</h3><p>'+received_date.toLocaleDateString('en-US', {month: 'short'})+' '+received_date.getDate()+', '+received_date.getFullYear()+'</p>')

    })
  </script>

  <style>
      /*
    .nav-dropdown-content .nav-dropdown-header {
      text-transform: lowercase;
    }
    */

    d-byline .byline {
      grid-template-columns: 2fr 2fr 2fr 2fr;
    }

    d-byline .rjournal {
      grid-column-end: span 2;
      grid-template-columns: 1fr 1fr;
      margin-bottom: 0;
    }

    d-title h1, d-title p, d-title figure,
    d-abstract p, d-abstract b {
      grid-column: page;
    }

    d-title .dt-tags {
      grid-column: page;
    }

    .dt-tags .dt-tag {
      text-transform: lowercase;
    }

    d-article h1 {
      line-height: 1.1em;
    }

    d-abstract p, d-article p {
      text-align: justify;
    }

    @media(min-width: 1000px) {
      .d-contents.d-contents-float {
        justify-self: end;
      }

      nav.toc {
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }

    .posts-list .dt-tags .dt-tag {
      text-transform: lowercase;
    }

    @keyframes highlight-target {
      0% {
        background-color: #ffa;
      }
      66% {
        background-color: #ffa;
      }
      100% {
        background-color: none;
      }
    }

    d-article :target, d-appendix :target {
       animation: highlight-target 3s;
    }

    .header-section-number {
      margin-right: 0.5em;
    }
    
    d-appendix .citation-appendix,
    .d-appendix .citation-appendix {
      color: rgb(60, 60, 60);
    }

    d-article h2 {
      border-bottom: 0px solid rgba(0, 0, 0, 0.1);
      padding-bottom: 0rem;
    }
    d-article h3 {
      font-size: 20px;
    }
    d-article h4 {
      font-size: 18px;
      text-transform: none;
    }

    @media (min-width: 1024px) {
      d-article h2 {
        font-size: 32px;
      }
      d-article h3 {
        font-size: 24px;
      }
      d-article h4 {
        font-size: 20px;
      }
    }
  </style>


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"drclust: An R Package for Simultaneous Clustering and Dimensionality Reduction","description":"The primary objective of simultaneous methodologies for clustering and\nvariable reduction is to identify both the optimal partition of units\nand the optimal subspace of variables, all at once. The optimality is\ntypically determined using least squares or maximum likelihood\nestimation methods. These simultaneous techniques are particularly\nuseful when working with Big Data, where the reduction (synthesis) is\nessential for both units and variables. Furthermore, a secondary\nobjective of reducing variables through a subspace is to enhance the\ninterpretability of the latent variables identified by the subspace\nusing specific methodologies. The drclust package implements double\nK-means (KM), reduced KM, and factorial KM to address the primary\nobjective. KM with disjoint principal components addresses both the\nprimary and secondary objectives, while disjoint principal component\nanalysis and disjoint factor analysis address the latter, producing\nthe sparsest loading matrix. The models are implemented in C++ for\nfaster execution, processing large data matrices in a reasonable\namount of time.","doi":"10.32614/RJ-2025-046","authors":[{"author":"Ionel Prunila","authorURL":"#","affiliation":"Department of Statistical Sciences, Sapienza University of Rome","affiliationURL":"#","orcidID":"0009-0009-3773-0481"},{"author":"Maurizio Vichi","authorURL":"#","affiliation":"Department of Statistical Sciences, Sapienza University of Rome","affiliationURL":"#","orcidID":"0000-0002-3876-444X"}],"publishedDate":"2026-02-04T00:00:00.000+00:00","citationText":"Prunila & Vichi, 2026"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<a class="logo" href="../../index.html">
<img src="../../resources/rlogo.png" alt="Logo"/>
</a>
<a href="../../index.html" class="title">The R Journal</a>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../issues/2025-4">Current</a>
<a href="../../issues.html">Issues</a>
<a href="../../news.html">News</a>
<a href="../../submissions.html">Submit</a>
<a href="../../contribute.html">Contribute</a>
<a href="../../editors.html">Editorial board</a>
<a href="../../articles.xml">
<i class="fa fa-rss" aria-hidden="true"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>drclust: An R Package for Simultaneous Clustering and Dimensionality Reduction</h1>

<!--radix_placeholder_categories-->
<!--/radix_placeholder_categories-->
<p><p>The primary objective of simultaneous methodologies for clustering and
variable reduction is to identify both the optimal partition of units
and the optimal subspace of variables, all at once. The optimality is
typically determined using least squares or maximum likelihood
estimation methods. These simultaneous techniques are particularly
useful when working with Big Data, where the reduction (synthesis) is
essential for both units and variables. Furthermore, a secondary
objective of reducing variables through a subspace is to enhance the
interpretability of the latent variables identified by the subspace
using specific methodologies. The drclust package implements double
K-means (KM), reduced KM, and factorial KM to address the primary
objective. KM with disjoint principal components addresses both the
primary and secondary objectives, while disjoint principal component
analysis and disjoint factor analysis address the latter, producing
the sparsest loading matrix. The models are implemented in C++ for
faster execution, processing large data matrices in a reasonable
amount of time.</p></p>
</div>

<div class="d-byline">
  Ionel Prunila  (Department of Statistical Sciences, Sapienza University of Rome)
  
,   Maurizio Vichi  (Department of Statistical Sciences, Sapienza University of Rome)
  
<br/>2026-02-04
</div>

<div class="d-article">
<div class="article">
<h3 data-number="1" id="Introduction"><span class="header-section-number">1</span> Introduction</h3>
<p>Cluster analysis is the process of identifying homogeneous groups of
units in the data so that those within clusters are perceived with a low
degree of dissimilarity with each other. In contrast, units in different
clusters are perceived as dissimilar, i.e., with a high degree of
dissimilarity. When dealing with large or extremely large data matrices,
often referred to as Big Data, the task of assessing these
dissimilarities becomes computationally intensive due to the sheer
volume of units and variables involved. To manage this vast amount of
information, it is essential to employ statistical techniques that
synthesize and highlight the most significant aspects of the data.
Typically, this involves dimensionality reduction for both units and
variables to efficiently summarize the data.</p>
<p>While cluster analysis synthesizes information across the rows of the
data matrix, variable reduction operates on the columns, aiming to
summarize the features and, ideally, facilitate their interpretation.
This key process involves extracting a subspace from the full space
spanned by the manifest variables, maintaining the principal informative
content. The process allows for the synthesis of common information
mainly among subsets of manifest variables, which represent concepts not
directly observable. As a result, subspace-based variable reduction
identifies a few uncorrelated latent variables that mainly capture
common relationships within these subsets. When using techniques like
Factor Analysis (FA) or Principal Component Analysis (PCA) for this
purpose, interpreting the resulting factors or components can be
challenging, particularly when variables significantly load onto
multiple factors, a situation known as <em>cross-loading</em>. Therefore, a
simpler structure in the loading matrix, focusing on the primary
relationship between each variable and its related factor, becomes
desirable for clarity and ease of interpretation. Furthermore, the
latent variables derived from PCA or FA do not provide a unique
solution. An equivalent model fit can be achieved by applying an
orthogonal rotation to the component axes. This aspect of non-uniqueness
is often exploited in practice through Varimax rotation, which is
designed to improve the interpretability of latent variables, without
affecting the fit of the analysis. The rotation promotes a simpler
structure in the loading matrix, however, the rotations do not always
ensure enhanced interpretability. An alternative approach has been
proposed by (Vichi and Saporta 2009) and (Vichi 2017), with Disjoint
Principal Component (DPCA) and Disjoint FA (DFA), suggesting to
construct each component/factor from a distinct subset of manifest
variables rather than using all available variables, still optimizing
the same estimation as in PCA and FA, respectively.</p>
<p>It is important to note that data matrix reduction for both rows and
columns is often performed without specialized methodologies by
employing a "tandem analysis." This involves sequentially applying two
methods, such as using PCA or FA for variable reduction, followed by
Cluster Analysis using KM on the resulting factors. Alternatively, one
could start with Cluster Analysis and then proceed to variable
reduction. The outcomes of these two tandem analyses differ since each
approach optimizes distinct objective functions, one before the other.
For instance, when PCA is applied first, the components maximize the
total variance of the manifest variables. However, if the manifest
variables include high-variance variables that lack a clustering
structure, these will be included in the components, even though they
are not necessary for KM, which focuses on explaining only the variance
between clusters. As a result, sequentially optimizing two different
objectives may lead to sub-optimal solutions. In contrast, when
combining KM with PCA or FA in a simultaneous approach, a single
integrated objective function is utilized. This function aims to
optimize both the clustering partition and the subspace simultaneously.
The optimization is typically carried out using an Alternating Least
Squares (ALS) algorithm, which updates the partition for the current
subspace in one step and the subspace for the current partition in the
next. This iterative process ensures convergence to a solution that
represents at least a local minimum of the integrated objective
function. In comparison, tandem analysis, which follows a sequential
approach (e.g., PCA followed by KM), does not guarantee joint
optimization. One potential limitation of this sequential method is that
the initial optimization through PCA may obscure relevant information
for the subsequent step of Cluster Analysis or emphasize irrelevant
patterns, ultimately leading to sub-optimal solutions, as mentioned by
(DeSarbo et al.1990). Indeed, the simultaneous strategy has been shown
to be effective in various studies, like (De Soete and Carroll 1994),
(Vichi and Kiers 2001), (Vichi 2001), (Vichi and Saporta 2009), (Rocci
and Vichi 2008), (Timmerman et al.2010), (Yamamoto and Hwang 2014).</p>
<p>In order to spread access to these techniques and their use, software
implementations are needed. Within the R Core Team (2015) environment,
there are different libraries available to perform dimensionality
reduction techniques. Indeed, the plain version of KM, PCA, and FA are
available in the built-in package stats, namely: <code>princomp</code>, <code>factanal</code>,
<code>kmeans</code>. Furthermore, some packages allow to go beyond the plain
estimation and output of such algorithms. Indeed, one of the most rich
libraries in R is <a href="https://CRAN.R-project.org/package=psych"><strong>psych</strong></a>
(W. R. Revelle 2017), which provides functions that allow to easily
simulate data according to different schemes, testing routines,
calculation of various estimates, as well as multiple estimation
methods. <a href="https://CRAN.R-project.org/package=ade4"><strong>ade4</strong></a> (Dray and
Dufour 2007) allows for dimensionality reduction in the presence of
different types of variables, along with many graphical instruments. The
<a href="https://CRAN.R-project.org/package=FactoMineR"><strong>FactoMineR</strong></a> (L et
al.2008) package allows for unit-clustering and extraction of latent
variables, also in the presence of mixed variables.
<a href="https://CRAN.R-project.org/package=FactoClass"><strong>FactoClass</strong></a> (Pardo
and Del Campo 2007) implements functions for PCA, Correspondence
Analysis (CA) as well as clustering, including the tandem approach.
<a href="https://CRAN.R-project.org/package=factoextra"><strong>factoextra</strong></a>
(Kassambara 2022) instead, provides visualization of the results, aiding
their assessment in terms of choice of the number of latent variables,
elegant dendrograms, screeplots and more. More focused on the choice of
the number of clusters is
<a href="https://CRAN.R-project.org/package=NbClust"><strong>NbClust</strong></a> (Charrad et
al.2014), offering 30 indices for determining the number of clusters,
proposing the best method by trying not only different numbers of groups
but also different distance measures and clustering methods, going
beyond the partitioning ones.</p>
<p>More closely related to the library here presented, to the knowledge of
the authors, there are two packages that implement a subset of the
techniques proposed within
<a href="https://CRAN.R-project.org/package=drclust"><strong>drclust</strong></a>.
<a href="https://CRAN.R-project.org/package=clustrd"><strong>clustrd</strong></a> (Markos et al.
2019) implements simultaneous methods of clustering and dimensionality
reduction. Besides offering functions for continuous data, they also
allow for categorical (or mixed) variables. Even more, they formulate,
at least for the continuous case, an implementation aligned with the
objective function proposed by Yamamoto and Hwang (2014), based on which
the reduced KM (RKM) and factorial KM (FKM) become special cases as
results of a tuning parameter.</p>
<p>Finally, there is
<a href="https://CRAN.R-project.org/package=biplotbootGUI"><strong>biplotbootGUI</strong></a>
(Nieto Librero and Freitas 2023), offering a GUI allowing to interact
with graphical tools, aiding in the choice of the number of components
and clusters. Furthermore, it implements KM with disjoint PCA (DPCA), as
described in (Vichi and Saporta 2009). Even more, they propose an
optimization algorithm for the choice of the initial starting point from
which the estimation process for the parameters begins.</p>
<p>Like <a href="https://CRAN.R-project.org/package=clustrd"><strong>clustrd</strong></a>, the
<a href="https://CRAN.R-project.org/package=drclust"><strong>drclust</strong></a> package
provides implementations of FKM and RKM. However, while
<a href="https://CRAN.R-project.org/package=clustrd"><strong>clustrd</strong></a> also supports
categorical and mixed-type variables, our implementation currently
handles only continuous variables. That said, appropriate pre-processing
of categorical variables, as suggested in Vichi et al.(2019), can make
them compatible with the proposed methods. In extreme essence, one
should dummy-encode all the qualitative variables. In terms of
performance, <a href="https://CRAN.R-project.org/package=drclust"><strong>drclust</strong></a>
offers significantly faster execution. Moreover, regarding FKM, our
proposal demonstrates superior results in both empirical applications
and simulations, in terms of model fit and the Adjusted Rand Index
(ARI). Another alternative,
<a href="https://CRAN.R-project.org/package=biplotbootGUI"><strong>biplotbootGUI</strong></a>,
implements KM with DPCA and includes built-in plotting functions and a
SDP-based initialization of parameters. However, our implementation
remains considerably faster and allows users to specify which variables
should be grouped together within the same (or different) principal
components. This capability enables a partially or fully confirmatory
approach to variable reduction. Beyond speed and the confirmatory
option, <a href="https://CRAN.R-project.org/package=drclust"><strong>drclust</strong></a> offers
three methods not currently available in other <code>R</code> packages: DPCA and
DFA, both designed for pure dimensionality reduction, and double KM
(DKM), which performs simultaneous clustering and variable reduction via
KM. All methods are implemented in C++ for computational efficiency.
Table<a href="#tab:T2">2</a>
summarizes the similarities and differences between <code>drclust</code> and
existing alternatives</p>
<p>The package presented within this work aims to facilitate the access to
and usability of some techniques that fall in two main branches, which
overlap. In order to do so, some statistical background is first
recalled.</p>
<h3 data-number="2" id="notation-and-theoretical-background"><span class="header-section-number">2</span> Notation and theoretical background</h3>
<p>The main pillars of
<a href="https://CRAN.R-project.org/package=drclust"><strong>drclust</strong></a> fall in two
main categories: dimensionality reduction and (partitioning) cluster
analysis. The former may be carried out individually or blended with the
latter. Because both rely on the language of linear algebra, Table
<a href="#tab:T1">1</a> contains,
for the convenience of the reader, the mathematical notation needed for
this context. Then some theoretical background is reported.</p>
<div id="tab:notation">
<table style="width:99%;">
<caption><span id="tab:T1">Table 1: </span> Notation</caption>
<colgroup>
<col style="width: 16%" />
<col style="width: 82%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Symbol</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><em>n</em>, <em>J</em>, <em>K</em>, <em>Q</em></td>
<td style="text-align: left;">number of: units, manifest variables, unit-clusters, latent factors</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\mathbf{X}\)</span></td>
<td style="text-align: left;"><em>n</em> x <em>J</em> data matrix, where the generic element <span class="math inline">\(x_{ij}\)</span> is the real observation on the <em>i</em>-th unit within the <em>j</em>-th variable</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\mathbf{x}_i\)</span></td>
<td style="text-align: left;"><em>J</em> x 1 vector representing the generic row of <span class="math inline">\(\mathbf{X}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\mathbf{U}\)</span></td>
<td style="text-align: left;"><em>n</em> x <em>K</em> unit-cluster membership matrix, binary and row stochastic, with <span class="math inline">\(u_{ik}\)</span> being the generic element</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\mathbf{V}\)</span></td>
<td style="text-align: left;"><em>J</em> x <em>Q</em> variable-cluster membership matrix, binary and row stochastic, with <span class="math inline">\(v_{jq}\)</span> as the generic element</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\mathbf{B}\)</span></td>
<td style="text-align: left;"><em>J</em> x <em>J</em> variable-weighting diagonal matrix</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\mathbf{Y}\)</span></td>
<td style="text-align: left;"><em>n</em> x <em>Q</em> component/factor score matrix defined on the reduced subspace</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\mathbf{y}_i\)</span></td>
<td style="text-align: left;"><em>Q</em> x 1 vector representing the generic row of <strong>Y</strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\mathbf{A}\)</span></td>
<td style="text-align: left;"><em>J</em> x <em>Q</em> variables - factors, "plain", loading matrix</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\mathbf{C}^+\)</span></td>
<td style="text-align: left;">Moore-Penrose pseudo-inverse of a matrix <strong>C</strong>. <span class="math inline">\(\mathbf{C}^+ = (\mathbf{C&#39;C})^{-1}\mathbf{C&#39;}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\bar{\textbf{X}}\)</span></td>
<td style="text-align: left;"><em>K</em> x <em>J</em> centroid matrix in the original feature space, i.e., <span class="math inline">\(\bar{\textbf{X}} = \textbf{U}^{+} \textbf{X}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\bar{\mathbf{Y}}\)</span></td>
<td style="text-align: left;"><em>K</em> x <em>Q</em> centroid matrix projected in the reduced subspace, i.e., <span class="math inline">\(\bar{\mathbf{Y}} = \bar{\mathbf{X}}\mathbf{A}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\mathbf{H}_{\mathbf{C}}\)</span></td>
<td style="text-align: left;">Projector operator <span class="math inline">\(\mathbf{H}_\mathbf{C} = \mathbf{C}(\mathbf{C}&#39;\mathbf{C})^{-1}\mathbf{C}&#39;\)</span> spanned by the columns of matrix <span class="math inline">\(\mathbf{C}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\mathbf{E}\)</span></td>
<td style="text-align: left;"><em>n</em> x <em>J</em> Error term matrix</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(||\cdot||\)</span></td>
<td style="text-align: left;">Frobenius norm</td>
</tr>
</tbody>
</table>
</div>
<h4 class="unnumbered" data-number="2.1" id="latent-variables-with-simple-structure-loading-matrix">Latent variables with simple-structure loading matrix</h4>
<p>Classical methods of PCA (Pearson 1901) or FA (Cattell 1965; Lawley and
Maxwell 1962) build each latent factor from combination of <em>all</em> the
manifest variables. As a consequence, the loading matrix, describing the
relations between manifest and latent variables, is usually not
immediately interpretable. Ideally, it is desirable to have variables
that are associated to a single factor. This is typically called <em>simple
structure</em>, which induces subsets of variables characterizing factors
and frequently the partition of the variables. While factor rotation
techniques go in this direction (especially Varimax), even if not
exactly, they do not guarantee the result. Alternative solutions have
been proposed. (Zou et al.2006), by framing the PCA problem as a
regression one, introducing an elastic-net penalty, aiming for a sparse
solution of the loading matrix <strong>A</strong>. For the present work, we consider
two techniques for this purpose: DPCA and DFA, implemented in the
proposed package.</p>
<h5 class="unnumbered" data-number="2.1.1" id="disjoint-principal-component-analysis">Disjoint principal component analysis</h5>
<p>Vichi and Saporta (2009) propose an alternative solution, DPCA, which
leads to the simplest possible structure on <strong>A</strong>, while still
maximizing the explained variance. Such a result is obtained by building
each latent factor from a subset of variables instead of allowing all
the variables to contribute to all the components. This means that it
provides <em>J</em> non-zero loadings instead of having <em>JQ</em> of them. To obtain
this setting, variables are grouped in such a way that they form a
partition of the initial set. The model can be described as a
constrained PCA, where the matrix <span class="math inline">\(\mathbf{A}\)</span> is restricted to be
reparametrized into the product <span class="math inline">\(\mathbf{A}=\mathbf{BV}\)</span>. Thus, the
model is described as:</p>
<p><span class="math display" id="eq:dpca1">\[\begin{equation}
\label{dpca1}
    \mathbf{X} = \mathbf{X}\mathbf{A}\mathbf{A}&#39; + \mathbf{E}= \mathbf{X}\mathbf{B}\mathbf{V}\mathbf{V}&#39;\mathbf{B} + \mathbf{E},
\end{equation}   \tag{1}\]</span>
subject to
<span class="math display" id="eq:dpca2">\[\begin{equation}
\label{dpca2}
    \mathbf{V} = [v_{jq} \in \{0,1\}] \ \ \ \ \ (binarity),
\end{equation}   \tag{2}\]</span></p>
<p><span class="math display" id="eq:dpca3">\[\begin{equation}
\label{dpca3}
    \mathbf{V}\mathbf{1}_{Q} = \mathbf{1}_{J} \ \ \ (row-stochasticity),
\end{equation}   \tag{3}\]</span></p>
<p><span class="math display" id="eq:dpca4">\[\begin{equation}
\label{dpca4}
\mathbf{V}&#39;\mathbf{B}\mathbf{B}&#39;\mathbf{V} = \mathbf{I}_{Q} \ \ \ \ \ (orthonormality),
\end{equation}   \tag{4}\]</span></p>
<p><span class="math display" id="eq:dpca5">\[\begin{equation}
\label{dpca5}
    \mathbf{B} =  diag(b_1, \dots, b_J) \ \ \ \ (diagonality).
\end{equation}   \tag{5}\]</span>
The estimation of the parameters <span class="math inline">\(\mathbf{B}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> is
carried out via least squares (LS) and, by solving the minimization
problem,
<span class="math display" id="eq:dpca6">\[\begin{equation}
\label{dpca6}
    RSS_{DPCA}(\mathbf{B}, \mathbf{V}) = ||\mathbf{X} - \mathbf{X}\mathbf{B}\mathbf{V}\mathbf{V}&#39;\mathbf{B}||^2
\end{equation}   \tag{6}\]</span>
subject to the the constraints (<a href="#eq:dpca2">(2)</a>, <a href="#eq:dpca3">(3)</a>,
<a href="#eq:dpca4">(4)</a>, <a href="#eq:dpca5">(5)</a>). An ALS algorithm is employed,
guaranteeing at least a local optimum. In order to (at least partially)
overcome this downside, multiple random starts are needed, and the best
solution is retained.</p>
<p>Therefore, the DPCA method is subject to more structural constraints
than standard PCA. Specifically, standard PCA does not enforce the
reparameterization <span class="math inline">\(\mathbf{A}=\mathbf{BV}\)</span>, meaning its loading matrix
<span class="math inline">\(\mathbf{A}\)</span> is free to vary among orthonormal matrices. In contrast,
DPCA still requires an orthonormal matrix <span class="math inline">\(\mathbf{A}\)</span> but also needs
that each principal component is associated with a disjoint subset of
variables that most reconstruct the data. This implies that each
variable contributes to only one component, resulting in a sparse and
block-diagonal loading matrix. In essence, DPCA fits <em>Q</em> separate PCAs
on the <em>Q</em> disjoint subsets of variables, and from each, extracts the
eigenvector associated with the largest eigenvalue. In general, the
total variance explained by DPCA is slightly lower, and the residual of
the objective function is larger compared to PCA. This trade-off is made
in exchange for the added constraint that clearly enhances
interpretability. The extent of the reduction depends on the true
underlying structure of the latent factors, specifically on whether they
are truly uncorrelated. When the observed correlation matrix is block
diagonal, with variables within blocks being highly correlated and
variables between blocks being uncorrelated, DPCA can explain almost the
same amount of variance of PCA, with the advantage to simplify
interpretation.<br />
It is important to note that, as DPCA is implemented, it allows for a
blend of exploratory and confirmatory approaches. In the confirmatory
framework, users can specify a priori which variables should
collectively contribute to a factor using the <code>constr</code> argument,
available for the last three functions in Table
<a href="#tab:T2">2</a>. The
algorithm assigns the remaining manifest variables, for which no
constraint has been specified, to the <em>Q</em> factors in a way that ensures
the latent variables best reconstruct the manifest ones, capturing the
maximum variance. This is accomplished by minimizing the loss function
(<a href="#eq:dpca6">(6)</a>). Although each of the <em>Q</em> latent variables is derived
from a different subset of variables, which involves the spectral
decomposition of multiple covariance matrices, their smaller size,
combined with the implementation in C++, enables very rapid execution of
the routine.</p>
<p>A very positive side effect of the additional constraint in DPCA
compared to standard PCA is the uniqueness of the solution, which
eliminates the need for factor rotation in DPCA.</p>
<h5 class="unnumbered" data-number="2.1.2" id="disjoint-factor-analysis">Disjoint factor analysis</h5>
<p>Proposed by Vichi (2017), this technique is the model-based counterpart
of the DPCA model. It pursues a similar goal in terms of building <em>Q</em>
factors from <em>J</em> variables, imposing a simple structure on the loading
matrix. However, the means by which the goal is pursued are different.
Unlike DPCA, the estimation method adopted for DFA is Maximum Likelihood
and the model requires additional statistical assumptions compared to
DPCA. The model can be formulated in a matrix form as,
<span class="math display" id="eq:dfa1">\[\begin{equation}
\label{dfa1}
    \mathbf{X} = \mathbf{Y}\mathbf{A}&#39;+\mathbf{E},
\end{equation}   \tag{7}\]</span>
where <span class="math inline">\(\mathbf{X}\)</span> is centered, meaning that the mean vector
<span class="math inline">\(\boldsymbol{\mu}\)</span> has been subtracted from each multivariate unit
<span class="math inline">\(\mathbf{x}_{i}\)</span>. Therefore, for a multivariate, centered, unit, the
previous model can be expressed as
<span class="math display" id="eq:dfa2">\[\begin{equation}
\label{dfa2}
    \mathbf{x}_i = \mathbf{A}\mathbf{y}_i + \mathbf{e}_i, \ \ i = 1, \dots, n.
\end{equation}   \tag{8}\]</span>
where <span class="math inline">\(\mathbf{y}_i\)</span> is the <em>i</em>-th row of <span class="math inline">\(\mathbf{Y}\)</span> and
<span class="math inline">\(\mathbf{x}_i\)</span>, <span class="math inline">\(\mathbf{e}_i\)</span> are, respectively, the <span class="math inline">\(i\)</span>-th rows of
<span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{E}\)</span>, with a multivariate normal distribution
on the <span class="math inline">\(J\)</span>-dimensional space,
<span class="math display" id="eq:FAassumptions1">\[\begin{equation}
\label{FAassumptions1}
    \mathbf{x}_i \sim \mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma_X}), \ \ \  \mathbf{e}_i \sim \mathcal{N}(\boldsymbol{0}, \mathbf{\Psi})
\end{equation}   \tag{9}\]</span>
The covariance structure of the FA model can be written,
<span class="math display">\[\begin{equation}
    Cov(\mathbf{x}_i) = \mathbf{\Sigma_X} = \mathbf{AA&#39;} + \mathbf{\Psi},
\end{equation}\]</span>
<span id="dfa6" data-label="dfa6"></span> where additional, assumptions are needed,
<span class="math display" id="eq:dfa4">\[\begin{equation}
\label{dfa4}
    Cov(\mathbf{y}_{i}) = \mathbf{\Sigma}_{\mathbf{Y}} = \mathbf{I}_Q,
\end{equation}   \tag{10}\]</span></p>
<p><span class="math display" id="eq:dfa5">\[\begin{equation}
\label{dfa5}
    Cov(\mathbf{e}_i) = \mathbf{\Sigma}_{\mathbf{E}} = \mathbf{\Psi}, \ \ \ \mathbf{\Psi} = diag(\psi_{1},\dots,\psi_{Q} : \psi_{q}&gt;0)&#39; , \ \  j = 1, \dots, J
\end{equation}   \tag{11}\]</span></p>
<p><span class="math display" id="eq:dfa5b">\[\begin{equation}
  Cov(\mathbf{e}_{i}, \mathbf{y}_{i}) = \mathbf{\Sigma}_{\mathbf{EY}} = 0
\label{dfa5b}
\end{equation}   \tag{12}\]</span></p>
<p><span class="math display" id="eq:dfa6b">\[\begin{equation}
\mathbf{A} = \mathbf{BV}
\label{dfa6b}
\end{equation}   \tag{13}\]</span>
The objective function can be formulated as the maximization of the
Likelihood function or as the minimization of the following discrepancy:
<span class="math display">\[\begin{align*}
    D_{DFA}(\mathbf{B},\mathbf{V}, \mathbf{\Psi})
    &amp; = |\text{ln}(\mathbf{B}\mathbf{V}\mathbf{V}&#39;\mathbf{B} + \mathbf{\Psi})| - \text{ln}|\mathbf{S}| + \text{tr}((\mathbf{B}\mathbf{V}\mathbf{V}&#39;\mathbf{B} + \mathbf{\Psi})^{-1}\mathbf{S}) - \textit{J}, \\
    &amp; \qquad j = 1, \dots, \textit{J}, \ q = 1, \dots, \textit{Q},\\
    &amp;  \qquad s.t.: \mathbf{V} = [v_{jq}], \ v_{jq} \in \{0,1\}, \ \sum_q{v_{jq}} = 1,
\end{align*}\]</span>
whose parameters are optimized by means of a coordinate descent
algorithm.</p>
<p>Apart from the methodological distinctions between DPCA and DFA, the
latter exhibits the scale equivariance property. The optimization of the
Likelihood function implies a higher computational load, thus, a longer
(compared to the DPCA) execution time.</p>
<p>As in the DPCA case, under the constraint <span class="math inline">\(\mathbf{A}=\mathbf{BV}\)</span>, the
solution provided by the model is, also in this case, unique.</p>
<h4 class="unnumbered" data-number="2.2" id="joint-clustering-and-variable-reduction">Joint clustering and variable reduction</h4>
<p>The four clustering methods discussed all follow the <span class="math inline">\(K\)</span>-means
framework, working to partition units. However, they differ primarily in
how they handle variable reduction.</p>
<p>Double KM (DKM) employs a symmetric approach, clustering both the units
(rows) and the variables (columns) of the data matrix at the same time.
This leads to the simultaneous identification of mean profiles for both
dimensions. DKM is particularly suitable for data matrices where both
rows and columns represent units. Examples of such matrices include
document-by-term matrices used in Text Analysis, product-by-customer
matrices in Marketing, and gene-by-sample matrices in Biology.</p>
<p>In contrast, the other three clustering methods adopt an asymmetric
approach. They treat rows and columns differently, focusing on means
profiles and clustering for rows, while employing components or factors
for the variables (columns). These methods are more appropriate for
typical units-by-variable matrices, where its beneficial to synthesize
variables using components or factors. At the same time, they emphasize
clustering and the mean profiles of the clusters specifically for the
rows. The methodologies that fall into this category are RKM, FKM, and
DPCAKM.</p>
<p>The estimation is carried out by the LS method, while the computation of
the estimates is performed via ALS.</p>
<h5 class="unnumbered" data-number="2.2.1" id="double-k-means-dkm">Double k-means (DKM)</h5>
<p>Proposed by Vichi (2001), DKM is one of the first introduced
bi-clustering methods that provides a simultaneous partition of the
units and variables, resulting in a two-way extension of the plain KM
(McQueen 1967). The model is described by the following equation,
<span class="math display" id="eq:dkm1">\[\begin{equation}
\label{dkm1}
    \mathbf{X} = \mathbf{U}\bar{\mathbf{Y}}\mathbf{V}&#39; + \mathbf{E}
\end{equation}   \tag{14}\]</span>
where <span class="math inline">\(\bar{\mathbf{Y}}\)</span> is the centroid matrix in the reduced space for
the rows and columns, enabling a comprehensive summarization of units
and variables. By optimizing a single objective function, the DKM method
captures valuable information from both dimensions of the dataset
simultaneously.</p>
<p>This bi-clustering approach can be applied in several impactful ways.
One key application is in the realm of Big Data. DKM can effectively
compress expansive datasets that includes a vast number of units and
variables into a compressed more manageable and robust data matrix
<span class="math inline">\(\bar{\mathbf{Y}}\)</span>. This compressed matrix, formed by mean profiles both
for rows and columns, can then be explored and analyzed using a variety
of subsequent statistical techniques, thus facilitating efficient data
handling and analysis of Big Data. The algorithm similarly to the
well-known KM is very fast and converges quickly to a solution, which is
at least a local minimum of the problem.</p>
<p>Another significant application of DKM is its capability to achieve
optimal clustering for both rows and columns. This dual clustering
ability is particularly advantageous in situations where it is essential
to discern meaningful patterns and relationships within complex
datasets, highlighting the utility of DKM in diverse fields and
scenarios.</p>
<p>The Least Squares estimation of the parameters <span class="math inline">\(\mathbf{U}\)</span>,
<span class="math inline">\(\mathbf{V}\)</span> and <span class="math inline">\(\bar{\mathbf{Y}}\)</span> leads to the minimization of the
problem
<span class="math display" id="eq:dkm2">\[\begin{equation}
\label{dkm2}
    RSS_{\textit{DKM}}(\mathbf{U}, \mathbf{V}, \bar{\mathbf{Y}}) = {||\mathbf{X} - \mathbf{U}\bar{\mathbf{Y}}\mathbf{V}&#39;||^2},
\end{equation}   \tag{15}\]</span></p>
<p><span class="math display" id="eq:dkm3">\[\begin{equation}
\label{dkm3}
    s.t.:  u_{ik} \in \{0,1\}, \ \ \sum_{k} u_{ik} = 1, \ \  i = 1 ,\dots, N, \ \ k = 1 ,\dots, K,
\end{equation}   \tag{16}\]</span></p>
<p><span class="math display" id="eq:dkm4">\[\begin{equation}
\label{dkm4}
    \ \ \ \ \ \ \ v_{jq} \in \{0,1\}, \ \ \sum_{q} v_{jq} = 1, \ \ j = 1, \dots, J, \ \ q = 1, \dots, Q.
\end{equation}   \tag{17}\]</span>
Since <span class="math inline">\(\mathbf{\bar{Y}} = \mathbf{U}^{+}\mathbf{X}\mathbf{V}^{+&#39;}\)</span>, then
(<a href="#eq:dkm2">(15)</a>) can be framed in terms of projector operators, thus:
<span class="math display" id="eq:dkm5">\[\begin{equation}
\label{dkm5}
RSS_{\textit{DKM}}(\mathbf{U}, \mathbf{V}) = ||\mathbf{X} - \mathbf{H}_\mathbf{U}\mathbf{X}\mathbf{H}_\mathbf{V}||^2.
\end{equation}   \tag{18}\]</span>
Minimizing in both cases the sum of squared-residuals (or, equivalently,
the within deviances associated to the <em>K</em> unit-clusters and <em>Q</em>
variable-clusters). In this way, one obtains a (hard) classification of
both units and variables. The optimization of <a href="#eq:dkm5">(18)</a> is done
via ALS, alternating, in essence, two assignment problems for rows and
columns similar to KM steps.</p>
<h5 class="unnumbered" data-number="2.2.2" id="reduced-k-means-rkm">Reduced k-means (RKM)</h5>
<p>Proposed by De Soete and Carroll (1994), RKM performs the reduction of
the variables by projecting the <em>J</em>-dimensional centroid matrix into a
<em>Q</em>-dimensional subspace (<span class="math inline">\(\textit{Q} \leq\)</span> <em>J</em>), spanned by the columns
of the loading matrix <span class="math inline">\(\mathbf{A}\)</span>, such that it best reconstructs
<span class="math inline">\(\mathbf{X}\)</span> by using the orthogonal projector matrix
<span class="math inline">\(\mathbf{A}\mathbf{A}&#39;\)</span>. Therefore, the model is described by the
following equation,
<span class="math display" id="eq:rkm1">\[\begin{equation}
\label{rkm1}
    \mathbf{X} = \mathbf{U}\bar{\mathbf{X}}\mathbf{A}\mathbf{A}&#39; + \mathbf{E}.
\end{equation}   \tag{19}\]</span>
The estimation of <strong>U</strong> and <strong>A</strong> can be done via LS, minimizing the
following equation,
<span class="math display" id="eq:rkm2">\[\begin{equation}
\label{rkm2}
    RSS_{\textit{RKM}}(\mathbf{U}, \mathbf{A})={||\mathbf{X} - \mathbf{U}\bar{\mathbf{X}}\mathbf{A}\mathbf{A}&#39;||^2},
\end{equation}   \tag{20}\]</span></p>
<p><span class="math display" id="eq:rkm3">\[\begin{equation}
\label{rkm3}
    s.t.: \ \ \  u_{ik} \in \{0,1\}, \ \ \sum_{k} u_{ik} = 1, \ \ \mathbf{A}&#39;\mathbf{A} = \mathbf{I}.
\end{equation}   \tag{21}\]</span>
which can be optimized, once again, via ALS. In essence, the model
alternates a KM step assigning each original unit <span class="math inline">\(\mathbf{x}_i\)</span> to the
closest centroid in the reduced space and a PCA step based on the
spectral decomposition of <span class="math inline">\(\mathbf{X}&#39;\mathbf{H}_\mathbf{U}\mathbf{X}\)</span>,
conditioned on the results of the previous iteration. The iterations
continue until when the difference between two subsequent objective
functions is smaller than a small arbitrary chosen constant
<span class="math inline">\(\epsilon &gt; 0\)</span>.</p>
<h5 class="unnumbered" data-number="2.2.3" id="factorial-k-means-fkm">Factorial k-means (FKM)</h5>
<p>Proposed by Vichi and Kiers (2001), FKM produces a dimension reduction
both of the units and centroids differently from RKM. Its goal is to
reconstruct the data in the reduced subspace, <span class="math inline">\(\mathbf{Y}\)</span>, by means of
the centroids in the reduced space. The FKM model can be obtained by
considering the RKM model and post-multiplying the right- and left-hand
side of it in equation (<a href="#eq:rkm1">(19)</a>), and rewriting the new error as
<span class="math inline">\(\mathbf{E}\)</span>,
<span class="math display">\[\begin{equation}
    \mathbf{X}\mathbf{A} = \mathbf{U}\bar{\mathbf{X}}\mathbf{A} + \mathbf{E}.
\end{equation}\]</span>
Its estimation via LS results in the optimization of the following
equation,
<span class="math display" id="eq:fkm1">\[\begin{equation}
\label{fkm1}
    RSS_{\textit{FKM}}(\mathbf{U}, \mathbf{A}, \bar{\mathbf{X}})={||\mathbf{X}\mathbf{A} - \mathbf{U}\bar{\mathbf{X}}\mathbf{A}||^2},
\end{equation}   \tag{22}\]</span></p>
<p><span class="math display">\[\begin{equation}
    s.t.: \ \ \  u_{ik} \in \{0,1\}, \ \ \sum_{k} u_{ik} = 1, \ \ \mathbf{A}&#39;\mathbf{A} = \mathbf{I}.
\end{equation}\]</span>
Although the connection with the RKM model appears straightforward, it
can be shown that the loss function of the former is always equal or
smaller compared to the latter. Practically, the KM step is applied on
<span class="math inline">\(\mathbf{X}\mathbf{A}\)</span>, instead of just <span class="math inline">\(\mathbf{X}\)</span>, as it happens in
the DKM and RKM. In essence, FKM works better when the data and
centroids are lying in the reduced subspace, and not just the centroids
as in RKM.</p>
<p>In order to decide when RKM or FKM can be properly applied, it is
important to recall that two types of residuals can be defined in
dimensionality reduction: <em>subspace residuals</em>, lying on the subspace
spanned by the columns of <span class="math inline">\(\mathbf{A}\)</span> and <em>complement residuals</em>, lying
on the complement of this subspace, i.e., those residual lying on the
subspace spanned by the columns of <span class="math inline">\(\mathbf{A}^\perp\)</span>, with
<span class="math inline">\(\mathbf{A}^\perp\)</span> a column-wise orthonormal matrix of order
<span class="math inline">\(J \times (J-Q)\)</span> such that
<span class="math inline">\(\mathbf{A}^\perp \mathbf{A}^{\perp ^\prime} = \mathbf{O}_{J-Q}\)</span>, where
<span class="math inline">\(\mathbf{O}_{J-Q}\)</span> is the matrix of zeroes of order <span class="math inline">\(Q \times (J-Q)\)</span>.
FKM is more effective when there is significant residual variance in the
subspace orthogonal to the clustering subspace. In other words, the
complement residuals typically represent the error given by those
observed variables that scarcely contribute to the clustering subspace
to be identified. FKM tends to recover the subspace and clustering
structure more accurately when the data contains variables with
substantial variance that does not reflect the clustering structure and
therefore mask it. FKM can better ignore these variables and focus on
the relevant clustering subspace. On the other hand, RKM performs better
when the data has significant residual variance within the clustering
subspace itself. This means that when the variables within the subspace
show considerable variance, RKM can more effectively capture the
clustering structure.</p>
<p>In essence, when most of the variables in the dataset reflect the
clustering structure, RKM is more likely to provide a good solution. If
this is not the case, FKM may be preferred.</p>
<h5 class="unnumbered" data-number="2.2.4" id="disjoint-principal-component-analysis-k-means-dpcakm">Disjoint principal component analysis k-means (DPCAKM)</h5>
<p>Starting from the FKM model, the goal here, beside the partition of the
units, is to have a parsimonious representation of the relationships
between latent and manifest variables, provided by the loading matrix
<strong>A</strong>. Vichi and Saporta (2009) propose for FKM the parametrization of
<strong>A</strong> = <strong>BV</strong>, that allows the simplest structure and thus simplifies
the interpretation of the factors,
<span class="math display" id="eq:cdpca1">\[\begin{equation}
\label{cdpca1}
    \mathbf{X} = \mathbf{U}\bar{\mathbf{X}}\mathbf{B}\mathbf{V}\mathbf{V}&#39;\mathbf{B} + \mathbf{E}.
\end{equation}   \tag{23}\]</span>
By estimating <span class="math inline">\(\mathbf{U}\)</span>, <span class="math inline">\(\mathbf{B}\)</span>, <span class="math inline">\(\mathbf{V}\)</span> and
<span class="math inline">\(\bar{\mathbf{X}}\)</span> via LS, the loss function of the proposed method
becomes:
<span class="math display" id="eq:cdpca2">\[\begin{equation}
\label{cdpca2}
    RSS_{DPCAKM}(\mathbf{U}, \mathbf{B}, \mathbf{V}, \bar{\mathbf{X}}) = ||\mathbf{X} - \mathbf{U}\bar{\mathbf{X}}\mathbf{B}\mathbf{V}\mathbf{V}&#39;\mathbf{B}||^2,
\end{equation}   \tag{24}\]</span></p>
<p><span class="math display" id="eq:cdpca3">\[\begin{equation}
\label{cdpca3}
    s.t.:  u_{ik} \in \{0,1\}, \ \ \sum_{k} u_{ik} = 1, \ \  i = 1 ,\dots, N, \ \ k = 1 ,\dots, K,
\end{equation}   \tag{25}\]</span></p>
<p><span class="math display" id="eq:cdpca4">\[\begin{equation}
\label{cdpca4}
    \ \ \ \ \ \ \ v_{jq} \in \{0,1\}, \ \ \sum_{q} v_{jq} = 1, \ \ j = 1, \dots, J, \ \ q = 1, \dots, Q,
\end{equation}   \tag{26}\]</span></p>
<p><span class="math display" id="eq:cdpca5">\[\begin{equation}
\label{cdpca5}
    \ \ \ \ \ \ \ \mathbf{V}&#39;\mathbf{B}\mathbf{B}\mathbf{V} = \mathbf{I}, \ \  \mathbf{B} =  diag(b_1, \dots, b_J).
\end{equation}   \tag{27}\]</span>
In practice, this model has traits of the DPCA given the projection on
the reduced subspace and the partitioning of the units, resulting in a
sparse loading matrix, but also of the DKM, given the presence of both
<strong>U</strong> and <strong>V</strong>. Thus, DPCAKM can be considered a bi-clustering
methodology with an asymmetric treatment of the rows and columns of
<strong>X</strong>. By inheriting the constraint on <strong>A</strong>, the overall fit of the
model compared with the FKM for example, is generally worse although it
offers an easier interpretation of the principal components.
Nevertheless, it is potentially able to identify a better partition of
the units. Like in the DPCA case, the difference is negligible when the
true latent variables are really disjoint. As implemented, the
assignment step is carried out by minimizing the unit-centroid
squared-Euclidean distance in the reduced subspace.</p>
<h3 data-number="3" id="the-package"><span class="header-section-number">3</span> The package</h3>
<p>The library offers the implementation of all the models mentioned in the
previous section. Each one of them corresponds to a specific function
implemented using <a href="https://CRAN.R-project.org/package=Rcpp"><strong>Rcpp</strong></a>
(Eddelbuettel and Francois 2011) and
<a href="https://CRAN.R-project.org/package=RcppArmadillo"><strong>RcppArmadillo</strong></a>
(Eddelbuettel and Sanderson 2014).</p>
<div id="tab:stat_models">
<table style="width:99%;">
<caption><span id="tab:T2">Table 2: </span> Statistical methods available in the <code>drclust</code> package</caption>
<colgroup>
<col style="width: 8%" />
<col style="width: 19%" />
<col style="width: 27%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Function</th>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Previous<br />
Implementations</th>
<th style="text-align: left;">Main differences<br />
in <code>drclust</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>doublekm</code></td>
<td style="text-align: left;">DKM<br />
(Vichi 2001)</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">Short runtime (C++);</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>redkm</code></td>
<td style="text-align: left;">RKM<br />
(De Soete and Carroll 1994)</td>
<td style="text-align: left;">in <code>clusterd</code>;<br />
Mixed variables;</td>
<td style="text-align: left;">&gt;50x faster (C++);<br />
Continuous variables;</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>factkm</code></td>
<td style="text-align: left;">FKM<br />
(Vichi and Kiers 2001)</td>
<td style="text-align: left;">in <code>clustrd</code>;<br />
Mixed variables</td>
<td style="text-align: left;">&gt;20x faster (C++);<br />
Continuous variables;<br />
Better fit and classification;</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>dpcakm</code></td>
<td style="text-align: left;">DPCAKM<br />
(Vichi and Saporta 2009)</td>
<td style="text-align: left;">in <code>biplotbootGUI</code>;<br />
Continuous variables;<br />
SDP-based initialization of parameters;</td>
<td style="text-align: left;">&gt;10x faster (C++);<br />
Constraint on variable allocation within principal components;</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>dispca</code></td>
<td style="text-align: left;">DPCA<br />
(Vichi and Saporta 2009)</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">Short runtime (C++);<br />
Constraint on variable allocation within principal components;</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>disfa</code></td>
<td style="text-align: left;">DFA<br />
(Vichi 2017)</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">Short runtime (C++);<br />
Constraint on variable allocation within factors;</td>
</tr>
</tbody>
</table>
</div>
<p>Some additional functions have been made available for the user. Most of
them are intended to aid the user in evaluating the quality of the
results, or in the choice of the hyper-parameters.</p>
<div id="tab:aux_methods">
<table style="width:99%;">
<caption><span id="tab:T3">Table 3: </span> Auxiliary functions available in the library</caption>
<colgroup>
<col style="width: 6%" />
<col style="width: 11%" />
<col style="width: 68%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Function</strong></th>
<th style="text-align: left;"><strong>Technique</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Goal</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>apseudoF</code></td>
<td style="text-align: left;">"relaxed" pseudoF</td>
<td style="text-align: left;">"Relaxed" version of Caliski and Harabasz (1974). Selects the second largest pseudoF value if the difference with the first is less than a fraction.</td>
<td style="text-align: left;">Parameter tuning</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>dpseudoF</code></td>
<td style="text-align: left;">DKM-pseudoF</td>
<td style="text-align: left;">Adaptation of the pseudoF criterion proposed by Rocci and Vichi (2008) to bi-clustering.</td>
<td style="text-align: left;">Parameter tuning</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>kaiserCrit</code></td>
<td style="text-align: left;">Kaiser criterion</td>
<td style="text-align: left;">Kaiser rule for selecting the number of principal components (Kaiser 1960).</td>
<td style="text-align: left;">Parameter tuning</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>centree</code></td>
<td style="text-align: left;">Dendrogram of the centroids</td>
<td style="text-align: left;">Graphical tool showing how close the centroids of a partition are.</td>
<td style="text-align: left;">Visualization</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>silhouette</code></td>
<td style="text-align: left;">Silhouette</td>
<td style="text-align: left;">Imported from <a href="https://CRAN.R-project.org/package=cluster"><strong>cluster</strong></a> (Maechler et al.2023) and <a href="https://CRAN.R-project.org/package=factoextra"><strong>factoextra</strong></a> (Kassambara 2022).</td>
<td style="text-align: left;">Visualization, parameter tuning</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>heatm</code></td>
<td style="text-align: left;">Heatmap</td>
<td style="text-align: left;">Heatmap of distance-ordered units within distance-ordered clusters, adapted from <a href="https://CRAN.R-project.org/package=pheatmap"><strong>pheatmap</strong></a> (Kolde 2019).</td>
<td style="text-align: left;">Visualization</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>CronbachAlpha</code></td>
<td style="text-align: left;">Cronbach Alpha Index</td>
<td style="text-align: left;">Proposed by Cronbach (1951). Assesses the unidimensionality of a dataset.</td>
<td style="text-align: left;">Assessment</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>mrand</code></td>
<td style="text-align: left;">ARI</td>
<td style="text-align: left;">Assesses clustering quality based on the confusion matrix (Rand 1971).</td>
<td style="text-align: left;">Assessment</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>cluster</code></td>
<td style="text-align: left;">Membership vector</td>
<td style="text-align: left;">Returns a multinomial 1  <em>n</em> membership vector from a binary, row-stochastic <em>n</em>  <em>K</em> membership matrix; mimics <code>kmeans$cluster</code>.</td>
<td style="text-align: left;">Encoding</td>
</tr>
</tbody>
</table>
</div>
<p>With regard to the auxiliary functions (Table
<a href="#tab:T3">3</a>), they
have all been implemented in the <code>R</code> language, building on top of
packages already available on CRAN, such as
<a href="https://CRAN.R-project.org/package=cluster"><strong>cluster</strong></a> by (Maechler
et al.2023),
<a href="https://CRAN.R-project.org/package=factoextra"><strong>factoextra</strong></a> by
(Kassambara 2022),
<a href="https://CRAN.R-project.org/package=pheatmap"><strong>pheatmap</strong></a> by (Kolde
2019), which allowed for an easier implementation. One of the main goals
of the proposed package, besides spreading the availability and
usability of the statistical methods considered, is the speed of
computation. By doing so (if the memory is sufficient), the results,
also for large data matrices, can be obtained in a reasonable amount of
time. A first mean adopted to pursue such a goal is the full
implementation of the statistical methods in the C++ language. The
libraries used are <a href="https://CRAN.R-project.org/package=Rcpp"><strong>Rcpp</strong></a>
(Eddelbuettel and Francois 2011) and
<a href="https://CRAN.R-project.org/package=RcppArmadillo"><strong>RcppArmadillo</strong></a>
(Eddelbuettel and Sanderson 2014), which significantly reduced the
required runtime.</p>
<p>A practical issue that happens very often in crisp (hard) clustering,
such as KM, is the presence of empty clusters after the assignment step.
When this happens, a column of <span class="math inline">\(\mathbf{U}\)</span> has all elements equal to
zero, which can be proved to be a local minimum solution, and impedes
obtaining a solution for <span class="math inline">\((\mathbf{U}&#39;\mathbf{U})^{-1}\)</span>. This typically
happens even more often when the number of clusters <em>K</em> specified by the
user is larger than the true one or in the case of a sub-optimal
solution. Among the possible solutions addressing this issue, the one
implemented here consists in splitting the cluster with higher
within-deviance. In practice, a KM with <span class="math inline">\(\textit{K} = 2\)</span> is applied to
it, assigning to the empty cluster one of the two clusters obtained by
the procedure, which is iterated until all the empty clusters are
filled. Such a strategy guarantees that the monotonicity of the ALS
algorithm is preserved, although it is the most time-consuming one.</p>
<p>Among all the six implementations of the statistical techniques, there
are some arguments that are set to a default value. Table
<a href="#tab:T4">4</a>
describes all the arguments that have a default value. In particular,
<code>print</code>, which displays a descriptive summary of the results, is set to
zero (so the user should explicitly require to the function such
output). <code>Rndstart</code> is set as default to 20, so that the algorithm is
run 20 times until convergence. In order to have more confidence (not
certainty) that the obtained solution is a global optimum, a higher
value for this argument can be provided. With particular regard to
<code>redkm</code> and <code>factkm</code>, the argument <code>rot</code>, which performs a Varimax
rotation on the loading matrix, is set by default to 0. If the user
would like to have this performed, it must be set equal to 1. Finally,
the <code>constr</code> argument, which is available for <code>dpcakm</code> and <code>dispca</code>, is
set by default to a vector (of length <em>J</em>) of zeros, so that each
variable is selected to contribute to the most appropriate latent
variable, according to the logic of the model.</p>
<div id="tab:defaultarguments">
<table style="width:99%;">
<caption><span id="tab:T4">Table 4: </span> Arguments accepted by functions in the <code>drclust</code>
package with default values</caption>
<colgroup>
<col style="width: 3%" />
<col style="width: 15%" />
<col style="width: 74%" />
<col style="width: 5%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"><strong>Argument</strong></th>
<th style="text-align: left;"><strong>Used In</strong></th>
<th style="text-align: left;"><strong>Description</strong></th>
<th style="text-align: left;"><strong>Default Value</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>Rndstart</code></td>
<td style="text-align: left;"><code>doublekm</code>, <code>redkm</code>, <code>factkm</code>, <code>dpcakm</code>, <code>dispca</code>, <code>disfa</code></td>
<td style="text-align: left;">Number of times the model is run until convergence.</td>
<td style="text-align: left;">20</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>verbose</code></td>
<td style="text-align: left;"><code>doublekm</code>, <code>redkm</code>, <code>factkm</code>, <code>dpcakm</code>, <code>dispca</code>, <code>disfa</code></td>
<td style="text-align: left;">Outputs basic summary statistics regarding each random start (1 = enabled; 0 = disabled).</td>
<td style="text-align: left;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>maxiter</code></td>
<td style="text-align: left;"><code>doublekm</code>, <code>redkm</code>, <code>factkm</code>, <code>dpcakm</code>, <code>dispca</code>, <code>disfa</code></td>
<td style="text-align: left;">Maximum number of iterations allowed for each random start (if convergence is not yet reached)</td>
<td style="text-align: left;">100</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>tol</code></td>
<td style="text-align: left;"><code>doublekm</code>, <code>redkm</code>, <code>factkm</code>, <code>dpcakm</code>, <code>dispca</code>, <code>disfa</code></td>
<td style="text-align: left;">Tolerance threshold (maximum difference between the values of the objective function of two consecutive iterations such that convergence is assumed</td>
<td style="text-align: left;"><span class="math inline">\(10^{-6}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>tol</code></td>
<td style="text-align: left;"><code>apseudoF</code></td>
<td style="text-align: left;">Approximation value. It is half of the length of the interval put for each pF value. 0 &lt;= <code>tol</code> &lt; 1</td>
<td style="text-align: left;">0.05</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>rot</code></td>
<td style="text-align: left;"><code>redkm</code>, <code>factkm</code></td>
<td style="text-align: left;">performs varimax rotation of axes obtained via PCA (0 = <code>False</code>; 1 = <code>True</code>)</td>
<td style="text-align: left;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>prep</code></td>
<td style="text-align: left;"><code>doublekm</code>, <code>redkm</code>, <code>factkm</code>, <code>dpcakm</code>, <code>dispca</code>, <code>disfa</code></td>
<td style="text-align: left;">Pre-processing of the data. 1 performs the <em>z</em>-score transform; 2 performs the min-max transform; 0 leaves the data un-pre-processed</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>print</code></td>
<td style="text-align: left;"><code>doublekm</code>, <code>redkm</code>, <code>factkm</code>, <code>dpcakm</code>, <code>dispca</code>, <code>disfa</code></td>
<td style="text-align: left;">Final summary statistics of the performed method (1 = enabled; 0 = disabled).</td>
<td style="text-align: left;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>constr</code></td>
<td style="text-align: left;"><code>dpcakm</code>, <code>dispca</code>, <code>disfa</code></td>
<td style="text-align: left;">Vector of length <span class="math inline">\(J\)</span> (number of variables) specifying variable-to-cluster assignments. Each element can be an integer from 0 to <span class="math inline">\(Q\)</span> (number of variable-clusters or components), indicating a fixed assignment, or 0 to leave the variable unconstrained (i.e., assigned by the algorithm).</td>
<td style="text-align: left;"><code>rep(0,J)</code></td>
</tr>
</tbody>
</table>
</div>
<p>By offering a fast execution time, all the implemented models allow to
run multiple random starts of the algorithm in a reasonable amount of
time. This feature comes particularly useful given the absence of
guarantees of global optima for the ALS algorithm, which has an ad-hoc
implementation for all the models. Table
<a href="#tab:T5">5</a> shows
that, compared to the two packages which implement 3 of the 6 models in
<a href="https://CRAN.R-project.org/package=drclust"><strong>drclust</strong></a>, our proposal
is much faster than the corresponding versions implemented in <code>R</code> (Table
<a href="#tab:T5">5</a>),
providing, nevertheless, compelling results.</p>
<p>The iris dataset has been used in order to measure the performance in
terms of fit, runtime, and ARI (Rand 1971). The <em>z</em>-transform has been
applied on all the variables of the dataset. This implies that all the
variables, post-transformation, have mean equal to 0 and variance equal
to 1, by subtracting the mean to each variable and dividing the result
by the standard deviation. The same result is typically obtained by the
<code>scale(X) R</code> function.</p>
<p><span class="math display" id="eq:ztransform">\[\begin{equation}
\label{eq:ztransform}
\mathbf{Z}_{\cdot j} = \frac{\mathbf{X}_{\cdot j} - \mu_j \mathbf{1_\textit{n}}}{\sigma_j}
\end{equation}   \tag{28}\]</span>
where <span class="math inline">\(\mu_j\)</span> is the mean of the <em>j</em>-th variable and <span class="math inline">\(\sigma_\textit{j}\)</span>
its standard deviation. The subscript .<em>j</em> refers to the whole <em>j</em>-th
column of the matrix. This operation avoids the measurement scale to
have impact on the final result (and is used by default, unless
otherwise specified by the user, within all the techniques implemented
by <code>drclust</code>. In order to avoid the comparison between potentially
different objective functions, the between deviance (intended as
described by the authors in the articles where the methods have been
proposed) has been used as a fit measure and computed based on the
output provided by the functions, aiming at having homogeneity in the
evaluation metric. <em>K=3</em> and <em>Q=2</em> have been used for the clustering
algorithms, maintaining, for the two-dimensionality reduction
techniques, just <em>Q</em> = 2.</p>
<p>For each method, 100 runs have been performed and the best solution has
been picked. For each run, the maximum allowed number of iterations =
100, with a tolerance error (i.e., precision) equal to <span class="math inline">\(10^{-6}\)</span>.</p>
<div id="tab:comparison">
<table style="width:98%;">
<caption><span id="tab:T5">Table 5: </span> Performance of the variable reduction and joint
clustering-variable reduction models</caption>
<colgroup>
<col style="width: 15%" />
<col style="width: 11%" />
<col style="width: 9%" />
<col style="width: 7%" />
<col style="width: 7%" />
<col style="width: 46%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Library</th>
<th style="text-align: left;">Technique</th>
<th style="text-align: left;">Runtime</th>
<th style="text-align: left;">Fit</th>
<th style="text-align: left;">ARI</th>
<th style="text-align: left;">Fit Measure</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">clustrd</td>
<td style="text-align: left;">RKM</td>
<td style="text-align: left;">0.73</td>
<td style="text-align: left;">21.38</td>
<td style="text-align: left;">0.620</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{Y}}\mathbf{A}&#39;||^2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">RKM</td>
<td style="text-align: left;">0.01</td>
<td style="text-align: left;">21.78</td>
<td style="text-align: left;">0.620</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{Y}}\mathbf{A}&#39;||^2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">clustrd</td>
<td style="text-align: left;">FKM</td>
<td style="text-align: left;">1.89</td>
<td style="text-align: left;">4.48</td>
<td style="text-align: left;">0.098</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{Y}}||^2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">FKM</td>
<td style="text-align: left;">0.03</td>
<td style="text-align: left;">21.89</td>
<td style="text-align: left;">0.620</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{Y}}||^2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">biplotbootGUI</td>
<td style="text-align: left;">CDPCA</td>
<td style="text-align: left;">2.83</td>
<td style="text-align: left;">21.32</td>
<td style="text-align: left;">0.676</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{Y}}\mathbf{A}&#39;||^2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">CDPCA</td>
<td style="text-align: left;">0.05</td>
<td style="text-align: left;">21.34</td>
<td style="text-align: left;">0.676</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{Y}}\mathbf{A}&#39;||^2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">DKM</td>
<td style="text-align: left;">0.03</td>
<td style="text-align: left;">21.29</td>
<td style="text-align: left;">0.652</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{H_V}||^2\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">DPCA</td>
<td style="text-align: left;">&lt;0.01</td>
<td style="text-align: left;">23.70</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{Y}\mathbf{A}&#39;||^2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">DFA</td>
<td style="text-align: left;">1.11</td>
<td style="text-align: left;">55.91</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{Y}\mathbf{A}&#39;||^2\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>The results of table <a href="#tab:T5">5</a> are visually represented in figure
<a href="#fig:iriscomparison">1</a>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:iriscomparison"></span>
<img src="figures/1iriscomparison.png" alt="ARI, Fit, Runtime for the available implementations" width="2076" />
<p class="caption">
Figure 1: ARI, Fit, Runtime for the available implementations
</p>
</div>
</div>
<p>Although the runtime heavily depends on the hardware characteristics,
they have been reported within Table <a href="#tab:T5">5</a> for a relative comparison purpose only,
having run all the techniques with the same one hardware. For all the
computations within the present work, the specifics of the machine used
are: Intel(R) Core(TM) i7-8550U CPU @ 1.80GHz 2.00 GHz.</p>
<p>Besides the already mentioned difference between DPCA and DFA, it is
worth mentioning that, in terms of implementation, they retrieve the
latent variables differently. Indeed, while the DPCA relies on the
eigendecomposition, the DFA uses an implementation of the power method
(Hotelling 1933).</p>
<p>In essence, the implementation of our proposal, while being very fast,
exhibits a goodness of fit very close (sometimes better, compared) to
the available alternatives.</p>
<h3 data-number="4" id="simulation-study"><span class="header-section-number">4</span> Simulation study</h3>
<p>To better understand the capabilities of the proposed methodologies and
evaluate the performance of the drclust package, a simulation study was
conducted. In this study, we assume that the number of clusters (K) and
the number of factors (Q) are known, and we examine how results vary
across the DKM, RKM, FKM, and DPCAKM methods.</p>
<h4 class="unnumbered" data-number="4.1" id="data-generation-process">Data generation process</h4>
<p>The performance of these algorithms is tested on synthetic data
generated through a specific procedure. Initially, centroids are created
using eigendecomposition on a transformed distance matrix, resulting in
three equidistant centroids in a reduced two-dimensional space. To model
the variances and covariances among the generated units within each
cluster and to introduce heterogeneity among the units, a
variance-covariance matrix (<span class="math inline">\(\Sigma_O\)</span>) is derived from samples taken
from a zero-mean Gaussian distribution, with a specified standard
deviation (<span class="math inline">\(\sigma_u\)</span>).</p>
<p>Membership for the 1,000 units is determined based on a (K  1) vector
of prior probabilities, utilizing a multinomial distribution with (0.2,
0.3, 0.5) probabilities. For each unit, a sample is drawn from a
multivariate Gaussian distribution centered around its corresponding
centroid, using the previously generated covariance matrix (<span class="math inline">\(\Sigma_O\)</span>).
Additionally, four masking variables, which do not exhibit any
clustering structure, are generated from a zero-mean multivariate
Gaussian and scaled by a standard deviation of <span class="math inline">\(\sigma\)</span>=6. These masking
variables are added to the 2 variables that form the clustering
structure of the dataset. Then, the final sample dataset is
standardized.</p>
<p>It is important to note that the standard deviation <span class="math inline">\(\sigma_u\)</span> controls
the amount of variance in the reduced space, thus influencing the level
of subspace residuals. Conversely, <span class="math inline">\(\sigma_m\)</span> regulates the variance of
the masking variables, impacting the complement residuals.</p>
<p>This study considers various scenarios where there are <span class="math inline">\(J\)</span> = 6
variables, <span class="math inline">\(n\)</span> = 1,000 units, <span class="math inline">\(K\)</span> = 3 clusters and <span class="math inline">\(Q\)</span> = 2 factors. We
explore high, medium, and low variance <span class="math inline">\(\sigma_u\)</span> of the heterogeneity
within clusters with values of 0.8, 0.55, and 0.3. For each combination
of these parameters, <span class="math inline">\(s\)</span>=100 samples are generated. Since the design is
fully crossed, a total of 300 datasets are produced. Examples of the
generated samples are illustrated in Figure
<a href="#fig:sim123">2</a>, which
shows that as the level of within-cluster variance increases, the
variables with a clustering structure tend to create overlapping
clusters. It is worthy to inform that the two techniques dedicated
solely to variable reduction, namely DPCA and DFA, were not included in
the simulation study. This is because the studys primary focus is on
clustering and dimension reduction and the comparison with competing
implementations. However, it is worth noting that these methods are
inherently quick, as can be observed from the speed of methodologies
that combine clustering with DPCA or DFA dimension reduction methods.</p>
<h4 class="unnumbered" data-number="4.2" id="performance-evaluation">Performance evaluation</h4>
<p>The performance of the proposed methods was assessed through a
simulation study. To evaluate the accuracy in recovering the true
cluster membership of the units (<strong>U</strong>), the ARI (Hubert and Arabie
1985) was employed. The ARI quantifies the similarity between the hard
partitions generated by the estimated classification matrices and those
defined by the true partition. It considers both the reference partition
and the one produced by the algorithm under evaluation. The ARI
typically ranges from 0 to 1, where 0 indicates a level of agreement
expected by random chance, and 1 denotes a perfect match. Negative
values may also occur, indicating agreement worse than what would be
expected by chance. In order to assess the models ability to
reconstruct the underlying data structure, the between deviance, denoted
by <span class="math inline">\(f\)</span>, was computed. This measure is defined in the original works
proposing the evaluated methods and is reported in the second column
(Fit Measure) of Table <a href="#tab:T6">6</a>. For comparison, the true between deviance
<span class="math inline">\(f^{*}\)</span>, calculated from the known true, known, values of <strong>U</strong> and
<strong>A</strong>, was also computed. The difference <span class="math inline">\(f - f^{*}\)</span> was considered,
where negative values suggest potential overfitting. Furthermore, the
squared Frobenius norm <span class="math inline">\(||\mathbf{A}^* - \mathbf{A}||^2\)</span> was computed to
assess how accurately each model estimated the true loading matrix
<span class="math inline">\(\mathbf{A}^*\)</span>. This evaluation was not applicable to the DKM method, as
it does not provide estimates of the loading matrix. For each
performance metric presented in Table <a href="#tab:T6">6</a>, the median value across <span class="math inline">\(s\)</span> = 100
replicates, for each level of error (within deviance), is reported.</p>
<p>It is important to note that fit and ARI reflect distinct objectives.
While fit measures the variance explained by the model, the ARI assesses
clustering accuracy. As such, the two metrics may diverge. A model may
achieve high fit by capturing subtle variation or even noise, which may
not correspond to well-separated clusters, leading to a lower ARI.
Conversely, a method focused on maximizing cluster separation may yield
high ARI while explaining less overall variance. This trade-off is
particularly relevant in unsupervised settings, where there is no
external supervision to guide the balance between reconstruction and
partitioning. For this reason, we report both metrics to provide a more
comprehensive assessment of model performance.</p>
<h4 class="unnumbered" data-number="4.3" id="algorithms-performances-and-comparison-with-the-competing-implementations">Algorithms performances and comparison with the competing implementations</h4>
<p>For each sample, the algorithms DKM, RKM, FKM, and DPCAKM are applied
using 100 random start solutions, selecting the best one. This
significantly reduces the impact of local minima in the clustering and
dimension reduction process. Figure
<a href="#fig:sim123">2</a> depicts
the typical situation for each scenario (low, medium, high
within-cluster variance).</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:sim123"></span>
<img src="figures/2sim123.png" alt="Within-cluster variance of the simulated data (in order: low, medium, high)" width="1738" />
<p class="caption">
Figure 2: Within-cluster variance of the simulated data (in order: low, medium, high)
</p>
</div>
</div>
<div id="tab:simulation">
<table style="width:97%;">
<caption><span id="tab:T6">Table 6: </span> Comparison of joint clustering-variable reduction
methods on simulated data</caption>
<colgroup>
<col style="width: 6%" />
<col style="width: 33%" />
<col style="width: 9%" />
<col style="width: 8%" />
<col style="width: 5%" />
<col style="width: 5%" />
<col style="width: 6%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Technique</th>
<th style="text-align: left;">Fit Measure</th>
<th style="text-align: left;">Library</th>
<th style="text-align: left;">Runtime (s)</th>
<th style="text-align: left;">Fit</th>
<th style="text-align: left;">ARI</th>
<th style="text-align: left;"><span class="math inline">\(f^* - f\)</span></th>
<th style="text-align: left;"><span class="math inline">\(||\mathbf{A}^* - \mathbf{A}||^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td colspan="8" style="text-align: left;"><strong>Low</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">RKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}\mathbf{A}&#39;||^2\)</span></td>
<td style="text-align: left;">clustrd</td>
<td style="text-align: left;">164.03</td>
<td style="text-align: left;">42.76</td>
<td style="text-align: left;">1.00</td>
<td style="text-align: left;">0.00</td>
<td style="text-align: left;">2.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">RKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}\mathbf{A}&#39;||^2\)</span></td>
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">0.48</td>
<td style="text-align: left;">42.76</td>
<td style="text-align: left;">1.00</td>
<td style="text-align: left;">0.00</td>
<td style="text-align: left;">2.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">FKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}||^2\)</span></td>
<td style="text-align: left;">clustrd</td>
<td style="text-align: left;">15.48</td>
<td style="text-align: left;">2.89</td>
<td style="text-align: left;">0.35</td>
<td style="text-align: left;">39.77</td>
<td style="text-align: left;">1.99</td>
</tr>
<tr class="odd">
<td style="text-align: left;">FKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}||^2\)</span></td>
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">0.52</td>
<td style="text-align: left;">42.76</td>
<td style="text-align: left;">1.00</td>
<td style="text-align: left;">0.00</td>
<td style="text-align: left;">2.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">DPCAKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}||^2\)</span></td>
<td style="text-align: left;">biplotbootGUI</td>
<td style="text-align: left;">41.70</td>
<td style="text-align: left;">42.74</td>
<td style="text-align: left;">1.00</td>
<td style="text-align: left;">0.01</td>
<td style="text-align: left;">2.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DPCAKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}||^2\)</span></td>
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">1.37</td>
<td style="text-align: left;">42.74</td>
<td style="text-align: left;">1.00</td>
<td style="text-align: left;">0.01</td>
<td style="text-align: left;">2.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">DKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{V}||^2\)</span></td>
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">0.78</td>
<td style="text-align: left;">61.55</td>
<td style="text-align: left;">0.46</td>
<td style="text-align: left;">-18.94</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="odd">
<td colspan="8" style="text-align: left;"><strong>Medium</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">RKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}\mathbf{A}&#39;||^2\)</span></td>
<td style="text-align: left;">clustrd</td>
<td style="text-align: left;">230.31</td>
<td style="text-align: left;">39.18</td>
<td style="text-align: left;">0.92</td>
<td style="text-align: left;">-0.27</td>
<td style="text-align: left;">2.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">RKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}\mathbf{A}&#39;||^2\)</span></td>
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">0.70</td>
<td style="text-align: left;">39.18</td>
<td style="text-align: left;">0.92</td>
<td style="text-align: left;">-0.27</td>
<td style="text-align: left;">2.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">FKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}||^2\)</span></td>
<td style="text-align: left;">clustrd</td>
<td style="text-align: left;">14.31</td>
<td style="text-align: left;">2.85</td>
<td style="text-align: left;">0.28</td>
<td style="text-align: left;">36.09</td>
<td style="text-align: left;">1.99</td>
</tr>
<tr class="odd">
<td style="text-align: left;">FKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}||^2\)</span></td>
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">0.76</td>
<td style="text-align: left;">39.18</td>
<td style="text-align: left;">0.92</td>
<td style="text-align: left;">-0.27</td>
<td style="text-align: left;">2</td>
</tr>
<tr class="even">
<td style="text-align: left;">DPCAKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}||^2\)</span></td>
<td style="text-align: left;">biplotbootGUI</td>
<td style="text-align: left;">47.76</td>
<td style="text-align: left;">39.15</td>
<td style="text-align: left;">0.92</td>
<td style="text-align: left;">-0.25</td>
<td style="text-align: left;">2.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DPCAKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}||^2\)</span></td>
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">1.64</td>
<td style="text-align: left;">39.15</td>
<td style="text-align: left;">0.92</td>
<td style="text-align: left;">-0.25</td>
<td style="text-align: left;">2.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">DKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{Y}}\mathbf{V}||^2\)</span></td>
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">0.81</td>
<td style="text-align: left;">5.93</td>
<td style="text-align: left;">0.39</td>
<td style="text-align: left;">-21.00</td>
<td style="text-align: left;">-</td>
</tr>
<tr class="odd">
<td colspan="8" style="text-align: left;"><strong>High</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">RKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}\mathbf{A}&#39;||^2\)</span></td>
<td style="text-align: left;">clustrd</td>
<td style="text-align: left;">314.89</td>
<td style="text-align: left;">36.61</td>
<td style="text-align: left;">0.62</td>
<td style="text-align: left;">-2.11</td>
<td style="text-align: left;">2.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">RKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}\mathbf{A}&#39;||^2\)</span></td>
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">0.94</td>
<td style="text-align: left;">36.61</td>
<td style="text-align: left;">0.61</td>
<td style="text-align: left;">-2.11</td>
<td style="text-align: left;">2.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">FKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}||^2\)</span></td>
<td style="text-align: left;">clustrd</td>
<td style="text-align: left;">13.87</td>
<td style="text-align: left;">2.90</td>
<td style="text-align: left;">0.19</td>
<td style="text-align: left;">31.55</td>
<td style="text-align: left;">2.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">FKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}||^2\)</span></td>
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">1.02</td>
<td style="text-align: left;">36.61</td>
<td style="text-align: left;">0.61</td>
<td style="text-align: left;">-2.11</td>
<td style="text-align: left;">2.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">DPCAKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}||^2\)</span></td>
<td style="text-align: left;">biplotbootGUI</td>
<td style="text-align: left;">55.49</td>
<td style="text-align: left;">36.53</td>
<td style="text-align: left;">0.64</td>
<td style="text-align: left;">-1.99</td>
<td style="text-align: left;">2.00</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DPCAKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{A}||^2\)</span></td>
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">2.06</td>
<td style="text-align: left;">36.53</td>
<td style="text-align: left;">0.63</td>
<td style="text-align: left;">-2.01</td>
<td style="text-align: left;">2.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">DKM</td>
<td style="text-align: left;"><span class="math inline">\(||\mathbf{U}\bar{\mathbf{X}}\mathbf{V}||^2\)</span></td>
<td style="text-align: left;">drclust</td>
<td style="text-align: left;">0.84</td>
<td style="text-align: left;">58.97</td>
<td style="text-align: left;">0.29</td>
<td style="text-align: left;">-24.37</td>
<td style="text-align: left;">-</td>
</tr>
</tbody>
</table>
</div>
<p>For the three scenarios, the results are reported in
<a href="#tab:T6">6</a>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:simboxplots1"></span>
<img src="figures/3Fit.png" alt="Boxplots of the Fit results in Table \@ref(tab:T6)" width="360" />
<p class="caption">
Figure 3: Boxplots of the Fit results in Table <a href="#tab:T6">6</a>
</p>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:simboxplots2"></span>
<img src="figures/4ARI.png" alt="Boxplots of the ARI results in Table \@ref(tab:T6)" width="360" />
<p class="caption">
Figure 4: Boxplots of the ARI results in Table <a href="#tab:T6">6</a>
</p>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:simboxplots3"></span>
<img src="figures/5fsf.png" alt="Boxplots of the $f^* - f$ results in Table \@ref(tab:T6)" width="360" />
<p class="caption">
Figure 5: Boxplots of the <span class="math inline">\(f^* - f\)</span> results in Table <a href="#tab:T6">6</a>
</p>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:simboxplots4"></span>
<img src="figures/6AsA.png" alt="Boxplots of the $||\mathbf{A} - \mathbf{A}^*||^2$ metric results in Table \@ref(tab:T6)" width="360" />
<p class="caption">
Figure 6: Boxplots of the <span class="math inline">\(||\mathbf{A} - \mathbf{A}^*||^2\)</span> metric results in Table <a href="#tab:T6">6</a>
</p>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:simboxplots5"></span>
<img src="figures/7runtime_RKM.png" alt="Boxplots of the runtime results in Table \@ref(tab:T6), for the RKM" width="360" />
<p class="caption">
Figure 7: Boxplots of the runtime results in Table <a href="#tab:T6">6</a>, for the RKM
</p>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:simboxplots6"></span>
<img src="figures/8runtime_others.png" alt="Boxplots of the runtime metric results in Table \@ref(tab:T6), for DKM, DPCAKM, FKM" width="360" />
<p class="caption">
Figure 8: Boxplots of the runtime metric results in Table <a href="#tab:T6">6</a>, for DKM, DPCAKM, FKM
</p>
</div>
</div>
<p>Regarding the RKM, the
<a href="https://CRAN.R-project.org/package=drclust"><strong>drclust</strong></a> and
<a href="https://CRAN.R-project.org/package=clustrd"><strong>clustrd</strong></a> performance is
very close, both in terms of the ability to recover the data (fit) and
in terms of identifying the true classification of the objects.</p>
<p>The FKM appears to be performing way better in the
<a href="https://CRAN.R-project.org/package=drclust"><strong>drclust</strong></a> case in terms
of fit and ARI. Considering both ARI and fit for the CDPCA algorithm,
the difference between the present proposal and the one of
<a href="https://CRAN.R-project.org/package=biplotbootGUI"><strong>biplotbootGUI</strong></a> is
almost absent. Referring to the CPU runtime, all of the models proposed
are significantly faster compared to the previously available ones (RKM,
FKM and KM with DPCA). For the architecture used for the experiments,
the order of magnitude for such differences are specified in the last
column of Table <a href="#tab:T2">2</a>.</p>
<p>In general, the
<a href="https://CRAN.R-project.org/package=drclust"><strong>drclust</strong></a> shows a slight
overfit, while there is no evident difference in the ability to recover
the true <strong>A</strong>. There is no alternative implementation for the DKM, so
no comparison can be made. However, except for the ARI which is lower
than the other techniques, its fit is very close, showing a compelling
ability to reconstruct the data. In general, except for the FKM, where
our proposal outperforms the one in
<a href="https://CRAN.R-project.org/package=clustrd"><strong>clustrd</strong></a>, our proposal
is equivalent in terms of fit and ARI. However, our versions outperform
every alternative in terms of runtime. Figures
(<a href="#fig:simboxplots1">3</a> -
<a href="#fig:simboxplots6">8</a>) visually depict the situation in
<a href="#tab:T6">6</a>, showing
also the variability for each scenario, among 100 replicates. In
general, with the exception of the FKM method, where our proposed
approach outperforms the implementation available in
<a href="https://CRAN.R-project.org/package=clustrd"><strong>clustrd</strong></a>, the methods
are comparable in terms of both fit and ARI. Nevertheless, our
implementations consistently outperform all alternatives in terms of
runtime.</p>
<p>Figure(<a href="#fig:simboxplots1">3</a> -
<a href="#fig:simboxplots6">8</a>) provide a visual summary of the results
reported in Table<a href="#tab:T6">6</a>, illustrating not only the central
tendencies but also the variability across the 100 simulation replicates
for each scenario.</p>
<h3 data-number="5" id="application-on-real-data"><span class="header-section-number">5</span> Application on real data</h3>
<p>The six statistical models implemented (Table
<a href="#tab:T2">2</a>) have a
binary argument <code>print</code> which, if set to one, displays at the end of the
execution the main statistics. In the following examples, such results
are shown, using as dataset the same used by Vichi and Kiers (2001) and
made available in
<a href="https://CRAN.R-project.org/package=clustrd"><strong>clustrd</strong></a> (Markos et al.
2019) and named <code>macro</code>, which has been standardized by setting the
argument <code>prep=1</code>, which is done by default by all the techniques.
Moreover, the commands reported in each example do not specify all the
arguments available for the function, for which the default values have
been kept.</p>
<p>The first example refers to the DKM (Vichi 2001). As shown, the output
contains the fit expressed as the percentage of the total deviance
(i.e., <span class="math inline">\(||\mathbf{X}||^2\)</span>) captured by the between deviance of the
model, implementing the fit measures in (Table
<a href="#tab:T5">5</a>). The
second output is the centroid matrix <span class="math inline">\(\bar{\mathbf{Y}}\)</span>, which describes
the <em>K</em> centroids in the <em>Q</em>-dimensional space induced by the partition
of the variables and its related variable-means. What follows are the
sizes and within deviances of each unit cluster and each variable
cluster. Finally, it shows the pseudoF (Caliski and Harabasz 1974)
index, which is always computed for the partition of the units. Please
note that the data matrix provided to each function implemented in the
package needs to be in matrix format.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Macro dataset (Vichi &amp; Kiers, 2001)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(clustrd)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(macro)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>macro <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(macro)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># DKM</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> dkm <span class="ot">&lt;-</span> <span class="fu">doublekm</span>(<span class="at">X =</span> macro, <span class="at">K =</span> <span class="dv">5</span>, <span class="at">Q =</span> <span class="dv">3</span>, <span class="at">print =</span> <span class="dv">1</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Variance Explained by the <span class="fu">DKM</span> (% BSS <span class="sc">/</span> TSS)<span class="sc">:</span>  <span class="fl">44.1039</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Centroid <span class="fu">Matrix</span> (Unit<span class="sc">-</span>centroids x Variable<span class="sc">-</span>centroids)<span class="sc">:</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>           V<span class="sc">-</span>Clust <span class="dv">1</span>   V<span class="sc">-</span>Clust <span class="dv">2</span>  V<span class="sc">-</span>Clust <span class="dv">3</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>U<span class="sc">-</span>Clust <span class="dv">1</span>  <span class="fl">0.1282052</span> <span class="sc">-</span><span class="fl">0.31086968</span> <span class="sc">-</span><span class="fl">0.4224182</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>U<span class="sc">-</span>Clust <span class="dv">2</span>  <span class="fl">0.0406931</span> <span class="sc">-</span><span class="fl">0.08362029</span>  <span class="fl">0.9046692</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>U<span class="sc">-</span>Clust <span class="dv">3</span>  <span class="fl">1.4321347</span>  <span class="fl">0.51191282</span> <span class="sc">-</span><span class="fl">0.7813761</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>U<span class="sc">-</span>Clust <span class="dv">4</span> <span class="sc">-</span><span class="fl">0.9372541</span>  <span class="fl">0.22627768</span>  <span class="fl">0.1175189</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>U<span class="sc">-</span>Clust <span class="dv">5</span>  <span class="fl">1.2221058</span> <span class="sc">-</span><span class="fl">2.59078258</span> <span class="sc">-</span><span class="fl">0.1660691</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Unit<span class="sc">-</span>clusters<span class="sc">:</span> </span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>         U<span class="sc">-</span>Clust <span class="dv">1</span> U<span class="sc">-</span>Clust <span class="dv">2</span> U<span class="sc">-</span>Clust <span class="dv">3</span> U<span class="sc">-</span>Clust <span class="dv">4</span> U<span class="sc">-</span>Clust <span class="dv">5</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>Size     <span class="dv">8</span>         <span class="dv">4</span>         <span class="dv">4</span>         <span class="dv">3</span>         <span class="dv">1</span> </span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>Deviance <span class="fl">23.934373</span> <span class="fl">31.737865</span> <span class="fl">5.878199</span>  <span class="fl">4.844466</span>  <span class="fl">0.680442</span> </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>         </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Variable<span class="sc">-</span>clusters<span class="sc">:</span> </span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>         V<span class="sc">-</span>Clust <span class="dv">1</span> V<span class="sc">-</span>Clust <span class="dv">2</span> V<span class="sc">-</span>Clust <span class="dv">3</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>Size     <span class="dv">3</span>         <span class="dv">2</span>         <span class="dv">1</span>        </span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>Deviance <span class="fl">40.832173</span> <span class="fl">23.024249</span> <span class="fl">3.218923</span> </span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> pseudoF <span class="fu">Statistic</span> (Calinski<span class="sc">-</span>Harabasz)<span class="sc">:</span> <span class="fl">2.23941</span></span></code></pre></div>
<p>The second example shows as output the main quantities computed for the
<code>redkm</code> (De Soete and Carroll 1994). Differently from the DKM where the
variable reduction is operated via averages, the RKM does this via PCA
leading to a better overall fit altering also the final unit-partition,
as observable from the sizes or deviances.</p>
<p>Additionally from the DKM example, the RKM also provides the loading
matrix which projects the <em>J</em>-dimensional centroids in the
<em>Q</em>-dimensional subspace. Another important difference is the summary of
the latent factors: this table shows the information captured by the
principal components with respect to the original data. In this sense,
the output allows to distinguish between the loss due to the variable
reduction (accounted in this table) and the overall loss of the
algorithm (which accounts for the loss in the reduction of the units and
the one due to the reduction of the variables, reported in the first
line of the output).</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># RKM</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> rkm <span class="ot">&lt;-</span> <span class="fu">redkm</span>(<span class="at">X =</span> macro, <span class="at">K =</span> <span class="dv">5</span>, <span class="at">Q =</span> <span class="dv">3</span>, <span class="at">print =</span> <span class="dv">1</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Variance Explained by the <span class="fu">RKM</span> (% BSS <span class="sc">/</span> TSS)<span class="sc">:</span> <span class="fl">55.0935</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Matrix of <span class="fu">Centroids</span> (Unit<span class="sc">-</span>centroids x Principal Components)<span class="sc">:</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>              PC <span class="dv">1</span>       PC <span class="dv">2</span>       PC <span class="dv">3</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>Clust <span class="dv">1</span> <span class="sc">-</span><span class="fl">1.3372534</span> <span class="sc">-</span><span class="fl">1.1457414</span> <span class="sc">-</span><span class="fl">0.6150841</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>Clust <span class="dv">2</span>  <span class="fl">1.8834878</span> <span class="sc">-</span><span class="fl">0.0853912</span> <span class="sc">-</span><span class="fl">0.8907303</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>Clust <span class="dv">3</span>  <span class="fl">0.5759906</span>  <span class="fl">0.4187003</span>  <span class="fl">0.3739608</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>Clust <span class="dv">4</span> <span class="sc">-</span><span class="fl">0.9538864</span>  <span class="fl">1.2392976</span>  <span class="fl">0.3454186</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>Clust <span class="dv">5</span>  <span class="fl">1.0417952</span> <span class="sc">-</span><span class="fl">2.2197178</span>  <span class="fl">3.0414445</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Unit<span class="sc">-</span>clusters<span class="sc">:</span> </span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>         Clust <span class="dv">1</span>   Clust <span class="dv">2</span>  Clust <span class="dv">3</span>   Clust <span class="dv">4</span>  Clust <span class="dv">5</span> </span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>Size     <span class="dv">5</span>         <span class="dv">5</span>        <span class="dv">5</span>         <span class="dv">4</span>        <span class="dv">1</span>       </span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>Deviance <span class="fl">26.204374</span> <span class="fl">9.921313</span> <span class="fl">11.231563</span> <span class="fl">6.112386</span> <span class="fl">0.418161</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Loading <span class="fu">Matrix</span> (Manifest Variables x Latent Variables)<span class="sc">:</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>          PC <span class="dv">1</span>        PC <span class="dv">2</span>        PC <span class="dv">3</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>GDP <span class="sc">-</span><span class="fl">0.5144915</span> <span class="sc">-</span><span class="fl">0.04436269</span>  <span class="fl">0.08985135</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>LI  <span class="sc">-</span><span class="fl">0.2346937</span> <span class="sc">-</span><span class="fl">0.01773811</span> <span class="sc">-</span><span class="fl">0.86115069</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>UR  <span class="sc">-</span><span class="fl">0.3529363</span>  <span class="fl">0.53044730</span>  <span class="fl">0.28002534</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>IR  <span class="sc">-</span><span class="fl">0.4065339</span> <span class="sc">-</span><span class="fl">0.42022401</span> <span class="sc">-</span><span class="fl">0.17016203</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>TB   <span class="fl">0.1975072</span>  <span class="fl">0.69145440</span> <span class="sc">-</span><span class="fl">0.36710245</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>NNS  <span class="fl">0.5927684</span> <span class="sc">-</span><span class="fl">0.24828525</span> <span class="sc">-</span><span class="fl">0.09062404</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Summary of the latent factors<span class="sc">:</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>     Explained Variance Expl. <span class="fu">Var.</span> (<span class="sc">%) Cumulated Var. Cum. Var (%</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>PC <span class="dv">1</span> <span class="fl">1.699343</span>           <span class="fl">28.322378</span>      <span class="fl">1.699343</span>       <span class="fl">28.322378</span>   </span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>PC <span class="dv">2</span> <span class="fl">1.39612</span>            <span class="fl">23.268663</span>      <span class="fl">3.095462</span>       <span class="fl">51.591041</span>   </span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>PC <span class="dv">3</span> <span class="fl">1.182372</span>           <span class="fl">19.706208</span>      <span class="fl">4.277835</span>       <span class="fl">71.297249</span>   </span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> pseudoF <span class="fu">Statistic</span> (Calinski<span class="sc">-</span>Harabasz)<span class="sc">:</span> <span class="fl">4.29923</span></span></code></pre></div>
<p>The <code>factkm</code> (Vichi and Kiers 2001) has the same output structure of the
<code>redkm</code>. It exhibits, for the same data and hyperparameters, a similar
fit (overall and variable-wise). However, the unit-partition, as well as
the latent variables are different. This difference can be (at least)
partially justified by the difference in the objective function, which
is most evident in the assignment step.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># factorial KM</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> fkm <span class="ot">&lt;-</span> <span class="fu">factkm</span>(<span class="at">X =</span> macro, <span class="at">K =</span> <span class="dv">5</span>, <span class="at">Q =</span> <span class="dv">3</span>, <span class="at">print =</span> <span class="dv">1</span>, <span class="at">rot =</span> <span class="dv">1</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Variance Explained by the <span class="fu">FKM</span> (% BSS <span class="sc">/</span> TSS)<span class="sc">:</span> <span class="fl">55.7048</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Matrix of <span class="fu">Centroids</span> (Unit<span class="sc">-</span>centroids x Principal Components)<span class="sc">:</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>              PC <span class="dv">1</span>        PC <span class="dv">2</span>        PC <span class="dv">3</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>Clust <span class="dv">1</span> <span class="sc">-</span><span class="fl">0.7614810</span>  <span class="fl">2.16045496</span> <span class="sc">-</span><span class="fl">1.21025666</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>Clust <span class="dv">2</span>  <span class="fl">1.1707159</span> <span class="sc">-</span><span class="fl">0.08840133</span> <span class="sc">-</span><span class="fl">0.29876729</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>Clust <span class="dv">3</span> <span class="sc">-</span><span class="fl">0.9602731</span> <span class="sc">-</span><span class="fl">1.33141866</span>  <span class="fl">0.02370092</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>Clust <span class="dv">4</span>  <span class="fl">1.0782934</span>  <span class="fl">1.17952330</span>  <span class="fl">3.59632116</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>Clust <span class="dv">5</span> <span class="sc">-</span><span class="fl">1.7634699</span>  <span class="fl">0.65075735</span>  <span class="fl">0.46486440</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Unit<span class="sc">-</span>clusters<span class="sc">:</span> </span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>         Clust <span class="dv">1</span>  Clust <span class="dv">2</span>  Clust <span class="dv">3</span>  Clust <span class="dv">4</span>  Clust <span class="dv">5</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>Size     <span class="dv">9</span>        <span class="dv">5</span>        <span class="dv">3</span>        <span class="dv">2</span>        <span class="dv">1</span>      </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>Deviance <span class="fl">6.390576</span> <span class="fl">2.827047</span> <span class="fl">5.018935</span> <span class="fl">3.215995</span> <span class="dv">0</span>      </span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Loading <span class="fu">Matrix</span> (Manifest Variables x Latent Variables)<span class="sc">:</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>          PC <span class="dv">1</span>       PC <span class="dv">2</span>        PC <span class="dv">3</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>GDP <span class="sc">-</span><span class="fl">0.6515084</span> <span class="sc">-</span><span class="fl">0.1780021</span>  <span class="fl">0.37482509</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>LI  <span class="sc">-</span><span class="fl">0.3164139</span>  <span class="fl">0.1809559</span> <span class="sc">-</span><span class="fl">0.68284917</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>UR  <span class="sc">-</span><span class="fl">0.2944864</span> <span class="sc">-</span><span class="fl">0.5235492</span>  <span class="fl">0.01561022</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>IR  <span class="sc">-</span><span class="fl">0.3316254</span>  <span class="fl">0.5884434</span> <span class="sc">-</span><span class="fl">0.22101070</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>TB   <span class="fl">0.1848264</span> <span class="sc">-</span><span class="fl">0.5367239</span> <span class="sc">-</span><span class="fl">0.57166730</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>NNS  <span class="fl">0.4945307</span>  <span class="fl">0.1647067</span>  <span class="fl">0.13164438</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Summary of the latent factors<span class="sc">:</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>     Explained Variance Expl. <span class="fu">Var.</span> (<span class="sc">%) Cumulated Var. Cum. Var (%</span>)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>PC <span class="dv">1</span> <span class="fl">1.68496</span>            <span class="fl">28.082675</span>      <span class="fl">1.68496</span>        <span class="fl">28.082675</span>   </span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>PC <span class="dv">2</span> <span class="fl">1.450395</span>           <span class="fl">24.173243</span>      <span class="fl">3.135355</span>       <span class="fl">52.255917</span>   </span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>PC <span class="dv">3</span> <span class="fl">1.079558</span>           <span class="fl">17.992635</span>      <span class="fl">4.214913</span>       <span class="fl">70.248552</span>   </span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> pseudoF <span class="fu">Statistic</span> (Calinski<span class="sc">-</span>Harabasz)<span class="sc">:</span> <span class="fl">4.26936</span></span></code></pre></div>
<p><code>dpcakm</code> (Vichi and Saporta 2009) shows the same output as RKM and FKM.
The partition of the variables, described by the <span class="math inline">\(\mathbf{V}\)</span> term in
(<a href="#eq:cdpca4">(26)</a>) - (<a href="#eq:cdpca5">(27)</a>), is readable within the loading
matrix, considering a <span class="math inline">\(1\)</span> for each non-zero value. For the <code>iris</code>
dataset, the additional constraint <span class="math inline">\(\mathbf{A} = \mathbf{B}\mathbf{V}\)</span>
does not cause a significant decrease in the objective function. The
clusters, however, differ from the previous cases as well.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># K-means DPCA</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> cdpca <span class="ot">&lt;-</span> <span class="fu">dpcakm</span>(<span class="at">X =</span> macro, <span class="at">K =</span> <span class="dv">5</span>, <span class="at">Q =</span> <span class="dv">3</span>, <span class="at">print =</span> <span class="dv">1</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Variance Explained by the <span class="fu">DPCAKM</span> (% BSS <span class="sc">/</span> TSS)<span class="sc">:</span> <span class="fl">54.468</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Matrix of <span class="fu">Centroids</span> (Unit<span class="sc">-</span>centroids x Principal Components)<span class="sc">:</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>              PC <span class="dv">1</span>        PC <span class="dv">2</span>       PC <span class="dv">3</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>Clust <span class="dv">1</span>  <span class="fl">0.6717536</span>  <span class="fl">0.01042978</span> <span class="sc">-</span><span class="fl">2.7309458</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>Clust <span class="dv">2</span>  <span class="fl">3.7343724</span> <span class="sc">-</span><span class="fl">1.18771685</span>  <span class="fl">0.6320673</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>Clust <span class="dv">3</span> <span class="sc">-</span><span class="fl">0.6729575</span> <span class="sc">-</span><span class="fl">1.80822745</span>  <span class="fl">0.7239541</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>Clust <span class="dv">4</span> <span class="sc">-</span><span class="fl">0.2496002</span>  <span class="fl">1.54537904</span>  <span class="fl">0.5263009</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>Clust <span class="dv">5</span> <span class="sc">-</span><span class="fl">0.1269212</span> <span class="sc">-</span><span class="fl">0.12464388</span> <span class="sc">-</span><span class="fl">0.1748282</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Unit<span class="sc">-</span>clusters<span class="sc">:</span> </span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>         Clust <span class="dv">1</span>  Clust <span class="dv">2</span>  Clust <span class="dv">3</span> Clust <span class="dv">4</span> Clust <span class="dv">5</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>Size     <span class="dv">7</span>        <span class="dv">6</span>        <span class="dv">4</span>       <span class="dv">2</span>       <span class="dv">1</span>      </span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>Deviance <span class="fl">3.816917</span> <span class="fl">2.369948</span> <span class="fl">1.14249</span> <span class="fl">4.90759</span> <span class="dv">0</span>      </span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Loading <span class="fu">Matrix</span> (Manifest Variables x Latent Variables)<span class="sc">:</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>          PC <span class="dv">1</span>      PC <span class="dv">2</span> PC <span class="dv">3</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>GDP  <span class="fl">0.5567605</span> <span class="fl">0.0000000</span>    <span class="dv">0</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>LI   <span class="fl">0.0000000</span> <span class="fl">0.7071068</span>    <span class="dv">0</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>UR   <span class="fl">0.5711396</span> <span class="fl">0.0000000</span>    <span class="dv">0</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>IR   <span class="fl">0.0000000</span> <span class="fl">0.0000000</span>    <span class="dv">1</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>TB   <span class="fl">0.0000000</span> <span class="fl">0.7071068</span>    <span class="dv">0</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>NNS <span class="sc">-</span><span class="fl">0.6031727</span> <span class="fl">0.0000000</span>    <span class="dv">0</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Summary of the latent factors<span class="sc">:</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>     Explained Variance Expl. <span class="fu">Var.</span> (<span class="sc">%) Cumulated Var. Cum. Var (%</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>PC <span class="dv">1</span> <span class="dv">1</span>                  <span class="fl">16.666667</span>      <span class="dv">1</span>              <span class="fl">16.666667</span>   </span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>PC <span class="dv">2</span> <span class="fl">1.703964</span>           <span class="fl">28.399406</span>      <span class="fl">2.703964</span>       <span class="fl">45.066073</span>   </span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>PC <span class="dv">3</span> <span class="fl">1.175965</span>           <span class="fl">19.599421</span>      <span class="fl">3.87993</span>        <span class="fl">64.665494</span>   </span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> pseudoF <span class="fu">Statistic</span> (Calinski<span class="sc">-</span>Harabasz)<span class="sc">:</span> <span class="fl">3.26423</span></span></code></pre></div>
<p>For the <code>dispca</code> (Vichi and Saporta 2009), the output is mostly similar
(except for the part of unit-clustering) to the ones already shown.
Nevertheless, because the focus here is exclusively on the variable
reduction process, some additional information is reported in the
summary of the latent factors. Indeed, because a single principal
component summarises a subset of manifest variables, the variance of the
second component related to each of the subsets, along with the Cronbach
(1951) Alpha index is computed, in order for the user to know when the
evidence supports such strategy of dimensionality reduction. As
mentioned, this function, like in the DPCAKM case, as well as the DFA
case, it allows to constrain a subset of the <em>J</em> variables to belong to
the same cluster. In the example that follows, the first two manifest
variables are constrained to contribute to the same principal component
(which is confirmed by the output <code>A</code>). Note that the manifest variables
that have indices (colum-position in the data matrix) in correspondence
of the zeros in <code>constr</code> remain unconstrained.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># DPCA</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Impose GDP and LI to be in the same cluster</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> out <span class="ot">&lt;-</span> <span class="fu">dispca</span>(<span class="at">X =</span> macro, <span class="at">Q =</span> <span class="dv">3</span>, <span class="at">print =</span> <span class="dv">1</span>, <span class="at">constr =</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Variance explained by the <span class="fu">DPCA</span> (% BSS <span class="sc">/</span> TSS)<span class="ot">=</span> <span class="fl">63.9645</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Loading <span class="fu">Matrix</span> (Manifest Variables x Latent variables) </span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>          PC <span class="dv">1</span>       PC <span class="dv">2</span>      PC <span class="dv">3</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>GDP  <span class="fl">0.0000000</span>  <span class="fl">0.0000000</span> <span class="fl">0.7071068</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>LI   <span class="fl">0.0000000</span>  <span class="fl">0.0000000</span> <span class="fl">0.7071068</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>UR  <span class="sc">-</span><span class="fl">0.7071068</span>  <span class="fl">0.0000000</span> <span class="fl">0.0000000</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>IR   <span class="fl">0.0000000</span> <span class="sc">-</span><span class="fl">0.7071068</span> <span class="fl">0.0000000</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>TB   <span class="fl">0.0000000</span>  <span class="fl">0.7071068</span> <span class="fl">0.0000000</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>NNS  <span class="fl">0.7071068</span>  <span class="fl">0.0000000</span> <span class="fl">0.0000000</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Summary of the latent factors<span class="sc">:</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>     Explained Variance Expl. <span class="fu">Var.</span> (%) Cumulated Var.</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>PC <span class="dv">1</span>           <span class="fl">1.388294</span>       <span class="fl">23.13824</span>       <span class="fl">1.388294</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>PC <span class="dv">2</span>           <span class="fl">1.364232</span>       <span class="fl">22.73721</span>       <span class="fl">2.752527</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>PC <span class="dv">3</span>           <span class="fl">1.085341</span>       <span class="fl">18.08902</span>       <span class="fl">3.837868</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>     Cum. <span class="fu">Var</span> (%) Var. <span class="dv">2</span>nd component Cronbach<span class="st">&#39;s Alpha</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="st">PC 1     23.13824          0.6117058        -1.269545</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="st">PC 2     45.87544          0.6357675        -1.145804</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="st">PC 3     63.96447          0.9146585         0.157262</span></span></code></pre></div>
<p>The <code>disfa</code> (Vichi 2017), by assuming a probabilistic underlying model,
allows additional evaluation metrics and statistics as well. The overall
objective function is not directly comparable with the other ones, and
is expressed in absolute (not relative, like in the previous cases)
terms. The <span class="math inline">\(\chi^2\)</span> (<code>X2</code>), along with <code>BIC</code>, <code>AIC</code> and <code>RMSEA</code> allow a
robust evaluation of the results in terms of fit/parsimony. Additionally
to the DPCA case, for each variable, the function displays the
commonality with the factors, providing a standard error, as well as an
associated <em>p</em>-value for the estimate.</p>
<p>It is possible to assess by comparing the loading matrix in the DPCA
case with the DFA one, the similarity in terms of latent variables. Part
of the difference can be justified (besides the well-known distinctions
between PCA and FA) with the method used to compute each factor. While
in all the previous cases, the eigendecomposition has been employed for
this purpose, the DFA makes use of the power iteration method for the
computation of the loading matrix (Hotelling 1933).</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># disjoint FA</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> out <span class="ot">&lt;-</span> <span class="fu">disfa</span>(<span class="at">X =</span> macro, <span class="at">Q =</span> <span class="dv">3</span>, <span class="at">print =</span> <span class="dv">1</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Discrepancy of DFA<span class="sc">:</span> <span class="fl">0.296499</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Summary statistics<span class="sc">:</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>  Unknown Parameters Chi<span class="sc">-</span>square Degrees of Freedom BIC       </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>  <span class="dv">9</span>                  <span class="fl">4.447531</span>   <span class="dv">12</span>                 <span class="fl">174.048102</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>  AIC        RMSEA   </span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>  <span class="fl">165.086511</span> <span class="fl">0.157189</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Loading <span class="fu">Matrix</span> (Manifest Variables x Latent Variables) </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>      Factor <span class="dv">1</span> Factor <span class="dv">2</span>   Factor <span class="dv">3</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>GDP  <span class="fl">0.5318618</span>        <span class="dv">0</span>  <span class="fl">0.0000000</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>LI   <span class="fl">0.0000000</span>        <span class="dv">1</span>  <span class="fl">0.0000000</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>UR   <span class="fl">0.5668542</span>        <span class="dv">0</span>  <span class="fl">0.0000000</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>IR   <span class="fl">0.0000000</span>        <span class="dv">0</span>  <span class="fl">0.6035160</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>TB   <span class="fl">0.0000000</span>        <span class="dv">0</span> <span class="sc">-</span><span class="fl">0.6035152</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>NNS <span class="sc">-</span><span class="fl">0.6849942</span>        <span class="dv">0</span>  <span class="fl">0.0000000</span></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span><span class="er">&gt;</span> Summary of the latent factors<span class="sc">:</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>         Explained Variance Expl. <span class="fu">Var.</span> (<span class="sc">%) Cum. Var Cum. Var (%</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>Factor <span class="dv">1</span>          <span class="fl">1.0734177</span>       <span class="fl">17.89029</span> <span class="fl">1.073418</span>     <span class="fl">17.89029</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>Factor <span class="dv">2</span>          <span class="fl">1.0000000</span>       <span class="fl">16.66667</span> <span class="fl">2.073418</span>     <span class="fl">34.55696</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>Factor <span class="dv">3</span>          <span class="fl">0.7284622</span>       <span class="fl">12.14104</span> <span class="fl">2.801880</span>     <span class="fl">46.69800</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>         Var. <span class="dv">2</span>nd component Cronbach<span class="st">&#39;s Alpha</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="st">Factor 1          0.7001954       -0.6451803</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="st">Factor 2          0.0000000        1.0000000</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="st">Factor 3          0.6357675       -1.1458039</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a><span class="st">&gt;&gt; Detailed Manifest-variable - Latent-factor relationships</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a><span class="st">    Associated Factor Corr. Coeff. Std. Error    Pr(p&gt;|Z|)</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a><span class="st">GDP                 1    0.5318618  0.1893572 0.0157923335</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="st">LI                  2    1.0000000  0.0000000 0.0000000000</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="st">UR                  1    0.5668542  0.1842113 0.0091557523</span></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="st">IR                  3    0.6035160  0.1782931 0.0048411219</span></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a><span class="st">TB                  3   -0.6035152  0.1782932 0.0048411997</span></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a><span class="st">NNS                 1   -0.6849942  0.1629084 0.0008606488</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a><span class="st">    Var. Error Communality</span></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a><span class="st">GDP  0.7171230   0.2828770</span></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a><span class="st">LI   0.0000000   1.0000000</span></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a><span class="st">UR   0.6786764   0.3213236</span></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a><span class="st">IR   0.6357684   0.3642316</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a><span class="st">TB   0.6357695   0.3642305</span></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a><span class="st">NNS  0.5307830   0.4692170</span></span></code></pre></div>
<p>In practice, usually the <code>K</code> and <code>Q</code> hyper-parameters are not known a
priori. In such case, a possible tool that allows to investigate
plausible values for <code>Q</code> is the Kaiser criterion (Kaiser 1960), in <code>R</code>,
<code>kaiserCrit</code>), takes as a single argument the dataset and outputs a
message, as well as a scalar output indicating the number of the optimal
components based on this rule.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Kaiser criterion for the choice of Q, the number of latent components</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">kaiserCrit</span>(<span class="at">X =</span> macro)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>The number of components suggested by the Kaiser criterion is<span class="sc">:</span>  <span class="dv">3</span> </span></code></pre></div>
<p>For selecting the number of clusters, <code>K</code>, one of the most commonly used
indices is the <em>pseudoF</em> statistic, which, however, tends to
underestimate the optimal number of clusters. To address this
limitation, a "relaxed" version, referred to as <code>apseudoF</code>, has been
implemented. The <code>apseudoF</code> procedure computes the standard <code>pseudoF</code>
index over a range of possible values up to <code>maxK</code>. If a higher value of
<code>K</code> yields a pseudoF that is less than <code>tol</code> <span class="math inline">\(\cdot\)</span> pseudoF (compared
to the maximum value suggested by the plain pseudoF), then <code>apseudoF</code>
selects this alternative <code>K</code> as the optimal number of clusters.
Additionally, it generates a plot of the pseudoF values computed across
the specified <em>K</em> range. Given the hybrid nature of the proposed
methods, the function also requires specifying the clustering model to
be used: 1 = <code>doublekm</code>, 2 = <code>redkm</code>, 3 = <code>factkm</code>, 4 = <code>dpcakm</code>.
Furthermore, the number of components, <code>Q</code>, must be provided, as it also
influences the final quality of the resulting partition.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">apseudoF</span>(<span class="at">X =</span> macro, <span class="at">maxK=</span><span class="dv">10</span>, <span class="at">tol =</span> <span class="fl">0.05</span>, <span class="at">model =</span> <span class="dv">2</span>, <span class="at">Q =</span> <span class="dv">3</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>The optimal number of clusters based on the pseudoF criterion is<span class="sc">:</span> <span class="dv">5</span></span></code></pre></div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:pF-fkm"></span>
<img src="figures/9pFfactkm.png" alt="Interval-pseudoF polygonal chain" width="312" />
<p class="caption">
Figure 9: Interval-pseudoF polygonal chain
</p>
</div>
</div>
<p>While this index has been thought for one-mode clustering methods,
(Rocci and Vichi 2008) extended it for two-mode clustering methods,
allowing to apply it for methods like the <code>doublekm</code>. The <code>dpseudoF</code>
function implements it and, besides the dataset, one provides the
maximum <code>K</code> and <code>Q</code> values.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">dpseudoF</span>(<span class="at">X =</span> macro, <span class="at">maxK =</span> <span class="dv">10</span>, <span class="at">maxQ =</span> <span class="dv">5</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>           Q <span class="ot">=</span> <span class="dv">2</span>     Q <span class="ot">=</span> <span class="dv">3</span>     Q <span class="ot">=</span> <span class="dv">4</span>     Q <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>K <span class="ot">=</span> <span class="dv">2</span>  <span class="fl">38.666667</span> <span class="fl">22.800000</span> <span class="fl">16.000000</span> <span class="fl">12.222222</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>K <span class="ot">=</span> <span class="dv">3</span>  <span class="fl">22.800000</span> <span class="fl">13.875000</span>  <span class="fl">9.818182</span>  <span class="fl">7.500000</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>K <span class="ot">=</span> <span class="dv">4</span>  <span class="fl">16.000000</span>  <span class="fl">9.818182</span>  <span class="fl">6.933333</span>  <span class="fl">5.263158</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>K <span class="ot">=</span> <span class="dv">5</span>  <span class="fl">12.222222</span>  <span class="fl">7.500000</span>  <span class="fl">5.263158</span>  <span class="fl">3.958333</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>K <span class="ot">=</span> <span class="dv">6</span>   <span class="fl">9.818182</span>  <span class="fl">6.000000</span>  <span class="fl">4.173913</span>  <span class="fl">3.103448</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>K <span class="ot">=</span> <span class="dv">7</span>   <span class="fl">8.153846</span>  <span class="fl">4.950000</span>  <span class="fl">3.407407</span>  <span class="fl">2.500000</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>K <span class="ot">=</span> <span class="dv">8</span>   <span class="fl">6.933333</span>  <span class="fl">4.173913</span>  <span class="fl">2.838710</span>  <span class="fl">2.051282</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>K <span class="ot">=</span> <span class="dv">9</span>   <span class="fl">6.000000</span>  <span class="fl">3.576923</span>  <span class="fl">2.400000</span>  <span class="fl">1.704545</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>K <span class="ot">=</span> <span class="dv">10</span>  <span class="fl">5.263158</span>  <span class="fl">3.103448</span>  <span class="fl">2.051282</span>  <span class="fl">1.428571</span></span></code></pre></div>
<p>Here, the indices of the maximum value within the matrix are chosen as
the best <code>Q</code> and <code>K</code> values.</p>
<p>Just by providing the centroid matrix, one can check how those are
related. Such information is usually not provided by partitive
clustering methods, but rather for the hierarchical ones. Nevertheless,
it is always possible to construct a distance matrix based on the
centroids and represent it via a dendrogram, using an arbitrary
distance. The <code>centree</code> function does exactly this, using the the Ward
(1963) distance, which corresponds to the squared Euclidean one. In
practice, one provides as an argument the output of one of the 4 methods
performing clustering.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> out <span class="ot">&lt;-</span> <span class="fu">factkm</span>(<span class="at">X =</span> macro, <span class="at">K =</span> <span class="dv">10</span>, <span class="at">Q =</span> <span class="dv">3</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">centree</span>(<span class="at">drclust_out =</span> out)</span></code></pre></div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:fig-centree"></span>
<img src="figures/10centreedpca10m.png" alt="Dendrogram of a 10-centroids" width="344" />
<p class="caption">
Figure 10: Dendrogram of a 10-centroids
</p>
</div>
</div>
<p>If, instead, one wants to assess visually the quality of the obtained
partition, there are another instrument typically used for this purpose.
The silhouette (Rousseeuw 1987), besides summarizing this numerically,
allows to also graphically represent it. By employing
<a href="https://CRAN.R-project.org/package=cluster"><strong>cluster</strong></a> for the
computational part and
<a href="https://CRAN.R-project.org/package=factoextra"><strong>factoextra</strong></a> for the
graphical part, <code>silhouette</code> takes as argument the output of one of the
four <a href="https://CRAN.R-project.org/package=drclust"><strong>drclust</strong></a>
clustering methods and the dataset, returning the results of the two
functions with just one command.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: The same data must be provided to dpcakm and silhouette </span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> out <span class="ot">&lt;-</span> <span class="fu">dpcakm</span>(<span class="at">X =</span> macro, <span class="at">K =</span> <span class="dv">5</span>, <span class="at">Q =</span> <span class="dv">3</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">silhouette</span>(<span class="at">X =</span> macro, <span class="at">drclust_out =</span> out)   </span></code></pre></div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:silhouette"></span>
<img src="figures/11silhouettek5q3.png" alt="Silhouette of a DPCA KM solution" width="404" />
<p class="caption">
Figure 11: Silhouette of a DPCA KM solution
</p>
</div>
</div>
<p>As can be seen in Figure <a href="#fig:silhouette">11</a>, the average silhouette width is also
displayed as a scalar above the plot.</p>
<p>A purely graphical tool used to assess the dis/homogeneity of the groups
is the <code>heatmap</code>. By employing the
<a href="https://CRAN.R-project.org/package=pheatmap"><strong>pheatmap</strong></a> library
(Kolde 2019) and the result of <code>doublekm</code>, <code>redkm</code>, <code>factkm</code> or
<code>dpcakm</code>, the function orders each cluster of observations in ascending
order with regard to the distance between observation and cluster to
which it has been assigned. After doing so for each group, groups are
sorted based on the distance between their centroid and the grand mean
(i.e., the mean of all observations). The <code>heatm</code> function allows to
obtain such result. Figure <a href="#fig:silhouette">11</a> represents its graphical output.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: The same data must be provided to dpcakm and silhouette </span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> out <span class="ot">&lt;-</span> <span class="fu">doublekm</span>(<span class="at">X =</span> macro, <span class="at">K =</span> <span class="dv">5</span>, <span class="at">Q =</span> <span class="dv">3</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">heatm</span>(<span class="at">X =</span> macro, <span class="at">drclust_out =</span> out)</span></code></pre></div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:heatmap-dkm"></span>
<img src="figures/12dkmk5q3heatm.png" alt="heatmap of a double-KM solution" width="398" />
<p class="caption">
Figure 12: heatmap of a double-KM solution
</p>
</div>
</div>
<p>Biplots and parallel coordinates plots can be obtained based on the
output of the techniques in the proposed package by means of few
instructions, using libraries available on <code>CRAN</code>, such as:
<a href="https://CRAN.R-project.org/package=ggplot2"><strong>ggplot2</strong></a> (Wickham et
al.2024), <code>grid</code> (which now became a base package,
<a href="https://CRAN.R-project.org/package=dplyr"><strong>dplyr</strong></a> (Wickham et al.
2023) and <a href="https://CRAN.R-project.org/package=GGally"><strong>GGally</strong></a> by
(Schloerke et al.2024). Therefore, the user can easily visualize the
subspaces provided by the statistical techniques. In future versions of
the package, the two functions will be available as built-in. Currently,
for the biplot, we have:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(grid)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">factkm</span>(macro, <span class="at">K =</span> <span class="dv">2</span>, <span class="at">Q =</span> <span class="dv">2</span>, <span class="at">Rndstart =</span> <span class="dv">100</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare data</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(macro<span class="sc">%*%</span>out<span class="sc">$</span>A); <span class="fu">colnames</span>(Y) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Dim1&quot;</span>, <span class="st">&quot;Dim2&quot;</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>Y<span class="sc">$</span>cluster <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">cluster</span>(out<span class="sc">$</span>U))</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>arrow_scale <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(out<span class="sc">$</span>A)[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>] <span class="sc">*</span> arrow_scale</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(A) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;PC1&quot;</span>, <span class="st">&quot;PC2&quot;</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>A<span class="sc">$</span>var <span class="ot">&lt;-</span> <span class="fu">colnames</span>(macro)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Axis limits</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>lims <span class="ot">&lt;-</span> <span class="fu">range</span>(<span class="fu">c</span>(Y<span class="sc">$</span>Dim1, Y<span class="sc">$</span>Dim2, A<span class="sc">$</span>PC1, A<span class="sc">$</span>PC2)) <span class="sc">*</span> <span class="fl">1.2</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Circle</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>circle <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">cos</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi, <span class="at">length.out =</span> <span class="dv">200</span>)) <span class="sc">*</span> arrow_scale,</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>                     <span class="at">y =</span> <span class="fu">sin</span>(<span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">2</span><span class="sc">*</span>pi, <span class="at">length.out =</span> <span class="dv">200</span>)) <span class="sc">*</span> arrow_scale)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(Y, <span class="fu">aes</span>(<span class="at">x =</span> Dim1, <span class="at">y =</span> Dim2, <span class="at">color =</span> cluster)) <span class="sc">+</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_segment</span>(</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> A, <span class="fu">aes</span>(<span class="at">x =</span> <span class="dv">0</span>, <span class="at">y =</span> <span class="dv">0</span>, <span class="at">xend =</span> PC1, <span class="at">yend =</span> PC2),</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">arrow =</span> <span class="fu">arrow</span>(<span class="at">length =</span> <span class="fu">unit</span>(<span class="fl">0.2</span>, <span class="st">&quot;cm&quot;</span>)), <span class="at">inherit.aes =</span> <span class="cn">FALSE</span>, <span class="at">color =</span> <span class="st">&quot;gray40&quot;</span></span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> A, <span class="fu">aes</span>(<span class="at">x =</span> PC1, <span class="at">y =</span> PC2, <span class="at">label =</span> <span class="fu">colnames</span>(macro)), <span class="at">inherit.aes =</span> <span class="cn">FALSE</span>,</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>    <span class="at">hjust =</span> <span class="fl">1.1</span>, <span class="at">vjust =</span> <span class="fl">1.1</span>, <span class="at">size =</span> <span class="dv">3</span></span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_path</span>(<span class="at">data =</span> circle, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">inherit.aes =</span> <span class="cn">FALSE</span>,</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>            <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">color =</span> <span class="st">&quot;gray70&quot;</span>) <span class="sc">+</span></span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_fixed</span>(<span class="at">xlim =</span> lims, <span class="at">ylim =</span> lims) <span class="sc">+</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Component 1&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Component 2&quot;</span>, <span class="at">title =</span> <span class="st">&quot;Biplot&quot;</span>) <span class="sc">+</span></span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p>which leads to the result shown in Figure
<a href="#fig:boxplot1">13</a>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:boxplot1"></span>
<img src="figures/13biplot.png" alt="Biplot of a FKM solution" width="534" />
<p class="caption">
Figure 13: Biplot of a FKM solution
</p>
</div>
</div>
<p>By using essential information in the output provided by <code>factkm</code>, we
are able to see the cluster of each observation, represented in the
estimated subspace induced by <span class="math inline">\(\mathbf{A}\)</span>, as well as the relationships
between observed and latent variables via the arrows.</p>
<p>In order to obtain the parallel coordinates plot, a single instruction
is sufficient, based on the same output as a starting point.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(GGally)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">factkm</span>(macro, <span class="at">K =</span> <span class="dv">3</span>, <span class="at">Q =</span> <span class="dv">2</span>, <span class="at">Rndstart =</span> <span class="dv">100</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggparcoord</span>(</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> Y, <span class="at">columns =</span> <span class="dv">1</span><span class="sc">:</span>(<span class="fu">ncol</span>(Y)<span class="sc">-</span><span class="dv">1</span>),     </span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">groupColumn =</span> <span class="st">&quot;cluster&quot;</span>, <span class="at">scale =</span> <span class="st">&quot;uniminmax&quot;</span>, </span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">showPoints =</span> <span class="cn">FALSE</span>, <span class="at">alphaLines =</span> <span class="fl">0.5</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>) <span class="sc">+</span> </span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme_minimal</span>() <span class="sc">+</span> </span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Parallel Coordinate Plot&quot;</span>, </span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>      <span class="at">x =</span> <span class="st">&quot;Variables&quot;</span>,  <span class="at">y =</span> <span class="st">&quot;Normalized Value&quot;</span>)</span></code></pre></div>
<p>For FKM applied on <code>macro</code> dataset, the output is reported in figure
<a href="#fig:parcoord">14</a>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure"><span style="display:block;" id="fig:parcoord"></span>
<img src="figures/14parcoord.png" alt="Parallel coordinates plot of a FKM solution" width="641" />
<p class="caption">
Figure 14: Parallel coordinates plot of a FKM solution
</p>
</div>
</div>
<h3 data-number="6" id="Conclusions"><span class="header-section-number">6</span> Conclusions</h3>
<p>This work presents an R library that implements techniques of joint
dimensionality reduction and clustering. Some of them are already
implemented by other packages. In general, the performance between the
proposed implementations and the earlier ones is very close, except for
the FKM, where the new one is always better for the metrics considered
here. As an element of novelty, the empty cluster(s) issue that may
occur in the estimation process has been addressed by applying 2-means
on the cluster with the highest deviance, preserving the monotonicity of
the algorithm and providing slightly better results, at a higher
computational costs.</p>
<p>The implementation of the two dimensionality reduction methods, <code>dispca</code>
and <code>disfa</code>, as well as <code>doublekm</code> offered by our library are novel in
the sense that they do not find previous implementation in R. Besides
the methodological difference between these last two, the latent
variables are computed differently: the former uses the well-known
eigendecomposition, while the latter adopts the power method. In
general, by implementing all the models in C/C++, the speed advantage
has been shown to be remarkable compared to all the existing
comparisons. These improvements allow the application of the techniques
on datasets that are relatively large, to obtain results in reasonable
amounts of time. Some additional functions have been implemented for the
purpose of helping in the choice process for the values of the
hyperparameters. Additionally, they can also be used as an assessment
tool in order to evaluate the quality of the results provided by the
implementations.</p>
</div>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-calinski1974" class="csl-entry" role="listitem">
Caliski, T., and J. Harabasz. 1974. A Dendrite Method for Cluster
Analysis. <em>Communications in Statistics</em> 3 (1): 127.
<a href="https://doi.org/10.1080/03610927408827101" class="uri">https://doi.org/10.1080/03610927408827101</a>.
</div>
<div id="ref-cattell1965" class="csl-entry" role="listitem">
Cattell, R. B. 1965. Factor Analysis: An Introduction to Essentials i.
The Purpose and Underlying Models. <em>Biometrics</em> 21 (1): 190215.
<a href="https://doi.org/10.2307/2528364" class="uri">https://doi.org/10.2307/2528364</a>.
</div>
<div id="ref-charrad2014" class="csl-entry" role="listitem">
Charrad, M., N. Ghazzali, V. Boiteau, and A. Niknafs. 2014. NbClust: An
R Package for Determining the Relevant Number of Clusters in a Data
Set. <em>Journal of Statistical Software</em> 61 (6): 136.
<a href="https://doi.org/10.18637/jss.v061.i06" class="uri">https://doi.org/10.18637/jss.v061.i06</a>.
</div>
<div id="ref-cronbach1951" class="csl-entry" role="listitem">
Cronbach, Lee J. 1951. Coefficient Alpha and the Internal Structure of
Tests. <em>Psychometrika</em> 16 (3): 297334.
<a href="https://doi.org/10.1007/BF02310555" class="uri">https://doi.org/10.1007/BF02310555</a>.
</div>
<div id="ref-desoete1994" class="csl-entry" role="listitem">
De Soete, G., and J. D. Carroll. 1994. K-Means Clustering in a
Low-Dimensional Euclidean Space. Chap. 24 in <em>New Approaches in
Classification and Data Analysis</em>, edited by E. Diday, Y. Lechevallier,
M. Schader, P. Bertrand, and B. Burtschy. Springer.
<a href="https://doi.org/10.1007/978-3-642-51175-2_24" class="uri">https://doi.org/10.1007/978-3-642-51175-2_24</a>.
</div>
<div id="ref-desarbo1990" class="csl-entry" role="listitem">
DeSarbo, W. S., K. Jedidi, K. Cool, and D. Schendel. 1990. Simultaneous
Multidimensional Unfolding and Cluster Analysis: An Investigation of
Strategic Groups. <em>Marketing Letters</em> 2: 12946.
<a href="https://doi.org/10.1007/BF00436033" class="uri">https://doi.org/10.1007/BF00436033</a>.
</div>
<div id="ref-dray2007" class="csl-entry" role="listitem">
Dray, S., and A.-B. Dufour. 2007. The Ade4 Package: Implementing the
Duality Diagram for Ecologists. <em>Journal of Statistical Software</em> 22
(4): 120. <a href="https://doi.org/10.18637/jss.v022.i04" class="uri">https://doi.org/10.18637/jss.v022.i04</a>.
</div>
<div id="ref-eddelbuettel2011" class="csl-entry" role="listitem">
Eddelbuettel, D., and R. Francois. 2011. Rcpp: Seamless R and C++
Integration. <em>Journal of Statistical Software</em> 40 (8): 118.
<a href="https://doi.org/10.18637/jss.v040.i08" class="uri">https://doi.org/10.18637/jss.v040.i08</a>.
</div>
<div id="ref-eddelbuettel2014" class="csl-entry" role="listitem">
Eddelbuettel, D., and C. Sanderson. 2014. RcppArmadillo: Accelerating R
with High-Performance C++ Linear Algebra. <em>Computational Statistics and
Data Analysis</em> 71: 105463.
<a href="https://doi.org/10.1016/j.csda.2013.02.005" class="uri">https://doi.org/10.1016/j.csda.2013.02.005</a>.
</div>
<div id="ref-hotelling1933" class="csl-entry" role="listitem">
Hotelling, H. 1933. Analysis of a Complex of Statistical Variables into
Principal Components. <em>Journal of Educational Psychology</em> 24: 41741,
and 498520. <a href="https://doi.org/10.1037/h0071325" class="uri">https://doi.org/10.1037/h0071325</a>.
</div>
<div id="ref-HubertArabie" class="csl-entry" role="listitem">
Hubert, L., and P. Arabie. 1985. Comparing Partitions. <em>Journal of
Classification</em> 2 (1): 193218. <a href="https://doi.org/10.1007/BF01908075" class="uri">https://doi.org/10.1007/BF01908075</a>.
</div>
<div id="ref-kaiser1960" class="csl-entry" role="listitem">
Kaiser, Henry F. 1960. The Application of Electronic Computers to
Factor Analysis. <em>Educational and Psychological Measurement</em> 20 (1):
14151. <a href="https://doi.org/10.1177/001316446002000116" class="uri">https://doi.org/10.1177/001316446002000116</a>.
</div>
<div id="ref-kassambara2022" class="csl-entry" role="listitem">
Kassambara, A. 2022. <em>Factoextra: Extract and Visualize the Results of
Multivariate Data Analyses</em>. R package version 1.0.7.
<a href="https://cran.r-project.org/package=factoextra" class="uri">https://cran.r-project.org/package=factoextra</a>.
</div>
<div id="ref-kolde2019" class="csl-entry" role="listitem">
Kolde, R. 2019. <em>Pheatmap: Pretty Heatmaps</em>. R package 1.0.12.
<a href="https://cran.r-project.org/package=pheatmap" class="uri">https://cran.r-project.org/package=pheatmap</a>.
</div>
<div id="ref-lawley1962" class="csl-entry" role="listitem">
Lawley, D. N., and A. E. Maxwell. 1962. Factor Analysis as a
Statistical Method. <em>Journal of the Royal Statistical Society. Series D
(The Statistician)</em> 12 (3): 20929. <a href="https://doi.org/10.2307/2986915" class="uri">https://doi.org/10.2307/2986915</a>.
</div>
<div id="ref-le2008" class="csl-entry" role="listitem">
L, S., J. Josse, and F. Husson. 2008. FactoMineR: An R Package for
Multivariate Analysis. <em>Journal of Statistical Software</em> 25 (1): 118.
<a href="https://doi.org/10.18637/jss.v025.i01" class="uri">https://doi.org/10.18637/jss.v025.i01</a>.
</div>
<div id="ref-maechler2023" class="csl-entry" role="listitem">
Maechler, M., P. Rousseeuw, A. Struyf, M. Hubert, and K. Hornik. 2023.
<em>Cluster: Cluster Analysis Basics and Extensions</em>. R package version
2.1.6. <a href="https://CRAN.R-project.org/package=cluster" class="uri">https://CRAN.R-project.org/package=cluster</a>.
</div>
<div id="ref-markos2019" class="csl-entry" role="listitem">
Markos, A., A. I. DEnza, and M. van de Velden. 2019. Beyond Tandem
Analysis: Joint Dimension Reduction and Clustering in R. <em>Journal of
Statistical Software</em> 91 (10): 124.
<a href="https://doi.org/10.18637/jss.v091.i10" class="uri">https://doi.org/10.18637/jss.v091.i10</a>.
</div>
<div id="ref-mcqueen1967" class="csl-entry" role="listitem">
McQueen, J. 1967. Some Methods for Classification and Analysis of
Multivariate Observations. <em>Computer and Chemistry</em> 4: 25772.
<a href="https://www.cs.cmu.edu/~bhiksha/courses/mlsp.fall2010/class14/macqueen.pdf" class="uri">https://www.cs.cmu.edu/~bhiksha/courses/mlsp.fall2010/class14/macqueen.pdf</a>.
</div>
<div id="ref-nietolibreiro2023" class="csl-entry" role="listitem">
Nieto Librero, A. B., and A. Freitas. 2023. <em>biplotbootGUI: Bootstrap on
Classical Biplots and Clustering Disjoint Biplot</em>.
<a href="https://cran.r-project.org/web/packages/biplotbootGUI/index.html" class="uri">https://cran.r-project.org/web/packages/biplotbootGUI/index.html</a>.
</div>
<div id="ref-pardo2007" class="csl-entry" role="listitem">
Pardo, C. E., and P. C. Del Campo. 2007. Combination of Factorial
Methods and Cluster Analysis in R: The Package FactoClass. <em>Revista
Colombiana de Estadstica</em> 30 (2): 23145.
<a href="https://revistas.unal.edu.co/index.php/estad/article/view/29478" class="uri">https://revistas.unal.edu.co/index.php/estad/article/view/29478</a>.
</div>
<div id="ref-pearson1901" class="csl-entry" role="listitem">
Pearson, K. 1901. On Lines and Planes of Closest Fit to Systems of
Points in Space. <em>The London, Edinburgh, and Dublin Philosophical
Magazine and Journal of Science</em> 2 (11): 55972.
<a href="https://doi.org/10.1080/14786440109462720" class="uri">https://doi.org/10.1080/14786440109462720</a>.
</div>
<div id="ref-R" class="csl-entry" role="listitem">
R Core Team. 2015. <em>R: A Language and Environment for Statistical
Computing</em>. R Foundation for Statistical Computing.
<a href="http://www.R-project.org/" class="uri">http://www.R-project.org/</a>.
</div>
<div id="ref-rand1971" class="csl-entry" role="listitem">
Rand, W. M. 1971. Objective Criteria for the Evaluation of Clustering
Methods. <em>Journal of the American Statistical Association</em> 66 (336):
84650. <a href="https://doi.org/10.2307/2284239" class="uri">https://doi.org/10.2307/2284239</a>.
</div>
<div id="ref-rocci2008" class="csl-entry" role="listitem">
Rocci, R., and M. Vichi. 2008. Two-Mode Multi-Partitioning.
<em>Computational Statistics &amp; Data Analysis</em> 52 (4): 19842003.
<a href="https://doi.org/10.1016/j.csda.2007.06.025" class="uri">https://doi.org/10.1016/j.csda.2007.06.025</a>.
</div>
<div id="ref-ROUSSEEUW198753" class="csl-entry" role="listitem">
Rousseeuw, Peter J. 1987. Silhouettes: A Graphical Aid to the
Interpretation and Validation of Cluster Analysis. <em>Journal of
Computational and Applied Mathematics</em> 20: 5365.
<a href="https://doi.org/10.1016/0377-0427(87)90125-7" class="uri">https://doi.org/10.1016/0377-0427(87)90125-7</a>.
</div>
<div id="ref-ggally" class="csl-entry" role="listitem">
Schloerke, B., D. Cook, H. Hofmann, et al.2024. <em>GGally: Extension to
Ggplot2</em>. R package version 2.1.2.
<a href="https://CRAN.R-project.org/package=GGally" class="uri">https://CRAN.R-project.org/package=GGally</a>.
</div>
<div id="ref-timmerman2010" class="csl-entry" role="listitem">
Timmerman, Marieke E., Eva Ceulemans, Henk A. L. Kiers, and Maurizio
Vichi. 2010. Factorial and Reduced k-Means Reconsidered.
<em>Computational Statistics &amp; Data Analysis</em> 54 (7): 185871.
<a href="https://doi.org/10.1016/j.csda.2010.02.009" class="uri">https://doi.org/10.1016/j.csda.2010.02.009</a>.
</div>
<div id="ref-maurizio2001a" class="csl-entry" role="listitem">
Vichi, M. 2001. Double k-Means Clustering for Simultaneous
Classification of Objects and Variables. Chap. 6 in <em>Advances in
Classification and Data Analysis</em>, edited by S. Borra, R. Rocci, M.
Vichi, and M. Schader. Springer.
<a href="https://doi.org/10.1007/978-3-642-59471-7_6" class="uri">https://doi.org/10.1007/978-3-642-59471-7_6</a>.
</div>
<div id="ref-vichi2017" class="csl-entry" role="listitem">
Vichi, M. 2017. Disjoint Factor Analysis with Cross-Loadings.
<em>Advances in Data Analysis and Classification</em> 11 (4): 56391.
<a href="https://doi.org/10.1007/s11634-016-0263-9" class="uri">https://doi.org/10.1007/s11634-016-0263-9</a>.
</div>
<div id="ref-vichi2001a" class="csl-entry" role="listitem">
Vichi, Maurizio, and Henk A. L. Kiers. 2001. Factorial k-Means Analysis
for Two-Way Data. <em>Computational Statistics &amp; Data Analysis</em> 37 (1):
4964. <a href="https://doi.org/10.1016/S0167-9473(00)00064-5" class="uri">https://doi.org/10.1016/S0167-9473(00)00064-5</a>.
</div>
<div id="ref-vichi2009" class="csl-entry" role="listitem">
Vichi, Maurizio, and Gilbert Saporta. 2009. Clustering and Disjoint
Principal Component Analysis. <em>Computational Statistics &amp; Data
Analysis</em> 53 (8): 3194208.
<a href="https://doi.org/10.1016/j.csda.2008.05.028" class="uri">https://doi.org/10.1016/j.csda.2008.05.028</a>.
</div>
<div id="ref-VichiVicariKiers" class="csl-entry" role="listitem">
Vichi, M., D. Vicari, and Henk A. L. Kiers. 2019. Clustering and
Dimension Reduction for Mixed Variables. <em>Behaviormetrika</em>, 24369.
<a href="https://doi.org/10.1007/s41237-018-0068-6" class="uri">https://doi.org/10.1007/s41237-018-0068-6</a>.
</div>
<div id="ref-revelle2017" class="csl-entry" role="listitem">
W. R. Revelle. 2017. <em>Psych: Procedures for Personality and
Psychological Research</em>.
<a href="https://cran.r-project.org/web/packages/psych/index.html" class="uri">https://cran.r-project.org/web/packages/psych/index.html</a>.
</div>
<div id="ref-ward1963" class="csl-entry" role="listitem">
Ward, J. H. 1963. Hierarchical Grouping to Optimize an Objective
Function. <em>Journal of the American Statistical Association</em> 58 (301):
23644. <a href="https://doi.org/10.1080/01621459.1963.10500845" class="uri">https://doi.org/10.1080/01621459.1963.10500845</a>.
</div>
<div id="ref-ggplot2" class="csl-entry" role="listitem">
Wickham, H., W. Chang, L. Henry, et al.2024. <em>Ggplot2: Elegant Graphics
for Data Analysis</em>. R package version 3.4.4.
<a href="https://CRAN.R-project.org/package=ggplot2" class="uri">https://CRAN.R-project.org/package=ggplot2</a>.
</div>
<div id="ref-dplyr" class="csl-entry" role="listitem">
Wickham, H., R. Franois, L. Henry, and K. Mller. 2023. <em>Dplyr: A
Grammar of Data Manipulation</em>. R package version 1.1.4.
<a href="https://CRAN.R-project.org/package=dplyr" class="uri">https://CRAN.R-project.org/package=dplyr</a>.
</div>
<div id="ref-yamamoto2014" class="csl-entry" role="listitem">
Yamamoto, M., and H. Hwang. 2014. A General Formulation of Cluster
Analysis with Dimension Reduction and Subspace Separation.
<em>Behaviormetrika</em> 41: 11529. <a href="https://doi.org/10.2333/bhmk.41.115" class="uri">https://doi.org/10.2333/bhmk.41.115</a>.
</div>
<div id="ref-zou2006" class="csl-entry" role="listitem">
Zou, H., T. Hastie, and R. Tibshirani. 2006. Sparse Principal Component
Analysis. <em>Journal of Computational and Graphical Statistics</em> 15 (2):
26586. <a href="https://doi.org/" class="uri">https://doi.org/</a><a href="https://doi.org/10.1198/106186006X113430" class="uri">https://doi.org/10.1198/106186006X113430</a>.
</div>
</div>
<div class="sourceCode" id="cb15"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<h3 class="appendix" data-number="7" id="supplementary-materials"><span class="header-section-number">7</span> Supplementary materials</h3>
<p>Supplementary materials are available in addition to this article. It can be downloaded at
<a href="RJ-2025-046.zip">RJ-2025-046.zip</a></p>
<h3 class="appendix" data-number="8" id="cran-packages-used"><span class="header-section-number">8</span> CRAN packages used</h3>
<p><a href="https://cran.r-project.org/package=psych">psych</a>, <a href="https://cran.r-project.org/package=ade4">ade4</a>, <a href="https://cran.r-project.org/package=FactoMineR">FactoMineR</a>, <a href="https://cran.r-project.org/package=FactoClass">FactoClass</a>, <a href="https://cran.r-project.org/package=factoextra">factoextra</a>, <a href="https://cran.r-project.org/package=NbClust">NbClust</a>, <a href="https://cran.r-project.org/package=drclust">drclust</a>, <a href="https://cran.r-project.org/package=clustrd">clustrd</a>, <a href="https://cran.r-project.org/package=biplotbootGUI">biplotbootGUI</a>, <a href="https://cran.r-project.org/package=Rcpp">Rcpp</a>, <a href="https://cran.r-project.org/package=RcppArmadillo">RcppArmadillo</a>, <a href="https://cran.r-project.org/package=cluster">cluster</a>, <a href="https://cran.r-project.org/package=pheatmap">pheatmap</a>, <a href="https://cran.r-project.org/package=ggplot2">ggplot2</a>, <a href="https://cran.r-project.org/package=dplyr">dplyr</a>, <a href="https://cran.r-project.org/package=GGally">GGally</a></p>
<h3 class="appendix" data-number="9" id="cran-task-views-implied-by-cited-packages"><span class="header-section-number">9</span> CRAN Task Views implied by cited packages</h3>
<p><a href="https://cran.r-project.org/view=ChemPhys">ChemPhys</a>, <a href="https://cran.r-project.org/view=Cluster">Cluster</a>, <a href="https://cran.r-project.org/view=Databases">Databases</a>, <a href="https://cran.r-project.org/view=Environmetrics">Environmetrics</a>, <a href="https://cran.r-project.org/view=HighPerformanceComputing">HighPerformanceComputing</a>, <a href="https://cran.r-project.org/view=MissingData">MissingData</a>, <a href="https://cran.r-project.org/view=ModelDeployment">ModelDeployment</a>, <a href="https://cran.r-project.org/view=NetworkAnalysis">NetworkAnalysis</a>, <a href="https://cran.r-project.org/view=NumericalMathematics">NumericalMathematics</a>, <a href="https://cran.r-project.org/view=Phylogenetics">Phylogenetics</a>, <a href="https://cran.r-project.org/view=Psychometrics">Psychometrics</a>, <a href="https://cran.r-project.org/view=Robust">Robust</a>, <a href="https://cran.r-project.org/view=Spatial">Spatial</a>, <a href="https://cran.r-project.org/view=TeachingStatistics">TeachingStatistics</a></p>
<h3 class="appendix" data-number="10" id="note"><span class="header-section-number">10</span> Note</h3>
<p>This article is converted from a Legacy LaTeX article using the
<a href="https://cran.r-project.org/package=texor">texor</a> package.
The pdf version is the official version. To report a problem with the html,
refer to CONTRIBUTE on the R Journal homepage.</p>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="references">References</h3>
  <div id="references-listing"></div>
  <h3 id="reuse">Reuse</h3>
  <p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
  <h3 id="citation">Citation</h3>
  <p>For attribution, please cite this work as</p>
  <pre class="citation-appendix short">Prunila &amp; Vichi, "drclust: An R Package for Simultaneous Clustering and Dimensionality Reduction", The R Journal, 2026</pre>
  <p>BibTeX citation</p>
  <pre class="citation-appendix long">@article{RJ-2025-046,
  author = {Prunila, Ionel and Vichi, Maurizio},
  title = {drclust: An R Package for Simultaneous Clustering and Dimensionality Reduction},
  journal = {The R Journal},
  year = {2026},
  note = {https://doi.org/10.32614/RJ-2025-046},
  doi = {10.32614/RJ-2025-046},
  volume = {17},
  issue = {4},
  issn = {2073-4859},
  pages = {103-132}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<!--radix_placeholder_navigation_after_body--><html><body>
<div class="distill-site-nav distill-site-footer">
<p> The R Foundation, <a href="mailto:r-journal@r-project.org">web page
contact</a>.</p>
</div>
<!--/radix_placeholder_navigation_after_body-->
</body></html>


</body>

</html>
