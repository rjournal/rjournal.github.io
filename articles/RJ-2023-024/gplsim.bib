
@article{ronnegard_hglm_2010,
	title = {hglm: {A} {Package} for {Fitting} {Hierarchical} {Generalized} {Linear} {Models}},
	volume = {2},
	issn = {2073-4859},
	shorttitle = {hglm},
	url = {https://journal.r-project.org/archive/2010/RJ-2010-009/index.html},
	doi = {10.32614/RJ-2010-009},
	language = {en},
	number = {2},
	urldate = {2020-04-05},
	journal = {The R Journal},
	author = {Rönnegård, Lars and Shen, Xia and Alam, Moudud},
	year = {2010},
	pages = {20},
	file = {Rönnegård et al. - 2010 - hglm A Package for Fitting Hierarchical Generaliz.pdf:C\:\\Users\\zzz19\\Zotero\\storage\\Y9MMF4QT\\Rönnegård et al. - 2010 - hglm A Package for Fitting Hierarchical Generaliz.pdf:application/pdf}
}

@article{greenwell_residuals_2018,
	title = {Residuals and {Diagnostics} for {Binary} and {Ordinal} {Regression} {Models}: {An} {Introduction} to the sure {Package}},
	volume = {10},
	issn = {2073-4859},
	shorttitle = {Residuals and {Diagnostics} for {Binary} and {Ordinal} {Regression} {Models}},
	url = {https://journal.r-project.org/archive/2018/RJ-2018-004/index.html},
	doi = {10.32614/RJ-2018-004},
	language = {en},
	number = {1},
	urldate = {2020-04-05},
	journal = {The R Journal},
	author = {Greenwell, M., Brandon and McCarthy, J., Andrew and Boehmke, C., Bradley and Liu, Dungang},
	year = {2018},
	pages = {381},
	file = {Greenwell et al. - 2018 - Residuals and Diagnostics for Binary and Ordinal R.pdf:C\:\\Users\\zzz19\\Zotero\\storage\\GWD44T48\\Greenwell et al. - 2018 - Residuals and Diagnostics for Binary and Ordinal R.pdf:application/pdf}
}

@article{inan_pgee_2017,
	title = {{PGEE}: {An} {R} {Package} for {Analysis} of {Longitudinal} {Data} with {High}-{Dimensional} {Covariates}},
	volume = {9},
	issn = {2073-4859},
	shorttitle = {{PGEE}},
	url = {https://journal.r-project.org/archive/2017/RJ-2017-030/index.html},
	doi = {10.32614/RJ-2017-030},
	abstract = {We introduce an R package PGEE that implements the penalized generalized estimating equations (GEE) procedure proposed by Wang et al. (2012) to analyze longitudinal data with a large number of covariates. The PGEE package includes three main functions: CVfit, PGEE, and MGEE. The CVfit function computes the cross-validated tuning parameter for penalized generalized estimating equations. The function PGEE performs simultaneous estimation and variable selection for longitudinal data with high-dimensional covariates; whereas the function MGEE ﬁts unpenalized GEE to the data for comparison. The R package PGEE is illustrated using a yeast cell-cycle gene expression data set.},
	language = {en},
	number = {1},
	urldate = {2020-04-05},
	journal = {The R Journal},
	author = {Inan, Gul and Wang, Lan},
	year = {2017},
	pages = {393},
	file = {Inan and Wang - 2017 - PGEE An R Package for Analysis of Longitudinal Da.pdf:C\:\\Users\\zzz19\\Zotero\\storage\\G39AL5TE\\Inan and Wang - 2017 - PGEE An R Package for Analysis of Longitudinal Da.pdf:application/pdf}
}

@article{yu_penalized_2002,
	title = {Penalized {Spline} {Estimation} for {Partially} {Linear} {Single}-{Index} {Models}},
	volume = {97},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/3085829},
	abstract = {Single-index models are potentially important tools for multivariate nonparametric regression. They generalize linear regression by replacing the linear combination α$_{\textrm{0}}$ $^{\textrm{Tx}}$ with a nonparametric component, η$_{\textrm{0}}$(α$_{\textrm{0}}$ $^{\textrm{T}}$ x), where η$_{\textrm{0}}$(·) is an unknown univariate link function. By reducing the dimensionality from that of a general covariate vector x to a univariate index α$^{\textrm{T}}$$_{\textrm{0}}$ x, single-index models avoid the so-called "curse of dimensionality." We propose penalized spline (P-spline) estimation of η$_{\textrm{0}}$(·) in partially linear single-index models, where the mean function has the form η$_{\textrm{0}}$(α$_{\textrm{0}}$ $^{\textrm{T}}$ x) + β$_{\textrm{0}}$ $^{\textrm{T}}$ z. The P-spline approach offers a number of advantages over other fitting methods for single-index models. All parameters in the P-spline single-index model can be estimated simultaneously by penalized nonlinear least squares. As a direct least squares fitting method, our approach is rapid and computationally stable. Standard nonlinear least squares software can be used. Moreover, joint inference for η$_{\textrm{0}}$(·), α$_{\textrm{0}}$, and β$_{\textrm{0}}$ is possible by standard estimating equations theory such as the sandwich formula for the joint covariance matrix. Using asymptotics where the number of knots is fixed, though potentially large, we show {\textless}tex-math{\textgreater}\${\textbackslash}sqrt n\${\textless}/tex-math{\textgreater} consistency and asymptotic normality of the estimators of all parameters. These asymptotic results permit joint inference for the parameters. Several examples illustrate that the model and proposed estimation methodology can be effective in practice. We investigate inference based on the sandwich estimate through a Monte Carlo study. General L$_{\textrm{q}}$ penalty functions can be readily implemented.},
	number = {460},
	urldate = {2020-04-05},
	journal = {Journal of the American Statistical Association},
	author = {Yu, Yan and Ruppert, David},
	year = {2002},
	pages = {1042--1054},
	file = {Yu and Ruppert - 2002 - Penalized Spline Estimation for Partially Linear S.pdf:C\:\\Users\\zzz19\\Zotero\\storage\\PGJH6Z7R\\Yu and Ruppert - 2002 - Penalized Spline Estimation for Partially Linear S.pdf:application/pdf}
}

@article{yu_penalised_2017,
	title = {Penalised spline estimation for generalised partially linear single-index models},
	volume = {27},
	issn = {0960-3174},
	url = {https://doi.org/10.1007/s11222-016-9639-0},
	doi = {10.1007/s11222-016-9639-0},
	number = {2},
	urldate = {2020-07-01},
	journal = {Statistics and Computing},
	author = {Yu, Yan and Wu, Chaojiang and Zhang, Yuankun},
	month = mar,
	year = {2017},
	keywords = {Generalised additive model, Generalised linear model, Low rank approximation, Penalised splines, Profile likelihood},
	pages = {571--582},
	file = {Yu et al. - 2017 - Penalised spline estimation for generalised partia.pdf:C\:\\Users\\zzz19\\Zotero\\storage\\ZMITXR9P\\Yu et al. - 2017 - Penalised spline estimation for generalised partia.pdf:application/pdf}
}

@article{carroll_generalized_1997,
	title = {Generalized {Partially} {Linear} {Single}-{Index} {Models}},
	volume = {92},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2965697},
	doi = {10.2307/2965697},
	abstract = {The typical generalized linear model for a regression of a response Y on predictors (X, Z) has conditional mean function based on a linear combination of (X, Z). We generalize these models to have a nonparametric component, replacing the linear combination αT 0X + βT 0Z by η 0(αT 0X) + βT 0Z, where η 0(·) is an unknown function. We call these generalized partially linear single-index models (GPLSIM). The models include the "single-index" models, which have β0 = 0. Using local linear methods, we propose estimates of the unknown parameters (α0, β0) and the unknown function η 0(·) and obtain their asymptotic distributions. Examples illustrate the models and the proposed estimation methodology.},
	number = {438},
	urldate = {2020-07-02},
	journal = {Journal of the American Statistical Association},
	author = {Carroll, R. J. and Fan, Jianqing and Gijbels, Irene and Wand, M. P.},
	year = {1997},
	pages = {477--489},
	file = {Carroll et al. - 1997 - Generalized Partially Linear Single-Index Models.pdf:C\:\\Users\\zzz19\\Zotero\\storage\\F9SQN2YE\\Carroll et al. - 1997 - Generalized Partially Linear Single-Index Models.pdf:application/pdf}
}

@book{mccullagh_generalized_1989,
	title = {Generalized {Linear} {Models}, {Second} {Edition}},
	isbn = {978-0-412-31760-6},
	abstract = {The success of the first edition of Generalized Linear Models led to the updated Second Edition, which continues to provide a definitive unified, treatment of methods for the analysis of diverse types of data. Today, it remains popular for its clarity, richness of content and direct relevance to agricultural, biological, health, engineering, and other applications.The authors focus on examining the way a response variable depends on a combination of explanatory variables, treatment, and classification variables. They give particular emphasis to the important case where the dependence occurs through some unknown, linear combination of the explanatory variables.The Second Edition includes topics added to the core of the first edition, including conditional and marginal likelihood methods, estimating equations, and models for dispersion effects and components of dispersion. The discussion of other topics-log-linear and related models, log odds-ratio regression models, multinomial response models, inverse linear and related models, quasi-likelihood functions, and model checking-was expanded and incorporates significant revisions.Comprehension of the material requires simply a knowledge of matrix theory and the basic ideas of probability theory, but for the most part, the book is self-contained. Therefore, with its worked examples, plentiful exercises, and topics of direct use to researchers in many disciplines, Generalized Linear Models serves as ideal text, self-study guide, and reference.},
	language = {en},
	publisher = {CRC Press},
	author = {McCullagh, P. and Nelder, John A.},
	month = aug,
	year = {1989},
	keywords = {Mathematics / Probability \& Statistics / General},
	file = {glmbook.pdf:C\:\\Users\\zzz19\\Zotero\\storage\\T36Y4SK5\\glmbook.pdf:application/pdf}
}

@article{Hastie1986,
author = {Trevor Hastie and Robert Tibshirani},
title = {{Generalized Additive Models}},
volume = {1},
journal = {Statistical Science},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {297 -- 310},
keywords = {generalized linear models, nonlinearity, Nonparametric regression, partial residuals, smoothing},
year = {1986},
doi = {10.1214/ss/1177013604},
URL = {https://doi.org/10.1214/ss/1177013604}
}

@article{hastie_general_1986,
	title = {Generalized Additive Models},
	volume = {1},
	issn = {0883-4237, 2168-8745},
	doi = {10.1214/ss/1177013604},
	language = {EN},
	number = {3},
	journal = {Statistical Science},
	author = {Hastie, Trevor and Tibshirani, Robert},
	month = aug,
	year = {1986},
	mrnumber = {MR858512},
	zmnumber = {0645.62068},
	pages = {297--310}
}

@article{xia_semi-parametric_2006,
	title = {Semi-parametric estimation of partially linear single-index models},
	volume = {97},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X05001995},
	doi = {10.1016/j.jmva.2005.11.005},
	abstract = {One of the most difficult problems in applications of semi-parametric partially linear single-index models (PLSIM) is the choice of pilot estimators and complexity parameters which may result in radically different estimators. Pilot estimators are often assumed to be root-n consistent, although they are not given in a constructible way. Complexity parameters, such as a smoothing bandwidth are constrained to a certain speed, which is rarely determinable in practical situations. In this paper, efficient, constructible and practicable estimators of PLSIMs are designed with applications to time series. The proposed technique answers two questions from Carroll et al. [Generalized partially linear single-index models, J. Amer. Statist. Assoc. 92 (1997) 477–489]: no root-n pilot estimator for the single-index part of the model is needed and complexity parameters can be selected at the optimal smoothing rate. The asymptotic distribution is derived and the corresponding algorithm is easily implemented. Examples from real data sets (credit-scoring and environmental statistics) illustrate the technique and the proposed methodology of minimum average variance estimation (MAVE).},
	language = {en},
	number = {5},
	urldate = {2020-07-29},
	journal = {Journal of Multivariate Analysis},
	author = {Xia, Yingcun and Härdle, Wolfgang},
	month = may,
	year = {2006},
	keywords = {Asymptotic distribution, Generalized partially linear model, Local linear smoother, Optimal consistency rate, Single-index model},
	pages = {1162--1184},
	file = {ScienceDirect Snapshot:C\:\\Users\\zzz19\\Zotero\\storage\\AKAZWJ2M\\S0047259X05001995.html:text/html;ScienceDirect Full Text PDF:C\:\\Users\\zzz19\\Zotero\\storage\\RLD482QR\\Xia and Härdle - 2006 - Semi-parametric estimation of partially linear sin.pdf:application/pdf}
}

@article{liang_estimation_2010,
	title = {{ESTIMATION} {AND} {TESTING} {FOR} {PARTIALLY} {LINEAR} {SINGLE}-{INDEX} {MODELS}},
	volume = {38},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/29765281},
	abstract = {In partially linear single-index models, we obtain the semiparametrically efficient profile least-squares estimators of regression coefficients. We also employ the smoothly clipped absolute deviation penalty (SCAD) approach to simultaneously select variables and estimate regression coefficients. We show that the resulting SCAD estimators are consistent and possess the oracle property. Subsequently, we demonstrate that a proposed tuning parameter selector, BIC, identifies the true model consistently. Finally, we develop a linear hypothesis test for the parametric coefficients and a goodness-of-fit test for the nonparametric component, respectively. Monte Carlo studies are also presented.},
	number = {6},
	urldate = {2020-07-29},
	journal = {The Annals of Statistics},
	author = {Liang, Hua and Liu, Xiang and Li, Runze and Tsai, Chih-Ling},
	year = {2010},
	pages = {3811--3836},
	file = {Liang et al. - 2010 - ESTIMATION AND TESTING FOR PARTIALLY LINEAR SINGLE.pdf:C\:\\Users\\zzz19\\Zotero\\storage\\IW99D6FU\\Liang et al. - 2010 - ESTIMATION AND TESTING FOR PARTIALLY LINEAR SINGLE.pdf:application/pdf}
}

@article{yi_analysis_2009,
	title = {Analysis of {Correlated} {Binary} {Data} under {Partially} {Linear} {Single}-{Index} {Logistic} {Models}},
	volume = {100},
	issn = {0047-259X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2678738/},
	doi = {10.1016/j.jmva.2008.04.012},
	abstract = {Clustered data arise commonly in practice and it is often of interest to estimate the mean response parameters as well as the association parameters. However, most research has been directed to address the mean response parameters with the association parameters relegated to a nuisance role. There is relatively little work concerning both the marginal and association structures, especially in the semiparametric framework. In this paper, our interest centers on inference on both the marginal and association parameters. We develop a semiparametric method for clustered binary data and establish the theoretical results. The proposed methodology is investigated through various numerical studies.},
	number = {2},
	urldate = {2020-07-29},
	journal = {Journal of multivariate analysis},
	author = {Yi, Grace Y. and He, Wenqing and Liang, Hua},
	month = feb,
	year = {2009},
	pmid = {20126288},
	pmcid = {PMC2678738},
	pages = {278--290},
	file = {PubMed Central Full Text PDF:C\:\\Users\\zzz19\\Zotero\\storage\\DMGJ3XG2\\Yi et al. - 2009 - Analysis of Correlated Binary Data under Partially.pdf:application/pdf}
}

@article{poon_bayesian_2013,
	title = {Bayesian analysis of generalized partially linear single-index models},
	volume = {68},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947313002636},
	doi = {10.1016/j.csda.2013.07.018},
	abstract = {We extend generalized partially linear single-index models by incorporating a random residual effect into the nonlinear predictor so that the new models can accommodate data with overdispersion. Based on the free-knot spline techniques, we develop a fully Bayesian method to analyze the proposed models. To make the models spatially adaptive, we further treat the number and positions of spline knots as random variables. As random residual effects are introduced, many of the completely conditional posteriors become standard distributions, which greatly facilitates sampling. We illustrate the proposed models and estimation method with a simulation study and an analysis of a recreational trip data set.},
	language = {en},
	urldate = {2020-07-30},
	journal = {Computational Statistics \& Data Analysis},
	author = {Poon, Wai-Yin and Wang, Hai-Bin},
	month = dec,
	year = {2013},
	keywords = {Single-index model, Free-knot spline, Generalized linear model, Gibbs sampler, Overdispersion, Reversible jump Markov chain Monte Carlo},
	pages = {251--261},
	file = {ScienceDirect Snapshot:C\:\\Users\\zzz19\\Zotero\\storage\\MYWMZXCX\\S0167947313002636.html:text/html}
}

@article{ruppert_theory_2000,
	title = {Theory \& {Methods}: {Spatially}-adaptive {Penalties} for {Spline} {Fitting}},
	volume = {42},
	copyright = {Australian Statistical Publishing Association Inc. 2000},
	issn = {1467-842X},
	shorttitle = {Theory \& {Methods}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-842X.00119},
	doi = {10.1111/1467-842X.00119},
	abstract = {The paper studies spline fitting with a roughness penalty that adapts to spatial heterogeneity in the regression function. The estimates are pth degree piecewise polynomials with p− 1 continuous derivatives. A large and fixed number of knots is used and smoothing is achieved by putting a quadratic penalty on the jumps of the pth derivative at the knots. To be spatially adaptive, the logarithm of the penalty is itself a linear spline but with relatively few knots and with values at the knots chosen to minimize the generalized cross validation (GCV) criterion. This locally-adaptive spline estimator is compared with other spline estimators in the literature such as cubic smoothing splines and knot-selection techniques for least squares regression. Our estimator can be interpreted as an empirical Bayes estimate for a prior allowing spatial heterogeneity. In cases of spatially heterogeneous regression functions, empirical Bayes confidence intervals using this prior achieve better pointwise coverage probabilities than confidence intervals based on a global-penalty parameter. The method is developed first for univariate models and then extended to additive models.},
	language = {en},
	number = {2},
	urldate = {2020-07-30},
	journal = {Australian \& New Zealand Journal of Statistics},
	author = {Ruppert, David and Carroll, Raymond J.},
	year = {2000},
	keywords = {additive models, Bayesian inference, confidence intervals, hierarchical Bayesian model, regression splines.},
	pages = {205--223},
}

@article{wood_fast_2011,
	title = {Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models},
	volume = {73},
	copyright = {© 2010 Royal Statistical Society},
	issn = {1467-9868},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2010.00749.x},
	doi = {10.1111/j.1467-9868.2010.00749.x},
	abstract = {Summary. Recent work by Reiss and Ogden provides a theoretical basis for sometimes preferring restricted maximum likelihood (REML) to generalized cross-validation (GCV) for smoothing parameter selection in semiparametric regression. However, existing REML or marginal likelihood (ML) based methods for semiparametric generalized linear models (GLMs) use iterative REML or ML estimation of the smoothing parameters of working linear approximations to the GLM. Such indirect schemes need not converge and fail to do so in a non-negligible proportion of practical analyses. By contrast, very reliable prediction error criteria smoothing parameter selection methods are available, based on direct optimization of GCV, or related criteria, for the GLM itself. Since such methods directly optimize properly defined functions of the smoothing parameters, they have much more reliable convergence properties. The paper develops the first such method for REML or ML estimation of smoothing parameters. A Laplace approximation is used to obtain an approximate REML or ML for any GLM, which is suitable for efficient direct optimization. This REML or ML criterion requires that Newton–Raphson iteration, rather than Fisher scoring, be used for GLM fitting, and a computationally stable approach to this is proposed. The REML or ML criterion itself is optimized by a Newton method, with the derivatives required obtained by a mixture of implicit differentiation and direct methods. The method will cope with numerical rank deficiency in the fitted model and in fact provides a slight improvement in numerical robustness on the earlier method of Wood for prediction error criteria based smoothness selection. Simulation results suggest that the new REML and ML methods offer some improvement in mean-square error performance relative to GCV or Akaike's information criterion in most cases, without the small number of severe undersmoothing failures to which Akaike's information criterion and GCV are prone. This is achieved at the same computational cost as GCV or Akaike's information criterion. The new approach also eliminates the convergence failures of previous REML- or ML-based approaches for penalized GLMs and usually has lower computational cost than these alternatives. Example applications are presented in adaptive smoothing, scalar on function regression and generalized additive model selection.},
	language = {en},
	number = {1},
	urldate = {2020-07-30},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Wood, Simon N.},
	year = {2011},
	keywords = {Model selection, Adaptive smoothing, Generalized additive mixed model, Generalized additive model, Generalized cross-validation, Marginal likelihood, Penalized generalized linear model, Penalized regression splines, Restricted maximum likelihood, Scalar on function regression, Stable computation},
	pages = {3--36},
	file = {Snapshot:C\:\\Users\\zzz19\\Zotero\\storage\\QMFRQSUS\\j.1467-9868.2010.00749.html:text/html;Accepted Version:C\:\\Users\\zzz19\\Zotero\\storage\\WKCQTEKS\\Wood - 2011 - Fast stable restricted maximum likelihood and marg.pdf:application/pdf}
}

@article{craven_smoothing_1978,
	title = {Smoothing noisy data with spline functions},
	volume = {31},
	issn = {0945-3245},
	url = {https://doi.org/10.1007/BF01404567},
	doi = {10.1007/BF01404567},
	abstract = {Smoothing splines are well known to provide nice curves which smooth discrete, noisy data. We obtain a practical, effective method for estimating the optimum amount of smoothing from the data. Derivatives can be estimated from the data by differentiating the resulting (nearly) optimally smoothed spline.},
	language = {en},
	number = {4},
	urldate = {2020-07-30},
	journal = {Numerische Mathematik},
	author = {Craven, Peter and Wahba, Grace},
	month = dec,
	year = {1978},
	pages = {377--403},
	file = {Springer Full Text PDF:C\:\\Users\\zzz19\\Zotero\\storage\\N8Z7CMH4\\Craven and Wahba - 1978 - Smoothing noisy data with spline functions.pdf:application/pdf}
}

@article{wahba_comparison_1985,
	title = {A {Comparison} of {GCV} and {GML} for {Choosing} the {Smoothing} {Parameter} in the {Generalized} {Spline} {Smoothing} {Problem}},
	volume = {13},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176349743},
	doi = {10.1214/aos/1176349743},
	language = {EN},
	number = {4},
	urldate = {2020-07-30},
	journal = {Annals of Statistics},
	author = {Wahba, Grace},
	month = dec,
	year = {1985},
	keywords = {cross validation, integral equations, maximum likelihood, Spline smoothing},
	pages = {1378--1402},
	file = {Full Text PDF:C\:\\Users\\zzz19\\Zotero\\storage\\4VGHNKQ2\\Wahba - 1985 - A Comparison of GCV and GML for Choosing the Smoot.pdf:application/pdf;Snapshot:C\:\\Users\\zzz19\\Zotero\\storage\\NCXRCDTH\\1176349743.html:text/html}
}

@article{anderssen_time_1974,
	title = {A {Time} {Series} {Approach} to {Numerical} {Differentiation}},
	volume = {16},
	issn = {0040-1706},
	url = {https://www.jstor.org/stable/1267494},
	doi = {10.2307/1267494},
	abstract = {The problem of obtaining the derivative of a set of data arises naturally in many fields. The usual methods for obtaining derivatives are based on abstract formulations of the problem, which do not take errors of observation explicitly into account. For this reason, their performance when applied to observational data is unpredictable. By introducing random errors into the model, one may derive methods whose performance may be stated in statistical terms. The theory of time series analysis provides useful tools for discussing such a model. A parametric family of models is introduced, and estimation of the parameters is discussed.},
	number = {1},
	urldate = {2020-07-30},
	journal = {Technometrics},
	author = {Anderssen, R. S. and Bloomfield, Peter},
	year = {1974},
	pages = {69--75},
	file = {Anderssen and Bloomfield - 1974 - A Time Series Approach to Numerical Differentiatio.pdf:C\:\\Users\\zzz19\\Zotero\\storage\\45ME4QR3\\Anderssen and Bloomfield - 1974 - A Time Series Approach to Numerical Differentiatio.pdf:application/pdf}
}

@article{wood_mgcv_2001,
	title = {mgcv: {GAMs} and {Generalized} {Ridge} {Regression} for {R}},
	volume = {Vol.1},
	number = {2},
	journal = {R news},
	author = {Wood, Simon N.},
	month = jun,
	year = {2001},
	pages = {20},
	file = {Rgam.pdf:C\:\\Users\\zzz19\\Zotero\\storage\\GG6JATV2\\Rgam.pdf:application/pdf}
}

@book{hardle_partially_2012,
	title = {Partially {Linear} {Models}},
	isbn = {978-3-642-57700-0},
	abstract = {In the last ten years, there has been increasing interest and activity in the general area of partially linear regression smoothing in statistics. Many methods and techniques have been proposed and studied. This monograph hopes to bring an up-to-date presentation of the state of the art of partially linear regression techniques. The emphasis is on methodologies rather than on the theory, with a particular focus on applications of partially linear regression techniques to various statistical problems. These problems include least squares regression, asymptotically efficient estimation, bootstrap resampling, censored data analysis, linear measurement error models, nonlinear measurement models, nonlinear and nonparametric time series models.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Härdle, Wolfgang and Liang, Hua and Gao, Jiti},
	month = dec,
	year = {2012},
	keywords = {Mathematics / Probability \& Statistics / General, Business \& Economics / Econometrics, Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{hardle_optimal_1993,
	title = {Optimal {Smoothing} in {Single}-{Index} {Models}},
	volume = {21},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/3035585},
	abstract = {Single-index models generalize linear regression. They have applications to a variety of fields, such as discrete choice analysis in econometrics and dose response models in biometrics, where high-dimensional regression models are often employed. Single-index models are similar to the first step of projection pursuit regression, a dimension-reduction method. In both cases the orientation vector can be estimated root-n consistently, even if the unknown univariate function (or nonparametric link function) is assumed to come from a large smoothness class. However, as we show in the present paper, the similarities end there. In particular, the amount of smoothing necessary for root-n consistent orientation estimation is very different in the two cases. We suggest a simple, empirical rule for selecting the bandwidth appropriate to single-index models. This rule is studies in a small simulation study and an application in binary response models.},
	number = {1},
	urldate = {2020-09-29},
	journal = {The Annals of Statistics},
	author = {Härdle, Wolfgang and Hall, Peter and Ichimura, Hidehiko},
	year = {1993},
	pages = {157--178}
}

@article{ichimura_semiparametric_1993,
	title = {Semiparametric least squares ({SLS}) and weighted {SLS} estimation of single-index models},
	volume = {58},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/030440769390114K},
	doi = {10.1016/0304-4076(93)90114-K},
	abstract = {For the class of single-index models, I construct a semiparametric estimator of coefficients up to a multiplicative constant that exhibits 1√n-consistency and asymptotic normality. This class of models includes censored and truncated Tobit models, binary choice models, and duration models with unobserved individual heterogeneity and random censoring. I also investigate a weighting scheme that achieves the semiparametric efficiency bound.},
	language = {en},
	number = {1},
	urldate = {2020-09-29},
	journal = {Journal of Econometrics},
	author = {Ichimura, Hidehiko},
	month = jul,
	year = {1993},
	pages = {71--120},
	file = {ScienceDirect Snapshot:C\:\\Users\\zzz19\\Zotero\\storage\\BGNHASFY\\030440769390114K.html:text/html;Submitted Version:C\:\\Users\\zzz19\\Zotero\\storage\\5T8M7UFI\\Ichimura - 1993 - Semiparametric least squares (SLS) and weighted SL.pdf:application/pdf}
}

@book{boor_practical_2001,
	address = {New York},
	title = {A {Practical} {Guide} to {Splines}},
	isbn = {978-0-387-95366-3},
	language = {English},
	publisher = {Springer},
	author = {Boor, Carl de},
	month = nov,
	year = {2001}
}

@book{ruppert2003semiparametric,
  title={Semiparametric regression},
  author={Ruppert, David and Wand, Matt P and Carroll, Raymond J},
  series={12},
  year={2003},
  publisher={Cambridge university press}
}


@book{hastie_generalized_1990,
	edition = {1st edition},
	title = {Generalized {Additive} {Models}},
	isbn = {978-1-58488-474-3},
	publisher = {Chapman and Hall/CRC},
	author = {Hastie, Trevor and Tibshirani, Robert},
	month = jun,
	year = {1990},
	doi = {10.1201/9780203753781},
}


@article{xia_multiple_index_2008,
	title = {A {Multiple}-{Index} {Model} and {Dimension} {Reduction}},
	volume = {103},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/27640210},
	doi = {10.1198/016214508000000805},
	number = {484},
	urldate = {2022-07-21},
	journal = {Journal of the American Statistical Association},
	author = {Xia, Yingcun},
	year = {2008},
	Publisher = {American Statistical Association, Taylor \& Francis, Ltd.},
	pages = {1631--1640},
}



@inproceedings{yang_high_dimensional_2017,
	title = {High-dimensional {Non}-{Gaussian} {Single} {Index} {Models} via {Thresholded} {Score} {Function} {Estimation}},
	booktitle = {{ICML}},
	author = {Yang, Zhuoran and Balasubramanian, K. and Liu, Han},
	year = {2017},
}

@article{zhang_ultra_high_2020,
	title = {Ultra-{High} {Dimensional} {Single}-{Index} {Quantile} {Regression}},
	volume = {21},
	url = {http://jmlr.org/papers/v21/19-173.html},
	number = {224},
	journal = {Journal of Machine Learning Research},
	author = {Zhang, Yuankun and Lian, Heng and Yu, Yan},
	year = {2020},
	pages = {1--25},
}

@article{hall_projection_1989,
	title = {On {Projection} {Pursuit} {Regression}},
	volume = {17},
	issn = {0090-5364, 2168-8966},
	doi = {10.1214/aos/1176347126},
	abstract = {We construct a tractable mathematical model for kernel-based projection pursuit regression approximation. The model permits computation of explicit formulae for bias and variance of estimators. It is shown that the bias of an orientation estimate dominates error about the mean--indeed, the latter is asymptotically negligible in comparison with bias. However, bias and error about the mean are of the same order in the case of projection pursuit curve estimates. Implications of our formulae for bias and variance are discussed.},
	number = {2},
	urldate = {2022-07-22},
	journal = {The Annals of Statistics},
	author = {Hall, Peter},
	month = jun,
	year = {1989},
	Publisher={Institute of Mathematical Statistics},
	pages = {573--588},
}

  @Article{Wood_Smoothing_2016,
    title = {Smoothing parameter and model selection for general smooth models (with discussion)},
    author = {S.N. Wood and {N.} and {Pya} and B. S{"a}fken},
    journal = {Journal of the American Statistical Association},
    year = {2016},
    pages = {1548-1575},
    volume = {111},
  }
  @Article{Wood_stable_2004,
    title = {Stable and efficient multiple smoothing parameter estimation for generalized additive models},
    journal = {Journal of the American Statistical Association},
    volume = {99},
    number = {467},
    pages = {673-686},
    year = {2004},
    author = {S. N. Wood},
  }
  @Book{Wood_general_2017,
    title = {Generalized Additive Models: An Introduction with R},
    year = {2017},
    author = {S.N Wood},
    edition = {2},
    publisher = {Chapman and Hall/CRC},
  }
  @Article{Wood_thin_2003,
    title = {Thin-plate regression splines},
    journal = {Journal of the Royal Statistical Society (B)},
    volume = {65},
    number = {1},
    pages = {95-114},
    year = {2003},
    author = {S. N. Wood},
  }

@Manual{minpack2022,
    title = {minpack.lm: R Interface to the Levenberg-Marquardt Nonlinear Least-Squares
Algorithm Found in MINPACK, Plus Support for Bounds},
    author = {Timur V. Elzhov and Katharine M. Mullen and Andrej-Nikolai Spiess and Ben Bolker},
    year = {2022},
    note = {R package version 1.2-2},
    url = {https://CRAN.R-project.org/package=minpack.lm},
  }

@Manual{R2021,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2021},
    url = {https://www.R-project.org/},
  }