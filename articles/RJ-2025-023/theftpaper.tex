% !TeX root = RJwrapper.tex
\title{Feature-Based Time-Series Analysis in R using the Theft Ecosystem}


\author{by Trent Henderson and Ben D. Fulcher}

\maketitle

\abstract{%
Time series are measured and analyzed across the sciences. One method of quantifying the structure of time series is by calculating a set of summary statistics or `features', and then representing a time series in terms of its properties as a feature vector. The resulting feature space is interpretable and informative, and enables conventional statistical learning approaches, including clustering, regression, and classification, to be applied to time-series datasets. Many open-source software packages for computing sets of time-series features exist across multiple programming languages, including `catch22' (22 features: Matlab, R, Python, Julia), `feasts' (43 features: R), `tsfeatures' (62 features: R), `Kats' (40 features: Python), `tsfresh' (783 features: Python), and `TSFEL' (156 features: Python). However, there are several issues: (i) a singular access point to these packages is not currently available; (ii) to access all feature sets, users must be fluent in multiple languages; and (iii) these feature-extraction packages lack extensive accompanying methodological pipelines for performing feature-based time-series analysis, such as applications to time-series classification. Here we introduce a solution to these issues in the form of two complementary statistical software packages for R called `theft': Tools for Handling Extraction of Features from Time series and `theftdlc': theft `downloadable content'. `theft' is a unified and extendable framework for computing features from the six open-source time-series feature sets listed above as well as custom user-specified features. `theftdlc' is an extension package to `theft' which includes a suite of functions for processing and interpreting the performance of extracted features, including extensive data-visualization templates, low-dimensional projections, and time-series classification. With an increasing volume and complexity of large time-series datasets in the sciences and industry, `theft' and `theftdlc' provide a standardized framework for comprehensively quantifying and interpreting informative structure in time series.
}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Taking repeated measurements of some quantity through time, forming a time series, is common across the sciences and industry.
The types of time series commonly analyzed are diverse, ranging from time-varying signals of an electroencephalogram \citep{westEvaluationComparisonEEG1999}, CO\(_2\) concentration in the atmosphere \citep{kodraExploringGrangerCausality2011}, light-curves from distant stars \citep{barbaraClassifyingKeplerLight2022}, and the daily number of clicks on a webpage \citep{kaoPredictionRemainingTime}.
We can ask many different questions about such data, for example: (i) ``can we distinguish the dynamics of brain disorders from neurotypical brain function?''; (ii) ``can we classify different geospatial regions based on their temporal CO\(_2\) concentration''; or (iii) ``can we classify new stars based on their light curves?''.
One approach to answering such questions is to capture properties of each time series and use that information to train a classification algorithm.
This can be achieved by extracting from each time series a set of interpretable summary statistics or `features'.
Using this procedure, a collection of univariate time series can be represented as a time series \(\times\) feature matrix which can be used as the basis for a range of conventional statistical learning procedures \citep{fulcherHctsaComputationalFramework2017, fulcherFeatureBasedTimeSeriesAnalysis2018}.

The range of time-series analysis methods that can be used to define time-series features is vast, including properties of the distribution, autocorrelation function, stationarity, entropy, methods from the physics nonlinear time-series analysis literature \citep{fulcherHighlyComparativeTimeseries2013}.
Because features are real-valued scalar outputs of a mathematical operation, and are often tightly linked to underlying theory (e.g., Fourier analysis or information theory), they can yield interpretable understanding of patterns in time series and the processes that produce them---information that can guide further investigation.
The first work to organize these methods from across the interdisciplinary literature encoded thousands of diverse time-series analysis methods as features and compared their behavior on a wide range of time series \citep{fulcherHighlyComparativeTimeseries2013}.
The resulting interdisciplinary library of thousands of time-series features has enabled new ways of doing time-series analysis, including the ability to discover high-performing methods for a given problem in a systematic, data-driven way through large-scale comparison (overcoming the subjective and time-consuming task of selecting methods manually) \citep{fulcherHighlyComparativeFeaturebased2014}.
This approach has been termed `highly comparative time-series analysis' and has been implemented in the Matlab software \texttt{hctsa}, which computes \(>7700\) time-series features \citep{fulcherHctsaComputationalFramework2017}.
The approach of automated discovery provided by \texttt{hctsa} has been applied successfully to many scientific problems, such as classifying zebra finch motifs across different social contexts \citep{paulBehavioralDiscriminationTimeseries2021}, classifying cord pH from fetal heart-rate dynamics \citep{fulcherHighlyComparativeFetal2012}, and classifying changes in cortical dynamics from manipulating the firing of excitatory and inhibitory neurons \citep{markicevicCorticalExcitationInhibition2020}.
While \texttt{hctsa} is comprehensive in its coverage of time-series analysis methods, calculating all of its features on a given dataset is computationally expensive and it requires access to the proprietary Matlab software, limiting its broader use.

The past decade has seen the development of multiple software libraries that implement different sets of time-series features across a range of open-source programming languages.
Here, we focus on the following six libraries:

\begin{itemize}
\item
  \texttt{catch22} (C, Matlab, R, Python, Julia) computes a representative subset of 22 features from \texttt{hctsa} \citep{lubbaCatch22CAnonicalTimeseries2019}.
  The \(>7700\) features in \texttt{hctsa} were applied to 93 time-series classification tasks to retain the smallest number of features that maintained high performance on these tasks while also being minimally redundant with each other, yielding the \texttt{catch22} set.
  \texttt{catch22} was coded in C for computational efficiency, with wrappers for Matlab, and packages for: R, as \CRANpkg{Rcatch22} \citep{Rcatch22_pkg}; Julia, as \texttt{Catch22.jl} \citep{catch22jl_pkg}; and Python, as \texttt{pycatch22}. \texttt{catch22} is also commonly extended to include mean and variance to form \texttt{"catch24"} in order to achieve competitive performance on tasks where the dataset has not been standardized \citep{henderson2023dullmomentdistributionalproperties}.
\item
  \CRANpkg{tsfeatures} (R) is the most prominent package for computing time-series features in R \citep{tsfeatures_pkg}.
  The 62 features in \CRANpkg{tsfeatures} include techniques commonly used by econometricians and forecasters, such as crossing points, seasonal and trend decomposition using Loess \citet{clevelandSeasonalTrend1990}{]}, autoregressive conditional heteroscedasticity (ARCH) models, unit-root tests, and sliding windows.
  \CRANpkg{tsfeatures} also includes sixteen features from \texttt{hctsa} that were previously used to organize tens of thousands of time series in the \emph{CompEngine} time-series database \citep{fulcherSelforganizingLivingLibrary2020}.
\item
  \CRANpkg{feasts} (R) shares a subset of the same features as \CRANpkg{tsfeatures}, computing a total of 43 features \citep{feasts_pkg}.
  However, the scope of \CRANpkg{feasts} as a software package is larger: it is a vehicle to incorporate time-series features into the software ecosystem known as the \pkg{tidyverts}\footnote{\url{https://tidyverts.org}}---a collection of packages for time series that follow tidy data principles \citep{wickhamTidyData2014}.
  This ensures alignment with the broader and popular \pkg{tidyverse} collection of packages for data wrangling, summarization, and statistical graphics \citep{wickhamWelcomeTidyverse2019}.
  \CRANpkg{feasts} also includes functions for producing graphics, but these are largely focused on exploring quantities of interest in econometrics, such as autocorrelation, seasonality, and Seasonal and Trend decomposition using Loess (STL).
\item
  \texttt{tsfresh} (Python) includes 783 features that measure properties of the autocorrelation function, entropy, quantiles, fast Fourier transforms, and distributional characteristics \citep{christTimeSeriesFeatuRe2018}.
  \texttt{tsfresh} also includes a built-in feature filtering procedure, FeatuRe Extraction based on Scalable Hypothesis tests (FRESH), that uses a hypothesis-testing process to control the percentage of irrelevant extracted features \citep{christDistributedParallelTime2017}.
  \texttt{tsfresh} has been used widely to solve time-series problems, such as anomaly detection in Internet-of-Things streaming data \citep{yangAnomalyDetectionAlgorithm2021} and sensor-fault classification \citep{liuSensorFaultsClassification2020}.
  \texttt{tsfresh} is also commonly accessed through the \texttt{FreshPRINCE} functionality within the popular \texttt{sktime} Python library \citep{loning2019sktime}.
\item
  \texttt{TSFEL} (Python) contains 156 features that measure properties associated with distributional characteristics, the autocorrelation function, spectral quantities, and wavelets \citep{barandasTSFELTimeSeries2020}.
  \texttt{TSFEL} was initially designed to support feature extraction of inertial data---such as data produced by human wearables---for the purpose of activity detection and rehabilitation.
\item
  \texttt{Kats} (Python), developed by Facebook Research, contains a broad range of time-series functionality, including operations for forecasting, outlier and property detection, and feature calculation \citep{Jiang_KATS_2022}.
  The feature-calculation module of \texttt{Kats} is called \CRANpkg{tsfeatures} and includes 40 features (30 of which mirror R's \CRANpkg{tsfeatures} package).
  \texttt{Kats} includes features associated with crossing points, STL decomposition, sliding windows, autocorrelation and partial autocorrelation, and Holt--Winters methods for detecting linear trends.
\end{itemize}

The six feature sets vary over several orders of magnitude in their computation time, and exhibit large differences in both within-set feature redundancy---how correlated features are within a given set---and between-set feature redundancy---how correlated, on average, features are between different pairwise comparisons of sets \citep{hendersonEmpiricalEvaluationTimeSeries2021}.
While each set contains a range of features that could be used to tackle time-series analysis problems, there are currently no guidelines for selecting an appropriate feature set for a given problem, nor methods for combining the different strengths of all sets.
Performance on a given time-series analysis task depends on the choice of the features that are used to represent the time series, highlighting the importance of being able to easily compute many different features from across different feature sets.
Furthermore, following feature extraction, there exists no set of visualization and analysis templates for common feature-based problem classes, such as feature-based time-series classification---like the tools provided in \texttt{hctsa} \citep{fulcherHctsaComputationalFramework2017}.
Here we present a solution for these challenges in the form of two connected open-source packages for called \CRANpkg{theft}: Tools for Handling Extraction of Features from Time series (which unifies the six disparate feature sets and provides a consistent interface for general feature extraction) \citep{theft_pkg}; and \CRANpkg{theftdlc}: theft downloadable content (which handles the subsequent processing, analysis, and visualization of time-series features) \citep{theftdlc_pkg}.
Together, we refer to these packages as the \CRANpkg{theft} `ecosystem'.

\hypertarget{the-theft-ecosystem-for-r}{%
\section{The theft ecosystem for R}\label{the-theft-ecosystem-for-r}}

\CRANpkg{theft} unifies the six free and open-source feature sets described in Section 1, thus overcoming barriers in using diverse feature sets developed in different software environments and the differences in their syntax and input-output structures.
The package also enables users to manually specify the functions for any number of features they wish to extract on top of the six pre-existing sets.
\CRANpkg{theftdlc} builds upon the foundation of \CRANpkg{theft} by providing an extensive analytical pipeline as well as statistical data visualization templates for understanding feature behavior and performance.
To our knowledge, such pipelines and templates do not currently exist in the free and open-source setting, making \CRANpkg{theftdlc} a useful tool for both computing and understanding features.
While there is some software support for computing features in a consistent setting (such as in \texttt{tsflex} \citep{vanderdoncktTsflexFlexibleTime2022}, which also provides sliding window extraction capability), such software is limited to only specifying the functional form of individual time-series features rather than automatically accessing features contained in existing feature sets.
We partition the analytical capabilities of \CRANpkg{theft} and \CRANpkg{theftdlc} into two separate packages for two reasons: (i) it reduces dependencies if users wish only to extract features and conduct analysis themselves; and (ii) having a separate analysis package means additional functionality can be continuously added without exhausting R package dependency limits.

An overview of the broad functionality of the \CRANpkg{theft} ecosystem is presented in Figure \ref{fig:workflow}.
The workflow begins in \CRANpkg{theft} with automated installation of Python libraries (if the three Python-based feature sets are required) through the function \texttt{install\_python\_pkgs} which takes the name of the virtual environment to create (\texttt{venv}) and the path to the Python interpreter to use (\texttt{python}) as arguments (Fig.\ref{fig:workflow}A).
The Python environment containing the installed software is then instantiated within the R session using \texttt{init\_theft} (Fig.\ref{fig:workflow}B).
Time-series data (Fig.\ref{fig:workflow}C) is loaded into the environment and converted to a \texttt{tsibble::tbl\_ts} data structure if required (Fig.\ref{fig:workflow}D) \citep{tsibble_pkg}.
Time-series features are then extracted in \CRANpkg{theft} using the desired pre-existing sets or user-supplied features (Fig.\ref{fig:workflow}E).
The workflow transitions to \CRANpkg{theftdlc} where the user can pass the extracted features into a range of statistical and visualization functions to derive interpretable understanding of the patterns in their dataset (Fig.\ref{fig:workflow}F--M).
A variety of plot types are readily available, including heatmaps of the time-series \(\times\) feature matrix (Fig.\ref{fig:workflow}F) and feature \(\times\) feature matrix (Fig.\ref{fig:workflow}G), and violin plots of feature distributions (Fig.\ref{fig:workflow}H).
Basic feature selection functionality is available through the \texttt{shrink} function (Fig.\ref{fig:workflow}I) which implements penalized maximum likelihood generalized linear models using a backend to the R package \CRANpkg{glmnet}.
Low-dimensional projection functionality is provided by the \texttt{project} function (Fig.\ref{fig:workflow}J).
Time-series classification operations are accessible via the \texttt{classify} function (Fig.\ref{fig:workflow}K).
Distributional summaries of time-series feature and time-series classification values are available through the \texttt{interval} function (Fig.\ref{fig:workflow}L).
Finally, basic cluster analysis is possible through the \texttt{cluster} function (Fig.\ref{fig:workflow}M).
Importantly, \CRANpkg{theft} and \CRANpkg{theftdlc} use R's \texttt{S3} object-oriented programming system, meaning classes and their methods are defined to ensure easy usage with R generic functions, such as \texttt{plot}.
Classes are defined for feature-calculation objects (Fig.\ref{fig:workflow}E; \texttt{feature\_calculations}), low dimensional projection objects (Fig.\ref{fig:workflow}I; \texttt{feature\_projection}), interval calculation objects (Fig.\ref{fig:workflow}K; \texttt{interval\_calculations}), and cluster objects (Fig.\ref{fig:workflow}L; \texttt{feature\_clusters}).
The individual functions of both \CRANpkg{theft} and \CRANpkg{theftdlc} are discussed in detail in the following sections.

\begin{figure}

{\centering \includegraphics[width=400px,height=450px,alt={Workflow of the theft and theftdlc packages for R which takes the user from tidy time-series data through to feature extraction through to visualization and statistical analysis of features.}]{figures/workflow-graphic_ecosystem-final} 

}

\caption{theft and theftdlc together implement a workflow for extracting features from univariate time series and processing and analyzing the results. First, the user can install the relevant Python libraries (A) within R and point it to the correctly installed versions (B). Next, a time-series dataset (C) is converted into a tsibble (D) with key variables, a time index variable, and a measured variable. One or more feature sets or custom user-supplied features are then computed on the dataset (E). A range of statistical analysis and data visualization functionality is available in theftdlc on the resulting feature data, including: (F) normalized time series x feature matrix visualization; (G) normalized feature x feature correlation matrix visualization; (H) violin plots of feature distributions (including by group/class where applicable); (I) basic feature selection using penalized maximum likelihood generalized linear models; (J) low-dimensional projections of the feature space; (K) time-series classification procedures (a common application of feature-based time-series analysis); (L) evaluating the uncertainty intervals of the resulting performance metrics through a range of distributional summary methods; and uncovering hidden structure in time-series data through cluster analysis in the feature space.}\label{fig:workflow}
\end{figure}

In this paper, we demonstrate how the \CRANpkg{theft} ecosystem can be used to tackle a time-series classification problem, using the Bonn University electroencephalogram (EEG) dataset as a case study \citep{andrzejakIndicationsNonlinearDeterministic2001}.
The dataset contains 500 time series, each of length \(T = 4097\), with 100 time series each from five labeled classes: (i) awake with eyes open (labeled \texttt{eyesOpen}); (ii) awake with eyes closed (\texttt{eyesClosed}); (iii) epileptogenic zone (\texttt{epileptogenic}); (iv) hippocampal formation of the opposite hemisphere of the brain (\texttt{hippocampus}); and (v) seizure activity (\texttt{seizure}).
Note that classes (i) and (ii) are from healthy volunteers, while classes (iii), (iv), and (v) are from a presurgical diagnosis archive.
The time series are comprised of EEG segments 23.6 seconds in duration that were cut out of continuous multichannel recordings, converted from analog to digital using 12-bit conversion, and then written onto a computer at a sampling rate of 173.614Hz.
Further trimming of start and end discontinuities from the original 4396 samples was then performed, resulting in a final time series of length T = 4097 samples.
This dataset was chosen as a demonstrative example because it has been widely studied as a time-series classification problem, and prior studies have focused on properties of the dynamics that accurately distinguish the classes---which is well-suited to the feature-based approach.
For example, prior analysis (using \texttt{hctsa}) revealed that seizure recordings are characterized most notably by higher variance, as well as lower entropy, lower long-range scaling exponents, and many other differences \citep{fulcherHighlyComparativeTimeseries2013}.
We can easily read the data file in from its zipper format online and convert to a \texttt{tsibble} ready for use:

\hypertarget{system-requirements}{%
\subsection{System requirements}\label{system-requirements}}

In order to access the features from \texttt{tsfresh}, \texttt{TSFEL}, and \texttt{Kats} in \CRANpkg{theft}, Python \textgreater=3.10 is required (Python 3.10 is recommended).
To use all other functionality across both \CRANpkg{theft} and \CRANpkg{theftdlc}, only R \textgreater=3.5.0 is required.

\hypertarget{installing-python-libraries}{%
\subsection{Installing Python libraries}\label{installing-python-libraries}}

Prior to calculating features, the requisite Python feature sets need to be installed.
The \texttt{install\_python\_pkgs} function in \CRANpkg{theft} handles this operation entirely within R---all that needs to be supplied is \texttt{venv} (a string specifying the name of the new virtual environment to create) and \texttt{python} (a string specifying the filepath to the Python interpreter to use.
Note that the filepath to the Python interpreter will differ depending on user operating system and will require correct specification.
For example, on Windows it might be a path similar to \texttt{"C:/Users/YourName/AppData/Local/Programs/Python/Python310/python.exe"}, whereas on Linux or MacOS (which was used for the present work) it could be a path similar to \texttt{"/usr/local/bin/python3.10"} or \texttt{"/usr/bin/python3"}.

An example call which installs the Python libraries into a new virtual environment and initiates the virtual environment within R is shown in the code below.
Note that \texttt{init\_theft} only needs to be run once per session.

\begin{verbatim}
install_python_pkgs(venv = "theft-eco-py", python = "/usr/local/bin/python3.10")
init_theft("theft-eco-py")
\end{verbatim}

\hypertarget{extracting-features}{%
\subsection{Extracting features}\label{extracting-features}}

In feature-based time-series analysis, each univariate time series in a dataset is represented as a feature vector, such that the dataset can be represented as a time series \(\times\) feature data matrix.
Any single feature set, or combination of multiple feature sets, can be computed for a given time-series dataset with the \CRANpkg{theft} function \texttt{calculate\_features}.
\texttt{calculate\_features} takes a \texttt{tbl\_ts} as input, using the data structure defined by the \CRANpkg{tsibble} package for R \citep{tsibble_pkg}.
This ensures consistency with the broader \texttt{tidyverts} collection of R packages for conducting time-series analysis.
A \texttt{tbl\_ts} is a temporal data structure which is defined by a \texttt{key} which identifies each unique time series and an \texttt{index} which identifies the time indices.
Other columns are treated as measured variables.
Since \texttt{theft} is a univariate tool, \texttt{calculate\_features} only accepts inputs that have one measured variable.
Since much of the functionality in \texttt{theftdlc} is associated with time-series classification problems, if multiple keys are defined, the first is treated as an ``ID'' variable, while the second is treated as the ``grouping'' variable for classification.

Users can control various aspects of the feature extraction process through modification of optional arguments.
The \texttt{catch22} feature set can be expanded to form `catch24' with included mean and standard deviation by setting \texttt{catch24\ =\ TRUE}.
Features from the \texttt{"compengine"} subset of \CRANpkg{tsfeatures} can be calculated by setting \texttt{use\_compengine\ =\ TRUE} which substantially increases computation time for the addition of \(16\) features.
The in-built algorithm in \texttt{tsfresh} for selecting relevant features can be used for that set by specifying \texttt{tsfresh\_cleanup\ =\ TRUE} \citep{christDistributedParallelTime2017}.
In addition, it is a common practice in feature-based time-series analysis to quantify the relative performance of features.
For situations where differences in mean and variance are not of interest, \emph{z}-scoring can be used to standardize each time series prior to the calculation of time-series features.
The argument \texttt{z\_score} enables automatic normalization of each time series within \texttt{calculate\_features} prior to computing features.
Further, users may always wish to disable package warnings when computing features.
This can be achieved by setting \texttt{warn\ =\ FALSE}.
Last, parallel processing can be engaged by setting the \texttt{n\_jobs} argument to a value \(\geq 2\) (\texttt{calculate\_features} defaults to serial processing).
Currently, parallelization has only been implemented for \CRANpkg{tsfeatures}, \texttt{tsfresh}, and \texttt{tsfel}

The output of \texttt{calculate\_features} is an S3 object of class \texttt{feature\_calculations}, which in this example is stored in the R environment as \texttt{all\_features}.
Within this object is a data frame which contains five columns if the dataset is labeled (as in time-series classification), and four otherwise: \texttt{id} (unique identifier for each time series), \texttt{names} (feature name), \texttt{values} (feature value), \texttt{feature\_set} (feature set name), and \texttt{group} (class label, if applicable).
This output structure ensures that, regardless of the feature set selected, the resulting object is always of the same format and can be used with the rest of the \CRANpkg{theftdlc} functions without manual data reshaping.

For the Bonn EEG dataset of 500 time series, each of length \(T = 4097\) samples, calculating features for all sets in \CRANpkg{theft} took \(\approx 5.7\) hours on a 2019 MacBook Pro with an Intel Core i7 2.6 GHz 6-Core CPU. The extensive computation time was largely driven by \CRANpkg{tsfeatures}, noting that the other five sets ranged from just seconds (\texttt{catch22}) to several minutes. With the sixteen \texttt{"compengine"} features enabled, computation time increased to \(\approx 11.8\) hours. Previous work provides a comprehensive discussion of computation speed between the sets and the scalability with time-series length \citep{hendersonEmpiricalEvaluationTimeSeries2021}. An example call which extracts features from all six sets for the Bonn EEG dataset is shown in the code below.

\begin{verbatim}
all_features <- calculate_features(
  data = bonn_eeg, 
  feature_set = c("catch22", "feasts", "tsfeatures", 
                  "tsfresh", "tsfel", "kats"),
  use_compengine = FALSE, catch24 = TRUE)
\end{verbatim}

\CRANpkg{theft} is also set up to enable users to calculate their own custom features For example, we could specify two new features such as the mean and standard deviation by adding the requisite functions and the names for those features in a list to the additional \texttt{features} argument, as demonstrated in the code below.
User-supplied functions must take a vector input and return a numeric scalar value to be a valid time-series feature.

\begin{verbatim}
all_features_msd <- calculate_features(
                        data = bonn_eeg, 
                        feature_set = c("catch22", "feasts", "tsfeatures", 
                                        "tsfresh", "tsfel", "kats"),
                        features = list("mean" = mean, "sd" = sd),
                        use_compengine = FALSE)
\end{verbatim}

In addition, previous work highlighted that it can be helpful to provide a simple benchmark of performance for the more comprehensive feature sets \citep{henderson2023dullmomentdistributionalproperties}.
As such, two simple feature sets---\texttt{“quantiles”} and \texttt{“moments”}---are also available in \texttt{calculate\_features} to enable the quick computation of a set of quantiles and the first four moments of the distribution to serve as a baseline for more sophisticated feature sets.
\texttt{“quantiles”} and \texttt{“moments”} are both able to be specified directly as values to the \texttt{feature\_set} argument of \texttt{calculate\_features}.
By default, the \texttt{"quantiles"} feature set includes a collection of \(100\) quantiles from the range \(0.01\) to \(1\).

Users who wish to explore the computed results efficiently can also downloaded the pre-computed features and classifiers used in this paper:

\begin{verbatim}
files <- c("all_features", "mf_results", "feature_classifiers")

for(f in files){
  temp <- tempfile()
  download.file(paste0("https://github.com/hendersontrent/bonn-eeg-data/raw/refs/heads/main/", 
                       f, ".Rda"), temp)
  load(temp)
  unlink(temp)
}
\end{verbatim}

\hypertarget{normalizing-features}{%
\subsection{Normalizing features}\label{normalizing-features}}

Different features vary over very different ranges; e.g., features that estimate \(p\)-values from a hypothesis test vary over the unit interval, whereas a feature that computes the length of a time series takes (often large) positive integer values.
These differences in scale can complicate the visualization of feature behavior and the construction of statistical learning algorithms involving diverse features.
To overcome these limitations, a common pre-processing step involves scaling all features.
Several of \CRANpkg{theftdlc}'s internal functions utilize rescaling functionality---specifically, providing the user the choice of five methods for converting a set of raw feature values, \(\mathbf{x}\), to a normalized version, \(\mathbf{z}\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(z\)-score (\texttt{"zScore"}): \(z_i = \frac{x_i - \mu}{\sigma}\),
\item
  linear scaling to unit interval (\texttt{"MinMax"}): \(z_i = \frac{x_i - \min(\mathbf{x})}{\max(\mathbf{x}) - \min(\mathbf{x})}\),
\item
  maximum absolute scaling (\texttt{"MaxAbs"}): \(z_{i} = \frac{x_{i}}{|\text{max}(\mathbf{x})|}\)
\item
  sigmoid (\texttt{"Sigmoid"}): \(z_i = \left[1 + \exp(-\frac{x_i - \mu}{\sigma})\right]^{-1}\),
\item
  and outlier-robust sigmoid (\texttt{"RobustSigmoid"}): \(z_i = \left[1 + \exp\left(-\frac{x_i - \mathrm{median}(\mathbf{x})}{\mathrm{IQR}(\mathbf{x})/{1.35}}\right)\right]^{-1}\),
\end{enumerate}

where \(\mu\) is the mean, \(\sigma\) is the standard deviation, and \(\mathrm{IQR}(\mathbf{x})\) is the interquartile range of \(\mathbf{x}\).
All four transformations end with a linear rescaling to the unit interval.
The outlier-robust sigmoid transformation, introduced in \citep{fulcherHighlyComparativeTimeseries2013}, can be helpful in normalizing feature-value distributions with large outliers.
Feature normalization is implemented in the R package \CRANpkg{normaliseR} which is a key dependency for \CRANpkg{theftdlc} \citep{normaliseR_pkg}.

\hypertarget{visualizing-the-feature-matrix}{%
\subsection{Visualizing the feature matrix}\label{visualizing-the-feature-matrix}}

A hallmark of large-scale feature extraction is the ability to visualize the intricate patterns of how different time-series analysis algorithms behave across a time-series dataset.
This can be achieved in \CRANpkg{theftdlc} using by specifying \texttt{type\ =\ "matrix"} when calling \texttt{plot} on a \texttt{feature\_calculations} object to produce a heatmap of the time series (rows) \(\times\) feature matrix (columns) which organizes the rows and columns to help reveal interesting patterns in the data.
The plot of the combination of all six open feature sets for the Bonn EEG dataset is shown in Figure \ref{fig:featureMatrix}.
We can see some informative structure in this graphic, including many groups of features with similar behavior on this dataset (i.e., columns with similar patterns), indicating substantial redundancy across the joint set of features \citep{hendersonEmpiricalEvaluationTimeSeries2021}.
The bottom block of 100 rows, which visually have the most distinctive properties, was found to correspond to time series from the \texttt{seizure} class, indicating the ability of this large combination of time-series features to meaningfully structure the dataset.

\begin{verbatim}
plot(all_features, 
     type = "matrix", 
     norm_method = "RobustSigmoid", 
     clust_method = "average")
\end{verbatim}

\begin{figure}
\centering
\includegraphics{theftpaper_files/figure-latex/featureMatrix-1.pdf}
\caption{\label{fig:featureMatrix}A time series by feature matrix heatmap produced by generating a matrix plot on the feature calculations object. Extracted feature vectors for each time series (500) in the Bonn EEG dataset using all six feature sets in theft (1005 features in total, after filtering out 85 features with NaN values) are represented as a heatmap. Similar features (columns) and time series (rows) are positioned close to each other using (average) hierarchical clustering. Each tile is a normalized value for a given time series and feature.}
\end{figure}

In matrix plots in \CRANpkg{theftdlc}, hierarchical clustering is used to reorder rows and columns so that time series (rows) with similar properties are placed close to each other and features (columns) with similar behavior across the dataset are placed close to each other---where similarity in behavior is quantified using Euclidean distance in both cases \citep{dayEfficientAlgorithmsAgglomerative1984}.
In Figure \ref{fig:featureMatrix}, we specify the usage of average (i.e., unweighted pair group method with arithmetic mean) agglomeration.
Default settings within \texttt{plot} enable users to easily generate outputs in a single line of code, but more advanced users may seek to tweak the optional arguments.
For example, different linkage algorithms for hierarchical clustering can be controlled supplied to \texttt{clust\_method}, which uses average agglomeration as a default, and the different rescaling methods defined earlier can be supplied to the \texttt{norm\_method} argument, which defaults to \texttt{"zScore"}.

\hypertarget{projecting-low-dimensional-feature-spaces}{%
\subsection{Projecting low-dimensional feature-spaces}\label{projecting-low-dimensional-feature-spaces}}

Low-dimensional projections are a useful tool for visualizing the structure of high-dimensional datasets in low-dimensional spaces.
Here we are interested in representing a time-series dataset in a two-dimensional projection of the feature space, which can reveal structure in the dataset, including how different labeled classes are organized.
For linear dimensionality reduction techniques, such as principal components analysis (PCA) \citep{PrincipalComponentAnalysis2002}, the results can be visualized in two dimensions as a scatterplot, where the principal component (PC) that explains the most variance in the data is positioned on the horizontal axis and the second PC on the vertical axis, and each time series is represented as a point (colored by its group label in the case of a labeled dataset).
When the structure of a dataset in the low-dimensional feature space matches known aspects of the dataset (such as class labels), it suggests that the combination of diverse time-series features can capture relevant dynamical properties that differ between the classes.
It can also reveal new types of structure in the dataset, like clear sub-clusters within a labeled class, that can guide new understanding of the dataset.
Low-dimensional projections of time-series features have been shown to meaningfully structure time-series datasets: revealing sex and day/night differences in \emph{Drosophila} \citep{fulcherHctsaComputationalFramework2017}, distinguishing types of stars based on their light curves \citep{barbaraClassifyingKeplerLight2022}, and categorizing sleep epochs \citep{decatTraditionalSleepScoring2022}.

Several algorithms for projecting feature spaces are available in \CRANpkg{theftdlc}:

\begin{itemize}
\tightlist
\item
  Principal components analysis (PCA)---\texttt{"PCA"}
\item
  \(t\)-Stochastic Neighbor Embedding (\(t\)-SNE)---\texttt{"tSNE"}
\item
  Classical multidimensional scaling (MDS)---\texttt{"ClassicalMDS"}
\item
  Kruskal's non-metric multidimensional scaling---\texttt{"KruskalMDS"}
\item
  Sammon's non-linear mapping non-metric multidimensional scaling---\texttt{"SammonMDS"}
\item
  Uniform Manifold Approximation and Projection for Dimension Reduction (UMAP)---\texttt{"UMAP"}
\end{itemize}

Time-series datasets can be projected in low-dimensional feature spaces in \CRANpkg{theftdlc} using the \texttt{project} function.
Users can control algorithm hyperparameters by supplying additional arguments to the \texttt{...} argument of \texttt{project}---the function then passes these arguments to the requisite dimension reduction algorithm internally.
The \texttt{project} function returns an \texttt{S3} object of class \texttt{feature\_projection}, which can then be passed to \texttt{plot} which will automatically draw the appropriate two-dimensional scatterplot.
The \texttt{feature\_projection} class is a list object which contains elements representing the user-supplied feature data frame, the model data, the two-dimensional projected data frame, and the model fit object itself.
The plot method for this object contains only one other argument (\texttt{show\_covariance}) which specifies whether to draw covariance ellipses for each group in the scatterplot (if a grouping variable is detected).

The low-dimensional projection plot for the Bonn EEG dataset (using \(t\)-SNE and all \(>1000\) non-\texttt{NaN} features across the six feature sets included in \CRANpkg{theft}) is shown in Figure \ref{fig:tsne} with perplexity \(15\), as produced by the code below.
The low-dimensional projection meaningfully structures the labeled classes of the dataset.
Specifically, two of the presurgical diagnosis classes---\texttt{epileptogenic} (epileptogenic zone) and \texttt{hippocampus} (hippocampal formation of the opposite hemisphere of the brain)---appear to exhibit considerable overlap in the projected space, while the two healthy volunteer classes \texttt{eyesOpen} (awake state with eyes open) and \texttt{eyesClosed} (awake state with eyes closed) occupy space further away from the other classes but closer to each other.
The \texttt{seizure} class occupies a space largely separate from the other four classes in the projection, consistent with its distinctive periodic dynamics \citep{fulcherHighlyComparativeTimeseries2013}.

\begin{verbatim}
low_dim_calc <- project(all_features, 
                        method = "MinMax", 
                        low_dim_method = "tSNE", 
                        perplexity = 15)

plot(low_dim_calc)
\end{verbatim}

\begin{figure}
\centering
\includegraphics{theftpaper_files/figure-latex/tsne-1.pdf}
\caption{\label{fig:tsne}Low-dimensional projection of the Bonn EEG dataset using theft. Using t-SNE with perplexity 15, the high-dimensional feature space of \textgreater1000 features is projected into two dimensions. Each point represents a time series which is colored according to its class label. Time series that are located close in this space have similar properties, as measured by the six feature sets in theft.}
\end{figure}

\hypertarget{constructing-classifiers-with-multiple-features}{%
\subsection{Constructing classifiers with multiple features}\label{constructing-classifiers-with-multiple-features}}

Combinations of complementary, discriminative features can often be used to construct accurate time-series classifiers \citep{fulcherHighlyComparativeFeaturebased2014}.
Drawing on computed time-series features (that may derive from one or more existing feature sets), \CRANpkg{theftdlc} can fit and evaluate classifiers using the \texttt{classify} function.
This allows users to evaluate the relative performance of each feature set, of the combination of all sets, or any other combination of features.
Providing easy access to a range of classification algorithms and accompanying inferential tools (such as null permutation testing to obtain \(p\)-values and performance distributions through the resampling-based algorithm) through \texttt{classify} allows users to compare sets of features to better understand the most accurate feature sets for a given time-series classification problem.
The code presented below provides an example usage for the Bonn EEG dataset with a linear support vector machine (SVM) classifier (which is the default and so does not require explicit specification).

\begin{verbatim}
mf_results <- classify(
                data = all_features, 
                by_set = TRUE, 
                train_size = 0.8,
                n_resamples = 100,
                use_null = TRUE)
\end{verbatim}

The \texttt{classify} function is flexible in that users can supply any function that can be used with R's native \texttt{stats::predict} generic.
For example, a user could easily use a radial basis function SVM instead:

\begin{verbatim}
rbfClassifier <- function(formula, data){
  mod <- e1071::svm(formula, data = data, kernel = "radial", scale = FALSE,
                    probability = TRUE)
}

mf_results_rbf <- classify(
                    data = all_features, 
                    classifier = rbfClassifier,
                    by_set = TRUE, 
                    train_size = 0.8,
                    n_resamples = 100,
                    use_null = TRUE)
\end{verbatim}

In the above code, we specified that we want \texttt{classify} to fit separate classifiers for each feature set, using a training set size that is \(80\%\) of the input data size, with \(100\) resamples for each feature set as a way to incorporate uncertainty.
We also enabled null model fitting for permutation testing.
In applications involving small datasets, or when small effects are expected, it is useful to quantify how different the observed classification performance is from a null setting in which data are classified randomly (i.e., could the same results have been obtained by chance?).
One method for inferring test statistics is to use permutation testing---a procedure that samples a null process many times to form a distribution against which a value of importance (i.e., the classification accuracy result from a model) can be compared to estimate a \(p\)-value \citep{ojalaPermutationTestsStudying2009}.
In \CRANpkg{theft}, permutation testing is implemented for evaluating classification performance in \texttt{classify} through the \texttt{use\_null} argument.
When set to \texttt{TRUE}, \texttt{classify} will compute results for \texttt{n\_resamples} models where the class labels match the data, but also \texttt{n\_resamples} models where the class labels are shuffled, thus severing the input-output relationship.
In the absence of any data errors, we would expect the mean classification accuracy for the empirical null distribution to approximate chance for the problem.
This provides a useful comparison point for the main (non-shuffled) models---if the main models statistically outperform the empirical null models, then the result, quantified by the \(p\)-value, is likely not due to chance, thus indicating that the set of time-series features represent quantities that can meaningfully capture differences between classes.

The resampling procedure begins by partitioning the input data into \texttt{n\_samples} number of seeded train-test splits for reproducibility, using \texttt{train\_size} to govern the size of the training set for each split.
Importantly, the procedure tracks the class representation in the first resample and preserves these proportions for all subsequent resamples.
The feature data for each resample is then normalized as a \(z\)-score by computing the mean and standard deviation of each feature in the training set, and using these values to rescale both the training and test sets.
This ensures that test data are completely unseen.
The procedure then iterates over each resample and fits the classification algorithm for each.
By default, \texttt{classify} calculates the following accuracy metrics, where \(C\) is the number of classes, \(TP\) is the number of true positives, \(FP\) is the number of false positives, and \(FN\) is the number of false negatives:

\begin{itemize}
\tightlist
\item
  Accuracy (\texttt{"accuracy"}): \(\frac{\sum^{C}_{i=1}TP_{i}}{\sum^{C}_{i=1} (TP_{i} + FP_{i} + FN_{i})}\)
\item
  Mean precision (\texttt{"precision"}): \(\frac{1}{C}\sum^{C}_{i=1}\frac{\text{TP}_{i}}{\text{TP}_{i} + \text{FP}_{i}}\)
\item
  Mean recall (\texttt{"recall"}): \(\frac{1}{C}\sum^{C}_{i=1}\frac{\text{TP}_{i}}{\text{TP}_{i} + \text{FN}_{i}}\)
\item
  F1 score (\texttt{"f1"}): \(2 \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}\)
\end{itemize}

\texttt{classify} returns a list with two elements: (i) the train and test sizes; and (ii) a data frame of classification results.
While the raw results are useful, \CRANpkg{theftdlc} enables automated analysis of them.
The \texttt{theftdlc::interval} and \texttt{compare\_features} functions were designed to enable fast and intuitive comparisons between individual features and entire sets.
\texttt{theftdlc::interval} produces summaries of classification results with uncertainty using three distinct methods: (i) standard deviation---\texttt{"sd"}; (ii) confidence interval based off the \(t\)-distribution---\texttt{"se"}; and (iii) quantile summary---\texttt{"quantile"}.

Users can compute different interval summaries by modifying the additional arguments:

\begin{itemize}
\tightlist
\item
  \texttt{metric}---the classification performance metric to calculate intervals for. Can be one of \texttt{"accuracy"}, \texttt{"precision"}, \texttt{"recall"}, or \texttt{"f1"}
\item
  \texttt{by\_set}---whether to compute intervals for each feature set. If \texttt{FALSE}, the function will instead calculate intervals for each individual feature
\item
  \texttt{type}---whether to calculate a \(\pm SD\) interval with \texttt{"sd"}, confidence interval based off the \(t\)-distribution with \texttt{"se"}, or a quantile with \texttt{"quantile"}
\item
  \texttt{interval}---the width of the interval to calculate. Defaults to \(1\) if \texttt{type\ =\ "sd"} to produce a \(\pm 1SD\) interval. Defaults to \texttt{0.95} if \texttt{type\ =\ "se"} or \texttt{type\ =\ "quantile"} for a \(95\%\) interval
\item
  \texttt{model\_type}---whether to calculate intervals for main models with \texttt{"main"} or null models with \texttt{"null"} if the \texttt{use\_null} argument of \texttt{classify} was \texttt{use\_null\ =\ TRUE}
\end{itemize}

Below we calculate the mean \(\pm 1SD\) for each feature set.
For the Bonn EEG dataset, we find that the set of all features (\texttt{All\ features}) produced the highest mean classification accuracy (\(91.2\%\)) and \texttt{catch22} (the smallest feature set) produced the lowest mean accuracy (\(80.1\%\)).
The best performing individual feature set was the largest---\texttt{tsfresh}---with a mean classification accuracy of \(90.8\%\) which was marginally below the set of all features.

\begin{verbatim}
set_intervals <- theftdlc::interval(
  mf_results, 
  metric = "accuracy",
  by_set = TRUE,
  type = "sd",
  model_type = "main"
  )

set_intervals
\end{verbatim}

\begin{verbatim}
#>    feature_set  .mean    .lower    .upper
#> 1 All features 0.9115 0.8827069 0.9402931
#> 2         Kats 0.8274 0.7908680 0.8639320
#> 3        TSFEL 0.8353 0.8019226 0.8686774
#> 4      catch22 0.8005 0.7691217 0.8318783
#> 5       feasts 0.8525 0.8172126 0.8877874
#> 6   tsfeatures 0.8363 0.8061940 0.8664060
#> 7      tsfresh 0.9079 0.8783138 0.9374862
\end{verbatim}

\texttt{theftdlc::interval} returns an \texttt{S3} object of class \texttt{interval\_calculations} which is a data frame that can be used with the \texttt{plot} generic through \CRANpkg{theftdlc}.
The resulting plot is presented in Figure \ref{fig:featureSetComparisons} where we see the considerable overlap in mean \(\pm 1SD\) classification accuracy between the feature sets.

\begin{verbatim}
plot(set_intervals)
\end{verbatim}

\begin{figure}
\centering
\includegraphics{theftpaper_files/figure-latex/featureSetComparisons-1.pdf}
\caption{\label{fig:featureSetComparisons}Comparison of mean classification accuracy between feature sets in theft for the five-class Bonn EEG classification task. Classification accuracy using a linear SVM is presented for each of the six feature sets in theft as well as the combination of all their features. The number of features retained for analysis after filtering is displayed in parentheses after the feature set name on the horizontal axis which has been sorted from highest to lowest mean accuracy. Mean classification accuracy across the same 100 resamples is displayed as colored points for each set with one standard deviation error bars.}
\end{figure}

While the visual aid is useful, we often want to understand if a given feature set has performed better than we might expect due to chance.
Or can we determine if a given feature set has statistically outperformed another (such as the set of all features compared to \texttt{tsfresh}), therefore providing guidance on which features to retain for subsequent analysis?
How can we estimate if the performance of the smallest and fastest-to-compute set -- \texttt{catch22} -- is meaningfully different from the larger feature sets with close average classification accuracy values (i.e., \texttt{Kats}, \texttt{TSFEL}, and \texttt{tsfeatures})?
The \texttt{compare\_features} function in \CRANpkg{theftdlc} enables pairwise comparisons between either individual features or entire feature sets, or to their own respective empirical null distributions (i.e., the `chance' distribution).
It does so through usage of the resampled \(t\)-test which accounts for the correlation between samples in the calculation of the test statistic \citep{nadeauInferenceGeneralizationError2003}.
The resampled \(t\)-test is implemented through the \CRANpkg{correctR} R package \citep{correctR_pkg}.
\texttt{compare\_features} returns a data frame with columns for the hypothesis that was tested, the test statistic, \(p\)-value, adjusted \(p\)-value (if specified), and the means of each group for each hypothesis test.
\texttt{compare\_features} can operate on any of the metrics computed in \texttt{classify}.
The function only takes a small number of arguments in addition to the output of \texttt{classify}:

\begin{itemize}
\tightlist
\item
  \texttt{metric}---the classification performance metric to calculate intervals for. Can be one of \texttt{"accuracy"}, \texttt{"precision"}, \texttt{"recall"}, or \texttt{"f1"}
\item
  \texttt{by\_set}---whether to compare entire feature sets (\texttt{TRUE}) or individual features (\texttt{FALSE})
\item
  \texttt{hypothesis}---whether to compare each entire feature set or individual feature to its own empirical null distribution (\texttt{"null"}, if permutation testing was used in \texttt{classify}) or to each other (\texttt{"pairwise"})
\item
  \texttt{p\_adj}---method for adjusting \(p\)-values for multiple comparisons. Defaults to \texttt{"none"} for no adjustments, but can take any valid option received by \texttt{stats::p.adjust}
\end{itemize}

For example, we can statistically compare the performance of each feature set against their empirical null distributions (formed by the distribution of performance on shuffled class label data) using the code presented below.
When specifying \texttt{hypothesis\ =\ "null"}, \CRANpkg{theftdlc} automatically applies a one-tailed test, since the hypothesis is that the main models should outperform their null counterparts.
We find that, for the full Bonn EEG dataset with 100 time series per class and strong differences between signals, mean classification accuracy for each feature set is far higher than chance level (\(20\%\)) over 100 resamples.
Further, we obtain extremely small \(p\)-values close to zero (displayed in the table below as zero due to rounding for visual clarity), providing support for the low probability of obtaining classification accuracy results at least as extreme as what we observed under the null hypothesis.
This confirms that time-series features can effectively distinguish between classes in the dataset.
Note that we drop some of the columns reported by \CRANpkg{theftdlc} here for spatial reasons.

\begin{verbatim}
compare_features(mf_results,
                 metric = "accuracy",
                 by_set = TRUE,
                 hypothesis = "null",
                 p_adj = "none")
\end{verbatim}

\begin{tabular}{lrrrr}
\toprule
hypothesis & set\_mean & null\_mean & t\_statistic & p.value\\
\midrule
All features != own null & 0.911 & 0.192 & 12.496 & 0\\
Kats != own null & 0.827 & 0.188 & 9.701 & 0\\
TSFEL != own null & 0.835 & 0.197 & 10.638 & 0\\
catch22 != own null & 0.800 & 0.203 & 8.699 & 0\\
feasts != own null & 0.853 & 0.192 & 10.003 & 0\\
\addlinespace
tsfeatures != own null & 0.836 & 0.197 & 11.157 & 0\\
tsfresh != own null & 0.908 & 0.192 & 13.787 & 0\\
\bottomrule
\end{tabular}

We can then compare the feature sets to one-another to provide statistical evidence for any differences in the performance ranges visualized in Figure \ref{fig:featureSetComparisons} using the code below.
For \texttt{hypothesis\ =\ "pairwise"}, \CRANpkg{theftdlc} applies a two-tailed test.
We find that the set of all features outperforms all individual sets at \(\alpha<0.05\) except for \texttt{tsfresh} (\(p = 0.820\)), \texttt{feasts} (\(p = 0.094\)), and \texttt{tsfeatures} (\(p = 0.077\)).

\begin{verbatim}
compare_features(mf_results,
                 metric = "accuracy",
                 by_set = TRUE,
                 hypothesis = "pairwise",
                 p_adj = "none")
\end{verbatim}

\begin{tabular}{lrrrr}
\toprule
hypothesis & set\_a\_mean & set\_b\_mean & t\_statistic & p.value\\
\midrule
All features != catch22 & 0.911 & 0.800 & 3.026 & 0.003\\
All features != feasts & 0.911 & 0.853 & 1.713 & 0.090\\
All features != Kats & 0.911 & 0.827 & 2.306 & 0.023\\
All features != tsfeatures & 0.911 & 0.836 & 2.066 & 0.041\\
All features != TSFEL & 0.911 & 0.835 & 2.058 & 0.042\\
\addlinespace
All features != tsfresh & 0.911 & 0.908 & 0.198 & 0.843\\
catch22 != feasts & 0.800 & 0.853 & -1.249 & 0.215\\
catch22 != Kats & 0.800 & 0.827 & -0.634 & 0.527\\
catch22 != tsfeatures & 0.800 & 0.836 & -0.871 & 0.386\\
catch22 != TSFEL & 0.800 & 0.835 & -0.873 & 0.385\\
\addlinespace
catch22 != tsfresh & 0.800 & 0.908 & -2.776 & 0.007\\
feasts != Kats & 0.853 & 0.827 & 0.579 & 0.564\\
feasts != tsfeatures & 0.853 & 0.836 & 0.414 & 0.680\\
feasts != TSFEL & 0.853 & 0.835 & 0.419 & 0.676\\
feasts != tsfresh & 0.853 & 0.908 & -1.436 & 0.154\\
\addlinespace
Kats != tsfeatures & 0.827 & 0.836 & -0.198 & 0.843\\
Kats != TSFEL & 0.827 & 0.835 & -0.183 & 0.855\\
Kats != tsfresh & 0.827 & 0.908 & -2.006 & 0.048\\
tsfeatures != TSFEL & 0.836 & 0.835 & 0.025 & 0.980\\
tsfeatures != tsfresh & 0.836 & 0.908 & -1.796 & 0.076\\
\addlinespace
TSFEL != tsfresh & 0.835 & 0.908 & -1.913 & 0.059\\
\bottomrule
\end{tabular}

\hypertarget{finding-and-understanding-informative-individual-features}{%
\subsection{Finding and understanding informative individual features}\label{finding-and-understanding-informative-individual-features}}

Fitting models which use multiple features as inputs is often useful for predicting class labels.
However, users are also typically interested in understanding patterns in their dataset, such as interpreting the types of time-series analysis methods that best separate different classes, and the relationships between these top-performing features.
This can be achieved using mass univariate statistical testing of individual features, quantifying their performance either relative to an empirical null distribution or each other.
\CRANpkg{theftdlc} implements the ability to identify top-performing features in the \texttt{compare\_features} function by setting \texttt{by\_set\ =\ FALSE}, with an example usage for the Bonn EEG dataset (using features from all six packages) shown in the code below.
We implement parallel processing to speed up computation time.

\begin{verbatim}
feature_classifiers <- classify(data = all_features, 
                               by_set = FALSE, 
                               train_size = 0.8,
                               n_resamples = 100,
                               use_null = TRUE)
\end{verbatim}

\begin{verbatim}
feature_vs_null <- compare_features(feature_classifiers,
                                    by_set = FALSE,
                                    hypothesis = "null",
                                    n_workers = 6)
\end{verbatim}

Straightforward \CRANpkg{dplyr} syntax can then be used to identify the top \(n\) features \citep{dplyr_pkg}.
We have the choice of either mean classification accuracy or \(p\)-values relative to the empirical null to determine informative features.
For illustrative purposes, here we have used mean classification accuracy to find the top \(n = 40\) features.
We show the top 20 features below for spatial reasons.
We see that the \(17\) best-performing individual features achieve \(>50\%\) accuracy (which far exceeds the chance probability for a five-class problem of \(20\%\)).

\begin{verbatim}
top_40 <- feature_vs_null |>
  dplyr::slice_max(feature_mean, n = 40)
\end{verbatim}

\begin{verbatim}
top_40 |>
  top_n(feature_mean, n = 20)
\end{verbatim}

\begin{tabular}{>{\raggedright\arraybackslash}p{6cm}rrrr}
\toprule
hypothesis & feature\_mean & null\_mean & t\_statistic & p.value\\
\midrule
tsfresh\_values\_\_
autocorrelation\_\_lag\_6 != own null & 0.537 & 0.138 & 4.054 & 0.000\\
tsfresh\_values\_\_
autocorrelation\_\_lag\_7 != own null & 0.537 & 0.136 & 4.244 & 0.000\\
tsfresh\_values\_\_
autocorrelation\_\_lag\_8 != own null & 0.527 & 0.138 & 4.513 & 0.000\\
tsfresh\_values\_\_
change\_quantiles\_\_f\_agg\_"mean"\_\_
isabs\_True\_\_qh\_1.0\_\_ql\_0.4 != own null & 0.524 & 0.142 & 7.373 & 0.000\\
tsfresh\_values\_\_
change\_quantiles\_\_f\_agg\_"mean"\_\_
isabs\_True\_\_qh\_1.0\_\_ql\_0.2 != own null & 0.521 & 0.146 & 6.819 & 0.000\\
\addlinespace
tsfresh\_values\_\_
change\_quantiles\_\_f\_agg\_"mean"\_\_
isabs\_True\_\_qh\_0.8\_\_ql\_0.4 != own null & 0.520 & 0.148 & 7.016 & 0.000\\
TSFEL\_0\_Wavelet energy\_25.0Hz != own null & 0.515 & 0.145 & 7.903 & 0.000\\
TSFEL\_0\_Wavelet standard deviation\_25.0Hz != own null & 0.515 & 0.145 & 7.903 & 0.000\\
tsfresh\_values\_\_
change\_quantiles\_\_f\_agg\_"mean"\_\_
isabs\_True\_\_qh\_0.8\_\_ql\_0.2 != own null & 0.512 & 0.148 & 6.362 & 0.000\\
tsfresh\_values\_\_
change\_quantiles\_\_f\_agg\_"mean"\_\_
isabs\_True\_\_qh\_0.6\_\_ql\_0.2 != own null & 0.511 & 0.141 & 7.350 & 0.000\\
\addlinespace
TSFEL\_0\_Median absolute diff != own null & 0.504 & 0.147 & 6.645 & 0.000\\
TSFEL\_0\_Mean absolute diff != own null & 0.504 & 0.146 & 6.298 & 0.000\\
TSFEL\_0\_Sum absolute diff != own null & 0.504 & 0.146 & 6.298 & 0.000\\
tsfresh\_values\_\_
absolute\_sum\_of\_changes != own null & 0.504 & 0.146 & 6.298 & 0.000\\
tsfresh\_values\_\_
change\_quantiles\_\_f\_agg\_"mean"\_\_
isabs\_True\_\_qh\_1.0\_\_ql\_0.0 != own null & 0.504 & 0.146 & 6.298 & 0.000\\
\addlinespace
tsfresh\_values\_\_
mean\_abs\_change != own null & 0.504 & 0.146 & 6.298 & 0.000\\
TSFEL\_0\_Signal distance != own null & 0.501 & 0.146 & 6.136 & 0.000\\
tsfresh\_values\_\_
change\_quantiles\_\_f\_agg\_"mean"\_\_
isabs\_True\_\_qh\_0.8\_\_ql\_0.0 != own null & 0.500 & 0.144 & 7.167 & 0.000\\
tsfresh\_values\_\_
autocorrelation\_\_lag\_5 != own null & 0.500 & 0.132 & 3.280 & 0.001\\
tsfresh\_values\_\_
augmented\_dickey\_fuller\_\_attr\_
"teststat"\_\_autolag\_"AIC" != own null & 0.497 & 0.140 & 5.100 & 0.000\\
\bottomrule
\end{tabular}

Understanding what each feature within the table measures can provide insight into the types of features relevant for the classification problem.
For example, we see features that measure properties associated with autocorrelation (e.g., \texttt{tsfresh\_values\_\_autocorrelation\_lag\_6} which measures the value of the autocorrelation function at lag 6) and signal peaks (e.g., \texttt{TSFEL\_0\_Median\ absolute\ diff} which measures the median value of all absolute differences along the signal), among others.
However, interpreting this table is challenging as the relationships between the features are unknown---are all the 40 features behaving differently, or are they all highly correlated to each other and essentially proxy metrics for the same underlying time-series property?
We can better understand these relationships by visualizing the pairwise feature \(\times\) feature correlation matrix.

To achieve this, we can filter the original feature data to only include the top features, assign it to a \texttt{feature\_calculations} object type, and make use of the \texttt{plot} function in \CRANpkg{theftdlc} to visualize pairwise absolute correlations between the top performing features.
This is presented visually in Figure \ref{fig:topFeatureCorrelations}.
The plot reveals two main groups of highly correlated (\(|\rho| \gtrapprox 0.8\)) features: in the bottom left and upper right of the plot.
The cluster in the bottom left contains features that capture different types of autocorrelation structure in the time series, including linear autocorrelation coefficients (e.g., \texttt{tsfresh\_values\_\_autocorrelation\_lag\_6}) and the variance of means over sliding windows (e.g., \texttt{tsfeatures\_stability}).
The large cluster in the top right (containing features from \texttt{tsfresh} and \texttt{TSFEL} exclusively) contains features sensitive to variance---including change quantiles (e.g., \texttt{tsfresh\_values\_\_change\_quantiles\_\_f\_agg\_"mean"\_\_isabs\_True\_\_qh\_1.0\_\_ql\_0.4} which measures the mean of the absolute change of the time series values inside quantiles \(0.4-1.0\)), wavelet variance (e.g., \texttt{TSFEL\_0\_Wavelet\ standard\ deviation\_25.0Hz} which measures the variance of coefficients from Ricker wavelets with widths \(1-10\) at the lower quarter of the assumed sampling frequency of \(100\)Hz), and the `distance traveled' by the signal (e.g., \texttt{TSFEL\_0\_Signal\ distance} which measures the sum of square root squared differences).
While the differences between classes---as identified through the list of top features---in this case were simple (i.e., autocorrelation and variance), other, more complex features may perform the strongest on other problems, or even different pairs of classes within the five-class dataset investigated here.
Identifying when simple features perform well is important as it can provide interpretable benchmarks for assessing relative performance gains achieved by more complex and/or less interpretable alternative classifiers \citep{henderson2023dullmomentdistributionalproperties}.

\begin{verbatim}
feature_matrix_filt <- all_features |>
  dplyr::filter(feature_set %in% top_40$feature_set & 
                  names %in% top_40$original_names) |>
  structure(class = c("feature_calculations", "data.frame"))

plot(feature_matrix_filt, type = "cor")
\end{verbatim}

\begin{figure}
\centering
\includegraphics{theftpaper_files/figure-latex/topFeatureCorrelations-1.pdf}
\caption{\label{fig:topFeatureCorrelations}A group of change quantile and difference-associated features and a group of autocorrelation-sensitive features perform the best at distinguishing between the five classes in the Bonn EEG dataset using the absolute Spearman correlation coefficient to capture feature-feature similarity. To aid the identification of similarly performing features, the matrix of correlation coefficients between features were then organized using hierarchical clustering (on Euclidean distances with average linkage) along rows and columns to order the heatmap graphic.}
\end{figure}

Having identified the discriminative features, it can be important to understand how they differ amongst the labeled classes of a dataset.
This can be achieved by visualizing the distribution of values for each class for each of the features.
In \CRANpkg{theftdlc}, a violin plot can be produced in \texttt{plot} by setting \texttt{type\ =\ "violin"}, where each time series is represented as a point organized and colored by its class label.
Note that a boxplot alternative (which highlights univariate outliers as points) is also possible through specifying \texttt{type\ =\ "box"}.
Here, for visual clarity, we show violin plots for a selected feature from the variance cluster of features from Figure \ref{fig:topFeatureCorrelations}: \texttt{0\_Signal\ distance} from \texttt{TSFEL} (mean classification accuracy \(50.1\%\) over 100 resamples); and a selected feature from the autocorrelation-sensitive cluster of features: \texttt{values\_autocorrelation\_lag\_6} from \texttt{tsfresh} (mean classification accuracy \(53.7\%\) over 100 resamples).
The outputs are shown in Figure \ref{fig:topFeatureViolins}.
Consistent with their high classification scores relative to chance (\(20\%\)), both features are individually informative of class differences.
The plot shows that with regards to autocorrelation structure, we see that \texttt{eyesClosed} exhibits typically weak to moderate negative coefficient values at lag 6, while \texttt{hippocampus} and \texttt{epileptogenic} exhibit typically moderate positive coefficients.
\texttt{eyesClosed} is characterized by weak to moderate coefficient values largely within the \(0-0.5\) range, while \texttt{seizure} exhibits a substantially wider distribution of values than the other classes---a distribution which almost spans the entire range of coefficients exhibited by the others.
This defining lack of temporal predictability in short-range autocorrelation coefficients for \texttt{seizure} time series is consistent with prior work and is characteristic of the more erratic nature of seizure state brain activity \citep{fulcherHighlyComparativeTimeseries2013}.

The plot also shows that with regards to signal distance (i.e., \(\sum_{t=1}^{T-1} \sqrt{1 + (x_{t+1} - x_{t})}\) where \(T\) is the length of the time series and \(x\) is the vector of values), all classes except \texttt{seizure} exhibit similar values with subtle differences in the mean and variance of their feature value distributions.
This is consistent with the lower classification performance of this feature compared to \texttt{values\_autocorrelation\_lag\_6}.
For \texttt{seizure}, we again see a wide distribution of feature values definitive of this class.
Practically, since signal distance measures the total distance traveled by the signal between time points, it can be inferred that \texttt{seizure} state brain activity (as measured by an EEG) fluctuates far more than healthy brain activity and measurements from the epileptogenic zone (i.e., the difference between values at any two consecutive time points is, on average, larger), consistent with known dynamics of seizure states \citep{andrzejakIndicationsNonlinearDeterministic2001}.

Together, these two feature case studies reinforce the interpretative benefits to a feature-based approach to time-series analysis.
Features can not only organize time-series data, reveal structure, and predict class membership, but they can also provide insight into the underlying generative properties that distinguish different time series---such as the difference in brain activity between seizure state and regular brain function.

\begin{verbatim}
plot(feature_matrix_filt,
     type = "violin", 
     feature_names = c("values__autocorrelation__lag_6", 
                       "0_Signal distance")) +
  theme(strip.text = element_text(size = 6))
\end{verbatim}

\begin{figure}
\centering
\includegraphics{theftpaper_files/figure-latex/topFeatureViolins-1.pdf}
\caption{\label{fig:topFeatureViolins}Violin plots (on original feature value scale) of a sample of two of the top 40 features of all six feature sets in theft for classifying Bonn EEG groups. Classes differ in their variance and autocorrelation properties.}
\end{figure}

\hypertarget{additional-functionality}{%
\subsection{Additional functionality}\label{additional-functionality}}

In addition to the functionality demonstrated here, \CRANpkg{theft} and \CRANpkg{theftdlc} include a collection of other functions, not demonstrated in this article for brevity, including cluster analysis (through the \texttt{cluster} function in \CRANpkg{theftdlc}), simple feature selection using penalized maximum likelihood generalized linear models (through the \texttt{shrink} function in \CRANpkg{theftdlc}), and the processing of \texttt{hctsa}-formatted Matlab files in \CRANpkg{theft}. Readers are encouraged to explore this additional functionality in the detailed vignettes included with the packages.

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

Feature-based time-series analysis is a powerful computational tool for tackling statistical learning problems using sequential (typically time-ordered) data.
We have introduced the \CRANpkg{theft} and \CRANpkg{theftdlc} packages for R which implement the extraction, processing, visualization, and statistical analysis of time-series features.
The value of time-series features stems from their interpretability and strong connection to theory that can be used to understand the empirical properties of their dynamics.
\CRANpkg{theft} provides a unified interface to extracting features from six open-source packages---\texttt{catch22}, \CRANpkg{feasts}, \CRANpkg{tsfeatures}, \texttt{Kats}, \texttt{tsfresh}, and \texttt{TSFEL}---while \CRANpkg{theftdlc} provides a comprehensive range of analyses to leverage the combined contributions from all of these packages.
For the first time in the free and open-source software setting, the \CRANpkg{theft} ecosystem provides a full workflow for conducting feature-based time-series analysis, taking the analyst from feature extraction through to generating interpretable insights about their data.
\CRANpkg{theftdlc} introduces a set of simply named functions that make analysis of time-series features calculated in \CRANpkg{theft} intuitive and streamlined: \texttt{classify}, \texttt{project}, \texttt{cluster}, \texttt{interval}, \texttt{plot}, and \texttt{compare\_features}.
\CRANpkg{theft} and \CRANpkg{theftdlc} reduce the need to construct complex, bespoke workflows with multiple software libraries that were not designed to work together---the \CRANpkg{theft} ecosystem provides an extensive suite of functions, but also presents a set of templates for advanced users to alter and adapt as their research requires.

We demonstrated the \CRANpkg{theft} ecosystem on the five-class Bonn EEG time-series classification problem \citep{andrzejakIndicationsNonlinearDeterministic2001}, in which the full feature-based classification analysis pipeline---from feature extraction to normalization, classification, and interpretation of individual features---was achieved using a small number of key functions in \CRANpkg{theft} and \CRANpkg{theftdlc}.
We showed that this intuitive pipeline could be used to derive insights about the temporal patterns which distinguish different classes of EEG time series and produce high-performing results in a simple statistical learning classification context.
In other settings, emphasis may be placed on the classification procedure, where more complex classifiers---such as Gaussian processes or generalized additive models---may yield strong results.
In others, users may not have a labeled dataset and instead seek to uncover structure in their data.
For such cases, the \texttt{cluster} functionality of \CRANpkg{theftdlc} may prove valuable in deriving scientific understanding.
Regardless of the feature-based time-series analysis context, \CRANpkg{theft} and \CRANpkg{theftdlc} enable consistent, end-to-end analytical pipelines.

As new and more powerful features (and feature sets) are developed in the future, they can be incorporated into \CRANpkg{theft} to enable ongoing assessments of the types of problems they are best placed to solve.
In addition to the analysis templates provided through functions in \CRANpkg{theftdlc}, there is much flexibility for users to adapt them or build new functionality for their own use-cases, such as applying different types of statistical learning algorithms on extracted feature matrices (e.g., feature selection), or to adapt the results to different applications such as extrinsic regression \citep{tanTimeSeriesExtrinsic2021} or forecasting \citep{montero-mansoFFORMAFeaturebasedForecast2020}.
Future work could also aim to reduce redundancy from across the combined features towards a new reduced feature set that combines the most generically informative and unique features from across the available feature-extraction packages (following the aims of the \texttt{catch22} feature set, selected from a library of \(>7700\) candidate features in \texttt{hctsa} \citep{lubbaCatch22CAnonicalTimeseries2019}).

\bibliography{RJreferences.bib}

\address{%
Trent Henderson\\
The University of Sydney\\%
School of Physics\\ Sydney, Australia\\
%
%
\textit{ORCiD: \href{https://orcid.org/0009-0005-5467-9914}{0009-0005-5467-9914}}\\%
\href{mailto:then6675@uni.sydney.edu.au}{\nolinkurl{then6675@uni.sydney.edu.au}}%
}

\address{%
Ben D. Fulcher\\
The University of Sydney\\%
School of Physics\\ Sydney, Australia\\
%
%
\textit{ORCiD: \href{https://orcid.org/0000-0002-3003-4055}{0000-0002-3003-4055}}\\%
\href{mailto:ben.fulcher@sydney.edu.au}{\nolinkurl{ben.fulcher@sydney.edu.au}}%
}
