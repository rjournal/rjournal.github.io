<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { color: #00769e; background-color: #f1f3f5; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #00769e; } /* Normal */
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #657422; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #00769e; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #00769e; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #00769e; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #5e5e5e; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>

<style>
  div.csl-bib-body { }
  div.csl-entry {
    clear: both;
    }
  .hanging div.csl-entry {
    margin-left:2em;
    text-indent:-2em;
  }
  div.csl-left-margin {
    min-width:2em;
    float:left;
  }
  div.csl-right-inline {
    margin-left:2em;
    padding-left:1em;
  }
  div.csl-indent {
    margin-left: 2em;
  }
</style>

  <!--radix_placeholder_meta_tags-->
<title>The R Journal: fairmodels: a Flexible Tool for Bias Detection, Visualization, and Mitigation in Binary Classification Models</title>

<meta property="description" itemprop="description" content="Machine learning decision systems are becoming omnipresent in our lives. From dating apps to rating loan seekers, algorithms affect both our well-being and future. Typically, however, these systems are not infallible. Moreover, complex predictive models are eager to learn social biases present in historical data that may increase discrimination. If we want to create models responsibly, we need tools for in-depth validation of models also from potential discrimination. This article introduces an R package fairmodels that helps to validate fairness and eliminate bias in binary classification models quickly and flexibly. The fairmodels package offers a model-agnostic approach to bias detection, visualization, and mitigation. The implemented functions and fairness metrics enable model fairness validation from different perspectives. In addition, the package includes a series of methods for bias mitigation that aim to diminish the discrimination in the model.  The package is designed to examine a single model and facilitate comparisons between multiple models."/>

<link rel="canonical" href="https://doi.org/10.32614/RJ-2022-019/"/>
<link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>
<link rel="icon" type="image/vnd.microsoft.icon" href="../../resources/favicon.ico"/>

<!--  https://schema.org/Article -->
<meta property="article:published" itemprop="datePublished" content="2022-06-27"/>
<meta property="article:created" itemprop="dateCreated" content="2022-06-27"/>
<meta name="article:author" content="Jakub Wiśniewski"/>
<meta name="article:author" content="Przemysław Biecek"/>

<!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
<meta property="og:title" content="The R Journal: fairmodels: a Flexible Tool for Bias Detection, Visualization, and Mitigation in Binary Classification Models"/>
<meta property="og:type" content="article"/>
<meta property="og:description" content="Machine learning decision systems are becoming omnipresent in our lives. From dating apps to rating loan seekers, algorithms affect both our well-being and future. Typically, however, these systems are not infallible. Moreover, complex predictive models are eager to learn social biases present in historical data that may increase discrimination. If we want to create models responsibly, we need tools for in-depth validation of models also from potential discrimination. This article introduces an R package fairmodels that helps to validate fairness and eliminate bias in binary classification models quickly and flexibly. The fairmodels package offers a model-agnostic approach to bias detection, visualization, and mitigation. The implemented functions and fairness metrics enable model fairness validation from different perspectives. In addition, the package includes a series of methods for bias mitigation that aim to diminish the discrimination in the model.  The package is designed to examine a single model and facilitate comparisons between multiple models."/>
<meta property="og:url" content="https://doi.org/10.32614/RJ-2022-019/"/>
<meta property="og:image" content="https://journal.r-project.org/articles/RJ-2022-019/table1.png"/>
<meta property="og:image:width" content="2696"/>
<meta property="og:image:height" content="506"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:site_name" content="The R Journal"/>

<!--  https://dev.twitter.com/cards/types/summary -->
<meta property="twitter:card" content="summary_large_image"/>
<meta property="twitter:title" content="The R Journal: fairmodels: a Flexible Tool for Bias Detection, Visualization, and Mitigation in Binary Classification Models"/>
<meta property="twitter:description" content="Machine learning decision systems are becoming omnipresent in our lives. From dating apps to rating loan seekers, algorithms affect both our well-being and future. Typically, however, these systems are not infallible. Moreover, complex predictive models are eager to learn social biases present in historical data that may increase discrimination. If we want to create models responsibly, we need tools for in-depth validation of models also from potential discrimination. This article introduces an R package fairmodels that helps to validate fairness and eliminate bias in binary classification models quickly and flexibly. The fairmodels package offers a model-agnostic approach to bias detection, visualization, and mitigation. The implemented functions and fairness metrics enable model fairness validation from different perspectives. In addition, the package includes a series of methods for bias mitigation that aim to diminish the discrimination in the model.  The package is designed to examine a single model and facilitate comparisons between multiple models."/>
<meta property="twitter:url" content="https://doi.org/10.32614/RJ-2022-019/"/>
<meta property="twitter:image" content="https://journal.r-project.org/articles/RJ-2022-019/table1.png"/>
<meta property="twitter:image:width" content="2696"/>
<meta property="twitter:image:height" content="506"/>

<!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
<meta name="citation_title" content="The R Journal: fairmodels: a Flexible Tool for Bias Detection, Visualization, and Mitigation in Binary Classification Models"/>
<meta name="citation_fulltext_html_url" content="https://doi.org/10.32614/RJ-2022-019"/>
<meta name="citation_pdf_url" content="RJ-2022-019.pdf"/>
<meta name="citation_volume" content="14"/>
<meta name="citation_issue" content="1"/>
<meta name="citation_doi" content="10.32614/RJ-2022-019"/>
<meta name="citation_journal_title" content="The R Journal"/>
<meta name="citation_issn" content="2073-4859"/>
<meta name="citation_firstpage" content="227"/>
<meta name="citation_lastpage" content="243"/>
<meta name="citation_fulltext_world_readable" content=""/>
<meta name="citation_online_date" content="2022/06/27"/>
<meta name="citation_publication_date" content="2022/06/27"/>
<meta name="citation_author" content="Jakub Wiśniewski"/>
<meta name="citation_author_institution" content="Warsaw University of Technology"/>
<meta name="citation_author" content="Przemysław Biecek"/>
<meta name="citation_author_institution" content="Warsaw University of Technology"/>
<!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare;citation_publication_date=2020;citation_volume=3;citation_doi=10.1038/s41746-020-0288-5;citation_issn=2398-6352;citation_author=Davide Cirillo;citation_author=Silvina Catuara-Solarz;citation_author=Czuee Morey;citation_author=Emre Guney;citation_author=Laia Subirats;citation_author=Simona Mellino;citation_author=Annalisa Gigante;citation_author=Alfonso Valencia;citation_author=María José Rementeria;citation_author=Antonella Santuccione Chadha;citation_author=Nikolaos Mavridis"/>
  <meta name="citation_reference" content="citation_title=Handbook on european non-discrimination law;citation_publication_date=2018;citation_publisher=Luxembourg: Publications Office of the European Union;citation_author=Handbook on european non-discrimination law"/>
  <meta name="citation_reference" content="citation_title=Fairness and machine learning;citation_publication_date=2019;citation_publisher=fairmlbook.org;citation_author=Solon Barocas;citation_author=Moritz Hardt;citation_author=Arvind Narayanan"/>
  <meta name="citation_reference" content="citation_title=Fairness: Algorithmic fairness metrics;citation_publication_date=2021;citation_author=Nikita Kozodoi;citation_author=Tibor V. Varga"/>
  <meta name="citation_reference" content="citation_title=AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias;citation_publication_date=2018;citation_author=Rachel K. E. Bellamy;citation_author=Kuntal Dey;citation_author=Michael Hind;citation_author=Samuel C. Hoffman;citation_author=Stephanie Houde;citation_author=Kalapriya Kannan;citation_author=Pranay Lohia;citation_author=Jacquelyn Martino;citation_author=Sameep Mehta;citation_author=Aleksandra Mojsilovic;citation_author=Seema Nagar;citation_author=Karthikeyan Natesan Ramamurthy;citation_author=John Richards;citation_author=Diptikalyan Saha;citation_author=Prasanna Sattigeri;citation_author=Moninder Singh;citation_author=Kush R. Varshney;citation_author=Yunfeng Zhang"/>
  <meta name="citation_reference" content="citation_title=DALEX: Explainers for Complex Predictive Models in R;citation_publication_date=2018;citation_volume=19;citation_author=Przemyslaw Biecek"/>
  <meta name="citation_reference" content="citation_title=Machine bias: There’s software used across the country to predict future criminals. And it’s biased against blacks;citation_publication_date=2016;citation_author=Julia Angwin;citation_author=Jeff Larson;citation_author=Surya Mattu;citation_author= Lauren Kirchner"/>
  <meta name="citation_reference" content="citation_title=[iFair: Learning individually fair data representations for algorithmic decision making];citation_publication_date=2019;citation_author=P. Lahoti;citation_author=K. P. Gummadi;citation_author=G. Weikum"/>
  <meta name="citation_reference" content="citation_title=H2O AutoML;citation_publication_date=2017;citation_author= H2O.ai"/>
  <meta name="citation_reference" content="citation_title=Fairness through awareness;citation_publication_date=2012;citation_publisher=Association for Computing Machinery;citation_author=Cynthia Dwork;citation_author=Moritz Hardt;citation_author=Toniann Pitassi;citation_author=Omer Reingold;citation_author=Richard Zemel"/>
  <meta name="citation_reference" content="citation_title=Equality of opportunity in supervised learning;citation_publication_date=2016;citation_publisher=Curran Associates, Inc.;citation_author=Moritz Hardt;citation_author=Eric Price;citation_author=Eric Price;citation_author=Nati Srebro"/>
  <meta name="citation_reference" content="citation_title=Fair prediction with disparate impact: A study of bias in recidivism prediction instruments;citation_publication_date=2016;citation_volume=5;citation_author=Alexandra Chouldechova"/>
  <meta name="citation_reference" content="citation_title=Algorithmic decision making and the cost of fairness;citation_publication_date=2017;citation_publisher=Association for Computing Machinery;citation_author=Sam Corbett-Davies;citation_author=Emma Pierson;citation_author=Avi Feller;citation_author=Sharad Goel;citation_author=Aziz Huq"/>
  <meta name="citation_reference" content="citation_title=Fairness in criminal justice risk assessments: The state of the art;citation_publication_date=2017;citation_author=Richard Berk;citation_author=Hoda Heidari;citation_author=Shahin Jabbari;citation_author=Michael Kearns;citation_author=Aaron Roth"/>
  <meta name="citation_reference" content="citation_title=Certifying and removing disparate impact;citation_publication_date=2015;citation_publisher=Association for Computing Machinery;citation_author=Michael Feldman;citation_author=Sorelle A. Friedler;citation_author=John Moeller;citation_author=Carlos Scheidegger;citation_author=Suresh Venkatasubramanian"/>
  <meta name="citation_reference" content="citation_title=Data pre-processing techniques for classification without discrimination;citation_publication_date=2011;citation_volume=33;citation_author=Faisal Kamiran;citation_author=Toon Calders"/>
  <meta name="citation_reference" content="citation_title=Decision theory for discrimination-aware classification;citation_publication_date=2012;citation_volume=;citation_author=F. Kamiran;citation_author=A. Karim;citation_author=X. Zhang"/>
  <meta name="citation_reference" content="citation_title=SECTION 4D, UNIFORM GUIDELINES ON EMPLOYEE SELECTION PROCEDURES (1978);citation_publication_date=1978;citation_author=SECTION 4D, UNIFORM GUIDELINES ON EMPLOYEE SELECTION PROCEDURES (1978)"/>
  <meta name="citation_reference" content="citation_title=Unlocking fairness: A trade-off revisited;citation_publication_date=2019;citation_publisher=Curran Associates, Inc.;citation_volume=32;citation_author=Michael Wick;citation_author=Swetasudha Panda;citation_author=Jean-Baptiste Tristan"/>
  <meta name="citation_reference" content="citation_title=Aequitas: A bias and fairness audit toolkit;citation_publication_date=2018;citation_author=Pedro Saleiro;citation_author=Benedict Kuester;citation_author=Abby Stevens;citation_author=Ari Anisfeld;citation_author=Loren Hinkson;citation_author=Jesse London;citation_author=Rayid Ghani"/>
  <meta name="citation_reference" content="citation_title=A survey on bias and fairness in machine learning;citation_publication_date=2019;citation_author=Ninareh Mehrabi;citation_author=Fred Morstatter;citation_author=Nripsuta Saxena;citation_author=Kristina Lerman;citation_author=Aram Galstyan"/>
  <meta name="citation_reference" content="citation_title=Fairlearn: A toolkit for assessing and improving fairness in AI;citation_publication_date=2020;citation_publisher=Microsoft;citation_number=MSR-TR-2020-32;citation_author=Sarah Bird;citation_author=Miro Dudík;citation_author=Richard Edgar;citation_author=Brandon Horn;citation_author=Roman Lutz;citation_author=Vanessa Milan;citation_author=Mehrnoosh Sameki;citation_author=Hanna Wallach;citation_author=Kathleen Walker"/>
  <meta name="citation_reference" content="citation_title=Gender shades: Intersectional accuracy disparities in commercial gender classification;citation_publication_date=2018;citation_volume=81;citation_author=Joy Buolamwini;citation_author=Timnit Gebru"/>
  <meta name="citation_reference" content="citation_title=Guidelines on facial recognition;citation_publication_date=2021;citation_author=Guidelines on facial recognition"/>
  <meta name="citation_reference" content="citation_title=Fair data adaptation with quantile preservation;citation_publication_date=2019;citation_author=Drago Plečko;citation_author=Nicolai Meinshausen"/>
  <meta name="citation_reference" content="citation_title=UCI machine learning repository;citation_publication_date=2017;citation_publisher=University of California, Irvine, School of Information; Computer Sciences;citation_author=Dheeru Dua;citation_author=Casey Graff"/>
  <meta name="citation_reference" content="citation_title=ranger: A fast implementation of random forests for high dimensional data in C++ and R;citation_publication_date=2017;citation_volume=77;citation_doi=10.18637/jss.v077.i01;citation_author=Marvin N. Wright;citation_author=Andreas Ziegler"/>
  <meta name="citation_reference" content="citation_title=On the apparent conflict between individual and group fairness;citation_publication_date=2020;citation_publisher=Association for Computing Machinery;citation_doi=10.1145/3351095.3372864;citation_author=Reuben Binns"/>
  <meta name="citation_reference" content="citation_title=Fairness in credit scoring: Assessment, implementation and profit implications;citation_publication_date=2021;citation_doi=https://doi.org/10.1016/j.ejor.2021.06.023;citation_issn=0377-2217;citation_author=Nikita Kozodoi;citation_author=Johannes Jacob;citation_author=Stefan Lessmann"/>
  <meta name="citation_reference" content="citation_title=Handbook on european non-discrimination law;citation_publication_date=2018;citation_doi=https://doi.org/10.2811/792676;citation_author=Handbook on european non-discrimination law"/>
  <meta name="citation_reference" content="citation_title=Debiasing classifiers: Is reality at variance with expectation?;citation_publication_date=2020;citation_author=Ashrya Agrawal;citation_author=Florian Pfisterer;citation_author=B. Bischl;citation_author=Jiahao Chen;citation_author=Srijan Sood;citation_author=Sameena Shah;citation_author=Francois Buet-Golfouse;citation_author=Bilal Akhter Mateen;citation_author=Sebastian Vollmer"/>
  <meta name="citation_reference" content="citation_title=Fairness measures for regression via probabilistic classification;citation_publication_date=2020;citation_volume=abs/2001.06089;citation_author=Daniel C. Steinberg;citation_author=Alistair Reid;citation_author=Simon Timothy O’Callaghan"/>
  <meta name="citation_reference" content="citation_title=Explanatory Model Analysis;citation_publication_date=2021;citation_publisher=Chapman; Hall/CRC, New York;citation_author=Przemyslaw Biecek;citation_author=Tomasz Burzykowski"/>
  <!--radix_placeholder_rmarkdown_metadata-->

<script type="text/json" id="radix-rmarkdown-metadata">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","type","output","bibliography","preamble","date","date_received","volume","issue","slug","journal","draft","pdf_url","citation_url","doi","creative_commons","packages","CTV","csl","canonical_url"]}},"value":[{"type":"character","attributes":{},"value":["fairmodels: a Flexible Tool for Bias Detection, Visualization, and Mitigation in Binary Classification Models"]},{"type":"character","attributes":{},"value":["Machine learning decision systems are becoming omnipresent in our lives. From dating apps to rating loan seekers, algorithms affect both our well-being and future. Typically, however, these systems are not infallible. Moreover, complex predictive models are eager to learn social biases present in historical data that may increase discrimination. If we want to create models responsibly, we need tools for in-depth validation of models also from potential discrimination. This article introduces an R package fairmodels that helps to validate fairness and eliminate bias in binary classification models quickly and flexibly. The fairmodels package offers a model-agnostic approach to bias detection, visualization, and mitigation. The implemented functions and fairness metrics enable model fairness validation from different perspectives. In addition, the package includes a series of methods for bias mitigation that aim to diminish the discrimination in the model.  The package is designed to examine a single model and facilitate comparisons between multiple models."]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","affiliation","address","email"]}},"value":[{"type":"character","attributes":{},"value":["Jakub Wiśniewski"]},{"type":"character","attributes":{},"value":["Warsaw University of Technology"]},{"type":"character","attributes":{},"value":["Faculty of Mathematics and Information Science","Poland"]},{"type":"character","attributes":{},"value":["jakwisn@gmail.com"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url","email","orcid_id","affiliation","address"]}},"value":[{"type":"character","attributes":{},"value":["Przemysław Biecek"]},{"type":"character","attributes":{},"value":["https://pbiecek.github.io/"]},{"type":"character","attributes":{},"value":["przemyslaw.biecek@pw.edu.pl"]},{"type":"character","attributes":{},"value":["0000-0001-8423-1823"]},{"type":"character","attributes":{},"value":["Warsaw University of Technology"]},{"type":"character","attributes":{},"value":["Faculty of Mathematics and Information Science","University of Warsaw","Faculty of Mathematics, Informatics, and Mechanics","Poland"]}]}]},{"type":"character","attributes":{},"value":["package"]},{"type":"character","attributes":{},"value":["rjtools::rjournal_pdf_article"]},{"type":"character","attributes":{},"value":["Wisniewski-Biecek.bib"]},{"type":"character","attributes":{},"value":["\\usepackage{longtable}"]},{"type":"character","attributes":{},"value":["2022-06-27"]},{"type":"character","attributes":{},"value":["2021-04-26"]},{"type":"double","attributes":{},"value":[14]},{"type":"double","attributes":{},"value":[1]},{"type":"character","attributes":{},"value":["RJ-2022-019"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","issn","firstpage","lastpage"]}},"value":[{"type":"character","attributes":{},"value":["The R Journal"]},{"type":"character","attributes":{},"value":["2073-4859"]},{"type":"integer","attributes":{},"value":[227]},{"type":"integer","attributes":{},"value":[243]}]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["RJ-2022-019.pdf"]},{"type":"character","attributes":{},"value":["https://doi.org/10.32614/RJ-2022-019"]},{"type":"character","attributes":{},"value":["10.32614/RJ-2022-019"]},{"type":"character","attributes":{},"value":["CC BY"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["cran","bioc"]}},"value":[{"type":"character","attributes":{},"value":["fairmodels","h2o","fairness","fairadapt","DALEX","ranger"]},{"type":"list","attributes":{},"value":[]}]},{"type":"character","attributes":{},"value":["HighPerformanceComputing","MachineLearning","ModelDeployment","Survival","TeachingStatistics"]},{"type":"character","attributes":{},"value":["/home/mitchell/R/x86_64-pc-linux-gnu-library/4.1/rjtools/rjournal.csl"]},{"type":"character","attributes":{},"value":["https://doi.org/10.32614/RJ-2022-019/"]}]}
</script>
<!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["ac.png","class_diagram.png","cm.png","cpc.png","cpcc.png","density.png","fc_last.png","fc_print.png","fc.png","fc2_print.png","fc2.png","flow.png","gm.png","heatmap.png","ms_2.png","ms.png","paf.png","pca.png","penguins.png","radar.png","RJ-2022-019_files/anchor-4.2.2/anchor.min.js","RJ-2022-019_files/bowser-1.9.3/bowser.min.js","RJ-2022-019_files/distill-2.2.21/template.v2.js","RJ-2022-019_files/figure-html5/all-1.png","RJ-2022-019_files/figure-html5/fairness-plot-1-1.png","RJ-2022-019_files/figure-html5/fairness-plot-2-1.png","RJ-2022-019_files/figure-html5/fairness-plot-3-1.png","RJ-2022-019_files/figure-html5/mitigation-1.png","RJ-2022-019_files/figure-latex/all-1.pdf","RJ-2022-019_files/figure-latex/fairness-plot-1-1.pdf","RJ-2022-019_files/figure-latex/fairness-plot-2-1.pdf","RJ-2022-019_files/figure-latex/fairness-plot-3-1.pdf","RJ-2022-019_files/figure-latex/mitigation-1.pdf","RJ-2022-019_files/header-attrs-2.14/header-attrs.js","RJ-2022-019_files/jquery-3.6.0/jquery-3.6.0.js","RJ-2022-019_files/jquery-3.6.0/jquery-3.6.0.min.js","RJ-2022-019_files/jquery-3.6.0/jquery-3.6.0.min.map","RJ-2022-019_files/popper-2.6.0/popper.min.js","RJ-2022-019_files/tippy-6.2.7/tippy-bundle.umd.min.js","RJ-2022-019_files/tippy-6.2.7/tippy-light-border.css","RJ-2022-019_files/tippy-6.2.7/tippy.css","RJ-2022-019_files/tippy-6.2.7/tippy.umd.min.js","RJ-2022-019_files/webcomponents-2.0.0/webcomponents.js","RJ-2022-019.pdf","RJ-2022-019.tex","RJ-2022-019.zip","RJwrapper.log","RJwrapper.tex","stacked.png","table1.png","Wisniewski-Biecek_files/figure-html5/all-1.png","Wisniewski-Biecek_files/figure-html5/mitigation-1.png","Wisniewski-Biecek_files/figure-html5/unnamed-chunk-4-1.png","Wisniewski-Biecek_files/figure-html5/unnamed-chunk-6-1.png","Wisniewski-Biecek_files/figure-html5/unnamed-chunk-9-1.png","Wisniewski-Biecek_files/figure-latex/all-1.pdf","Wisniewski-Biecek_files/figure-latex/mitigation-1.pdf","Wisniewski-Biecek_files/figure-latex/unnamed-chunk-4-1.pdf","Wisniewski-Biecek_files/figure-latex/unnamed-chunk-6-1.pdf","Wisniewski-Biecek_files/figure-latex/unnamed-chunk-9-1.pdf","Wisniewski-Biecek_files/jquery-3.6.0/jquery-3.6.0.js","Wisniewski-Biecek_files/jquery-3.6.0/jquery-3.6.0.min.js","Wisniewski-Biecek_files/jquery-3.6.0/jquery-3.6.0.min.map","Wisniewski-Biecek_files/popper-2.6.0/popper.min.js","Wisniewski-Biecek_files/webcomponents-2.0.0/webcomponents.js","Wisniewski-Biecek-corrected.tex","Wisniewski-Biecek-v7.html","Wisniewski-Biecek-v7.pdf","Wisniewski-Biecek.bib","Wisniewski-Biecek.html","Wisniewski-Biecek.pdf","Wisniewski-Biecek.tex"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
<meta name="distill:offset" content="../.."/>

<script type="application/javascript">

  window.headroom_prevent_pin = false;

  window.document.addEventListener("DOMContentLoaded", function (event) {

    // initialize headroom for banner
    var header = $('header').get(0);
    var headerHeight = header.offsetHeight;
    var headroom = new Headroom(header, {
      tolerance: 5,
      onPin : function() {
        if (window.headroom_prevent_pin) {
          window.headroom_prevent_pin = false;
          headroom.unpin();
        }
      }
    });
    headroom.init();
    if(window.location.hash)
      headroom.unpin();
    $(header).addClass('headroom--transition');

    // offset scroll location for banner on hash change
    // (see: https://github.com/WickyNilliams/headroom.js/issues/38)
    window.addEventListener("hashchange", function(event) {
      window.scrollTo(0, window.pageYOffset - (headerHeight + 25));
    });

    // responsive menu
    $('.distill-site-header').each(function(i, val) {
      var topnav = $(this);
      var toggle = topnav.find('.nav-toggle');
      toggle.on('click', function() {
        topnav.toggleClass('responsive');
      });
    });

    // nav dropdowns
    $('.nav-dropbtn').click(function(e) {
      $(this).next('.nav-dropdown-content').toggleClass('nav-dropdown-active');
      $(this).parent().siblings('.nav-dropdown')
         .children('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $("body").click(function(e){
      $('.nav-dropdown-content').removeClass('nav-dropdown-active');
    });
    $(".nav-dropdown").click(function(e){
      e.stopPropagation();
    });
  });
</script>

<style type="text/css">

/* Theme (user-documented overrideables for nav appearance) */

.distill-site-nav {
  color: rgba(255, 255, 255, 0.8);
  background-color: #0F2E3D;
  font-size: 15px;
  font-weight: 300;
}

.distill-site-nav a {
  color: inherit;
  text-decoration: none;
}

.distill-site-nav a:hover {
  color: white;
}

@media print {
  .distill-site-nav {
    display: none;
  }
}

.distill-site-header {

}

.distill-site-footer {

}


/* Site Header */

.distill-site-header {
  width: 100%;
  box-sizing: border-box;
  z-index: 3;
}

.distill-site-header .nav-left {
  display: inline-block;
  margin-left: 8px;
}

@media screen and (max-width: 768px) {
  .distill-site-header .nav-left {
    margin-left: 0;
  }
}


.distill-site-header .nav-right {
  float: right;
  margin-right: 8px;
}

.distill-site-header a,
.distill-site-header .title {
  display: inline-block;
  text-align: center;
  padding: 14px 10px 14px 10px;
}

.distill-site-header .title {
  font-size: 18px;
  min-width: 150px;
}

.distill-site-header .logo {
  padding: 0;
}

.distill-site-header .logo img {
  display: none;
  max-height: 20px;
  width: auto;
  margin-bottom: -4px;
}

.distill-site-header .nav-image img {
  max-height: 18px;
  width: auto;
  display: inline-block;
  margin-bottom: -3px;
}



@media screen and (min-width: 1000px) {
  .distill-site-header .logo img {
    display: inline-block;
  }
  .distill-site-header .nav-left {
    margin-left: 20px;
  }
  .distill-site-header .nav-right {
    margin-right: 20px;
  }
  .distill-site-header .title {
    padding-left: 12px;
  }
}


.distill-site-header .nav-toggle {
  display: none;
}

.nav-dropdown {
  display: inline-block;
  position: relative;
}

.nav-dropdown .nav-dropbtn {
  border: none;
  outline: none;
  color: rgba(255, 255, 255, 0.8);
  padding: 16px 10px;
  background-color: transparent;
  font-family: inherit;
  font-size: inherit;
  font-weight: inherit;
  margin: 0;
  margin-top: 1px;
  z-index: 2;
}

.nav-dropdown-content {
  display: none;
  position: absolute;
  background-color: white;
  min-width: 200px;
  border: 1px solid rgba(0,0,0,0.15);
  border-radius: 4px;
  box-shadow: 0px 8px 16px 0px rgba(0,0,0,0.1);
  z-index: 1;
  margin-top: 2px;
  white-space: nowrap;
  padding-top: 4px;
  padding-bottom: 4px;
}

.nav-dropdown-content hr {
  margin-top: 4px;
  margin-bottom: 4px;
  border: none;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.nav-dropdown-active {
  display: block;
}

.nav-dropdown-content a, .nav-dropdown-content .nav-dropdown-header {
  color: black;
  padding: 6px 24px;
  text-decoration: none;
  display: block;
  text-align: left;
}

.nav-dropdown-content .nav-dropdown-header {
  display: block;
  padding: 5px 24px;
  padding-bottom: 0;
  text-transform: uppercase;
  font-size: 14px;
  color: #999999;
  white-space: nowrap;
}

.nav-dropdown:hover .nav-dropbtn {
  color: white;
}

.nav-dropdown-content a:hover {
  background-color: #ddd;
  color: black;
}

.nav-right .nav-dropdown-content {
  margin-left: -45%;
  right: 0;
}

@media screen and (max-width: 768px) {
  .distill-site-header a, .distill-site-header .nav-dropdown  {display: none;}
  .distill-site-header a.nav-toggle {
    float: right;
    display: block;
  }
  .distill-site-header .title {
    margin-left: 0;
  }
  .distill-site-header .nav-right {
    margin-right: 0;
  }
  .distill-site-header {
    overflow: hidden;
  }
  .nav-right .nav-dropdown-content {
    margin-left: 0;
  }
}


@media screen and (max-width: 768px) {
  .distill-site-header.responsive {position: relative; min-height: 500px; }
  .distill-site-header.responsive a.nav-toggle {
    position: absolute;
    right: 0;
    top: 0;
  }
  .distill-site-header.responsive a,
  .distill-site-header.responsive .nav-dropdown {
    display: block;
    text-align: left;
  }
  .distill-site-header.responsive .nav-left,
  .distill-site-header.responsive .nav-right {
    width: 100%;
  }
  .distill-site-header.responsive .nav-dropdown {float: none;}
  .distill-site-header.responsive .nav-dropdown-content {position: relative;}
  .distill-site-header.responsive .nav-dropdown .nav-dropbtn {
    display: block;
    width: 100%;
    text-align: left;
  }
}

/* Site Footer */

.distill-site-footer {
  width: 100%;
  overflow: hidden;
  box-sizing: border-box;
  z-index: 3;
  margin-top: 30px;
  padding-top: 30px;
  padding-bottom: 30px;
  text-align: center;
}

/* Headroom */

d-title {
  padding-top: 6rem;
}

@media print {
  d-title {
    padding-top: 4rem;
  }
}

.headroom {
  z-index: 1000;
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
}

.headroom--transition {
  transition: all .4s ease-in-out;
}

.headroom--unpinned {
  top: -100px;
}

.headroom--pinned {
  top: 0;
}

/* adjust viewport for navbar height */
/* helps vertically center bootstrap (non-distill) content */
.min-vh-100 {
  min-height: calc(100vh - 100px) !important;
}

</style>

<script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="../../site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet"/>
<link href="../../site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet"/>
<script src="../../site_libs/headroom-0.9.4/headroom.min.js"></script>
<!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

<style type="text/css">

body {
  background-color: white;
}

.pandoc-table {
  width: 100%;
}

.pandoc-table>caption {
  margin-bottom: 10px;
}

.pandoc-table th:not([align]) {
  text-align: left;
}

.pagedtable-footer {
  font-size: 15px;
}

d-byline .byline {
  grid-template-columns: 2fr 2fr;
}

d-byline .byline h3 {
  margin-block-start: 1.5em;
}

d-byline .byline .authors-affiliations h3 {
  margin-block-start: 0.5em;
}

.authors-affiliations .orcid-id {
  width: 16px;
  height:16px;
  margin-left: 4px;
  margin-right: 4px;
  vertical-align: middle;
  padding-bottom: 2px;
}

d-title .dt-tags {
  margin-top: 1em;
  grid-column: text;
}

.dt-tags .dt-tag {
  text-decoration: none;
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0em 0.4em;
  margin-right: 0.5em;
  margin-bottom: 0.4em;
  font-size: 70%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

d-article table.gt_table td,
d-article table.gt_table th {
  border-bottom: none;
  font-size: 100%;
}

.html-widget {
  margin-bottom: 2.0em;
}

.l-screen-inset {
  padding-right: 16px;
}

.l-screen .caption {
  margin-left: 10px;
}

.shaded {
  background: rgb(247, 247, 247);
  padding-top: 20px;
  padding-bottom: 20px;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .html-widget {
  margin-bottom: 0;
  border: 1px solid rgba(0, 0, 0, 0.1);
}

.shaded .shaded-content {
  background: white;
}

.text-output {
  margin-top: 0;
  line-height: 1.5em;
}

.hidden {
  display: none !important;
}

d-article {
  padding-top: 2.5rem;
  padding-bottom: 30px;
}

d-appendix {
  padding-top: 30px;
}

d-article>p>img {
  width: 100%;
}

d-article h2 {
  margin: 1rem 0 1.5rem 0;
}

d-article h3 {
  margin-top: 1.5rem;
}

d-article iframe {
  border: 1px solid rgba(0, 0, 0, 0.1);
  margin-bottom: 2.0em;
  width: 100%;
}

/* Tweak code blocks */

d-article div.sourceCode code,
d-article pre code {
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: auto;
}

d-article div.sourceCode {
  background-color: white;
}

d-article div.sourceCode pre {
  padding-left: 10px;
  font-size: 12px;
  border-left: 2px solid rgba(0,0,0,0.1);
}

d-article pre {
  font-size: 12px;
  color: black;
  background: none;
  margin-top: 0;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

d-article pre a {
  border-bottom: none;
}

d-article pre a:hover {
  border-bottom: none;
  text-decoration: underline;
}

d-article details {
  grid-column: text;
  margin-bottom: 0.8em;
}

@media(min-width: 768px) {

d-article pre,
d-article div.sourceCode,
d-article div.sourceCode pre {
  overflow: visible !important;
}

d-article div.sourceCode pre {
  padding-left: 18px;
  font-size: 14px;
}

d-article pre {
  font-size: 14px;
}

}

figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

/* CSS for d-contents */

.d-contents {
  grid-column: text;
  color: rgba(0,0,0,0.8);
  font-size: 0.9em;
  padding-bottom: 1em;
  margin-bottom: 1em;
  padding-bottom: 0.5em;
  margin-bottom: 1em;
  padding-left: 0.25em;
  justify-self: start;
}

@media(min-width: 1000px) {
  .d-contents.d-contents-float {
    height: 0;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: center;
    padding-right: 3em;
    padding-left: 2em;
  }
}

.d-contents nav h3 {
  font-size: 18px;
  margin-top: 0;
  margin-bottom: 1em;
}

.d-contents li {
  list-style-type: none
}

.d-contents nav > ul {
  padding-left: 0;
}

.d-contents ul {
  padding-left: 1em
}

.d-contents nav ul li {
  margin-top: 0.6em;
  margin-bottom: 0.2em;
}

.d-contents nav a {
  font-size: 13px;
  border-bottom: none;
  text-decoration: none
  color: rgba(0, 0, 0, 0.8);
}

.d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6)
}

.d-contents nav > ul > li > a {
  font-weight: 600;
}

.d-contents nav > ul > li > ul {
  font-weight: inherit;
}

.d-contents nav > ul > li > ul > li {
  margin-top: 0.2em;
}


.d-contents nav ul {
  margin-top: 0;
  margin-bottom: 0.25em;
}

.d-article-with-toc h2:nth-child(2) {
  margin-top: 0;
}


/* Figure */

.figure {
  position: relative;
  margin-bottom: 2.5em;
  margin-top: 1.5em;
}

.figure .caption {
  color: rgba(0, 0, 0, 0.6);
  font-size: 12px;
  line-height: 1.5em;
}

.figure img.external {
  background: white;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
  padding: 18px;
  box-sizing: border-box;
}

.figure .caption a {
  color: rgba(0, 0, 0, 0.6);
}

.figure .caption b,
.figure .caption strong, {
  font-weight: 600;
  color: rgba(0, 0, 0, 1.0);
}

/* Citations */

d-article .citation {
  color: inherit;
  cursor: inherit;
}

div.hanging-indent{
  margin-left: 1em; text-indent: -1em;
}

/* Citation hover box */

.tippy-box[data-theme~=light-border] {
  background-color: rgba(250, 250, 250, 0.95);
}

.tippy-content > p {
  margin-bottom: 0;
  padding: 2px;
}


/* Tweak 1000px media break to show more text */

@media(min-width: 1000px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 16px;
  }

  .grid {
    grid-column-gap: 16px;
  }

  d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
  }
  figure .caption, .figure .caption, figure figcaption {
    font-size: 13px;
  }
}

@media(min-width: 1180px) {
  .base-grid,
  distill-header,
  d-title,
  d-abstract,
  d-article,
  d-appendix,
  distill-appendix,
  d-byline,
  d-footnote-list,
  d-citation-list,
  distill-footer {
    grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
    grid-column-gap: 32px;
  }

  .grid {
    grid-column-gap: 32px;
  }
}


/* Get the citation styles for the appendix (not auto-injected on render since
   we do our own rendering of the citation appendix) */

d-appendix .citation-appendix,
.d-appendix .citation-appendix {
  font-size: 11px;
  line-height: 15px;
  border-left: 1px solid rgba(0, 0, 0, 0.1);
  padding-left: 18px;
  border: 1px solid rgba(0,0,0,0.1);
  background: rgba(0, 0, 0, 0.02);
  padding: 10px 18px;
  border-radius: 3px;
  color: rgba(150, 150, 150, 1);
  overflow: hidden;
  margin-top: -12px;
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* Include appendix styles here so they can be overridden */

d-appendix {
  contain: layout style;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-top: 60px;
  margin-bottom: 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  color: rgba(0,0,0,0.5);
  padding-top: 60px;
  padding-bottom: 48px;
}

d-appendix h3 {
  grid-column: page-start / text-start;
  font-size: 15px;
  font-weight: 500;
  margin-top: 1em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.65);
}

d-appendix h3 + * {
  margin-top: 1em;
}

d-appendix ol {
  padding: 0 0 0 15px;
}

@media (min-width: 768px) {
  d-appendix ol {
    padding: 0 0 0 30px;
    margin-left: -30px;
  }
}

d-appendix li {
  margin-bottom: 1em;
}

d-appendix a {
  color: rgba(0, 0, 0, 0.6);
}

d-appendix > * {
  grid-column: text;
}

d-appendix > d-footnote-list,
d-appendix > d-citation-list,
d-appendix > distill-appendix {
  grid-column: screen;
}

/* Include footnote styles here so they can be overridden */

d-footnote-list {
  contain: layout style;
}

d-footnote-list > * {
  grid-column: text;
}

d-footnote-list a.footnote-backlink {
  color: rgba(0,0,0,0.3);
  padding-left: 0.5em;
}



/* Anchor.js */

.anchorjs-link {
  /*transition: all .25s linear; */
  text-decoration: none;
  border-bottom: none;
}
*:hover > .anchorjs-link {
  margin-left: -1.125em !important;
  text-decoration: none;
  border-bottom: none;
}

/* Social footer */

.social_footer {
  margin-top: 30px;
  margin-bottom: 0;
  color: rgba(0,0,0,0.67);
}

.disqus-comments {
  margin-right: 30px;
}

.disqus-comment-count {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  cursor: pointer;
}

#disqus_thread {
  margin-top: 30px;
}

.article-sharing a {
  border-bottom: none;
  margin-right: 8px;
}

.article-sharing a:hover {
  border-bottom: none;
}

.sidebar-section.subscribe {
  font-size: 12px;
  line-height: 1.6em;
}

.subscribe p {
  margin-bottom: 0.5em;
}


.article-footer .subscribe {
  font-size: 15px;
  margin-top: 45px;
}


.sidebar-section.custom {
  font-size: 12px;
  line-height: 1.6em;
}

.custom p {
  margin-bottom: 0.5em;
}

/* Styles for listing layout (hide title) */
.layout-listing d-title, .layout-listing .d-title {
  display: none;
}

/* Styles for posts lists (not auto-injected) */


.posts-with-sidebar {
  padding-left: 45px;
  padding-right: 45px;
}

.posts-list .description h2,
.posts-list .description p {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

.posts-list .description h2 {
  font-weight: 700;
  border-bottom: none;
  padding-bottom: 0;
}

.posts-list h2.post-tag {
  border-bottom: 1px solid rgba(0, 0, 0, 0.2);
  padding-bottom: 12px;
}
.posts-list {
  margin-top: 60px;
  margin-bottom: 24px;
}

.posts-list .post-preview {
  text-decoration: none;
  overflow: hidden;
  display: block;
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  padding: 24px 0;
}

.post-preview-last {
  border-bottom: none !important;
}

.posts-list .posts-list-caption {
  grid-column: screen;
  font-weight: 400;
}

.posts-list .post-preview h2 {
  margin: 0 0 6px 0;
  line-height: 1.2em;
  font-style: normal;
  font-size: 24px;
}

.posts-list .post-preview p {
  margin: 0 0 12px 0;
  line-height: 1.4em;
  font-size: 16px;
}

.posts-list .post-preview .thumbnail {
  box-sizing: border-box;
  margin-bottom: 24px;
  position: relative;
  max-width: 500px;
}
.posts-list .post-preview img {
  width: 100%;
  display: block;
}

.posts-list .metadata {
  font-size: 12px;
  line-height: 1.4em;
  margin-bottom: 18px;
}

.posts-list .metadata > * {
  display: inline-block;
}

.posts-list .metadata .publishedDate {
  margin-right: 2em;
}

.posts-list .metadata .dt-authors {
  display: block;
  margin-top: 0.3em;
  margin-right: 2em;
}

.posts-list .dt-tags {
  display: block;
  line-height: 1em;
}

.posts-list .dt-tags .dt-tag {
  display: inline-block;
  color: rgba(0,0,0,0.6);
  padding: 0.3em 0.4em;
  margin-right: 0.2em;
  margin-bottom: 0.4em;
  font-size: 60%;
  border: 1px solid rgba(0,0,0,0.2);
  border-radius: 3px;
  text-transform: uppercase;
  font-weight: 500;
}

.posts-list img {
  opacity: 1;
}

.posts-list img[data-src] {
  opacity: 0;
}

.posts-more {
  clear: both;
}


.posts-sidebar {
  font-size: 16px;
}

.posts-sidebar h3 {
  font-size: 16px;
  margin-top: 0;
  margin-bottom: 0.5em;
  font-weight: 400;
  text-transform: uppercase;
}

.sidebar-section {
  margin-bottom: 30px;
}

.categories ul {
  list-style-type: none;
  margin: 0;
  padding: 0;
}

.categories li {
  color: rgba(0, 0, 0, 0.8);
  margin-bottom: 0;
}

.categories li>a {
  border-bottom: none;
}

.categories li>a:hover {
  border-bottom: 1px solid rgba(0, 0, 0, 0.4);
}

.categories .active {
  font-weight: 600;
}

.categories .category-count {
  color: rgba(0, 0, 0, 0.4);
}


@media(min-width: 768px) {
  .posts-list .post-preview h2 {
    font-size: 26px;
  }
  .posts-list .post-preview .thumbnail {
    float: right;
    width: 30%;
    margin-bottom: 0;
  }
  .posts-list .post-preview .description {
    float: left;
    width: 45%;
  }
  .posts-list .post-preview .metadata {
    float: left;
    width: 20%;
    margin-top: 8px;
  }
  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.5em;
    font-size: 16px;
  }
  .posts-with-sidebar .posts-list {
    float: left;
    width: 75%;
  }
  .posts-with-sidebar .posts-sidebar {
    float: right;
    width: 20%;
    margin-top: 60px;
    padding-top: 24px;
    padding-bottom: 24px;
  }
}


/* Improve display for browsers without grid (IE/Edge <= 15) */

.downlevel {
  line-height: 1.6em;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  margin: 0;
}

.downlevel .d-title {
  padding-top: 6rem;
  padding-bottom: 1.5rem;
}

.downlevel .d-title h1 {
  font-size: 50px;
  font-weight: 700;
  line-height: 1.1em;
  margin: 0 0 0.5rem;
}

.downlevel .d-title p {
  font-weight: 300;
  font-size: 1.2rem;
  line-height: 1.55em;
  margin-top: 0;
}

.downlevel .d-byline {
  padding-top: 0.8em;
  padding-bottom: 0.8em;
  font-size: 0.8rem;
  line-height: 1.8em;
}

.downlevel .section-separator {
  border: none;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
}

.downlevel .d-article {
  font-size: 1.06rem;
  line-height: 1.7em;
  padding-top: 1rem;
  padding-bottom: 2rem;
}


.downlevel .d-appendix {
  padding-left: 0;
  padding-right: 0;
  max-width: none;
  font-size: 0.8em;
  line-height: 1.7em;
  margin-bottom: 0;
  color: rgba(0,0,0,0.5);
  padding-top: 40px;
  padding-bottom: 48px;
}

.downlevel .footnotes ol {
  padding-left: 13px;
}

.downlevel .base-grid,
.downlevel .distill-header,
.downlevel .d-title,
.downlevel .d-abstract,
.downlevel .d-article,
.downlevel .d-appendix,
.downlevel .distill-appendix,
.downlevel .d-byline,
.downlevel .d-footnote-list,
.downlevel .d-citation-list,
.downlevel .distill-footer,
.downlevel .appendix-bottom,
.downlevel .posts-container {
  padding-left: 40px;
  padding-right: 40px;
}

@media(min-width: 768px) {
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
  padding-left: 150px;
  padding-right: 150px;
  max-width: 900px;
}
}

.downlevel pre code {
  display: block;
  border-left: 2px solid rgba(0, 0, 0, .1);
  padding: 0 0 0 20px;
  font-size: 14px;
}

.downlevel code, .downlevel pre {
  color: black;
  background: none;
  font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.5;

  -moz-tab-size: 4;
  -o-tab-size: 4;
  tab-size: 4;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

.downlevel .posts-list .post-preview {
  color: inherit;
}



</style>

<script type="application/javascript">

function is_downlevel_browser() {
  if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                 window.navigator.userAgent)) {
    return true;
  } else {
    return window.load_distill_framework === undefined;
  }
}

// show body when load is complete
function on_load_complete() {

  // add anchors
  if (window.anchors) {
    window.anchors.options.placement = 'left';
    window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
  }


  // set body to visible
  document.body.style.visibility = 'visible';

  // force redraw for leaflet widgets
  if (window.HTMLWidgets) {
    var maps = window.HTMLWidgets.findAll(".leaflet");
    $.each(maps, function(i, el) {
      var map = this.getMap();
      map.invalidateSize();
      map.eachLayer(function(layer) {
        if (layer instanceof L.TileLayer)
          layer.redraw();
      });
    });
  }

  // trigger 'shown' so htmlwidgets resize
  $('d-article').trigger('shown');
}

function init_distill() {

  init_common();

  // create front matter
  var front_matter = $('<d-front-matter></d-front-matter>');
  $('#distill-front-matter').wrap(front_matter);

  // create d-title
  $('.d-title').changeElementType('d-title');

  // create d-byline
  var byline = $('<d-byline></d-byline>');
  $('.d-byline').replaceWith(byline);

  // create d-article
  var article = $('<d-article></d-article>');
  $('.d-article').wrap(article).children().unwrap();

  // move posts container into article
  $('.posts-container').appendTo($('d-article'));

  // create d-appendix
  $('.d-appendix').changeElementType('d-appendix');

  // flag indicating that we have appendix items
  var appendix = $('.appendix-bottom').children('h3').length > 0;

  // replace footnotes with <d-footnote>
  $('.footnote-ref').each(function(i, val) {
    appendix = true;
    var href = $(this).attr('href');
    var id = href.replace('#', '');
    var fn = $('#' + id);
    var fn_p = $('#' + id + '>p');
    fn_p.find('.footnote-back').remove();
    var text = fn_p.html();
    var dtfn = $('<d-footnote></d-footnote>');
    dtfn.html(text);
    $(this).replaceWith(dtfn);
  });
  // remove footnotes
  $('.footnotes').remove();

  // move refs into #references-listing
  $('#references-listing').replaceWith($('#refs'));

  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    var id = $(this).attr('id');
    $('.d-contents a[href="#' + id + '"]').parent().remove();
    appendix = true;
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
  });

  // show d-appendix if we have appendix content
  $("d-appendix").css('display', appendix ? 'grid' : 'none');

  // localize layout chunks to just output
  $('.layout-chunk').each(function(i, val) {

    // capture layout
    var layout = $(this).attr('data-layout');

    // apply layout to markdown level block elements
    var elements = $(this).children().not('details, div.sourceCode, pre, script');
    elements.each(function(i, el) {
      var layout_div = $('<div class="' + layout + '"></div>');
      if (layout_div.hasClass('shaded')) {
        var shaded_content = $('<div class="shaded-content"></div>');
        $(this).wrap(shaded_content);
        $(this).parent().wrap(layout_div);
      } else {
        $(this).wrap(layout_div);
      }
    });


    // unwrap the layout-chunk div
    $(this).children().unwrap();
  });

  // remove code block used to force  highlighting css
  $('.distill-force-highlighting-css').parent().remove();

  // remove empty line numbers inserted by pandoc when using a
  // custom syntax highlighting theme
  $('code.sourceCode a:empty').remove();

  // load distill framework
  load_distill_framework();

  // wait for window.distillRunlevel == 4 to do post processing
  function distill_post_process() {

    if (!window.distillRunlevel || window.distillRunlevel < 4)
      return;

    // hide author/affiliations entirely if we have no authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
    if (!have_authors)
      $('d-byline').addClass('hidden');

    // article with toc class
    $('.d-contents').parent().addClass('d-article-with-toc');

    // strip links that point to #
    $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

    // add orcid ids
    $('.authors-affiliations').find('.author').each(function(i, el) {
      var orcid_id = front_matter.authors[i].orcidID;
      if (orcid_id) {
        var a = $('<a></a>');
        a.attr('href', 'https://orcid.org/' + orcid_id);
        var img = $('<img></img>');
        img.addClass('orcid-id');
        img.attr('alt', 'ORCID ID');
        img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
        a.append(img);
        $(this).append(a);
      }
    });

    // hide elements of author/affiliations grid that have no value
    function hide_byline_column(caption) {
      $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
    }

    // affiliations
    var have_affiliations = false;
    for (var i = 0; i<front_matter.authors.length; ++i) {
      var author = front_matter.authors[i];
      if (author.affiliation !== "&nbsp;") {
        have_affiliations = true;
        break;
      }
    }
    if (!have_affiliations)
      $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

    // published date
    if (!front_matter.publishedDate)
      hide_byline_column("Published");

    // document object identifier
    var doi = $('d-byline').find('h3:contains("DOI")');
    var doi_p = doi.next().empty();
    if (!front_matter.doi) {
      // if we have a citation and valid citationText then link to that
      if ($('#citation').length > 0 && front_matter.citationText) {
        doi.html('Citation');
        $('<a href="#citation"></a>')
          .text(front_matter.citationText)
          .appendTo(doi_p);
      } else {
        hide_byline_column("DOI");
      }
    } else {
      $('<a></a>')
         .attr('href', "https://doi.org/" + front_matter.doi)
         .html(front_matter.doi)
         .appendTo(doi_p);
    }

     // change plural form of authors/affiliations
    if (front_matter.authors.length === 1) {
      var grid = $('.authors-affiliations');
      grid.children('h3:contains("Authors")').text('Author');
      grid.children('h3:contains("Affiliations")').text('Affiliation');
    }

    // remove d-appendix and d-footnote-list local styles
    $('d-appendix > style:first-child').remove();
    $('d-footnote-list > style:first-child').remove();

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // hoverable references
    $('span.citation[data-cites]').each(function() {
      const citeChild = $(this).children()[0]
      // Do not process if @xyz has been used without escaping and without bibliography activated
      // https://github.com/rstudio/distill/issues/466
      if (citeChild === undefined) return true

      if (citeChild.nodeName == "D-FOOTNOTE") {
        var fn = citeChild
        $(this).html(fn.shadowRoot.querySelector("sup"))
        $(this).id = fn.id
        fn.remove()
      }
      var refs = $(this).attr('data-cites').split(" ");
      var refHtml = refs.map(function(ref) {
        // Could use CSS.escape too here, we insure backward compatibility in navigator
        return "<p>" + $('div[id="ref-' + ref + '"]').html() + "</p>";
      }).join("\n");
      window.tippy(this, {
        allowHTML: true,
        content: refHtml,
        maxWidth: 500,
        interactive: true,
        interactiveBorder: 10,
        theme: 'light-border',
        placement: 'bottom-start'
      });
    });

    // fix footnotes in tables (#411)
    // replacing broken distill.pub feature
    $('table d-footnote').each(function() {
      // we replace internal showAtNode methode which is triggered when hovering a footnote
      this.hoverBox.showAtNode = function(node) {
        // ported from https://github.com/distillpub/template/pull/105/files
        calcOffset = function(elem) {
            let x = elem.offsetLeft;
            let y = elem.offsetTop;
            // Traverse upwards until an `absolute` element is found or `elem`
            // becomes null.
            while (elem = elem.offsetParent && elem.style.position != 'absolute') {
                x += elem.offsetLeft;
                y += elem.offsetTop;
            }

            return { left: x, top: y };
        }
        // https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetTop
        const bbox = node.getBoundingClientRect();
        const offset = calcOffset(node);
        this.show([offset.left + bbox.width, offset.top + bbox.height]);
      }
    })

    // clear polling timer
    clearInterval(tid);

    // show body now that everything is ready
    on_load_complete();
  }

  var tid = setInterval(distill_post_process, 50);
  distill_post_process();

}

function init_downlevel() {

  init_common();

   // insert hr after d-title
  $('.d-title').after($('<hr class="section-separator"/>'));

  // check if we have authors
  var front_matter = JSON.parse($("#distill-front-matter").html());
  var have_authors = front_matter.authors && front_matter.authors.length > 0;

  // manage byline/border
  if (!have_authors)
    $('.d-byline').remove();
  $('.d-byline').after($('<hr class="section-separator"/>'));
  $('.d-byline a').remove();

  // remove toc
  $('.d-contents').remove();

  // move appendix elements
  $('h1.appendix, h2.appendix').each(function(i, val) {
    $(this).changeElementType('h3');
  });
  $('h3.appendix').each(function(i, val) {
    $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
  });


  // inject headers into references and footnotes
  var refs_header = $('<h3></h3>');
  refs_header.text('References');
  $('#refs').prepend(refs_header);

  var footnotes_header = $('<h3></h3');
  footnotes_header.text('Footnotes');
  $('.footnotes').children('hr').first().replaceWith(footnotes_header);

  // move appendix-bottom entries to the bottom
  $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
  $('.appendix-bottom').remove();

  // remove appendix if it's empty
  if ($('.d-appendix').children().length === 0)
    $('.d-appendix').remove();

  // prepend separator above appendix
  $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

  // trim code
  $('pre>code').each(function(i, val) {
    $(this).html($.trim($(this).html()));
  });

  // move posts-container right before article
  $('.posts-container').insertBefore($('.d-article'));

  $('body').addClass('downlevel');

  on_load_complete();
}


function init_common() {

  // jquery plugin to change element types
  (function($) {
    $.fn.changeElementType = function(newType) {
      var attrs = {};

      $.each(this[0].attributes, function(idx, attr) {
        attrs[attr.nodeName] = attr.nodeValue;
      });

      this.replaceWith(function() {
        return $("<" + newType + "/>", attrs).append($(this).contents());
      });
    };
  })(jQuery);

  // prevent underline for linked images
  $('a > img').parent().css({'border-bottom' : 'none'});

  // mark non-body figures created by knitr chunks as 100% width
  $('.layout-chunk').each(function(i, val) {
    var figures = $(this).find('img, .html-widget');
    // ignore leaflet img layers (#106)
    figures = figures.filter(':not(img[class*="leaflet"])')
    if ($(this).attr('data-layout') !== "l-body") {
      figures.css('width', '100%');
    } else {
      figures.css('max-width', '100%');
      figures.filter("[width]").each(function(i, val) {
        var fig = $(this);
        fig.css('width', fig.attr('width') + 'px');
      });

    }
  });

  // auto-append index.html to post-preview links in file: protocol
  // and in rstudio ide preview
  $('.post-preview').each(function(i, val) {
    if (window.location.protocol === "file:")
      $(this).attr('href', $(this).attr('href') + "index.html");
  });

  // get rid of index.html references in header
  if (window.location.protocol !== "file:") {
    $('.distill-site-header a[href]').each(function(i,val) {
      $(this).attr('href', $(this).attr('href').replace(/^index[.]html/, "./"));
    });
  }

  // add class to pandoc style tables
  $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
  $('.kable-table').children('table').addClass('pandoc-table');

  // add figcaption style to table captions
  $('caption').parent('table').addClass("figcaption");

  // initialize posts list
  if (window.init_posts_list)
    window.init_posts_list();

  // implmement disqus comment link
  $('.disqus-comment-count').click(function() {
    window.headroom_prevent_pin = true;
    $('#disqus_thread').toggleClass('hidden');
    if (!$('#disqus_thread').hasClass('hidden')) {
      var offset = $(this).offset();
      $(window).resize();
      $('html, body').animate({
        scrollTop: offset.top - 35
      });
    }
  });
}

document.addEventListener('DOMContentLoaded', function() {
  if (is_downlevel_browser())
    init_downlevel();
  else
    window.addEventListener('WebComponentsReady', init_distill);
});

</script>

<style type="text/css">
/* base variables */

/* Edit the CSS properties in this file to create a custom
   Distill theme. Only edit values in the right column
   for each row; values shown are the CSS defaults.
   To return any property to the default,
   you may set its value to: unset
   All rows must end with a semi-colon.                      */

/* Optional: embed custom fonts here with `@import`          */
/* This must remain at the top of this file.                 */



html {
  /*-- Main font sizes --*/
  --title-size:      50px;
  --body-size:       1.06rem;
  --code-size:       14px;
  --aside-size:      12px;
  --fig-cap-size:    13px;
  /*-- Main font colors --*/
  --title-color:     #000000;
  --header-color:    rgba(0, 0, 0, 0.8);
  --body-color:      rgba(0, 0, 0, 0.8);
  --aside-color:     rgba(0, 0, 0, 0.6);
  --fig-cap-color:   rgba(0, 0, 0, 0.6);
  /*-- Specify custom fonts ~~~ must be imported above   --*/
  --heading-font:    sans-serif;
  --mono-font:       monospace;
  --body-font:       sans-serif;
  --navbar-font:     sans-serif;  /* websites + blogs only */
}

/*-- ARTICLE METADATA --*/
d-byline {
  --heading-size:    0.6rem;
  --heading-color:   rgba(0, 0, 0, 0.5);
  --body-size:       0.8rem;
  --body-color:      rgba(0, 0, 0, 0.8);
}

/*-- ARTICLE TABLE OF CONTENTS --*/
.d-contents {
  --heading-size:    18px;
  --contents-size:   13px;
}

/*-- ARTICLE APPENDIX --*/
d-appendix {
  --heading-size:    15px;
  --heading-color:   rgba(0, 0, 0, 0.65);
  --text-size:       0.8em;
  --text-color:      rgba(0, 0, 0, 0.5);
}

/*-- WEBSITE HEADER + FOOTER --*/
/* These properties only apply to Distill sites and blogs  */

.distill-site-header {
  --title-size:       18px;
  --text-color:       rgba(255, 255, 255, 0.8);
  --text-size:        15px;
  --hover-color:      white;
  --bkgd-color:       #0F2E3D;
}

.distill-site-footer {
  --text-color:       rgba(255, 255, 255, 0.8);
  --text-size:        15px;
  --hover-color:      white;
  --bkgd-color:       #0F2E3D;
}

/*-- Additional custom styles --*/
/* Add any additional CSS rules below                      */
</style>
<style type="text/css">
/* base variables */

/* Edit the CSS properties in this file to create a custom
   Distill theme. Only edit values in the right column
   for each row; values shown are the CSS defaults.
   To return any property to the default,
   you may set its value to: unset
   All rows must end with a semi-colon.                      */

/* Optional: embed custom fonts here with `@import`          */
/* This must remain at the top of this file.                 */



html {
  /*-- Main font sizes --*/
  --title-size:      50px;
  --body-size:       1.06rem;
  --code-size:       14px;
  --aside-size:      12px;
  --fig-cap-size:    13px;
  /*-- Main font colors --*/
  --title-color:     #000000;
  --header-color:    rgba(0, 0, 0, 0.8);
  --body-color:      rgba(0, 0, 0, 0.8);
  --aside-color:     rgba(0, 0, 0, 0.6);
  --fig-cap-color:   rgba(0, 0, 0, 0.6);
  /*-- Specify custom fonts ~~~ must be imported above   --*/
  --heading-font:    sans-serif;
  --mono-font:       monospace;
  --body-font:       sans-serif;
  --navbar-font:     sans-serif;  /* websites + blogs only */
}

/*-- ARTICLE METADATA --*/
d-byline {
  --heading-size:    0.6rem;
  --heading-color:   rgba(0, 0, 0, 0.5);
  --body-size:       0.8rem;
  --body-color:      rgba(0, 0, 0, 0.8);
}

/*-- ARTICLE TABLE OF CONTENTS --*/
.d-contents {
  --heading-size:    18px;
  --contents-size:   13px;
}

/*-- ARTICLE APPENDIX --*/
d-appendix {
  --heading-size:    15px;
  --heading-color:   rgba(0, 0, 0, 0.65);
  --text-size:       0.8em;
  --text-color:      rgba(0, 0, 0, 0.7);
}

/*-- WEBSITE HEADER + FOOTER --*/
/* These properties only apply to Distill sites and blogs  */

.distill-site-header {
  --title-size:       18px;
  --text-color:       rgba(0, 0, 0, 0.8);
  --text-size:        15px;
  --hover-color:      black;
  --bkgd-color:       #ffffff;
}

.distill-site-footer {
  --text-color:       rgba(0, 0, 0, 0.8);
  --text-size:        15px;
  --hover-color:      black;
  --bkgd-color:       #fafafa;
}

/*-- Additional custom styles --*/
/* Add any additional CSS rules below                      */

.nav-right > a {
  text-transform: uppercase;
}

d-title h1, d-title p, d-title figure,
d-abstract p, d-abstract b {
  grid-column: page;
}

.rj-blue {
  color: #2467bb;
}

ul li {
  line-height: 1.6;
  margin-top: 0em;
  margin-bottom: 0em;
}</style>
<style type="text/css">
/* base style */

/* FONT FAMILIES */

:root {
  --heading-default: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  --mono-default: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  --body-default: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
}

body,
.posts-list .post-preview p,
.posts-list .description p {
  font-family: var(--body-font), var(--body-default);
}

h1, h2, h3, h4, h5, h6,
.posts-list .post-preview h2,
.posts-list .description h2 {
  font-family: var(--heading-font), var(--heading-default);
}

d-article div.sourceCode code,
d-article pre code {
  font-family: var(--mono-font), var(--mono-default);
}


/*-- TITLE --*/
d-title h1,
.posts-list > h1 {
  color: var(--title-color, black);
}

d-title h1 {
  font-size: var(--title-size, 50px);
}

/*-- HEADERS --*/
d-article h1,
d-article h2,
d-article h3,
d-article h4,
d-article h5,
d-article h6 {
  color: var(--header-color, rgba(0, 0, 0, 0.8));
}

/*-- BODY --*/
d-article > p,  /* only text inside of <p> tags */
d-article > ul, /* lists */
d-article > ol {
  color: var(--body-color, rgba(0, 0, 0, 0.8));
  font-size: var(--body-size, 1.06rem);
}


/*-- CODE --*/
d-article div.sourceCode code,
d-article pre code {
  font-size: var(--code-size, 14px);
}

/*-- ASIDE --*/
d-article aside {
  font-size: var(--aside-size, 12px);
  color: var(--aside-color, rgba(0, 0, 0, 0.6));
}

/*-- FIGURE CAPTIONS --*/
figure .caption,
figure figcaption,
.figure .caption {
  font-size: var(--fig-cap-size, 13px);
  color: var(--fig-cap-color, rgba(0, 0, 0, 0.6));
}

/*-- METADATA --*/
d-byline h3 {
  font-size: var(--heading-size, 0.6rem);
  color: var(--heading-color, rgba(0, 0, 0, 0.5));
}

d-byline {
  font-size: var(--body-size, 0.8rem);
  color: var(--body-color, rgba(0, 0, 0, 0.8));
}

d-byline a,
d-article d-byline a {
  color: var(--body-color, rgba(0, 0, 0, 0.8));
}

/*-- TABLE OF CONTENTS --*/
.d-contents nav h3 {
  font-size: var(--heading-size, 18px);
}

.d-contents nav a {
  font-size: var(--contents-size, 13px);
}

/*-- APPENDIX --*/
d-appendix h3 {
  font-size: var(--heading-size, 15px);
  color: var(--heading-color, rgba(0, 0, 0, 0.65));
}

d-appendix {
  font-size: var(--text-size, 0.8em);
  color: var(--text-color, rgba(0, 0, 0, 0.5));
}

d-appendix d-footnote-list a.footnote-backlink {
  color: var(--text-color, rgba(0, 0, 0, 0.5));
}

/*-- WEBSITE HEADER + FOOTER --*/
.distill-site-header .title {
  font-size: var(--title-size, 18px);
  font-family: var(--navbar-font), var(--heading-default);
}

.distill-site-header a,
.nav-dropdown .nav-dropbtn {
  font-family: var(--navbar-font), var(--heading-default);
}

.nav-dropdown .nav-dropbtn {
  color: var(--text-color, rgba(255, 255, 255, 0.8));
  font-size: var(--text-size, 15px);
}

.distill-site-header a:hover,
.nav-dropdown:hover .nav-dropbtn {
  color: var(--hover-color, white);
}

.distill-site-header {
  font-size: var(--text-size, 15px);
  color: var(--text-color, rgba(255, 255, 255, 0.8));
  background-color: var(--bkgd-color, #0F2E3D);
}

.distill-site-footer {
  font-size: var(--text-size, 15px);
  color: var(--text-color, rgba(255, 255, 255, 0.8));
  background-color: var(--bkgd-color, #0F2E3D);
}

.distill-site-footer a:hover {
  color: var(--hover-color, white);
}</style>
<!--/radix_placeholder_distill-->
  <script src="../../site_libs/header-attrs-2.14/header-attrs.js"></script>
  <script src="../../site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="../../site_libs/popper-2.6.0/popper.min.js"></script>
  <link href="../../site_libs/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="../../site_libs/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="../../site_libs/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="../../site_libs/anchor-4.2.2/anchor.min.js"></script>
  <script src="../../site_libs/bowser-1.9.3/bowser.min.js"></script>
  <script src="../../site_libs/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="../../site_libs/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
<!--/radix_placeholder_site_in_header-->
  <script>
    $(function() {
      console.log("Starting...")

      // Always show Published - distill hides it if not set
      function show_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'visible');
      }

      show_byline_column('Published')

      // tweak function
      var rmd_meta = JSON.parse($("#radix-rmarkdown-metadata").html());
      function get_meta(name, meta) {
        var ind = meta.attributes.names.value.findIndex((e) => e == name)
        var val = meta.value[ind]
        if (val.type != 'list') {
          return val.value.toString()
        }
        return val
      }

      // tweak description
      // Add clickable tags
      const slug = get_meta('slug', rmd_meta)
      const doi = get_meta('doi', rmd_meta)

      var title = $("d-title").text

      const buttons = $('<div class="dt-tags" style="grid-column: page;">')
      buttons.append('<a href="#citation" class="dt-tag"><i class="fas fa-quote-left"></i> Cite</a>')
      buttons.append('<a href="' + slug + '.pdf" class="dt-tag"><i class="fas fa-file-pdf"></i> PDF</a>')
      buttons.append('<a href="https://twitter.com/intent/tweet?text='+title+'&url=https%3A%2F%2Fdoi.org%2F' + doi + '" class="dt-tag"><i class="fab fa-twitter"></i> Tweet</a>')

      const abstract = $('<d-abstract>')
      abstract.append('<b>Abstract:</b><br>')
      abstract.append($("d-title p:not(:empty)").first()) // Move description to d-abstract
      $("d-title p:empty").remove() // Remove empty paragraphs after title
      abstract.append(buttons)
      abstract.insertAfter($('d-title')) // Add abstract section after title

      // tweak by-line
      var byline = $("d-byline div.byline")
      ind = rmd_meta.attributes.names.value.findIndex((e) => e == "journal")
      const journal = get_meta('journal', rmd_meta)
      const volume = get_meta('volume', rmd_meta)
      const issue = get_meta('issue', rmd_meta)
      const year = 2008 + parseInt(volume)
      const jrtitle = get_meta('title', journal)
      const firstpage = get_meta('firstpage', journal)
      const lastpage = get_meta('lastpage', journal)
      byline.append('<div class="rjournal grid">')
      $('div.rjournal').append('<h3>Volume</h3>')
      $('div.rjournal').append('<h3>Pages</h3>')
      $('div.rjournal').append('<a class="volume" href="../../issues/'+year+'-'+issue+'">'+volume+'/'+issue+'</a>')
      $('div.rjournal').append('<p class="pages">'+firstpage+' - '+lastpage+'</p>')

      const received_date = new Date(get_meta('date_received', rmd_meta))
      byline.find('h3:contains("Published")').parent().append('<h3>Received</h3><p>'+received_date.toLocaleDateString('en-US', {month: 'short'})+' '+received_date.getDate()+', '+received_date.getFullYear()+'</p>')

    })
  </script>

  <style>
      /*
    .nav-dropdown-content .nav-dropdown-header {
      text-transform: lowercase;
    }
    */

    d-byline .byline {
      grid-template-columns: 2fr 2fr 2fr 2fr;
    }

    d-byline .rjournal {
      grid-column-end: span 2;
      grid-template-columns: 1fr 1fr;
      margin-bottom: 0;
    }

    d-title h1, d-title p, d-title figure,
    d-abstract p, d-abstract b {
      grid-column: page;
    }

    d-title .dt-tags {
      grid-column: page;
    }

    .dt-tags .dt-tag {
      text-transform: lowercase;
    }

    d-article h1 {
      line-height: 1.1em;
    }

    d-abstract p, d-article p {
      text-align: justify;
    }

    @media(min-width: 1000px) {
      .d-contents.d-contents-float {
        justify-self: end;
      }

      nav.toc {
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }

    .posts-list .dt-tags .dt-tag {
      text-transform: lowercase;
    }

    @keyframes highlight-target {
      0% {
        background-color: #ffa;
      }
      66% {
        background-color: #ffa;
      }
      100% {
        background-color: none;
      }
    }

    d-article :target, d-appendix :target {
       animation: highlight-target 3s;
    }
  </style>


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"fairmodels: a Flexible Tool for Bias Detection, Visualization, and Mitigation in Binary Classification Models","description":"Machine learning decision systems are becoming omnipresent in our lives. From dating apps to rating loan seekers, algorithms affect both our well-being and future. Typically, however, these systems are not infallible. Moreover, complex predictive models are eager to learn social biases present in historical data that may increase discrimination. If we want to create models responsibly, we need tools for in-depth validation of models also from potential discrimination. This article introduces an R package fairmodels that helps to validate fairness and eliminate bias in binary classification models quickly and flexibly. The fairmodels package offers a model-agnostic approach to bias detection, visualization, and mitigation. The implemented functions and fairness metrics enable model fairness validation from different perspectives. In addition, the package includes a series of methods for bias mitigation that aim to diminish the discrimination in the model.  The package is designed to examine a single model and facilitate comparisons between multiple models.","doi":"10.32614/RJ-2022-019","authors":[{"author":"Jakub Wiśniewski","authorURL":"#","affiliation":"Warsaw University of Technology","affiliationURL":"#","orcidID":""},{"author":"Przemysław Biecek","authorURL":"https://pbiecek.github.io/","affiliation":"Warsaw University of Technology","affiliationURL":"#","orcidID":"0000-0001-8423-1823"}],"publishedDate":"2022-06-27T00:00:00.000+00:00","citationText":"Wiśniewski & Biecek, 2022"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<header class="header header--fixed" role="banner">
<nav class="distill-site-nav distill-site-header">
<div class="nav-left">
<a class="logo" href="../../index.html">
<img src="../../resources/rlogo.png" alt="Logo"/>
</a>
<a href="../../index.html" class="title">The R Journal</a>
</div>
<div class="nav-right">
<a href="../../index.html">Home</a>
<a href="../../issues.html">Issues</a>
<a href="../../news.html">News</a>
<a href="../../issues/2023-1">Current</a>
<a href="../../submissions.html">Submit</a>
<a href="../../articles.xml">
<i class="fa fa-rss" aria-hidden="true"></i>
</a>
<a href="javascript:void(0);" class="nav-toggle">&#9776;</a>
</div>
</nav>
</header>
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>fairmodels: a Flexible Tool for Bias Detection, Visualization, and Mitigation in Binary Classification Models</h1>
<!--radix_placeholder_categories-->
<!--/radix_placeholder_categories-->
<p><p>Machine learning decision systems are becoming omnipresent in our lives. From dating apps to rating loan seekers, algorithms affect both our well-being and future. Typically, however, these systems are not infallible. Moreover, complex predictive models are eager to learn social biases present in historical data that may increase discrimination. If we want to create models responsibly, we need tools for in-depth validation of models also from potential discrimination. This article introduces an R package fairmodels that helps to validate fairness and eliminate bias in binary classification models quickly and flexibly. The fairmodels package offers a model-agnostic approach to bias detection, visualization, and mitigation. The implemented functions and fairness metrics enable model fairness validation from different perspectives. In addition, the package includes a series of methods for bias mitigation that aim to diminish the discrimination in the model. The package is designed to examine a single model and facilitate comparisons between multiple models.</p></p>
</div>

<div class="d-byline">
  Jakub Wiśniewski  (Warsaw University of Technology)
  
,   Przemysław Biecek <a href="https://pbiecek.github.io/" class="uri">https://pbiecek.github.io/</a> (Warsaw University of Technology)
  
<br/>2022-06-27
</div>

<div class="d-article">
<h1 id="introduction">Introduction</h1>
<p>Responsible machine learning and, in particular, fairness is gaining attention within the machine learning community. This is because predictive algorithms are becoming more and more decisive and influential in our lives. This impact could be less or more significant in areas ranging from user feeds on social platforms, displayed ads, and recommendations at an online store to loan decisions, social scoring, and facial recognition systems used by police and authorities. Sometimes it leads to automated systems that learn some undesired bias preserved in data for some historical reason. Whether seeking a job <span class="citation" data-cites="8731591">(<a href="#ref-8731591" role="doc-biblioref">Lahoti et al. 2019</a>)</span> or having one’s data processed by court systems <span class="citation" data-cites="propublica">(<a href="#ref-propublica" role="doc-biblioref">Angwin et al. 2016</a>)</span>, sensitive attributes such as sex, race, religion, ethnicity, etc., might play a significant role in the decision. Even if such variables are not directly included in the model, they are often captured by proxy variables such as zip code (a proxy for the race and wealth), purchased products (a proxy for gender and age), eye colour (a proxy for ethnicity). As one would expect, they can give an unfair advantage to a privileged group. Discrimination takes the form of more favorable predictions or higher accuracy for a privileged group. For example, some popular commercial gender classifiers were found to perform the worst on darker females <span class="citation" data-cites="pmlr-v81-buolamwini18a">(<a href="#ref-pmlr-v81-buolamwini18a" role="doc-biblioref">Buolamwini and Gebru 2018</a>)</span>. From now on, such unfair and harmful decisions towards people with specific sensitive attributes will be called biased.</p>
<p>The list of protected attributes may depend on the region and domain for which the model is built. For example, the European Union law is summarized in the Handbook on European non-discrimination law <span class="citation" data-cites="European-non-discrimination">European Union Agency for Fundamental Rights and Council of Europe (<a href="#ref-European-non-discrimination" role="doc-biblioref">2018</a>)</span>, which lists the following protected attributes that cannot be the basis for inferior treatment: sex, gender identity, sexual orientation, disability, age, race, ethnicity, nationality or national origin, religion or belief, social origin, birth, and property, language, political or other opinions. This list, though long, does not include all potentially relevant items, e.g. in the USA, a protected attribute is also pregnancy, the status of a war veteran, or genetic information.</p>
<p>While there are historical and economic reasons for this to happen, such decisions are unacceptable in society, where nobody should have an unfair advantage.
The problem is not simple, especially when the only criterion set for the system is performance. We observe a trade-off between accuracy and fairness in some cases where lower discrimination leads to lower performance <span class="citation" data-cites="kamiran">(<a href="#ref-kamiran" role="doc-biblioref">Kamiran and Calders 2011</a>)</span>. Sometimes labels, which are considered ground truth, might also be biased <span class="citation" data-cites="labelwrong">(<a href="#ref-labelwrong" role="doc-biblioref">Wick et al. 2019</a>)</span>, and when controlling for that bias, the performance and fairness might improve simultaneously. However fairness is not a concept that a single number can summarize, so most of the time, when we want to improve fairness from one perspective, it becomes worse in another <span class="citation" data-cites="barocas-hardt-narayanan">(<a href="#ref-barocas-hardt-narayanan" role="doc-biblioref">Barocas et al. 2019</a>)</span>.</p>
<p>The bias in machine learning systems has potentially many different sources. <span class="citation" data-cites="mehrabi2019survey">Mehrabi et al. (<a href="#ref-mehrabi2019survey" role="doc-biblioref">2019</a>)</span> categorized bias into its types like historical bias, where unfairness is already embedded into the data reflecting the world, observer bias, sampling bias, ranking and social biases, and many more. That shows how many dangers are potentially hidden in the data itself. Whether one would like to act on it or not, it is essential to detect bias and make well-informed decisions whose consequences could potentially harm many groups of people. Repercussions of such systems can be unpredictable. As argued by <span class="citation" data-cites="barocas-hardt-narayanan">Barocas et al. (<a href="#ref-barocas-hardt-narayanan" role="doc-biblioref">2019</a>)</span>, machine learning systems can even aggravate the disparities between groups, which is called by the authors’ feedback loops. Sometimes the risk of potential harm resulting from the usage of such systems is high. This was noticed, for example, by the Council of Europe that wrote the set of guidelines where it states that the usage of facial recognition for the sake of determining a person’s sex, age, origin, or even emotions should be mostly prohibited <span class="citation" data-cites="facialrecognition">(<a href="#ref-facialrecognition" role="doc-biblioref">Council of Europe 2021</a>)</span>.</p>
<p>Not every difference in treatment is discrimination. <span class="citation" data-cites="cirillo_sex_2020">Cirillo et al. (<a href="#ref-cirillo_sex_2020" role="doc-biblioref">2020</a>)</span> presents examples of desirable and undesirable biases based on the medical domain. For example, in the case of cardiovascular diseases, documented medical knowledge indicates that different treatments are more effective for different genders. So different treatment regimens according to medical knowledge are examples of desirable bias. Later in this paper, we present tools to identify differences between groups defined by some protected attribute but note that this does not automatically mean that there is discrimination.</p>
<p>We would also like to point out that focusing on the machine learning model may not be enough in some cases, and sometimes the design of the data acquisition and/or annotation cause the model to be biased <span class="citation" data-cites="barocas-hardt-narayanan">(<a href="#ref-barocas-hardt-narayanan" role="doc-biblioref">Barocas et al. 2019</a>)</span>.</p>
<h2 id="related-work">Related work</h2>
<p>Assembling predictive models is getting easier nowadays. Packages like <a href="https://cran.r-project.org/package=h2o">h2o</a> <span class="citation" data-cites="H2OAutoML">(<a href="#ref-H2OAutoML" role="doc-biblioref">H2O.ai 2017</a>)</span> provide AutoML frameworks where non-experts can train quickly accurate models without deep domain knowledge. Model validation should also be that simple. Yet this is not the case. There are still very few tools to support the fairness diagnostics of the model.</p>
<p>Two main kinds of fairness are a concern to multiple stakeholders. These are group and individual fairness. The first one concerns groups of people with the same protected attributes (gender, race, etc.). It focuses on measuring if these groups are treated similarly by the model. The second one is focused on the individual. It is most intuitively defined as treating similar individuals similarly <span class="citation" data-cites="statisticalparity">(<a href="#ref-statisticalparity" role="doc-biblioref">Dwork et al. 2012</a>)</span>. Both concepts are sometimes considered to conflict with each other, but they don’t need to be if we factor in certain assumptions, such as whether the disparities are due to personal choices or unjust structures <span class="citation" data-cites="reuben">(<a href="#ref-reuben" role="doc-biblioref">Binns 2020</a>)</span>.</p>
<p>Several frameworks have emerged for Python to verify various fairness criteria, the most popular are <strong>aif360</strong> <span class="citation" data-cites="aif360-oct-2018">(<a href="#ref-aif360-oct-2018" role="doc-biblioref">Bellamy et al. 2018</a>)</span>, <strong>fairlearn</strong> <span class="citation" data-cites="bird2020fairlearn">(<a href="#ref-bird2020fairlearn" role="doc-biblioref">Bird et al. 2020</a>)</span>, or <strong>aequitas</strong> <span class="citation" data-cites="2018aequitas">(<a href="#ref-2018aequitas" role="doc-biblioref">Saleiro et al. 2018</a>)</span>. They have various features for detecting, visualization, and mitigating bias in machine learning models.</p>
<p>For the R language, until recently, the only available tool was the <a href="https://cran.r-project.org/package=fairness">fairness</a> <span class="citation" data-cites="fairness">(<a href="#ref-fairness" role="doc-biblioref">Kozodoi and V. Varga 2021</a>)</span> package which compares various fairness metrics for specified subgroups. The <a href="#">fairness</a> package is very helpful, but it lacks some features. For example, it does not allow comparing the machine learning models and aggregating fairness metrics to facilitate the visualization. Still, most of all, it does not give a quick verdict on whether a model is fair or not. Package <a href="https://cran.r-project.org/package=fairadapt">fairadapt</a> aims at removing bias from machine learning models by implementing pre-processing procedure described in <span class="citation" data-cites="plecko2019fair">Plečko and Meinshausen (<a href="#ref-plecko2019fair" role="doc-biblioref">2019</a>)</span>. Our package tries to combine the detection and mitigation processes. It encourages the user to experiment with the bias, try different mitigation methods and compare results. The package <a href="#">fairmodels</a> not only allows for that comparison between models and multiple exposed groups of people, but it gives direct feedback if the model is fair or not (more on that in the next section). Our package also equips the user with a so-called <code>fairness_object</code>, an object aggregating possibly many models, information about data, and fairness metrics. <code>fairness_object</code> can later be transformed into many other objects that can facilitate the visualization of metrics and models from different perspectives. If a model does not meet fairness criteria, various pre-processing and post-processing bias mitigation algorithms are implemented and ready to use. It aims to be a complete tool for dealing with discriminatory models in a group fairness setting.</p>
<p>In particular, in the following sections, we show how to use this package to address four key questions: <em>How to measure bias? How to detect bias? How to visualize bias?</em> and <em>How to mitigate bias?</em></p>
<p>It is important to remember that fairness is not a binary concept that can be unambiguously defined, and there is no silver bullet that will make any model fair. The presented tools allow for fairness exploratory analysis, thanks to which we will be able to detect differences in the behavior of the model for different protected groups. But such analysis will not guarantee that all possible fairness problems have been detected. Also, fairness analysis is only one of a wide range of techniques for Explanatory Model Analysis <span class="citation" data-cites="ema2021">(<a href="#ref-ema2021" role="doc-biblioref">Biecek and Burzykowski 2021</a>)</span>.
Like other explanatory tools, it should be used with caution and awareness.</p>
<h1 id="measuring-and-detecting-bias">Measuring and detecting bias</h1>
<p>In model fairness analysis, a distinction is often made between group fairness and individual fairness analysis. The former is defined by the equality of certain statistics determined on protected subgroups, and we focus on this approach in this section. We write more about the latter later in this paper.</p>
<h2 id="detect">Fairness metrics</h2>
<p>Machine learning models, just like human-based decisions, can be biased against observations related to people with certain sensitive attributes, which are also called protected groups. This is because they consist of subgroups - people who share the same sensitive attribute, like gender, race, or other features.</p>
<p>To address this problem, we need first to introduce fairness criteria. Following <span class="citation" data-cites="barocas-hardt-narayanan">Barocas et al. (<a href="#ref-barocas-hardt-narayanan" role="doc-biblioref">2019</a>)</span>, we will present these criteria based on the following notation.</p>
<ul>
<li>Let <span class="math inline">\(A \in \{a,b, ...\}\)</span> mean protected group and values <span class="math inline">\(A \neq a\)</span> denote membership to unprivileged subgroups while <span class="math inline">\(A = a\)</span> membership to privileged subgroup. To simplify the notation, we will treat this as a binary variable (so <span class="math inline">\(A = b\)</span> will denote membership to unprivileged subgroup), but all results hold if <span class="math inline">\(A\)</span> has a larger number of groups.<br />
</li>
<li>Let <span class="math inline">\(Y \in \{0,1\}\)</span> be a binary label (binary target = binary classification) where <span class="math inline">\(1\)</span> is preferred, favorable outcome.</li>
<li>Let <span class="math inline">\(R \in [0,1]\)</span> be a probabilistic response of the model, and <span class="math inline">\(\hat{Y} \in \{0,1\}\)</span> is the binarised model response, so <span class="math inline">\(\hat{Y} = 1\)</span> when <span class="math inline">\(R \geq 0.5\)</span>, otherwise <span class="math inline">\(\hat{Y} = 0\)</span>.</li>
</ul>
<p>Figure <a href="#fig:fairnessTable1">1</a> summarizes possible situations for the subgroup <span class="math inline">\(A=a\)</span>. We can draw up the same table for each of the subgroups.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fairnessTable1"></span>
<img src="table1.png" alt="Summary of possible model outcomes for subpopulation $A = a$. We assume that outcome $Y = 1$ is favourable." width="100%" />
<p class="caption">
Figure 1: Summary of possible model outcomes for subpopulation <span class="math inline">\(A = a\)</span>. We assume that outcome <span class="math inline">\(Y = 1\)</span> is favourable.
</p>
</div>
</div>
<p>According to <span class="citation" data-cites="barocas-hardt-narayanan">Barocas et al. (<a href="#ref-barocas-hardt-narayanan" role="doc-biblioref">2019</a>)</span> most discrimination criteria can be derived as tests that validate the following probabilistic definitions:</p>
<ul>
<li>Independence, i.e. <span class="math inline">\(R \perp A\)</span>,</li>
<li>Separation, i.e. <span class="math inline">\(R \perp A \mid Y\)</span>,</li>
<li>Sufficiency, i.e. <span class="math inline">\(Y \perp A \mid R\)</span>.</li>
</ul>
<p>Those criteria and their relaxations might be expressed via different metrics based on a confusion matrix for a certain subgroup. To check if those fairness criteria are addressed, we propose checking five metrics among privileged group (a) and unprivileged group (b):</p>
<ul>
<li>Statistical parity: <span class="math inline">\(P(\hat{Y} = 1 | A = a) = P(\hat{Y} = 1 | A = b)\)</span>. Statistical parity (STP) ensures that fractions of assigned positive labels are the same in subgroups. It is equivalent of Independence <span class="citation" data-cites="statisticalparity">(<a href="#ref-statisticalparity" role="doc-biblioref">Dwork et al. 2012</a>)</span>. In other words, the values in the last column of Figure <a href="#fig:fairnessTable1">1</a> are the same for each subgroup.</li>
<li>Equal opportunity: <span class="math inline">\(P(\hat{Y} = 1 | A = a, Y = 1) = P(\hat{Y} = 1 | A = b, Y = 1)\)</span>. Checks if classifier has equal True Positive Rate (TPR) for each subgroup. In other words, the column normalized values in the second column of Figure <a href="#fig:fairnessTable1">1</a> are the same for each subgroup. It is a relaxation of Separation <span class="citation" data-cites="NIPS20166374">(<a href="#ref-NIPS20166374" role="doc-biblioref">Hardt et al. 2016</a>)</span>.</li>
<li>Predictive parity: <span class="math inline">\(P(Y = 1 | A = a, \hat{Y} = 1) = P(Y = 1 | A = b, \hat{Y} = 1)\)</span>. Measures if a model has equal Positive Predictive Value (PPV) for each subgroup. In other words, the row normalized values in the second row of Figure <a href="#fig:fairnessTable1">1</a> are the same for each subgroup. It is relaxation of Sufficiency <span class="citation" data-cites="ppv">(<a href="#ref-ppv" role="doc-biblioref">Chouldechova 2016</a>)</span>.</li>
<li>Predictive equality: <span class="math inline">\(P(\hat{Y} = 1 | A = a, Y = 0) = P(\hat{Y} = 1 | A = b, Y = 0)\)</span>. Warrants that classifiers have equal False Positive Rate (FPR) for each subgroup. In other words, the column normalized values in the third column of Figure <a href="#fig:fairnessTable1">1</a> are the same for each subgroup. It is relaxation of Separation <span class="citation" data-cites="ppe">(<a href="#ref-ppe" role="doc-biblioref">Corbett-Davies et al. 2017</a>)</span>.</li>
<li>(Overall) Accuracy equality: <span class="math inline">\(P(\hat{Y} = Y | A = a) = P(\hat{Y} = Y | A = b)\)</span>. Makes sure that models have the same Accuracy (ACC) for each subgroup. <span class="citation" data-cites="accuracy">(<a href="#ref-accuracy" role="doc-biblioref">Berk et al. 2017</a>)</span></li>
</ul>
<p>The reader should note that if the classifier passes Equal opportunity and Predictive equality, it also passes Equalized Odds <span class="citation" data-cites="NIPS20166374">(<a href="#ref-NIPS20166374" role="doc-biblioref">Hardt et al. 2016</a>)</span>, which is equivalent to Separation criteria.</p>
<p>Let us illustrate the intuition behind Independence, Separation, and Sufficiency criteria using the well-known example of the COMPAS model for estimating recidivism risk.
Fulfilling the Independence criterion means that the rate of sentenced prisoners should be equal in each subpopulation. It can be said that such an approach is fair from society’s perspective.</p>
<p>Fulfilling the Separation criterion means that the fraction of innocents/guilty sentenced should be equal in subgroups. Such an approach is fair from the prisoner’s perspective. The reasoning is the following: <em>“If I am innocent, I should have the same chance of acquittal regardless of sub-population”</em>. This was the expectation presented by the ProPublica Foundation in their study.</p>
<p>Meeting the Sufficiency criterion means that there should be an equal fraction of innocents among the convicted, similarly, for the non-convicted. This approach is fair from the judge’s perspective. The reasoning is the following: <em>“If I convicted someone, he should have the same chance of being innocent regardless of the sub-population”</em>. This approach is presented by the company developing the COMPAS model, Northpointe.
Unfortunately, as we have already written, it is not possible to meet all these criteria at the same time.</p>
<p>While defining the metrics above, we assumed only two subgroups. This was done to facilitate notation, but there might be more unprivileged subgroups. A perfectly fair model would pass all criteria for each subgroup <span class="citation" data-cites="barocas-hardt-narayanan">(<a href="#ref-barocas-hardt-narayanan" role="doc-biblioref">Barocas et al. 2019</a>)</span>.</p>
<p>Not all fairness metrics are equally important in all cases. The metrics above aim to give a more holistic view into the fairness of the machine learning model. Practitioners informed in the domain may consider only those metrics that are relevant and beneficial from their point of view. For example, in <span class="citation" data-cites="KOZODOI2021">Kozodoi et al. (<a href="#ref-KOZODOI2021" role="doc-biblioref">2021</a>)</span> in the fair credit scoring use case, the authors concluded that the separation is the most suitable non-discrimination criteria. More general instructions can also be found in <span class="citation" data-cites="EUhandbook">European Union Agency for Fundamental Rights (<a href="#ref-EUhandbook" role="doc-biblioref">2018</a>)</span>, along with examples of protected attributes. Sometimes, however, non-technical solutions to fairness problems might be beneficial. Note that group fairness metrics will discover not all types of unfairness, and the end-user should decide whether a model is acceptable in terms of bias or not.</p>
<p>However tempting it is to think that all the criteria described above can be met at the same time, unfortunately, this is not possible. <span class="citation" data-cites="barocas-hardt-narayanan">Barocas et al. (<a href="#ref-barocas-hardt-narayanan" role="doc-biblioref">2019</a>)</span> shows that, apart from a few hypothetical situations, no two of <em>{Independence, Separation, Sufficiency}</em> can be fulfilled simultaneously. So we are left balancing between the degree of imbalance of the different criteria or deciding to control only one criterion.</p>
<h2 id="bias">Acceptable amount of bias</h2>
<p>It would be hard for any classifier to maintain the same relations between subgroups. That is why some margins around the perfect agreement are needed. To address this issue, we accepted the four-fifths rule <span class="citation" data-cites="adverseimpact">(<a href="#ref-adverseimpact" role="doc-biblioref">Code of Federal Regulations 1978</a>)</span> as the benchmark for discrimination rate, which states that <em>“A selection rate for any race, sex, or ethnic group which is less than four-fifths (<em><span class="math inline">\(\frac{4}{5}\)</span></em>) (or eighty percent) of the rate for the group with the highest rate will generally be regarded by the Federal enforcement agencies as evidence of adverse impact[…].”</em> The selection rate is originally represented by statistical parity, but we adopted this rule to define acceptable rates between subgroups for all metrics. There are a few caveats to the preceding citation concerning the size of the sample and the boundary itself. Nevertheless, the four-fifths rule is an excellent guideline to adhere to. In the implementation, this boundary is represented by <span class="math inline">\(\varepsilon\)</span>, and it is adjustable by the user, but the default value will be 0.8.
This rule is often used, but users should check if the fairness criteria should be set differently in each case.</p>
<p>Let <span class="math inline">\(\varepsilon &gt; 0\)</span> be the acceptable amount of a bias. In this article, we would say that the model is not discriminatory for a particular metric if the ratio between every unprivileged <span class="math inline">\(b, c, ...\)</span> and privileged subgroup <span class="math inline">\(a\)</span> is within <span class="math inline">\((\varepsilon, \frac{1}{\varepsilon})\)</span>. The common choice for the epsilon is 0.8, which corresponds to the four-fifths rule. For example, for the metric Statistical Parity (<span class="math inline">\(STP\)</span>), a model would be <span class="math inline">\(\varepsilon\)</span>-non-discriminatory for privileged subgroup <span class="math inline">\(a\)</span> if it satisfies.</p>
<p><span class="math display" id="eq:ratio">\[\begin{equation}
\forall_{b \in A \setminus \{a\}} \;\;
   \varepsilon &lt; STP_{ratio} = \frac{STP_b}{STP_a} &lt; \frac{1}{\varepsilon}.
  \tag{1}
\end{equation}\]</span></p>
<h2 id="evaluating-fairness">Evaluating fairness</h2>
<p>The main function in the <a href="#">fairmodels</a> package is <code>fairness_check</code>. It returns <code>fairness_object</code>, which can be visualized or processed by other functions. This will be further explained in the “Structure” section. When calling <code>fairness_check</code> for the first time, the following three arguments are mandatory:</p>
<ul>
<li><code>explainer</code> - an object that combines model and data that gives a unified interface for predictions. It is a wrapper over a model created with the <a href="https://cran.r-project.org/package=DALEX">DALEX</a> <span class="citation" data-cites="JMLRv19">(<a href="#ref-JMLRv19" role="doc-biblioref">Biecek 2018</a>)</span> package.</li>
<li><code>protected</code> - a factor, vector containing sensitive attributes (protected group). It does not need to be binary. Instead, each level denotes a distinct subgroup. The most common examples are gender, race, nationality, etc.</li>
<li><code>privileged</code> - a character/factor denoting a level in the protected vector which is suspected to be the most privileged one.</li>
</ul>
<h3 id="example">Example</h3>
<p>In the following example, we are using <em>German Credit Data</em> dataset <span class="citation" data-cites="Dua2019">(<a href="#ref-Dua2019" role="doc-biblioref">Dua and Graff 2017</a>)</span>. In the dataset, there is information about people like age, sex, purpose, credit amount, etc. For each person, there is a risk assessed with taking credit, either good or bad. Therefore, it will be a target variable. We will train the model on the whole dataset and then measure fairness metrics to facilitate the notation (as opposed to training and testing on different subsets, which is also possible and advisable).</p>
<p>First, we create a model. Let’s start with logistic regression.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='st'><a href='https://fairmodels.drwhy.ai/'>"fairmodels"</a></span><span class='op'>)</span>
<span class='fu'><a href='https://rdrr.io/r/utils/data.html'>data</a></span><span class='op'>(</span><span class='st'>"german"</span><span class='op'>)</span>

<span class='va'>lm_model</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/stats/glm.html'>glm</a></span><span class='op'>(</span><span class='va'>Risk</span><span class='op'>~</span><span class='va'>.</span>, data <span class='op'>=</span> <span class='va'>german</span>, family <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/stats/family.html'>binomial</a></span><span class='op'>(</span>link <span class='op'>=</span> <span class='st'>"logit"</span><span class='op'>)</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Then, create a wrapper that unifies the model interface.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='st'><a href='https://modeloriented.github.io/DALEX/'>"DALEX"</a></span><span class='op'>)</span>

<span class='va'>y_numeric</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/base/numeric.html'>as.numeric</a></span><span class='op'>(</span><span class='va'>german</span><span class='op'>$</span><span class='va'>Risk</span><span class='op'>)</span> <span class='op'>-</span><span class='fl'>1</span>
<span class='va'>explainer_lm</span> <span class='op'>&lt;-</span> <span class='fu'>DALEX</span><span class='fu'>::</span><span class='fu'><a href='https://modeloriented.github.io/DALEX/reference/explain.html'>explain</a></span><span class='op'>(</span><span class='va'>lm_model</span>, data <span class='op'>=</span> <span class='va'>german</span><span class='op'>[</span>,<span class='op'>-</span><span class='fl'>1</span><span class='op'>]</span>, y <span class='op'>=</span> <span class='va'>y_numeric</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Now we are ready to calculate and plot the fairness checks. Resulting plot is presented in Figure <a href="#fig:fairness-plot-1">2</a>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>fobject</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/pkg/fairmodels/man/fairness_check.html'>fairness_check</a></span><span class='op'>(</span><span class='va'>explainer_lm</span>,
                protected <span class='op'>=</span> <span class='va'>german</span><span class='op'>$</span><span class='va'>Sex</span>, privileged <span class='op'>=</span> <span class='st'>"male"</span>,
                verbose <span class='op'>=</span> <span class='cn'>FALSE</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='fu'><a href='https://rdrr.io/r/graphics/plot.default.html'>plot</a></span><span class='op'>(</span><span class='va'>fobject</span><span class='op'>)</span>
</code></pre>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fairness-plot-1"></span>
<img src="RJ-2022-019_files/figure-html5/fairness-plot-1-1.png" alt="The Fairness Check plot summarises the ratio of fairness measures between unprivileged and privileged subgroups. The light green areas correspond to values within $(\varepsilon, \frac{1}{\varepsilon})$ and signify an acceptable difference in fairness metrics. They are bounded by red rectangles indicating values that do not meet the 4/5 rule. Fairness metrics names are given along the formulas used to calculate the score in some subgroups to facilitate interpretation. For example, the ratio here means that after metric scores were calculated, the values for unprivileged groups (female) were divided by values for the privileged subgroup (male). In this example, except for the predictive equality ratio, the other measures are $\varepsilon$-non-discriminatory. " width="75%" />
<p class="caption">
Figure 2: The Fairness Check plot summarises the ratio of fairness measures between unprivileged and privileged subgroups. The light green areas correspond to values within <span class="math inline">\((\varepsilon, \frac{1}{\varepsilon})\)</span> and signify an acceptable difference in fairness metrics. They are bounded by red rectangles indicating values that do not meet the 4/5 rule. Fairness metrics names are given along the formulas used to calculate the score in some subgroups to facilitate interpretation. For example, the ratio here means that after metric scores were calculated, the values for unprivileged groups (female) were divided by values for the privileged subgroup (male). In this example, except for the predictive equality ratio, the other measures are <span class="math inline">\(\varepsilon\)</span>-non-discriminatory.
</p>
</div>
</div>
<p>For a quick assessment, if a model passes fairness criteria, the object created with <code>fairness_check()</code> might be summarized with the <code>print()</code> function. Total loss is the sum of all fairness metrics. See equation <a href="#eq:parityLoss">(2)</a> for more details.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='fu'><a href='https://rdrr.io/r/base/print.html'>print</a></span><span class='op'>(</span><span class='va'>fobject</span>, colorize <span class='op'>=</span> <span class='cn'>FALSE</span><span class='op'>)</span>
</code></pre>
</div>
<pre><code>
Fairness check for models: lm 

lm passes 4/5 metrics
Total loss :  0.6153324 </code></pre>
</div>
<p>In this example, fairness criteria are satisfied in all but one metric. The logistic regression model has a lower false-positive rate (FP/(FP+TN))) in the unprivileged group than in the privileged group. It exceeds the acceptable limit set by <span class="math inline">\(\varepsilon\)</span>. Thus it does not satisfy the Predictive Equality ratio criteria.</p>
<p>More detailed visualizations are available, like <em>Metric scores</em> plot. It might be helpful to understand the intuition behind the <em>Fairness check</em> plot presented above. See an example in Figure <a href="#fig:fairness-plot-2">3</a>. This plot might be a good first point for understanding the <em>Fairness check</em> plot. In fact, checks can be directly derived from the <em>Metric scores</em> plot. To do this, we need to divide the score denoted by the dot with the score denoted by the vertical line. This way, we obtain a value indicated by the height of the barplot. The orientation of the barplot depends on whether the value is bigger or lower than 1. Intuitively the longer the horizontal line in the figure below (the one connecting the dot with the vertical line) is, the longer the bar will be in <em>Fairness check</em> plot. If the scores of privileged and unprivileged subgroups are the same, then the bar will start from 1 and point to 1, so it will have a height equal to 0.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='fu'><a href='https://rdrr.io/r/graphics/plot.default.html'>plot</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/pkg/fairmodels/man/metric_scores.html'>metric_scores</a></span><span class='op'>(</span><span class='va'>fobject</span><span class='op'>)</span><span class='op'>)</span>
</code></pre>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fairness-plot-2"></span>
<img src="RJ-2022-019_files/figure-html5/fairness-plot-2-1.png" alt="The Metric Scores plot summarises raw fairness metrics scores for subgroups. The dots stand for unprivileged subgroups (female) while vertical lines stans for the privileged subgroup (male). The horizontal lines act as a visual aid for measuring the difference between the scores of the metrics between the privileged and unprivileged subgroups." width="75%" />
<p class="caption">
Figure 3: The Metric Scores plot summarises raw fairness metrics scores for subgroups. The dots stand for unprivileged subgroups (female) while vertical lines stans for the privileged subgroup (male). The horizontal lines act as a visual aid for measuring the difference between the scores of the metrics between the privileged and unprivileged subgroups.
</p>
</div>
</div>
<p>It is rare that a model perfectly meets all the fairness criteria. Therefore, a handy feature is the ability to compare several models on the same scale. We add two more explainers to the fairness assessment in the example below. Now <code>fairness_object</code> (in code: <code>fobject</code>) wraps three models together with different labels and cutoffs for subgroups. The <code>fairness_object</code> can be later used as a basis for another <code>fairness_object</code>. In detail, while running <code>fairness_check()</code> for the first time, explainer/explainers have to be provided along with three arguments described at the start of this section. However, as shown below, when providing explainers with a <code>fairness_object</code>, those arguments are not necessary as they are already a part of the previously created <code>fairness_object</code>.</p>
<p>First, let us create two more models based on the <em>German Credit Data</em>. The first will be a logistic regression model that uses fewer columns and has access to the <code>Sex</code> feature. The second is random forest from <a href="https://cran.r-project.org/package=ranger">ranger</a> <span class="citation" data-cites="ranger">(<a href="#ref-ranger" role="doc-biblioref">Wright and Ziegler 2017</a>)</span>. It will be trained on the whole dataset.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>discriminative_lm_model</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/stats/glm.html'>glm</a></span><span class='op'>(</span><span class='va'>Risk</span><span class='op'>~</span><span class='va'>.</span>,
         data   <span class='op'>=</span> <span class='va'>german</span><span class='op'>[</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='st'>"Risk"</span>, <span class='st'>"Sex"</span>,<span class='st'>"Age"</span>,
                <span class='st'>"Checking.account"</span>, <span class='st'>"Credit.amount"</span><span class='op'>)</span><span class='op'>]</span>,
         family <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/stats/family.html'>binomial</a></span><span class='op'>(</span>link <span class='op'>=</span> <span class='st'>"logit"</span><span class='op'>)</span><span class='op'>)</span>

<span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='st'><a href='https://github.com/imbs-hl/ranger'>"ranger"</a></span><span class='op'>)</span>
<span class='va'>rf_model</span> <span class='op'>&lt;-</span> <span class='fu'>ranger</span><span class='fu'>::</span><span class='fu'><a href='https://rdrr.io/pkg/ranger/man/ranger.html'>ranger</a></span><span class='op'>(</span><span class='va'>Risk</span> <span class='op'>~</span><span class='va'>.</span>,
         data <span class='op'>=</span> <span class='va'>german</span>, probability <span class='op'>=</span> <span class='cn'>TRUE</span>,
         max.depth <span class='op'>=</span> <span class='fl'>4</span>, seed <span class='op'>=</span> <span class='fl'>123</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>These models differ in the way how the predict function works. To unify operations on these models, we need to create <a href="https://cran.r-project.org/package=DALEX">DALEX</a> explainer objects. The <code>label</code> argument specifies how these models are named on plots.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>explainer_dlm</span> <span class='op'>&lt;-</span> <span class='fu'>DALEX</span><span class='fu'>::</span><span class='fu'><a href='https://modeloriented.github.io/DALEX/reference/explain.html'>explain</a></span><span class='op'>(</span><span class='va'>discriminative_lm_model</span>,
        data <span class='op'>=</span> <span class='va'>german</span><span class='op'>[</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='st'>"Sex"</span>, <span class='st'>"Age"</span>, <span class='st'>"Checking.account"</span>, <span class='st'>"Credit.amount"</span><span class='op'>)</span><span class='op'>]</span>,
        y <span class='op'>=</span> <span class='va'>y_numeric</span>,
        label <span class='op'>=</span> <span class='st'>"discriminative_lm"</span><span class='op'>)</span> 

<span class='va'>explainer_rf</span> <span class='op'>&lt;-</span> <span class='fu'>DALEX</span><span class='fu'>::</span><span class='fu'><a href='https://modeloriented.github.io/DALEX/reference/explain.html'>explain</a></span><span class='op'>(</span><span class='va'>rf_model</span>, 
        data <span class='op'>=</span> <span class='va'>german</span><span class='op'>[</span>,<span class='op'>-</span><span class='fl'>1</span><span class='op'>]</span>, y <span class='op'>=</span> <span class='va'>y_numeric</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Now we are ready to assess fairness. The resulting plot is presented in Figure <a href="#fig:fairness-plot-3">4</a>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>fobject</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/pkg/fairmodels/man/fairness_check.html'>fairness_check</a></span><span class='op'>(</span><span class='va'>explainer_rf</span>, <span class='va'>explainer_dlm</span>, <span class='va'>fobject</span><span class='op'>)</span>
<span class='fu'><a href='https://rdrr.io/r/graphics/plot.default.html'>plot</a></span><span class='op'>(</span><span class='va'>fobject</span><span class='op'>)</span>
</code></pre>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fairness-plot-3"></span>
<img src="RJ-2022-019_files/figure-html5/fairness-plot-3-1.png" alt="The Fairness Check plot for multiple models. It helps to compare models based on five selected fairness measures. " width="75%" />
<p class="caption">
Figure 4: The Fairness Check plot for multiple models. It helps to compare models based on five selected fairness measures.
</p>
</div>
</div>
<p>When plotted, new bars appear on the fairness check plot. Those are new metric scores for added models. This information can be summarized in a numerical way with the <code>print()</code> function.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='fu'><a href='https://rdrr.io/r/base/print.html'>print</a></span><span class='op'>(</span><span class='va'>fobject</span>, colorize <span class='op'>=</span> <span class='cn'>FALSE</span><span class='op'>)</span>
</code></pre>
</div>
<pre><code>
Fairness check for models: ranger, discriminative_lm, lm 

ranger passes 5/5 metrics
Total loss :  0.1699186 

discriminative_lm passes 3/5 metrics
Total loss :  0.7294678 

lm passes 4/5 metrics
Total loss :  0.6153324 </code></pre>
</div>
<h1 id="package-architecture">Package architecture</h1>
<p>The <a href="#">fairmodels</a> package provides a unified interface for predictive models independently of their internal structure. Using a model agnostic approach with <a href="#">DALEX</a> explainers facilitates this process <span class="citation" data-cites="JMLRv19">(<a href="#ref-JMLRv19" role="doc-biblioref">Biecek 2018</a>)</span>. There is a unified way for each explainer to check if explained model lives up to user fairness standards. Checking fairness with <a href="#">fairmodels</a> is straightforward and can be done with the three-step pipeline.</p>
<pre><code>classification model   |&gt;   explain()   |&gt;   fairness_check() </code></pre>
<p>The output of such a pipeline is an object of class <code>fairness_object</code>, a unified structure to wrap model explainer or multiple model explainers and other <code>fairness_objects</code> in a single container. Aggregation of fairness measures is done based on groups defined by model labels. This is why model explainers (even those wrapped by <code>fairness_objects</code>) must have different labels. Moreover, some visualizations for model comparison assume that all models are created from the same data. Of course, each model can use different variables or different feature transformations, but the order and number of rows shall stay the same. To facilitate aggregation of models <a href="#">fairmodels</a> allows creating <code>fairness_objects</code> in other ways:</p>
<ul>
<li><code>explainers |&gt; fairness_check()</code> - possibly many explainers can be passed to <code>fairness_check()</code>,</li>
<li><code>fairness_objects |&gt; fairness_check()</code> - explainers stored in <code>fairness_objects</code> passed to <code>fairness_check()</code> will be aggregated into one <code>fairness_object</code>,</li>
<li><code>explainer</code> &amp; <code>fairness_objects |&gt; fairness_check()</code> - explainers passed directly and explainers from <code>fairness_objects</code> will be aggregated into one <code>fairness_object</code>.</li>
</ul>
<p>When using the last two pipelines, protected vectors and privileged parameters are assumed to be the same, so passing them to <code>fairness_check()</code> is unnecessary.</p>
<p>To create a <code>fairness_object</code>, at least one explainer needs to be passed to <code>fairness_check()</code> function, which returns the said object. <code>fairness_object</code> metrics for each subgroup are calculated from the separate confusion matrices.</p>
<p>The <code>fairness_object</code> has numerous fields. Some of them are:</p>
<ul>
<li><code>parity_loss_metric_data</code> - data.frame containing parity loss for each metric and classifier,</li>
<li><code>groups_data</code> - list of metric scores for each metric and model,</li>
<li><code>group_confusion_matrices</code> - list of values in confusion matrices for each model and metric,</li>
<li><code>explainers</code> - list of DALEX explainers. When explainers and/or <code>fairness_object</code> are added, then explainers and/or explainers extracted from <code>fairness_object</code> are added to that list,</li>
<li><code>label</code> - character vector of labels for each explainer.</li>
<li><code>...</code> - other fields.</li>
</ul>
<p>The <code>fairness_object</code> methods are used to create numerous objects that help to visualize bias. In the next sections, we list more detailed functions for deeper exploration of bias. Detailed relations between objects created with <a href="#">fairmodels</a> are depicted in Figure <a href="#fig:classdiagram">5</a>.
The general overview of the workflow is presented in Figure <a href="#fig:flowchart">6</a>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:classdiagram"></span>
<img src="class_diagram.png" alt="Class diagram for objects created by functions from the fairmodels package. Each rectangle corresponds to one class, the name of this class is in the header of the rectangle. Each of these classes is a list containing a certain list of objects. The top slot lists the names and types of each object the list. The bottom slot contains a list of functions that can be performed on objects of the specified class. If two classes are connected by a line ending in a diamond it means that one class contains objects of the other class. If two rectangles are connected by a dashed line, it means that on the basis of one object, an object of another class can be produced. In this case, more detailed fairness statistics can be produced from the central object of the fairness check class. See the full resolution at https://bit.ly/3HNbNvo" width="100%" />
<p class="caption">
Figure 5: Class diagram for objects created by functions from the fairmodels package. Each rectangle corresponds to one class, the name of this class is in the header of the rectangle. Each of these classes is a list containing a certain list of objects. The top slot lists the names and types of each object the list. The bottom slot contains a list of functions that can be performed on objects of the specified class. If two classes are connected by a line ending in a diamond it means that one class contains objects of the other class. If two rectangles are connected by a dashed line, it means that on the basis of one object, an object of another class can be produced. In this case, more detailed fairness statistics can be produced from the central object of the fairness check class. See the full resolution at <a href="https://bit.ly/3HNbNvo" class="uri">https://bit.ly/3HNbNvo</a>
</p>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:flowchart"></span>
<img src="flow.png" alt="Flowchart for the fairness assessment with the fairmodels package. The arrows describe typical sequences of actions when exploring the fairness of the models. For ease of use, the names of the functions that can be used in a given step are indicated. Note that this procedure is intended to look at the model from multiple perspectives in order to track down potential problems in the model. Merely satisfying the fairness criteria does not automatically mean that the model is free of any errors" width="100%" />
<p class="caption">
Figure 6: Flowchart for the fairness assessment with the fairmodels package. The arrows describe typical sequences of actions when exploring the fairness of the models. For ease of use, the names of the functions that can be used in a given step are indicated. Note that this procedure is intended to look at the model from multiple perspectives in order to track down potential problems in the model. Merely satisfying the fairness criteria does not automatically mean that the model is free of any errors
</p>
</div>
</div>
<h1 id="visualization">Visualizing bias</h1>
<p>In <a href="#">fairmodels</a> there are 12 metrics based on confusion matrices for each subgroup, see the following table for the complete list. Some of them were already introduced before.</p>
<div class="layout-chunk" data-layout="l-body">
<table>
<caption>
Fairness metrics implemented in the <strong>fairmodels</strong>
package.
</caption>
<colgroup>
<col style="width: 19%" />
<col style="width: 21%" />
<col style="width: 14%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr class="header">
<th>
Metric
</th>
<th>
Formula
</th>
<th>
Name
</th>
<th>
Fairness criteria
</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>
TPR
</td>
<td>
<span class="math inline"><span class="math inline">\(\frac{TP}{TP + FN}\)</span></span>
</td>
<td>
True positive rate
</td>
<td>
Equal opportunity <span class="citation" data-cites="NIPS20166374">(<a href="#ref-NIPS20166374"
role="doc-biblioref">Hardt et al. 2016</a>)</span>
</td>
</tr>
<tr class="even">
<td>
TNR
</td>
<td>
<span class="math inline"><span class="math inline">\(\frac{TN}{TN + FP}\)</span></span>
</td>
<td>
True negative rate
</td>
<td>
</td>
</tr>
<tr class="odd">
<td>
PPV
</td>
<td>
<span class="math inline"><span class="math inline">\(\frac{TP}{TP + FP}\)</span></span>
</td>
<td>
Positive predictive value
</td>
<td>
Predictive parity <span class="citation" data-cites="ppv">(<a
href="#ref-ppv" role="doc-biblioref">Chouldechova 2016</a>)</span>
</td>
</tr>
<tr class="even">
<td>
NPV
</td>
<td>
<span class="math inline"><span class="math inline">\(\frac{TN}{TN + FN}\)</span></span>
</td>
<td>
Negative predictive value
</td>
<td>
</td>
</tr>
<tr class="odd">
<td>
FNR
</td>
<td>
<span class="math inline"><span class="math inline">\(\frac{FN}{FN + TP}\)</span></span>
</td>
<td>
False negative rate
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
FPR
</td>
<td>
<span class="math inline"><span class="math inline">\(\frac{FP}{FP + TN}\)</span></span>
</td>
<td>
False positive rate
</td>
<td>
Predictive equality <span class="citation" data-cites="ppe">(<a
href="#ref-ppe" role="doc-biblioref">Corbett-Davies et al.
2017</a>)</span>
</td>
</tr>
<tr class="odd">
<td>
FDR
</td>
<td>
<span class="math inline"><span class="math inline">\(\frac{FP}{FP + TP}\)</span></span>
</td>
<td>
False discovery rate
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
FOR
</td>
<td>
<span class="math inline"><span class="math inline">\(\frac{FN}{FN + TN}\)</span></span>
</td>
<td>
False omission rate
</td>
<td>
</td>
</tr>
<tr class="odd">
<td>
TS
</td>
<td>
<span class="math inline"><span class="math inline">\(\frac{TP}{TP + FN + FP}\)</span></span>
</td>
<td>
Threat score
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
F1
</td>
<td>
<span class="math inline"><span class="math inline">\(\frac{2 \cdot PPV * TPR}{PPV + TPR}\)</span></span>
</td>
<td>
F1 score
</td>
<td>
</td>
</tr>
<tr class="odd">
<td>
STP
</td>
<td>
<span class="math inline"><span class="math inline">\(\frac{TP + FP}{TP + FP + TN + FN}\)</span></span>
</td>
<td>
Positive rate
</td>
<td>
Statistical parity
</td>
</tr>
<tr class="even">
<td>
ACC
</td>
<td>
<span class="math inline"><span class="math inline">\(\frac{TP + TN}{TP + TN + FP + FN}\)</span></span>
</td>
<td>
Accuracy
</td>
<td>
Overall accuracy equality
</td>
</tr>
</tbody>
</table>
</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<p>Not all metrics are needed to determine if the discrimination exists, but they are helpful to acquire a fuller picture. To facilitate the visualization over many subgroups, we introduce a function that maps metric scores among subgroups to a single value. This function, which we call <code>parity_loss</code>, has an attractive property. Due to the usage of the absolute value of the natural logarithm, it will return the same value whether the ratio is inverted or not.</p>
<p>So, for example, when we would like to know the parity loss of Statistical Parity between unprivileged (b) and privileged (a) subgroups, we mean value like this:</p>
<p><span class="math display">\[\begin{equation}
STP_{\textit{parity loss}} = \Big | \ln \Big( \frac{STP_b}{STP_a} \Big)\Big|.
\end{equation}\]</span></p>
<p>This notation is very helpful because it allows to accumulate <span class="math inline">\(STP_{\textit{parity loss}}\)</span> overall unprivileged subgroups, so not only in the binary case.</p>
<p><span class="math display" id="eq:parityLoss">\[\begin{equation}
STP_{\textit{parity loss}} = \sum_{i \in \{a, b, ...\}} \Big|\ln \Big(\frac{STP_i}{STP_a} \Big)\Big|.  
  \tag{2}
\end{equation}\]</span></p>
<p>The <code>parity_loss</code> relates strictly to ratios. The classifier is more fair if <code>parity_loss</code> is low. This property is helpful in visualizations.</p>
<p>There are several modifying functions that operate on <code>fairness_object</code>. Their usage will return other objects. The relations between them is depicted on the class diagram (Figure <a href="#fig:classdiagram">5</a>). The objects can then be plotted with a generic <code>plot()</code> function. Additionally, a special plotting function works immediately on <code>fairness_object</code>, which is <code>plot_density</code>. The user can directly specify which metrics shall be visible in the plot in some functions. The detailed technical introduction for all these functions is presented in <a href="#">fairmodels</a>.</p>
<p>Plots visualizing different aspects of <code>parity_loss</code> can be created with one of the following pipelines:</p>
<ul>
<li><code>fairness_object |&gt; modifying_function(...) |&gt; plot()</code><br />
This pipe is preferred and allows setting parameters in both modifying functions and certain plot functions, which is not the case with the next pipeline.</li>
<li><code>fairness_object |&gt; plot_fairmodels(type = modifying_function, ...)</code><br />
Additional parameters are passed to the modifying functions and not to the plot function.</li>
</ul>
<p>Using the pipelines, different plots can be obtained by superseding the <code>modifying_function</code> with function names.
Four examples of additional graphical functions available in the <code>fairmodels</code> can be seen in Figure <a href="#fig:all">7</a>. This package implements a total of 8 different diagnostic plots, each describing a different fairness perspective. To see different aspects of fairness and bias, the user can choose the model with the smallest bias, find out the similarity between metrics and models, compare models in both fairness and performance, and see how cutoff manipulation might change the <code>parity_loss</code>. Find more information about each of them in the documentation.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>fp1</span>  <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/graphics/plot.default.html'>plot</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/pkg/fairmodels/man/ceteris_paribus_cutoff.html'>ceteris_paribus_cutoff</a></span><span class='op'>(</span><span class='va'>fobject</span>, <span class='st'>"male"</span>, cumulated<span class='op'>=</span><span class='cn'>TRUE</span><span class='op'>)</span><span class='op'>)</span>
<span class='va'>fp2</span>  <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/graphics/plot.default.html'>plot</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/pkg/fairmodels/man/fairness_heatmap.html'>fairness_heatmap</a></span><span class='op'>(</span><span class='va'>fobject</span><span class='op'>)</span><span class='op'>)</span>
<span class='va'>fp3</span>  <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/graphics/plot.default.html'>plot</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/pkg/fairmodels/man/plot_stacked_barplot.html'>stack_metrics</a></span><span class='op'>(</span><span class='va'>fobject</span><span class='op'>)</span><span class='op'>)</span>
<span class='va'>fp4</span>  <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/graphics/plot.default.html'>plot</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/pkg/fairmodels/man/plot_density.html'>plot_density</a></span><span class='op'>(</span><span class='va'>fobject</span><span class='op'>)</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='st'><a href='https://patchwork.data-imaginist.com'>"patchwork"</a></span><span class='op'>)</span>
<span class='va'>fp1</span> <span class='op'>+</span> <span class='va'>fp2</span> <span class='op'>+</span> <span class='va'>fp3</span> <span class='op'>+</span> <span class='va'>fp4</span> <span class='op'>+</span> 
  <span class='fu'><a href='https://patchwork.data-imaginist.com/reference/plot_layout.html'>plot_layout</a></span><span class='op'>(</span>ncol <span class='op'>=</span> <span class='fl'>2</span><span class='op'>)</span>
</code></pre>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:all"></span>
<img src="RJ-2022-019_files/figure-html5/all-1.png" alt="Four examples of additional graphical functions are available in the fairmodels package that facilitates model and bias exploration. The Ceteris Paribus Cuttoff plot helps select the cutoff values for each model to maximize a particular measure of fairness. In this case, the suggested cutoff point for both linear models is similar. However, the ranger model does not have calibrated probabilities and thus requires a different cutoff. The Heatmap plot is very helpful when comparing large numbers of models. It shows profiles of selected fairness measures for each of the models under consideration. In this case, the fairness profiles for both linear models are similar. The Stacked Metric plot helps you compare models by summing five different fairness measures. The different layers of this plot allow you to compare individual measures, but if you don't know which one to focus on, it is useful to look at the sum of the measures. In this case, the ranger model has the highest fairness values. Finally, the Density plot helps to compare the score distributions of the models between the advantaged and disadvantaged groups. In this case, we find that for females the distributions of the scores are lower in all models, with the largest difference for the lm model. " width="100%" />
<p class="caption">
Figure 7: Four examples of additional graphical functions are available in the fairmodels package that facilitates model and bias exploration. The Ceteris Paribus Cuttoff plot helps select the cutoff values for each model to maximize a particular measure of fairness. In this case, the suggested cutoff point for both linear models is similar. However, the ranger model does not have calibrated probabilities and thus requires a different cutoff. The Heatmap plot is very helpful when comparing large numbers of models. It shows profiles of selected fairness measures for each of the models under consideration. In this case, the fairness profiles for both linear models are similar. The Stacked Metric plot helps you compare models by summing five different fairness measures. The different layers of this plot allow you to compare individual measures, but if you don’t know which one to focus on, it is useful to look at the sum of the measures. In this case, the ranger model has the highest fairness values. Finally, the Density plot helps to compare the score distributions of the models between the advantaged and disadvantaged groups. In this case, we find that for females the distributions of the scores are lower in all models, with the largest difference for the lm model.
</p>
</div>
</div>
<h1 id="mitigation">Bias mitigation</h1>
<p>What can be done if the model does not meet the fairness criteria? Machine learning practitioners might use other algorithms or variables to construct unbiased models, but this does not guarantee passing the <code>fairness_check()</code>. An alternative is to use bias mitigation techniques that adjust the data or model to meet fairness conditions.
There are essentially three types of such methods. The first is data pre-processing. There are many ways to “correct” the data when there are unwanted correlations between variables or sample sizes among subgroups in data. The second one is in-processing, which is, for example, optimizing classifiers not only to reduce classification error but also to minimize a fairness metric. Last but not least is post-processing which modifies model output so that predictions and miss-predictions among subgroups are more alike.</p>
<p>The <a href="#">fairmodels</a> package offers five functions for bias mitigation, three for pre-processing, and two for post-processing. Most of these approaches are also implemented in <span class="citation" data-cites="aif360-oct-2018">(<a href="#ref-aif360-oct-2018" role="doc-biblioref">Bellamy et al. 2018</a>)</span>. However, in <a href="#">fairmodels</a> there are separate implementations of them in R. There are a lot of useful mitigation techniques that are not in <a href="#">fairmodels</a> like those in <span class="citation" data-cites="NIPS20166374">Hardt et al. (<a href="#ref-NIPS20166374" role="doc-biblioref">2016</a>)</span> and numerous in-processing algorithms.</p>
<h2 id="data-pre-processing">Data pre-processing</h2>
<ul>
<li><strong>Disparate impact remover</strong><br />
In <a href="#">fairmodels</a> geometric repair, an algorithm originally introduced by <span class="citation" data-cites="disparateImpact">Feldman et al. (<a href="#ref-disparateImpact" role="doc-biblioref">2015</a>)</span>, works on ordinal, numeric features. Depending on the <span class="math inline">\(\lambda \in [0,1]\)</span> parameter, this method will transform the distribution of a given feature. The idea is simple. Given feature distribution in different subgroups, the algorithm finds optimal distribution (according to earth mover’s distance) and transforms distribution for each subgroup to match the optimal one. For example, if age is an important feature and its distribution is different in two subgroups, and we want to change that, then the geometric repair will map each individual’s age to a new distribution (different age). It will be preserving the order - the ranks (in our case, seniority) of observations are preserved. Parameter <span class="math inline">\(\lambda\)</span> is responsible for the repair degree, so for full repair, lambda should be set to 1. The method does not focus on a particular metric but rather tries to level out them by transforming potentially harmful feature distributions.</li>
<li><strong>Reweighting</strong><br />
Reweighting is a rather straightforward approach. This method was implemented according to <span class="citation" data-cites="kamiran">Kamiran and Calders (<a href="#ref-kamiran" role="doc-biblioref">2011</a>)</span>. It computes weights by dividing the theoretical probability of assigning favorable labels for a subgroup by real (observed) probability (based on the data). Theoretic probability for a subgroup is computed by multiplying the probability of assigning a favorable label (for all populations) by picking observation from a certain subgroup. It focuses on mitigating statistical parity.</li>
<li><strong>Resampling</strong><br />
Resampling is based on weights calculated in <code>reweighting</code>. Each weight for a subgroup is multiplied by the size of the subgroup. Then, whether the subgroup is deprived or not (if weight is higher than one, the subgroup is considered deprived), observations are duplicated from either one that were assigned a favorable label or not. There are two types of resampling- uniform and preferential. The uniform is making algorithm pick or omit observations randomly without considering its probabilistic score. Preferential uses another probabilistic classifier, potentially different from the main model for final predictions. In <span class="citation" data-cites="kamiran">Kamiran and Calders (<a href="#ref-kamiran" role="doc-biblioref">2011</a>)</span> it is called ranker - it predicts the probabilities for the observations to decide which observations are close to the cutoff border (usually 0.5). Based on the probabilistic output of the ranker, the observations are sorted, and the ones with the highest/lowest ranks are either left out or duplicated depending on the case—more on that on <span class="citation" data-cites="kamiran">Kamiran and Calders (<a href="#ref-kamiran" role="doc-biblioref">2011</a>)</span>. The <a href="#">fairmodels</a> implementation, instead of training the ranker as in the aforementioned paper, uses a vector of previously calculated probabilities provided by the user. With this, it shifts the decision and responsibility of choosing a ranker to the user. It focuses on mitigating statistical parity.</li>
</ul>
<h2 id="model-post-processing">Model post-processing</h2>
<ul>
<li><strong>Reject Option based Classification Pivot</strong><br />
The <code>roc_pivot</code> method is implemented based on <span class="citation" data-cites="postKamiran">Kamiran et al. (<a href="#ref-postKamiran" role="doc-biblioref">2012</a>)</span> in the <a href="#">fairmodels</a> package. Let <span class="math inline">\(\theta \in (0,1)\)</span> be the value that determines the radius of the so-called critical region, which is an area around the cutoff. The user specifies the <span class="math inline">\(\theta\)</span>, and it should describe how big the critical region should be. For example if <span class="math inline">\(\theta = 0.1\)</span> and cutoff is 0.6, then the critical region will be (0.5, 0.7). Let’s assume that we are predicting a favorable outcome. If the assigned probability of observation is in the described region, then the probabilities are pivoting on the other side of the cutoff with a certain assumption. If an observation in a critical region is considered to be the privileged and it is on the right side of the cutoff, then its probabilities are pivoting from the right side of the cutoff to the left. So if an observation is in the critical region and it is considered unprivileged, then if it is on the left side of the cutoff, it will pivot to the right side. Pivoting here means changing the side of the cutoff so that the distance from the cutoff stays unchanged. It does not intend to mitigate a single metric but rather changes predictions in the critical region (the region with low certainty). By pivoting the predictions, it might lower more metrics.<br />
</li>
<li><strong>Cutoff manipulation</strong><br />
The <a href="#">fairmodels</a> package supports setting cutoff for each subgroup. Users may pick <code>parity_loss</code> metrics of their choice and find the minimal <code>parity_loss</code>. It is part of <code>ceteris_paribus_cutoff()</code> function. Based on picked metrics, the sum of parity loss is calculated for each cutoff of the chosen subgroup. Then the minimal value is found—this way, optimal values might be found for metrics of interest. The minimum is marked with a dashed vertical line (see Figure <a href="#fig:all">7</a>). This approach however might be to some extent concerning. Some might argue that setting different cutoffs for different subgroups is unfair and is punishing privileged subgroups for something they have no control of. Especially in the individual fairness field, it would be concerning if two similar people with different sensitive attributes would have two different thresholds and potentially two different outcomes. This is a valid point, and this method should be used with knowledge of all its drawbacks. The cutoff manipulation method targets metrics chosen by the user.</li>
</ul>
<p>All pre-processing methods can be used with two pipelines, whereas post-processing can be used in one specific way.</p>
<ul>
<li>Pre-processing pipelines
<ul>
<li><code>data/explainer |&gt; method</code><br />
Returns either weights, indexes, or changed data depending on the method used.</li>
<li><code>data/explainer |&gt; pre_process_data(data, protected, y, type = ...)</code><br />
Always returns data.frame. In case of weights data has additional column called <code>_weights_</code>.</li>
</ul></li>
<li>Post-processing pipelines
<ul>
<li><code>fairness_object |&gt; ceteris_paribus_cutoff(subgroup, ...) |&gt; print/plot</code><br />
This is the pipeline for creating ceteris paribus cutoff print and plot.</li>
<li><code>explainer |&gt; roc_pivot(protected, privileged, ...)</code><br />
The pipeline will return explainer with <code>y_hat</code> field changed.</li>
</ul></li>
</ul>
<p>The user should be aware that debiasing one metric might enhance bias in another. It is a so-called fairness-fairness trade-off. There is also a fairness-performance trade-off where debiasing one metric leads to worse performance. Another thing to remember is, as found in <span class="citation" data-cites="Agrawal2020DebiasingCI">Agrawal et al. (<a href="#ref-Agrawal2020DebiasingCI" role="doc-biblioref">2020</a>)</span>, metrics might not generalize well to out-of-distribution examples, so it is advised also to check the fairness metrics on a separate test set.</p>
<h2 id="example-1">Example</h2>
<p>Now we will show an example usage of one pre-processing and one post-processing method. As before, the <strong>German Credit Data</strong> will be used along with the previously created <code>lm_model</code>. So firstly, we create a new dataset using <code>pre_process_data</code> and then we use it to train the logistic regression classifier.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>resampled_german</span>   <span class='op'>&lt;-</span> <span class='va'>german</span> |&gt; <span class='fu'><a href='https://rdrr.io/pkg/fairmodels/man/pre_process_data.html'>pre_process_data</a></span><span class='op'>(</span>protected <span class='op'>=</span> <span class='va'>german</span><span class='op'>$</span><span class='va'>Sex</span>,
                <span class='va'>y_numeric</span>, type <span class='op'>=</span> <span class='st'>'resample_uniform'</span><span class='op'>)</span>

<span class='va'>lm_model_resample</span>  <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/r/stats/glm.html'>glm</a></span><span class='op'>(</span><span class='va'>Risk</span><span class='op'>~</span><span class='va'>.</span>,
                data   <span class='op'>=</span> <span class='va'>resampled_german</span>,
                family <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/stats/family.html'>binomial</a></span><span class='op'>(</span>link <span class='op'>=</span> <span class='st'>"logit"</span><span class='op'>)</span><span class='op'>)</span>

<span class='va'>explainer_lm_resample</span> <span class='op'>&lt;-</span> <span class='fu'>DALEX</span><span class='fu'>::</span><span class='fu'><a href='https://modeloriented.github.io/DALEX/reference/explain.html'>explain</a></span><span class='op'>(</span><span class='va'>lm_model_resample</span>,
                data <span class='op'>=</span> <span class='va'>german</span><span class='op'>[</span>,<span class='op'>-</span><span class='fl'>1</span><span class='op'>]</span>, y <span class='op'>=</span> <span class='va'>y_numeric</span>, verbose <span class='op'>=</span> <span class='cn'>FALSE</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>Then we make other explainers. We use previously created <code>explainer_lm</code> with the post-processing function <code>roc_pivot</code>. We set parameter <code>theta = 0.05</code> for a rather narrow area of a pivot.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>new_explainer</span> <span class='op'>&lt;-</span> <span class='va'>explainer_lm</span> |&gt; <span class='fu'><a href='https://rdrr.io/pkg/fairmodels/man/roc_pivot.html'>roc_pivot</a></span><span class='op'>(</span>protected <span class='op'>=</span> <span class='va'>german</span><span class='op'>$</span><span class='va'>Sex</span>,
                privileged <span class='op'>=</span> <span class='st'>"male"</span>, theta <span class='op'>=</span> <span class='fl'>0.05</span><span class='op'>)</span>
</code></pre>
</div>
</div>
<p>In the end, we create <code>fairness_object</code> with explainers obtained with the code above and one created in the first example to see the difference.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span class='va'>fobject</span> <span class='op'>&lt;-</span> <span class='fu'><a href='https://rdrr.io/pkg/fairmodels/man/fairness_check.html'>fairness_check</a></span><span class='op'>(</span><span class='va'>explainer_lm_resample</span>, <span class='va'>new_explainer</span>, <span class='va'>explainer_lm</span>,
                protected <span class='op'>=</span> <span class='va'>german</span><span class='op'>$</span><span class='va'>Sex</span>, privileged <span class='op'>=</span> <span class='st'>"male"</span>,
                label <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='st'>"resample"</span>, <span class='st'>"roc"</span>, <span class='st'>"base"</span><span class='op'>)</span>,
                verbose <span class='op'>=</span> <span class='cn'>FALSE</span><span class='op'>)</span>

<span class='va'>fobject</span> |&gt; <span class='fu'><a href='https://rdrr.io/r/graphics/plot.default.html'>plot</a></span><span class='op'>(</span><span class='op'>)</span>
</code></pre>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mitigation"></span>
<img src="RJ-2022-019_files/figure-html5/mitigation-1.png" alt="Graphical summary of a base model (blue bars) and model after applying two bias mitigation techniques (red and green bars). By comparing adjacent rectangles one can read how the respective technique affected the corresponding fairness measure" width="80%" />
<p class="caption">
Figure 8: Graphical summary of a base model (blue bars) and model after applying two bias mitigation techniques (red and green bars). By comparing adjacent rectangles one can read how the respective technique affected the corresponding fairness measure
</p>
</div>
</div>
<p>The result of the code above is presented in Figure <a href="#fig:mitigation">8</a>. The mitigation methods successfully eliminated bias in all of the metrics. Both models are better than the original <code>base</code>. This is not always the case - sometimes, eliminating bias in one metric may increase bias in another metric. For example, let’s consider a perfectly accurate model, but some subgroups receive few positive predictions (bias in Statistical parity). In that case, mitigating the bias in Statistical parity would decrease the Accuracy equality ratio.</p>
<h1 id="summary-and-future-work">Summary and future work</h1>
<p>This paper showed that checking for bias in machine learning models can be done conveniently and flexibly. The package <a href="#">fairmodels</a> described above is a self-sufficient tool for bias detection, visualization, and mitigation in classification machine learning models. We presented theory, package architecture, suggested usage, and examples along with plots. Along the way, we introduced the core concepts and assumptions that come along the bias detection and plot interpretation. The package is still improved and enhanced, which can be seen by adding the announced regression module based on <span class="citation" data-cites="regression">Steinberg et al. (<a href="#ref-regression" role="doc-biblioref">2020</a>)</span>. We did not cover it in this article because it is still an experimental tool. Another tool for in-processing classification closely related to <a href="#">fairmodels</a> has also been added and can be found on <a href="https://github.com/ModelOriented/FairPAN" class="uri">https://github.com/ModelOriented/FairPAN</a>.</p>
<p>The source code of the package, vignettes, examples, and documentation can be found at <a href="https://modeloriented.github.io/fairmodels/" class="uri">https://modeloriented.github.io/fairmodels/</a>. The stable version is available on CRAN. The code and the development version can be found on GitHub <a href="https://github.com/ModelOriented/fairmodels" class="uri">https://github.com/ModelOriented/fairmodels</a>. This is also a place to report bugs or requests (through GitHub issues).</p>
<p>In the future, we plan to enhance the spectrum of bias visualization plots and introduce regression and individual fairness methods. The potential way to explore would be an in-processing bias mitigation - training models that minimize cost function and adhere to certain fairness criteria. This field is heavily developed in Python and lacks appropriate attention in R.</p>
<h1 class="unnumbered" id="acknowledgements">Acknowledgements</h1>
<p>Work on this package was financially supported by the NCN Sonata Bis-9 grant 2019/34/E/ST6/00052.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<h2 class="appendix" id="supplementary-materials">Supplementary materials</h2>
<p>Supplementary materials are available in addition to this article. It can be downloaded at
<a href="RJ-2022-019.zip">RJ-2022-019.zip</a></p>
<h2 class="appendix" id="cran-packages-used">CRAN packages used</h2>
<p><a href="https://cran.r-project.org/package=fairmodels">fairmodels</a>, <a href="https://cran.r-project.org/package=h2o">h2o</a>, <a href="https://cran.r-project.org/package=fairness">fairness</a>, <a href="https://cran.r-project.org/package=fairadapt">fairadapt</a>, <a href="https://cran.r-project.org/package=DALEX">DALEX</a>, <a href="https://cran.r-project.org/package=ranger">ranger</a></p>
<h2 class="appendix" id="cran-task-views-implied-by-cited-packages">CRAN Task Views implied by cited packages</h2>
<p><a href="https://cran.r-project.org/view=HighPerformanceComputing">HighPerformanceComputing</a>, <a href="https://cran.r-project.org/view=MachineLearning">MachineLearning</a>, <a href="https://cran.r-project.org/view=ModelDeployment">ModelDeployment</a>, <a href="https://cran.r-project.org/view=Survival">Survival</a>, <a href="https://cran.r-project.org/view=TeachingStatistics">TeachingStatistics</a></p>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Agrawal2020DebiasingCI" class="csl-entry" role="doc-biblioentry">
A. Agrawal, F. Pfisterer, B. Bischl, J. Chen, S. Sood, S. Shah, F. Buet-Golfouse, B. A. Mateen and S. Vollmer. Debiasing classifiers: Is reality at variance with expectation? <em>Electronic</em>, 2020.
</div>
<div id="ref-propublica" class="csl-entry" role="doc-biblioentry">
J. Angwin, J. Larson, S. Mattu and and Lauren Kirchner. <span class="nocase">Machine bias: There’s software used across the country to predict future criminals. And it’s biased against blacks</span>. <em>ProPublica</em>, 2016. URL <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a>.
</div>
<div id="ref-barocas-hardt-narayanan" class="csl-entry" role="doc-biblioentry">
S. Barocas, M. Hardt and A. Narayanan. <em>Fairness and machine learning.</em> fairmlbook.org, 2019. <a href="http://www.fairmlbook.org" class="uri">http://www.fairmlbook.org</a>.
</div>
<div id="ref-aif360-oct-2018" class="csl-entry" role="doc-biblioentry">
R. K. E. Bellamy, K. Dey, M. Hind, S. C. Hoffman, S. Houde, K. Kannan, P. Lohia, J. Martino, S. Mehta, A. Mojsilovic, et al. <span>AI Fairness</span> 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. 2018. URL <a href="https://arxiv.org/abs/1810.01943">https://arxiv.org/abs/1810.01943</a>.
</div>
<div id="ref-accuracy" class="csl-entry" role="doc-biblioentry">
R. Berk, H. Heidari, S. Jabbari, M. Kearns and A. Roth. Fairness in criminal justice risk assessments: The state of the art. <em>Sociological Methods &amp; Research</em>, 2017. URL <a href="https://doi.org/10.1177/0049124118782533">https://doi.org/10.1177/0049124118782533</a>.
</div>
<div id="ref-JMLRv19" class="csl-entry" role="doc-biblioentry">
P. Biecek. <span class="nocase">DALEX: Explainers for Complex Predictive Models in R</span>. <em>Journal of Machine Learning Research</em>, 19(84): 1–5, 2018. URL <a href="http://jmlr.org/papers/v19/18-416.html">http://jmlr.org/papers/v19/18-416.html</a>.
</div>
<div id="ref-ema2021" class="csl-entry" role="doc-biblioentry">
P. Biecek and T. Burzykowski. <em><span>Explanatory Model Analysis</span>.</em> Chapman; Hall/CRC, New York, 2021. URL <a href="https://pbiecek.github.io/ema/">https://pbiecek.github.io/ema/</a>.
</div>
<div id="ref-reuben" class="csl-entry" role="doc-biblioentry">
R. Binns. On the apparent conflict between individual and group fairness. In <em>Proceedings of the 2020 conference on fairness, accountability, and transparency</em>, pages. 514–524 2020. New York, NY, USA: Association for Computing Machinery. ISBN 9781450369367. URL <a href="https://doi.org/10.1145/3351095.3372864">https://doi.org/10.1145/3351095.3372864</a>.
</div>
<div id="ref-bird2020fairlearn" class="csl-entry" role="doc-biblioentry">
S. Bird, M. Dudík, R. Edgar, B. Horn, R. Lutz, V. Milan, M. Sameki, H. Wallach and K. Walker. Fairlearn: A toolkit for assessing and improving fairness in <span>AI</span>. MSR-TR-2020-32. Microsoft. 2020. URL <a href="https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/">https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/</a>.
</div>
<div id="ref-pmlr-v81-buolamwini18a" class="csl-entry" role="doc-biblioentry">
J. Buolamwini and T. Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In <em>Proceedings of the 1st conference on fairness, accountability and transparency</em>, Eds S. A. Friedler and C. Wilson pages. 77–91 2018. New York, NY, USA. URL <a href="http://proceedings.mlr.press/v81/buolamwini18a.html">http://proceedings.mlr.press/v81/buolamwini18a.html</a>.
</div>
<div id="ref-ppv" class="csl-entry" role="doc-biblioentry">
A. Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. <em>Big Data</em>, 5: 2016. URL <a href="https://doi.org/10.1089/big.2016.0047">https://doi.org/10.1089/big.2016.0047</a>.
</div>
<div id="ref-cirillo_sex_2020" class="csl-entry" role="doc-biblioentry">
D. Cirillo, S. Catuara-Solarz, C. Morey, E. Guney, L. Subirats, S. Mellino, A. Gigante, A. Valencia, M. J. Rementeria, A. S. Chadha, et al. Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare. <em>npj Digital Medicine</em>, 3(1): 81, 2020. URL <a href="https://doi.org/10.1038/s41746-020-0288-5">https://doi.org/10.1038/s41746-020-0288-5</a> [online; last accessed October 24, 2021].
</div>
<div id="ref-adverseimpact" class="csl-entry" role="doc-biblioentry">
Code of Federal Regulations. SECTION 4D, UNIFORM GUIDELINES ON EMPLOYEE SELECTION PROCEDURES (1978). 1978. URL <a href="https://www.govinfo.gov/content/pkg/CFR-2014-title29-vol4/xml/CFR-2014-title29-vol4-part1607.xml">https://www.govinfo.gov/content/pkg/CFR-2014-title29-vol4/xml/CFR-2014-title29-vol4-part1607.xml</a>.
</div>
<div id="ref-ppe" class="csl-entry" role="doc-biblioentry">
S. Corbett-Davies, E. Pierson, A. Feller, S. Goel and A. Huq. Algorithmic decision making and the cost of fairness. In <em>Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</em>, pages. 797–806 2017. New York, NY, USA: Association for Computing Machinery. URL <a href="https://doi.org/10.1145/3097983.3098095">https://doi.org/10.1145/3097983.3098095</a>.
</div>
<div id="ref-facialrecognition" class="csl-entry" role="doc-biblioentry">
Council of Europe. Guidelines on facial recognition. 2021. URL <a href="https://www.coe.int/en/web/portal/-/facial-recognition-strict-regulation-is-needed-to-prevent-human-rights-violations-">https://www.coe.int/en/web/portal/-/facial-recognition-strict-regulation-is-needed-to-prevent-human-rights-violations-</a>.
</div>
<div id="ref-Dua2019" class="csl-entry" role="doc-biblioentry">
D. Dua and C. Graff. <span>UCI</span> machine learning repository. 2017. URL <a href="https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)">https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)</a>.
</div>
<div id="ref-statisticalparity" class="csl-entry" role="doc-biblioentry">
C. Dwork, M. Hardt, T. Pitassi, O. Reingold and R. Zemel. Fairness through awareness. In <em>Proceedings of the 3rd innovations in theoretical computer science conference</em>, pages. 214–226 2012. New York, NY, USA: Association for Computing Machinery. URL <a href="https://doi.org/10.1145/2090236.2090255">https://doi.org/10.1145/2090236.2090255</a>.
</div>
<div id="ref-European-non-discrimination" class="csl-entry" role="doc-biblioentry">
European Union Agency for Fundamental Rights and Council of Europe. <em>Handbook on european non-discrimination law.</em> Luxembourg: Publications Office of the European Union, 2018. <a href="https://fra.europa.eu/en/publication/2018/handbook-european-non-discrimination-law-2018-edition" class="uri">https://fra.europa.eu/en/publication/2018/handbook-european-non-discrimination-law-2018-edition</a>.
</div>
<div id="ref-EUhandbook" class="csl-entry" role="doc-biblioentry">
European Union Agency for Fundamental Rights. Handbook on european non-discrimination law. 2018. DOI <a href="https://doi.org/10.2811/792676">https://doi.org/10.2811/792676</a>.
</div>
<div id="ref-disparateImpact" class="csl-entry" role="doc-biblioentry">
M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger and S. Venkatasubramanian. Certifying and removing disparate impact. In <em>Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</em>, pages. 259–268 2015. New York, NY, USA: Association for Computing Machinery. URL <a href="https://doi.org/10.1145/2783258.2783311">https://doi.org/10.1145/2783258.2783311</a>.
</div>
<div id="ref-H2OAutoML" class="csl-entry" role="doc-biblioentry">
H2O.ai. <em>H2O AutoML.</em> 2017. URL <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html">http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html</a>. H2O version 3.30.0.1.
</div>
<div id="ref-NIPS20166374" class="csl-entry" role="doc-biblioentry">
M. Hardt, E. Price, E. Price and N. Srebro. Equality of opportunity in supervised learning. In <em>Advances in neural information processing systems 29</em>, Eds D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon and R. Garnett pages. 3315–3323 2016. Curran Associates, Inc. URL <a href="http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf">http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf</a>.
</div>
<div id="ref-kamiran" class="csl-entry" role="doc-biblioentry">
F. Kamiran and T. Calders. Data pre-processing techniques for classification without discrimination. <em>Knowledge and Information Systems</em>, 33: 2011. URL <a href="https://doi.org/10.1007/s10115-011-0463-8">https://doi.org/10.1007/s10115-011-0463-8</a>.
</div>
<div id="ref-postKamiran" class="csl-entry" role="doc-biblioentry">
F. Kamiran, A. Karim and X. Zhang. Decision theory for discrimination-aware classification. In <em>2012 IEEE 12th international conference on data mining</em>, pages. 924–929 2012. URL <a href="https://doi.org/10.1109/ICDM.2012.45">https://doi.org/10.1109/ICDM.2012.45</a>.
</div>
<div id="ref-KOZODOI2021" class="csl-entry" role="doc-biblioentry">
N. Kozodoi, J. Jacob and S. Lessmann. Fairness in credit scoring: Assessment, implementation and profit implications. <em>European Journal of Operational Research</em>, 2021. URL <a href="https://doi.org/10.1016/j.ejor.2021.06.023">https://doi.org/10.1016/j.ejor.2021.06.023</a>.
</div>
<div id="ref-fairness" class="csl-entry" role="doc-biblioentry">
N. Kozodoi and T. V. Varga. <em>Fairness: Algorithmic fairness metrics.</em> 2021. URL <a href="https://CRAN.R-project.org/package=fairness">https://CRAN.R-project.org/package=fairness</a>. R package version 1.2.1.
</div>
<div id="ref-8731591" class="csl-entry" role="doc-biblioentry">
P. Lahoti, K. P. Gummadi and G. Weikum. [iFair: Learning individually fair data representations for algorithmic decision making]. In <em>2019 IEEE 35th international conference on data engineering (ICDE)</em>, pages. 1334–1345 2019. URL <a href="https://doi.org/10.1109/ICDE.2019.00121">https://doi.org/10.1109/ICDE.2019.00121</a>.
</div>
<div id="ref-mehrabi2019survey" class="csl-entry" role="doc-biblioentry">
N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman and A. Galstyan. A survey on bias and fairness in machine learning. 2019. URL <a href="https://arxiv.org/abs/1908.09635">https://arxiv.org/abs/1908.09635</a>.
</div>
<div id="ref-plecko2019fair" class="csl-entry" role="doc-biblioentry">
D. Plečko and N. Meinshausen. Fair data adaptation with quantile preservation. 2019. URL <a href="https://arxiv.org/abs/1911.06685">https://arxiv.org/abs/1911.06685</a>.
</div>
<div id="ref-2018aequitas" class="csl-entry" role="doc-biblioentry">
P. Saleiro, B. Kuester, A. Stevens, A. Anisfeld, L. Hinkson, J. London and R. Ghani. Aequitas: A bias and fairness audit toolkit. 2018. URL <a href="https://arxiv.org/abs/1811.05577">https://arxiv.org/abs/1811.05577</a>.
</div>
<div id="ref-regression" class="csl-entry" role="doc-biblioentry">
D. C. Steinberg, A. Reid and S. T. O’Callaghan. Fairness measures for regression via probabilistic classification. <em>ArXiv</em>, abs/2001.06089: 2020.
</div>
<div id="ref-labelwrong" class="csl-entry" role="doc-biblioentry">
M. Wick, S. Panda and J.-B. Tristan. Unlocking fairness: A trade-off revisited. In <em>Advances in neural information processing systems</em>, Eds H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox and R. Garnett pages. 8783–8792 2019. Curran Associates, Inc. URL <a href="https://proceedings.neurips.cc/paper/2019/file/373e4c5d8edfa8b74fd4b6791d0cf6dc-Paper.pdf">https://proceedings.neurips.cc/paper/2019/file/373e4c5d8edfa8b74fd4b6791d0cf6dc-Paper.pdf</a>.
</div>
<div id="ref-ranger" class="csl-entry" role="doc-biblioentry">
M. N. Wright and A. Ziegler. <span class="nocase">ranger</span>: A fast implementation of random forests for high dimensional data in <span>C++</span> and <span>R</span>. <em>Journal of Statistical Software</em>, 77(1): 1–17, 2017. DOI <a href="https://doi.org/10.18637/jss.v077.i01">10.18637/jss.v077.i01</a>.
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
  <h3 id="references">References</h3>
  <div id="references-listing"></div>
  <h3 id="reuse">Reuse</h3>
  <p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
  <h3 id="citation">Citation</h3>
  <p>For attribution, please cite this work as</p>
  <pre class="citation-appendix short">Wiśniewski &amp; Biecek, "The R Journal: fairmodels: a Flexible Tool for Bias Detection, Visualization, and Mitigation in Binary Classification Models", The R Journal, 2022</pre>
  <p>BibTeX citation</p>
  <pre class="citation-appendix long">@article{RJ-2022-019,
  author = {Wiśniewski, Jakub and Biecek, Przemysław},
  title = {The R Journal: fairmodels: a Flexible Tool for Bias Detection, Visualization, and Mitigation in Binary Classification Models},
  journal = {The R Journal},
  year = {2022},
  note = {https://doi.org/10.32614/RJ-2022-019},
  doi = {10.32614/RJ-2022-019},
  volume = {14},
  issue = {1},
  issn = {2073-4859},
  pages = {227-243}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<!--radix_placeholder_navigation_after_body--><html><body>
<div class="distill-site-nav distill-site-footer">
<p>© The R Foundation, <a href="mailto:r-journal@r-project.org">web page
contact</a>.</p>
</div>
<!--/radix_placeholder_navigation_after_body-->
</body></html>


</body>

</html>
