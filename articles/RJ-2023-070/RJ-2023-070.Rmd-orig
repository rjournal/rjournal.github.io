---
title: 'fnets: An R Package for Network Estimation and Forecasting via Factor-Adjusted
  VAR Modelling'
abstract: ' Vector autoregressive (VAR) models are useful for modelling high-dimensional
  time series data. This paper introduces the package [fnets](https://CRAN.R-project.org/package=fnets),
  which implements the suite of methodologies proposed by [@barigozzi2022fnets] for
  the network estimation and forecasting of high-dimensional time series under a factor-adjusted
  vector autoregressive model, which permits strong spatial and temporal correlations
  in the data. Additionally, we provide tools for visualising the networks underlying
  the time series data after adjusting for the presence of factors. The package also
  offers data-driven methods for selecting tuning parameters including the number
  of factors, the order of autoregression, and thresholds for estimating the edge
  sets of the networks of interest in time series analysis. We demonstrate various
  features of fnets on simulated datasets as well as real data on electricity prices. '
author:
- name: Dom Owens
  affiliation: School of Mathematics, University of Bristol
  email: |
    domowens1@gmail.com
  address: Supported by EPSRC Centre for Doctoral Training (EP/S023569/1)
- name: Haeran Cho
  affiliation: School of Mathematics, University of Bristol
  email: |
    haeran.cho@bristol.ac.uk
  address: Supported by the Leverhulme Trust (RPG-2019-390)
- name: Matteo Barigozzi
  affiliation: Department of Economics, Università di Bologna
  email: |
    matteo.barigozzi@unibo.it
  address: Supported by MIUR (PRIN 2017, Grant 2017TA7TYC)
date: '2023-12-18'
date_received: '2023-02-07'
journal:
  firstpage: ~
  lastpage: ~
volume: 15
issue: 3
slug: RJ-2023-070
packages:
  cran: ~
  bioc: ~
draft: no
preview: preview.png
bibliography: fnets.bib
CTV: ~
output:
  rjtools::rjournal_web_article:
    self_contained: no
    toc: no
    legacy_pdf: yes

---

# Introduction

Vector autoregressive (VAR) models have been popularly adopted for
modelling time series data across many disciplines including economics
[@koop2013forecasting], finance [@barigozzi2019nets], neuroscience
[@kirch2015eeg], and systems biology [@shojaie2010discovering]. By
fitting VAR models to data, we can infer dynamic interdependence between
the variables and forecast future values. In particular, by inferring
the non-zero elements of the VAR parameter matrices, we can find a
network representation of the data which embeds Granger causal linkages.
Besides, by estimating the precision matrix (inverse of the covariance
matrix) of the VAR innovations, we can define a network representing
their contemporaneous dependencies by means of partial correlations.
Finally, the inverse of the long-run covariance matrix of the data
simultaneously captures lead-lag and contemporaneous co-movements of the
variables. For further discussions on the network interpretation of VAR
modelling, we refer to [@dahlhaus2000graphical], [@eichler2007granger],
[@billio2012econometric] and [@barigozzi2019nets].

Fitting VAR models to the data can quickly become a high-dimensional
problems since the number of parameters grows quadratically with the
dimensionality of the data. There exists a mature literature on
$\ell_1$-regularisation methods for estimating VAR models in high
dimensions under suitable sparsity assumptions on the parameters
[@basu2015regularized; @han2015direct; @kock2015oracle; @medeiros2016; @nicholson2020high; @liu2021robust].
Consistency of such methods is derived under the assumption that the
spectral density matrix of the data has bounded eigenvalues. However, in
many applications, the datasets exhibit strong serial and
cross-sectional correlations which leads to the violation of this
assumption. As a motivating example, we introduce a dataset of
node-specific prices in the PJM (Pennsylvania, New Jersey and Maryland)
power pool area in the United States, see [Energy price
data](#sec:real:energy) for further details.
Figure [1](#fig:eigen){reference-type="ref" reference="fig:eigen"}
demonstrates that the leading eigenvalue of the long-run covariance
matrix (i.e. spectral density matrix at frequency $0$) increases
linearly as the dimension of the data increases, which implies the
presence of latent common factors in the panel data
[@forni2000generalized]. Additionally, the left panel of
Figure [2](#fig:q0q1){reference-type="ref" reference="fig:q0q1"} shows
the inadequacy of fitting a VAR model to such data under the sparsity
assumption via $\ell_1$-regularisation methods, unless the presence of
strong correlations is accounted for by a *factor-adjustment* step as in
the right panel.

![Box plots of the two largest eigenvalues ($y$-axis) of the long-run
covariance matrix estimated from the energy price data collected between
01/01/2021 and 19/07/2021 ($n = 200$), see [Real data
example](#sec:real) for further details. Cross-sections of the data are
randomly sampled $100$ times for each given dimension
$p \in \{2, \dots, 50\}$ ($x$-axis) to produce the box
plots.](figs/eigplot.pdf){#fig:eigen width=".6\\textwidth"}

<figure id="fig:q0q1">
<p><embed src="figs/q0.pdf" /> <embed src="figs/q1.pdf" /></p>
<figcaption>Granger causal networks defined in <a href="#eq:net:dir"
data-reference-type="eqref" data-reference="eq:net:dir">[eq:net:dir]</a>
obtained from fitting a VAR(<span class="math inline">1</span>) model to
the energy price data analysed in Figure <a href="#fig:eigen"
data-reference-type="ref" data-reference="fig:eigen">1</a>, without
(left) and with (right) the factor adjustment step outlined in <a
href="#sec:estimation">FNETS: Network estimation</a>. Edge weights
(proportional to the size of coefficient estimates) are visualised by
the width of each edge, and the nodes are coloured according to their
groupings, see <a href="#sec:real">Real data example</a> for further
details.</figcaption>
</figure>

[@barigozzi2022fnets] propose the FNETS method for factor-adjusted VAR
modelling of high-dimensional, second-order stationary time series.
Under their proposed model, the data is decomposed into two latent
components such that the *factor-driven* component accounts for
pervasive leading, lagging or contemporaneous co-movements of the
variables, while the remaining *idiosyncratic* dynamic dependence
between the variables is modelled by a sparse VAR process. Then, FNETS
provides tools for inferring the networks underlying the latent VAR
process and forecasting.

In this paper, we present an R package named fnets which implements the
FNETS method. It provides a range of user-friendly tools for estimating
and visualising the networks representing the interconnectedness of time
series variables, and for producing forecasts. In addition, fnets
includes a range of methods for selecting tuning parameters ranging from
the number of factors and the VAR order, to regularisation and
thresholding parameters adopted for producing sparse and interpretable
networks. The main routine of fnets outputs an object of S3 class fnets
which is supported by a plot method for network visualisation and a
predict method for time series forecasting.

There exist several packages for fitting VAR models and their extensions
to high-dimensional time series, see
[LSVAR](https://CRAN.R-project.org/package=LSVAR) [@lsvar],
[sparsevar](https://CRAN.R-project.org/package=sparsevar) [@sparsevar],
[nets](https://CRAN.R-project.org/package=nets) [@nets],
[mgm](https://CRAN.R-project.org/package=mgm) [@haslbeck2020mgm],
[graphicalVAR](https://CRAN.R-project.org/package=graphicalVAR)
[@epskamp2018gaussian],
[BigVAR](https://CRAN.R-project.org/package=BigVAR)
[@nicholson2017bigvar], and
[bigtime](https://CRAN.R-project.org/package=bigtime)
[@wilms2021bigtime]. There also exist R packages for time series factor
modelling such as [dfms](https://CRAN.R-project.org/package=dfms)
[@dfms] and [sparseDFM](https://CRAN.R-project.org/package=sparseDFM)
[@mosley2023sparsedfm], and
[FAVAR](https://CRAN.R-project.org/package=FAVAR)
[@bernanke2005measuring] for Bayesian inference of factor-augmented VAR
models. The advantage of
[fnets](https://CRAN.R-project.org/package=fnets) over the
above-mentioned packages is its ability to handle strong cross-sectional
and serial correlations in the data via factor-adjustment step performed
in the frequency domain. In addition, the FNETS method operates under
the most general approach to high-dimensional time series factor
modelling termed the Generalised Dynamic Factor (GDFM), first proposed
in [@forni2000generalized] and further investigated in
[@forni2015dynamic]. Accordingly, fnets is the first R package to
provide tools for high-dimensional panel data analysis under the GDFM,
such as fast computation of spectral density and autocovariance matrices
via the Fast Fourier Transform, but it is flexible enough to allow for
more restrictive static factor models. While there exist some packages
for network-based time series modelling
(e.g. [GNAR](https://CRAN.R-project.org/package=GNAR),
@knight2020generalized, [-@knight2020generalized]), we highlight that
the goal of [fnets](https://CRAN.R-project.org/package=fnets) is to
learn the networks underlying a time series and does not require a
network as an input.

# FNETS methodology {#sec:models}

In this section, we introduce the factor-adjusted VAR model and describe
the FNETS methodology proposed in [@barigozzi2022fnets] for network
estimation and forecasting of high-dimensional time series. We limit
ourselves to describing the key steps of FNETS and refer to the above
paper for its comprehensive treatment.

## Factor-adjusted VAR model

A zero-mean, $p$-variate process $\bm\xi_t$ follows a VAR($d$) model if
it satisfies $$\begin{aligned}
\label{eq:idio:var}
    \bm\xi_t &= \sum_{\ell = 1}^{d} \mathbf A_{\ell} \bm\xi_{t - \ell} + \bm\Gamma^{1/2} \bm\varepsilon_t,
\end{aligned}$$ where
$\mathbf A_\ell \in R^{p \times p}, \, 1 \le \ell \le d$, determine how
future values of the series depend on the past values. For the
$p$-variate random vector
$\bm\varepsilon_t = (\varepsilon_{1t}, \ldots, \varepsilon_{pt})^\top$,
we assume that $\varepsilon_{it}$ are independently and identically
distributed (i.i.d.) for all $i$ and $t$ with $%
  \mathop{\operator@font I\hspace{-1.5pt}E\hspace{.13pt}}(\varepsilon_{it}) = 0$
and $\mathsf{Var}(\varepsilon_{it}) = 1$. Then, the positive definite
matrix $\bm\Gamma \in R^{p \times p}$ is the covariance matrix of the
innovations $\bm\Gamma^{1/2} \bm\varepsilon_t$.

In the literature on factor modelling of high-dimensional time series,
the factor-driven component exhibits strong cross-sectional and/or
serial correlations by 'loading' finite-dimensional vectors of factors
linearly. Among many time series factor models, the GDFM
[@forni2000generalized] provides the most general approach where the
$p$-variate factor-driven component $\bm\chi_t$ admits the following
representation $$\begin{aligned}
    \bm\chi_t &= \mathcal B(L) \mathbf u_t = \sum_{\ell = 0}^\infty \mathbf B_\ell \mathbf u_{t - \ell} \text{ \ with \ } \mathbf u_t = (u_{1t}, \ldots, u_{qt})^\top \text{ and } \mathbf B_\ell \in R^{p \times q}, \label{eq:gdfm}
\end{aligned}$$ for some fixed $q$, where $L$ stands for the lag
operator. The $q$-variate random vector $\mathbf u_t$ contains the
common factors which are loaded across the variables and time by the
filter $\mathcal B(L) = \sum_{\ell = 0}^\infty \mathbf B_\ell L^\ell$,
and it is assumed that $u_{jt}$ are i.i.d. with $%
  \mathop{\operator@font I\hspace{-1.5pt}E\hspace{.13pt}}(u_{jt}) = 0$
and $\mathsf{Var}(u_{jt}) = 1$. The
model [\[eq:gdfm\]](#eq:gdfm){reference-type="eqref"
reference="eq:gdfm"} reduces to a static factor model
[@bai2003; @stock2002forecasting; @fan2013large], when
$\mathcal B(L) = \sum_{\ell = 0}^s \mathbf B_\ell L^\ell$ for some
finite integer $s \ge 0$. Then, we can write $$\begin{aligned}
\label{eq:static}
    \bm\chi_t = \bm\Lambda \mathbf F_t \text{ \ where \ } \mathbf F_t = (\mathbf u_t^\top, \ldots, \mathbf u_{t - s}^\top)^\top \text{ \ and \ } \bm\Lambda = [\mathbf B_0, \ldots, \mathbf B_s]
    % \text{ \ and \ } \mbf f_t = \sum_{\ell = 0}^{m_2} \mbf M^{(2)}_\ell \mbf u_{t - \ell},
\end{aligned}$$ with $r = q (s + 1)$ as the dimension of static factors
$\mathbf F_t$. Throughout, we refer to the
models [\[eq:gdfm\]](#eq:gdfm){reference-type="eqref"
reference="eq:gdfm"}
and [\[eq:static\]](#eq:static){reference-type="eqref"
reference="eq:static"} as *unrestricted* and *restricted* to highlight
that the latter imposes more restrictions on the model.

[@barigozzi2022fnets] propose a factor-adjusted VAR model under which we
observe a zero-mean, second-order stationary process
$\mathbf X_t = (X_{1t}, \ldots, X_{pt})^\top$ for $t = 1, \ldots, n$,
that permits a decomposition into the sum of the unobserved components
$\bm\xi_t$ and $\bm\chi_t$, i.e. $$\begin{aligned}
    \label{eq:model}
    \mathbf X_t = \bm\xi_t + \bm\chi_t.
\end{aligned}$$ We assume that $%
  \mathop{\operator@font I\hspace{-1.5pt}E\hspace{.13pt}}(\varepsilon_{it} u_{jt'}) = 0$
for all $i$, $j$, $t$ and $t'$ as is commonly assumed in the literature,
such that $%
  \mathop{\operator@font I\hspace{-1.5pt}E\hspace{.13pt}}(\xi_{it} \chi_{i't'}) = 0$
for all $1 \le i, i' \le p$ and $t, t' \in \mathbb{Z}$.

## Networks {#sec:networks}

Under [\[eq:model\]](#eq:model){reference-type="eqref"
reference="eq:model"}, it is of interest to infer three types of
networks representing the interconnectedness of $\mathbf X_t$ after
factor adjustment. Let $\mathcal V = \{1, \ldots, p\}$ denote the set of
vertices representing the $p$ cross-sections. Then, the VAR parameter
matrices, $\mathbf A_\ell = [A_{\ell, ii'}, \, 1 \le i, i' \le p]$,
encode the directed network
$\mathcal N^{\text{\upshape{G}}} = (\mathcal V, \mathcal E^{\text{\upshape{G}}})$
representing Granger causal linkages, where the set of edges are given
by $$\begin{aligned}
\label{eq:net:dir}
\mathcal E^{\text{\upshape{G}}} = \left\{(i, i') \in \mathcal V \times \mathcal V: \, A_{\ell, ii'} \ne 0 \text{ for some } 1 \le \ell \le d \right\}.
\end{aligned}$$ Here, the presence of an edge
$(i, i') \in \mathcal E^{\text{\upshape{G}}}$ indicates that
$\xi_{i', t - \ell}$ Granger causes $\xi_{it}$ at some lag
$1 \le \ell \le d$ [@dahlhaus2000graphical].

The second network contains undirected edges representing
contemporaneous cross-sectional dependence in VAR innovations
$\bm\Gamma^{1/2} \bm\varepsilon_t$, denoted by
$\mathcal N^{\text{\upshape{C}}} = (\mathcal V, \mathcal E^{\text{\upshape{C}}})$.
We have $(i, i') \in \mathcal E^{\text{\upshape{C}}}$ if and only if the
partial correlation between the $i$-th and $i'$-th elements of
$\bm\Gamma^{1/2} \bm\varepsilon_t$ is non-zero, which in turn is given
by $- \delta_{ii'}/\sqrt{\delta_{ii} \cdot \delta_{i'i'}}$ where
$\bm\Gamma^{-1} = \bm\Delta = [\delta_{ii'}, \, 1 \le i, i' \le p]$
[@peng2009partial]. Hence, the set of edges for
$\mathcal N^{\text{\upshape{C}}}$ is given by $$\begin{aligned}
\label{eq:net:undir}
\mathcal E^{\text{\upshape{C}}} = \left\{ (i, i') \in \mathcal V \times \mathcal V: \, i \ne i' \text{ and }
- \frac{\delta_{ii'}}{\sqrt{\delta_{ii} \cdot \delta_{i'i'}}} \ne 0 \right\},
\end{aligned}$$

Finally, we can summarise the aforementioned lead-lag and
contemporaneous relations between the variables in a single, undirected
network
$\mathcal N^{\text{\upshape{L}}} = (\mathcal V, \mathcal E^{\text{\upshape{L}}})$
by means of the long-run partial correlations of $\bm\xi_t$. Let
$\bm\Omega = [\omega_{ii'}, \, 1 \le i, i' \le p]$ denote the inverse of
the zero-frequency spectral density (a.k.a. long-run covariance) of
$\bm\xi_t$, which is given by
$\bm \Omega = 2\pi \mathcal A^\top(1) \bm\Delta \mathcal A(1)$ with
$\mathcal A(z) = \mathbf I - \sum_{\ell = 1}^{d} \mathbf A_\ell z^\ell$.
Then, the long-run partial correlation between the $i$-th and $i'$-th
elements of $\bm\xi_t$, is obtained as
$- \omega_{ii'}/\sqrt{\omega_{ii} \cdot \omega_{i'i'}}$
[@dahlhaus2000graphical], so the edge set of
$\mathcal N^{\text{\upshape{L}}}$ is given by $$\begin{aligned}
\label{eq:net:lr}
\mathcal E^{\text{\upshape{L}}} = \left\{ (i, i') \in \mathcal V \times \mathcal V: \, i \ne i' \text{ and }
- \frac{\omega_{ii'}}{\sqrt{\omega_{ii} \cdot \omega_{i'i'}}} \ne 0 \right\}.
\end{aligned}$$

## FNETS: Network estimation {#sec:estimation}

We describe the three-step methodology for estimating the networks
$\mathcal N^{\text{\upshape{G}}}$, $\mathcal N^{\text{\upshape{C}}}$ and
$\mathcal N^{\text{\upshape{L}}}$. Throughout, we assume that the number
of factors, either $q$ under the more general model
in [\[eq:gdfm\]](#eq:gdfm){reference-type="eqref" reference="eq:gdfm"}
or $r$ under the restricted model
in [\[eq:static\]](#eq:static){reference-type="eqref"
reference="eq:static"}, and the VAR order $d$, are known, and discuss
its selection in [Tuning parameter selection](#sec:tuning).

### Step 1: Factor adjustment {#sec:step:one}

The autocovariance (ACV) matrices of $\bm\xi_t$, denoted by
$\bm\Gamma_\xi(\ell) = %
  \mathop{\operator@font I\hspace{-1.5pt}E\hspace{.13pt}}(\bm\xi_{t - \ell} \bm\xi_t^\top)$
for $\ell \ge 0$ and
$\bm\Gamma_\xi(\ell) = (\bm\Gamma_\xi(- \ell))^\top$ for $\ell < 0$,
play a key role in network estimation. Since $\bm\xi_t$ is not directly
observed, we propose to adjust for the presence of the factor-driven
$\bm\chi_t$ and estimate $\bm\Gamma_\xi(\ell)$. For this, we adopt a
frequency domain-based approach and perform the dynamic principal
component analysis (PCA). Spectral density matrix $\bm\Sigma_x(\omega)$
of a time series $\{\mathbf X_t\}_{t \in \mathbb{Z}}$ aggregates
information of its ACV $\bm\Gamma_x(\ell), \, \ell \in \mathbb{Z}$, at a
specific frequency $\omega \in [-\pi, \pi]$, and is obtained by the
Fourier transform
$\bm\Sigma_x(\omega) = (2\pi)^{-1} \sum_{\ell = -\infty}^\infty \bm\Gamma_x(\ell) \exp(-\iota \ell \omega)$
where $\iota = \sqrt{ - 1}$. Denoting the sample ACV matrix of
$\mathbf X_t$ at lag $\ell$ by $$\begin{aligned}
\widehat{\bm\Gamma}_x(\ell) = \frac{1}{n} \sum_{t = \ell + 1}^n \mathbf X_{t - \ell} \mathbf X_t^\top
\text{ when } \ell \ge 0 \quad \text{and} \quad
\widehat{\bm\Gamma}_x(\ell) = (\widehat{\bm\Gamma}_x(-\ell))^\top
\text{ when } \ell < 0,
\end{aligned}$$ we estimate the spectral density of $\mathbf X_t$ by
$$\begin{aligned}
\label{eq:sigma:hat}
\widehat{\bm\Sigma}_x(\omega_k) = \frac{1}{2\pi} \sum_{\ell = -m}^m K\left(\frac{\ell}{m}\right)
\widehat{\bm\Gamma}_x(\ell) \exp(-\iota \ell \omega_k), 
\end{aligned}$$ where $K(\cdot)$ denotes a kernel, $m$ the kernel
bandwidth (for its choice, see [Tuning parameter
selection](#sec:tuning)) and $\omega_k = 2\pi k / (2m + 1)$ the Fourier
frequencies. We adopt the Bartlett kernel as $K(\cdot)$, which ensures
positive semi-definiteness of $\widehat{\bm\Sigma}_x(\omega)$ and also
$\widehat{\bm\Gamma}_\xi(\ell)$ estimating $\bm\Gamma_\xi(\ell)$
obtained as described below.

Performing PCA on $\widehat{\bm\Sigma}_x(\omega_k)$ at each $\omega_k$,
we obtain the estimator of the spectral density matrix of $\bm\chi_t$ as
$\widehat{\bm\Sigma}_\chi(\omega_k) = \sum_{j = 1}^q \widehat{\mu}_{x, j}(\omega_k) \widehat{\mathbf e}_{x, j}(\omega_k) (\widehat{\mathbf e}_{x, j}(\omega_k))^*$,
where $\widehat{\mu}_{x, j}(\omega_k)$ denotes the $j$-th largest
eigenvalue of $\widehat{\bm\Sigma}_x(\omega_k)$,
$\widehat{\mathbf e}_{x, j}(\omega_k)$ its associated eigenvector, and
for any vector $\mathbf a \in \mathbb{C}^n$, we denote its transposed
complex conjugate by $\mathbf a^*$. Then taking the inverse Fourier
transform of $\widehat{\bm\Sigma}_\chi(\omega_k), \, -m \le k \le m$,
leads to an estimator of $\bm\Gamma_\chi(\ell)$, the ACV matrix of
$\bm\chi_t$, as $$\begin{aligned}
\widehat{\bm\Gamma}_\chi(\ell) = \frac{2\pi}{2m + 1} \sum_{k = -m}^m
\widehat{\bm\Sigma}_\chi(\omega_k) \exp(\iota \ell \omega_k) \quad \text{for \ } -m \le \ell \le m.
\end{aligned}$$ Finally, we estimate the ACV of $\bm\xi_t$ by
$$\begin{aligned}
\label{eq:acv:xi}
\widehat{\bm\Gamma}_\xi(\ell) = \widehat{\bm\Gamma}_x(\ell) - \widehat{\bm\Gamma}_\chi(\ell).
\end{aligned}$$

When we assume the restricted factor model
in [\[eq:static\]](#eq:static){reference-type="eqref"
reference="eq:static"}, the factor-adjustment step is simplified as it
suffices to perform PCA in the time domain, i.e. eigenanalysis of the
sample covariance matrix $\widehat{\bm\Gamma}_x(0)$. Denoting the
eigenvector of $\widehat{\bm\Gamma}_x(0)$ associated with its $j$-th
largest eigenvalue by $\widehat{\mathbf e}_{x, j}$, we obtain
$\widehat{\bm\Gamma}_\xi(\ell) = \widehat{\bm\Gamma}_x(\ell) - \widehat{\mathbf E}_x \widehat{\mathbf E}_x^\top \widehat{\bm\Gamma}_x(\ell) \widehat{\mathbf E}_x \widehat{\mathbf E}_x^\top$
where
$\widehat{\mathbf E}_x = [\widehat{\mathbf e}_{x, j}, \, 1 \le j \le r]$.

### Step 2: Estimation of $\mathcal N^{\text{\upshape{G}}}$ {#sec:step:two}

Recall from [\[eq:net:dir\]](#eq:net:dir){reference-type="eqref"
reference="eq:net:dir"} that $\mathcal N^{\text{\upshape{G}}}$,
representing Granger causal linkages, has its edge set determined by the
VAR transition matrices $\mathbf A_\ell, \, 1 \le \ell \le d$. By the
Yule-Walker equation, we have
$\bm\beta = [ \mathbf A_1, \dots, \mathbf A_{d} ]^\top = {\mathbf G(d)}^{-1} \mathbf g(d)$,
where $$\begin{aligned}
\label{eq:gg}
\mathbf G(d) = \begin{bmatrix}
\bm\Gamma_\xi(0) & \bm\Gamma_\xi(-1) & \ldots & \bm\Gamma_\xi(-d + 1) 
\\
\bm\Gamma_\xi(1) & \bm\Gamma_\xi(0) & \ldots & \bm\Gamma_\xi(-d + 2)
\\
& & \ddots & 
\\
\bm\Gamma_\xi(d - 1) & \bm\Gamma_\xi(d - 2) & \ldots & \bm\Gamma_\xi(0)
\end{bmatrix}
\quad \text{and} \quad
\mathbf g(d) = \begin{bmatrix}
\bm\Gamma_\xi(1)
\\
\bm\Gamma_\xi(2)
\\
\vdots
\\
\bm\Gamma_\xi(d)
\end{bmatrix}.
\end{aligned}$$ We propose to estimate $\bm\beta$ as a regularised
Yule-Walker estimator based on $\widehat{\mathbf G}(d)$ and
$\widehat{\mathbf g}(d)$, each of which is obtained by replacing
$\bm\Gamma_\xi(\ell)$ with $\widehat{\bm\Gamma}_\xi(\ell)$,
see [\[eq:acv:xi\]](#eq:acv:xi){reference-type="eqref"
reference="eq:acv:xi"}, in the definition of $\mathbf G(d)$ and
$\mathbf g(d)$.

For any matrix $\mathbf M = [m_{ij}] \in R^{n_1 \times n_2}$, let
$\vert \mathbf M \vert_1 = \sum_{i = 1}^{n_1} \sum_{j = 1}^{n_2} \vert m_{ij} \vert$,
$\vert \mathbf M \vert_\infty = \max_{1 \le i \le n_1} \max_{1 \le j \le n_2} \vert m_{ij} \vert$
and $\mathsf{tr}(\mathbf M) = \sum_{i = 1}^{n_1} m_{ii}$ when
$n_1 = n_2$. We consider two estimators of $\bm\beta$. Firstly, we adopt
a Lasso-type estimator which solves an $\ell_1$-regularised
$M$-estimation problem $$\begin{aligned}
\label{eq:lasso}
\widehat{\bm\beta}^{\text{\upshape{las}}} = \underset{\mathbf M \in \mathbb{R}^{pd \times p}}{\arg\min} \
\mathsf{tr}\left( \mathbf M^\top \widehat{\mathbf G}(d) \mathbf M - 2\mathbf M^\top\widehat{\mathbf g}(d) \right)
+ \lambda \vert \mathbf M \vert_1
\end{aligned}$$ with a tuning parameter $\lambda > 0$. In the
implementation, we
solve [\[eq:lasso\]](#eq:lasso){reference-type="eqref"
reference="eq:lasso"} via the fast iterative shrinkage-thresholding
algorithm (FISTA, @beck2009fast, [-@beck2009fast]). Alternatively, we
adopt a constrained $\ell_1$-minimisation approach closely related to
the Dantzig selector (DS, @candes2007dantzig, [-@candes2007dantzig]):
$$\begin{aligned}
\label{eq:ds}
\widehat{\bm\beta}^{\text{\upshape{DS}}} =  \underset{\mathbf M \in \mathbb{R}^{pd \times p}}{\arg\min} \ \vert \mathbf M \vert_1
\quad \text{subject to} \quad
\left\vert \widehat{\mathbf G}(d) \mathbf M - \widehat{\mathbf g}(d) \right\vert_\infty \le \lambda
\end{aligned}$$ for some tuning parameter $\lambda > 0$. We
divide [\[eq:ds\]](#eq:ds){reference-type="eqref" reference="eq:ds"}
into $p$ sub-problems and obtain each column of
$\widehat{\bm\beta}^{\text{\upshape{DS}}}$ via the simplex algorithm
(using the function lp in
[lpSolve](https://CRAN.R-project.org/package=lpSolve) [@lpsolve]), which
is performed in parallel with
[doParallel](https://CRAN.R-project.org/package=doParallel) and
[foreach](https://CRAN.R-project.org/package=foreach)
[@doparallel; @foreach].

[@barigozzi2022fnets] establish the consistency of both
$\widehat{\bm\beta}^{\text{\upshape{las}}}$ and
$\widehat{\bm\beta}^{\text{\upshape{DS}}}$ but, as is typically the case
for $\ell_1$-regularisation methods, they do not achieve exact recovery
of the support of $\bm\beta$. Hence we propose to estimate the edge set
of $\mathcal N^{\text{\upshape{G}}}$ by thresholding the elements of
$\widehat{\bm\beta}$ with some threshold $\mathfrak{t} > 0$, where
either $\widehat{\bm\beta} = \widehat{\bm\beta}^{\text{\upshape{las}}}$
or $\widehat{\bm\beta} = \widehat{\bm\beta}^{\text{\upshape{DS}}}$,
i.e. $$\begin{aligned}
\label{eq:threshold}
\widetilde{\bm\beta}(\mathfrak{t}) &= 
\begin{bmatrix}\widehat{\beta}_{ij} \cdot \mathbb{I}_{\{\vert \widehat{\beta}_{ij} \vert > \mathfrak{t} \}},
\, 1 \le i \le pd, \, 1 \le j \le p \end{bmatrix}.
\end{aligned}$$ We discuss cross validation and information criterion
methods for selecting $\lambda$, and a data-driven choice of
$\mathfrak{t}$, in [Tuning parameter selection](#sec:tuning).

### Step 3: Estimation of $\mathcal N^{\text{\upshape{C}}}$ and $\mathcal N^{\text{\upshape{L}}}$ {#sec:step:three}

From the definitions of $\mathcal N^{\text{\upshape{C}}}$ and
$\mathcal N^{\text{\upshape{L}}}$ given
in [\[eq:net:undir\]](#eq:net:undir){reference-type="eqref"
reference="eq:net:undir"}
and [\[eq:net:lr\]](#eq:net:lr){reference-type="eqref"
reference="eq:net:lr"}, their edge sets are obtained by estimating
$\bm\Delta = \bm\Gamma^{-1}$ and
$\bm\Omega = 2\pi \mathcal A^\top(1) \bm\Delta \mathcal A(1)$. Suppose
that we are given
$\widehat{\bm\beta} = [\widehat{\mathbf A}_1, \ldots, \widehat{\mathbf A}_d]^\top$,
some estimator of the VAR parameter matrices obtained as in
either [\[eq:lasso\]](#eq:lasso){reference-type="eqref"
reference="eq:lasso"} or [\[eq:ds\]](#eq:ds){reference-type="eqref"
reference="eq:ds"}. Then, a natural estimator of $\bm\Gamma$ arises from
the Yule-Walker equation
$\bm\Gamma = \bm\Gamma_\xi(0) - \sum_{\ell = 1}^{d} \mathbf A_{\ell} \bm\Gamma_\xi(\ell) = \bm\Gamma_\xi(0) - \bm\beta^\top {\mathbf g}$,
as
$\widehat{\bm\Gamma} = \widehat{\bm\Gamma}_\xi(0) - \widehat{\bm\beta}^\top \widehat{\mathbf g}$.
In high dimensions, it is not feasible or recommended to directly invert
$\widehat{\bm\Gamma}$ to estimate $\bm\Delta$. Therefore, we adopt a
constrained $\ell_1$-minimisation method motivated by the CLIME
methodology of [@cai2011constrained].

Specifically, the CLIME estimator of $\bm\Delta$ is obtained by first
solving $$\begin{aligned}
\label{eq:clime}
\check{\bm\Delta} & = {\arg\min}_{\mathbf M \in \mathbb{R}^{p \times p}} \vert \mathbf M \vert_1 
\quad \text{subject to} \quad
\left\vert \widehat{\bm\Gamma} \mathbf M - \mathbf I \right\vert_\infty \le \eta,
\end{aligned}$$ and applying a symmetrisation step to
$\check{\bm\Delta} = [\check\delta_{ii'}, \, 1 \le i, j \le p]$ as
$$\begin{aligned}
\label{eq:delta:clime}
\widehat{\bm\Delta} &= [\widehat\delta_{ii'}, \, 1 \le i, i' \le p]
\text{ with } 
\widehat\delta_{ii'} = \check\delta_{ii'} \cdot \mathbb{I}_{\{\vert \check\delta_{ii'} \vert
\le \vert \check\delta_{i'i} \vert \}}
+ \check\delta_{i'i} \cdot \mathbb{I}_{\{\vert \check\delta_{i'i} \vert
< \vert \check\delta_{ii'} \vert \}}.
\end{aligned}$$ for some tuning parameter $\eta > 0$.
[@cai2016estimating] propose ACLIME, which improves the CLIME estimator
by selecting the parameter $\eta$
in [\[eq:delta:clime\]](#eq:delta:clime){reference-type="eqref"
reference="eq:delta:clime"} adaptively. It first produces the estimators
of the diagonal entries $\delta_{ii}, 1 \le i \le p$, as
in [\[eq:delta:clime\]](#eq:delta:clime){reference-type="eqref"
reference="eq:delta:clime"} with $\eta_1 = 2 \sqrt{\log (p) / n}$ as the
tuning parameter. Then these estimates are used for adaptive tuning
parameter selection in the second step. We provide the full description
of the ACLIME estimator along with the details of its implementation in
[ACLIME estimator](#sec:aclime) of the Appendix.

Given the estimators
$\widehat{\mathcal A}(1) = \mathbf I - \sum_{\ell = 1}^d \widehat{\mathbf A}_\ell$
and $\widehat{\bm\Delta}$, we estimate $\bm\Omega$ by
$\widehat{\bm\Omega} = 2\pi \widehat{\mathcal A}^\top(1) \widehat{\bm\Delta} \widehat{\mathcal A}(1)$.
In [@barigozzi2022fnets], $\widehat{\bm\Delta}$ and
$\widehat{\bm\Omega}$ are shown to be consistent in $\ell_\infty$- and
$\ell_1$-norms under suitable sparsity assumptions. However, an
additional thresholding step as
in [\[eq:threshold\]](#eq:threshold){reference-type="eqref"
reference="eq:threshold"} is required to guarantee consistency in
estimating the support of $\bm\Delta$ and $\bm\Omega$ and consequently
the edge sets of $\mathcal N^{\text{\upshape{C}}}$ and
$\mathcal N^{\text{\upshape{L}}}$. We discuss data-driven selection of
these thresholds and $\eta$ in [Tuning parameter
selection](#sec:tuning).

## FNETS: Forecasting

Following the estimation procedure, FNETS performs forecasting by
estimating the best linear predictor of $\mathbf X_{n + a}$ given
$\mathbf X_t, \, t \le n$, for a fixed integer $a \ge 1$. This is
achieved by separately producing the best linear predictors of
$\bm\chi_{n + a}$ and $\bm\xi_{n + a}$ as described below, and then
combining them.

### Forecasting the factor-driven component {#sec:common:pred}

For given $a \ge 0$, the best linear predictor of $\bm\chi_{n + a}$
given $\mathbf X_t, \, t \le n$,
under [\[eq:gdfm\]](#eq:gdfm){reference-type="eqref"
reference="eq:gdfm"} is $$\begin{aligned}
\bm\chi_{n + a \vert n} = \sum_{\ell = 0}^\infty \mathbf B_{\ell + a} \mathbf u_{n  - \ell}.
\end{aligned}$$ [@forni2015dynamic] show that the
model [\[eq:gdfm\]](#eq:gdfm){reference-type="eqref"
reference="eq:gdfm"} admits a low-rank VAR representation with
$\mathbf u_t$ as the innovations under mild conditions, and
[@forni2017dynamic] propose the estimators of $\mathbf B_\ell$ and
$\mathbf u_t$ based on this representation which make use of the
estimators of the ACV of $\bm\chi_t$ obtained as described in [Step
1](#sec:step:one). Then, a natural estimator of
$\bm\chi_{n + a \vert n}$ is $$\begin{aligned}
\label{eq:chi:va}
\widehat{\bm\chi}^{\text{\upshape{unr}}}_{n + a \vert n} = \sum_{\ell = 0}^K \widehat{\mathbf B}_{\ell + a} \widehat{\mathbf u}_{n  - \ell}
\end{aligned}$$ for some truncation lag $K$. We refer to
$\widehat{\bm\chi}^{\text{\upshape{unr}}}_{n + a \vert n}$ as the
*unrestricted* estimator of $\bm\chi_{n + a \vert n}$ as it is obtained
without imposing any restrictions on the factor
model [\[eq:gdfm\]](#eq:gdfm){reference-type="eqref"
reference="eq:gdfm"}.

When $\bm\chi_t$ admits the static representation
in [\[eq:static\]](#eq:static){reference-type="eqref"
reference="eq:static"}, we can show that
$\bm\chi_{n + a \vert n} = \bm{\Gamma}_\chi( - a) \mathbf E_\chi \bm{\mathcal M}_\chi^{-1} \mathbf E_\chi^\top \bm\chi_n$,
where $\bm{\mathcal M}_\chi \in R^{r \times r}$ is a diagonal matrix
with the $r$ eigenvalues of $\bm\Gamma_\chi(0)$ on its diagonal and
$\mathbf E_\chi \in R^{p \times r}$ the matrix of the corresponding
eigenvectors; see Section 4.1 of [@barigozzi2022fnets] and also
[@forni2005generalized]. This suggests an estimator $$\begin{aligned}
\label{eq:chi:static}
\widehat{\bm\chi}^{\text{\upshape {res}}}_{n + a \vert n} = \widehat{\bm\Gamma}_\chi(-a) \widehat{\mathbf E}_\chi \widehat{\bm{\mathcal M}}_\chi^{-1} \widehat{\mathbf E}_\chi^\top \mathbf X_n,
\end{aligned}$$ where $\widehat{\bm{\mathcal M}}_\chi$ and
$\widehat{\mathbf E}_\chi$ are obtained from the eigendecomposition of
$\widehat{\bm\Gamma}_\chi(0)$. We refer to
$\widehat{\bm\chi}^{\text{\upshape {res}}}_{n + a \vert n}$ as the
*restricted* estimator of $\bm\chi_{n + a \vert n}$. As a by-product, we
obtain the in-sample estimators of $\bm\chi_t, \, t \le n$, as
$\widehat{\bm\chi}_{t \vert n} = \widehat{\bm\chi}_t$, with either of
the two estimators in [\[eq:chi:va\]](#eq:chi:va){reference-type="eqref"
reference="eq:chi:va"}
and [\[eq:chi:static\]](#eq:chi:static){reference-type="eqref"
reference="eq:chi:static"}.

### Forecasting the latent VAR process {#sec:idio:pred}

Once the VAR parameters are estimated either as
in [\[eq:lasso\]](#eq:lasso){reference-type="eqref"
reference="eq:lasso"} or [\[eq:ds\]](#eq:ds){reference-type="eqref"
reference="eq:ds"}, we produce an estimator of
$\bm\xi_{n + a \vert n} = \sum_{\ell = 1}^{d} \mathbf A_{\ell} \bm\xi_{n + a - \ell}$,
the best linear predictor of $\bm\xi_{n + a}$ given
$\mathbf X_t, \, t \le n$, as $$\begin{aligned}
\label{eq:idio:best:lin:pred}
\widehat{\bm\xi}_{n + a \vert n} = \sum_{\ell = 1}^{\max(1, a) - 1} \widehat{\mathbf A}_{\ell} \widehat{\bm\xi}_{n + a - \ell \vert n} + \sum_{\ell = \max(1, a)}^{d} \widehat{\mathbf A}_{\ell} \widehat{\bm\xi}_{n + a - \ell}.
\end{aligned}$$ Here,
$\widehat{\bm\xi}_{n + 1 - \ell} = \mathbf X_{n + 1 - \ell} - \widehat{\bm\chi}_{n + 1 - \ell}$
denotes the in-sample estimator of $\bm\xi_{n + 1 - \ell}$, which may be
obtained with either of the two (in-sample) estimators of the
factor-driven component
in [\[eq:chi:va\]](#eq:chi:va){reference-type="eqref"
reference="eq:chi:va"}
and [\[eq:chi:static\]](#eq:chi:static){reference-type="eqref"
reference="eq:chi:static"}.

# Tuning parameter selection {#sec:tuning}

## Factor numbers $q$ and $r$ {#sec:tuning:factor}

The estimation and forecasting tools of the FNETS methodology require
the selection of the number of factors, i.e. $q$ under the unrestricted
factor model in [\[eq:gdfm\]](#eq:gdfm){reference-type="eqref"
reference="eq:gdfm"}, and $r$ under the restricted, static factor model
in [\[eq:static\]](#eq:static){reference-type="eqref"
reference="eq:static"}.
Under [\[eq:gdfm\]](#eq:gdfm){reference-type="eqref"
reference="eq:gdfm"}, there exists a large gap between the $q$ leading
eigenvalues of the spectral density matrix of $\mathbf X_t$ and the
remainder which diverges with $p$ (see also
Figure [1](#fig:eigen){reference-type="ref" reference="fig:eigen"}). We
provide two methods for selecting the factor number $q$, which make use
of the postulated eigengap using
$\widehat\mu_{x, j}(\omega_k), \, 1 \le j \le p$, the eigenvalues of the
spectral density estimator of $\mathbf X_t$
in [\[eq:sigma:hat\]](#eq:sigma:hat){reference-type="eqref"
reference="eq:sigma:hat"} at a given Fourier frequency
$\omega_k, \, -m \le k \le m$.

[@hallin2007determining] propose an information criterion for selecting
the number of factors under the
model [\[eq:gdfm\]](#eq:gdfm){reference-type="eqref"
reference="eq:gdfm"} and further, a methodology for tuning the
multiplicative constant in the penalty. Define $$\begin{aligned}
\label{eq:ic}
	\text{IC}(b, c) = \log \left(\frac{1}{p} \sum _{j = b + 1}^p \frac{1}{2m + 1} \sum_{k = -m}^m \widehat{\mu}_{x, j}(\omega_k) \right) 
 + b \cdot c \cdot \text{pen}(n, p),
\end{aligned}$$ where
$\text{pen}(n, p) = \min(p, m^2, \sqrt{n / m})^{-1/2}$ by default (for
other choices of the information criterion, see
[Appendix A](#sec:factornumber)), and $c > 0$ a constant. Provided that
$\text{pen}(n, p) \to 0$ sufficiently slowly, for an arbitrary value of
$c$, the factor number $q$ is consistently estimated by the minimiser of
$\text{IC}(b, c)$ over $b \in \{0, \ldots, \bar{q}\}$, with some fixed
$\bar{q}$ as the maximum allowable number of factors. However, this is
not the case in finite sample, and [@hallin2007determining] propose to
simultaneously select $q$ and $c$. First, we identify
$\widehat q(n_l, p_l, c) = \arg\min_{0 \le b \le \bar{q}} \text{IC}(n_l, p_l, b, c)$
where $\text{IC}(n_l, p_l, b, c)$ is constructed analogously to
$\text{IC}(b, c)$, except that it only involves the sub-sample
$\{X_{it}, \, 1 \le i \le p_l, \, 1 \le t \le n_l\}$, for sequences
$0 < n_1 < \ldots < n_L = n$ and $0 < p_1 < \ldots < p_L = p$. Then,
denoting the sample variance of
$\widehat q(n_l, p_l, c), \, 1 \le l \le L$, by $S(c)$, we select
$\widehat q = \widehat q(n, p, \widehat c)$ with $\widehat c$
corresponding to the second interval of stability with $S(c) = 0$ for
the mapping $c \mapsto S(c)$ as $c$ increases from $0$ to some
$c_{\max}$ (the first stable interval is where $\bar{q}$ is selected
with a very small value of $c$).
Figure [3](#fig:qplot){reference-type="ref" reference="fig:qplot"} plots
$\widehat q(n, p, c)$ and $S(c)$ for varying values of $c$ obtained from
a dataset simulated in [Data simulation](#sec:package:data). In the
implementation of this methodology, we set
$n_l = n - (L - l) \lfloor n/20 \rfloor$ and
$p_l = \lfloor 3p/4 + lp/40 \rfloor$ with $L = 10$, and
$\bar{q} = \min(50, \lfloor \sqrt{\min(n - 1, p)} \rfloor)$.

![Plots of $c$ against $\widehat q(n, p, c)$ (in circles, $y$-axis on
the left) and $S(c)$ (in triangles, $y$-axis on the right) with the six
IC (see [Appendix A](#sec:factornumber)) implemented in the function
factor.number of fnets, on a dataset simulated as described in [Data
simulation](#sec:package:data) (with $n = 500$, $p = 50$ and $q = 2$).
With the default choice of IC
in [\[eq:ic\]](#eq:ic){reference-type="eqref" reference="eq:ic"}
(IC$_5$), we obtain $\widehat q = \widehat q(n, p, \widehat c) = 2$
correctly estimating $q = 2$.](figs/IC.pdf){#fig:qplot
width=".6\\textwidth"}

Alternatively, we can adopt the ratio-based estimator
$\widehat q = \arg\min_{1 \le b \le \bar{q}} \text{ER}(b)$ proposed in
[@avarucci2022main], where $$\begin{aligned}
\label{eq:er}
\text{ER}(b) = \left( \sum_{k = -m}^m \widehat{\mu}_{x, b + 1}(\omega_k) \right)^{-1} \left( \sum_{k = -m}^m \widehat{\mu}_{x, b}(\omega_k) \right).
\end{aligned}$$

These methods are readily modified to select the number of factors $r$
under the restricted factor model
in [\[eq:static\]](#eq:static){reference-type="eqref"
reference="eq:static"}, by replacing
$(2m + 1)^{-1} \sum_{k = -m}^m \widehat{\mu}_{x, j}(\omega_k)$ with
$\widehat\mu_{x, j}$, the $j$-th largest eigenvalues of the sample
covariance matrix $\widehat{\bm\Gamma}_x(0)$. We refer to [@bai2002] and
[@alessi2010improved] for the discussion of the information
criterion-based method in this setting, and [@ahn2013eigenvalue] for
that of the eigenvalue ratio-based method.

## Threshold $\mathfrak{t}$ {#sec:tuning:thresh}

Motivated by [@liu2021simultaneous], we propose a method for data-driven
selection of the threshold $\mathfrak{t}$, which is applied to the
estimators of $\mathbf A_\ell, \, 1 \le \ell \le d$, $\bm\Delta$ or
$\bm\Omega$ for estimating the edge sets of
$\mathcal N^{\text{\upshape{G}}}$, $\mathcal N^{\text{\upshape{C}}}$ or
$\mathcal N^{\text{\upshape{L}}}$, respectively, see
also [\[eq:threshold\]](#eq:threshold){reference-type="eqref"
reference="eq:threshold"}.

Let $\mathbf B = [b_{ij}] \in R^{m \times n}$ denote a matrix for which
a threshold is to be selected, i.e. $\mathbf B$ may be either
$\widehat{\bm\beta} = [\widehat{\mathbf A}_1, \ldots, \widehat{\mathbf A}_d]^\top$,
$\widehat{\bm\Delta}_0$ ($\widehat{\bm\Delta}$ with diagonals set to
zero), or $\widehat{\bm\Omega}_0$ ($\widehat{\bm\Omega}$ with diagonals
set to zero), obtained from Steps 2 and 3 of FNETS. We work with
$\widehat{\bm\Delta}_0$ and $\widehat{\bm\Omega}_0$ since we do not
threshold the diagonal entries of $\widehat{\bm\Delta}$ and
$\widehat{\bm\Omega}$. As such estimators have been shown to achieve
consistency in $\ell_\infty$-norm, we expect there exists a large gap
between the entries of $\mathbf B$ corresponding to true positives and
false positives. Further, it is expected that the number of edges
reduces at a faster rate when increasing the threshold from $0$ towards
this (unknown) gap, compared to when increasing the threshold from the
gap to $\vert \mathbf B \vert_\infty$. Therefore, we propose to identify
this gap by casting the problem as that of locating a single change
point in the trend of the ratio of edges to non-edges, $$\begin{aligned}
\text{Ratio}_k = \frac{\vert \mathbf B(\mathfrak{t}_k) \vert_0}{ \max( N - \vert \mathbf B(\mathfrak{t}_k) \vert_0, 1) }, 
\quad k = 1, \dots, M.
\end{aligned}$$ Here,
$\mathbf B(\mathfrak{t}) = [b_{ij} \cdot \mathbb{I}_{\{\vert b_{ij} \vert > \mathfrak{t} \}}]$,
$\vert \mathbf B(\mathfrak{t}) \vert_0 = \sum_{i = 1}^{m_1} \sum_{j = 1}^{m_2} \mathbb{I}_{\{\vert b_{ij} \vert > \mathfrak{t} \}}$
and
$\{\mathfrak{t}_k, \, 1 \le k \le M: \, 0 = \mathfrak{t}_1 < \mathfrak{t}_2 < \dots < \mathfrak{t}_M = \vert \mathbf B \vert_\infty\}$
denotes a sequence of candidate threshold values. We recommend using an
exponentially growing sequence for $\{\mathfrak{t}_k\}_{k = 1}^M$ since
the size of the false positive entries tends to be very small. The
quantity $N$ in the denominator of Ratio$_k$ is set as $N = p^2d$ when
$\mathbf B = \widehat{\bm\beta}$, and $N = p(p - 1)$ when
$\mathbf B = \widehat{\bm\Delta}_0$ or
$\mathbf B = \widehat{\bm\Omega}_0$. Then, from the difference quotient
$$\begin{aligned}
\text{Diff}_k = \frac{\text{Ratio}_k - \text{Ratio}_{k - 1}}{\mathfrak{t}_k - \mathfrak{t}_{k - 1}},
\quad k = 2, \ldots, M,
\end{aligned}$$ we compute the cumulative sum (CUSUM) statistic
$$\begin{aligned}
\text{CUSUM}_k = \sqrt{\frac{k (M - k)}{M}} \left\vert \frac{1}{k} \sum_{l = 2}^k \text{Diff}_l - \frac{1}{M - k} \sum_{l = k + 1}^M \text{Diff}_l \right\vert, \quad k = 2, \ldots, M-1,
\end{aligned}$$ and select
$\mathfrak{t}_{\text{ada}} = \mathfrak{t}_{k^*}$ with
$k^* = {\arg\max}_{2 \le k \le M - 1} \text{CUSUM}_k$. For illustration,
Figure [4](#fig:thresh){reference-type="ref" reference="fig:thresh"}
plots Ratio$_k$ and CUSUM$_k$ against candidate thresholds for the
dataset simulated in [Data simulation](#sec:package:data).

<figure id="fig:thresh">
<p><embed src="figs/threshold1.pdf" /> <embed
src="figs/threshold2.pdf" /></p>
<figcaption>Ratio<span class="math inline"><sub><em>k</em></sub></span>
(left) and CUSUM<span class="math inline"><sub><em>k</em></sub></span>
(right) plotted against <span
class="math inline">𝔱<sub><em>k</em></sub></span> when <span
class="math inline">$\mathbf B =
\widehat{\bm\beta}^{\text{\upshape{las}}}$</span> obtained from the data
simulated in <a href="#sec:package:data">Data simulation</a> with <span
class="math inline"><em>n</em> = 500</span> and <span
class="math inline"><em>p</em> = 50</span>, as a Lasso estimator of the
VAR parameter matrix, with the selected <span
class="math inline">𝔱<sub>ada</sub></span> denoted by the vertical
lines.</figcaption>
</figure>

## VAR order $d$, $\lambda$ and $\eta$ {#sec:order:lambda}

[Step 2](#sec:step:two) and [Step 3](#sec:step:three) of the network
estimation methodology of FNETS involve the selection of the tuning
parameters $\lambda$ and $\eta$
(see [\[eq:lasso\]](#eq:lasso){reference-type="eqref"
reference="eq:lasso"}, [\[eq:ds\]](#eq:ds){reference-type="eqref"
reference="eq:ds"} and [\[eq:clime\]](#eq:clime){reference-type="eqref"
reference="eq:clime"}) and the VAR order $d$. While there exist a
variety of methods available for VAR order selection in fixed dimensions
[@lutk Chapter 4], the data-driven selection of $d$ in high dimensions
remains largely unaddressed with a few exceptions
[@nicholson2020high; @krampe2021; @zheng2022interpretable]. We suggest
two methods for jointly selecting $\lambda$ and $d$ for Step 2. The
first method is also applicable for selecting $\eta$ in Step 3.

### Cross validation {#sec:tuning:cv}

Cross validation (CV) methods have been popularly adopted for tuning
parameter and model selection. [@bergmeir2018note] study the usage of a
conventional CV procedure that randomly partitions the data, in the time
series settings when the model is correctly specified. However, such
arguments do not apply to our problem since the VAR process is latent.
Instead, we propose to adopt a modified CV procedure that bears
resemblance to out-of-sample evaluation or rolling forecasting
validation [@wang2021rate], for simultaneously selecting $d$ and
$\lambda$ in Step 2. For this, the data is partitioned into $L$ folds,
$\mathcal I_l = \{n^\circ_l + 1, \ldots, n^\circ_{l + 1}\}$ with
$n^\circ_l = \min(l \lceil n/L \rceil, n), \, 1 \le l \le L$, and each
fold is split into a training set
$\mathcal I^{\text{train}}_l = \{n^\circ_l + 1, \ldots, \lceil (n^\circ_l + n^\circ_{l + 1})/2 \rceil\}$
and a test set
$\mathcal I^{\text{test}}_l = \mathcal I_l \setminus \mathcal I^{\text{train}}_l$.
On each fold, $\bm\beta$ is estimated from
$\{\mathbf X_t, \, t \in \mathcal I^{\text{train}}_l\}$ as either the
Lasso [\[eq:lasso\]](#eq:lasso){reference-type="eqref"
reference="eq:lasso"} or the Dantzig
selector [\[eq:ds\]](#eq:ds){reference-type="eqref" reference="eq:ds"}
estimators with $\lambda$ as the tuning parameter and some $b$ as the
VAR order, say $\widehat{\bm\beta}^{\text{train}}_l(\lambda, b)$, using
which we compute the CV measure $$\begin{aligned}
\text{CV}(\lambda, b) &= \sum_{l = 1}^L \mathsf{tr}\left( \widehat{\bm\Gamma}^{\text{test}}_{\xi, l}(0) - (\widehat{\bm\beta}_l^{\text{train}}(\lambda, b))^\top \widehat{\mathbf g}_l^{\text{test}}(b) -  
\right. \\  & \qquad \qquad \qquad \left. 
(\widehat{\mathbf g}_l^{\text{test}}(b))^\top \widehat{\bm\beta}_l^{\text{train}}(\lambda, b)
+  (\widehat{\bm\beta}_l^{\text{train}}(\lambda, b))^\top \widehat{\mathbf G}_l^{\text{test}}(b)
\widehat{\bm\beta}_l^{\text{train}}(\lambda, b) \right),
\end{aligned}$$ where
$\widehat{\bm\Gamma}_{\xi, l}^{\text{test}}(\ell), \widehat{\mathbf G}_l^{\text{test}}(b)$
and $\widehat{\mathbf g}_l^{\text{test}}(b)$ are generated analogously
as $\widehat{\bm\Gamma}_\xi(\ell)$, $\widehat{\mathbf G}(b)$ and
$\widehat{\mathbf g}(b)$, respectively, from the test set
$\{\mathbf X_t, \, t \in \mathcal I^{\text{test}}_l\}$. Although we do
not directly observe $\bm\xi_t$, the measure $\text{CV}(\lambda, b)$
gives an approximation of the prediction error. Then, we select
$(\widehat{\lambda}, \widehat{d}) = \arg\min_{\lambda \in \Lambda, 1 \le b \le \bar{d}} \text{CV} (\lambda, b)$,
where $\Lambda$ is a grid of values for $\lambda$, and $\bar{d} \ge 1$
is a pre-determined upper bound on the VAR order. A similar approach is
taken for the selection of $\eta$ with a Burg matrix divergence-based CV
measure: $$\begin{aligned}
\text{CV}(\eta) = \sum_{l = 1}^L \mathsf{tr}\left( \widehat{\bm\Delta}_l^{\text{train}}(\eta) \widehat{\bm\Gamma}_l^{\text{test}} \right) - \log \left\vert \widehat{\bm\Delta}_l^{\text{train}}(\eta) \widehat{\bm\Gamma}_l^{\text{test}} \right\vert - p.
\end{aligned}$$ Here, $\widehat{\bm\Delta}_l^{\text{train}}(\eta)$
denotes the estimator of $\bm\Delta$ with $\eta$ as the tuning parameter
from $\{\mathbf X_t, \, t \in \mathcal I^{\text{train}}_l\}$, and
$\widehat{\bm\Gamma}_l^{\text{test}}$ the estimator of $\bm\Gamma$ from
$\{\mathbf X_t, \, t \in \mathcal I^{\text{test}}_l\}$, see [Step
3](#sec:step:three) for the descriptions of the estimators. In the
numerical results reported in [Simulations](#sec:sim), the sample size
is relatively small (ranging between $n = 200$ and $n = 500$ while
$p \in \{50, 100, 200\}$ and the number of parameters increasing with
$p^2$), and we set $L = 1$ which returns reasonably good performance.
When more observations are available, relative to the dimensionality, we
may use the number of folds greater than one.

### Extended Bayesian information criterion {#sec:tuning:ebic}

Alternatively, to select the pair $(\lambda, d)$ in Step 2, we propose
to use the extended Bayesian information criterion (eBIC) of
[@chen2008extended], originally proposed for variable selection in
high-dimensional linear regression. Let
$\widetilde{\bm\beta}(\lambda, b, \mathfrak{t}_{\text{ada}})$ denote the
thresholded version of $\widehat{\bm\beta}(\lambda, b)$ as
in [\[eq:threshold\]](#eq:threshold){reference-type="eqref"
reference="eq:threshold"} with the threshold $\mathfrak{t}_{\text{ada}}$
chosen as described in [Threshold $\mathfrak{t}$](#sec:tuning:thresh).
Then, letting
$s(\lambda, b) = \vert \widetilde{\bm\beta}(\lambda, b, \mathfrak{t}_{\text{ada}}) \vert_0$,
we define $$\begin{aligned}
\label{eq:ebic}
\text{eBIC}_{\alpha} (\lambda, b) &= \frac{n}{2} \log \left( \mathcal L(\lambda, b) \right) +  s(\lambda, b) \log(n) + 2 \alpha  \log{ bp^2 \choose s(\lambda, b)}, \quad \text{where}
\\
\mathcal{L}(\lambda, b) &= \mathsf{tr}\left( \widehat{\mathbf G}(b) - (\widetilde{\bm\beta}(\lambda, b))^\top \widehat{\mathbf g}(b) - (\widehat{\mathbf g}(b))^\top \widetilde{\bm\beta}(\lambda, b) + (\widetilde{\bm\beta}(\lambda, b))^\top \widehat{\mathbf G}(b) \widetilde{\bm\beta}(\lambda, b) \right).
	\nonumber
\end{aligned}$$ Then, we select
$(\widehat\lambda, \widehat d) = \arg\min_{\lambda \in \Lambda, 1 \le b \le \bar{d}} \text{eBIC}_{\alpha} (\lambda, b)$.
The constant $\alpha \in (0, 1)$ determines the degree of penalisation
which may be chosen from the relationship between $n$ and $p$.
Preliminary simulations suggest that $\alpha = 0$ is a suitable choice
for the dimensions $(n, p)$ considered in our numerical studies.

## Other tuning parameters

Motivated by theoretical results reported in [@barigozzi2022fnets], we
select the kernel bandwidth for Step 1 of FNETS as
$m = \lfloor 4 (n/\log(n))^{1/3} \rfloor$. In forecasting the
factor-driven component as
in [\[eq:chi:va\]](#eq:chi:va){reference-type="eqref"
reference="eq:chi:va"}, we set the truncation lag at $K = 20$, as it is
expected that the elements of $\mathbf B_\ell$ decay rapidly as $\ell$
increases for short-memory processes.

# Package overview {#sec:package}

fnets is available from the Comprehensive R Archive Network (CRAN). The
main function, fnets, implements the FNETS method for the input data and
returns an object of S3 class fnets. fnets.var implements Step 2 of the
FNETS methodology estimating the VAR parameters only, and is applicable
directly for VAR modelling of high-dimensional time series; its outputs
are of class fnets. fnets.factor.model performs factor modelling under
either of the two models [\[eq:gdfm\]](#eq:gdfm){reference-type="eqref"
reference="eq:gdfm"}
and [\[eq:static\]](#eq:static){reference-type="eqref"
reference="eq:static"}, and returns an object of class fm. We provide
predict methods for the objects of classes and fm, and a plot method for
the fnets class objects. Prior to using these functions to fit VAR
models, we recommend to perform a unit root test and, if necessary,
transform the time series such that it is stationary. In this section,
we demonstrate how to use the functions included with the package.

## Data simulation {#sec:package:data}

For illustration, we generate an example dataset of $n = 500$ and
$p = 50$, following the model described
in [\[eq:model\]](#eq:model){reference-type="eqref"
reference="eq:model"}. fnets provides functions for this purpose. For
given $n$ and $p$, the function sim.var generates the VAR($1$) process
following [\[eq:idio:var\]](#eq:idio:var){reference-type="eqref"
reference="eq:idio:var"} with $d = 1$, $\bm\Gamma$ as supplied to the
function ($\bm\Gamma = \mathbf I$ by default), and $\mathbf A_1$
generated as described in [Simulations](#sec:sim). The function
sim.unrestricted generates the factor-driven component under the
unrestricted factor model
in [\[eq:gdfm\]](#eq:gdfm){reference-type="eqref" reference="eq:gdfm"}
with $q$ dynamic factors ($q = 2$ by default) and the filter
$\mathcal B(L)$ generated as in
model [\[m:ar\]](#m:ar){reference-type="ref" reference="m:ar"} of
[Simulations](#sec:sim).

::: example
set.seed(111) n \<- 500 p \<- 50 x \<- sim.var(n,
p)$data + sim.unrestricted(n, p)$data
:::

Throughout this section, we use the generated dataset for demonstrating
the use of fnets, unless specified otherwise. There also exists
sim.restricted which generates the factor-driven component under the
restricted factor model
in [\[eq:static\]](#eq:static){reference-type="eqref"
reference="eq:static"}. For all data simulation functions, the default
is to use the standard normal distribution when generating $\mathbf u_t$
and $\bm\varepsilon_t$. However, by specifying the argument heavy =
TRUE, the innovations are generated from $\sqrt{3/5} \cdot t_5$, the
$t$-distribution with $5$ degrees of freedom scaled to have unit
variance. The package also comes attached with pre-generated datasets
data.restricted and data.unrestricted.

## Calling fnets with default parameters

The function fnets can be called with the $n \times p$ data matrix x as
the only input, which sets all other arguments to their default choices.
It then performs the factor-adjustment under the unrestricted model
in [\[eq:gdfm\]](#eq:gdfm){reference-type="eqref" reference="eq:gdfm"}
with $q$ estimated by minimising the IC
in [\[eq:ic\]](#eq:ic){reference-type="eqref" reference="eq:ic"}. The
VAR parameter matrix is estimated via the Lasso estimator
in [\[eq:lasso\]](#eq:lasso){reference-type="eqref"
reference="eq:lasso"}, with $d = 1$ as the VAR order, and the tuning
parameters $\lambda$ and $\eta$ chosen via CV, without any thresholding
step. This returns an object of class fnets whose entries are described
in Table [1](#table:output){reference-type="ref"
reference="table:output"}.

::: example
fnets(x)

Factor-adjusted vector autoregressive model with n: 500, p: 50
Factor-driven common component --------- Factor model: unrestricted
Factor number: 2 Factor number selection method: ic Information
criterion: IC5 Idiosyncratic VAR component --------- VAR order: 1 VAR
estimation method: lasso Tuning method: cv Threshold: FALSE Non-zero
entries: 95/2500 Long-run partial correlations --------- LRPC: TRUE
:::

::: {#table:output}
                     Name Description                                                                                            Type
  ----------------------- ------------------------------------------------------------------------------------------------------ ---------
    1-1 (lr)2-2 (lr)3-3 q Factor number                                                                                          integer
                     spec Spectral density matrices for $\mathbf X_t$, $\bm\chi_t$ and $\bm\xi_t$ (when fm.restricted = FALSE)   list
                      acv Autocovariance matrices for $\mathbf X_t$, $\bm\chi_t$ and $\bm\xi_t$                                  list
                 loadings Estimates of $\mathbf B_\ell, \, 0 \le \ell \le K$ (when fm.restricted = FALSE)                        array
                          or $\bm\Lambda$ (when fm.restricted = TRUE)                                                            
                  factors Estimates of $\{\mathbf u_t\}$ (when fm.restricted = FALSE)                                            array
                          or $\{\mathbf F_t\}$ (when fm.restricted = TRUE)                                                       
                 idio.var Estimates of $\mathbf A_\ell, \, 1 \le \ell \le d$, and $\bm\Gamma$, and $d$ and $\lambda$ used        list
                     lrpc Estimates of $\bm\Delta$, $\bm\Omega$, (long-run) partial correlations and $\eta$ used                 list
                   mean.x Sample mean vector                                                                                     vector
               var.method Estimation method for $\mathbf A_\ell$ (input parameter)                                               string
                  do.lrpc Whether to estimate the long-run partial correlations (input parameter)                                Boolean
                  kern.bw Kernel bandwidth (when fm.restricted = FALSE, input parameter)                                         double

  : Entries of S3 objects of class fnets
:::

## Calling fnets with optional parameters

We can also specify the arguments of fnets to control how Steps 1--3 of
FNETS are to be performed. The full model call is as follows:

::: example
out \<- fnets(x, center = TRUE, fm.restricted = FALSE, q = c(\"ic\",
\"er\"), ic.op = NULL, kern.bw = NULL, common.args =
list(factor.var.order = NULL, max.var.order = NULL, trunc.lags = 20,
n.perm = 10), var.order = 1, var.method = c(\"lasso\", \"ds\"), var.args
= list(n.iter = NULL, n.cores = min(parallel::detectCores() - 1, 3)),
do.threshold = FALSE, do.lrpc = TRUE, lrpc.adaptive = FALSE, tuning.args
= list(tuning = c(\"cv\", \"bic\"), n.folds = 1, penalty = NULL,
path.length = 10) )
:::

Here, we discuss a selection of input arguments. The center argument
will de-mean the input. fm.restricted determines whether to perform the
factor-adjustment under the restricted factor model
in [\[eq:static\]](#eq:static){reference-type="eqref"
reference="eq:static"} or not. If the number of factors is known, we can
specify q with a non-negative integer. Otherwise, it can be set as
\"ic\" or \"er\", which specifies
either [\[eq:ic\]](#eq:ic){reference-type="eqref" reference="eq:ic"}
or [\[eq:er\]](#eq:er){reference-type="eqref" reference="eq:er"} as the
factor number estimator, respectively. When q = \"ic\", setting the
argument ic.op as an integer between $1$ and $6$ specifies the choice of
the IC (see [Appendix A](#sec:factornumber)) where the default is ic.op
= 5. kern.bw takes a positive integer which specifies the bandwidth to
be used in Step 1 of FNETS. The list common.args specifies arguments for
estimating $\mathbf B_\ell$ and $\mathbf u_t$
under [\[eq:gdfm\]](#eq:gdfm){reference-type="eqref"
reference="eq:gdfm"}, and relates to the low-rank VAR representation of
$\bm\chi_t$ under the unrestricted factor model. var.order specifies a
vector of positive integers to be considered in VAR order selection.
var.method determines the method for VAR parameter estimation, which can
be either \"lasso\" (for the estimator
in [\[eq:lasso\]](#eq:lasso){reference-type="eqref"
reference="eq:lasso"}) or \"ds\" (for that
in [\[eq:ds\]](#eq:ds){reference-type="eqref" reference="eq:ds"}). The
list var.args takes additional parameters for Step 2 of FNETS, such as
the number of gradient descent steps ( n.iter, when var.method =
\"lasso\") or the number of cores to use for parallel computing (
n.cores, when var.method = \"ds\"). do.threshold specifies whether to
threshold the estimators of $\mathbf A_\ell, \, 1 \le \ell \le d$,
$\bm\Delta$ and $\bm\Omega$. It is possible to perform Steps 1--2 of
FNETS only without estimating $\bm\Delta$ and $\bm\Omega$ by setting
do.lrpc = FALSE. If do.lrpc = TRUE, lrpc.adaptive specifies whether to
use the non-adaptive estimator
in [\[eq:clime\]](#eq:clime){reference-type="eqref"
reference="eq:clime"} or the ACLIME estimator. The list tuning.args
supplies arguments to the CV or eBIC procedures, including the number of
folds $L$ ( n.folds), the eBIC parameter $\alpha$ ( penalty,
see [\[eq:ebic\]](#eq:ebic){reference-type="eqref" reference="eq:ebic"})
and the length of the grid of values for $\lambda$ and/or $\eta$ (
path.length). Finally, it is possible to set only a subset of the
arguments of common.args, var.args and tuning.args whereby the
unspecified arguments are set to their default values.

The factor adjustment (Step 1) and VAR parameter estimation (Step 2)
functionalities can be accessed individually by calling
fnets.factor.model and fnets.var, respectively. The latter is equivalent
to calling fnets with q = 0 and do.lrpc = FALSE. The former returns an
object of class fm which contains the entries of the fnets object in
Table [1](#table:output){reference-type="ref" reference="table:output"}
that relate to the factor-driven component only.

## Network visualisation {#sec:ex:network}

Using the plot method available for the objects of class fnets, we can
visualise the Granger network $\mathcal N^{\text{\upshape{G}}}$ induced
by the estimated VAR parameter matrices (see the left panel of
Figure [\[figure:heatmap\]](#figure:heatmap){reference-type="ref"
reference="figure:heatmap"}):

::: example
plot(out, type = \"granger\", display = \"network\")
:::

With display = \"network\", the function plots an igraph object from the
[igraph](https://CRAN.R-project.org/package=igraph) package [@igraph].
Setting the argument type to \"pc\" or \"lrpc\", we can visualise
$\mathcal N^{\text{\upshape{C}}}$ given by the partial correlations of
VAR innovations or $\mathcal N^{\text{\upshape{L}}}$ given by the
long-run partial correlations of $\bm\xi_t$. By setting display =
\"heatmap\", we can visualise the networks as a heat map instead, with
colour indicating edge weights. This plot relies on the
[fields](https://CRAN.R-project.org/package=fields) package [@fields]
and [RColorBrewer](https://CRAN.R-project.org/package=RColorBrewer)
[@RColorBrewer]. We plot $\mathcal N^{\text{\upshape{L}}}$ as a heat map
in the right panel of
Figure [\[figure:heatmap\]](#figure:heatmap){reference-type="ref"
reference="figure:heatmap"} using the following command:

::: example
plot(out, type = \"lrpc\", display = \"heatmap\")
:::

It is also possible to directly produce an igraph object from the
objects of class fnets via the network method as:

::: example
g \<- network(out, type = \"granger\")$network
plot(g, layout = igraph::layout_in_circle(g), 
     vertex.color = grDevices::rainbow(1, alpha = 0.2), vertex.label = NA, 
     main = "Granger causal network")
\end{example}
This produces a plot identical to the left panel of Figure~\ref{figure:heatmap} using the \texorpdfstring%
{{\normalfont\ttfamily\hyphenchar\font=-1 igraph}}%
{igraph} object \texorpdfstring%
{{\normalfont\ttfamily\hyphenchar\font=-1 g}}%
{g}.

\begin{figure}
\centering
\includegraphics[width = .45\textwidth]{figs/Granger.pdf}
\includegraphics[width = .45\textwidth]{figs/Heatmap.pdf}
\caption{Estimated networks for simulated data described in \hyperref[sec:package:data]{Data simulation}.
Left: Granger causal network~$\mathcal N^{\text{\upshape{G}}}$. A directed arrow from node $i$ to node $i'$ indicates that variable $i$ Granger causes node $i'$, and the width of the arrow indicates the edge weight or estimated coefficient.
Right: Long-run partial correlation network $\mathcal N^{\text{\upshape{L}}}$ with edge weights (i.e.\ partial correlations) visualised by the colour.}
\label{figure:heatmap}
\end{figure}

\subsection{Forecasting}

The \texorpdfstring%
{{\normalfont\ttfamily\hyphenchar\font=-1 fnets}}%
{fnets} objects also implement the \texorpdfstring%
{{\normalfont\ttfamily\hyphenchar\font=-1 predict}}%
{predict} method with which we can forecast the input data \verb+n.ahead+ steps.
For example, we can produce a one-step ahead forecast of$X\_n + 1$as
\begin{example}
pr <- predict(out, n.ahead = 1, fc.restricted = TRUE)
pr$forecast
:::

The argument fc.restricted specifies whether to use the estimator
$\widehat{\bm\chi}^{\text{\upshape {res}}}_{n + h \vert n}$
in [\[eq:chi:static\]](#eq:chi:static){reference-type="eqref"
reference="eq:chi:static"} generated under a restricted factor
model [\[eq:static\]](#eq:static){reference-type="eqref"
reference="eq:static"}, or
$\widehat{\bm\chi}^{\text{\upshape{unr}}}_{n + h \vert n}$
in [\[eq:chi:va\]](#eq:chi:va){reference-type="eqref"
reference="eq:chi:va"} generated without such a restriction.
Table [2](#table:output:predict){reference-type="ref"
reference="table:output:predict"} lists the entries from the output from
`predict.fnets`. We can similarly produce forecasts from `fnets` objects
output from `fnets.var`, or `fm` objects from `fnets.factor.model`.

::: {#table:output:predict}
                            Name Description                                                                    Type
  ------------------------------ ------------------------------------------------------------------------------ --------
    1-1 (lr)2-2 (lr)3-3 forecast $h \times p$ matrix containing the $h$-step ahead forecasts of $\mathbf X_t$   matrix
                  common.predict A list containing                                                              list
                            \$is $n \times p$ matrix containing the in-sample estimator of $\bm\chi_t$          
                            \$fc $h \times p$ matrix containing the $h$-step ahead forecasts of $\bm\chi_t$     
                             \$h Input parameter                                                                
                             \$r Factor number (only produced when fc.restricted = TRUE)                        
                    idio.predict A list containing is, fc and h, see common.predict                             list
                          mean.x Sample mean vector                                                             vector

  : Entries of the output from predict.fnets
:::

## Factor number estimation

It may be of interest to estimate the number of factors (if any) in the
input dataset, independent of any estimation procedure. The function
factor.number provides access to the two methods for selecting $q$
described in [Factor numbers $q$ and $r$](#sec:tuning:factor). The
following code calls the information criterion-based factor number
estimation method in [\[eq:ic\]](#eq:ic){reference-type="eqref"
reference="eq:ic"}, and prints the output:

::: example
fn \<- factor.number(x, fm.restricted = FALSE) print(fn)

Factor number selection Factor model: unrestricted Method: Information
criterion Number of factors: IC1: 2 IC2: 2 IC3: 3 IC4: 2 IC5: 2 IC6: 2
:::

Calling plot(fn) returns Figure [3](#fig:qplot){reference-type="ref"
reference="fig:qplot"} which visualises the factor number estimators
from six information criteria implemented. Alternatively, we call the
eigenvalue ratio-based method
in [\[eq:er\]](#eq:er){reference-type="eqref" reference="eq:er"} as

::: example
fn \<- factor.number(x, method = \"er\", fm.restricted = FALSE)
:::

In this case, plot(fn) produces a plot of $\text{ER}(b)$ against the
candidate factor number $b \in \{1, \ldots, \bar{q}\}$.

## Visualisation of tuning parameter selection procedures {#sec:package:order}

The method for threshold selection discussed in [Threshold
$\mathfrak{t}$](#sec:tuning:thresh) is implemented by the threshold
function, which returns objects of threshold class supported by print
and plot methods.

::: example
th \<- threshold(out$idio.var$beta) th

Thresholded matrix Threshold: 0.0297308643 Non-zero entries: 62/2500
:::

The call plot(th) generates Figure [4](#fig:thresh){reference-type="ref"
reference="fig:thresh"}. Additionally, we provide tools for visualising
the tuning parameter selection results adopted in Steps 2 and 3 of FNETS
(see [VAR order $d$, $\lambda$ and $\eta$](#sec:order:lambda)). These
tools are accessible from both fnets and fnets.var by calling the plot
method with the argument display = \"tuning\", e.g. 

This generates the two plots reported in
Figure [5](#figure:order_ex){reference-type="ref"
reference="figure:order_ex"} which visualise the CV errors computed as
described in [Cross validation](#sec:tuning:cv) and, in particular, the
left plot shows that the VAR order is correctly selected by this
approach. When tuning.args contains tuning = \"bic\", the results from
the eBIC method described in [Extended Bayesian information
criterion](#sec:tuning:ebic) adopted in Step 2, is similarly visualised
in place of the left panel of
Figure [5](#figure:order_ex){reference-type="ref"
reference="figure:order_ex"}.

![ Plots of CV$(\lambda, b)$ against $\lambda$ with $b \in \{1, 2, 3\}$
(left) and CV$(\eta)$ against $\eta$ (right). Vertical lines denote
where the minimum CV measure is attained with respect to $\lambda$ and
$\eta$, respectively.](figs/order_example.pdf){#figure:order_ex
width=".8\\textwidth"}

# Simulations {#sec:sim}

[@barigozzi2022fnets] provide comprehensive simulation results on the
estimation and forecasting performance of FNETS in comparison with
competing methodologies. Therefore in this paper, we focus on assessing
the performance of the methods for selecting tuning parameters such as
the threshold and VAR order discussed in [Tuning parameter
selection](#sec:tuning). Additionally in
[Appendix B](#sec:sim:adaptive), we compare the adaptive and the
non-adaptive estimators in estimating $\bm\Delta$ and also investigate
how their performance is carried over to estimating $\bm\Omega$.

## Settings

We consider the following data generating processes for the
factor-driven component $\bm\chi_t$:

1.  []{#m:ar label="m:ar"} Taken from [@forni2017dynamic], $\chi_{it}$
    is generated as a sum of AR processes
    $\chi_{it} = \sum_{j = 1}^q a_{ij} (1 - \alpha_{ij} L)^{-1} u_{jt}$
    with $q = 2$, where
    $u_{jt} \sim_{\text{\upshape iid}} \mathcal N(0, 1)$,
    $a_{ij} \sim_{\text{\upshape iid}} \mathcal U[-1, 1]$ and
    $\alpha_{ij} \sim_{\text{\upshape iid}} \mathcal U[-0.8, 0.8]$ with
    $\mathcal U[a, b]$ denoting a uniform distribution. Then,
    $\bm\chi_t$ does not admit a static representation
    in [\[eq:static\]](#eq:static){reference-type="eqref"
    reference="eq:static"}.

2.  []{#m:oracle label="m:oracle"} $\bm\chi_t = \mathbf 0$, i.e. the VAR
    process is directly observed as $\mathbf X_t = \bm\xi_t$.

For generating a VAR($d$) process $\bm\xi_t$, we first generate a
directed Erdős-Rényi random graph
$\mathcal N = (\mathcal V, \mathcal E)$ on
$\mathcal V = \{1, \ldots, p\}$ with the link probability $1/p$, and set
entries of $\mathbf A_d$ such that $A_{d, ii'} = 0.275$ when
$(i, i') \in \mathcal E$ and $A_{d, ii'} = 0$ otherwise. Also, we set
$\mathbf A_\ell = \mathbf O$ for $\ell < d$. The VAR innovations are
generated as below.

1.  []{#e:one label="e:one"} Gaussian with the covariance matrix
    $\bm\Gamma = \bm\Delta^{-1} = \mathbf I$.

2.  []{#e:four label="e:four"} Gaussian with the covariance matrix
    $\bm\Gamma = \bm\Delta^{-1}$ such that $\delta_{ii} = 1$,
    $\delta_{i, i + 1} = \delta_{i + 1, i} = 0.6$,
    $\delta_{i, i + 2} = \delta_{i + 2, i} = 0.3$, and
    $\delta_{ii'} = 0$ for $\vert i - i' \vert \ge 3$.

For each setting, we generate $100$ realisations.

## Results: Threshold selection

We assess the performance of the adaptive threshold. We generate
$\bm\chi_t$ as in [\[m:ar\]](#m:ar){reference-type="ref"
reference="m:ar"} and fix $d = 1$ for generating $\bm\xi_t$ and further,
treat $d$ as known. We consider
$(n, p) \in \{(200, 50), (200, 100), (500, 100), (500, 200)\}$. Then we
estimate $\bm\Omega$ using the thresholded Lasso estimator of
$\mathbf A_1$ (see [\[eq:lasso\]](#eq:lasso){reference-type="eqref"
reference="eq:lasso"}
and [\[eq:threshold\]](#eq:threshold){reference-type="eqref"
reference="eq:threshold"}) with two choices of thresholds,
$\mathfrak{t} = \mathfrak{t}_{\text{ada}}$ generated as described in
[Threshold $\mathfrak{t}$](#sec:tuning:thresh) and $\mathfrak{t} = 0$.
To assess the performance of
$\widehat{\bm\Omega} = [\widehat\omega_{ii'}]$ in recovering the support
of $\bm\Omega = [\omega_{ii'}]$,
i.e. $\{(i, i'): \, \omega_{ii'} \ne 0 \}$, we plot the receiver
operating characteristic (ROC) curves of the true positive rate (TPR)
against false positive rate (FPR), where $$\begin{aligned}
\text{TPR} = \frac{ \vert \{ (i, i'): \, \widehat\omega_{ii'} \ne 0 \text{ and } \omega_{ii'} \ne 0 \} \vert }{\vert \{ (i, i'): \, \omega_{ii'} \ne 0 \} \vert}
\quad \text{and} \quad
\text{FPR} = \frac{ \vert \{ (i, i'): \, \widehat\omega_{ii'} \ne 0 \text{ and } \omega_{ii'} = 0 \} \vert }{\vert \{ (i, i'): \, \omega_{ii'} = 0 \} \vert}.
\end{aligned}$$ Figure [6](#fig:sim:omegathresh){reference-type="ref"
reference="fig:sim:omegathresh"} plots the ROC curves averaged over
$100$ realisations when $\mathfrak{t} = \mathfrak{t}_{\text{ada}}$ and
$\mathfrak{t} = 0$. When $\bm\Delta = \mathbf I$
under [\[e:one\]](#e:one){reference-type="ref" reference="e:one"}, we
see little improvement from adopting $\mathfrak{t}_{\text{ada}}$ as the
support recovery performance is already good even without thresholding.
However, when $\bm\Delta \ne \mathbf I$
under [\[e:four\]](#e:four){reference-type="ref" reference="e:four"},
the adaptive threshold leads to improved support recovery especially
when the sample size is large.
Tables [3](#table:thresholdbeta){reference-type="ref"
reference="table:thresholdbeta"} and [4](#table:thresholdomega){reference-type="ref"
reference="table:thresholdomega"} in [Appendix C](#sec:appendix:sim)
additionally report the errors in estimating $\mathbf A_1$ and
$\bm\Omega$ with and without thresholding, where we see little change is
brought by thresholding. In summary, we conclude that the estimators
already perform reasonably well without thresholding, and the adaptive
threshold $\mathfrak{t}_{\text{ada}}$ brings marginal improvement in
support recovery which is of interest in network estimation.

![ROC curves of TPR against FPR for
$\widetilde{\bm\beta}(\mathfrak{t})$ [\[eq:threshold\]](#eq:threshold){reference-type="eqref"
reference="eq:threshold"} (with
$\widehat{\bm\beta} = \widehat{\bm\beta}^{\text{\upshape{las}}}$) when
$\mathfrak{t} =\mathfrak{t}_{ada}$ and $\mathfrak{t} = 0$ in recovering
the support of $\bm\Omega$, averaged over $100$ realisations. Vertical
lines indicate FPR $= 0.05$ ](figs/threshomega.pdf){#fig:sim:omegathresh
width=".8\\textwidth"}

## Results: VAR order selection {#sec:sim:order}

We compare the performance of the CV and eBIC methods proposed in [VAR
order $d, \lambda$ and $\eta$](#sec:order:lambda) for selecting the
order of the VAR process. Here, we consider the case when
$\bm\chi_t = \mathbf 0$
(setting [\[m:oracle\]](#m:oracle){reference-type="ref"
reference="m:oracle"}) and when $\bm\xi_t$ is generated
under [\[e:one\]](#e:one){reference-type="ref" reference="e:one"} with
$d \in \{1, 3\}$. We set
$(n, p) \in \{(200, 10), (200, 20), (500, 10), (500, 20)\}$ where the
range of $p$ is in line with the simulation studies conducted in the
relevant literature (see e.g. [@zheng2022interpretable]). We consider
$\{1, 2, 3, 4\}$ as the candidate VAR orders.
Figure [7](#fig:order){reference-type="ref" reference="fig:order"} and
Table [5](#table:order){reference-type="ref" reference="table:order"} in
[Appendix C](#sec:appendix:sim) show that CV works reasonably well
regardless of $d \in \{1, 3\}$, with slightly better performance
observed together with the DS estimator. On the other hand, eBIC tends
to over-estimate the VAR order when $d = 1$ while under-estimating it
when $d = 3$, and hence is less reliable compared to the CV method.

![Box plots of $\widehat{d} - d$ over $100$ realisations when the VAR
order is selected by the CV and eBIC methods in combination with the
Lasso [\[eq:lasso\]](#eq:lasso){reference-type="eqref"
reference="eq:lasso"} and the
DS [\[eq:ds\]](#eq:ds){reference-type="eqref" reference="eq:ds"}
estimators.](figs/order.pdf){#fig:order width=".8\\textwidth"}

# Real-world data example {#sec:real}

## Energy price data {#sec:real:energy}

Compared with physical commodities, electricity is more difficult to
store, and this results in high volatility and seasonality in spot
prices [@han2022extremal]. Global market deregulation has increased the
volume of electricity trading, which promotes the development of better
forecasting and risk management methods. We analyse a dataset of
node-specific prices in the PJM (Pennsylvania, New Jersey and Maryland)
power pool area in the United States, accessed using
[dataminer2.pjm.com](dataminer2.pjm.com){.uri}. There are four node
types in the panel, which are Zone, Aggregate, Hub, and Extra High
Voltage (EHV) (for definitions, names, and types of the $p = 50$ nodes,
see Tables [9](#table:data:info){reference-type="ref"
reference="table:data:info"}
and [8](#table:definitions){reference-type="ref"
reference="table:definitions"} in [Appendix D](#sec:real:data)). The
series we model is the sum of the real time congestion price and
marginal loss price or, equivalently, the difference between the spot
price at a given location and the overall system price, where the latter
can be thought of as an observed factor in the local spot price. These
are obtained as hourly prices and then averaged over each day as per
[@maciejowska2013forecasting]. We remove any short-term seasonality by
subtracting a separate mean for each day of the week. Since the energy
prices may take negative values, we adopt the inverse hyperbolic sine
transformation as in [@uniejewski2017variance] for variance
stabilisation.

## Network estimation {#sec:real:network}

We analyse the data collected between 01/01/2021 and 19/07/2021
($n = 200$). The information criterion
in [\[eq:ic\]](#eq:ic){reference-type="eqref" reference="eq:ic"} selects
a single factor ($\widehat q = 1$), and $\widehat d =1$ is selected by
CV. See
Figure [\[fig:real:lasso\]](#fig:real:lasso){reference-type="ref"
reference="fig:real:lasso"} for the heat maps visualising the three
networks $\mathcal N^{\text{\upshape{G}}}$,
$\mathcal N^{\text{\upshape{C}}}$ and $\mathcal N^{\text{\upshape{L}}}$
described in [Networks](#sec:networks), which are produced by fnets.

::: widefigure
  ----------------------------------------------------- --------------------------------------------------- ---------------------------------------------------
   ![image](figs/granger_1.pdf){width=".3\\textwidth"}   ![image](figs/delta_1.pdf){width=".3\\textwidth"}   ![image](figs/omega_1.pdf){width=".3\\textwidth"}
  ----------------------------------------------------- --------------------------------------------------- ---------------------------------------------------
:::

The non-zero entries of the VAR parameter matrix estimates tend to take
positive values, indicating that high energy prices are persistent and
spill over to other nodes. Considering the node types, Hub-type nodes
(blue) tend to have out-going edges to nodes of different types, which
reflects the behaviour of the electrical transmission system. Some
Zone-type nodes (red) have several in-coming edges from Aggregate-types
(green) and Hub-types, while EHV-types (purple) have few edges in
$\mathcal N^{\text{\upshape{G}}}$, which carries forward to
$\mathcal N^{\text{\upshape{L}}}$ where we observe that those Zone-type
nodes have strong long-run correlations with other nodes while EHV-types
do not.

# Summary {#sec:summary}

We introduce the R package fnets which implements the FNETS methodology
proposed by [@barigozzi2022fnets] for network estimation and forecasting
of high-dimensional time series exhibiting strong correlations. The
package further implements several data-driven methods for selecting
tuning parameters, and provides tools for high-dimensional time series
factor modelling under the GDFM. The efficacy of our package is
demonstrated on both real and simulated datasets.

# Appendix A: Information criteria for factor number selection {#sec:factornumber}

Here we list information criteria for factor number estimation which are
implemented in fnets and accessible by the functions fnets,
fnets.factor.model and factor.number by setting the argument ic.op at an
integer belonging to $\{1, \ldots, 6\}$. When fm.restricted = FALSE, we
have

-   $\left(\frac{1}{p} \sum _{j = b + 1}^p \frac{1}{2m + 1} \sum_{k = -m}^m \widehat{\mu}_{x, j}(\omega_k) \right)
     + b \cdot c \cdot( m^{-2} + \sqrt{m / n} + p^{-1}) \cdot \log(\min (p, m^2, \sqrt{n / m} ))$,

-   $\left(\frac{1}{p} \sum _{j = b + 1}^p \frac{1}{2m + 1} \sum_{k = -m}^m \widehat{\mu}_{x, j}(\omega_k) \right)
     + b \cdot c \cdot (\min(p, m^2, \sqrt{n / m}))^{-1/2}$,

-   $\left(\frac{1}{p} \sum _{j = b + 1}^p \frac{1}{2m + 1} \sum_{k = -m}^m \widehat{\mu}_{x, j}(\omega_k) \right) 
     + b \cdot c \cdot (\min(p, m^2, \sqrt{n / m}))^{-1} \cdot \log(\min(p, m^2, \sqrt{n / m}))$,

-   $\log \left(\frac{1}{p} \sum _{j = b + 1}^p \frac{1}{2m + 1} \sum_{k = -m}^m \widehat{\mu}_{x, j}(\omega_k) \right) 
     + b \cdot c \cdot( m^{-2} + \sqrt{m / n} + p^{-1}) \cdot \log(\min (p, m^2, \sqrt{n / m} ))$,

-   $\log \left(\frac{1}{p} \sum _{j = b + 1}^p \frac{1}{2m + 1} \sum_{k = -m}^m \widehat{\mu}_{x, j}(\omega_k) \right) 
     + b \cdot c \cdot (\min(p, m^2, \sqrt{n / m}))^{-1/2}$,

-   $\log \left(\frac{1}{p} \sum _{j = b + 1}^p \frac{1}{2m + 1} \sum_{k = -m}^m \widehat{\mu}_{x, j}(\omega_k) \right) 
     + b \cdot c \cdot (\min(p, m^2, \sqrt{n / m}))^{-1} \cdot \log(\min(p, m^2, \sqrt{n / m}))$
    .

When fm.restricted = TRUE, we use one of

-   $\left(\frac{1}{p} \sum _{j = b + 1}^p \widehat{\mu}_{x, j} \right)
     + b \cdot c \cdot (n + p) / (n p) \cdot \log(n p / (n + p))$,

-   $\left(\frac{1}{p} \sum _{j = b + 1}^p \widehat{\mu}_{x, j} \right)
     + b \cdot c \cdot (n + p) / (n p) \cdot \log(n p / (n + p))$,

-   $\left(\frac{1}{p} \sum _{j = b + 1}^p \widehat{\mu}_{x, j} \right) 
     + b \cdot c \cdot \log(\min(n, p)) / (\min(n, p))$,

-   $\log\left(\frac{1}{p} \sum _{j = b + 1}^p \widehat{\mu}_{x, j} \right)
     + b \cdot c \cdot (n + p) / (n p) \cdot \log(n p / (n + p))$,

-   $\log\left(\frac{1}{p} \sum _{j = b + 1}^p \widehat{\mu}_{x, j} \right)
     + b \cdot c \cdot (n + p) / (n p) \cdot \log(n p / (n + p))$,

-   $\log\left(\frac{1}{p} \sum _{j = b + 1}^p \widehat{\mu}_{x, j} \right) 
     + b \cdot c \cdot \log(\min(n, p)) / (\min(n, p))$.

Whether fm.restricted = FALSE or not, the default choice is ic.op = 5.

# Appendix B: ACLIME estimator {#sec:aclime}

We provide a detailed description of the adaptive extension of the CLIME
estimator of $\bm\Delta$
in [\[eq:clime\]](#eq:clime){reference-type="eqref"
reference="eq:clime"}, extending the methodology proposed in
[@cai2016estimating] for precision matrix estimation in the independent
setting. Let
$\widehat{\bm\Gamma}^* = \widehat{\bm\Gamma} + n^{-1}{\mathbf I}$ and
$\eta_1 = 2 \sqrt{{\log (p)} / {n}}$ .

1.  Let $\check{\bm\Delta}^{(1)} = [\check{\delta}_{ii'}^{(1)}]$ be the
    solution to $$\begin{aligned}
    \label{eq:adap:step1}
    & \check{\bm\Delta}^{(1)}_{\cdot i'}  = {\arg\min}_{\mathbf m \in R^p} \vert \mathbf m \vert_1 
    \quad \text{subject to} 
    \\
    & \left\vert ( \widehat{\bm\Gamma}^* \mathbf m - \mathbf e_{i'})_i \right\vert \le \eta_1 (\widehat{\gamma}_{ii} \vee \widehat{\gamma}_{i'i'})  m_{i'} \ \forall \ 1 \le i \le p \text{ \ and \ }  m_{i'} > 0, \nonumber
    \end{aligned}$$ for $i' = 1, \ldots, p$. Then we obtain truncated
    estimates $$\begin{aligned}
    	\widehat{\delta}_{ii}^{(1)} = \check{\delta}_{ii}^{(1)} \cdot  \mathbb{I}_{\{\vert \widehat{\gamma}_{ii} \vert \le \sqrt{n/\log(p)} \}} + 
    	 \sqrt{\frac{\log(p)}{n}} \cdot \mathbb{I}_{\{\vert \widehat{\gamma}_{ii} \vert > \sqrt{n/\log(p)} \}}.
    \end{aligned}$$

2.  We obtain $$\begin{aligned}
    %\label{eq:inv:deltaadap}
    \check{\bm\Delta}^{(2)}_{\cdot i'} & = {\arg\min}_{\mathbf m \in \mathbb{R}^p } \vert \mathbf m \vert_1 
    \quad \text{subject to} \quad 
    \left\vert ( \widehat{\bm\Gamma}^* \mathbf m - \mathbf e_{i'})_i \right\vert \le \eta_2 \sqrt{\widehat{\gamma}_{ii} \widehat{\delta}_{i'i'}^{(1)} } \quad \forall \ 1 \le i \le p,
    \end{aligned}$$ where $\eta_2 > 0$ is a tuning parameter. Since
    $\check{\bm\Delta}^{(2)}$ is not guaranteed to be symmetric, the
    final estimator is obtained after a symmetrisation step:
    $$\begin{aligned}
    \label{eq:delta:aclime}
    \widehat{\bm\Delta}_{ada} &= [\widehat\delta_{ii'}, \, 1 \le i, i' \le p]
    \text{ with } 
    \widehat\delta_{ii'}^{(2)} = \check\delta_{ii'}^{(2)} \cdot \mathbb{I}_{\{\vert \check\delta_{ii'}^{(2)} \vert
    \le \vert \check\delta_{i'i}^{(2)} \vert \}}
    + \check\delta_{i'i}^{(2)} \cdot \mathbb{I}_{\{\vert \check\delta_{i'i}^{(2)} \vert
    < \vert \check\delta_{ii'}^{(2)} \vert \}}.
    \end{aligned}$$

The constraints in
[\[eq:adap:step1\]](#eq:adap:step1){reference-type="eqref"
reference="eq:adap:step1"} incorporate the parameter in the right-hand
side. To use linear programming software to solve this, we formulate the
constraints for each $1\le i' \le p$ as $$\begin{aligned}
\forall 1\le i\le p, \quad ( (\widehat{\bm\Gamma}^* - \bm Q^{i'}) \mathbf m - \mathbf e_{i'})_i \le   0,\\
\forall 1\le i\le p, \quad -( (\widehat{\bm\Gamma}^* + \bm Q^{i'}) \mathbf m - \mathbf e_{i'})_i \le 0,\\
  m_{i'} > 0.
\end{aligned}$$ where $\bm Q^{i'}$ has entries
$q_{ii'} = \eta_1 (\widehat{\gamma}_{ii} \vee \widehat{\gamma}_{i'i'})$
in column $i'$ and $0$ elsewhere.

# Appendix C: Additional simulation results {#sec:appendix:sim}

## Threshold selection

Tables [3](#table:thresholdbeta){reference-type="ref"
reference="table:thresholdbeta"} and [4](#table:thresholdomega){reference-type="ref"
reference="table:thresholdomega"} report the errors in estimating
$\mathbf A_1$ and $\bm\Omega$ when the threshold
$\mathfrak{t} = \mathfrak{t}_{\text{ada}}$ or $\mathfrak{t} = 0$ is
applied to the estimator of $\mathbf A_1$ obtained by either the
Lasso [\[eq:lasso\]](#eq:lasso){reference-type="eqref"
reference="eq:lasso"} or the
DS [\[eq:ds\]](#eq:ds){reference-type="eqref" reference="eq:ds"}
estimators. With a matrix $\bm\gamma$ as an estimand we measure the
estimation error of its estimator $\widehat{\bm\gamma}$ using the
following (scaled) matrix norms: $$\begin{aligned}
L_F = \frac{\Vert \widehat{\bm\gamma} - \bm\gamma \Vert_F}{\Vert \bm\gamma \Vert_F} \quad \text{and} \quad L_2 = \frac{\Vert \widehat{\bm\gamma} - \bm\gamma \Vert}{\Vert \bm\gamma \Vert}.
\end{aligned}$$

::: {#table:thresholdbeta}
                                                                                                              $\mathfrak{t} = 0$                                                                                                      $\mathfrak{t}=\mathfrak{t}_{\text{ada}}$                                                                               
  ---------------------------------------------------------------------------------- ----- ----- --------------------------------------------- --------- --------- -------------------------------------------- --------- --------- --------------------------------------------- --------- --------- -------------------------------------------- --------- ---------
                                     4-9(lr)10-15                                                 $\widehat{\bm\beta}^{\text{\upshape{las}}}$                       $\widehat{\bm\beta}^{\text{\upshape{DS}}}$                       $\widehat{\bm\beta}^{\text{\upshape{las}}}$                       $\widehat{\bm\beta}^{\text{\upshape{DS}}}$            
                         4-6(lr)7-9 (lr)10-12 (lr)13-15 Model                          n     p                        TPR                        $L_F$     $L_2$                       TPR                        $L_F$     $L_2$                        TPR                        $L_F$     $L_2$                       TPR                        $L_F$     $L_2$
   1-3(lr)4-9 (lr)10-15 [\[e:one\]](#e:one){reference-type="ref" reference="e:one"}   200   50                      0.9681                      0.6234    0.7204                      0.8991                     0.4299    0.3747                      0.9413                      0.6226    0.7204                      0.6932                     0.4487    0.3960
                                                                                                                    (0.050)                     (0.081)   (0.118)                    (0.096)                     (0.280)   (0.225)                     (0.112)                     (0.088)   (0.121)                    (0.216)                     (0.256)   (0.206)
                                                                                      200   100                     0.9398                      0.6696    0.8113                      0.8810                     0.5772    0.4362                      0.8832                      0.6710    0.8132                      0.6491                     0.6025    0.4642
                                                                                                                    (0.091)                     (0.096)   (0.096)                    (0.094)                     (0.449)   (0.271)                     (0.182)                     (0.108)   (0.100)                    (0.246)                     (0.418)   (0.250)
                                                                                      500   100                     0.9990                      0.4648    0.6682                      0.9304                     0.2740    0.2604                      0.9971                      0.4608    0.6645                      0.7237                     0.2806    0.2699
                                                                                                                    (0.003)                     (0.054)   (0.094)                    (0.065)                     (0.158)   (0.138)                     (0.010)                     (0.056)   (0.095)                    (0.199)                     (0.133)   (0.111)
                                                                                      500   200                     0.9986                      0.5068    0.7729                      0.9167                     0.3680    0.3882                      0.9964                      0.5023    0.7637                      0.7095                     0.3889    0.4014
                                                                                                                    (0.003)                     (0.058)   (0.081)                    (0.076)                     (0.196)   (0.134)                     (0.006)                     (0.061)   (0.082)                    (0.256)                     (0.187)   (0.126)
            [\[e:four\]](#e:four){reference-type="ref" reference="e:four"}            200   50                      0.9595                      0.6375    0.7075                      0.8828                     0.4673    0.4280                      0.9442                      0.6356    0.7079                      0.6720                     0.4835    0.4433
                                                                                                                    (0.053)                     (0.077)   (0.094)                    (0.107)                     (0.324)   (0.255)                     (0.064)                     (0.079)   (0.096)                    (0.212)                     (0.303)   (0.241)
                                                                                      200   100                     0.9624                      0.6200    0.6909                      0.8093                     0.4519    0.4090                      0.9435                      0.6175    0.6913                      0.5903                     0.4765    0.4324
                                                                                                                    (0.072)                     (0.079)   (0.089)                    (0.100)                     (0.385)   (0.251)                     (0.093)                     (0.082)   (0.090)                    (0.182)                     (0.371)   (0.243)
                                                                                      500   100                     0.9970                      0.4657    0.5533                      0.9304                     0.3434    0.3621                      0.9958                      0.4638    0.5525                      0.8384                     0.3370    0.3634
                                                                                                                    (0.006)                     (0.056)   (0.076)                    (0.089)                     (0.158)   (0.153)                     (0.008)                     (0.058)   (0.077)                    (0.182)                     (0.140)   (0.144)
                                                                                      500   200                     0.9981                      0.4702    0.5658                      0.9205                     0.3684    0.3740                      0.9945                      0.4686    0.5665                      0.8154                     0.3663    0.3803
                                                                                                                    (0.003)                     (0.065)   (0.091)                    (0.088)                     (0.182)   (0.162)                     (0.014)                     (0.068)   (0.093)                    (0.205)                     (0.159)   (0.145)

  : Errors in estimating $\mathbf A_1$ with
  $\mathfrak{t} \in \{0, \mathfrak{t}_{\text{ada}}\}$ in combination
  with the Lasso [\[eq:lasso\]](#eq:lasso){reference-type="eqref"
  reference="eq:lasso"} and the
  DS [\[eq:ds\]](#eq:ds){reference-type="eqref" reference="eq:ds"}
  estimators, measured by $L_F$ and $L_2$, averaged over $100$
  realisations (with standard errors reported in brackets). We also
  report the average TPR when FPR $= 0.05$ and the corresponding
  standard error. See [Results: Threshold selection](#sec:sim:order) in
  the main text for further information.
:::

::: {#table:thresholdomega}
                                                                                                              $\mathfrak{t} = 0$                                                                                                      $\mathfrak{t}=\mathfrak{t}_{\text{ada}}$                                                                               
  ---------------------------------------------------------------------------------- ----- ----- --------------------------------------------- --------- --------- -------------------------------------------- --------- --------- --------------------------------------------- --------- --------- -------------------------------------------- --------- ---------
                                     4-9(lr)10-15                                                 $\widehat{\bm\beta}^{\text{\upshape{las}}}$                       $\widehat{\bm\beta}^{\text{\upshape{DS}}}$                       $\widehat{\bm\beta}^{\text{\upshape{las}}}$                       $\widehat{\bm\beta}^{\text{\upshape{DS}}}$            
                         4-6(lr)7-9 (lr)10-12 (lr)13-15 Model                          n     p                        TPR                        $L_F$     $L_2$                       TPR                        $L_F$     $L_2$                        TPR                        $L_F$     $L_2$                       TPR                        $L_F$     $L_2$
   1-3(lr)4-9 (lr)10-15 [\[e:one\]](#e:one){reference-type="ref" reference="e:one"}   200   50                      0.8714                      0.4143    0.5553                      0.8622                     0.4217    0.5691                      0.8685                      0.4145    0.5559                      0.8640                     0.4217    0.5695
                                                                                                                    (0.108)                     (0.048)   (0.066)                    (0.119)                     (0.054)   (0.070)                     (0.118)                     (0.049)   (0.067)                    (0.121)                     (0.055)   (0.070)
                                                                                      200   100                     0.8827                      0.4320    0.5890                      0.8961                     0.4379    0.5949                      0.8684                      0.4326    0.5892                      0.8867                     0.4386    0.5960
                                                                                                                    (0.084)                     (0.050)   (0.072)                    (0.080)                     (0.046)   (0.065)                     (0.139)                     (0.052)   (0.074)                    (0.120)                     (0.048)   (0.066)
                                                                                      500   100                     0.9909                      0.3311    0.4916                      0.9886                     0.3391    0.4989                      0.9928                      0.3303    0.4901                      0.9901                     0.3380    0.4975
                                                                                                                    (0.016)                     (0.031)   (0.069)                    (0.021)                     (0.036)   (0.065)                     (0.015)                     (0.032)   (0.069)                    (0.018)                     (0.037)   (0.066)
                                                                                      500   200                     0.9942                      0.3520    0.5287                      0.9916                     0.3511    0.5400                      0.9954                      0.3512    0.5273                      0.9672                     0.3528    0.5399
                                                                                                                    (0.009)                     (0.038)   (0.054)                    (0.018)                     (0.045)   (0.065)                     (0.008)                     (0.039)   (0.055)                    (0.129)                     (0.055)   (0.072)
            [\[e:four\]](#e:four){reference-type="ref" reference="e:four"}            200   50                      0.4074                      0.7831    0.8353                      0.4027                     0.7942    0.8335                      0.4063                      0.7832    0.8353                      0.4045                     0.7943    0.8336
                                                                                                                    (0.073)                     (0.089)   (0.072)                    (0.087)                     (0.079)   (0.034)                     (0.072)                     (0.089)   (0.072)                    (0.089)                     (0.079)   (0.034)
                                                                                      200   100                     0.4178                      0.8406    0.8690                      0.3541                     0.9119    0.8879                      0.4486                      0.8407    0.8690                      0.4038                     0.9120    0.8880
                                                                                                                    (0.091)                     (0.108)   (0.036)                    (0.107)                     (0.126)   (0.045)                     (0.091)                     (0.108)   (0.036)                    (0.123)                     (0.126)   (0.045)
                                                                                      500   100                     0.5405                      0.8267    0.8118                      0.5632                     0.7910    0.7953                      0.5406                      0.8267    0.8117                      0.5628                     0.7910    0.7951
                                                                                                                    (0.111)                     (0.125)   (0.047)                    (0.122)                     (0.166)   (0.062)                     (0.111)                     (0.125)   (0.047)                    (0.123)                     (0.166)   (0.062)
                                                                                      500   200                     0.5951                      0.8713    0.8519                      0.6487                     0.8184    0.8259                      0.6918                      0.8713    0.8519                      0.7101                     0.8184    0.8258
                                                                                                                    (0.175)                     (0.165)   (0.088)                    (0.159)                     (0.182)   (0.090)                     (0.148)                     (0.165)   (0.088)                    (0.122)                     (0.182)   (0.090)

  : Errors in estimating $\bm\Omega$ with
  $\mathfrak{t} \in \{0, \mathfrak{t}_{\text{ada}}\}$ applied to the
  estimator of $\mathbf A_1$ in combination with the
  Lasso [\[eq:lasso\]](#eq:lasso){reference-type="eqref"
  reference="eq:lasso"} and the
  DS [\[eq:ds\]](#eq:ds){reference-type="eqref" reference="eq:ds"}
  estimators, measured by $L_F$ and $L_2$, averaged over $100$
  realisations (with standard errors reported in brackets). We also
  report the average TPR when FPR $= 0.05$ and the corresponding
  standard error. See [Results: Threshold selection](#sec:sim:order) in
  the main text for further information.
:::

## VAR order selection

Table [5](#table:order){reference-type="ref" reference="table:order"}
reports the results of VAR order estimation over $100$ realisations.

::: {#table:order}
                                                                               CV                                                                                                                      eBIC                                                                                            
  ------------------------------------------- ----- ----- --------------------------------------------- ---- ---- ---- -------------------------------------------- ---- ---- ---- --------------------------------------------- ---- ---- ---- -------------------------------------------- ---- ---- ----
                4-11 (lr)12-19                             $\widehat{\bm\beta}^{\text{\upshape{las}}}$                  $\widehat{\bm\beta}^{\text{\upshape{DS}}}$                  $\widehat{\bm\beta}^{\text{\upshape{las}}}$                  $\widehat{\bm\beta}^{\text{\upshape{DS}}}$            
      4-7(lr)8-11 (lr)12-15 (lr)16-19 $d$      $n$   $p$                        0                        1    2    3                        0                        1    2    3                         0                        1    2    3                        0                        1    2    3
   1-3 (lr)4-7(lr)8-11 (lr)12-15 (lr)16-19 1   200   10                        81                        10   4    5                        91                       6    2    1                        64                        17   11   8                        64                       12   16   8
                                               200   20                        94                        6    0    0                        94                       5    1    0                        68                        10   9    13                       75                       10   7    8
                                               500   10                        94                        5    1    0                        86                       7    4    3                        65                        17   11   7                        65                       18   9    8
                                               500   20                        97                        2    0    1                        98                       1    1    0                        70                        15   8    7                        64                       14   10   12
                     1-19                                                      -2                        -1   0    1                        -2                       -1   0    1                        -2                        -1   0    1                        -2                       -1   0    1
       4-7(lr)8-11 (lr)12-15 (lr)16-19 3       200   10                         0                        0    77   23                       0                        0    78   22                       27                        3    49   21                       30                       6    49   15
                                               200   20                         0                        0    97   3                        0                        0    85   15                       32                        1    48   19                       31                       2    58   9
                                               500   10                         0                        0    76   24                       0                        0    83   17                       30                        4    43   23                       29                       2    40   29
                                               500   20                         0                        0    74   26                       0                        0    97   3                        29                        3    45   23                       25                       4    53   18

  : Distribution of $\widehat{d} - d$ over $100$ realisations when the
  VAR order is selected by the CV and eBIC methods in combination with
  the Lasso [\[eq:lasso\]](#eq:lasso){reference-type="eqref"
  reference="eq:lasso"} and the
  DS [\[eq:ds\]](#eq:ds){reference-type="eqref" reference="eq:ds"}
  estimators, see [Results: VAR order selection](#sec:sim:order) in the
  main text for further information.
:::

## CLIME vs. ACLIME estimators {#sec:sim:adaptive}

We compare the performance of the adaptive and non-adaptive estimators
for the VAR innovation precision matrix $\bm\Delta$ and its impact on
the estimation of $\bm\Omega$, the inverse of the long-run covariance
matrix of the data (see [Step 3](#sec:step:three)). We generate
$\bm\chi_t$ as in [\[m:ar\]](#m:ar){reference-type="ref"
reference="m:ar"}, fix $d = 1$ and treat it as known and consider
$(n, p) \in \{(200, 50), (200, 100), (500, 100), (500, 200)\}$.

In Tables [6](#table:lrpc:delta){reference-type="ref"
reference="table:lrpc:delta"} and [7](#table:lrpc:omega){reference-type="ref"
reference="table:lrpc:omega"}, we report the errors of $\bm\Delta$ and
$\bm\Omega$. We consider both the
Lasso [\[eq:lasso\]](#eq:lasso){reference-type="eqref"
reference="eq:lasso"} and DS [\[eq:ds\]](#eq:ds){reference-type="eqref"
reference="eq:ds"} estimators of VAR parameters, and CLIME and ACLIME
estimators for $\bm\Delta$, which lead to four different estimators for
$\bm\Delta$ and $\bm\Omega$, respectively. Overall, we observe that with
increasing $n$, the performance of all estimators improve according to
all metrics regardless of the
scenarios [\[e:one\]](#e:one){reference-type="ref" reference="e:one"}
or [\[e:four\]](#e:four){reference-type="ref" reference="e:four"}, while
increasing $p$ has an adverse effect. The two methods perform similarly
in setting [\[e:one\]](#e:one){reference-type="ref" reference="e:one"}
when $\bm\Delta = \mathbf I$. There is marginal improvement for adopting
the ACLIME estimator noticeable
under [\[e:four\]](#e:four){reference-type="ref" reference="e:four"},
particularly in TPR. Figures [8](#fig:roc:delta){reference-type="ref"
reference="fig:roc:delta"} and [9](#fig:roc:omega){reference-type="ref"
reference="fig:roc:omega"} shows the ROC curves for the support recovery
of $\bm\Delta$ and $\bm\Omega$ when the Lasso estimator is used.

::: {#table:lrpc:delta}
                                                                                                                     CLIME                                                                                                                             ACLIME                                                                                                
  ---------------------------------------------------------------------------------- ----- ----- --------------------------------------------- --------- --------- -------------------------------------------- --------- --------- --------------------------------------------- --------- --------- -------------------------------------------- --------- ---------
                                     4-9(lr)10-15                                                 $\widehat{\bm\beta}^{\text{\upshape{las}}}$                       $\widehat{\bm\beta}^{\text{\upshape{DS}}}$                       $\widehat{\bm\beta}^{\text{\upshape{las}}}$                       $\widehat{\bm\beta}^{\text{\upshape{DS}}}$            
                         4-6(lr)7-9 (lr)10-12 (lr)13-15 Model                          n     p                        TPR                        $L_F$     $L_2$                       TPR                        $L_F$     $L_2$                        TPR                        $L_F$     $L_2$                       TPR                        $L_F$     $L_2$
   1-3(lr)4-9 (lr)10-15 [\[e:one\]](#e:one){reference-type="ref" reference="e:one"}   200   50                       1.000                       0.215     0.489                      1.000                       0.220     0.497                       1.000                       0.207     0.472                      1.000                       0.209     0.469
                                                                                                                    (0.000)                     (0.047)   (0.223)                    (0.000)                     (0.047)   (0.182)                     (0.002)                     (0.043)   (0.173)                    (0.000)                     (0.041)   (0.116)
                                                                                      200   100                      1.000                       0.235     0.513                      1.000                       0.241     0.521                       1.000                       0.223     0.507                      1.000                       0.228     0.518
                                                                                                                    (0.000)                     (0.036)   (0.089)                    (0.000)                     (0.036)   (0.107)                     (0.000)                     (0.033)   (0.084)                    (0.000)                     (0.034)   (0.099)
                                                                                      500   100                      1.000                       0.181     0.458                      1.000                       0.183     0.466                       1.000                       0.176     0.452                      1.000                       0.178     0.458
                                                                                                                    (0.000)                     (0.022)   (0.062)                    (0.000)                     (0.029)   (0.087)                     (0.000)                     (0.022)   (0.052)                    (0.000)                     (0.028)   (0.069)
                                                                                      500   200                      1.000                       0.198     0.510                      1.000                       0.193     0.492                       1.000                       0.187     0.505                      1.000                       0.182     0.489
                                                                                                                    (0.000)                     (0.027)   (0.066)                    (0.000)                     (0.035)   (0.065)                     (0.000)                     (0.026)   (0.056)                    (0.000)                     (0.033)   (0.057)
         1-15 [\[e:four\]](#e:four){reference-type="ref" reference="e:four"}          200   50                       0.659                       0.422     0.816                      0.662                       0.391     0.608                       0.682                       0.397     0.706                      0.687                       0.380     0.600
                                                                                                                    (0.058)                     (0.101)   (0.654)                    (0.057)                     (0.031)   (0.144)                     (0.055)                     (0.056)   (0.351)                    (0.054)                     (0.030)   (0.176)
                                                                                      200   100                      0.639                       0.417     0.695                      0.637                       0.420     0.720                       0.669                       0.404     0.663                      0.668                       0.405     0.684
                                                                                                                    (0.044)                     (0.039)   (0.205)                    (0.042)                     (0.043)   (0.249)                     (0.041)                     (0.037)   (0.162)                    (0.039)                     (0.037)   (0.193)
                                                                                      500   100                      0.730                       0.372     0.764                      0.726                       0.499     1.708                       0.735                       0.358     0.650                      0.734                       0.361     0.718
                                                                                                                    (0.035)                     (0.097)   (0.828)                    (0.039)                     (1.101)   (7.586)                     (0.032)                     (0.038)   (0.322)                    (0.031)                     (0.056)   (0.517)
                                                                                      500   200                      0.729                       0.370     0.711                      0.728                       0.362     0.736                       0.737                       0.363     0.647                      0.737                       0.354     0.673
                                                                                                                    (0.028)                     (0.035)   (0.355)                    (0.028)                     (0.035)   (0.384)                     (0.023)                     (0.026)   (0.239)                    (0.024)                     (0.028)   (0.279)

  : Errors in estimating $\bm\Delta$ using CLIME and ACLIME estimators,
  measured by $L_F$ and $L_2$, averaged over $100$ realisations (with
  standard errors reported in brackets). We also report the average TPR
  when FPR $= 0.05$ and the corresponding standard errors.
:::

::: {#table:lrpc:omega}
                                                                                                                     CLIME                                                                                                                             ACLIME                                                                                                
  ---------------------------------------------------------------------------------- ----- ----- --------------------------------------------- --------- --------- -------------------------------------------- --------- --------- --------------------------------------------- --------- --------- -------------------------------------------- --------- ---------
                                     4-9(lr)10-15                                                 $\widehat{\bm\beta}^{\text{\upshape{las}}}$                       $\widehat{\bm\beta}^{\text{\upshape{DS}}}$                       $\widehat{\bm\beta}^{\text{\upshape{las}}}$                       $\widehat{\bm\beta}^{\text{\upshape{DS}}}$            
                         4-6(lr)7-9 (lr)10-12 (lr)13-15 Model                          n     p                        TPR                        $L_F$     $L_2$                       TPR                        $L_F$     $L_2$                        TPR                        $L_F$     $L_2$                       TPR                        $L_F$     $L_2$
   1-3(lr)4-9 (lr)10-15 [\[e:one\]](#e:one){reference-type="ref" reference="e:one"}   200   50                       0.871                       0.415     0.557                      0.862                       0.422     0.571                       0.867                       0.411     0.558                      0.856                       0.417     0.570
                                                                                                                    (0.108)                     (0.050)   (0.070)                    (0.119)                     (0.055)   (0.080)                     (0.106)                     (0.051)   (0.088)                    (0.114)                     (0.053)   (0.083)
                                                                                      200   100                      0.883                       0.432     0.589                      0.896                       0.438     0.595                       0.868                       0.423     0.583                      0.883                       0.429     0.587
                                                                                                                    (0.084)                     (0.050)   (0.072)                    (0.080)                     (0.046)   (0.065)                     (0.088)                     (0.048)   (0.077)                    (0.085)                     (0.045)   (0.061)
                                                                                      500   100                      0.991                       0.331     0.492                      0.989                       0.339     0.499                       0.991                       0.328     0.490                      0.989                       0.337     0.498
                                                                                                                    (0.016)                     (0.031)   (0.069)                    (0.021)                     (0.036)   (0.065)                     (0.015)                     (0.033)   (0.070)                    (0.019)                     (0.036)   (0.067)
                                                                                      500   200                      0.994                       0.352     0.529                      0.992                       0.351     0.540                       0.994                       0.344     0.525                      0.990                       0.342     0.537
                                                                                                                    (0.009)                     (0.038)   (0.054)                    (0.018)                     (0.045)   (0.065)                     (0.009)                     (0.038)   (0.056)                    (0.014)                     (0.044)   (0.068)
         1-15 [\[e:four\]](#e:four){reference-type="ref" reference="e:four"}          200   50                       0.509                       0.532     0.724                      0.510                       0.514     0.664                       0.504                       0.518     0.679                      0.507                       0.506     0.658
                                                                                                                    (0.078)                     (0.071)   (0.243)                    (0.068)                     (0.043)   (0.137)                     (0.071)                     (0.055)   (0.162)                    (0.063)                     (0.043)   (0.141)
                                                                                      200   100                      0.511                       0.541     0.683                      0.513                       0.542     0.695                       0.509                       0.531     0.674                      0.504                       0.531     0.679
                                                                                                                    (0.059)                     (0.047)   (0.082)                    (0.065)                     (0.051)   (0.093)                     (0.062)                     (0.045)   (0.084)                    (0.061)                     (0.046)   (0.084)
                                                                                      500   100                      0.640                       0.450     0.655                      0.624                       0.544     1.099                       0.642                       0.441     0.597                      0.637                       0.440     0.617
                                                                                                                    (0.066)                     (0.072)   (0.402)                    (0.079)                     (0.866)   (3.714)                     (0.059)                     (0.036)   (0.118)                    (0.060)                     (0.047)   (0.204)
                                                                                      500   200                      0.670                       0.461     0.630                      0.658                       0.450     0.630                       0.677                       0.456     0.612                      0.661                       0.445     0.605
                                                                                                                    (0.045)                     (0.041)   (0.116)                    (0.043)                     (0.040)   (0.117)                     (0.041)                     (0.036)   (0.075)                    (0.037)                     (0.037)   (0.082)

  : Errors in estimating $\bm\Omega$ using CLIME and ACLIME estimators
  of $\bm\Delta$, measured by $L_F$ and $L_2$, averaged over $100$
  realisations (with standard errors reported in brackets). We also
  report the average TPR when FPR $= 0.05$ and the corresponding
  standard errors.
:::

![ROC curves of TPR against FPR for $\widehat{\bm\Delta}$ with CLIME and
ACLIME estimators in recovering the support of $\bm\Delta$, averaged
over $100$ realisations. Vertical lines indicate FPR
$= 0.05$.](figs/roc_delta_aclime.pdf){#fig:roc:delta
width=".8\\textwidth"}

![ROC curves of TPR against FPR for $\widehat{\bm\Omega}$ with CLIME and
ACLIME estimators in recovering the support of $\bm\Omega$, averaged
over $100$ realisations. Vertical lines indicate FPR
$= 0.05$.](figs/roc_omega_aclime.pdf){#fig:roc:omega
width=".8\\textwidth"}

# Appendix D: Dataset information {#sec:real:data}

Table [8](#table:definitions){reference-type="ref"
reference="table:definitions"} defines the four node types in the panel.
Table [9](#table:data:info){reference-type="ref"
reference="table:data:info"} describes the dataset analysed in [Real
data example](#sec:real).

::: {#table:definitions}
             Name            Definition
  -------------------------- ------------------------------------------------------------------------------
       1-1 (lr)2-2 Zone      A transmission owner's area within the PJM Region.
          Aggregate          A group of more than one individual bus into a pricing node (pnode)
                             that is considered as a whole in the Energy Market and other various systems
                             and Markets within PJM.
             Hub             A group of more than one individual bus into a regional pricing node (pnode)
                             developed to produce a stable price signal in the Energy Market
                             and other various systems and Markets within PJM.
   Extra High Voltage (EHV)  Nodes at 345kV and above on the PJM system.

  : Node type definitions for energy price data.
:::

::: {#table:data:info}
                Name                 Node ID   Node Type
  --------------------------------- --------- -----------
       1-1 (lr)2-2 (lr)3-3 PJM          1        ZONE
                AECO                  51291      ZONE
                 BGE                  51292      ZONE
                 DPL                  51293      ZONE
                JCPL                  51295      ZONE
                METED                 51296      ZONE
                PECO                  51297      ZONE
                PEPCO                 51298      ZONE
                 PPL                  51299      ZONE
               PENELEC                51300      ZONE
                PSEG                  51301      ZONE
    1-1 (lr)2-2 (lr)3-3 BRANDONSH     51205    AGGREGATE
              BRUNSWICK               51206    AGGREGATE
              COOKSTOWN               51211    AGGREGATE
                DOVER                 51214    AGGREGATE
              DPL NORTH               51215    AGGREGATE
              DPL SOUTH               51216    AGGREGATE
               EASTON                 51218    AGGREGATE
                ECRRF                 51219    AGGREGATE
               EPHRATA                51220    AGGREGATE
              FAIRLAWN                51221    AGGREGATE
              HOMERCIT                51229    AGGREGATE
           HOMERCIT UNIT1             51230    AGGREGATE
           HOMERCIT UNIT2             51231    AGGREGATE
           HOMERCIT UNIT3             51232    AGGREGATE
            KITTATNY 230              51238    AGGREGATE
               MANITOU                51239    AGGREGATE
              MONTVILLE               51241    AGGREGATE
              PENNTECH                51246    AGGREGATE
             PPL_ALLUGI               51252    AGGREGATE
               SENECA                 51255    AGGREGATE
            SOUTHRIV 230              51261    AGGREGATE
            SUNBURY LBRG              51270    AGGREGATE
               TRAYNOR                51277    AGGREGATE
                 UGI                  51279    AGGREGATE
              VINELAND                51280    AGGREGATE
              WELLSBORO               51285    AGGREGATE
   1-1 (lr)2-2 (lr)3-3 EASTERN HUB    51217       HUB
            WEST INT HUB              51287       HUB
             WESTERN HUB              51288       HUB
    1-1 (lr)2-2 (lr)3-3 ALBURTIS      52443       EHV
             BRANCHBURG               52444       EHV
              BRIGHTON                52445       EHV
             BURCHESHILL              52446       EHV
              CALVERTC                52447       EHV
               CHALKPT                52448       EHV
              CONASTONE               52449       EHV
              CONEMAUGH               52450       EHV
                DEANS                 52451       EHV
                ELROY                 52452       EHV

  : Names, IDs and Types for the $50$ power nodes in the energy price
  dataset.
:::
