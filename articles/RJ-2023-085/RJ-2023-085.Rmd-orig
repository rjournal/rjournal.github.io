---
title: 'RobustCalibration: Robust Calibration of Computer Models in R'
abstract: ' Two fundamental research tasks in science and engineering are forward
  predictions and data inversion. This article introduces a new R package [RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
  for Bayesian data inversion and model calibration using experiments and field observations.
  Mathematical models for forward predictions are often written in computer code,
  and they can be computationally expensive to run. To overcome the computational
  bottleneck from the simulator, we implemented a statistical emulator from the [RobustGaSP](https://CRAN.R-project.org/package=RobustGaSP)
  package for emulating both scalar-valued or vector-valued computer model outputs.
  Both posterior sampling and maximum likelihood approach are implemented in the [RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
  package for parameter estimation. For imperfect computer models, we implement the
  Gaussian stochastic process and scaled Gaussian stochastic process for modeling
  the discrepancy function between the reality and mathematical model. This package
  is applicable to various other types of field observations and models, such as repeated
  experiments, multiple sources of measurements and correlated measurement bias. We
  discuss numerical examples of calibrating mathematical models that have closed-form
  expressions, and differential equations solved by numerical methods. '
author:
- name: Mengyang Gu
  affiliation: University of California, Santa Barbara
  email: |
    mengyang@pstat.ucsb.edu
  address:
  - Department of Statistics and Applied Probability
  - Santa Barbara, California, USA
date: '2024-04-11'
date_received: '2022-09-22'
journal:
  firstpage: ~
  lastpage: ~
volume: 15
issue: 4
slug: RJ-2023-085
packages:
  cran: ~
  bioc: ~
draft: no
preview: preview.png
bibliography: References_2020.bib
CTV: ~
output:
  rjtools::rjournal_web_article:
    self_contained: no
    toc: no
    legacy_pdf: yes

---

# Introduction

Complex processes are often represented as mathematical models,
implemented in computer code. These mathematical models are often called
*computer models* or *simulators*, and are widely used to simulate
different processes given a set of parameters and initial conditions
[@sacks1989design]. The initial conditions and model parameters may be
unknown or uncertain in practice. Calibrating the computer models based
on real experiments or observations is one of the fundamental tasks in
science and engineering, widely known as *data inversion* or *model
calibration*.

We follow the notation in [@bayarri2007computer] for defining the model
calibration problem. Let $y^F(\mathbf x)$ be the real-valued field
observation with a $p_x$-dimensional observable input vector
$\mathbf x$. The field observation is often decomposed by
$y^F(\mathbf x)=y^R(\mathbf x)+\epsilon$, where $y^R(\cdot)$ denotes a
function of unknown reality, and $\epsilon$ denotes the noise, with the
superscript 'F' and 'R' denoting the field observations and reality,
respectively. Scientists often model the unknown reality by a computer
model, denoted by $f^M(\mathbf x, \bm \theta)$ at the $p_x$-dimensional
observable input $\mathbf  x$, and $p_{\theta}$-dimensional calibration
parameters $\bm \theta$, unobservable from the experiments, with
superscript 'M' denoting the computer model.

When the computer model describes the reality perfectly, a field
observation can be written as:
$$y^F(\mathbf x)= f^M(\mathbf x, \bm \theta)+ \epsilon,
\label{equ:calibration_no_discrepancy}$$ where $\epsilon$ is a zero-mean
Gaussian noise. We call the method by Equation
([\[equ:calibration_no_discrepancy\]](#equ:calibration_no_discrepancy){reference-type="ref"
reference="equ:calibration_no_discrepancy"}) the *no-discrepancy
calibration*.

When computer models are imperfect, the statistical calibration model
below is often used:
$$y^F(\mathbf x)= f^M(\mathbf x, \bm \theta)+ \delta(\mathbf x)+\epsilon,
\label{equ:calibration_discrepancy}$$ where $\delta(\cdot)$ is the
unobservable discrepancy between the mathematical model and reality.
Equation
([\[equ:calibration_discrepancy\]](#equ:calibration_discrepancy){reference-type="ref"
reference="equ:calibration_discrepancy"}) implies that the reality can
be written as
$y^R(\mathbf x)= f^M(\mathbf x, \bm \theta)+ \delta(\mathbf x)$ at any
observable input $\mathbf x \in \mathcal X$. The model calibration
framework has been studied extensively. In [@kennedy2001bayesian], the
discrepancy function $\delta(\cdot)$ is modeled via a Gaussian
stochastic process (GaSP), leading to more accurate prediction based on
the joint model of calibrated computer model and discrepancy compared to
either using the computer model or discrepancy model for prediction
alone. We call this method the *GaSP Calibration*. The GaSP calibration
has been applied in a wide range of studies of continuous and
categorical outputs
[@bayarri2007computer; @higdon2008computer; @paulo2012calibration; @chang2016calibrating; @chang2022ice].
Both no-discrepancy calibration and GaSP calibration are implemented in
the
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
package.

The calibrated computer model output can be far from the observations in
terms of $L_2$ distance when the discrepancy function is modeled by the
GaSP, [@arendt2012quantification], since a large proportion of the
variability in the data can be explained by discrepancy function. To
solve this problem, we implemented the scaled Gaussian stochastic
process (S-GaSP) calibration, or *S-GaSP Calibration* in the
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
package. The S-GaSP model of discrepancy function was first introduced
in [@gu2018sgasp], where more prior probability mass of the $L_2$ loss
is placed on small values. Consequently, the calibrated computer model
by the S-GaSP fits the reality better in terms of $L_2$ loss than the
GaSP calibration. Furthermore, historical data may be used for reducing
the parameter space [@williamson2013history].

A few recent approaches aim to minimize the $L_2$ distance between the
reality and computer model [@tuo2015efficient; @wong2017frequentist],
where the reality and discrepancy function are often estimated in two
steps separately. The S-GaSP calibration bridges the two-step $L_2$
calibration with the GaSP calibration. Compared with two-step
approaches, parameters and discrepancy $(\bm \theta, \delta(.))$ are
estimated in GaSP calibration and S-GaSP calibration jointly, and the
uncertainty of these estimation can be obtained based on the likelihood
function of the sampling model. Furthermore, compared with the
orthogonal calibration model [@plumlee2017bayesian], the S-GaSP process
places more prior mass on inputs leading to small values of the random
$L_2$ loss, instead of more prior mass on inputs close to the critical
points of the loss function.

Another concern is the computational cost of model calibration, as
computer models may involve numerical solutions of differential
equations, which can be expensive to run. We utilize a statistical
emulator for approximating computationally expensive computer models.
The GaSP emulator is a well-developed framework for emulating
computationally expensive computer models and it was implemented in a
few R packages, such as
[DiceKriging](https://CRAN.R-project.org/package=DiceKriging)
[@roustant2012dicekriging],
[GPfit](https://CRAN.R-project.org/package=GPfit) [@macdonald2015gpfit]
and [RobustGaSP](https://CRAN.R-project.org/package=RobustGaSP)
[@gu2018robustgasp]. Here we specify the functions rgasp, ppgasp and
predict in [RobustGaSP](https://CRAN.R-project.org/package=RobustGaSP)
for emulating computer models with scalar-valued and vector-valued
outputs, respectively.

Building packages for Bayesian model calibration is much more
complicated than packages of statistical emulator, as both field
observations and computer models need to be integrated. A few
statistical packages are available for Bayesian model calibration, such
as [BACCO](https://CRAN.R-project.org/package=BACCO)
[@hankin2005introducing],
[SAVE](https://CRAN.R-project.org/package=SAVE) package
[@palomo2015save], and
[CaliCo](https://CRAN.R-project.org/package=CaliCo)
[@carmassi2018calico]. The
[CaliCo](https://CRAN.R-project.org/package=CaliCo) package, for
example, integrates
[DiceKriging](https://CRAN.R-project.org/package=DiceKriging) for
emulating computational expensive simulations. It offers different types
of diagnostic plots based on
[ggplot2](https://CRAN.R-project.org/package=ggplot2)
[@wickham2011ggplot2], and distinct prior choices of the parameters. In
the [BACCO](https://CRAN.R-project.org/package=BACCO) and
[SAVE](https://CRAN.R-project.org/package=SAVE), the GaSP model of the
discrepancy function [@kennedy2001bayesian] is assumed, and GaSP
emulators can be used to approximate expensive computer models.

Although a large number of studies were developed for Bayesian model
calibration, to the authors' best knowledge, many methods implemented in
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
have not been implemented in another software package previously. We
highlight a few unique features of the
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
package. First of all, we allow users to specify different types of
output of field observations, for different scenarios through the output
argument in the rcalibration function. The simplest scenario is to have
a vector of fields observations. We also allow users to input a matrix
or a list of fields observations with the same or different number of
replications, respectively. Efficient computation from sufficient
statistics was implemented for model calibration with replications,
which can improve computation up to $k^3$ times, where $k$ is the number
of repeated experiments. Second, distinct models can be calibrated with
different sets of parameters for data from multiple sources by the
calibration_MS function. Third, we implemented emulators for
approximating expensive computer models with both scalar-valued and
vector-valued outputs. Emulators for vector-valued outputs, capable of
handling a large number of temporal or spatial coordinates, were not
implemented in other packages for model calibration. Here we use the
parallel partial Gaussian stochastic process (PP-GaSP) emulator from
[RobustGaSP](https://CRAN.R-project.org/package=RobustGaSP) package
[@gu2018robustgasp] as a computationally scalable emulator of
vector-valued output. The
[RobustGaSP](https://CRAN.R-project.org/package=RobustGaSP) package has
been applied to various applications, such as emulating geophysical
models of ground deformation [@anderson2019magma], and storm surge
simulation [@ma2022multifidelity]. Furthermore, users can choose
no-discrepancy calibration, GaSP or S-GaSP models of discrepancy
functions for different scenarios. Statistical inferences by both
posterior samples and the maximum likelihood estimator (MLE) are made
available for these calibration approaches by the method argument in the
rcalibration function.

The rest of the paper is organized below. We first give an overview of
the
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
package to introduce the main functions and their usage. We further
introduce methodology and numerical examples to illustrate the
implemented methods in the
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
package. Closed form expressions of likelihood functions, derivatives,
posterior distributions and computational algorithms are outlined in the
Appendix.

# An overview of [RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)

Figure [1](#fig:schematic_graph){reference-type="ref"
reference="fig:schematic_graph"} gives a schematic overview of the
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
package. We consider predicting the reality in two ways: using both
calibrated computer model and discrepancy (predictive accuracy), and the
calibrated computer model alone (calibration accuracy), both evaluated
in terms of the $L_2$ loss. The left panel gives comparison of accuracy
and computational cost between the implemented methods. The
no-discrepancy calibration is faster than GaSP and S-GaSP calibration,
as one does not need to compute the inversion and log determinant of the
covariance matrix of the field data, unlike GaSP calibration and S-GaSP
calibration. Because model discrepancy is modeled, predictive accuracy
by GaSP and S-GaSP calibration is typically higher than no-discrepancy
calibration. The
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
package can handle model calibration and prediction at a wide range of
settings. We first introduce the main functions of the package.

![Schematic overview of the
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
package. The left panel compares the expected predictive accuracy and
calibration accuracy by different calibration methods. The calibration
and calibration_MS in the right panel are two main functions for
parameter estimation for observations from single and multiple sources,
respectively. The arguments of these functions are given in blue boxes.
The object classes calibration and calibration_MS (orange boxes) can be
supplied in predict.rcalibration and predict_MS.rcalibration_MS
functions for making predictions.
](figures/schematic_graph.png){#fig:schematic_graph
height=".5\\textwidth" width=".98\\textwidth"}

## Main functions

The
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
package can be used for estimating unobservable parameters from computer
models and predicting reality. The conventional model calibration and
prediction from a single source of field observations are achieved by
rcalibration and predict.rcalibration, as shown in upper blue and yellow
boxes in Figure [1](#fig:schematic_graph){reference-type="ref"
reference="fig:schematic_graph"}, respectively. The function
rcalibration allows users to call either a Markov chain Monte Carlo
(MCMC) algorithm for posterior sampling or a numerical optimization
algorithm for computing the maximum likelihood estimator (MLE) of the
parameters. Additional arguments can be specified to handle observations
from repeated experiments, select a different trend or discrepancy
function, and build a surrogate model for approximating expensive
computer models, shown in Figure
[1](#fig:schematic_graph){reference-type="ref"
reference="fig:schematic_graph"}. The rcalibration function returns an
object of the rcalibration S4 class with posterior samples or MLE of the
parameters, shown in the upper orange box in Figure
[1](#fig:schematic_graph){reference-type="ref"
reference="fig:schematic_graph"}. Then rcalibration S4 class is used as
the input in the predict.rcalibration function to perform predictions on
a set of test inputs. The predict.rcalibration function returns a
predictobj.rcalibration S4 class, which contains the predictive mean of
the calibrated computer model, estimated trend, and discrepancy
function. The estimated interval at a given quantile can also be
returned by specifying the vector of the interval_est argument in the
predict.raclibration function.

In some applications, we have different sources or types of
observations. For instance, in calibrating geophysical models, time
series of continuous GPS observations and satellite radar interferogram
can be jointly used to estimate unobservable parameters
[@anderson2019magma]. In the
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
package, we implement parameter estimation for observations from
multiple sources by the rcalibration_MS function, which returns a
rcalibration_MS S4 class that contains posterior samples, shown in
Figure [1](#fig:schematic_graph){reference-type="ref"
reference="fig:schematic_graph"}. Since each source of observations can
induce a separate set of model parameters, making the dimension of the
parameter space large, the MLE is unstable, and as such is not
implemented in rcalibration_MS. The rcalibration_MS class can be used as
an input into the predict_MS.rcalibration_MS function for predicting the
reality.

## The rcalibration function

To use the rcalibration function, users need to specify 4 inputs: 1)
design, an $n\times p_x$ matrix of observable inputs, 2) observations,
field observations that can have three types specified below, 3)
theta_range, a $p_{\theta}\times 2$ matrix of the lower and upper bounds
of calibration parameters, and 4) either math_model, a function of
mathematical model, or input_simul and output_simul, simulation runs by
for building the emulator. The
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
package can handle three different kinds of field observations as
specified by the argument observations. First, one can input an n-
vector $(y^F(\mathbf x_1),...,y^F(\mathbf x_n))^T$, when one field datum
is available at each observable input. Second, when $k$ repeated
measurements are available for each observable input, one can input an
$n\times k$ matrix of field observations, where each row contains $k$
replicates of one observable input. Third, one can input a list, where
each element contains $k_i$ replications at the $i$th observable input,
for $i=1,...,n$.

An important feature of the
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
package is the flexible specification of the computer model. Users can
specify a mathematical model by supplying a generic R function through
the argument math_model and selecting the default choice of the
simulator simul_type=1. The second choice is to use an emulator to
predict the computer model by choosing simul_type=0. Users need to input
a set of simulation runs by input_simul and output_simul to call an
emulator by the
[RobustGaSP](https://CRAN.R-project.org/package=RobustGaSP) package for
this choice. Users can either give a $D\times (p_x+p_{\theta})$ matrix
of input and a $D$-vector of output to input_simul and output_simul
arguments, for calling rgasp function from
[RobustGaSP](https://CRAN.R-project.org/package=RobustGaSP) for
emulation, or give a $D \times p_{\theta}$ matrix of input and a
$D\times p_x$ matrix of output to input_simul and output_simul, to call
for ppgasp function from
[RobustGaSP](https://CRAN.R-project.org/package=RobustGaSP) for
emulation. To account for stochastic error or numerical approximation
error in the simulator, we also allow users to specify a nugget
parameter estimated by the simulator data by the argument simul_nug=T.

The rcalibration function contains a few optional arguments. First,
users can specify GaSP or S-GaSP discrepancy models by the argument
discrepancy_type=\"GaSP\" or discrepancy_type=\"S-GaSP\", respectively.
The default choice is to assume a S-GaSP discrepancy model. The model
without a discrepancy can be specified by the argument
discrepancy_type=\"no-discrepancy\". Second, a trend can be specified by
the argument X. By default, the trend (or mean) of the calibration model
is zero. Third, the parameter estimation approach can be specified by
the method argument. The default choice is method=\"post_sample\", where
the posterior samples will be drawn, and stored in the slot post_sample
of the rcalibration class. The MLE can be specified by the argument
method=\"mle\", and the estimated parameters are stored in the slot
param_est of the rcalibration class. Users can specify the number of
MCMC samples and burn-in samples by the argument S and S_0,
respectively. A vector containing the standard deviation of the proposal
distribution for the calibration parameters, the logarithm of the
inverse range parameters, and the logarithm of the nugget parameter can
be specified by the sd_proposal argument. Furthermore, users can "thin"
the MCMC samples and only record a subset by the argument thinning. For
instance, thinning=5 means only 1/5 of the posterior samples will be
recorded. Besides, the inverse variances of noises in the field
observations can be specified by the output_weights. Finally, arguments
initial_values and num_initial_starts can be specified in numerical
optimization to find MLE, and when posterior sampling is used, initial
values of the calibration parameters can be specified through the
argument initial_values.

The object rcalibration created by the rcalibration function have a few
key properties. First of all, if we have method='post_sample', the after
burn-in posterior samples of parameters will be stored in the slot
post_sample, where each row contains posterior samples in one iteration.
The first $p_\theta$ columns contain posterior samples of the
calibration parameters $\bm \theta$. In the no-discrepancy calibration,
the $p_\theta+1$ column of post_sample contains posterior draws of the
noise variance $\sigma^2_0$ and the $p_\theta+2$ to $p_\theta+q+1$
columns contain the trend parameters $\bm \theta_m$, for a non-zero mean
basis. In the GaSP or S-GaSP calibration, the $p_\theta+1$ to
$p_\theta+p_x+1$ columns record posterior samples of the log inverse
range and log nugget parameters. The $p_\theta+p_x+2$ to
$p_\theta+p_x+q$ columns record the noise variance parameter and trend
parameters if they are specified. The parameter $\lambda_z$ in S-GaSP
are recorded in slot lambda_z. The indices of the accepted proposed
samples are recorded in slots accept_S as one of the diagnostic
statistics.

## The predict.rcalibration function

After calling the rcalibration function, an object of the rcalibration
S4 class will be created, and it can be used as an input into the
predict.rcalibration function for predicting the reality on a specified
matrix of test input using the argument testing_input. Besides, if the
mean basis is specified by the argument X in rcalibration function, the
user also needs to specify the mean basis of the reality at the test
inputs by the argument X_testing in function predict.rcalibration. If
the emulator was not constructed in rcalibration, users also need to
specify the mathematical model by the argument math_model in the
predict.rcalibration function. Finally, a predictive interval can be
specified by the argument interval_est. For instance, the 95% predictive
interval can be obtained by the interval_est=c(0.025,0.975) argument.
The interval of the field data will be computed if interval_data=T.
Otherwise, the predictive interval of the reality will be computed. If
the variance parameter of the noise in field data was specified by the
output_weights, users should also specify the variance of test data by
testing_output_weights, which affects the predictive interval of test
data.

After calling predict.rcalibration function, an object
predictobj.rcalibration will be created and it contains three different
predictors for reality. The slot math_model_mean_no_trend gives the
predictive mean based on the calibrated computer model ($f^M$). The slot
math_model_mean gives the predictive mean based on the calibrated
computer model and the estimated trend ($f^M+\mu$), if the basis
functions of the trend at the observable input and the test input are
specified through X and X_testing in rcalibration and
rcalibration.predict, respectively. The slot mean gives the predictive
mean based on the calibrated computer model, estimated trend, and
discrepancy ($f^M+\mu+\delta$). If interval_est is specified, a matrix
of the intervals will be created for quantifying the uncertainty of
predictions.

## The rcalibration_MS function and the predict_MS.rcalibration_MS function

Model calibration and predictions using multiple sources or different
types of data are implemented by rcalibration_MS and
rcalibration_MS.predict_MS functions, respectively. One needs to specify
4 inputs: design, observations, theta_range and a form of mathematical
model. Suppose we have a data set produced by $k$ sources of
observations. The design is a list of $k$ elements, where each element
is a matrix of observable input for each source. The argument
observations takes a list of field observations, where each element is a
vector of field observations for each source. In principle, one can have
different number of observations from each source and the type of the
observations may not be the same. The argument theta_range is a
$p_{\theta}\times 2$ matrix of range of parameters, where the $i$th row
contains the minimum and maximum values of the $i$th coordinate of the
calibration parameter vector $\bm \theta$. Furthermore, if closed form
expressions of mathematical models are available for each source of
data, users can input a list of functions into the math_model for each
source of data. We also allow users to emulate the expensive simulation.
In this scenario, one needs to let simul_type be a vector of $1$s,
indicating emulators will be called for each computer model.

Additional arguments can be specified in rcalibration_MS. For instance,
the argument index_theta takes a list of indices for the associated
calibration parameter for each computer model. Suppose we have three
calibration parameters $\bm \theta=(\theta_1,\theta_2,\theta_3)$, and
the computer model for first source of data is related to the first two
calibration parameters and the computer model for second source of data
is related to second two calibration parameters. Then index_theta is a
list where index_theta\[\[1\]\]=c(1,2), and index_theta\[\[2\]\]=c(2,3).
More arguments will be introduced along with the examples. After calling
rcalibration_MS, an S4 class rcalibration_MS will be built, and used as
an input to predict_MS.rcalibration_MS for making predictions.

# Methods and examples

## No-discrepancy calibration

Let us start with the simplest method: no-discrepancy calibration.
First, suppose we have a vector of real-valued field observations
$\mathbf y^F=(y^F(\mathbf x_1),...,y^F(\mathbf x_n))^T$ at $n$
observable inputs, and the corresponding computer model output is
denoted by
$\mathbf f^M_{\bm \theta}=(f^M(\mathbf x_1,\bm \theta),...,f^M(\mathbf x_n,\bm \theta))^T$
for any calibration parameters $\bm \theta$. The vector of measurement
noise is assumed to follow
$\bm \epsilon \sim \mathcal{MN}(0,\, \sigma^2_0 \bm \Lambda)$, where the
covariance of the noise is diagonal with the $i$th term being
$\sigma^2_0\Lambda_{ii}$ and $\mathcal{MN}$ denote the multivariate
normal distribution. In the default setting, $\sigma^2_0$ is estimated
from the data and $\bm \Lambda=\mathbf I_n$, where $\mathbf I_n$ is an
$n\times n$ identity matrix. Users can manually specify the inverse of
the diagonal terms of $\bm \Lambda$ by the output_weights argument in
rcalibration and rcalibration_MS functions, as the variances of
measurement errors can be different. We assume the diagonal matrix
$\bm \Lambda$ is given in this section.

For a no-discrepancy calibration model in
([\[equ:calibration_no_discrepancy\]](#equ:calibration_no_discrepancy){reference-type="ref"
reference="equ:calibration_no_discrepancy"}), the MLE of the variance of
the noise is $\hat \sigma^2_0=S^2_0/n$ with
$S^2_0=(\mathbf y^F-\mathbf f^M_{\theta} )^T \bm \Lambda^{-1}(\mathbf y^F-\mathbf f^M_{\theta} )$.
The likelihood function is given in the Appendix. As $\bm \Lambda$ is a
diagonal matrix with diagonal terms denoted as $\Lambda_{ii}:=1/w_i$,
the profile likelihood follows
$$\mathcal L( \bm \theta\mid  \hat \sigma^2_0 ) \propto  \left\{ \sum^n_{i=1} w_i \left(y^F(\mathbf x_i)- f^M(\mathbf x_i, \bm \theta)\right)^2\right\}^{-n/2},$$
where $w_i=1$ is used in the default scenario. Maximizing the profile
likelihood of a no-discrepancy model is equivalent to minimizing the
weighted squared error
$\sum^n_{i=1} w_i \left(y^F(\mathbf x_i)- f^M(\mathbf x_i, \bm \theta)\right)^2$.
We numerically find $\bm \theta$ by the low-storage quasi-Newton
optimization method [@liu1989limited] implemented in lbfgs function in
the [nloptr](https://CRAN.R-project.org/package=nloptr) package
([@nloptr2014]) for optimization.

In the
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
package, we allow the users to specify the trend or mean function
modeled as
$\bm \mu(\mathbf x_i)=  \mathbf h(\mathbf x) \bm \theta_{m}=\sum^q_{t=1}h_t(\mathbf x)  \theta_{m,t}$
for any $\mathbf x$, where
$\bm \theta_{m}=(\theta_{m,1},...,\theta_{m,q})^T$ is a vector of trend
parameters. Maximizing the profile likelihood of a no-discrepancy model
with a trend is equivalent to minimizing the squared error loss:
$$\bm {\hat \theta}^{LS}_{m}=\mathop{\mathrm{argmin}}_{\bm \theta}\sum^n_{i=1} w_i \left(y^F(\mathbf x_i)- f^M(\mathbf x_i, \bm \theta) - \mathbf h(\mathbf x_i)\hat {\bm \theta}^{LS}_m    \right)^2,$$
where the solution follows
$\hat {\bm \theta}^{LS}_{m}= (\mathbf H^T \bm \Lambda^{-1} \mathbf H)^{-1}\mathbf H^T \bm \Lambda^{-1}( \mathbf y^F-\mathbf f^M_{\theta})$
with
$\mathbf H=(\mathbf h^T(\mathbf x_1),..., \mathbf h^T(\mathbf x_n))^T$
being an $n\times q$ matrix of the mean basis.

After calling the rcalibration function with method='mle', the point
estimator $\bm {\hat \theta}_{LS}$ is recorded in the slot param_est in
the rcalibration class. The MLE is a computationally cheap way to obtain
estimates of calibration parameter vector $\bm \theta$. After obtaining
the MLE, we can plug the MLE into the computer model for predicting
reality:
$f^M(\mathbf x_i,  \bm {\hat \theta}) + h(\mathbf x_i)\hat {\bm \theta}^{LS}_m$,
using the predict function.

We also implement the MCMC algorithm for Bayesian inference. The
posterior distribution follows
$$p(\bm \theta_m, \sigma^2_0, \bm \theta\mid \mathbf y^F) \propto  p(\mathbf y^F \mid \bm \theta, {\bm  \theta_m},  \sigma^2_0 ) \pi(\bm \theta) \pi({\bm  \theta_m},  \sigma^2_0).
\label{equ:post}$$ We assume an objective prior for the mean and
variance parameters $\pi(\bm \theta_m, \sigma^2_0)\propto 1/\sigma^2_0$.
The posterior distribution
$p(\bm \theta_m, \sigma^2_0 \mid \bm \theta, \mathbf y^F)$ can be
sampled by the Gibbs algorithm, since the prior is conjugate. We assume
that the default choice of the prior of the calibration parameters
$\pi(\bm \theta)$ is uniform over the parameter space and sample
$(\bm \theta \mid \mathbf y^F,\bm \theta_m, \sigma^2_0)$ using the
Metropolis algorithm. The posterior distributions and predictions from
the posterior samples will be discussed in the Appendix.

Here we show one example from [@bayarri2007framework], where data are
observed from unknown reality: $y^F(x)=3.5\exp(-1.7x)+1.5+\epsilon$,
where $\epsilon \sim \mathcal N(0,0.3^2)$. We have 30 observations at 10
inputs in the domain $x_i \in [0,3]$, for $i=1,2,...,10$, each
containing $3$ replications. The computer model is
$f^M(x,\theta)= 5 \exp(-\theta x)$ with $\theta \in [0,\, 50]$. The goal
is to estimate the calibration parameters and predict the unknown
reality ($3.5\exp(-1.7x)+1.5$) at $x\in [0,5]$. The observable inputs
and observations from [@bayarri2007framework] can be generated by the
code below.

::: example
R\> input=c(.110, .432, .754, 1.077, 1.399, 1.721, 2.043, 2.366,
2.688,3.010) R\> n=length(input) R\> k=3 R\>
output=t(matrix(c(4.730,4.720,4.234,3.177,2.966,3.653,1.970,2.267,2.084,2.079, +
2.409,2.371, 1.908,1.665,1.685, 1.773,1.603,1.922,1.370,1.661, + 1.757,
1.868,1.505,1.638,1.390, 1.275,1.679,1.461,1.157,1.530),k,n)) R\>
Bayarri_07\<-function(x,theta) + 5\*exp(-x\*theta) + R\>
theta_range=matrix(c(0,50),1,2)
:::

Here the mathematical model has a closed-form expression so we code it
as a function called Bayarri_07. The parameter range of calibration
parameter $\theta$ is given by a vector called theta_range.

The no-discrepancy calibration is implemented by the code below.

::: example
R\> set.seed(1) R\> X=matrix(1,n,1) R\>
m_no_discrepancy=rcalibration(input, output,math_model = Bayarri_07, +
theta_range = theta_range, X =X, + have_trend = T,discrepancy_type =
'no-discrepancy') R\> m_no_discrepancy type of discrepancy function:
no-discrepancy number of after burn-in posterior samples: 8000 0.1385 of
proposed calibration parameters are accepted median and 95% posterior
credible interval of calibration parameter 1 are 2.951997 2.264356
3.930725
:::

Here we use a constant mean basis using the mean (or trend) argument X,
such that the mathematical model is $f^M(x,\theta)+\mu$. We draw
$10,000$ posterior samples with the first $2,000$ samples used as
burn-in samples. One can adjust the number of posterior and burn-in
samples by argument S and S_0 in the calibration function, respectively.
We found that the median of the posterior samples of $\theta$ is $2.95$,
and the $95\%$ posterior interval is around $(2.26, 3.93)$. Around
$14\%$ of the samples were accepted by the Metropolis algorithm. Here
the first $p_{\theta}$ terms in sd_proposal is the standard deviation of
the proposal distribution of the calibration parameter, where the
default choice is $0.05$, and the next $p_x+1$ parameters are the
standard deviation of the inverse logarithm of the range parameter and
nugget parameters in GaSP and S-GaSP calibration, discussed in the next
subsection. The code below reduces sd_proposal to $0.025$ of the range
of the parameter space to make a larger proportion of the posterior
samples accepted by the algorithm:

::: example
R\> m_no_discrepancy_small_sd=rcalibration(input, output,math_model =
Bayarri_07, + theta_range = theta_range, + sd_proposal =
c(rep(0.025,dim(theta_range)\[1\]), + rep(0.25,dim(as.matrix(input)
)\[2\]),0.25), + X =X, have_trend = T,discrepancy_type =
'no-discrepancy') R\> m_no_discrepancy_small_sd type of discrepancy
function: no-discrepancy number of after burn-in posterior samples: 8000
0.2613 of proposed calibration parameters are accepted median and 95%
posterior credible interval of calibration parameter 1 are 2.935007
2.19352 3.933492
:::

Around 26% of the posterior samples are accepted. In this example, the
median and $95\%$ posterior credible intervals are similar to the one
having a proposal distribution with smaller standard deviation.

The following code gives the predictions and the $95\%$ predictive
interval of the reality for 200 test inputs equally spaced at
$x\in [0,5]$. Since we use a constant trend in model calibration, we
also specify a constant trend for making predictions, through the
argument X_testing in the predict function:

::: example
R\> testing_input=seq(0,5,5/199) R\>
X_testing=matrix(1,length(testing_input),1) R\>
m_no_discrepancy_pred=predict(m_no_discrepancy,testing_input,math_model=Bayarri_07, +
interval_est=c(0.025, 0.975),X_testing=X_testing)
:::

Posterior samples of the calibration parameter $\theta$ and the mean
parameter $\theta_m$ in the no-discrepancy calibration are plotted as
the red rectangles in the right panel in Figure
[2](#fig:bayarri_2007){reference-type="ref"
reference="fig:bayarri_2007"}. The predictive mean and 95% predictive
interval are graphed as the red curve and grey shaded area in the left
panel, respectively. Compared to models with an estimated discrepancy
function, the posterior samples from the no-discrepancy calibration are
more concentrated, and the average length of 95% predictive interval is
shorter. However, a large proportion of reality at $x\in[0, 1.5]$ is not
covered by the 95% posterior credible interval, due to large discrepancy
between the mathematical model and reality in this domain. Next, we
introduce two discrepancy models to solve this problem.

<figure id="fig:bayarri_2007">
<table>
<tbody>
<tr class="odd">
<td style="text-align: center;"><embed
src="figures/Bayarri_2007_math_model.pdf" /> <embed
src="figures/Bayarri_2007_math_model_discrepancy.pdf" /> <embed
src="figures/Bayarri_2007_post_samples.pdf" /></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<figcaption>Predictions and posterior samples by the <a
href="https://CRAN.R-project.org/package=RobustCalibration">RobustCalibration</a>
package for the example introduced in <span class="citation"
data-cites="bayarri2007framework"></span>. In the left and middle
panels, the unknown reality is graphed as black curves, and field
observations that contain with 3 replicates at each observable input are
plotted as black <span>dots</span>. In the left panel, predictions of
reality based on calibrated computer model with trend by GaSP, S-GaSP
and no-discrepancy calibration are graphed as light blue, blue and red
curves, respectively. ‘CM’ denotes the computer model and the shaded
area is the <span class="math inline">95%</span> posterior predictive
interval by the no-discrepancy calibration. In the middle panel,
predictions based on the calibrated computer model, discrepancy, and the
trend by GaSP and S-GaSP are graphed as light blue and blue curves,
respectively. The shaded area is the <span
class="math inline">95%</span> predictive interval by the GaSP
calibration. After burn-in posterior samples of calibration parameter
<span class="math inline"><em>θ</em></span> and mean parameter <span
class="math inline"><em>θ</em><sub><em>m</em></sub></span> are plotted
in the right panel. </figcaption>
</figure>

## Gaussian stochastic process models of discrepancy functions

The discrepancy function can be modeled as a GaSP, meaning that for any
$\{\mathbf x_1,...,\mathbf x_n\}$, the marginal distribution of the
discrepancy function follows a multivariate normal distribution:
$$(\delta(\mathbf x_1),..., \delta(\mathbf x_n))^T \mid  \bm \mu, \sigma^2, \mathbf R \sim  \mathcal{MN}(\bm \mu,\,  \sigma^2 \mathbf R),$$
where $\bm \mu=(\mu(\mathbf x_1),...,\mu(\mathbf x_n))^T$ is a vector of
the mean (or trend), $\sigma^2$ is a variance parameter, and $\mathbf R$
is the correlation matrix between outputs.

Similar to the no-discrepancy calibration, we allow user to model the
trend as
$\bm \mu(\mathbf x_i)=  \mathbf h(\mathbf x_i) \bm \theta_{m}=\sum^q_{t=1}h_t(\mathbf x_i)  \theta_{m,t}$,
where $\mathbf h(\mathbf x_i)$ is a row vector of mean basis and
$\bm \theta_{m}$ is a vector of trend parameters. Since the trend is
often modeled in the computer model, the default value of the trend
parameters is $\mathbf 0$.

The $(i,j)$th element of the correlation matrix is modeled by a kernel
function $R_{i,j}=K(\mathbf x_i, \mathbf x_j)$. We implement the
separable kernel
$K(\mathbf x_i, \mathbf x_j)=\prod^{p_x}_{l=1}K_l(\mathbf x_i, \mathbf x_j)$,
where $K_l$ models the correlation between the $l$th coordinate of the
observable input, for $l=1,...,p_x$. Some frequently used kernel
functions are listed in Table [1](#tab:kernel){reference-type="ref"
reference="tab:kernel"}. The range parameters $\bm \gamma$ are estimated
by default.

::: center
::: {#tab:kernel}
  GaSP calibration      Product kernel ($K(\mathbf d)=\prod^{p_x}_{l=1}K_l(d_l)$)
  --------------------- -------------------------------------------------------------------------------------------------------------------------------------------------------
  Matérn $\alpha=5/2$   $K_l(d_l)=\left(1+\frac{\sqrt{5}d_l}{\gamma_l}+\frac{5d^2_l}{3\gamma_l^2}\right)\exp\left(-\frac{\sqrt{5}d_l}{\gamma_l}\right)$
  Matérn $\alpha=3/2$   $K_l(d_l)=\left(1+\frac{\sqrt{3}d_l}{\gamma_l}\right)\exp\left(-\frac{\sqrt{3}d_l}{\gamma_l}\right)$
  Power exponential     $K_l(d_l)=\exp\left\{-\left(\frac{d_l}{\gamma_l}\right)^{\alpha_l}\right\}$, $0<\alpha_l\leq 2$
  S-GaSP calibration    Scaled kernel ($K_{Z_d}(\mathbf x_a,\mathbf x_b )=K(\mathbf x_a,\mathbf x_b ) - \mathbf r^T(\mathbf x_a)  {\mathbf R}_z^{-1} \mathbf r(\mathbf x_b)$)

  : Kernel functions implemented in
  [RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration).
  For any $\mathbf x_a, \mathbf x_b \in \mathcal X$, denote
  $\mathbf d=\mathbf x_a-\mathbf x_d=(d_1,...,d_{p_x})^T$. For any
  kernel $K$, the discretized scaled kernel
  $K_{Z_d}(\mathbf x_a,\mathbf x_b )$ with discretization points on
  observed points $\mathbf x_1,..., \mathbf x_n$ is implemented in the
  S-GaSP calibration, where
  ${\mathbf R}_z:=\mathbf R+n\mathbf I_n / \lambda_z$, and
  $\mathbf r(\mathbf x)=(K(\mathbf x, \mathbf x_1),...,K(\mathbf x, \mathbf x_n))^T$
  for any $\mathbf x$.
:::
:::

We denote the nugget parameter by the noise variance to signal variance
ratio $\eta=\sigma^2_0/\sigma^2$. Marginalizing out the random
discrepancy function $\delta(\cdot)$, one has
$\mathbf y^F \sim \mathcal{MN}(\mathbf f^M_{\theta}+\mathbf H \bm \theta_{m},\, \sigma^2_0 \tilde {\mathbf R})$,
where $\tilde {\mathbf R}=\mathbf R/\eta+ \bm \Lambda$. Here by default
$\bm \Lambda=\mathbf I_n$, and we allow users to specify the inverse
diagonal terms of $\bm \Lambda$ by the argument weights:
$w_i=1/\Lambda_{ii}$, for $i=1,...,n$.

The MLE of the trend and noise variance parameters in the GaSP
calibration follows
$\hat {\bm \theta}_m= \left( \mathbf H^T {{\mathbf { \tilde R}}}^{-1}  \mathbf H \right)^{-1} \mathbf H^T {{\mathbf { \tilde R}}}^{-1}{(\mathbf{y}-\mathbf f^M_{\theta})}$
and $\hat{\sigma}^2_0 =S^2_{K}/n$, where
$S^2_K=  (\mathbf{y}-\mathbf f^M_{\theta}- \mathbf H^T \hat{\bm \theta}_m  )^T  {{\mathbf { \tilde R}}}^{-1}(\mathbf{y}-\mathbf f^M_{\theta}- \mathbf H^T \hat{\bm \theta}_m  )$.
Plugging in the MLE of trend and noise variance parameters, one can
numerically maximize the profile likelihood to obtain the calibration,
kernel and nugget parameters in the GaSP calibration:
$$(\hat {\bm \theta}, \hat {\bm \gamma}, \hat \eta ) =\mathop{\mathrm{argmax}}_{(\bm \theta, \bm \gamma, \eta)} \mathcal L_K( \bm \theta, \bm \gamma, \eta \mid \hat {\bm \theta}_m, \hat \sigma^2_0 )  
\label{equ:MLE_GaSP}$$ where the closed form expression of the profile
likelihood
$\mathcal L_K( \bm \theta, \bm \gamma, \eta \mid { \hat {\bm \theta}_m}, \hat \sigma^2_0 )$
with kernel $K(.,.)$ is given in the Appendix.

After obtaining the estimation of the parameters, the predictive
distribution of reality on any $\mathbf x^*$ can be obtained by plugging
in the estimator
$$y^R(\mathbf x^*)\mid \mathbf y^F, \hat{\bm \theta}_m, \hat \sigma^2_0, \hat {\bm \theta}, \hat {\bm \gamma}, \hat \eta \sim \mathcal N( \hat \mu(\mathbf x^*), \hat \sigma^2_0(K^*/\hat \eta+   \Lambda^*) ), 
 \label{equ:predict_dist_gasp}$$ where $\Lambda^*$ is the weight at
$\mathbf x^*$ set to be 1 by default, and $$\begin{aligned}
\hat \mu(\mathbf x^*)&=f^M(\mathbf x^*, \hat {\bm \theta} )+\mathbf h(\mathbf x^*) \hat{\bm \theta}_m+ \mathbf r^T(\mathbf x^*)  \tilde {\mathbf R}^{-1}(\mathbf y^F- \mathbf f(\mathbf x_{1:n},\hat {\bm \theta} )-\mathbf H \hat{\bm \theta}_m ), \\
K^*&=K(\mathbf x^*, \mathbf x^*)- \mathbf r^T(\mathbf x^*)  \tilde {\mathbf R}^{-1} \mathbf r(\mathbf x^*), 
  
\end{aligned}$$ with
$\mathbf r(\mathbf x^*)=(K(\mathbf x^*, \mathbf x_1),...,K(\mathbf x^*, \mathbf x_n))^T$.
Though the predictive mean $\hat \mu(\mathbf x^*)$ may be the most
accurate for predictions, the calibrated computer model
$f^M(\mathbf x^*, \hat {\bm \theta} )$ is sometimes of interest for
predicting the reality, due to its interpretability. Thus, we recorded
three slots for predicting the reality: 1) the calibrated computer model
$f^M(\mathbf x^*, \hat {\bm \theta} )$ by math_model_mean_no_trend, 2)
The calibrated computer model with trend
$f^M(\mathbf x^*, \hat {\bm \theta} )+\mathbf h(\mathbf x^*) \hat {\bm \theta}_m$
by math_model_mean , and 3) the predictive mean $\hat \mu(\mathbf x^*)$
by mean. Furthermore, users can also output the predictive interval
based on
([\[equ:predict_dist_gasp\]](#equ:predict_dist_gasp){reference-type="ref"
reference="equ:predict_dist_gasp"}) by specifying interval_est in the
predict.rcalibration function. For example, setting
interval_est=c(0.025,0.975) gives the 95% predictive interval of the
reality. Furthermore, if interval_data=T, the predictive interval of the
data will be computed.

Numerically computing the MLE can be unstable as the profile likelihood
$\mathcal L_K( \bm \theta, \bm \gamma, \eta\mid {\bm \hat \theta_m}, \hat \sigma^2_0 )$
is typically nonlinear and nonconvex with respect to the parameters
$(\bm \theta, \bm \gamma, \eta)$. To obtain the uncertainty assessment
of the estimates and to avoid instability in the numerical search in
computing the MLE, we implement the MCMC algorithm for Bayesian
inference. The posterior distribution is
$$p(\bm \theta_m, \sigma^2_0, \bm \theta, \bm \gamma, \eta\mid \mathbf y^F) \propto  p_K(\mathbf y^F \mid \bm \theta, \bm \gamma, \eta, {\bm  \theta_m},  \sigma^2_0 ) \pi(\bm \theta) \pi({\bm  \theta_m},  \sigma^2_0, \bm \gamma,\eta), 
\label{equ:post}$$ where $p_K(.)$ is a multivariate normal density with
covariance function of the discrepancy being $\sigma^2K(\cdot,\cdot)$,
and $\pi(\bm \theta)$ is the prior of the calibration parameters,
assumed to be uniform over the parameter space. The prior of the mean,
variance, kernel and nugget parameters takes the form below
$$\pi({\bm  \theta_m},  \sigma^2_0, \bm \gamma,\eta) \propto \frac{\pi(\bm \gamma,\eta)}{\sigma^2_0},$$
where the usual reference prior of the trend and variance parameters
$\pi({\bm  \theta_m},  \sigma^2_0)\propto 1/\sigma^2_0$ is assumed. We
use jointly robust prior for the kernel and nugget parameters
$\pi(\bm \gamma,\eta)$, as it has a similar tail density decay rate as
the reference prior, but it is easier and more robust to compute
[@gu2019jointly]. Closed form expressions of the posterior distributions
and MCMC algorithm are provided in the Appendix.

In
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration),
after burn-in posterior samples
$(\bm \theta^{(i)}_m, (\sigma^2_0)^{(i)}, \bm \theta^{(i)}, \bm \gamma^{(i)}, \eta^{(i)})$
are recorded for $i=S_0+1,...,S$. Users can record a subset of posterior
samples to reduce storage by thinning the Markov chain. For instance,
thinning=5 in the rcalibration function records $1/5$ of posterior
samples.

We also output three types of predictions. For instance, suppose we
obtain $S$ posterior samples with first $S_0$ samples used as burn-in
samples, the predictive mean of the reality can be computed by
$$\hat \mu(\mathbf x^*)=\sum^S_{i=S_0+1} f^M(\mathbf x^*,  {\bm \theta}^{(i)} )+\mathbf h(\mathbf x^*)  {\bm \theta_m^{(i)}}+ (\mathbf r^{(i)}(\mathbf x^*))^T  (\tilde {\mathbf R}^{(i)})^{-1}(\mathbf y^F- \mathbf f^M(\mathbf x_{1:n}, {\bm \theta}^{(i)})-\mathbf H {\bm \theta_m^{(i)}} ),$$
where $\mathbf r^{(i)}(\mathbf x^*)$ and ${\mathbf {\tilde R}}^{(i)}$
are obtained by plugging in the $i$th posterior samples. The predictive
interval can also be computed by specifying the argument interval_est in
predict.rcalibration function.

Code below implements the GaSP calibration of parameter estimation and
predictions for the example in [@bayarri2007framework] that was
discussed in the previous subsection.

::: example
R\> m_gasp=rcalibration(input, output,math_model =
Bayarri_07,theta_range = theta_range, + X =X, have_trend =
T,discrepancy_type = 'GaSP') R\>
m_gasp_pred=predict(m_gasp,testing_input,math_model=Bayarri_07, +
interval_est=c(0.025, 0.975),X_testing=X_testing)
:::

Predictions and posterior samples from the GaSP calibration are plotted
in Figure [2](#fig:bayarri_2007){reference-type="ref"
reference="fig:bayarri_2007"}. The associated predictive error is shown
in Table [2](#tab:prediction_bayarri_2007){reference-type="ref"
reference="tab:prediction_bayarri_2007"}. Comparing the first two rows
in Table [2](#tab:prediction_bayarri_2007){reference-type="ref"
reference="tab:prediction_bayarri_2007"}, the predictive RMSE in the
no-discrepancy calibration is around $0.25$, and it decreases to $0.15$,
after adding the discrepancy function modeled by GaSP. Around 97.5% of
the held-out truth is covered by the 95% predictive interval of the
reality in the GaSP calibration. In comparison, the 95% predictive
interval of the reality in the no-discrepancy calibration only covers
around $80\%$ of the held-out reality.

As shown in the right panel in Figure
[2](#fig:bayarri_2007){reference-type="ref"
reference="fig:bayarri_2007"}, the posterior samples from GaSP spread
over a large range, reflecting the large uncertainty in parameter
estimation. This is because the discrepancy modeled by GaSP with the
frequently used kernel listed in Table
[1](#tab:kernel){reference-type="ref" reference="tab:kernel"} is very
flexible. As a result, the calibrated computer model by GaSP can be less
accurate in predicting the reality. To address this problem, we
introduce a new approach that induces a scaled kernel to constrain the
discrepancy function.

::: center
::: {#tab:prediction_bayarri_2007}
                     RMSE (CM+trend)   RMSE (with discrepancy)   $P_{CI}(95\%)$   $L_{CI}(95\%)$ 
  ---------------- ----------------- ------------------------- ---------------- ---------------- --
  GaSP                         0.253                     0.151            0.975            0.880 
  S-GaSP                       0.228                     0.131            0.955             1.15 
  No-discrepancy               0.250                         /            0.795            0.409 

  : Predictive accuracy and uncertainty assessment. The RMSE of the
  out-of-sample prediction based on the calibrated computer model (CM)
  and trend are shown in the second column. The predictive RMSE by the
  summation of the calibrated computer model, trend and discrepancy is
  given in the third column. The proportion of the held-out reality
  covered in the $95\%$ predictive interval, and average lengths of
  predictive intervals are given in the last two columns, respectively.
:::
:::

## Scaled Gaussian stochastic process models of discrepancy functions

Scaling the kernel of GaSP for model calibration was introduced in
[@gu2018sgasp], where the random $L_2$ distance between the discrepancy
model was scaled to have more prior probability mass at small values, as
small distance indicates the computer model fits the reality well. Note
that we leave $\sigma^2$ as a free parameter to be estimated from data,
and thus S-GaSP is still a flexible model of discrepancy.

In
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration),
we implemented the discretized S-GaSP to scale the random mean squared
error between the reality and computer model: $$\begin{aligned}
&\delta_{z_d}(\mathbf x) = \left\{ \delta(\mathbf x) \mid \frac{1}{n}{\sum^{n}_{i=1} \delta( \mathbf x_{i} )^2} =Z_d\right\}, 
\label{equ:delta_z_d}
\end{aligned}$$ with the subscript 'd' denoting discretization, and the
density of $Z_d$ is defined as $$\begin{aligned}
p_{Z_d}(z) &=	\frac{g_{Z_d}\left(z  \right) p_{ \delta}\left(Z_d=z  \right)}{\int_0^\infty g_{Z_d}\left( t \right)  p_{ \delta}\left(Z_d = t \right)d t },
\label{equ:p_Z_d}
\end{aligned}$$ where $p_{ \delta}(Z_d = z)$ is the density of $Z_d$
induced by the GaSP with kernel $K(\cdot,\cdot)$, and $g_Z(\cdot)$ is a
nondecreasing function that places more probability mass on smaller
values of the $L_2$ loss. For a frequently used kernel function in Table
[1](#tab:kernel){reference-type="ref" reference="tab:kernel"}, the
probability measure of the GaSP places a large probability at large
$L_2$ loss $p_{ \delta}\left(Z_d=z  \right)$ when the correlation is
large, dragging the calibrated computer model away from the reality. In
the S-GaSP calibration, the measure for $Z_d$ was scaled to have more
probability mass near zero by a scaling function $g_{Z_d}(z)$. The
default scaling function is assumed to be an exponential distribution,
$$g_{Z_d}(z) = \frac{\lambda_z }{2\sigma^2 } \exp\left(-\frac {\lambda_z z}{2\sigma^2 }\right),
 \label{equ:g_Z_d}$$ where $\lambda_z$ controls how large the scaling
factor is. Large $\lambda_z$ concentrates more prior probability mass at
the origin. As shown in [@gu2022theoretical],
$\lambda_z \propto \sqrt{n}$ guarantees the convergence of the
calibration parameters to the ones that minimize the $L_2$ distance
between the reality and computer model. We let the default choice be
$\lambda_z=(\lambda || \bm {\tilde \gamma}||)^{-1/2}$, where
$\lambda=(\sigma^2_0/\sigma^2n)$ is the regularization parameter in the
kernel ridge regression, and
$\tilde {\bm \gamma}=(\gamma_1/L_1,...,\gamma_{p_x}/L_{p_x})^T$, with
$L_i$ being the length of domain of the $i$th coordinate of the
calibration parameter. We allow users to specify $\lambda_z$ via the
argument lambda_z in the rcalibration function.

The default choice of the scaling function in
([\[equ:g_Z_d\]](#equ:g_Z_d){reference-type="ref"
reference="equ:g_Z_d"}) has computational advantages. After
marginalizing out $Z_d$, it follows from Lemma 2.4 in [@gu2018sgasp]
that $\delta_{z_d}(\cdot)$ has the covariance function
$$\sigma^2 K_{z_d}(\mathbf x_a, \mathbf x_b)=\sigma^2 \left\{K(\mathbf x_a,\mathbf x_b ) - \mathbf r^T(\mathbf x_a)  \left(\mathbf R+\frac{n\mathbf I_n}{\lambda_z} \right)^{-1} \mathbf r(\mathbf x_b)\right\},
\label{equ:sigma_2_K_z_d}$$ for any $\mathbf x_a, \mathbf x_b$, where
$\mathbf r(\mathbf x)=(K(\mathbf x, \mathbf x_1),...,K(\mathbf x, \mathbf x_n))^T$
for any $\mathbf x$. The covariance matrix of the S-GaSP model of
$(\delta_z(\mathbf x_1),..., \delta_z(\mathbf x_n))^T$ follows
$$\sigma^2 \mathbf R_z=\sigma^2  \left\{\mathbf R- \mathbf R   \left(\mathbf R+\frac{n\mathbf I_n}{\lambda_z} \right)^{-1}\mathbf R\right\}.    
 \label{equ:R_z}$$ The closed-form expression in Equation
([\[equ:sigma_2_K_z_d\]](#equ:sigma_2_K_z_d){reference-type="ref"
reference="equ:sigma_2_K_z_d"}) avoids sampling
$(\delta_z(\mathbf x_1),...,\delta_z(\mathbf x_n))$ to obtain $Z_d$ from
the posterior distribution, which greatly improves the computational
speed.

For any covariance function $K(\cdot,\cdot)$ in the GaSP calibration, a
scaled kernel was induced in S-GaSP calibration, shown in the last row
of Table [1](#tab:kernel){reference-type="ref" reference="tab:kernel"}.
The S-GaSP model of the discrepancy can be specified by the argument
discrepancy_type='S-GaSP' in the rcalibration function. Similar to the
model of GaSP-calibration, we also provide MLE and posterior samples for
estimating the parameters, with arguments method='mle' and
method='post_sample', respectively. The MLE of parameters in S-GaSP are
computed by maximizing the profile likelihood
$$(\hat {\bm \theta}, \hat {\bm \gamma}, \hat \eta ) =\mathop{\mathrm{argmax}}_{(\bm \theta, \bm \gamma, \eta)} \mathcal L_{K_{Z_d}}( \bm \theta, \bm \gamma, \eta, \mid { \hat {\bm \theta}_m},\hat \sigma^2_0 ),   
\label{equ:MLE_SGaSP}$$ where
$({ \hat {\bm \theta}_m}, \hat \sigma^2_0)$ denotes the MLE of trend and
noise variance parameters in S-GaSP calibration, and
$\mathcal L_{K_{Z_d}}$ is the profile likelihood with respect to the
scaled kernel $K_{Z_d}$ in
([\[equ:sigma_2_K_z_d\]](#equ:sigma_2_K_z_d){reference-type="ref"
reference="equ:sigma_2_K_z_d"}). Furthermore, point predictions and
intervals can be obtained by the predict.rcalibration function as well.

The code below gives S-GaSP calibration and prediction of the example in
[@bayarri2007framework].

::: example
R\> m_sgasp=rcalibration(input, output,math_model =
Bayarri_07,theta_range = theta_range, + X =X, have_trend =
T,discrepancy_type = 'S-GaSP') R\>
m_sgasp_pred=predict(m_sgasp,testing_input,math_model=Bayarri_07, +
interval_est=c(0.025, 0.975),X_testing=X_testing)
:::

Predictions from S-GaSP are plotted as blue curves in Figure
[2](#fig:bayarri_2007){reference-type="ref"
reference="fig:bayarri_2007"} and numerical comparisons are given in
Table [2](#tab:prediction_bayarri_2007){reference-type="ref"
reference="tab:prediction_bayarri_2007"}. With the discrepancy modeled
by S-GaSP, the prediction of reality is more accurate at
$x \in [0.5,1.5]$, compared to no-discrepancy calibration. Note that
predictions from the S-GaSP model also extrapolate the reality at
$x\in [3,5]$ accurately, because the calibrated computer model is closer
to the truth. Furthermore, the 95% predictive interval by S-GaSP covers
around $95\%$ of the held-out samples.

To compare and illustrate different calibration approaches for
differential equations, we discuss another example of the Lorenz-96
system used in modeling atmospheric dynamics
[@lorenz1996predictability]. The mathematical model is written as the
following differential equations: $$\begin{aligned}
\dot x_j(t) = (x_{j+1}(t)-x_{j-2}(t))x_{j-1}(t) - x_j(t)  + \theta, 
 
\end{aligned}$$ for $j=1,...,k$ states, with $k=40$ and $\theta$ is a
scalar value of the force. We further denote that $x_{-1}=x_{k-1}$,
$x_0 = x_k$ and $x_{k+1}=x_{1}$ for any $t$. The latent variable $x_j$
can model the atmospheric quantities, such as temperature or pressure,
measured at $k$ positions along a constant latitude circle. This model
is widely used for data assimilation
[@maclean2020surrogate; @brajard2020combining].

We consider model calibration in two scenarios: $$\begin{aligned}
 \mbox{Scenario 1: } \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  y_j(t)&= x_j(t) +\epsilon,    \quad \quad \quad \quad \quad \quad \quad   \quad \quad \quad \quad \quad  \label{equ:no_discrepancy}\\ 
\mbox{Scenario 2: } \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad y_j(t)&= x_j(t) +2t \sin\left(\frac{2\pi j}{k}\right)+ \epsilon,   \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \label{equ:with_discrepancy}
 
\end{aligned}$$ for $j=1,...,k$ and $\epsilon \sim \mathcal{N}(0, 1)$
being an independent Gaussian noise. In the first scenario, the
mathematical model is the true model and in the second scenario, a
discrepancy term is included for simulating the field data. The
discrepancy is treated as unknown. We initialize the states by a
multivariate normal distribution with the covariance generated from a
Wishart distribution with $k$ degrees of freedom and the scale matrix
being a diagonal matrix. We use the Runge Kutta method with order 4 to
numerically solve the system. Since the computer model is fast, we do
not include an emulator. We simulate the reality at $40$ time points,
with a time step of $0.05$ between the time points.

<figure id="fig:lorenz_96">
<table>
<tbody>
<tr class="odd">
<td style="text-align: center;"><embed
src="figures/Lorenz_96_reality.pdf" style="width:33.0%;height:50.0%" />
<embed src="figures/Lorenz_96_obs.pdf"
style="width:33.0%;height:50.0%" /> <embed
src="figures/sample_theta_Lorenz_96.pdf"
style="width:33.0%;height:50.0%" /></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><embed
src="figures/no_discrepancy_Lorenz_96.pdf"
style="width:33.0%;height:50.0%" /> <embed
src="figures/gasp_Lorenz_96.pdf" style="width:33.0%;height:50.0%" />
<embed src="figures/sgasp_discrepancy_Lorenz_96.pdf"
style="width:33.0%;height:50.0%" /></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<figcaption>The reality and full observations of Lorenz-96 system in
Scenario 1 are plotted in the upper left panel and upper middle panel,
respectively, where the black circles are the <span
class="math inline">5%</span> observations used for model calibration.
Posterior samples of calibration parameters of different calibration
methods are plotted in the upper right panel. The lower panels give the
difference between the reality and calibrated computer model of all
latent states. </figcaption>
</figure>

<figure id="fig:lorenz_96_discrepancy">
<table>
<tbody>
<tr class="odd">
<td style="text-align: center;"><embed
src="figures/Lorenz_96_reality_with_discrepancy.pdf"
style="width:33.0%;height:50.0%" /> <embed
src="figures/Lorenz_96_obs_with_discrepancy.pdf"
style="width:33.0%;height:50.0%" /> <embed
src="figures/sample_theta_Lorenz_96_with_discrepancy.pdf"
style="width:33.0%;height:50.0%" /></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><embed
src="figures/no_discrepancy_Lorenz_96_with_discrepancy.pdf"
style="width:33.0%;height:50.0%" /> <embed
src="figures/gasp_Lorenz_96_with_discrepancy.pdf"
style="width:33.0%;height:50.0%" /> <embed
src="figures/sgasp_discrepancy_Lorenz_96_with_discrepancy.pdf"
style="width:33.0%;height:50.0%" /></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<figcaption>The reality of the discrepancy-included Lorenz-96 system
(Scenario 2) is plotted in the upper left panel. The simulated
observations are plotted in the upper middle panel, where the <span
class="math inline">5%</span> of the observations plotted as black
circles are used in model calibration. Posterior samples of calibration
parameters of different calibration methods are plotted in the upper
right panel. The lower panels show the differences between the reality
and the predictive mean of all latent states. </figcaption>
</figure>

The unobserved reality simulated by the Lorenz-96 system is plotted in
upper panel in Figure [3](#fig:lorenz_96){reference-type="ref"
reference="fig:lorenz_96"}. Only $5\%$ observations (i.e. 2 observations
at each time point, plotted as black circles in upper middle panel) are
available for model calibration. The posterior parameters of
no-discrepancy calibration, GaSP and S-GaSP calibration are plotted in
the upper right panel. Since the computer model (Lorenz-96) is the true
model in this scenario, the no-discrepancy calibration is expected to
perform well. Even though one allows a flexible discrepancy function,
modeled as GaSP or S-GaSP, it seems parameters are estimated reasonably
well in both approaches. In all methods, posterior samples are close to
the true parameter ($\theta=8$), and the range of the posterior samples
is narrow compared to the parameter range $[-20, 20]^2$. Furthermore,
the estimate states by the calibrated computer model and the reality are
plotted in the lower panels in Figure
[3](#fig:lorenz_96){reference-type="ref" reference="fig:lorenz_96"}. All
methods have small estimation errors, as the parameters are estimated
well.

The reality, observations, posterior samples, and predictions from the
different calibration models of a discrepancy-included Lorenz-96 system
are graphed in Figure
[4](#fig:lorenz_96_discrepancy){reference-type="ref"
reference="fig:lorenz_96_discrepancy"}. Note that since the discrepancy
is included, there is no true calibration parameter. The states
estimated by the no-discrepancy calibration model (shown in the lower
left panel) have a relatively large error. This is not surprising as the
Lorenz-96 system is a misspecified model. The predictive means of GaSP
and S-GaSP models which includes both the calibrated computer model and
discrepancy function is more accurate, as both models capture the
discrepancy between the reality and computer model. The estimation error
of all models is larger than the ones when there is no discrepancy. This
is because the measurement error is large and we only observe 2 states
at each time point. Increasing the number of observations or reducing
the variance of measurement error can improve predictive accuracy.

## Calibration with repeated experiments

Repeated experiments or replicates are commonly used in experiments, as
they are helpful for assessing the experimental error. Consider, for
example, $k_i$ replicates available for each observable input
$\mathbf x_i$, denoted as
$\mathbf y^F_i=(y_1^F(\mathbf x_i),...,y^F_{k_i}(\mathbf x_i))^T$.
Denote
$\bar{\mathbf y}^F=(\sum^{k_1}_{j=1}y^F_{j}(\mathbf x_1)/k_1,...,\sum^{k_n}_{j=1}y^F_{j}(\mathbf x_n)/k_n)^T$
the aggregated data. The probability density of the field observations
can be written as: $$\begin{aligned}
p(\mathbf y^F_1,...,  \mathbf y^F_n \mid \bm \mu, \bm \delta, \bm \theta,  \sigma^2_0) &= (2\pi \sigma^2_0)^{-\frac{\sum^n_{i=1} k_i}{2}} \exp\left(-\frac{\sum^n_{i=1} \omega_i \sum^{k_i}_{j=1}  (y^F_j(\mathbf x_i)- f^M(\mathbf x_i,\bm \theta)-\mu_i-\delta(\mathbf x_i) )^2  }{2\sigma^2_0}\right) \\
&=   (2\pi\sigma^2_0)^{-\frac{\sum^n_{i=1} k_i}{2}} \exp\left(-\frac{ S^2_f }{2\sigma^2_0}- \frac{\sum^n_{i=1} k_i \omega_i ( \bar{ y}^F_i-f^M(\mathbf x_i,\bm \theta)-\mu_i-\delta(\mathbf x_i) )^2 }{2\sigma^2_0}\right)
\end{aligned}$$ where
$S^2_f=\sum^n_{i=1}\omega_i\sum^{k_i}_{j=1}   (y^F_j(\mathbf x_i)-\bar{ y}^F_i )^2=\sum^n_{i=1}{\omega_i(\mathbf y^F_i-\bar{ y}^F_i\mathbf 1_{k_i} )^T(\mathbf y^F_i-\bar{ y}^F_i \mathbf 1_{k_i}) }$
with the subscript 'f' denoting the field observations. After
integrating out the discrepancy term, assumed to be modeled as a GaSP as
an example, the marginal likelihood of the parameters follows
$$\begin{aligned}
\mathcal L_K(\bm \theta, \bm \theta_m, \sigma^2_0, \bm \gamma,\eta )\propto (\sigma^2_0)^{-\frac{\sum^n_{i=1} k_i}{2} }| \tilde {\tilde {\bm R}}|^{-\frac{1}{2} }  \exp\left\{- \frac{(\bar{\mathbf y}^F-\mathbf f^M_{\theta}-\mu \mathbf 1_n )^T \tilde {\tilde {\bm R}}^{-1} (\bar{\mathbf y}^F- \mathbf f^M_{\theta}-\mu \mathbf 1_n)}{2 \sigma^2_0} -\frac{S^2_f}{2\sigma^2_0} \right\}, 
\label{equ:lik_replicates}
\end{aligned}$$ where
$\tilde {\tilde {\bm R}}= ( \mathbf R/\eta+  \tilde {\bm \Lambda})$,
with $\tilde {\bm \Lambda}$ being an diagonal matrix with diagonal entry
$\Lambda_{ii}=1/(\omega_ik_i)$, for $i=1,2,...,n$, and
$\eta= \sigma^2_0/\sigma^2$. The S-GaSP calibration with replications
can also be defined, by simply replacing $\mathbf R$ by $\mathbf R_z$
where the $(i,j)$th term is defined by the scaled kernel in Table
[1](#tab:kernel){reference-type="ref" reference="tab:kernel"}.

The likelihood in
([\[equ:lik_replicates\]](#equ:lik_replicates){reference-type="ref"
reference="equ:lik_replicates"}) has a clear computational advantage. If
one directly computes the covariance and its inverse, the computational
complexity is $O( (\sum^{n}_{i=1}k_i)^3 )$, whereas the computational
order by Equation
([\[equ:lik_replicates\]](#equ:lik_replicates){reference-type="ref"
reference="equ:lik_replicates"}) is only $O(n^3)+O(\sum^{n}_{i=1}k_i)$.
Suppose $k_i=k$, for $i=1,...,n$, using Equation
([\[equ:lik_replicates\]](#equ:lik_replicates){reference-type="ref"
reference="equ:lik_replicates"}) is around $k^3$ times faster than
directly computing the likelihood, with no loss of information.

One can specify replications by inputting a $n\times k$ matrix into the
argument observations in the rcalibration function for scenarios with
$n$ observable inputs, each containing $k$ repeated measurements. Or one
can give a list of observations in the rcalibration function with size
$n$, where each element in the list contains $k_i$ values of replicates,
for $i=1,...,n$. For observations with replicates, MLE and posterior
sampling were implemented in the rcalibration function, and predictions
were implemented in the predict.rcalibration function.

## Statistical emulators

Users can specify a mathematical model via the math_model argument in
rcalibration function if the closed-form expression of the mathematical
model is available. However, closed-form solutions of physical systems
are typically unavailable, and numerical solvers are required.
Simulating the outputs from computer models can be slow. For instance,
the TITAN2D computer model, which simulates pyroclastic flows for
geological hazard quantification, takes roughly 8 minutes per set of
inputs [@simakov2019modernizing]. As hundreds of thousands of model runs
are sometimes required for model calibration [@anderson2019magma],
directly computing the computer model by numerical methods is often
prohibitive. In this scenario, we construct a statistical emulator as a
surrogate model, to approximate the computer simulation, based on a
small number of computer model runs.

To start, assume that we have obtained the output of the computer model
at $D$ input design points, $(\bm \theta_1,...,\bm \theta_D)$, often
selected from an "space-filling\" design, such as the Latin hypercube
design [@santner2003design]. There are two types of computer model
outputs: 1) scalar-valued outputs: $f^M(\bm \theta) \in \mathbb R$, and
2) vector-valued outputs at $k$ coordinates
$\mathbf f^M(\mathbf x_{1:k}, \bm \theta) \in \mathbb R^k$, where $k$
can be as large as $10^6$. Various packages are available for fitting
scalar-valued GP emulators, such as
[DiceKriging](https://CRAN.R-project.org/package=DiceKriging)
([@roustant2012dicekriging]),
[GPfit](https://CRAN.R-project.org/package=GPfit)
([@macdonald2015gpfit]), and
[RobustGaSP](https://CRAN.R-project.org/package=RobustGaSP)
([@gu2018robustgasp]). Some of these packages were used in Bayesian
model calibration packages [@palomo2015save; @carmassi2018calico].
However, the emulator of computer models with vector-valued outputs, a
common scenario in different disciplines
[@higdon2008computer; @ma2022multifidelity; @li2022efficient; @fang2022reliable]
was rarely implemented in the model calibration packages.

In
[Robustcalibration](https://CRAN.R-project.org/package=Robustcalibration),
we call the rgasp function and ppgasp function from the
[RobustGaSP](https://CRAN.R-project.org/package=RobustGaSP) package for
emulating scalar-valued and vector-valued computer model outputs,
respectively. The parallel partial Gaussian process (PP-GaSP) by the
ppgasp function has two advantages. First, computing the predictive mean
by the PP-GaSP only takes $O(kD)+O(D^3)$ operations, which is
particularly suitable for computer models with a large number of outputs
($k$). Second, as the covariance over $\bm \theta$ is shared across all
grids, the estimation of PP-GaSP is more stable than building a separate
emulator on each output coordinate. Furthermore, the marginal posterior
mode with robust parameterization typically avoids the degenerated
estimation of the covariance matrix ([@gu2019jointly]).

To call the emulator, users can select the argument simulator=0, and
then specify the simulation runs in arguments input_simul, and
output_simul. One can input a vector into output_simul to emulate
scalar-valued computer models, or a matrix into output_simul to call the
PP-GaSP emulator for vector-valued outputs. For both scenarios, we use a
modular approach to fit the PP-GaSP emulator
[@bayarri2007framework; @liu2009modularization], where the predictive
distribution of the emulator depends on simulator runs, but not on field
observations. After fitting an emulator, the predictive mean of the
PP-GaSP emulator will be used to approximate the computer model at any
unobserved $\bm \theta^*$.

We illustrate the efficiency and accuracy of the PP-GaSP emulator for
approximating differential equations from [@box1956application], where
the interaction between two chemical substances $y_1$ and $y_2$ are
modeled as $$\begin{aligned}
\dot{y}_1(t)&=10^{\theta_1-3} y_1(t), \\
\dot{y}_2(t)&=10^{\theta_1-3} y_1(t) - 10^{\theta_2-3}{y}_2(t).
\end{aligned}$$ In each of the 6 time points $t=10,20,40,80,160$, and
$320$, 2 replicates of the second chemical substance are available in
[@box1956application]. Initial conditions are $y_{1}(t=0)=100$ and
$y_2(t=0)=0$. The computer model contains two parameters
$\theta_1\in [0.5, 1.5]$ and $\theta_2\in [0.5,1.5]$.

To start, we first use the default method in the ode function from the
[deSolve](https://CRAN.R-project.org/package=deSolve) package
[@soetaert2010solving] to numerically solve the ODEs at a given initial
condition and parameter set:

::: example
library(deSolve) R\> Box_model \<- function(time, state, parameters)  +
par \<- as.list(c(state, parameters)) + with(par,  +
dM1=-10\^parameters\[1\]-3\*M1 +
dM2=10\^parameters\[1\]-3\*M1-10\^parameters\[2\]-3\*M2 + list(c(dM1,
dM2)) + ) + R\> Box_model_solved\<-function(input, theta) + init \<-
c(M1 = 100 , M2 = 0) + out \<- ode(y = init, times = c(0,input), func =
Box_model, parms = theta) + return(out\[-1,3\]) +
:::

We specify the observations and range of calibration parameters from
[@box1956application] below.

::: example
R\> n=6 R\>
output=t(matrix(c(19.2,14,14.4,24,42.3,30.8,42.1,40.5,40.7,46.4,27.1,22.3),2,n))
R\> input=c(10,20,40,80,160,320) R\>
theta_range=matrix(c(0.5,0.5,1.5,1.5),2,2) ###the testing input for
emulator should contain the observed inputs R\>
testing_input=as.matrix(seq(1,350,1)) #if observed inputs are not
included, then add it R\> set_diff_obs=setdiff(input,testing_input) R\>
testing_input=sort(c(testing_input,set_diff_obs))
:::

We compare model calibration and predictions based on the numerical
solver by the [deSolve](https://CRAN.R-project.org/package=deSolve)
package, and by the GaSP emulator approach below. We first implement the
direct approach, where the numerical solver is called to generate
posterior samples.

::: example
R\> m_sgasp_time=system.time( +
m_sgasp=rcalibration(input,output,math_model=Box_model_solved, +
sd_proposal=c(0.25,0.25,1,1), + theta_range=theta_range) +
m_sgasp_pred=predict(m_sgasp,testing_input,math_model=Box_model_solved, +
interval_est=c(0.025,0.975)))
:::

We then generate 50 design points from maximin Latin hypercube design by
the [lhs](https://CRAN.R-project.org/package=lhs) package, and run a
numerical solver at these design points. As the simulator outputs a
vector $\mathbf f^M(t_{1:k},\bm \theta) \in \mathbb R^k$ for calibration
parameter, we call the PP-GaSP emulator to predict the output at all
time points. The simulator runs are then specified as input_simul and
output_simul in the rcalibration function. We let simul_type=0 to call
the emulator instead of the numerical solver to generate posterior
samples. The loc_index_emulator below gives a subset of the output
coordinates for the the PP-GaSP emulator to be predicted and by default,
the function will predict all output coordinates.

::: example
R\> m_sgasp_emulator_time=system.time( + ##constructing simulation
data + D=50 + p=2 + lhs_sample=maximinLHS(n=D,k=p) +
input_simul=matrix(NA,D,p) + input_simul\[,1\]=lhs_sample\[,1\]+0.5 +
input_simul\[,2\]=lhs_sample\[,2\]+0.5 +
k_simul=length(testing_input) + + output_simul=matrix(NA,D,k_simul) +
for(i_D in 1:D) +
output_simul\[i_D,\]=Box_model_solved(c(testing_input),
input_simul\[i_D,\]) +  + ##create loc_index_emulator as a subset of the
simulation output + loc_index_emulator=rep(NA,n) + for(i in 1:n) +
loc_index_emulator\[i\]=which(testing_input==input\[i\]) +  + +
##emulator +
m_sgasp_with_emulator=rcalibration(input,output,simul_type=0, +
input_simul=input_simul, output_simul=output_simul,simul_nug=T, +
loc_index_emulator=loc_index_emulator, + sd_proposal=c(0.25,0.25,1,1), +
theta_range=theta_range) +
m_sgasp_with_emulator_pred=predict(m_sgasp_with_emulator,testing_input) +
) R\> m_sgasp_time\[3\]/m_sgasp_emulator_time\[3\] elapsed 6.252316
:::

The approach with an emulator only costs around $1/6$ of the
computational time in this example, even though the numerical solver of
this simple ODE is fast. For computer models that take a few or hours to
run, the emulator approach can substantially reduce the computational
cost.

<figure id="fig:box_model">
<table>
<tbody>
<tr class="odd">
<td style="text-align: center;"><embed src="figures/Box_sgasp.pdf" />
<embed src="figures/Box_post_samples.pdf" /></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<figcaption>Comparison between the Bayesian model calibration based on
the numerical solver and the PP-GaSP emulator of the computer model. In
the left panel, the green solid curve is the predictive mean from the
calibrated computer model and the discrepancy function, and the
<span>blue</span> dashed curves are predictions from the calibrated
computer model alone, both of which call the numerical solver for each
posterior sample. The same result based on the emulator is plotted as
the <span>green</span> curves. The black dots are field observations and
the shaded area is the <span class="math inline">95%</span> predictive
interval of the reality by the approach that uses the PP-GaSP emulator.
In the right panel, posterior samples based on the numerical solver and
the PP-GaSP emulator are denoted by the <span>blue</span> dots and
<span>green</span> dots, respectively. </figcaption>
</figure>

Figure [5](#fig:box_model){reference-type="ref"
reference="fig:box_model"} gives predictions and posterior samples based
on a numerical solver and the PP-GaSP emulator. First, predictions from
the calibrated computer model and the discrepancy function, and by the
calibrated computer model itself are close to each other, implying that
the discrepancy modeled by a S-GaSP is close to be zero. Second,
predictions and posterior distributions from the numerical solver and
the emulator are close to each other, meaning that the emulator
approximates the solver well. Calibration with an emulator is preferred
as it is faster than numerically solving ODEs each time when generating
the posterior samples.

## Calibration with multiple sources of observations

In reality, one may have observations of different types and they may
come from multiple sources. In [@anderson2019magma], for instance,
multiple satellite interferogram and GPS observations measuring the
ground deformation during the Kı̄lauea volcano eruption in Summer 2018
are used for calibrating mechanical models that relate the observations
to calibration parameters, such as depth and shape of the magma chamber,
magma density, pressure change rate, and so on. In these applications,
the measurement bias, such as the atmospheric error, can substantially
affect the satellite measurements of the ground deformation
[@Zebker1997; @agram2015noise].

Model calibration using multiple sources of observations was studied in
[@gu2022calibration]. For each source $l$, $l=1,2,...,k$, let the field
observations of the $l$ th source at observable input $\mathbf x$ be
modeled as
$$\mathbf y^F_l(\mathbf x)=  f^M_l(\mathbf x, \bm \theta)+ \delta(\mathbf x)+ \delta_l(\mathbf x)  +\mu_l+\epsilon_{l},
\label{equ:measurement_bias}$$ where $f^M_l(\mathbf x, \bm \theta)$ is
the computer model for source $l$. The discrepancy function is denoted
by $\delta(\mathbf x)$ and the source-specific measurement bias is
denoted by $\delta_l(\mathbf x)$. Measurement bias often appears in the
satellite radar interferogram, as atmospheric error could affect the
quality of the image. The mean of each data source is denoted by
$\mu_l$, and the independent measurement noise is denoted by
$\epsilon_{l}$.

In
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration),
we allow users to integrate observations from multiple sources to
calibrate computer models by the function rcalibration_MS and to make
predictions using the function predict_MS.rcalibration_MS. The posterior
sampling method is implemented for parameter estimation for multiple
sources of data. Besides, both GaSP and S-GaSP models can be chosen to
model the measurement bias $\delta_l$, for $l=1,...,k$. Similar to
calibration model with a single source of data, we allow users to choose
a model with no-discrepancy, and with GaSP or S-GaSP model of the
discrepancy function $\delta$. Users can choose to have measurement bias
or not. We also allow users to integrate different types of measurements
for model calibration by using different designs of observable inputs.

To illustrate, we study a synthetic example for calibrating computer
models using multiple sources of observations. We assume the $l$th
source of data, $l=1,...,k$, is simulated below
$$y^F_l(x)=sin(\pi x)+ \delta(x)+\delta_l(x)+\epsilon_l(x),
  \label{equ:multi_calibration_measurement_bias}$$ where $\delta(.)$ and
$\delta(.)$ are independently simulated from Gaussian processes with
covariance $\sigma^2K(.,.)$ and $\sigma^2_lK_l(\cdot,\cdot)$, and the
independent noise follows
$\epsilon_l(x) \overset{i.i.d}{\sim} N(0,\sigma^2_0)$. We let
$\sigma_0=0.05$, $\sigma=0.2$ and $\sigma^2_l=0.5\times l/(l-1)$, for
$l=1,...,k$. The $K(.,.)$ and $K_l(\cdot,\cdot)$ are assumed to follow
Matérn kernel with roughness parameter being $\alpha=2.5$, and the range
parameter being $\gamma=1/30$ and $\gamma=1/10$, respectively. We
collect $n=100$ observations for each source $l$ with $x$ equally spaced
from $[0,1]$. We assume data from $k=5$ sources are available. Here the
mathematical model follows $f^M_l(x,\theta)=sin(\theta x)$, for
$l=1,...,k$, and the reality is $y^R(x)=f^M_l(x,\theta)+\delta(x)$. The
goal is to estimate the calibration parameter ($\theta$), the reality
($y^R(x)=f^M_l(x,\theta)+\delta(x)$), model discrepancy ($\delta(x)$),
and measurement bias ($\delta_l(x)$).

We compare four methods. The first two methods are GaSP and S-GaSP
calibration methods to model individual sources of data, with the
discrepancy model specified as a GaSP and S-GaSP respectively. The
measurement bias terms are modeled by GaSPs. For instance, the code
below implements S-GaSP model of discrepancy and GaSP model of the
measurement bias:

::: example
\>
model_sgasp=rcalibration_MS(design=input_measurement,observations=output,p_theta=1,
math_model=math_model, simul_type=rep(1,length(input_measurement)),
S=5000,S_0=2000,thinning=1,measurement_bias=T,shared_design=input_model,
have_measurement_bias_recorded=T,
discrepancy_type=c(rep('GaSP',length(input_measurement)),'S-GaSP'),
theta_range=matrix(c(-2\*pi,2\*pi),1,2),sd_proposal_theta=rep(0.02,2));
:::

where math_model, input_measurement and output are lists where each
contains the model, input and output of the field observations for each
source. The input_model is a matrix for the input of the discrepancy
function.

As modeling each source of the data can be time-consuming for some
applications, a common solution is to use the aggregated or stacked data
$\bar y^R(x)=\sum^k_{l=1} y^R_l(x)/k$ in calibration, as modeling the
aggregated data is around $k$ times faster than modeling the individual
sources of data. Thus, we also include two methods of modeling the
aggregated data for comparison, namely the GaSP Stack and S-GaSP Stack,
representing GaSP or S-GaSP discrepancy model, respectively.

<figure id="fig:multicalibration_eg">
<table>
<tbody>
<tr class="odd">
<td style="text-align: center;"><embed src="figures/reality_k_5.pdf"
style="width:50.0%;height:40.0%" /> <embed
src="figures/model_discrepancy_k_5.pdf"
style="width:50.0%;height:40.0%" /></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: center;"><embed
src="figures/measurement_bias_1_k_5.pdf"
style="width:50.0%;height:40.0%" /> <embed
src="figures/measurement_bias_2_k_5.pdf"
style="width:50.0%;height:40.0%" /></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<figcaption>Comparison between modeling individual data and aggregated
data for simulation from Equation (<a
href="#equ:multi_calibration_measurement_bias" data-reference-type="ref"
data-reference="equ:multi_calibration_measurement_bias">[equ:multi_calibration_measurement_bias]</a>).
The upper left and right panels give the reality and model discrepancy,
whereas the lower two panels give the measurement bias for the first two
sources. The estimation results from GaSP, S-GaSP, GaSP Stack and S-GaSP
calibration are plotted as red, blue, pink and cyan curves. The
observations from the first two sources are graphed as the triangles and
dots in the upper left panel. </figcaption>
</figure>

In Figure [6](#fig:multicalibration_eg){reference-type="ref"
reference="fig:multicalibration_eg"}, we plot the truth, observations of
the first two sources of observations, and the estimates of reality,
discrepancy, and measurement bias in two sources from four different
methods. Here the GaSP and S-GaSP perform better than the GaSP Stack and
S-GaSP Stack calibration methods, as aggregating data leads to loss of
information when the data contain measurement bias. The RMSE of reality,
discrepancy and measurement bias estimation of different methods are
given in the Table
[3](#tab:prediction_multicalibration){reference-type="ref"
reference="tab:prediction_multicalibration"}, which shows the S-GaSP
calibration of individual sources of data achieves the highest
calibration and predictive accuracy.

::: center
::: {#tab:prediction_multicalibration}
  RMSE             reality   discrepancy   measurement bias    
  -------------- --------- ------------- ------------------ -- --
  GaSP               0.238         0.194              0.247    
  S-GaSP             0.204         0.159              0.214    
  GaSP Stack         0.501         0.483              0.504    
  S-GaSP Stack       0.502         0.484              0.504    

  : The RMSE of the reality, discrepancy and measurement bias, where the
  smaller number indicates a smaller error. For measurement bias, we
  average the RMSE for each source of data.
:::
:::

Finally, the mean of posterior samples $\theta$ from the GaSP, S-GaSP,
GaSP Stack and S-GaSP Stack calibration for calibration parameter
$\theta$ are 2.46, 2.73, 2.15, and 2.17 respectively, whereas the truth
$\theta=\pi\approx 3.14$. This is a challenging scenario, as the large
measurement bias makes estimation hard. We found model calibration by
modeling individual sources of data is more accurate than modeling the
aggregated data, whereas modeling the aggregated data is more
computationally scalable.

# Concluding remarks

We have introduced the
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
package for Bayesian model calibration and data inversion. This package
has implemented a range of estimation methods and models, such as
posterior sampling and MLE, for no-discrepancy calibration, GaSP and
S-GaSP models of the discrepancy function. We implement statistical
emulators for approximating computationally expensive computer models
with both scalar-valued output or vector-valued output. The package is
applicable to observations from a single source, with or without
replications, and to observations from multiple sources or having
different types. We illustrate our approaches using mathematical models
with closed-form expressions or written as differential equations.

Even though we tried our best to consider different possible scenarios,
a comprehensive statistical package for model calibration and data
inversion is an ambitious goal. The
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
package provides tools for researchers to perform Bayesian model
inversion without the need to write emulators or MCMC samplers
themselves. We plan to improve the
[RobustCalibration](https://CRAN.R-project.org/package=RobustCalibration)
package with several specific directions in mind. First, we will make
the package more computationally scalable for structured data, such as
imaging and time series observations. Furthermore, we plan allow users
to specify other prior distributions of the parameters and proposal
distributions to sample from in future versions of the package. Third,
for problems like the Lorzen-96 system, it may be important to emulate
the time evolution operator, if forecasting future states is the goal.

# Appendix

## Auxiliary facts

1.  Let
    $\tilde {\bm R}_{\gamma,\eta}=\mathbf R_{\gamma}/\eta+ \bm \Lambda$
    be an $n\times n$ positive definite matrix as a function of
    parameters $\bm \gamma$ and $\eta$. Then for $l=1,...,p_x$:
    $$\begin{aligned}
    \frac{\partial \log |\tilde {\bm R}_{\gamma,\eta} | }{\partial  \gamma_l}=\mathop{\mathrm{tr}}\left( \frac{\tilde {\bm R}_{\gamma,\eta}^{-1}}{\eta} \frac{\partial \mathbf R_\gamma}{\partial \bm \gamma_l} \right) \quad \mbox{ and } \quad \frac{\partial \log |\tilde {\bm R}_{\gamma,\eta} | }{\partial \eta}=-\mathop{\mathrm{tr}}\left( \frac{\tilde {\bm R}_{\gamma,\eta}^{-1} \mathbf R_\gamma}{\eta^2} \right). 
    \end{aligned}$$ []{#item:derivative_1 label="item:derivative_1"}

2.  Let $\mathbf H$ be an $n\times q$ full rank matrix with $q<n$,
    $\mathbf Q=\left(\tilde {\bm R}^{-1}_{\gamma,\eta}- \tilde {\bm R}^{-1}_{\gamma,\eta} \mathbf H^T(\mathbf H^T \tilde {\bm R}^{-1}_{\gamma,\eta} \mathbf H)^{-1}   \mathbf H^T \tilde {\bm R}^{-1}_{\gamma,\eta} \right)$
    with
    $\tilde {\bm R}_{\gamma,\eta}=\mathbf R_{\gamma}/\eta+ \bm \Lambda$
    and $\mathbf y$ is an $n\times 1$ vector. Then for $l=1,...,p_x$:
    $$\begin{aligned}
    \frac{\partial \log  (\mathbf y^T \mathbf Q_{\gamma,\eta} \mathbf y)   }{\partial  \gamma_l}=-\frac{ \mathbf y^T \mathbf Q_{\gamma,\eta} \frac{\partial \mathbf R_\gamma}{\partial  \gamma_l} \mathbf Q_{\gamma,\eta} \mathbf y }{\eta \mathbf y^T \mathbf Q_{\gamma,\eta} \mathbf y } \quad \mbox{ and } \quad \frac{\partial \log  (\mathbf y^T \mathbf Q_{\gamma,\eta} \mathbf y)   }{\partial   \eta }=\frac{ \mathbf y^T \mathbf Q_{\gamma,\eta} \mathbf R_\gamma \mathbf Q_{\gamma,\eta} \mathbf y }{ \eta^2 \mathbf y^T \mathbf Q_{\gamma,\eta} \mathbf y }.
    \end{aligned}$$

3.  Suppose $v_{\bm \theta}$ is an $n\times 1$ vector as a function of
    $\bm \theta$ and $\mathbf {\tilde R}$ is an $n\times n$ positive
    definite matrix. Then for $i=1,...,p_{\theta}$:
    $$\frac{\partial \log(\mathbf v_{\bm \theta}^T \mathbf R^{-1} \mathbf v_{\bm \theta})}{ \partial \theta_l}= \frac{2 \mathbf v_{ \theta}^T\mathbf {\tilde R}^{-1}   \frac{\partial \mathbf v_{\theta}}{\partial  \theta_l}}{\mathbf v^T_{ \theta} \mathbf {\tilde R}^{-1} \mathbf v_{ \theta}}.$$
    []{#item:derivative_2 label="item:derivative_2"}

## Likelihood functions and posterior distributions

. Using the objective prior for the mean and variance parameters,
$\pi(\bm \theta_m, \sigma^2_0)\propto 1/\sigma^2_0$, the full
conditional posterior distributions of the trend and variance parameters
follow: $$\begin{aligned}
 \sigma^{-2}_0 \mid \bm \theta_m,  \bm \theta &\sim \mbox{Gamma}\left(\frac{n}{2},  \frac{(\mathbf y^F-\mathbf f^M_{\theta}- \mathbf H \bm \theta_m )^T\bm \Lambda^{-1}(\mathbf y^F-\mathbf f^M_{\theta}- \mathbf H \bm \theta_m )}{2}  \right), \\ 
\bm \theta_m \mid \sigma^2_0, \bm \theta &\sim \mathcal N\left(   \bm { \hat \theta}^{LS}_m, \,  \sigma^{-2}_0(\mathbf H^T\bm \Lambda^{-1} \mathbf H)^{-1} \right).  
\end{aligned}$$ The posterior distribution of the calibration parameters
follows: $$\begin{aligned}
p(\bm \theta \mid \bm \theta_m, \bm \sigma^{2}_0, \bm \beta, \eta )\propto \exp\left( - \frac{(\mathbf y^F-\mathbf f^M_{\theta}- \mathbf H \bm \theta_m )^T\bm \Lambda^{-1}(\mathbf y^F-\mathbf f^M_{\theta}- \mathbf H \bm \theta_m )  }{2\sigma^2_0}\right)\pi(\bm \theta),
\end{aligned}$$ where $\pi(\bm \theta)$ is the prior of calibration
parameters, where the default prior distribution is the uniform
distribution $\pi(\bm \theta)\propto 1$. We use the Metropolis algorithm
to sample the calibration parameters as a block, as we only need to
compute the likelihood once in each iteration. The proposal distribution
of each coordinate of the calibration parameter vector is chosen as a
normal distribution with a pre-specified standard deviation,
proportional to the range of the parameters. The default proportion is
$0.05$, and users can adjust the proportion by changing the first
$p_{\theta}$ value in the argument sd_proposal in rcalibration function.

After obtaining $S$ posterior samples with the first $S_0$ burn-in
samples, the predictive mean of reality based on calibrated computer
model and trend by the no-discrepancy calibration can be computed by:
$$\hat \mu(\mathbf x^*)=\frac{1}{S-S_0}\sum^S_{i=S_0+1} f^M(\mathbf x^*,  {\bm \theta}^{(i)} )+\mathbf h(\mathbf x^*)  {\bm \theta_m^{(i)}}.$$
The posterior credible interval of the reality $y^R(\mathbf x^*)$ in the
no-discrepancy calibration can be obtained by the empirical quantile of
posterior samples
$f^M(\mathbf x^*,  {\bm \theta}^{(i)} )+\mathbf h(\mathbf x^*)  {\bm \theta_m^{(i)}}$.
For example, the posterior credible interval of $1-\alpha$ percentile of
the data in no-discrepancy calibration can be computed by based on
$\frac{1}{S-S_0}\sum^S_{i=S_0+1}\left(f^M(\mathbf x^*,  {\bm \theta}^{(i)} )+\mathbf h(\mathbf x^*)  {\bm \theta_m^{(i)}}+\sqrt{\frac{\sigma^{(i)}_0}{ w^*} }Z_{\alpha}\right)$,
where $Z_{\alpha}$ is an upper $\alpha$ quantile of the standard normal
distribution and $w^*$ is the relative test output weight, assumed to be
1 by default.

. The profile likelihood function in GaSP calibration follows
$$\mathcal L_K( \bm \theta, \bm \gamma, \eta \mid { \hat {\bm \theta}_m}, \hat \sigma^2_0 ) \propto  |{\mathbf {\tilde R} }|^{-\frac{1}{2}} (S^2_{K})^{-\frac{n}{2}}, 
\label{equ:PL_GP}$$ where
$S^2_K=  (\mathbf{y}-\mathbf f^M_{\theta}- \mathbf H^T \hat{\bm \theta}_m  )^T  {{\mathbf { \tilde R}}}^{-1}(\mathbf{y}-\mathbf f^M_{\theta}- \mathbf H^T \hat{\bm \theta}_m  )$.

Denote
$\mathbf v_{\bm \theta_m}=(\mathbf{y}-\mathbf f^M_{\theta}- \mathbf H^T \hat{\bm \theta}_m  )$.
Similar to the no-discrepancy calibration, we call the lbfgs function in
the [nloptr](https://CRAN.R-project.org/package=nloptr) package
([@nloptr2014]) for optimization. By facts
[\[item:derivative_1\]](#item:derivative_1){reference-type="ref"
reference="item:derivative_1"} and
[\[item:derivative_2\]](#item:derivative_2){reference-type="ref"
reference="item:derivative_2"}, directly differentiating the profile
likelihood function with respect to kernel and nugget parameters in
([\[equ:PL_GP\]](#equ:PL_GP){reference-type="ref"
reference="equ:PL_GP"}) gives: $$\begin{aligned}
\frac{\partial \log( \mathcal L_K( \bm \theta, \bm \gamma, \eta \mid {\bm \hat \theta_m}, \hat \sigma^2 ) )}{ \partial \bm \gamma_l } &= -\frac{1}{2}\mathop{\mathrm{tr}}\left( \frac{\tilde {\bm R}_{\gamma,\eta}^{-1}}{\eta} \frac{\partial \mathbf R_\gamma}{\partial \bm \gamma_l} \right)  +\frac{n}{2}\times \frac{ \mathbf v_{\bm \theta_m}^T   \mathbf Q_{\gamma,\eta} \frac{\partial \mathbf R_\gamma}{\partial  \gamma_l} \mathbf Q_{\gamma,\eta} \mathbf v_{\bm \theta_m} }{\eta \mathbf v_{\bm \theta_m}^T \mathbf Q_{\gamma,\eta} \mathbf v_{\bm \theta_m} } \\
\frac{\partial \log( \mathcal L_K( \bm \theta, \bm \gamma, \eta \mid {\bm \hat \theta_m}, \hat \sigma^2 ) )}{ \partial \eta } &= \frac{1}{2}\mathop{\mathrm{tr}}\left( \frac{\tilde {\bm R}_{\gamma,\eta}^{-1} \mathbf R_\gamma}{\eta^2} \right) +\frac{n}{2}\times \frac{ \mathbf v_{\bm \theta_m}^T   \mathbf Q_{\gamma,\eta} \mathbf R_{\gamma}  \mathbf Q_{\gamma,\eta} \mathbf v_{\bm \theta_m} }{\eta^2 \mathbf v_{\bm \theta_m}^T \mathbf Q_{\gamma,\eta} \mathbf v_{\bm \theta_m} } \\
\frac{\partial \log( \mathcal L_K( \bm \theta, \bm \gamma, \eta \mid {\bm \hat \theta_m}, \hat \sigma^2 ) )}{ \partial \theta_l }  &= \frac{n}{2}\times \frac{  \mathbf v_{\bm \theta_m}^T  \mathbf Q_{\gamma,\eta} \frac{\partial \mathbf f^M_\theta}{\partial \theta_l}}{ \mathbf v_{\bm \theta_m}^T   \mathbf Q_{\gamma,\eta}  \mathbf v_{\bm \theta_m} } %% double check
\label{equ:PL}
\end{aligned}$$ for $l=1,...,p_x$ and $i=1,...,p_{\theta}$. When the
mean function is zero, one can simply replace $\mathbf Q_{\gamma,\eta}$
by $\tilde {\bm R}_{\gamma,\eta}$ in the formula above to obtain the
derivatives. Besides, when
$\frac{\partial \mathbf f^M_\theta}{\partial \theta_l}$ is not
available, we use the numerical derivatives to approximate.

. After marginalizing out the discrepancy function, the marginal
distribution follows
$\mathbf y^F \sim \mathcal{MN}(\mathbf f^M_{\theta}+\mathbf H \bm \theta_{m},\, \sigma^2_0 \tilde {\mathbf R})$,
where $\tilde {\mathbf R}=\mathbf R/\eta+ \bm \Lambda$. We assume an
objective prior for the mean and noise variance parameters
$\pi(\bm \theta_m, \sigma^2_0)\propto 1/\sigma^2_0$. The full
conditional posterior distributions of the trend and variance parameters
follow $$\begin{aligned}
\bm \sigma^{-2}_0 \mid \bm \theta_m,  \bm \theta &\sim \mbox{Gamma}\left(\frac{n}{2},  \frac{(\mathbf y^F-\mathbf f^M_{\theta}- \mathbf H \bm \theta_m )^T\bm {\tilde R}^{-1}(\mathbf y^F-\mathbf f^M_{\theta}- \mathbf H \bm \theta_m )}{2}  \right), \\ 
\bm \theta_m \mid \sigma^2_0, \bm \theta &\sim  \mathcal{MN}\left(   \bm { \hat \theta}_m, \,  \sigma^{-2}_0(\mathbf H^T\bm {\tilde R}^{-1} \mathbf H)^{-1} \right), 
\end{aligned}$$ where
$\hat {\bm \theta}_m= \left( \mathbf H^T {{\mathbf { \tilde R}}}^{-1}  \mathbf H \right)^{-1} \mathbf H^T {{\mathbf { \tilde R}}}^{-1}{(\mathbf{y}-\mathbf f^M_{\theta})}$.
A Gibbs sampler is used to sample $\bm \sigma^{-2}_0$ and $\bm \theta_m$
from the full conditional distribution.

Given the trend and variance parameters, the posterior distribution of
the calibration parameters follows $$\begin{aligned}
p(\bm \theta \mid \bm \theta_m, \bm \sigma^{2} )\propto \exp\left( - \frac{(\mathbf y^F-\mathbf f^M_{\theta}- \mathbf H \bm \theta_m )^T \mathbf { \tilde R}^{-1}(\mathbf y^F-\mathbf f^M_{\theta}- \mathbf H \bm \theta_m )  }{2\sigma^2_0}\right)\pi(\bm \theta).
\end{aligned}$$ We implement the Metropolis algorithm where the proposal
distribution is a normal distribution with the mean centered around the
current value of the posterior sample. Users can adjust the standard
deviation of the proposal distribution by argument sd_proposal in the
rcalibration function.

Denote the inverse range parameter $\beta_l=1/\gamma_l$, $l=1,...,p_x$.
The conditional distribution of the inverse range and nugget parameters
follow $$\begin{aligned}
p(\bm \beta, \eta \mid  \bm \theta, \bm \theta_m, \bm \sigma^{2}_0 )\propto |\mathbf { \tilde R}|^{-\frac{n}{2}} \exp\left( - \frac{(\mathbf y^F-\mathbf f^M_{\theta}- \mathbf H \bm \theta_m )^T \mathbf { \tilde R}^{-1}(\mathbf y^F-\mathbf f^M_{\theta}- \mathbf H \bm \theta_m )  }{2\sigma^2_0}\right)\pi(\bm \gamma, \eta).
\end{aligned}$$ We define the inverse range parameter
$\beta_l=1/\gamma_l$, and assume that the prior for the inverse range
and nugget parameters follows the jointly robust (JR) prior
[@gu2019jointly]
$$\pi(\bm \beta, \eta)=C\left( \sum^{p_x}_{l=1} C_l \beta_l+\eta \right)^a \exp\left\{-b \left( \sum^{p_x}_l C_l \beta_l+\eta \right) \right\},$$
where $C$ is a normalizing constant. By default, we use the following
prior parameters $a=1/2-p_x$, $b=1$, and
$C_l= n^{-1/{p_x}}|x^{max}_{l}-x^{min}_{l}|$, with $x^{max}_{l}$ and
$x^{min}_{l}$ being the maximum and minimum values of the observable
input at the $l$th coordinate, respectively. The default choices of
prior parameters induces a small penalty on very large correlation,
preventing the identifiability issues of the calibration parameters due
to large correlation [@gu2019jointly]. Users can adjust $a$ and $b$ in
the rcalibration function.

We use the Metropolis algorithm to sample the logarithm of the inverse
range parameter $\log( \beta_l)$ and nugget parameter $\log(\eta)$. The
proposal distribution of each parameter is a normal distribution
centered around the current value with the standard deviation chosen to
be $0.25$ by default. User can change the $p_{\theta}+1$ to
$p_x+p_{\theta}+1$ coordinates of the vector in the argument sd_proposal
in the rcalibration function to specify the standard deviation in the
proposal distribution for these parameters.

. The likelihood function and posterior distribution of S-GaSP follow
from the those in GaSP calibration, by simply replacing the kernel $K$
to $K_{Z_d}$ in Table [1](#tab:kernel){reference-type="ref"
reference="tab:kernel"}.

# Acknowledgements {#acknowledgements .unnumbered}

This research was supported by the National Science Foundation under
Award Number DMS-2053423. We thank Xubo Liu for implementing the
numerical method for simulating the Lorenz 96 model.
