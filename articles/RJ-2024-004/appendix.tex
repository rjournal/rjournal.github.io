\title{Supplementary Document for \\ Prediction, Bootstrapping and Monte Carlo Analyses Based on Linear Mixed Models with QAPE 2.0 Package}
\author{by Alicja Wolny--Dominiak and Tomasz \.{Z}\c{a}d{\l}o}

\maketitle             % Produces the title.

%\bigskip
%\abstract{
	%An abstract of less than 150 words.
	
	
	%Introductory section which may include references in parentheses
	%\citep{R}, or cite a reference such as \citet{R} in the text.
	
	%\section{Introduction}
	%\begin{figure}[htbp]
	%  \centering
	%  \includegraphics{Rlogo}
	%  \caption{The logo of R.}
	%  \label{figure:rlogo}
	%\end{figure}
	
	%\section{Introduction}
	
	
	% Please keep the abstract below 300 words
	
	\abstract{
	The paper presents a new R package \CRANpkg{qape} for prediction, accuracy estimation of various predictors and Monte Carlo simulation studies of properties of both predictors and estimators of accuracy measures.}

\section*{The Monte Carlo procedure to compute EBP}
The Monte Carlo procedure to compute EBP according to \cite{molina2010small} used in \code{ebpLMMne()} is presented.

\begin{enumerate}
	\item $\boldsymbol{\psi}$ is estimated based on sample data and estimator $\boldsymbol{\hat{\psi}}$ is obtained.
	\item Using the distribution function of $\mathbf{Y}_r|\mathbf{Y}_s$, whose functional form is assumed to be known, and where $\boldsymbol{\psi}$ is replaced by $\boldsymbol{\hat{\psi}}$, $L$ vectors $\mathbf{Y}_r$ are generated of unobserved values of the dependent variable, denoted by $\mathbf{Y}_r^{(l)}$ (where $l=1,2,...,L$).
	\item $L$ population vectors are built based on one subvector of the dependent variables observed in the sample and $L$ subvectors of unobserved values of the dependent variable generated in the previous step. The result is: $\mathbf{Y}^{(l)} = \left[ \mathbf{Y}_s^T  \mathbf{Y}_r^{(l)T}\right]^T$, where $l=1,2,...,L$.
	\item The EBP value is computed as follows: $\hat\theta_{EBP}={L^{- 1}}\sum\limits_{l = 1}^L \theta (\mathbf{Y}^{(l)})$. If the LMM is not assumed for the original variable of interest but for its transformation $T(.)$, the back-transformation is used additionally: $\hat\theta_{EBP}={L^{- 1}}\sum\limits_{l = 1}^L \theta (T^{-1}(\mathbf{Y}^{(l)}))$.
\end{enumerate}

It is worth nothing, that if the distribution of $\mathbf{Y}$ is multivariate normal, then the distribution of $\mathbf{Y}_r|\mathbf{Y}_s$ (used in step (ii) above) is also multivariate normal, which means the generation process of $L$ population vectors in the algorithm presented above is very time-consuming in real-life surveys. Therefore, the EBP is considered under the special case of the LMM, which makes it possible to accelerate the algorithm by generating $\mathbf{Y}_r^{(l)}$, $l=1, 2, ...,L$, not from the multivariate normal distribution but using the univariate normal distribution. The model, called the nested error LMM, is given by:
\begin{equation} \label{neLMM}
	\mathbf{Y}_k=\mathbf{X}_k\boldsymbol{\beta} + v_k \mathbf{1}_{N_k} +\mathbf{e}_k,
\end{equation}
where $k=1,2,...,K$ and $\mathbf{1}_{N_k}$ is a vector of ones of size $N_k \times 1$,  $v_{k}$ is a random effect, such that $v_{k}$ are independent for $k=1, 2, ..., K$,  $\mathbf{e}_{k}$ ($N_k \times 1$) is a vector of random components. Let us additionally assume that  $v_k \sim N(0,\sigma^2_v)$ and $\mathbf{e}_{k} \sim N({\bf{0}},\sigma_e^2{{\bf{I}}_{N_k}})$. Let the number of elements of $\mathbf{Y}_k$ observed in the sample be denoted by $n_k$. Under (\ref{neLMM}), step (ii) of the above procedure is as follows (cf. \cite{molina2010small} p. 375):
\begin{itemize}
	\item for $k$ where $n_k>0$  we generate $\mathbf{Y}_r^{(l)}=\begin{bmatrix}
		\mathbf{Y}_{r1}^{(l)T} & ...& \mathbf{Y}_{rk}^{(l)T} & ... & \mathbf{Y}_{rK}^{(l)T}
	\end{bmatrix}^T$, $l=1, 2, ...,L$, based on the following model:
	\begin{equation}\label{codnorm_k3}
		{{\bf{Y}}_{rk}} = {{\boldsymbol{\mu }}_{rk|sk}} + {u_k}{{\bf{1}}_{{N_k} - {n_k}}} + {{\boldsymbol{\varepsilon }}_{rk}},
	\end{equation}
	where ${{\boldsymbol{\mu }}_{rk|sk}} = {{\bf{X}}_{rk}}{\ebbeta } + \hat{\sigma}_v^2{{\bf{1}}_{{N_k} - {n_k}}}{\bf{1}}_{{n_k}}^T{\left( {\hat{\sigma}_v^2{{\bf{1}}_{{n_k}}}{\bf{1}}_{{n_k}}^T + \hat{\sigma}_e^2{{\bf{I}}_{{n_k}}}} \right)^{ - 1}}({{\bf{Y}}_{sk}} - {{\bf{X}}_{sk}}{{\ebbeta }})$, ${u_k}$ and ${{\boldsymbol{\varepsilon }}_{rk}}$ are independent, ${{\boldsymbol{\varepsilon }}_{rk}}\sim N({\bf{0}},\hat{\sigma}_e^2{{\bf{I}}_{{N_k} - {n_k}}})$, ${u_k}\sim N(0,\hat{\sigma}_v^2(1-{\omega_k}))$, ${\omega_k} = \hat{\sigma}_v^2{(\hat{\sigma}_v^2 + \hat{\sigma}_e^2 n_k^{ - 1})^{ - 1}}$,
	\item for $k$ where $n_k=0$  we generate $\mathbf{Y}_r^{(l)}$, $l=1, 2, ...,L$, based on (\ref{neLMM}), where unknown parameters are replaced with estimates.
\end{itemize}


\section*{The correction procedure in residual bootstrap}
This appendix presents the correction procedure according to \cite{carpenter2003novel}, \cite{chambers2013random} and \cite{thai2013comparison}, which can be used in the residual bootstrap to avoid the problem of  underdispersion of the classic residual bootstrap distribution.

Let us consider the $l$th vector of random effects in Equation (5) given by Equation (6), both presented in the paper. Let $\boldsymbol{G}_l$ be the variance-covariance matrix of size $K_l \times K_l$ defined as $\boldsymbol{G}_l=$ $Var\left(\left[ v_{l1j} \dots v_{lkj} \dots v_{lK_lj} \right]^T  \right)$, where $v_{lkj}$ is the $j$th element of $\mathbf{v}_{lk}$. Let the estimated (e.g. using restricted maximum likelihood method) matrix  $\boldsymbol{G}_l$  be denoted by $\hat{\boldsymbol{G}}_{l}$ and the empirical covariance matrix of size $K_l \times K_l$  be defined as follows ${\boldsymbol{G}}_{(emp)l}=J_l^{-1}
\left[ \begin{array}{c}
	\hat{\mathbf{v}}_{l1}^T \\
	\dots \\
	\hat{\mathbf{v}}_{lk}^T \\
	\dots \\
	\hat{\mathbf{v}}_{lK_l}^T
\end{array}
\right]
\left[ \begin{array}{c}
	\hat{\mathbf{v}}_{l1}^T \\
	\dots \\
	\hat{\mathbf{v}}_{lk}^T \\
	\dots \\
	\hat{\mathbf{v}}_{lK_l}^T
\end{array}
\right]^T
$, where $\hat{\mathbf{v}}_{lk}$ are the estimated best linear unbiased predictors of ${\mathbf{v}}_{lk}$.\\

Let us write the estimated and the empirical covariance matrices using the Cholesky decomposition, in terms of a lower triangular matrix, as $\hat{\boldsymbol{G}}_l = \mathbf{L}_{(est)l} \mathbf{L}^T_{(est)l} $ and $\boldsymbol{G}_{(emp)l} = \mathbf{L}_{(emp)l} \mathbf{L}_{(emp)l}^T$. Let $\mathbf{A}_l=(\mathbf{L}_{(est)l} \mathbf{L}_{(emp)l}^{-1})^T$. Let us define the corrected estimates of $\mathbf{v}_l$ as (\cite{carpenter2003novel}, \cite{thai2013comparison}): $\hat{\mathbf{v}}_{(cor)l} =\hat{\mathbf{v}_l} \mathbf{A}_l$, where $\hat{\mathbf{v}}$ is the empirical best linear unbiased predictor of $\mathbf{v}$. Let us additionally assume that $Var(\mathbf{e})=\mathbf{R}=\sigma^2_e diag_{1 \leq \ i \leq N} (d_i)$, where $d_i$ values are known weights. The corrected residuals are as follows (\cite{chambers2013random}): $\hat{{e}}_{(cor)i}=\hat{\sigma}_e \sqrt{d_i} \hat{e}_i (n^{-1}\sum_{k=1}^{n} \hat{e}_i )^{-0.5}$, where $i=1,2,...,n$, $\hat{\sigma}^2_e$ is the estimate (e.g. REML estimate) of ${\sigma}^2_e$, $\hat{e}_i$ are residuals computed under the model given by Equation (5) in the paper.

Replacing $\hat{\mathbf{v}_l}$ and $\hat{{e}}_{i}$ by specified above $\hat{\mathbf{v}}_{(cor)l}$ and $\hat{{e}}_{(cor)i}$, respectively, the corrected version of the residual bootstrap procedure is obtained.

\bibliography{wolny-zadlo}

\address{Alicja Wolny--Dominiak\\
	Department of Statistical and Mathematical Methods in Economics \\
	University of Economics in Katowice\\
50, 1 Maja Street\\
	40--287 Katowice\\
	Poland\\}
\email{alicja.wolny-dominiak@uekat.pl} \\
\url{web.ue.katowice.pl/woali/}

\address{Tomasz \.{Z}\c{a}d{\l}o\\
Department of Statistics, Econometrics and Mathematics \\
University of Economics in Katowice\\
50, 1 Maja Street\\
40--287 Katowice\\
Poland\\}
\email{tomasz.zadlo@uekat.pl} \\
\url{web.ue.katowice.pl/zadlo/}

%\email{} \\

\end{article}

\end{document}



