\section{Introduction} \label{Introduction}


In statistical analysis, regression models are {important methods for characterizing the relationship between response and the covariates. When} the response follows exponential family distributions, generalized linear models (GLM) are commonly used to link the response and the covariates. If the response is taken as failure time and is incomplete due to censoring (e.g., \citealp{Lawless:2003}), the accelerated failure time model (AFT) might be one of useful strategies to characterize the survival outcome in survival analysis. In recent years, complex modeling structures have been explored when building GLM or survival models, including semi-parametric or mixed-effects structures. To address these challenges, several statistical learning methods, including the boosting approaches (e.g., \citealp{Hastie:2008}), have been developed.

In the contemporary statistical analysis, researchers may frequently encounter high-dimensionality in variables. In particular, {high-dimensional data} may contain many irrelevant covariates that {may} affect analysis results. Therefore, it is crucial to do variable selection. In the development of statistical methods, some useful strategies have been proposed, such as regularization approaches (e.g., \citealp{Tibshirani:1996}; \citealp{Zou:2006}; \citealp{Zou:2005}) or feature screening methods (e.g., \citealp{Chen:2021}; \citealp{Chen:2020}). In addition, \cite{Wolfson:2011} and \cite{Brown:2017} proposed the boosting method to do variable selection, which avoids {having to deal with} non-differentiable penalty functions. The other important feature is measurement error in variables, which is ubiquitous in applications. Moreover, ignoring measurement error effects may affect the estimation results (e.g., \citealp{ChenYi:2021}). Therefore, it is necessary to correct for measurement error effects. {A} large body of methods has been well established to address variable selection, correction of measurement error, or both. Recently, \cite{Chen:2023} developed the SIMEX method and the regression calibration method with the boosting algorithm accommodated to handle variable selection and measurement error correction for GLMs. \cite{ChenQiu:2023} considered the AFT model to fit time-to-event responses and proposed the SIMEX method to address measurement error. \cite{Chen:2023b} derived the corrected estimating function based on the logistic regression or probit models and applied the boosting method to do variable selection for binary outcomes.


In applications, several commonly used packages associated with existing methods have been developed for public use. A detailed list is summarized in Table~\ref{tab:compare}. Specifically, with variable selection and measurement error ignored,  most packages related to boosting methods, including \CRANpkg{bst} and \CRANpkg{adabag}, can handle classification with binary or multi-class responses. {Some packages can deal with different response families.} For example, \CRANpkg{xgboost} and \CRANpkg{lightgbm} can deal with continuous responses; \CRANpkg{gbm} and \CRANpkg{gamboostLSS} {can be used to} model survival data under the Cox model and the AFT model, respectively; \CRANpkg{GMMBoost} is useful for handling mixed-effects models. In the presence of high-dimensional but precisely measured variables,  \CRANpkg{glmnet} and \CRANpkg{SIS} are two popular packages {for} variable selection or feature screening, respectively. On the contrary, if variables are subject to measurement error, the two packages \CRANpkg{GLSME} and \CRANpkg{mecor} focus on linear models and aim to adjust for measurement error effects in the response and/or covariates. Moreover, the simulation and extrapolation (SIMEX) method (e.g., \citealp{ChenYi:2021}; \citealp{Carroll:2006}) has been a powerful strategy to correct for measurement error effects, and has been widely used under several types of regression models in existing R packages, including \CRANpkg{simex} for GLM,  \CRANpkg{augSIMEX} for GLM with error-prone continuous and discrete variables, and \CRANpkg{simexaft} for the AFT model. In addition to the R software, \cite{BOOME} developed a Python package {\bf BOOME} to handle variable selection and measurement error for binary outcomes. 
 
%\begin{landscape}
 \begin{table}[!ht]
    %   \huge
     \caption{Comparisons among existing and proposed packages. {This table summarizes} three categories {of packages}: (i) variable selection without measurement error correction (\CRANpkg{glmnet}, \CRANpkg{SIS}), (ii) measurement error correction without variable selection (\CRANpkg{GLSME}, \CRANpkg{mecor}, \CRANpkg{augSIMEX}, \CRANpkg{simex}, \CRANpkg{simexaft}), (iii) statistical learning approaches that handle estimation without consideration of measurement error and variable selection (\CRANpkg{bst}, \CRANpkg{xgboost}, \CRANpkg{gbm}, \CRANpkg{adabag}, \CRANpkg{lightgbm}, \CRANpkg{GMMBoost}, \CRANpkg{gamboostLSS}). The proposed package \CRANpkg{SIMEXBoost} is included in these three categories. In the Usage heading, `LM' denotes the linear model; `Class' is the classification; `Pois' is the Poisson regression, `Cox' is the Cox model; `AFT' is the AFT model; `SL' is statistical learning; `ME' represents measurement error correction; `VS' is variable selection; and `Col' represents collinearity.  } \label{tab:compare}

\small
% \scriptsize

\center
   \renewcommand{\arraystretch}{0.75}
 \begin{tabular}{l c c c c c c c c c c c c c c c c c c c c c c c c c c }

 \\
 \hline
 & \multicolumn{9}{c}{Usage} \\ \cline{2-10}
Packages & LM & Class$^1$  & Pois  & Cox & AFT & SL$^2$ & ME & VS  & Col \\
\CRANpkg{SIMEXBoost}  & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$  \\
\cite{SIMEXBoostR} \\
\CRANpkg{glmnet} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\checkmark$ \\
\cite{glmnetR} \\
\CRANpkg{SIS}  & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ \\
\cite{SISR} \\
\CRANpkg{GLSME} & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\times$ \\
\cite{GLSMER} \\
\CRANpkg{mecor} & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\times$ \\
\cite{mecorR} \\
\CRANpkg{augSIMEX} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\times$  \\
\cite{augSIMEXR} \\
\CRANpkg{simex} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\times$ \\
\cite{simexR} \\
\CRANpkg{simexaft} & $\times$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\checkmark$ & $\times$ & $\times$ \\ 
\cite{simexaftR} \\
\CRANpkg{bst} & $\times$ & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\times$ & $\times$ \\
\cite{bstR} \\
\CRANpkg{xgboost} & $\checkmark$ & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\times$ & $\times$ \\
\cite{xgboostR} \\
\CRANpkg{gbm} & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$ & $\times$ & $\times$ & $\times$ \\
\cite{gbmR} \\
\CRANpkg{adabag} & $\times$ & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\times$ & $\times$ \\
\cite{adabagR} \\
\CRANpkg{lightgbm} & $\checkmark$ & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\times$ & $\times$ \\
\cite{lightgbmR} \\
\CRANpkg{GMMBoost} & $\checkmark$ & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\times$ & $\times$ \\
\cite{GMMBoostR} \\
\CRANpkg{gamboostLSS} & $\checkmark$ & $\times$& $\checkmark$ & $\times$ & $\checkmark$ & $\checkmark$ & $\times$ & $\times$ & $\times$ \\
\cite{gamboostLSSR} \\
{\bf BOOME} (in Python) & $\times$ &  $\checkmark$  & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\
\cite{BOOME} \\

 \hline

\end{tabular}
\scriptsize
\raggedright 
\\$^1$ ``Class'' includes binary or multiclass classification, and the construction of logistic regression models.
\\$^2$ ``SL'' contains several estimation methods based on machine learning approaches, such as tree and random forest. In addition, the corresponding packages may handle complex structures, including semi-paramatric models, mixed-effects models, and generalized additive models.  
\end{table}
%\end{landscape}
 
 

While many packages have been available to handle either variable selection or measurement error correction, {few packages} deal with these two features simultaneously. To address those concerns, we develop the R package \CRANpkg{SIMEXBoost} (\citealp{SIMEXBoostR}) by extending the method in \cite{Chen:2023} and \cite{ChenQiu:2023}, which covers commonly used GLM and AFT models. Motivated by {the idea of} the boosting algorithm ({see} e.g., \citealp{Hastie:2008}, Section 16.2),   \CRANpkg{SIMEXBoost} aims to {use} estimating functions to iteratively retain informative covariates and exclude unimportant ones, yielding variable selection result. In addition, to deal with measurement error effects, the package \CRANpkg{SIMEXBoost} primarily employs the SIMEX method to efficiently correct for measurement error effects for different types of regression models. {There are several advantages of \CRANpkg{SIMEXBoost} over the existing packages}. Specifically, as summarized in Table~\ref{tab:compare}, while \CRANpkg{glmnet} and \CRANpkg{SIS} are able to handle variable selection, they fail to deal with measurement error effects. In addition, the two packages \CRANpkg{GLSME} and \CRANpkg{mecor} focus on linear models and aim to adjust for measurement error effects in the response and/or covariates, but they cannot deal with variable selection. On the contrary, the contribution of the package \CRANpkg{SIMEXBoost} is able handle measurement error in variables, and do variable selection and estimation simultaneously. Moreover, boosting iteration may reduce the possibility of falsely excluding important covariates and enhance the accuracy of the estimator. Most importantly, \CRANpkg{SIMEXBoost}  is able to deal with collinearity  of variables by using the $L_2$-norm penalty function.%, which is one of the key contributions and is the main difference from existing packages (e.g., \CRANpkg{glmnet}, \CRANpkg{SIS}).


The remainder is organized as follows. In the second section, we introduce the data structure and the corresponding regression models. In addition, the boosting algorithm is outlined. In the  third section, we introduce the measurement error model and extend the correction of measurement error effects to the boosting algorithm. In the fourth section, we introduce functions and their arguments in the R package \CRANpkg{SIMEXBoost}. In the fifth section, we demonstrate the application of the R package \CRANpkg{SIMEXBoost} and conduct simulation studies to assess the performance of the boosting estimators. Moreover, we also implement \CRANpkg{SIMEXBoost} in a real dataset. A general discussion is presented in the last section. The supporting information, including a real dataset, programming code, and numerical results in csv files, are placed in the corresponding author's GitHub, whose link is given by \url{https://github.com/lchen723/SIMEXBoost.git}.




\section{Notation, Models, and Boosting Procedure} \label{notation}


\subsection{Model} \label{Def: Data}


Let $Y$ denote the response, and let $\mathbf{X}$ be the $p$-dimensional vector of covariates. Suppose that we have a sample of $n$ subjects and for $i=1, \cdots, n$, $\{Y_i, \mathbf{X}_i\}$ has the same distribution as $\left\{ Y, \mathbf{X} \right\}$.


Let $\boldsymbol{\beta}$ be a $p$-dimensional vector of (unknown) parameters associated with the covariates $\mathbf{X}$, and write $\mathbf{X}^\top \boldsymbol{\beta}$ as the linear predictor. In the framework of statistical learning, to characterize the relationship between $Y$ and $\mathbf{X}$, a commonly used approach is to link $Y$ and $\mathbf{X}^\top \boldsymbol{\beta}$ through the convex loss function $L: \mathbb{S} \times \mathbb{R} \rightarrow \mathbb{R}$, where $\mathbb{S}$ is the support of $Y$. Let the risk function be defined as the expectation of the loss function, i.e., $R(\boldsymbol{\beta}) \triangleq E\left\{ L(Y, \mathbf{X}^\top \boldsymbol{\beta}) \right\}$. Under the finite sample size $n$, the empirical version of $R(\boldsymbol{\beta})$ is given by
\begin{eqnarray*}
\frac{1}{n} \sum \limits_{i=1}^n L(Y_i, \mathbf{X}_i^\top \boldsymbol{\beta} ).
\end{eqnarray*}
Our goal is to estimate $\boldsymbol{\beta}$ by minimizing the risk function, and the resulting estimator is given by
\begin{eqnarray*}
\widehat{\boldsymbol{\beta}} = \argmin \limits_{\boldsymbol{\beta}} \bigg\{ \frac{1}{n} \sum \limits_{i=1}^n L(Y_i, \mathbf{X}_i^\top \boldsymbol{\beta} ) \bigg\}.
\end{eqnarray*}
Equivalently, $\widehat{\boldsymbol{\beta}}$ satisfies the estimating equation $\mathbf{g}(\mathbf{X},\boldsymbol{\beta}) = 0$, where $\mathbf{g}(\mathbf{X},\boldsymbol{\beta})$ is the estimating function of $\boldsymbol{\beta}$, defined as the first order derivative of $\frac{1}{n} \sum \limits_{i=1}^n L(Y_i, \mathbf{X}_i^\top \boldsymbol{\beta} )$ with respect to $\boldsymbol{\beta}$.

\subsection{Boosting Procedure} \label{Boost_VSE}

High-dimensionality and sparsity of $\boldsymbol{\beta}$ are crucial concerns, which reflect {the idea} that some covariates are not informative with respect to $Y$.  To address these issues and provide {a reliable} estimator of $\boldsymbol{\beta}$, we employ the boosting procedure to perform variable selection and estimation (e.g., \citealp{Hastie:2008}). {This version of the boosting algorithm} is motivated by \cite{Wolfson:2011} and \cite{Brown:2017}, and is applied to handle GLM (\citealp{Chen:2023}) as well as AFT models (\citealp{ChenQiu:2023}). An overall procedure is presented in Algorithm~\ref{algo-ThrEEBoost} with three key steps. Specifically, Steps 1 and 2 in Algorithm~\ref{algo-ThrEEBoost} treat the estimating function evaluated at  an iterated value as the signal, and use it to determine informative indexes of covariates and parameters. Noting that there is a parameter $\tau$ in Step {2} that is used to control the number of selected covariates in each iteration. In our numerical studies, $\tau=0.9$ seems to be a suitable choice and has a satisfactory performance.


After that, Step 3 in Algorithm~\ref{algo-ThrEEBoost} updates $\boldsymbol{\beta}$ {using the} informative indexes determined in {Step 2} by the sign of signals with increment $\kappa$. {This approach follows the steepest descent method (see e.g., \citealp{Boyd:2004}, Section 9.4.2) and can be used to deal with the $L_1$-norm for variable selection.} In addition, as discussed in Section 16.2.1 of \cite{Hastie:2008}, the value $\kappa$ has the opposite relationship with the number of iteration $M$ that smaller $\kappa$ requires larger $M$. In our consideration, we specify $\kappa = 0.05$. Finally, repeating the iteration $M$ times gives the desired estimator. With $M$ time iterations, we can obtain the final set $\mathcal{J}_M$ containing informative covariates and the corresponding $\boldsymbol{\beta}^{(M)} = \big( \beta_1^{(M)}, \cdots, \beta_p^{(M)}  \big)^\top$, {accomplishing} variable selection. 




\begin{algorithm}[H]
%\SetAlgoLined
%\KwResult{Gradient Boosting}
 Let $\boldsymbol{\beta}^{(0)}=0$ denote an initial value\;
 \For{iteration $m$ with $m=0,1,2,\cdots,M$}{
% \begin{enumerate}
%     \item[(a)] 
% \end{enumerate}
 Step 1: calculate $\boldsymbol{\Delta}^{(m-1)} = \mathbf{g}(\mathbf{X},\boldsymbol{\beta})\big|_{\boldsymbol{\beta} = \boldsymbol{\beta}^{(m-1)}}$\;
 Step 2:   determine $\mathcal{J}_m = \Big\{ j: \left| \Delta_j^{(m-1)} \right| \geq \tau \max \limits_{j'} \left| \Delta_{j'}^{(m-1)} \right| \Big\}$ \;
Step 3: update $\beta_j^{(m)} \leftarrow \beta_j^{(m-1)} + \kappa \cdot \text{sign}(\Delta_j^{(m-1)})$ for all $j \in \mathcal{J}_m$, and define $\boldsymbol{\beta}^{(m)} = \big( \beta_1^{(m)}, \cdots, \beta_p^{(m)}  \big)^\top$ \;
 }
 \caption{Boost\_VSE Algorithm} \label{algo-ThrEEBoost}
\end{algorithm}


Finally, for the application of Algorithm~\ref{algo-ThrEEBoost}, we consider some specific {models} and the corresponding regression models listed below. With models specified, we can further determine the estimating function $\mathbf{g}(\mathbf{X},\boldsymbol{\beta})$.

\begin{description}

\item[Linear regression models:]
\ \\
Given the dataset $\big\{ \{Y_i, \mathbf{X}_i\} : i=1,\cdots,n\big\}$ with $Y_i$ being a continuous and univariate random variable, linear models are characterized as
\begin{eqnarray} \label{LM}
Y_i = \mathbf{X}_i^\top \boldsymbol{\beta} + \epsilon_i
\end{eqnarray}
where $\epsilon_i$ is the noise term with $E(\epsilon_i)=0$ and $\text{var}(\epsilon_i)=\sigma_\epsilon^2$. The estimating function is defined as the first order derivative of the least squares function:
\begin{eqnarray} \label{LM-g}
\mathbf{g}(\mathbf{X},\boldsymbol{\beta}) = \sum \limits_{i=1}^n -\mathbf{X}_i ( Y_i - \mathbf{X}_i^\top \boldsymbol{\beta}).
\end{eqnarray}


\item[Logistic regression models:]
\ \\
If $Y_i$ is a binary and univariate random variable, then $Y_i$ and $\mathbf{X}_i$ are usually characterized by a logistic regression model:
\begin{eqnarray} \label{Logit}
\pi_i = \frac{\exp\big(\mathbf{X}_i^\top \boldsymbol{\beta}\big)}{1+\exp\big(\mathbf{X}_i^\top \boldsymbol{\beta}\big)},
\end{eqnarray}
where $\pi_i \triangleq P(Y_i=1 | \mathbf{X}_i)$. Following the idea in \cite{Agresti:2012}, we can construct the likelihood function based on (\ref{Logit}). Therefore, the resulting estimating function is given by the first order derivative of the likelihood function:
\begin{eqnarray} \label{Logit-g}
\mathbf{g}(\mathbf{X},\boldsymbol{\beta}) = - \sum \limits_{i=1}^n \mathbf{X}_i \left\{ Y_i - \frac{\exp\big(\mathbf{X}_i^\top \boldsymbol{\beta}\big)}{1+\exp\big(\mathbf{X}_i^\top \boldsymbol{\beta}\big)} \right\}. \end{eqnarray}

\item[Poisson regression models:]
\ \\
When $Y_i$ is a count and univariate random variable, one can adopt the Poisson regression model to fit $Y_i$ and $\mathbf{X}_i$:
\begin{eqnarray} \label{Pois}
\log \lambda_i = \mathbf{X}_i^\top \boldsymbol{\beta} 
\end{eqnarray}
where $\lambda_i$ is the parameter of the Poisson distribution. Following the framework of generalized linear models, the likelihood function based on (\ref{Pois}) can be determined. Therefore, the first order derivative of the likelihood function under (\ref{Pois}) yields the corresponding estimating function, which is given by
\begin{eqnarray} \label{Pois-g}
\mathbf{g}(\mathbf{X},\boldsymbol{\beta}) = - \sum \limits_{i=1}^n \mathbf{X}_i \left\{ Y_i - \exp\big(\mathbf{X}_i^\top \boldsymbol{\beta}\big) \right\}.
\end{eqnarray}

\item[Accelerated failure time models:]
\ \\
In survival analysis, the response is {known as} the failure time, denoted $\widetilde{T}_i>0$, and the accelerated failure time (AFT) model is a commonly used {model for  characterizing the relationship between} the survival time and the covariates (e.g., \citealp{Lawless:2003}). Specifically, the AFT model is formulated as
\begin{eqnarray} \label{AFT}
\log \widetilde{T}_i =\mathbf{X}_i^\top \boldsymbol{\beta}+\eta_i,
\end{eqnarray}
where $\eta_i$ is the noise term of (\ref{AFT}). In the framework of survival analysis, the main challenge is that $\widetilde{T}_i$ is usually incomplete due to {how the observations are collected}. In particular, in this study, the failure time may suffer from {\em length-biased} and {\em interval-censoring}, which cause the data to be biased and incomplete. 

{\ \ \ \ } Specifically, for the length-biased sampling, it is common to assume that the incidence rate of the initial event is constant over calendar time,  and the truncation time, denoted $\widetilde{A}_i$, is uniformly distributed in $[0,\xi]$, where $\xi$ is the maximum support of $\widetilde{T}_i$ (e.g., \citealp{ChenQiu:2023}). For the length-biased data, we can observe  $(\widetilde{A}_i$,$\widetilde{T}_i)$ only if $\widetilde{T}_i \geq \widetilde{A}_i$, and thus, we denote $(T_i,A_i) \equiv (\widetilde{T}_i,\widetilde{A}_i) \big| \widetilde{T}_i \geq \widetilde{A}_i$ as the observed version of $(\widetilde{T}_i$,$\widetilde{A}_i)$.

{\ \ \ \ } On the other hand, for the observed $T_i$, {we} may encounter the interval-censoring. Suppose that $T_i$ is not exactly observed but only determined at a sequence of examination times, denoted as
$A_i = U_0 <U_{1} < \cdots < U_{N} \leq  \xi$ for some constant $N>0$. The failure time is then known to lie in the interval $(L,R)$, where $L_i = \max\{U_k : U_k < T_i, k = 0, \cdots ,N\}$ and $R_i = \min\{U_k : U_k \geq T_i, k=1, \cdots ,N+1\}$ with $U_{N+1}\triangleq \infty$. Moreover, if $T_i$ occurs before the first examination time, then $(L_i,R_i) \triangleq (A_i, U_1)$; if the failure has not occurred at the last examination time, then $(L_i,R_i) \triangleq (U_N, \infty)$. Finally, let $\Delta_i$ denote the indicator, where a value 1 indicates that $T_i$ is observed and zero otherwise. {As a consequence}, for a sample with size $n$, the length-biased and interval-censored survival data is given by $ \big\{ \{ A_i, \Delta_i, Y_i, X_i\} : i=1,\cdots,n \big\} $ with $Y_i \triangleq \{\Delta_iT_i, (1-\Delta_i)L_i, (1-\Delta_i)R_i\}$.

{\ \ \ \ } Based on the length-biased and interval-censored data, we can construct the estimating function (e.g., \citealp{ChenQiu:2023})
\begin{eqnarray} \label{eq7}
\mathbf{g}(\mathbf{X},\boldsymbol{\beta}) = \sum_{i=1}^n \mathbf{X}_i \Bigg\{\Delta_i\frac{Y_{\beta,i}}{{\rm exp}(Y_{\beta,i})}+(1-\Delta_i)\frac{\int_{L_{i,0}}^{R_{i,0}} u^{-1}{\rm log}udF_{0}(u)}{F_{0}(R_{i,0})-F_{0}(L_{i,0})} \Bigg\},
\end{eqnarray}
where $Y_{\beta,i}=\log T_i-\mathbf{X}_i^\top \boldsymbol{\beta}$, $R_{i,0}=R_i  \exp(-\mathbf{X}_i^\top\beta)$, $L_{i,0}=L_i \exp(-\mathbf{X}_i^\top\beta)$, and $F_{0}$ is the cumulative distribution function of $\eta_i$.

\end{description}



\section{A Modified Boosting Method with the Presence of Covariate Measurement Error} \label{Main-Result}


\subsection{Measurement Error Models} 
 
For $i=1,\cdots,n$, let $\mathbf{X}_i^\ast$ denote the surrogate, or observed covariate, of $\mathbf{X}_i$. Let $\boldsymbol{\Sigma}_{X^\ast}$ and $\boldsymbol{\Sigma}_X$ be the $p \times p$ covariance matrices of $\mathbf{X}_i^\ast$ and $\mathbf{X}_i$, respectively. In our development, we focus on the classical measurement error model (e.g., \citealp{Carroll:2006}, Chapter 1):
\begin{eqnarray} \label{mea_classic}
\mathbf{X}_i^\ast =  \mathbf{X}_i + \mathbf{e}_i,
\end{eqnarray}
where $\mathbf{e}_i$ is independent of $\left\{\mathbf{X}_i, Y_i \right\}$ and  $\epsilon_i$ in (\ref{LM}), $\mathbf{e}_i$ follows a normal distribution with mean zero and the covariance matrix $\boldsymbol{\Sigma}_e$, say $N(0,\boldsymbol{\Sigma}_e)$. Noting that the covariance matrix $\boldsymbol{\Sigma}_e$ is usually unknown. To determine it, we can either employ sensitivity analyses to reasonably specify values, or directly estimate it if additional information, such as repeated measurements or validation sample is available ({see} e.g., \citealp{ChenYi:2021}). As a result, in the presence of measurement error, the {\em observed} dataset is now given by $\big\{ \{Y_i, \mathbf{X}_i^\ast\} : i=1,\cdots,n\big\}$.


%we follow the same models in Section~\ref{Boost_VSE} with $\mathbf{X}_i$ replaced by its surrogate version $\mathbf{X}_i^\ast$. 


\subsection{Boosting with Measurement Error Correction}

In the presence of measurement error, it is known that directly using $\mathbf{X}_i^\ast$ in the estimating procedure without the correction of measurement error effects may incur biased estimate and wrong conclusion ({see} e.g., \citealp{Carroll:2006}). Therefore, even though Algorithm~\ref{algo-ThrEEBoost} is valid to estimate $\boldsymbol{\beta}$ for regression models in the `Boosting Procedure' subsection, it {is insufficient in the presence of} measurement error effects. 

To deal with measurement error in covariates, we extend Algorithm~\ref{algo-ThrEEBoost} by adopting the SIMEX method to eliminate the impact of measurement error (e.g., \citealp{ChenYi:2021}). The modified algorithm, called {\it SIMEXBoost}, is summarized in Algorithm~\ref{algo-SIMEX}. 

The idea of the SIMEX method is to first establish the trend of measurement error-induced biases as a function of the variance of  measurement error by artificially creating a sequence of surrogate measurements, and then extrapolate this trend back to the case without measurement error. Specifically, in Step 1 of Algorithm~\ref{algo-SIMEX}, we artificially create a sequence of error-contaminated surrogate measurements by introducing different degrees of measurement error. After that, as shown in Step 2 of Algorithm~\ref{algo-SIMEX}, we apply those surrogate measurements to the boosting procedure, and use a new function $\mathbf{g}_\text{\tiny SIM}(\boldsymbol{\beta};b,\zeta)$, which is defined as (\ref{LM-g}), (\ref{Logit-g}), (\ref{Pois-g}), or (\ref{eq7}) with $\mathbf{X}_i$ replaced by $\mathbf{W}_i \left(b,\zeta\right)$ defined in (\ref{Sim_W}), to obtain biased estimates by running an estimation method developed for error-free settings. Finally, Step 3 in Algorithm~\ref{algo-SIMEX} traces the  pattern of biased estimates against varying magnitudes of measurement error and then does extrapolation based on linear or quadratic regression models. 

Noting that there are several parameters in Algorithm~\ref{algo-SIMEX}. The parameters $M$, $\kappa$, and $\tau$ in Step 2 are the same as those in Algorithm~\ref{algo-ThrEEBoost}. On the other hand, Step 1 contains a value $B$ and a sequence of $\mathcal{Z}$ that are usually user-specified. Typically, $\mathcal{Z}$ is usually defined as $K$ equal-width cutpoints in an interval $[0,1]$ for some positive constant $K$. A value $B$ is used to perform the Monte Carlo computation in (\ref{SIMEX-ave}) and make the estimator more stable. A larger value of $B$ may implicitly incur longer computational time. Our numerical experiments show that $B=50$ gives satisfactory performance.


\begin{algorithm}[H]
%\SetAlgoLined
%\KwResult{Gradient Boosting}
\begin{description}
    \item[Step 1:] Generate the working data $\mathbf{W}_i(b,\zeta)$ by
    \begin{eqnarray} \label{Sim_W}
\mathbf{W}_i \left(b,\zeta\right) = \mathbf{X}_i^\ast + \sqrt{\zeta} \mathbf{e}_{i,b}
\end{eqnarray}
for $b=1,\cdots,B$ and $\zeta \in \mathcal{Z}$, where $\mathbf{e}_{i,b} \sim N(0,\boldsymbol{\Sigma}_e)$ independently.
    
    \item[Step 2:] Boosting estimation.\\  Perform the following boosting procedure with $M$ iterations.
\end{description}    
\For{$b=1,\cdots,B$ and $\zeta \in \mathcal{Z}$}{
Let $\boldsymbol{\beta}^{(0)}(b,\zeta)=\mathbf{0}$ denote an initial value\;
    \For{iteration $m$ with $m=0,1,2,\cdots,M$}{
 Step 2.1: calculate $\boldsymbol{\Delta}^{(m-1)}(b,\zeta) = \mathbf{g}_\text{\tiny SIM}(\boldsymbol{\beta};b,\zeta)\big|_{\boldsymbol{\beta} = \boldsymbol{\beta}^{(m-1)}}$\;
 Step 2.2:  determine $\mathcal{J}_m(b,\zeta) = \Big\{ j: \left| \Delta_j^{(m-1)}(b,\zeta) \right| \geq \tau \max \limits_{j} \left| \Delta_j^{(m-1)}(b,\zeta) \right| \Big\}$ \;
Step 2.3: update $\beta_j^{(m)}(b,\zeta) \leftarrow \beta_j^{(m-1)}(b,\zeta) + \kappa \cdot \text{sign}(\Delta_j^{(m-1)}(b,\zeta))$ for all $j \in \mathcal{J}_m(b,\zeta)$;} 
Write $\boldsymbol{\beta}^{(M)}(b,\zeta) = \left(\beta_1^{(M)}(b,\zeta), \cdots, \beta_p^{(M)}(b,\zeta) \right)^\top$\;
}
When $\boldsymbol{\beta}^{(M)}(b,\zeta)$ is obtained for $b=1,\cdots,B$ and $\zeta \in \mathcal{Z}$, compute an average 
\begin{eqnarray} \label{SIMEX-ave}
\boldsymbol{\beta}^{(M)}(\zeta) = \frac{1}{B} \sum \limits_{b=1}^B \boldsymbol{\beta}^{(M)}(b,\zeta) \ \ \text{for}\ \ \zeta \in \mathcal{Z}.
\end{eqnarray}


\begin{description}
\item[Step 3:] Extrapolation.
\\
Fit a sequence $\left\{ \left( \zeta, \boldsymbol{\beta}^{(M)} (\zeta) \right) : \zeta \in \mathcal{Z} \right\}$ by a regression model, and the final value is given by the extrapolated value at $\zeta=-1$.
 
    
    
\end{description}



 \caption{SIMEXBoost Algorithm} \label{algo-SIMEX}
\end{algorithm}




\section{Description and Implementation of \CRANpkg{SIMEXBoost}} \label{Sec-package}


We develop an R package, called \CRANpkg{SIMEXBoost}, to implement the variable selection and estimation with measurement error correction described in the preceding section. This package {depends on the \CRANpkg{MASS} package only}. The package  \CRANpkg{SIMEXBoost} contains three functions: \code{ME\_Data}, \code{Boost\_VSE}, and \code{SIMEXBoost}. The function \code{ME\_Data} aims to generate artificial data under specific models listed in `Boosting Procedure' subsection and error-prone covariates. The function \code{Boost\_VSE} implements the boosting procedure in Algorithm~\ref{algo-ThrEEBoost}, and the function \code{SIMEXBoost} implements the error-eliminated boosting procedure as displayed in Algorithm~\ref{algo-SIMEX}. We now describe the details of these three functions.


\subsection*{\code{ME\_Data}}

We use the following command to obtain the artificial data:

\begin{example}
                    ME_Data(X,beta,type="normal",sigmae,pr0=0.5)
\end{example}
where the meaning of each argument is described as follows:

\begin{itemize}
    \item \code{X}: An $n\times p$ matrix with components generated by random variables. It is  provided by {the user}.
    
    \item \code{beta}: A $p$-dimensional vector of parameters, which is specified by {the user}.
    
    \item \code{type}: A regression model that is specified to generate the response. Some choices listed in `Boosting Procedure' subsection are provided in this argument.    \code{normal} means the linear regression model (\ref{LM}) with the error term generated by the standard normal distribution; \code{binary} means the logistic regression model (\ref{Logit}); \code{poisson} means the Poisson regression model (\ref{Pois}). In addition, the accelerated failure time (AFT) model is considered to fit length-biased and interval-censored survival data. Specifically, \code{AFT-normal} generates the length-biased and interval-censored survival data under the AFT model (\ref{AFT}) with the error term being normal distributions; \code{AFT-loggamma} generates the length-biased and interval-censored survival data under the AFT model with the error term being log-gamma distributions.
    
    \item \code{sigmae}: A $p \times p$ covariance matrix $\boldsymbol{\Sigma}_e$ in the measurement error model (\ref{mea_classic}). Given $\boldsymbol{\Sigma}_e$ with non-zero entries, by (\ref{mea_classic}), one can generate the error-prone covariates $\mathbf{X}_i^\ast$. Moreover, if $\boldsymbol{\Sigma}_e$ is given by the zero matrix, then $\mathbf{e}_i$ is generated as zero values, yielding that $\mathbf{X}_i^\ast$ is equal to $\mathbf{X}_i$, and thus, the resulting covariate is the original input given by users.
    
    \item \code{pr0}: A numerical value in an interval $(0,1)$. It is used to determine the censoring rate for the length-biased and interval-censored data.
    
\end{itemize}

The function \code{ME\_Data} returns a list of components:

\begin{itemize}
    \item \code{response}: It gives the response generated by a specific regression model. \code{type="normal"} gives a n-dimensional continuous vector; \code{type="binary"} gives a n-dimensional vector with binary entries; \code{type="poisson"} gives a n-dimensional vector with entries being counting numbers. In addition, \code{type="AFT-normal"} and \code{type="AFT-loggamma"} generates a $n\times 2$ matrix of length-biased and interval-censored responses, where the first column is the lower bound of an interval-censored response and the second column is the upper bound of an interval-censored response.
    
    \item \code{ME\_covariate}: This output gives a $n \times p$ matrix of ``error-prone'' or ``precisely measured'' covariates. Specifically, as discussed in an argument \code{sigmae}, if $\boldsymbol{\Sigma}_e$ is a non-zero covariance matrix, then the result of \code{ME\_covariate} is given by $\mathbf{X}_i^\ast$; if $\boldsymbol{\Sigma}_e$ is a zero matrix, then the result of \code{ME\_covariate} is the original input, say $\mathbf{X}_i$.
    
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\code{Boost\_VSE}}

We use the following function to perform Algorithm~\ref{algo-ThrEEBoost}:
\begin{example}
                 Boost_VSE(Y,Xstar,type="normal",Iter=200,Lambda=0)
\end{example}
where the meaning of each argument is described as follows:

\begin{itemize}
    \item \code{Y}: {The response variable}. If \code{type} is specified as \code{normal}, \code{binary}, or \code{poisson}, then \code{Y} should be a n-dimensional vector; if \code{type} is given by \code{AFT-normal} or \code{AFT-loggamma}, then \code{Y} should be a $n \times 2$ matrix of interval-censored responses, where the first column is the lower bound of an interval-censored response and the second column is the upper bound of an interval-censored response.
    
    \item \code{Xstar}: This argument needs a $n \times p$ matrix of covariates. It can be error-prone or precisely measured.
    
    \item \code{type}: This argument {specifies} the regression models as previously described in the function \texttt{ME\_Data} as well as their corresponding estimating functions given by (\ref{LM-g}),  (\ref{Logit-g}), (\ref{Pois-g}), and (\ref{eq7}).

    
    \item \code{Iter}: The number of iterations $M$ for the boosting procedure in Algorithm~\ref{algo-ThrEEBoost}.
    
    \item \code{Lambda}: A tuning parameter that aims to deal with the collinearity of covariates. \code{Lambda=0} means that no $L_2$-norm is involved, and it is {the} default value.
    
\end{itemize}

The function \code{Boost\_VSE} returns a list of components:

\begin{itemize}
    \item \code{BetaHat}: The vector of estimated coefficient obtained by Algorithm~\ref{algo-ThrEEBoost}. In particular, if the covariate \code{Xstar} is generated by \code{ME\_Data} with the argument \code{sigmae} being the zero matrix, then the resulting vector is the ``ordinary'' estimator based on the boosting procedure; if the covariate \code{Xstar} is generated by \code{ME\_Data} with the argument \code{sigmae} being a non-zero covariance matrix, then we call the resulting vector as the {\em naive} estimator due to involvement of measurement error effects without correction.
    

\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{\code{SIMEXBoost}}

Basically, some arguments in this function are the same as \code{Boost\_VSE}, except for some slightly different requirements and additional arguments that are related to the SIMEX method.

We use the following function to perform Algorithm~\ref{algo-SIMEX}:
\begin{example}
        SIMEXBoost(Y,Xstar,zeta=c(0,0.25,0.5,0.75,1),B=500,type="normal",sigmae,
                   Iter=100, Lambda=0, Extrapolation="linear")
\end{example}
where the meanings of arguments \code{Y}, \code{Xstar}, \code{type}, \code{Iter}, and \code{Lambda} are the same as those in the function \code{Boost\_VSE}; additional arguments \code{zeta}, \code{B}, \code{sigmae}, and \code{Extrapolation}, which are used to implement the SIMEX method to correct for measurement error effects, are described as follows:

\begin{itemize}

    \item \code{zeta}: The user-specific sequence of values described as $\mathcal{Z}$ in Step 1 of Algorithm~\ref{algo-SIMEX}.

    \item \code{B}: The user-specific positive number of repetition described as $B$ in Step 1 of Algorithm~\ref{algo-SIMEX}.
    

    \item \code{sigmae}: An $p \times p$ covariance matrix $\boldsymbol{\Sigma}_e$ in the measurement error model (\ref{mea_classic}). In practical applications, if auxiliary information is unavailable, sensitivity analyses can be adopted to reasonably specify values of $\boldsymbol{\Sigma}_e$. If additional information, such as repeated measurements or validation samples, is available, one can directly estimate $\boldsymbol{\Sigma}_e$. 
    

    \item \code{Extrapolation}: A extrapolation function for the SIMEX method implemented to Step 3 of Algorithm~\ref{algo-SIMEX}. In the framework of the SIMEX method, quadratic and linear functions are common. Therefore, in this argument, we provide two choices of the extrapolation functions, \code{linear} and \code{quadratic}. 

\end{itemize}

The function \code{SIMEXBoost} returns a list of components:

\begin{itemize}
    \item \code{BetaHatCorrect}:  The resulting vector of corrected {estimates} obtained by Algorithm~\ref{algo-SIMEX}. 
    

\end{itemize}

\section{Numerical Studies and Demonstration of Programming Code }
\label{Numerical}

In this section, we demonstrate the usage of the functions in the package \CRANpkg{SIMEXBoost}. There are two parts in this section: we first illustrate simulation studies for linear regression, Poisson regression, and AFT  models. After that, we apply the package to analyze a {real-world} dataset with binary responses based on a logistic regression model.

\subsection{Simulation Studies}

In this section, we use simulation studies to demonstrate applications of functions in the package \CRANpkg{SIMEXBoost} and assess the performance of the estimators derived by two functions \texttt{Boost\_VSE} and \texttt{SIMEXBoost}.


We consider the dimension of covariates $p=200$ or $500$, and let the sample size $n=400$. Let the true value $\boldsymbol{\beta}_0 = (1,1,1,\mathbf{0}_{p-3}^\top)^\top$, where $\mathbf{0}_q$ is a $q$-dimensional zero vector. The unobserved covariate $\mathbf{X}_i$ is generated by the standard normal distribution, and it can be used to generate the response $Y_i$ based on (\ref{LM}), (\ref{Pois}), and (\ref{AFT}). For the error-prone covariate $\mathbf{X}_i^\ast$, it can be generated by (\ref{mea_classic}), where $\boldsymbol{\Sigma}_e$ is a diagonal matrix with common entries $\sigma_e$ being 0.1, 0.3, and 0.5. 

Based on the artificial data $\Big\{ \{Y_i,\mathbf{X}_i^\ast\} : i=1,\cdots,n \Big\}$, we first use the function \code{Boost\_VSE} to obtain the estimator {\em without} measurement error correction, which is called the {\it naive} estimator. Next, we apply the function \code{SIMEXBoost} to derive the {\it corrected} estimator. To assess the performance of variable selection, we examine the specificity (Spe) and the sensitivity (Sen), where the specificity is defined as the proportion of zero coefficients that are correctly estimated to be zero, and the sensitivity is defined as the proportion of non-zero coefficients that are correctly estimated to be non-zero. In addition, to evaluate the performance of estimation, we use the $L_1$ and $L_2$-norms to {measure bias}, which are respectively defined as
\begin{eqnarray} \label{eva-bias}
\big\| \widehat{\boldsymbol{\beta}} - \boldsymbol{\beta}_0 \big\|_1 = \sum \limits_{j=1}^p \big| \widehat{\beta}_j - \beta_{0,j} \big| \ \ \text{and} \ \ \big\| \widehat{\boldsymbol{\beta}} - \boldsymbol{\beta}_0 \big\|_2 = \bigg\{\sum \limits_{j=1}^p \big( \widehat{\beta}_j - \beta_{0,j} \big)^2 \bigg\}^{1/2},
\end{eqnarray}
where $\widehat{\boldsymbol{\beta}}$ is the estimator, $\widehat{\beta}_j$ and $\beta_{0,j}$ are the $j$th component in $\widehat{\boldsymbol{\beta}}$ and $\boldsymbol{\beta}_0$, respectively.


{We use} two `\code{for}' loops cover all {combinations} of $p$ and $\sigma_e$ (\code{sigmae}). For each \code{p} and \code{sigmae}, we first {adapt} the function \code{ME\_Data} to generate the simulated data, where \code{Y} and \code{Xstar} represent the response and error-prone covariates, respectively. After that, to examine the naive estimator derived by Algorithm~\ref{algo-ThrEEBoost}, we use the function \code{Boost\_VSE} to derive the naive estimator and denote it by \code{naive}. To implement Algorithm~\ref{algo-SIMEX}, we adopt the function \code{SIMEXBoost}, where two components $\mathcal{Z}$ and $B$ in Step 1 of Algorithm~\ref{algo-SIMEX} are specified as \code{zeta=c(0,0.25,0.5,0.75,1)} and \code{B=50}, respectively. For the SIMEXBoost method, we also examine linear and quadratic extrapolation functions with the argument \code{Extrapolation="linear"} or \code{Extrapolation="quadratic"}, and denote two resulting estimators as \code{correctL} and \code{correctQ}, respectively. For the two functions \code{Boost\_VSE} and \code{SIMEXBoost}, we set the number of iterations \code{Iter=50}. To save the space in the limited text, we simply illustrate the model (\ref{LM}) with the argument \code{type="normal"}; numerical results under other models can be reprocued by the following code with \code{type="normal"} replaced by \code{type="poisson"} or \code{type="AFT-normal"}.


Next, we assess the performance of \code{naive}, \code{correctL}, and \code{correctQ}. Given a true vector of parameter \code{beta0}, we compute $L_1$ and $L_2$-norms in (\ref{eva-bias}) to examine the bias, and compute Spe and Sen to examine variable selection. Under a given \code{p} and \code{sigma}, biases and variable selection results are recorded by \code{EST1}, \code{EST2}, and \code{EST3} for the estimators \code{naive}, \code{correctL}, and \code{correctQ}, respectively. Finally, numerical results of three estimators \code{naive}, \code{correctL}, and \code{correctQ} under all settings are summarized by \code{NAIVE}, \code{SIMEXL}, and \code{SIMEXQ}, respectively.

\begin{example}
library(SIMEXBoost)
library(MASS)

NAIVE = NULL   # naive method
SIMEXL = NULL  # simex method with linear extrapolation function
SIMEXQ = NULL  # simex method with quadratic extrapolation function

for (p in c(200, 500)) {
  for (sigma in c(0.1, 0.3, 0.5)) {
    set.seed(202270)
    beta0 = c(1, 1, 1, rep(0, p - 3))
    
    X = matrix(rnorm((p) * 400),
               nrow = 400,
               ncol = p,
               byrow = TRUE)
    
    Sig = diag(sigma ^ 3, dim(X)[2])

    data = ME_Data(
      X = X,
      beta = beta0,
      type = "normal",
      sigmae = Sig
    )
    Y = data$response
    Xstar = data$ME_covariate
    
    naive = Boost_VSE(Y, Xstar, type = "normal", Iter = 50)$BetaHat
    
    correctL = SIMEXBoost(
      Y,
      Xstar,
      zeta = c(0, 0.25, 0.5, 0.75, 1),
      B = 50,
      type = "normal",
      sigmae = Sig,
      Iter = 50,
      Lambda = 0,
      Extrapolation = "linear"
    )$BetaHatCorrect
    correctL[which(abs(correctL) < 0.5)] = 0
    correctQ = SIMEXBoost(
      Y,
      Xstar,
      zeta = c(0, 0.25, 0.5, 0.75, 1),
      B = 50,
      type = "normal",
      sigmae = Sig,
      Iter = 50,
      Lambda = 0,
      Extrapolation = "quadratic"
    )$BetaHatCorrect
    correctQ[which(abs(correctQ) < 0.5)] = 0
    
    #############
    
    Sen = which(beta0 != 0)
    Spe0 =  which(beta0 == 0)
    
    ## results for the naive estimator
    naive = as.numeric(naive)
    L1_norm = sum(abs(naive - beta0))
    L2_norm = sqrt(sum((naive - beta0) ^ 2))
    Spe = length(which(naive[Spe0] == 0)) / length(Spe0)
    Sen = length(which(naive[Sen0] != 0)) / length(Sen0)
    
    ## results for the error-corrected estimator based on Extrapolation="linear"
    L1_norm_l = sum(abs(correctL - beta0))
    L2_norm_l = sqrt(sum((correctL - beta0) ^ 2))
    Spe_l = length(which(correctL[Spe0] == 0)) / length(Spe0)
    Sen_l = length(which(correctL[Sen0] != 0)) / length(Sen0)
    
    ## results for the error-corrected estimator based on Extrapolation="quadratic"
    L1_norm_q = sum(abs(correctQ - beta0))
    L2_norm_q = sqrt(sum((correctQ - beta0) ^ 2))
    Spe_q = length(which(correctQ[Spe0] == 0)) / length(Spe0)
    Sen_q = length(which(correctQ[Sen0] != 0)) / length(Sen0)
    
    
    #############
    
    NAIVE = rbind(NAIVE, c(L1_norm, L2_norm, Spe, Sen))
    SIMEXL = rbind(SIMEXL, c(L1_norm_l, L2_norm_l, Spe_l, Sen_l))
    SIMEXQ = rbind(SIMEXQ, c(L1_norm_q, L2_norm_q, Spe_q, Sen_q))
  }
}
\end{example}


Numerical results under (\ref{LM}),  (\ref{Pois}), and (\ref{AFT}) are placed in Tables~\ref{tab:Sim-1}-\ref{tab:Sim-4}, respectively.  We observe that the naive and corrected {estimates} are affected by the magnitudes of measurement error effects and the dimension $p$. When values of \code{p} and \code{sigma} become large, the biases given by $L_1$ and $L_2$-norms are increasing. For the comparison between the naive and corrected estimators, we can see that biases produced by the naive estimator are significantly larger than those obtained by the corrected estimator. In addition, for the variable selection result, the corrected estimator is able to correctly retain the informative covariates and exclude unimportant ones, except for some cases that one or two covariates may be falsely included. On the contrary, we can observe from the naive method that values of \code{Spe\_naive} are always small while values of \code{Sen\_naive} are equal to one. It indicates that the naive estimator retains the truly important covariates, and meanwhile, includes a lot of unimportant ones, which shows an evidence that the naive method fails to do variable selection. Finally, for the comparison between two extrapolation functions, \code{Extrapolation="linear"} and \code{Extrapolation="quadratic"}, we observe that the specification of a linear extrapolation function has slightly better performance than the case under a quadratic extrapolation function, especially when \code{sigma} is large. %This phenomenon is consistent with the discussion in \cite{Carroll:2006}. 

While the proposed SIMEXBoost method {can handle measurement error well}, the main concern is the computational time. According to the record of the system CPU time (in seconds) from the R function \texttt{proc.time()} under the device ASUS DESKTOP-HJSD47S with the processor Intel(R) Core(TM) i7-10700 CPU @ 2.90GHz, we find that, for each setting with fixed \code{p} and \code{sigmae}, the proposed SIMEXBoost method requires 14.09 and 15.04 seconds under \texttt{Extrapolation="linear"} and \texttt{Extrapolation="quadratic"} to derive the estimator for $p=200$, respectively. Unsurprising, when the dimension increases to $p=500$, the computational times under \texttt{Extrapolation="linear"} and \texttt{Extrapolation="quadratic"} are increasing to 17.70 and 19.68 seconds, respectively. On the other hand, without measurement error correction under each setting, the naive method needs 12.98 and 15.71 seconds to derive the estimator for $p=200$ and $500$, yielding the slightly faster computational time than the SIMEXBoost method. It might be due to the measurement error correction with the involvement of $\mathcal{Z}$ and the number of repetitions $B$ in Step 2 of Algorithm~\ref{algo-SIMEX}. As a result, we comment that the SIMEXBoost method is able to address measurement error correction, but a slightly longer computational time is the price that the users should pay for.


 \begin{table}[!ht]
       \huge
     \caption{Simulation results for the model (\ref{LM}) with \code{type="normal"}. ``Naive'' is the naive method obtained by the function \code{Boost\_VSE}. ``SIMEXBoost-Linear'' and ``SIMEXBoost-Quadratic'' refer to the proposed method obtained by the function \code{SIMEXBoost} with the argument \code{Extrapolation = "linear"} and \code{Extrapolation = "quadratic"}, respectively. $L_1$-norm and $L_2$-norm are given by (\ref{eva-bias}). Spe and Sen are specificity and sensitivity, respectively. } \label{tab:Sim-1}


 \scriptsize

\center
%   \renewcommand{\arraystretch}{0.75}
 \begin{tabular}{c c c c  ccccccccccccc}


 \hline
$p$  & $\sigma_e$  & Methods & $L_1$-norm & $L_2$-norm & Spe & Sen \\
\hline
200 & 0.1 & Naive & 6.900 & 0.738 & 0.487 & 1.000 \\
 &  & SIMEXBoost-Linear & 0.116 & 0.099 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 0.109 & 0.090 & 1.000 & 1.000 \\
 & 0.3 & Naive & 6.350 & 0.684 & 0.503 & 1.000 \\
 &  & SIMEXBoost-Linear & 0.101 & 0.072 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 0.106 & 0.075 & 1.000 & 1.000 \\
 & 0.5 & Naive & 7.600 & 0.758 & 0.426 & 1.000 \\
 &  & SIMEXBoost-Linear & 0.257 & 0.182 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 0.281 & 0.188 & 1.000 & 1.000 \\ \hline
500 & 0.1 & Naive & 8.050 & 0.733 & 0.732 & 1.000 \\
 &  & SIMEXBoost-Linear & 0.054 & 0.051 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 0.069 & 0.057 & 1.000 & 1.000 \\
 & 0.3 & Naive & 8.900 & 0.778 & 0.710 & 1.000 \\
 &  & SIMEXBoost-Linear & 0.300 & 0.187 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 0.300 & 0.187 & 1.000 & 1.000 \\
 & 0.5 & Naive & 10.000 & 0.889 & 0.692 & 1.000 \\
 &  & SIMEXBoost-Linear & 0.600 & 0.354 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 0.600 & 0.354 & 1.000 & 1.000 \\

\hline    
 %[0.5 ex]


\end{tabular}

\end{table}


 \begin{table}[!ht]
       \huge
     \caption{Simulation results for the model (\ref{Pois}) with \texttt{type="poisson"}. ``Naive'' is the naive method obtained by the function \texttt{Boost\_VSE}. ``SIMEXBoost-Linear'' and ``SIMEXBoost-Quadratic'' refer to the proposed method obtained by the function \code{SIMEXBoost} with the argument \code{Extrapolation = "linear"} and \code{Extrapolation = "quadratic"}, respectively. $L_1$-norm and $L_2$-norm are given by (\ref{eva-bias}). Spe and Sen are specificity and sensitivity, respectively. } \label{tab:Sim-3}


 \scriptsize

\center
%   \renewcommand{\arraystretch}{0.75}
 \begin{tabular}{c c c c  ccccccccccccc}


 \hline
$p$  & $\sigma_e$  & Methods & $L_1$-norm & $L_2$-norm & Spe & Sen \\
\hline
200 & 0.1 & Naive & 1.600 & 0.283 & 0.848 & 1.000 \\
 &  & SIMEXBoost-Linear & 0.208 & 0.126 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 0.538 & 0.407 & 1.000 & 1.000 \\
 & 0.3 & Naive & 1.750 & 0.304 & 0.838 & 1.000 \\
 &  & SIMEXBoost-Linear & 0.178 & 0.129 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 0.520 & 0.301 & 1.000 & 1.000 \\
 & 0.5 & Naive & 2.500 & 0.387 & 0.782 & 1.000 \\
 &  & SIMEXBoost-Linear & 0.366 & 0.230 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 1.371 & 0.877 & 0.995 & 1.000 \\
500 & 0.1 & Naive & 1.400 & 0.265 & 0.944 & 1.000 \\
 &  & SIMEXBoost-Linear & 0.096 & 0.087 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 0.428 & 0.385 & 1.000 & 1.000 \\
 & 0.3 & Naive & 1.850 & 0.304 & 0.930 & 1.000 \\
 &  & SIMEXBoost-Linear & 0.354 & 0.236 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 0.279 & 0.193 & 1.000 & 1.000 \\
 & 0.5 & Naive & 3.000 & 0.458 & 0.903 & 1.000 \\
 &  & SIMEXBoost-Linear & 0.554 & 0.393 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 1.238 & 0.771 & 1.000 & 1.000 \\

\hline    
 %[0.5 ex]


\end{tabular}

\end{table}


 \begin{table}[!ht]
       \huge
     \caption{Simulation results for the model (\ref{AFT}) with \texttt{type="AFT-normal"}. ``Naive'' is the naive method obtained by the function \texttt{Boost\_VSE}. ``SIMEXBoost-Linear'' and ``SIMEXBoost-Quadratic'' refer to the proposed method obtained by the function \code{SIMEXBoost} with the argument \code{Extrapolation = "linear"} and \code{Extrapolation = "quadratic"}, respectively. $L_1$-norm and $L_2$-norm are given by (\ref{eva-bias}). Spe and Sen are specificity and sensitivity, respectively. } \label{tab:Sim-4}


 \scriptsize

\center
%   \renewcommand{\arraystretch}{0.75}
 \begin{tabular}{c c c c  ccccccccccccc}


 \hline
$p$  & $\sigma_e$  & Methods & $L_1$-norm & $L_2$-norm & Spe & Sen \\
\hline
200 & 0.1 & Naive & 0.875 & 0.603 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Linear & 0.378 & 0.232 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 0.411 & 0.258 & 1.000 & 1.000 \\
 & 0.3 & Naive & 1.450 & 1.078 & 1.000 & 0.667 \\
 &  & SIMEXBoost-Linear & 0.400 & 0.247 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 0.877 & 0.605 & 1.000 & 1.000 \\
 & 0.5 & Naive & 4.700 & 2.801 & 1.000 & 0.667 \\
 &  & SIMEXBoost-Linear & 1.614 & 1.179 & 0.995 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 2.958 & 2.204 & 0.995 & 1.000 \\
500 & 0.1 & Naive & 2.750 & 1.521 & 0.998 & 0.333 \\
 &  & SIMEXBoost-Linear & 0.959 & 0.560 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 2.710 & 2.288 & 0.998 & 1.000 \\
 & 0.3 & Naive & 8.425 & 5.398 & 1.000 & 0.667 \\
 &  & SIMEXBoost-Linear & 0.949 & 0.554 & 1.000 & 1.000 \\
 &  & SIMEXBoost-Quadratic & 0.820 & 0.518 & 1.000 & 1.000 \\
 & 0.5 & Naive & 4.350 & 2.743 & 1.000 & 0.333 \\
 &  & SIMEXBoost-Linear & 2.541 & 1.364 & 0.998 & 0.667 \\
 &  & SIMEXBoost-Quadratic & 4.003 & 2.604 & 0.998 & 1.000 \\

\hline    
 %[0.5 ex]


\end{tabular}

\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection{Real Data Example}



In this section, we apply the package \CRANpkg{SIMEXBoost} to analyze the Company Bankruptcy Prediction data, which was from the Taiwan Economic Journal during a period 1999–2009, and it is now available on the Kaggle website (\url{https://www.kaggle.com/datasets/fedesoriano/company-bankruptcy-prediction}). In this dataset, there are 6819 observations and {95 continuous covariates related to customers' banking records}, such as assets, liability, income, and so on. In addition, the response is a binary random variable, where {the} value 1 reflects that the company is bankrupt, and 0 otherwise. All variables' names and descriptions can be found {on} the Kaggle website. The main interest in this study is to identify covariates that are informative to the bankruptcy status, and our goal is to apply (\ref{Logit}) to characterize the bankruptcy status and covariates. 


In addition to detecting important covariates from the multivariate variables, as mentioned in \cite{Chen:2023b}, measurement error is ubiquitous {in variables related to customers' banking records.} Hence, one should take measurement error effects into account when doing variable selection. Following {the example of the} simulation studies, we primarily consider two scenarios: first, without consideration of measurement error, one can directly apply the function \code{Boost\_VSE} to do variable selection. Second, if we wish to implement the function \code{SIMEXBoost} and take measurement error correction into account, the covariance matrix \code{sigmae} is needed. Since the dataset has no additional information to estimate $\boldsymbol{\Sigma}_e$, we may conduct {\em sensitivity analyses}, which enable us to characterize various degrees of measurement error and examine the different magnitudes of measurement error effects (e.g., \citealp{ChenYi:2021}; \citealp{Chen:2023b}). Specifically, we specify $\boldsymbol{\Sigma}_e$ as a diagonal matrix with common entries being $R=0.1, 0.3$ and $0.5$, and the extrapolation function is taken as linear or quadratic functions in Algorithm~\ref{algo-SIMEX}. 



To show the implementation of data analysis, we demonstrate the programming code below. For the convenience of data analysis, we {export} the full dataset {as a} csv file, which is available in \url{https://github.com/lchen723/SIMEXBoost.git}. One can download the dataset `bankruptcy\_data.csv'. Based on the dataset, we denote \code{Y} and \code{Xstar} as the response and the observed covariates, respectively. For the naive method without measurement error correction, we adopt the function \code{Boost\_VSE} with 50 iterations. For the implementation of sensitivity analyses, we use a `\code{for}' loop for different values of \code{R}. For each \code{R}, we run \code{SIMEXBoost} with \code{type="binary"} and two extrapolation functions \code{Extrapolation = "linear"} and \code{Extrapolation = "quadratic"}.

\begin{example}
library(MASS)
library(SIMEXBoost)
R = c(0.1, 0.3, 0.5)
data = read.table("bankruptcy_data.csv", sep = ",", head = T)
data = data[, -94]
Y = data[, 95]
Y = as.numeric(Y)
Xstar = t(as.matrix(data[, -95]))
Xstar = scale(Xstar)

p = dim(Xstar)[1]
n = dim(Xstar)[2]

set.seed(202270)
##naive
EST = NULL
naive = Boost_VSE(Y,
                  t(Xstar),
                  type = "binary",
                  Iter = 50,
                  Lambda = 0)$BetaHat
EST = cbind(EST, naive)
##linear
for (i in R) {
  correctL  =  SIMEXBoost(
    Y,
    t(Xstar),
    zeta = c(0, 0.25, 0.5, 0.75, 1),
    B = 50,
    type = "binary",
    sigmae = diag(i, p),
    Iter = 50,
    Lambda = 0,
    Extrapolation = "linear"
  )$BetaHatCorrect
  
  EST = rbind(EST, t(correctL))
}
##Quadratic

for (i in R) {
  correctQ  =  SIMEXBoost(
    Y,
    t(Xstar),
    zeta = c(0, 0.25, 0.5, 0.75, 1),
    B = 50,
    type = "binary",
    sigmae = diag(i, p),
    Iter = 50,
    Lambda = 0,
    Extrapolation = "quadratic"
  )$BetaHatCorrect
  
  EST = rbind(EST, t(correctQ))
}
round(EST, 3)

\end{example}


Numerical results are summarized in Table~\ref{rda}, where the column ``ID'' is the indexes of selected covariates, and the {heading} ``EST'' is the estimates of the coefficients. According to our analysis results, we find that the covariate ``Net Income to Stockholder's Equity'' (ID \#90) is only one that is commonly selected from  \code{Boost\_VSE} and \code{SIMEXBoost} under different values of $R$, which indicates that the covariate \#90 is an informative variable regardless of measurement error effects have been corrected or not. For the corrected estimator with different $R$ and extrapolation functions, we observe that variables ``Operating Profit Rate: Operating Income/Net Sales'' (ID \#6),  ``Pre-tax net Interest Rate: Pre-Tax Income/Net Sales'' (ID \#7), ``Continuous interest rate (after tax): Net Income-Exclude Disposal Gain or Loss/Net Sales'' (ID \#10), and ``Liability-Assets Flag'' (ID \#85) are selected, except for the scenarios ``Correct-L-0.3'' and ``Correct-L-0.5''. Moreover, different variables are selected under different values of $R$. It might be due to the impact of different magnitudes of measurement error. On the other hand, without measurement error correction, we observe from the naive estimator that most selected variables are different from the proposed estimator, such as ``Research and development expense rate: (Research and Development Expenses)/Net Sales'' (ID \#12), ``Tax rate (A): Effective Tax Rate'' (ID \#15), ``Per Share Net profit before tax (Yuan ¥): Pretax Income Per Share'' (ID \#23), ``Total Asset Growth Rate: Total Asset Growth'' (ID \#29), and ``Cash Reinvestment \%: Cash Reinvestment Ratio'' (ID \#32), and those estimates are close to zero. It implies that noninformative variables are possibly selected by the naive method if measurement error is not taken into account in analysis, and it shows an impact of measurement error in data analysis. 

Finally, we use the function \code{proc.time()} to record the system CPU time, and we find that the function \code{SIMEXBoost} requires 3.84 and 3.50 seconds to run, under \code{Extrapolation = "linear"} and \code{Extrapolation = "quadratic"} with a fixed $R$, respectively, while the function \code{Boost\_VSE} needs 0.45 seconds to derive the {estimates}. Consistent with the finding in simulation studies, the SIMEXBoost method requires slightly longer computational time than the naive method, which is caused by the repetition of boosting procedure and SIMEX correction. %{\color{red}However, it is also interesting to see that the computational time for real data analysis is longer than simulation studies. The possible reason might be the larger sample size in a real dataset.} {Similar to the finding in simulation studies,} longer computational time is a price to correct for measurement error effects and derive a precise estimator. 


%when \code{Extrapolation="linear"}, the selected gene expressions may be sensitive for the specified values of \code{sigmae}; if \code{Extrapolation="quadratic"}, then the selection result might be more robust regardless of the change of \code{sigmae}. This seems to be consistent with the finding in simulation studies that quadratic function may be more reliable to do extrapolation.




\begin{table}[!ht]

       \huge
     \caption{Variable selection and estimation for the Company Bankruptcy Prediction data based on the model (\ref{Logit}) and \texttt{type="binary"}. ``Naive'' refers to the naive method based on \code{Boost\_VSE}. ``Correct-L-0.1'', ``Correct-L-0.3'', and ``Correct-L-0.5'' refer to the function \code{SIMEXBoost} with \code{Extrapolation="linear"} and $R=0.1,0.3$ and $0.5$, respectively. ``Correct-Q-0.1'', ``Correct-Q-0.3'', and ``Correct-Q-0.5'' refer to the function \code{SIMEXBoost} with \code{Extrapolation="quadratic"} and $R=0.1,0.3$ and $0.5$, respectively. {The label ``$-$'' indicates that the variables are not selected.} } \label{rda}


 \scriptsize

\center
 \renewcommand{\arraystretch}{1.2}

\begin{tabular}{ccccccccccccccccccccccc}
\hline

ID & \multicolumn{7}{c} {EST } \\  \cline{2-8}  
 & Naive & Correct-L-0.1 & Correct-L-0.3 & Correct-L-0.5 & Correct-Q-0.1 & Correct-Q-0.3 & Correct-Q-0.5 \\
%%%%%
6 & $-$ & 0.400 & $-$ & -0.383 & 3.025 & 1.781 & 2.296\\
7 & $-$ & 0.474 & $-$ & $-$ & 2.895 & 2.126 & 2.943\\
8 & $-$ & 0.322 & $-$ & -0.267 & 3.143 & 2.628 & 2.122\\
10 & $-$ & 0.396 & 0.216 & $-$ & 1.861 & 1.760 & 1.749\\
12 & -0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
15 & -0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
23 & 0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
25 & $-$ & $-$ & $-$ & $-$ & -0.257 & $-$ & $-$\\
29 & -0.100 & $-$ & $-$ & $-$ &  & $-$ & $-$\\
32 & -0.050 & $-$ & $-$ & $-$ &  & $-$ & $-$\\
37 & $-$ & $-$ & -0.235 & $-$ & 0.766 & 1.452 & 0.365\\
38 & -0.150 & $-$ & -0.307 & $-$ & 1.604 & 1.679 & 0.404\\
43 & 0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
46 & $-$ & -0.300 & $-$ & $-$ & 0.257 & $-$ & $-$\\
48 & -0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
55 & -0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
59 & 0.050 & -0.300 & $-$ & $-$ & -1.282 & -1.285 & $-$\\
64 & -0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
65 & $-$ & 0.306 & 0.300 & $-$ & 0.916 & 1.261 & $-$\\
68 & -0.150 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
74 & -0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
77 & -0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
84 & 0.050 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
85 & $-$ & -0.472 & $-$ & 0.237 & -6.365 & -2.636 & -2.956\\
86 & -0.150 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
87 & $-$ & -0.300 & $-$ & $-$ & -0.912 & $-$ & $-$\\
90 & -1.350 & -1.518 & -2.397 & -2.433 & 1.613 & -1.602 & -3.668\\
94 & 0.200 & 0.240 & $-$ & $-$ & -1.553 & -1.532 & -0.362\\

 
\hline                         
\end{tabular}

\end{table}





 
\section{Discussion}

The package \CRANpkg{SIMEXBoost} provides a novel {method for handling} high-dimensional data subject to measurement error in covariates. It covers widely used GLM and AFT models in survival analysis, and provides a  strategy to deal with variable selection and  measurement error correction simultaneously. Moreover, our package is able to handle the collinearity in the covariates. {As evidence by longer computational times in numerical studies, the function \code{SIMEXBoost} seems to be more computationally demanding compared to the naive implementation, which is basically} caused by settings of $B$ and $\mathcal{Z}$ for correction of measurement error. Typically, following the similar idea of the Monte Carlo simulation, larger values of $B$ and $\mathcal{Z}$ usually give the stable estimator {but also incur} longer computational time. This is a common phenomenon in measurement error analysis and the SIMEX method (e.g., \citealp{Yi:2017}). {In summary, users need to consider whether to take measurement error effects into account based on their data and set arguments, such as $B$ and $\mathcal{Z}$, for the implementation based on their computational resources.}

The current {state of the package allows for} measurement error in continuous covariates and parametric models for exponential family distributed responses or time-to-event outcomes. There are still many {possible extensions to the methods}, such as consideration of measurement error in binary covariates or measurement error in the response, variable selection for semi-parametric regression models, including the Cox model in survival analysis or the partially linear single index model.


\section*{R Software}

The R package \CRANpkg{SIMEXBoost} is now available on the CRAN website (\url{https://cran.r-project.org/web/packages/SIMEXBoost/index.html}).


\section*{Acknowledgments}

The authors would like to thank the editorial team for useful comments to improve the initial manuscript.  Chen's research was supported by National Science and Technology Council with grant ID 110-2118-M-004-006-MY2.%\vspace*{-8pt}



