Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@misc{RProgrammingLanguage,
address = {Vienna, Austria},
author = {{R Core Team}},
publisher = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {https://www.r-project.org/},
year = {2020}
}
@article{Kelter2021BMCHodgesLehmann,
abstract = {Background: Null hypothesis significance testing (NHST) is among the most frequently employed methods in the biomedical sciences. However, the problems of NHST and p-values have been discussed widely and various Bayesian alternatives have been proposed. Some proposals focus on equivalence testing, which aims at testing an interval hypothesis instead of a precise hypothesis. An interval hypothesis includes a small range of parameter values instead of a single null value and the idea goes back to Hodges and Lehmann. As researchers can always expect to observe some (although often negligibly small) effect size, interval hypotheses are more realistic for biomedical research. However, the selection of an equivalence region (the interval boundaries) often seems arbitrary and several Bayesian approaches to equivalence testing coexist. Methods: A new proposal is made how to determine the equivalence region for Bayesian equivalence tests based on objective criteria like type I error rate and power. Existing approaches to Bayesian equivalence testing in the two-sample setting are discussed with a focus on the Bayes factor and the region of practical equivalence (ROPE). A simulation study derives the necessary results to make use of the new method in the two-sample setting, which is among the most frequently carried out procedures in biomedical research. Results: Bayesian Hodges-Lehmann tests for statistical equivalence differ in their sensitivity to the prior modeling, power, and the associated type I error rates. The relationship between type I error rates, power and sample sizes for existing Bayesian equivalence tests is identified in the two-sample setting. Results allow to determine the equivalence region based on the new method by incorporating such objective criteria. Importantly, results show that not only can prior selection influence the type I error rate and power, but the relationship is even reverse for the Bayes factor and ROPE based equivalence tests. Conclusion: Based on the results, researchers can select between the existing Bayesian Hodges-Lehmann tests for statistical equivalence and determine the equivalence region based on objective criteria, thus improving the reproducibility of biomedical research.},
author = {Kelter, Riko},
doi = {10.1186/s12874-021-01341-7},
issn = {1471-2288},
journal = {BMC Medical Research Methodology},
keywords = {Bayes factor,Bayesian Biostatistics,Bayesian equivalence testing,Bayesian testing,Region of practical equivalence (ROPE),Student's t-test},
number = {1},
publisher = {BioMed Central},
title = {{Bayesian Hodges-Lehmann tests for statistical equivalence in the two-sample setting: Power analysis, type I error rates and equivalence boundary selection in biomedical research}},
volume = {21},
year = {2021}
}
@book{Berger1980,
abstract = {Decision theory is generally taught in one of two very different ways. When of opti taught by theoretical statisticians, it tends to be presented as a set of mathematical techniques mality principles, together with a collection of various statistical procedures. When useful in establishing the optimality taught by applied decision theorists, it is usually a course in Bayesian analysis, showing how this one decision principle can be applied in various practical situations. The original goal I had in writing this book was to find some middle ground. I wanted a book which discussed the more theoretical ideas and techniques of decision theory, but in a manner that was constantly oriented towards solving statistical problems. In particular, it seemed crucial to include a discussion of when and why the various decision prin ciples should be used, and indeed why decision theory is needed at all. This original goal seemed indicated by my philosophical position at the time, which can best be described as basically neutral. I felt that no one approach to decision theory (or statistics) was clearly superior to the others, and so planned a rather low key and impartial presentation of the competing ideas. In the course of writing the book, however, I turned into a rabid Bayesian. There was no single cause for this conversion; just a gradual realization that things seemed to ultimately make sense only when looked at from the Bayesian viewpoint. 1 Basic Concepts -- 2 Utility and Loss -- 3 Prior Information and Subjective Probability -- 4 Bayesian Analysis -- 5 Minimax Analysis -- 6 Invariance -- 7 Preposterior and Sequential Analysis -- 8 Complete and Essentially Complete Classes -- Appendix 1. Common Statistical Densities -- I. Continuous -- II. Discrete -- Appendix 2. Technical Arguments from Chapter 4 -- I. Verification of Formulas (4.6) through (4.8) -- II. Verification of Formula (4.10) -- III. Verification of Formula (4.12) -- Appendix 3. Technical Arguments from Chapter 7 -- I. Verification of Formula (7.8) -- II. Verification of Formula (7.10) -- Notation and Abbreviations -- Author Index.},
author = {Berger, J.O.},
isbn = {9781475717273},
pages = {428},
publisher = {Springer New York},
title = {{Statistical Decision Theory : Foundations, Concepts, and Methods}},
year = {1980}
}
@article{Good1983a,
abstract = {(1983). C169. Barndorff-nielsen's plausibility function and a principle of least surprise. Journal of Statistical Computation and Simulation: Vol. 18, No. 2-3, pp. 215-218.},
author = {Good, I. J.},
doi = {10.1080/00949658308810689},
issn = {15635163},
journal = {Journal of Statistical Computation and Simulation},
number = {2-3},
pages = {215--218},
publisher = { Gordon and Breach Science Publishers },
title = {{Cl69. Baendorff-nielsen's plausibility function and a principle of least surprise}},
url = {https://www.tandfonline.com/doi/abs/10.1080/00949658308810689},
volume = {18},
year = {1983}
}
@book{field_discovering_spss_2017,
address = {London},
author = {Field, Andy},
edition = {5th},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Field - 2017 - Disovering Statistics Using IBM SPSS Statistics.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Field - 2017 - Disovering Statistics Using IBM SPSS Statistics(2).pdf:pdf},
publisher = {SAGE Publications Ltd},
title = {{Disovering Statistics Using IBM SPSS Statistics}},
year = {2017}
}
@book{hartmann_informatikunterricht_2007,
address = {Berlin Heidelberg},
annote = {Literaturangaben
OCLC: 255476551},
author = {Hartmann, Werner and N{\"{a}}f, Michael and Reichert, Raimond},
edition = {1. korrigi},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hartmann, N{\"{a}}f, Reichert - 2007 - Informatikunterricht planen und durchf{\"{u}}hren.pdf:pdf},
isbn = {978-3-540-34484-1},
keywords = {Informatikunterricht,Unterrichtsmethode},
publisher = {Springer},
series = {{\{}eXamen{\}}.press},
title = {{Informatikunterricht planen und durchf{\"{u}}hren}},
year = {2007}
}
@article{Fisher1923a,
abstract = {Reproduced with permission of Blackwell Publishers},
author = {Fisher, Ronald Aylmer},
doi = {10.2307/2548482},
issn = {00130427},
journal = {Economica},
month = {jun},
number = {3},
pages = {139--147},
title = {{Statistical Tests of Agreement Between Observation and Hypothesis.}},
url = {https://www.jstor.org/stable/10.2307/2548482?origin=crossref https://digital.library.adelaide.edu.au/dspace/handle/2440/15178},
year = {1923}
}
@article{Wagenmakers2010,
abstract = {In the field of cognitive psychology, the p-value hypothesis test has established a stranglehold on statistical reporting. This is unfortunate, as the p-value provides at best a rough estimate of the evidence that the data provide for the presence of an experimental effect. An alternative and arguably more appropriate measure of evidence is conveyed by a Bayesian hypothesis test, which prefers the model with the highest average likelihood. One of the main problems with this Bayesian hypothesis test, however, is that it often requires relatively sophisticated numerical methods for its computation. Here we draw attention to the Savage-Dickey density ratio method, a method that can be used to compute the result of a Bayesian hypothesis test for nested models and under certain plausible restrictions on the parameter priors. Practical examples demonstrate the method's validity, generality, and flexibility. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
author = {Wagenmakers, Eric-Jan and Lodewyckx, Tom and Kuriyal, Himanshu and Grasman, Raoul},
doi = {10.1016/j.cogpsych.2009.12.001},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Wagenmakers et al. - 2010 - Bayesian hypothesis testing for psychologists A tutorial on the Savage-Dickey method.pdf:pdf},
issn = {00100285},
journal = {Cognitive Psychology},
keywords = {Bayes factor,Hierarchical modeling,Model selection,Order-restrictions,Random effects,Statistical evidence},
number = {3},
pages = {158--189},
publisher = {Elsevier Inc.},
title = {{Bayesian hypothesis testing for psychologists: A tutorial on the Savage-Dickey method}},
volume = {60},
year = {2010}
}
@article{Gelfand2009,
author = {Gelfand, Alan E.},
journal = {Journal of the American Statistical Association},
keywords = {conditional probability},
number = {410},
pages = {398--409},
title = {{Sampling-Based Approaches to Calculating Marginal Densities}},
volume = {85},
year = {2009}
}
@article{Monahan,
author = {Monahan, John},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Monahan - Unknown - Statistical Literacy A Prerequisite for Evidence-Based Medicine.pdf:pdf},
title = {{Statistical Literacy A Prerequisite for Evidence-Based Medicine}},
url = {http://journals.sagepub.com/doi/pdf/10.1111/j.1539-6053.2008.00033{\_}1.x}
}
@article{Du2019,
abstract = {Due to some widely known critiques of traditional hypothesis testing, Bayesian hypothesis testing using the Bayes factor has been considered as a better alternative. Previous research about the influence of the prior focuses on the prior for the effect size and there is a debate about how to specify the prior. Thus, the focus of this paper is to explore the impact of different priors on the population mean and variance separately (separate priors) on the Bayes factor, and compare the separate priors with the priors on the effect size. Our simulation results show that both the prior distributions on mean and variance have a considerable influence on the Bayes factor, and different types of priors (different separate priors and priors on the effect size) have different influence patterns. We also find that regardless of separate priors or priors on the effect size, and shapes and centers of the priors, different priors could yield similar Bayes factors. Because noninformative prior distributions bias the Bayes factor in support of the null hypothesis, and very informative priors could be risky, we suggest that researchers use weakly informative priors as reasonable priors and they are expected to provide similar conclusions across different shapes and centers of prior distributions. Conducting sensitivity analysis is helpful in examining the influence of prior distributions and specifying reasonable prior distributions for the Bayes factor. A real data example is used to illustrate how to choose reasonable priors by a sensitivity analysis. We hope our results will help researchers choose prior distributions when conducting Bayesian hypothesis testing.},
author = {Du, Han and Edwards, Michael C. and Zhang, Zhiyong},
doi = {10.3758/s13428-019-01262-w},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Du, Edwards, Zhang - 2019 - Bayes factor in one-sample tests of means with a sensitivity analysis A discussion of separate prior distrib.pdf:pdf},
issn = {15543528},
journal = {Behavior Research Methods},
keywords = {Bayes factor,Bayesian hypothesis testing},
month = {oct},
number = {5},
pages = {1998--2021},
pmid = {31161425},
publisher = {Springer New York LLC},
title = {{Bayes factor in one-sample tests of means with a sensitivity analysis: A discussion of separate prior distributions}},
url = {https://asu.pure.elsevier.com/en/publications/bayes-factor-in-one-sample-tests-of-means-with-a-sensitivity-anal},
volume = {51},
year = {2019}
}
@book{kroemer_2015,
address = {Siegen},
author = {Kr{\"{o}}mer, Ralf and Nickel, Gregor},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kr{\"{o}}mer, Nickel - 2015 - SieB - Siegener Beitr{\"{a}}ge zur Geschichte und Philosophie der Mathematik.pdf:pdf},
publisher = {Universi - Universit{\"{a}}tsverlag Siegen},
title = {{SieB - Siegener Beitr{\"{a}}ge zur Geschichte und Philosophie der Mathematik}},
year = {2015}
}
@article{knobelsdorf_computer_2015,
abstract = {In North-Rhine Westphalia, the most populated state in Germany, Computer Science (CS) has been taught in secondary schools since the early 1970s. This article provides an overview of the past and current situation of CS education in North-Rhine Westphalia, including lessons learned through efforts to introduce and to maintain CS in secondary education. In particular, we focus on the differential school system and the educational landscape of CS education, the different facets of CS teacher education, and CS education research programs and directions that are directly connected with these aspects. In addition, this report offers a rationale for including CS education in general education, which includes the educational value of CS for students in today's information and knowledge society. Through this article, we ultimately provide an overview of the significant elements that are crucial for the successful integration of CS as a compulsory subject within secondary schools.},
author = {Knobelsdorf, Maria and Magenheim, Johannes and Brinda, Torsten and Engbring, Dieter and Humbert, Ludger and Pasternak, Arno and Schroeder, Ulrik and Thomas, Marco and Vahrenhold, Jan},
doi = {10.1145/2716313},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Knobelsdorf et al. - 2015 - Computer Science Education in North-Rhine Westphalia, Germany—A Case Study.pdf:pdf},
issn = {1946-6226},
journal = {Trans. Comput. Educ.},
keywords = {Computer science education,compulsory subject,computer science education research,computer science teacher education,curriculum development,extracurricular activities,k-12,rationale,secondary computer science education in Germany,secondary schools},
month = {apr},
number = {2},
pages = {9:1----9:22},
title = {{Computer Science Education in North-Rhine Westphalia, Germany—A Case Study}},
url = {http://doi.acm.org/10.1145/2716313},
volume = {15},
year = {2015}
}
@article{Ly2020,
abstract = {Despite an ongoing stream of lamentations, many empirical disciplines still treat the p value as the sole arbiter to separate the scientific wheat from the chaff. The continued reign of the p value is arguably due in part to a perceived lack of workable alternatives. In order to be workable, any alternative methodology must be (1) relevant: it has to address the practitioners' research question, which-for better or for worse-most often concerns the test of a hypothesis, and less often concerns the estimation of a parameter; (2) available: it must have a concrete implementation for practitioners' statistical workhorses such as the t test, regression, and ANOVA; and (3) easy to use: methods that demand practitioners switch to the theoreticians' programming tools will face an uphill struggle for adoption. The above desiderata are fulfilled by Harold Jeffreys's Bayes factor methodology as implemented in the open-source software JASP. We explain Jeffreys's methodology and showcase its practical relevance with two examples.},
author = {Ly, Alexander and Stefan, Angelika and van Doorn, Johnny and Dablander, Fabian and van den Bergh, Don and Sarafoglou, Alexandra and Kucharsk{\'{y}}, Simon and Derks, Koen and Gronau, Quentin F. and Raj, Akash and Boehm, Udo and van Kesteren, Erik-Jan and Hinne, Max and Matzke, Dora and Marsman, Maarten and Wagenmakers, Eric-Jan},
doi = {10.1007/s42113-019-00070-x},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ly et al. - 2020 - The Bayesian Methodology of Sir Harold Jeffreys as a Practical Alternative to the P Value Hypothesis Test.pdf:pdf},
issn = {2522-0861},
journal = {Computational Brain {\&} Behavior},
keywords = {Bayes factor,Hypothesis testing,Replication,Statistical evidence,Statistical practice},
number = {2},
pages = {153--161},
publisher = {Springer Science and Business Media LLC},
title = {{The Bayesian Methodology of Sir Harold Jeffreys as a Practical Alternative to the P Value Hypothesis Test}},
url = {https://doi.org/10.1007/s42113-019-00070-x},
volume = {3},
year = {2020}
}
@book{Laarhoven1987,
abstract = {It isn't that they can't see the solution. It is Approach your problems from the right end and begin with the answers. Then one day, that they can't see the problem. perhaps you will find the final question. O.K. Chesterton. The Scandal of Father 'The Hermit Clad in Crane Feathers' in R. Brown 'The point of a Pin'. van Oulik's The Chinese Maze Murders. Growing specialization and diversification have brought a host of monographs and textbooks or increasingly specialized topics. However, the "tree" of knowledg{\~{}} of mathematics and related fields does not grow only by putting forth new branches. It also {\textperiodcentered}happens, quite often in fact, that branches which were thought to be completely disparate are suddenly seen to be related. Further, the {\~{}}d and level of sophistication of mathematics applied in various sciences has changed drastically in recent years: measure theory is used (non-trivially) in regional and theoretical economics; algebraic geometry interacts with physics; the Minkowsky lemma, coding theory and the structure of water meet one another in packing and covering theory; quantum fields, crystal defects and mathematical programming profit from homotopy theory; Lie algebras are relevant to filtering; and prediction and electrical engineering can use Stein spaces. And in addition to this there are such new emerging subdisciplines as "experimental mathematics", "CFD", "completely integrable systems", "chaos, synergetics and large-scale order", which are almost impossible to fit into the existing classification schemes. They draw upon widely different sections of mathematics. 1 Introduction -- 2 Simulated annealing -- 3 Asymptotic convergence results -- 4 The relation with statistical physics -- 5 Towards implementing the algorithm -- 6 Performance of the simulated annealing algorithm -- 7 Applications -- 8 Some miscellaneous topics -- 9 Summary and conclusions.},
author = {Laarhoven, Peter J. M. and Aarts, E. H. L.},
isbn = {9789048184385},
pages = {187},
publisher = {Springer Netherlands},
title = {{Simulated Annealing: Theory and Applications}},
year = {1987}
}
@book{Ibragimov1981,
address = {New York},
author = {Ibragimov, I. A. and Has'minskii, R. Z.},
booktitle = {Statistical Estimation},
doi = {10.1007/978-1-4899-0027-2},
publisher = {Springer New York},
title = {{Statistical Estimation: Asymptotic Theory}},
year = {1981}
}
@article{Simmons2011,
abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
archivePrefix = {arXiv},
arxivId = {2021},
author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
doi = {10.1177/0956797611417632},
eprint = {2021},
isbn = {1467-9280 (Electronic)$\backslash$n0956-7976 (Linking)},
issn = {14679280},
journal = {Psychological Science},
keywords = {disclosure,methodology,motivated reasoning,publication},
month = {nov},
number = {11},
pages = {1359--1366},
pmid = {22006061},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant}},
url = {http://journals.sagepub.com/doi/10.1177/0956797611417632},
volume = {22},
year = {2011}
}
@article{Kelter2020c,
abstract = {Testing for differences between two groups is among the most frequently carried out statistical methods in empirical research. The traditional frequentist approach is to make use of null hypothesis significance tests which use p values to reject a null hypothesis. Recently, a lot of research has emerged which proposes Bayesian versions of the most common parametric and nonparametric frequentist two-sample tests. These proposals include Student's two-sample t-test and its nonparametric counterpart, the Mann–Whitney U test. In this paper, the underlying assumptions, models and their implications for practical research of recently proposed Bayesian two-sample tests are explored and contrasted with the frequentist solutions. An extensive simulation study is provided, the results of which demonstrate that the proposed Bayesian tests achieve better type I error control at slightly increased type II error rates. These results are important, because balancing the type I and II errors is a crucial goal in a variety of research, and shifting towards the Bayesian two-sample tests while simultaneously increasing the sample size yields smaller type I error rates. What is more, the results highlight that the differences in type II error rates between frequentist and Bayesian two-sample tests depend on the magnitude of the underlying effect.},
author = {Kelter, Riko},
doi = {10.1007/s00180-020-01034-7},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kelter - 2020 - Analysis of type I and II error rates of Bayesian and frequentist parametric and nonparametric two-sample hypothesis tes.pdf:pdf},
issn = {16139658},
journal = {Computational Statistics},
keywords = {Bayesian hypothesis testing,Null hypothesis significance testing,Parametric and non-parametric two-sample tests,Two-sample hypothesis tests,Type I and II error rates},
number = {in press},
publisher = {Springer Science and Business Media Deutschland GmbH},
title = {{Analysis of type I and II error rates of Bayesian and frequentist parametric and nonparametric two-sample hypothesis tests under preliminary assessment of normality}},
year = {2020}
}
@book{Fisher1950,
address = {New York},
author = {Fisher, R.A.},
publisher = {John Wiley {\&} Sons},
title = {{Contributions to Mathematical Statistics}},
url = {https://www.amazon.de/Contributions-Mathematical-Statistics-R-Fisher/dp/B0000CHSZU},
year = {1950}
}
@article{Ferguson2009,
author = {Ferguson, C. J.},
doi = {https://doi.org/10.1037/a0015808},
journal = {Professional Psychology: Research and Practice},
number = {5},
pages = {532--538},
title = {{An effect size primer: A guide for clinicians and researchers}},
volume = {40},
year = {2009}
}
@article{Pivtoraiko2015,
abstract = {Posterior cingulate cortex (PCC) accumulates amyloid-$\beta$ (A$\beta$) early in Alzheimer's disease (AD). The relative concentrations of full-length A$\beta$ and truncated, pyroglutamate-modified A$\beta$ (NpE3) forms, and their correlations to cognitive dysfunction in AD, are unknown. We quantified A$\beta$NpE3-42, A$\beta$NpE3-40, A$\beta$1-42, and A$\beta$1-40 concentrations in soluble (nonfibrillar) and insoluble (fibrillar) pools in PCC from subjects with an antemortem clinical diagnosis of no cognitive impairment, mild cognitive impairment, or mild-moderate AD. In clinical AD, increased PCC concentrations of A$\beta$ were observed for all A$\beta$ forms in the insoluble pool but only for A$\beta$1-42 in the soluble pool. Lower Mini-Mental State Exam and episodic memory scores correlated most strongly with higher concentrations of soluble and insoluble A$\beta$1-42. Greater neuropathology severity by Consortium to Establish a Registry for Alzheimer's Disease and National Institute on Aging-Reagan pathologic criteria was associated with higher concentrations of all measured A$\beta$ forms, except soluble A$\beta$NpE3-40. Low concentrations of soluble pyroglutamate A$\beta$ across clinical groups likely reflect its rapid sequestration into plaques, thus, the conversion to fibrillar A$\beta$ may be a therapeutic target.},
author = {Pivtoraiko, Violetta N and Abrahamson, Eric E and Leurgans, Sue E and DeKosky, Steven T and Mufson, Elliott J and Ikonomovic, Milos D},
doi = {10.1016/j.neurobiolaging.2014.06.021},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Pivtoraiko et al. - 2015 - Cortical pyroglutamate amyloid-$\beta$ levels and cognitive decline in Alzheimer's disease.pdf:pdf},
issn = {15581497},
journal = {Neurobiology of Aging},
keywords = {Alzheimer's disease,Amyloid-$\beta$,Episodic memory,MCI,Posterior cingulate cortex,Pyroglutamate-modified A$\beta$},
month = {jan},
number = {1},
pages = {12--19},
pmid = {25048160},
publisher = {NIH Public Access},
title = {{Cortical pyroglutamate amyloid-$\beta$ levels and cognitive decline in Alzheimer's disease}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25048160 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4268150},
volume = {36},
year = {2015}
}
@article{Good1993,
abstract = {(1993). C397 Refutation and rejection versus inexactification, and other comments concerning terminology. Journal of Statistical Computation and Simulation: Vol. 47, No. 1-2, pp. 91-92.},
author = {Good, I.J.},
doi = {10.1080/00949659308811514},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Good - 1993 - C397. Refutation and Rejection Versus Inexactification, and Other Comments Concerning Terminology.pdf:pdf},
issn = {15635163},
journal = {Journal of Statistical Computation and Simulation},
keywords = {Inexactification,refutation,rejection,terminology in statistics and philosophy of scienc},
number = {1-2},
pages = {91--92},
publisher = {Gordon and Breach Science Publishers},
title = {{C397. Refutation and Rejection Versus Inexactification, and Other Comments Concerning Terminology}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=gscs20},
volume = {47},
year = {1993}
}
@article{Gelman1996,
author = {Gelman, A. and Meng, Xiao-Li and Stern, H.},
journal = {Statistica Sinica},
number = {4},
pages = {733--807},
title = {{Posterior predictive assessment of model fitness via realized discrepancies}},
volume = {6},
year = {1996}
}
@article{Morey2011,
abstract = {Psychological theories are statements of constraint. The role of hypothesis testing in psychology is to test whether specific theoretical constraints hold in data. Bayesian statistics is well suited to the task of finding supporting evidence for constraint, because it allows for comparing evidence for 2 hypotheses against each another. One issue in hypothesis testing is that constraints may hold only approximately rather than exactly, and the reason for small deviations may be trivial or uninteresting. In the large-sample limit, these uninteresting, small deviations lead to the rejection of a useful constraint. In this article, we develop several Bayes factor 1-sample tests for the assessment of approximate equality and ordinal constraints. In these tests, the null hypothesis covers a small interval of non-0 but negligible effect sizes around 0. These Bayes factors are alternatives to previously developed Bayes factors, which do not allow for interval null hypotheses, and may especially prove useful to researchers who use statistical equivalence testing. To facilitate adoption of these Bayes factor tests, we provide easy-to-use software. {\textcopyright} 2011 American Psychological Association.},
author = {Morey, Richard D. and Rouder, Jeffrey N.},
doi = {10.1037/a0024377},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Morey, Rouder - 2011 - Bayes Factor Approaches for Testing Interval Null Hypotheses.pdf:pdf},
issn = {1082989X},
journal = {Psychological Methods},
keywords = {Bayes factor,Bayesian analysis,Effect size,Equivalence tests},
number = {4},
pages = {406--419},
pmid = {21787084},
publisher = {Psychol Methods},
title = {{Bayes Factor Approaches for Testing Interval Null Hypotheses}},
volume = {16},
year = {2011}
}
@article{Moussouris1974,
abstract = {This paper concerns random systems made up out of a finite collection of elements. We are interested in how a fixed structure of interactions reflects on the assignment of probabilities to overall states. In particular, we consider two simple models of random systems : one generalizing ...},
author = {Moussouris, John},
doi = {10.1007/BF01011714},
issn = {00224715},
journal = {Journal of Statistical Physics},
keywords = {Gibbs potential,Gibbs-Markov equivalence,Markov assumption,Random system,barriers and wells,constraints,higher-order equations,inversion formula for potentials,limit representations,local Markov conditions,strongly Markovian systems},
number = {1},
pages = {11--33},
title = {{Gibbs and Markov random systems with constraints}},
volume = {10},
year = {1974}
}
@article{Kultus2004,
author = {f{\"{u}}r Kultus, Ministerium and Jugend and {Sport des Landes Baden-W{\"{u}}rttemberg}, Stuttgart},
title = {{Bildungsstandard Informatik Gymnasium}},
url = {http://www.bildung-staerkt-menschen.de/service/downloads/Bildungsstandards/Gym/Gym{\_}Inf{\_}wb{\_}bs.pdf},
year = {2004}
}
@incollection{Barbini2013,
archivePrefix = {arXiv},
arxivId = {0803973233},
author = {Barbini, Emanuela and Manzi, Pietro and Barbini, Paolo},
booktitle = {Current Topics in Public Health 2.},
doi = {10.5772/52402},
eprint = {0803973233},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Barbini, Manzi, Barbini - 2013 - Bayesian Approach in Medicine and Health Management.pdf:pdf},
isbn = {9789533077826},
issn = {9789533070865},
month = {may},
pages = {17--35},
pmid = {100220790},
publisher = {InTech},
title = {{Bayesian Approach in Medicine and Health Management}},
year = {2013}
}
@book{Carnap1950,
address = {Chicago},
author = {Carnap, Rudolf},
publisher = {University of Chicago Press},
title = {{Logical foundations of probability}},
year = {1950}
}
@article{xinogalos_object-oriented_2015,
abstract = {The Object-Oriented Programming (OOP) technique is nowadays the most popular programming technique among tertiary education institutions. However, learning OOP is a cognitively demanding task for undergraduate students. Several difficulties and misconceptions have been recorded in the literature for both OOP concepts and languages, mainly Java. This article focuses on reviewing and advancing research on the most fundamental OOP concepts, namely, the concepts of “object” and “class” and their role during program execution. The results of a long-term investigation on the subject are presented, focusing on a study exploring undergraduate students' conceptions on “objects” and “classes.” The study advances related research on categories of conceptions on “objects” and “classes” by providing quantitative results, in addition to qualitative results, regarding the frequency of the recorded conceptions. Nearly half the students seem to comprehend the modeling and static/dynamic aspects of the concepts “object” and “class.” Implications for achieving a deep conceptual understanding of text, action, and modeling aspects of these fundamental concepts are also discussed. Information regarding the programming environments utilized in the course and key features of the applied teaching approach are presented, in order to facilitate both a better understanding of the context and a better employment of the results of the presented study. Finally, proposals for enhancing the contribution of this and similar studies are made.},
annote = {Macedonia},
author = {Xinogalos, Stelios},
doi = {10.1145/2700519},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Xinogalos - 2015 - Object-{\{}Oriented{\}} {\{}Design{\}} and {\{}Programming{\}} {\{}An{\}} {\{}Investigation{\}} of {\{}Novices{\}}' {\{}Conceptions{\}} on {\{}Objects{\}} and {\{}Cla.pdf:pdf}},
issn = {1946-6226},
journal = {Trans. Comput. Educ.},
keywords = {class,conceptions,misconceptions,object,object-oriented programming,teaching/learning programming},
month = {jul},
number = {3},
pages = {13:1----13:21},
shorttitle = {Object-{\{}Oriented{\}} {\{}Design{\}} and {\{}Programming{\}}},
title = {{Object-Oriented Design and Programming: An Investigation of Novices' Conceptions on Objects and Classes}},
url = {http://doi.acm.org/10.1145/2700519},
volume = {15},
year = {2015}
}
@article{Fisher1939,
author = {Fisher, Ronald A.},
doi = {10.1214/aoms/1177732151},
issn = {00034851},
journal = {The Annals of Mathematical Statistics},
month = {dec},
number = {4},
pages = {383--388},
publisher = {Institute of Mathematical Statistics},
title = {{A Note on Fiducial Inference}},
url = {http://projecteuclid.org/euclid.aoms/1177732151 http://www.jstor.org/stable/2235616},
volume = {10},
year = {1939}
}
@article{Besag1986,
author = {Besag, Julian},
journal = {1986},
keywords = {auto-,classification,gibbs sampler,grey-level scenes,image processing,image restoration,interactions,iterated conditional modes,markov random fields,maximum a posteriori estimation,pairwise,pattern recognition,remote sensing,segmentation,simulated annealing},
number = {3},
pages = {259--302},
title = {{On the Statistical Analysis of Dirty Pictures}},
volume = {48},
year = {1986}
}
@phdthesis{Ly2017BayesFactorsForResearchWorkers,
author = {Ly, Alexander},
school = {University of Amsterdam},
title = {{Bayes factors for research workers}},
url = {https://dare.uva.nl/personal/pure/en/publications/bayes-factors-for-research-workers(e601b852-1b29-407b-a276-1ccd2a2ed37b).html},
year = {2017}
}
@incollection{Bingham1980,
author = {Bingham, Christopher},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_17},
pages = {171--181},
publisher = {Springer, New York, NY},
title = {{Distribution on the Sphere}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}17},
year = {1980}
}
@article{Anderson1983,
abstract = {The problem of testing for equivalence in clinical trials is restated here in terms of the proper clinical hypotheses and a simple classical frequentist significance test based on the central t distribution is derived. This method is then shown to be more powerful than the methods based on usual (shortest) and symmetric confidence intervals. We begin by considering a noncentral t statistic and then consider three approximations to it. A simulation is used to compare actual test sizes to the nominal values in crossover and completely randomized designs. A central t approximation was the best. The power calculation is then shown to be based on a central t distribution, and a method is developed for obtaining the sample size required to obtain a specified power. For the approximations, a simulation compares actual powers to those obtained for the t distribution and confirms that the theoretical results are close to the actual powers. {\textcopyright} 1983 Taylor {\&} Francis Group, LLC. All rights reserved.},
author = {Anderson, Sharon and Hauck, Walter W.},
doi = {10.1080/03610928308828634},
issn = {1532415X},
journal = {Communications in Statistics - Theory and Methods},
keywords = {bioequivalence,clinical trials,completely randomized design,composite null-hypothesis,crossover design,noncentral t distribution},
number = {23},
pages = {2663--2692},
publisher = {Marcel Dekker, Inc.},
title = {{A New Procedure for Testing Equivalence in Comparative Bioavailability and Other Clinical Trials}},
volume = {12},
year = {1983}
}
@article{DeGunst2003,
abstract = {We describe a Bayesian approach to incorporate between-individual heterogeneity associated with parameters of complicated biological models. We emphasize the use of the Markov chain Monte Carlo (MCMC) method in this context and demonstrate the implementation and use of MCMC by analysis of simulated overdispersed Poisson counts and by analysis of an experimental data set on preneoplastic liver lesions (their number and sizes) in the presence of heterogeneity. These examples show that MCMC-based estimates, derived from the posterior distribution with uniform priors, may agree well with maximum likelihood estimates (if available). However, with heterogeneous parameters, maximum likelihood estimates can be difficult to obtain, involving many integrations. In this case, the MCMC method offers substantial computational advantages. Copyright (C) 2003 John Wiley Sons, Ltd.},
author = {de Gunst, Mathisca C.M. and Dewanji, Anup and Luebeck, E. Georg},
doi = {10.1002/sim.1441},
isbn = {0277-6715},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {Inter-individual variation,Markov chain Monte Carlo (MCMC),N-nitrosomorpholine (NNM),Premalignant lesions,Stochastic growth model},
month = {may},
number = {10},
pages = {1691--1707},
pmid = {12720305},
publisher = {John Wiley and Sons Ltd},
title = {{Exploring heterogeneity in tumour data using Markov chain Monte Carlo}},
url = {http://doi.wiley.com/10.1002/sim.1441},
volume = {22},
year = {2003}
}
@article{Dawid1991,
author = {Dawid, A.P.},
journal = {Journal of the Royal Stastical Society Series B (Methodological)},
number = {1},
pages = {79--109},
title = {{Fisherian Inference and Likelihood: Prequential Frames of Reference}},
volume = {53},
year = {1991}
}
@book{Bernstein1917,
address = {Moscow},
author = {Bernstein, S.},
title = {{Theory of Probability}},
year = {1917}
}
@article{Ridgway2017,
author = {Ridgway, Jim and Nicholson, James and Durham, U and Gal, Iddo and Haifa, U},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ridgway et al. - 2017 - Conceptual Framework for Civic Statistics.pdf:pdf},
title = {{Conceptual Framework for Civic Statistics}},
year = {2017}
}
@techreport{Gelfand1992,
address = {Stanford},
author = {Gelfand, Alan E and Dey, Dipak K and Chang, Hong},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gelfand, Dey, Chang - 1992 - Model Determination Using Predictive Distributions with Implementation via Sampling-Based Methods.pdf:pdf},
institution = {Stanford University, Department of Statistics},
title = {{Model Determination Using Predictive Distributions with Implementation via Sampling-Based Methods}},
url = {https://apps.dtic.mil/docs/citations/ADA258777},
year = {1992}
}
@article{goodman2001,
abstract = {An abstract is unavailable.},
author = {Goodman, Steven N},
doi = {10.1097/00001648-200105000-00006},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Goodman - 2001 - Of P-Values and Bayes A Modest Proposal.pdf:pdf},
isbn = {1044-3983},
issn = {1044-3983},
journal = {Epidemiology},
number = {3},
pages = {295--297},
pmid = {11337600},
title = {{Of P-Values and Bayes: A Modest Proposal}},
url = {https://swfsc.noaa.gov/uploadedFiles/Divisions/PRD/Programs/ETP{\_}Cetacean{\_}Assessment/Of{\_}P{\_}Values{\_}and{\_}Bayes{\_}{\_}A{\_}Modest{\_}Proposal.6.pdf http://content.wkhealth.com/linkback/openurl?sid=WKPTLP:landingpage{\&}an=00001648-200105000-00006},
volume = {12},
year = {2001}
}
@article{Deutsch1982,
author = {Deutsch, Rahmenplan},
journal = {Sekundarstufe II. Land},
title = {{Gymnasiale Oberstufe}},
year = {1982}
}
@book{Staddon,
abstract = {"This book shows how science works, fails to work, or pretends to work, by looking at examples from such diverse fields as physics, biomedicine, psychology, and economics. Social science affects our lives every day through the predictions of experts and the rules and regulations they devise. Such sciences, from economics and social psychology to health science and epidemiology, operate under very different (and often more tenuous) conditions from physical sciences such as chemistry. Yet, their methods and results must also be judged according to the same scientific standards, and every literate citizen should understand these standards and be able to tell the difference between good science and bad. Scientific Method enables readers to develop a critical, informed view of scientific practice by discussing concrete examples of how real scientists have approached the problems of their fields. It is ideal for students and professionals trying to make sense of the role of science in society, and of the meaning, value, and limitations of scientific methodology in the social sciences."--Provided by publisher. Basic science -- Experiment -- Null hypothesis statistical testing -- Social science : psychology -- Social science : economics -- Behavioral economics -- 'Efficient' markets -- Summing up.},
author = {Staddon, John},
booktitle = {Scientific Method: How Science Works, Fails to Work, and Pretends to Work},
doi = {10.4324/9781315100708},
isbn = {9781315100708},
keywords = {1-13-829536-1,1138295361,1st edition,978-1-13-829536-0,9781138295360,John Staddon,Routledge,basic,behavioral,economics,experimental design {\&} research methods,experiments,fails,how,hypothesis,method,methodology,methods,null,paperback,philosophy,philosophy of psychology,pretends,psychological,psychology,quantitative methods,research methods for arts and humanities,research methods for social and behavioral science,research methods in management,science,scientific,work,works},
pages = {1--148},
publisher = {Routledge},
title = {{Scientific method: How science works, fails to work, and pretends to work}},
url = {https://www.routledge.com/Scientific-Method-How-Science-Works-Fails-to-Work-and-Pretends-to-Work/Staddon/p/book/9781138295360},
year = {2017}
}
@incollection{Wallace1980,
author = {Wallace, David L.},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_14},
pages = {119--147},
publisher = {Springer, New York, NY},
title = {{The Behrens-Fisher and Fieller-Creasy Problems}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}14},
year = {1980}
}
@inproceedings{berglund_debating_2007,
abstract = {In this paper we discuss problems related to the teaching of object-oriented programming (OOP). We argue that more research on how the computer science teacher understands OOP would be beneficial. Our argument takes its point of departure in three sets of studies: (1) an ongoing study on how computer science teachers understand core concepts of OOP, (2) a study of how the teaching of OOP is discussed within the CS community, and (3) a set of studies that discuss the different ways in which CS teachers experience their teaching. This paper reports on an ongoing study of the different ways in which computing science teachers understand object-oriented programming, and what they mean when use the term objects first. The phenomenographic research approach has been applied to the analysis of a discussion that occurred in the SIGCSE-members mailing list. Two understandings of objects first have been identified: (1) as an extension of imperative programming, and (2) as conceptually different from imperative programming. These two understandings are illustrated via the differing ways in which computing science teachers use the term polymorphism.},
address = {Darlinghurst, Australia, Australia},
annote = {Sweden {\&} Australia},
author = {Berglund, Anders and Lister, Raymond},
booktitle = {Proceedings of the Seventh Baltic Sea Conference on Computing Education Research - Volume 88},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Berglund, Lister - 2007 - Debating the OO Debate Where is the Problem.pdf:pdf},
isbn = {978-1-920682-69-9},
keywords = {object-oriented programming,objects-first,phenomenography},
pages = {171--174},
publisher = {Australian Computer Society, Inc.},
series = {Koli {\{}Calling{\}} '07},
shorttitle = {Debating the {\{}OO{\}} {\{}Debate{\}}},
title = {{Debating the OO Debate: Where is the Problem?}},
url = {http://dl.acm.org/citation.cfm?id=2449323.2449344},
year = {2007}
}
@book{Savage1954,
address = {New York},
author = {Savage, Leonard J.},
publisher = {John Wiley {\&} Sons},
title = {{The Foundations of Statistics}},
year = {1954}
}
@book{Howie2002,
abstract = {This book investigates how Bayesianism as one theory of probability was discredited during the 1920s and 1930s by two British scientists and shows how the choice of a certain interpretation of probability depends on the experiences of the individuals involved. Cover; Half-title; Series-title; Title; Copyright; Contents; Acknowledgments; 1 Introduction; 2 Probability up to the Twentieth Century; 3 R.A. Fisher and Statistical Probability; 4 Harold Jeffreys and Inverse Probability; 5 The Fisher ... Jeffreys Exchange, 1932 ... 1934; 6 Probability During the 1930s; 7 Epilogue and Conclusions; Appendix 1 Sources for Chapter 2; Appendix 2 Bayesian Conditioning as a Model of Scientific Inference; Appendix 3 Abbreviations Used in the Footnotes; Bibliography; Index.},
address = {Cambridge},
author = {Howie, David},
isbn = {0521812518},
pages = {262},
publisher = {Cambridge University Press},
title = {{Interpreting Probability : Controversies and Developments in the Early Twentieth Century}},
year = {2002}
}
@article{Colquhoun2019,
abstract = {It is widely acknowledged that the biomedical literature suffers from a surfeit of false positive results. Part of the reason for this is the persistence of the myth that observation of p {\textless} 0.05 is sufficient justification to claim that you have made a discovery. It is hopeless to expect users to change their reliance on p-values unless they are offered an alternative way of judging the reliability of their conclusions. If the alternative method is to have a chance of being adopted widely, it will have to be easy to understand and to calculate. One such proposal is based on calculation of false positive risk(FPR). It is suggested that p-values and confidence intervals should continue to be given, but that they should be supplemented by a single additional number that conveys the strength of the evidence better than the p-value. This number could be the minimum FPR (that calculated on the assumption of a prior probability of 0.5, the largest value that can be assumed in the absence of hard prior data). Alternatively one could specify the prior probability that it would be necessary to believe in order to achieve an FPR of, say, 0.05. ARTICLE HISTORY},
author = {Colquhoun, David},
doi = {10.1080/00031305.2018.1529622},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Colquhoun - 2019 - The False Positive Risk A Proposal Concerning What to Do About p-Values.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {bayes,false,false positive,false positive risk,fpr,positive report probability},
number = {sup1},
pages = {192--201},
title = {{The False Positive Risk: A Proposal Concerning What to Do About p-Values}},
volume = {73},
year = {2019}
}
@article{Rosenman1975,
abstract = {Clinical coronary heart disease (CHD) occurred in 257 subjects during eight to nine years of follow-up (average, 8 1/2 years) in a prospective study of 39- to 59-year-old employed men. Incidence of CHD was significantly associated with parental CHD history, reported diabetes, schooling, smoking habits, overt behavior pattern, blood pressure, and serum levels of cholesterol, triglyceride, and beta-lipoproteins. The type A behavior pattern was strongly related to the CHD incidence, and this association could not be explained by association of behavior pattern with any single predictive risk factor or with any combination of them.},
author = {Rosenman, R H and Brand, R J and Jenkins, D and Friedman, M and Straus, R and Wurm, M},
issn = {0098-7484},
journal = {JAMA},
month = {aug},
number = {8},
pages = {872--7},
pmid = {1173896},
title = {{Coronary heart disease in Western Collaborative Group Study. Final follow-up experience of 8 1/2 years.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/1173896},
volume = {233},
year = {1975}
}
@article{Chen2014,
abstract = {Polya tree priors are random probability distributions that are easily centered at standard parametric families, such as the normal. As such, they provide a convenient avenue toward creating a parametric/nonparametric test statistic "blend" for the classic problem of testing whether data distributions are the same across several subpopulations. Test-statistics that are (empirical) Bayes factors constructed from independent Polya tree priors are proposed. The Polya tree centering distributions are Gaussian with parameters estimated from the data and the p-values are obtained through the permutation of group membership indicators. Generalizations to censored and multivariate data are provided. The conceptually simple test statistic fares surprisingly well against competitors in simulations. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Chen, Yuhui and Hanson, Timothy E.},
doi = {10.1016/j.csda.2012.11.003},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {ANOVA,Behrens-Fisher problem,Log-rank test,MANOVA,Polya tree},
pages = {335--346},
title = {{Bayesian nonparametric k-sample tests for censored and uncensored data}},
volume = {71},
year = {2014}
}
@book{Weaver1963,
address = {Garden City},
author = {Weaver, W.},
publisher = {Science Study Series, Doubleday and Co. Inc.},
title = {{Lady Luck, The Theory of Probability}},
year = {1963}
}
@inproceedings{xinogalos_comparison_2007,
abstract = {Teaching Object Oriented Programming (OOP) to novices is quite problematic. In order to overcome the difficulties various teaching approaches have been proposed. In this paper, we present our findings regarding two teaching approaches based on two different environments: (i) teaching with the Educational Programming Environment of BlueJ; (ii) teaching with the Programming Microworld of objectKarel. The presented results are based on students' answers to a questionnaire that was given to students at the last lesson of the two studies, and refer exclusively to the comprehension of the most basic concepts of OOP: objects and classes. The analysis of the results shows that the combined use of the two environments seems to constitute a better didactic strategy from just using one of the two environments.},
address = {Anaheim, CA, USA},
author = {Xinogalos, Stelios and Satratzemi, Maya and Dagdilelis, Vassilios},
booktitle = {Proceedings of the 10th IASTED International Conference on Computers and Advanced Technology in Education},
isbn = {978-0-88986-700-0},
keywords = {educational programming environments,educational software,microworlds,object-oriented programming},
pages = {49--54},
publisher = {ACTA Press},
series = {{\{}CATE{\}} '07},
title = {{A Comparison of Two Object-oriented Programming Environments for Novices}},
url = {http://dl.acm.org/citation.cfm?id=1650165.1650176},
year = {2007}
}
@misc{Yarrow2009,
abstract = {Events like the World Championships in athletics and the Olympic Games raise the public profile of competitive sports. They may also leave us wondering what sets the competitors in these events apart from those of us who simply watch. Here we attempt to link neural and cognitive processes that have been found to be important for elite performance with computational and physiological theories inspired by much simpler laboratory tasks. In this way we hope to inspire neuroscientists to consider how their basic research might help to explain sporting skill at the highest levels of performance. {\textcopyright} 2009 Macmillan Publishers Limited. All rights reserved.},
author = {Yarrow, Kielan and Brown, Peter and Krakauer, John W.},
booktitle = {Nature Reviews Neuroscience},
doi = {10.1038/nrn2672},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Yarrow, Brown, Krakauer - 2009 - Inside the brain of an elite athlete The neural processes that support high achievement in sports.pdf:pdf},
issn = {1471003X},
keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,Neurobiology,Neurosciences,general},
month = {aug},
number = {8},
pages = {585--596},
pmid = {19571792},
publisher = {Nature Publishing Group},
title = {{Inside the brain of an elite athlete: The neural processes that support high achievement in sports}},
url = {www.nature.com/reviews/neuro},
volume = {10},
year = {2009}
}
@article{Rothman2021,
author = {Rothman, Kenneth J.},
doi = {10.1093/aje/kwaa137},
issn = {14766256},
journal = {American Journal of Epidemiology},
keywords = {Comment,Kenneth J Rothman,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PubMed Abstract,doi:10.1093/aje/kwaa137,pmid:33524113},
month = {feb},
number = {2},
pages = {194--195},
pmid = {32648896},
publisher = {Oxford University Press},
title = {{Rothman Responds to "surprise!"}},
url = {https://pubmed.ncbi.nlm.nih.gov/33524113/},
volume = {190},
year = {2021}
}
@article{Pereira2008,
abstract = {The Full Bayesian Significance Test, FBST, is extensively reviewed. Its test statistic, a genuine Bayesian measure of evidence, is discussed in detail. Its behavior in some problems of statistical inference like testing for independence in contingency tables is discussed. {\textcopyright} 2008 International Society for Bayesian Analysis.},
author = {Pereira, C. A. d. B. and Stern, J. M. and Wechsler, S.},
doi = {10.1214/08-BA303},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Pereira, Stern, Wechsler - 2008 - Can a Significance Test be genuinely Bayesian.pdf:pdf},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Bayes tests,Sharp hypotheses,Significance tests},
number = {1},
pages = {79--100},
title = {{Can a Significance Test be genuinely Bayesian?}},
volume = {3},
year = {2008}
}
@book{Ibrahim2001,
abstract = {Survival analysis arises in many fields of study including medicine, biology, engineering, public health, epidemiology, and economics. This book provides a comprehensive treatment of Bayesian survival analysis. Several topics are addressed, including parametric models, semiparametric models based on prior processes, proportional and non-proportional hazards models, frailty models, cure rate models, model selection and comparison, joint models for longitudinal and survival data, models with time varying covariates, missing covariate data, design and monitoring of clinical trials, accelerated failure time models, models for mulitivariate survival data, and special types of hierarchial survival models. Also various censoring schemes are examined including right and interval censored data. Several additional topics are discussed, including noninformative and informative prior specificiations, computing posterior qualities of interest, Bayesian hypothesis testing, variable selection, model selection with nonnested models, model checking techniques using Bayesian diagnostic methods, and Markov chain Monte Carlo (MCMC) algorithms for sampling from the posteiror and predictive distributions. The book presents a balance between theory and applications, and for each class of models discussed, detailed examples and analyses from case studies are presented whenever possible. The applications are all essentially from the health sciences, including cancer, AIDS, and the environment. The book is intended as a graduate textbook or a reference book for a one semester course at the advanced masters or Ph. D. level. This book would be most suitable for second or third year graduate students in statistics or biostatistics. It would also serve as a useful reference book for applied or theoretical researchers as well as practitioners. Introduction -- Parametric Models -- Semiparametric Models -- Fraility Models -- Cure Rate Models -- Model Comparison -- Joint Models for Longitudinal and Survival Data -- Missing Covariate Data -- Design and Monitoring of Randomized Clinical Trials -- Other Topics.},
address = {New York},
author = {Ibrahim, Joseph G. and Chen, Ming-Hui. and Sinha, Debajyoti.},
isbn = {9781441929334},
pages = {481},
publisher = {Springer New York},
title = {{Bayesian Survival Analysis}},
year = {2001}
}
@article{Robert1995,
author = {Robert, Christian P.},
doi = {10.1214/ss/1177009937},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Gibbs sampling,Metropolis algorithm,Rao-Blackwellization,asymptotic variance,central limit theorem,duality principle,ergodic theorem,finite state Markov chains,importance sampling,missing data,renewal theory,trapezoidal integration},
month = {aug},
number = {3},
pages = {231--253},
publisher = {Institute of Mathematical Statistics},
title = {{Convergence Control Methods for Markov Chain Monte Carlo Algorithms}},
url = {http://projecteuclid.org/euclid.ss/1177009937},
volume = {10},
year = {1995}
}
@article{Scheffe1936,
author = {Scheffe, Henry},
doi = {10.2307/1989666},
issn = {00029947},
journal = {Transactions of the American Mathematical Society},
month = {jul},
number = {1},
pages = {127},
title = {{Asymptotic Solutions of Certain Linear Differential Equations in which the Coefficient of the Parameter May Have a Zero}},
url = {https://www.jstor.org/stable/1989666?origin=crossref},
volume = {40},
year = {1936}
}
@misc{Blei2017,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this article, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find a member of that family which is close to the target density. Closeness is measured by Kullback–Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this article is to catalyze statistical research on this class of algorithms. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1601.00670},
author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
booktitle = {Journal of the American Statistical Association},
doi = {10.1080/01621459.2017.1285773},
eprint = {1601.00670},
issn = {1537274X},
keywords = {Algorithms,Computationally intensive methods,Statistical computing},
month = {apr},
number = {518},
pages = {859--877},
publisher = {American Statistical Association},
title = {{Variational Inference: A Review for Statisticians}},
volume = {112},
year = {2017}
}
@book{Bernardo1994,
abstract = {This highly acclaimed text, now available in paperback, provides a thorough account of key concepts and theoretical results, with particular emphasis on viewing statistical inference as a special case of decision theory. Information-theoretic concepts play a central role in the development of the theory, which provides, in particular, a detailed discussion of the problem of specification of so-called prior ignorance . The work is written from the authors s committed Bayesian perspective, but an overview of non-Bayesian theories is also provided, and each chapter contains a wide-ranging critical re-examination of controversial issues. The level of mathematics used is such that most material is accessible to readers with knowledge of advanced calculus. In particular, no knowledge of abstract measure theory is assumed, and the emphasis throughout is on statistical concepts rather than rigorous mathematics. The book will be an ideal source for all students and researchers in statistics, mathematics, decision analysis, economic and business studies, and all branches of science and engineering, who wish to further their understanding of Bayesian statistics.},
address = {New York},
author = {Bernardo, Jos{\'{e}} M. and Smith, Adrian F.M.},
booktitle = {Bayesian Theory},
doi = {10.1002/9780470316870},
editor = {Bernardo, Jos M. and Smith, Adrian F. M.},
isbn = {9780470316870},
month = {may},
pages = {1--595},
publisher = {Wiley Blackwell},
series = {Wiley Series in Probability and Statistics},
title = {{Bayesian Theory}},
url = {http://doi.wiley.com/10.1002/9780470316870},
year = {1994}
}
@article{Lunn2009,
abstract = {BUGS is a software package for Bayesian inference using Gibbs sampling. The software has been instrumental in raising awareness of Bayesian modelling among both academic and commercial communities internationally, and has enjoyed considerable success over its 20-year life span. Despite this, the software has a number of shortcomings and a principal aim of this paper is to provide a balanced critical appraisal, in particular highlighting how various ideas have led to unprecedented flexibility while at the same time producing negative side effects. We also present a historical overview of the BUGS project and some future perspectives.},
author = {Lunn, David and Spiegelhalter, David and Thomas, Andrew and Best, Nicky},
doi = {10.1002/sim.3680},
issn = {02776715},
journal = {Statistics in Medicine},
month = {nov},
number = {25},
pages = {3049--3067},
pmid = {19630097},
title = {{The BUGS project: Evolution, critique and future directions}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19630097 http://doi.wiley.com/10.1002/sim.3680},
volume = {28},
year = {2009}
}
@article{Chen2020,
abstract = {Aims: Studies have indicated that chloroquine (CQ) shows antagonism against COVID-19 in vitro. However, evidence regarding its effects in patients is limited. This study aims to evaluate the efficacy of hydroxychloroquine (HCQ) in the treatment of patients with COVID-19. Main methods: From February 4 to February 28, 2020, 62 patients suffering from COVID-19 were diagnosed and admitted to Renmin Hospital of Wuhan University. All participants were randomized in a parallel-group trial, 31 patients were assigned to receive an additional 5-day HCQ (400 mg/d) treatment, Time to clinical recovery (TTCR), clinical characteristics, and radiological results were assessed at baseline and 5 days after treatment to evaluate the effect of HCQ. Key findings: For the 62 COVID-19 patients, 46.8{\%} (29 of 62) were male and 53.2{\%} (33 of 62) were female, the mean age was 44.7 (15.3) years. No difference in the age and sex distribution between the control group and the HCQ group. But for TTCR, the body temperature recovery time and the cough remission time were significantly shortened in the HCQ treatment group. Besides, a larger proportion of patients with improved pneumonia in the HCQ treatment group (80.6{\%}, 25 of 32) compared with the control group (54.8{\%}, 17 of 32). Notably, all 4 patients progressed to severe illness that occurred in the control group. However, there were 2 patients with mild adverse reactions in the HCQ treatment group. Significance: Among patients with COVID-19, the use of HCQ could significantly shorten TTCR and promote the absorption of pneumonia. {\#}{\#}{\#} Competing Interest Statement The authors have declared no competing interest. {\#}{\#}{\#} Clinical Trial ChiCTR2000029559 {\#}{\#}{\#} Funding Statement This study was supported by the Epidemiological Study of COVID-19 Pneumonia to Science and Technology Department of Hubei Province (2020FCA005). {\#}{\#}{\#} Author Declarations All relevant ethical guidelines have been followed; any necessary IRB and/or ethics committee approvals have been obtained and details of the IRB/oversight body are included in the manuscript. Yes All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived. Yes I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance). Yes I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable. Yes The dataset supporting the conclusions of this article is included within the article.},
author = {Chen, Zhaowei and Hu, Jijia and Zhang, Zongwei and Jiang, Shan and Han, Shoumeng and Yan, Dandan and Zhuang, Ruhong and Hu, Ben and Zhang, Zhan},
doi = {10.1101/2020.03.22.20040758},
journal = {medRxiv},
month = {apr},
publisher = {Cold Spring Harbor Laboratory Press},
title = {{Efficacy of hydroxychloroquine in patients with COVID-19: results of a randomized clinical trial}},
volume = {7},
year = {2020}
}
@book{Chow2008,
address = {Boca Raton},
author = {Chow, Shein-Chung and Liu, Jen-Pei},
edition = {3rd},
publisher = {Chapman {\&} Hall/CRC Press},
title = {{Design and Analysis of Bioavailability and Bioequivalence Studies}},
year = {2008}
}
@article{brmsPackage,
author = {B{\"{u}}rkner, Paul-Christian},
journal = {R package version 2.13.0},
title = {{brms: Bayesian Regression Models using 'Stan'}},
year = {2020}
}
@article{Lenhard2006,
author = {Lenhard, Johannes},
doi = {10.1093/bjps/axi152},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Lenhard - 2006 - Models and Statistical Inference The Controversy between Fisher and Neyman–Pearson.pdf:pdf},
issn = {1464-3537},
journal = {The British Journal for the Philosophy of Science},
number = {1},
pages = {69--91},
publisher = {Oxford University Press},
title = {{Models and Statistical Inference: The Controversy between Fisher and Neyman–Pearson}},
volume = {57},
year = {2006}
}
@article{FDAInteractingComplexInnovativeeTrialDesigns,
author = {{U.S. Food and Drug Administration Center for Drug Evaluation and Research and Center for Biologics Evaluation and Research}},
journal = {https://www.fda.gov/media/130897/download (accessed 01/08/2021)},
title = {{Interacting with the FDA on Complex Innovative Trial Designs for Drugs and Biological Products: Guidance for Industry}},
year = {2020}
}
@article{Good1977,
author = {Good, I. J.},
journal = {Proceedings of the Royal Society of London Series A},
pages = {303--330},
title = {{Explicativity: A Mathematical Theory of Explanation with Statistical Applications}},
url = {https://www.jstor.org/stable/79232?refreqid=excelsior{\%}3Af7a15b242c91ba078a5fbeb5ee996097},
volume = {354},
year = {1977}
}
@article{Anscombe1963,
author = {Anscombe, F.J.},
doi = {10.1080/00401706.1976.10489459},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Anscombe - 1963 - Sequential Medical Trials.pdf:pdf},
issn = {15372723},
journal = {Journal of the American Statistical Association},
pages = {365--383},
title = {{Sequential Medical Trials}},
volume = {58},
year = {1963}
}
@article{Fisher1922MathematicalFoundationsOfTheoreticalStatistics,
abstract = {Reproduced with permission of the Royal Society.},
author = {Fisher, Ronald Aylmer},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1922 - On the Mathematical Foundations of Theoretical Statistics(3).pdf:pdf},
journal = {Philosophical Transactions of the Royal Society},
number = {222},
pages = {309--368},
title = {{On the Mathematical Foundations of Theoretical Statistics.}},
volume = {A},
year = {1922}
}
@inproceedings{thomasson_identifying_2006,
abstract = {We report on a study of novice programmers' object oriented class designs. These designs were analysed to discover what faults they displayed. The two most common faults related to non-referenced classes (inability to integrate them into the solution), and problems with attributes and class cohesion. The paper ends with some implication for teaching that may be indicated by the empirical results.},
address = {New York, NY, USA},
author = {Thomasson, Benjy and Ratcliffe, Mark and Thomas, Lynda},
booktitle = {Proceedings of the 11th Annual SIGCSE Conference on Innovation and Technology in Computer Science Education},
doi = {10.1145/1140124.1140135},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Thomasson, Ratcliffe, Thomas - 2006 - Identifying Novice Difficulties in Object Oriented Design.pdf:pdf},
isbn = {978-1-59593-055-2},
keywords = {design,design faults,introductory programming,software design},
pages = {28--32},
publisher = {ACM},
series = {{\{}ITICSE{\}} '06},
title = {{Identifying Novice Difficulties in Object Oriented Design}},
url = {http://doi.acm.org/10.1145/1140124.1140135},
year = {2006}
}
@incollection{Cook1980,
author = {Cook, R. Dennis},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_18},
pages = {182--191},
publisher = {Springer, New York, NY},
title = {{Smoking and Lung Cancer}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}18},
year = {1980}
}
@article{Dienes2014,
abstract = {No scientific conclusion follows automatically from a statistically non-significant result, yet people routinely use non-significant results to guide conclusions about the status of theories (or the effectiveness of practices). To know whether a non-significant result counts against a theory, or if it just indicates data insensitivity, researchers must use one of: power, intervals (such as confidence or credibility intervals), or else an indicator of the relative evidence for one theory over another, such as a Bayes factor. I argue Bayes factors allow theory to be linked to data in a way that overcomes the weaknesses of the other approaches. Specifically, Bayes factors use the data themselves to determine their sensitivity in distinguishing theories (unlike power), and they make use of those aspects of a theory's predictions that are often easiest to specify (unlike power and intervals, which require specifying the minimal interesting value in order to address theory). Bayes factors provide a coherent approach to determining whether non-significant results support a null hypothesis over a theory, or whether the data are just insensitive. They allow accepting and rejecting the null hypothesis to be put on an equal footing. Concrete examples are provided to indicate the range of application of a simple online Bayes calculator, which reveal both the strengths and weaknesses of Bayes factors.},
author = {Dienes, Zoltan},
doi = {10.3389/fpsyg.2014.00781},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Dienes - 2014 - Using Bayes to get the most out of non-significant results.pdf:pdf},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {Bayes factor,Significance testing,confidence interval,credibility interval,statistical inference},
pages = {781},
publisher = {Frontiers Media SA},
title = {{Using Bayes to get the most out of non-significant results}},
volume = {5},
year = {2014}
}
@article{Borges2007,
author = {Borges, Wagner and Stern, J. M.},
doi = {10.1093/jigpal/jzm032},
issn = {1367-0751},
journal = {Logic Journal of the IGPL},
number = {5-6},
pages = {401--420},
publisher = {Oxford Academic},
title = {{The Rules of Logic Composition for the Bayesian Epistemic e-Values}},
volume = {15},
year = {2007}
}
@article{Colvin2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Colvin, Kimberly F.},
doi = {10.1111/cobi.12805},
eprint = {arXiv:1011.1669v3},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Colvin - 2013 - Book Review - Kruschke, J. K. (2011). Doing Bayesian Data Analysis A Tutorial with R and BUGS.pdf:pdf},
isbn = {9780987666048},
issn = {0092-8674},
journal = {Journal of Educational Measurement},
number = {4},
pages = {469--471},
pmid = {25246403},
title = {{Book Review - Kruschke, J. K. (2011). Doing Bayesian Data Analysis: A Tutorial with R and BUGS}},
volume = {50},
year = {2013}
}
@article{Thompson1990,
abstract = {R. A. Fisher (1890-1962) was a professor of genetics, and many of his statistical innovations found expression in the development of methodology in statistical genetics. However, whereas his contributions in mathematical statistics are easily identified, in population genetics he shares his preeminence with Sewall Wright (1889-1988) and J. B. S. Haldane (1892-1965). This paper traces some of Fisher's major contributions to the foundations of statistical genetics, and his interactions with Wright and with Haldane which contributed to the development of the subject. With modern technology, both statistical methodology and genetic data are changing. Nonetheless much of Fisher's work remains relevant, and may even serve as a foundation for future research in the statistical analysis of DNA data. For Fisher's work reflects his view of the role of statistics in scientific inference, expressed in 1949: There is no wide or urgent demand for people who will define methods of proof in set theory in the name of improving mathematical statistics. There is a widespread and urgent demand for mathematicians who understand that branch of mathematics known as theoretical statistics, but who are capable also of recognising situations in the real world to which such mathematics is applicable. In recognising features of the real world to which his models and analyses should be applicable, Fisher laid a lasting foundation for statistical inference in genetic analyses.},
author = {Thompson, E A},
issn = {0006-341X},
journal = {Biometrics},
month = {dec},
number = {4},
pages = {905--14},
pmid = {2085639},
title = {{R.A. Fisher's contributions to genetical statistics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/2085639},
volume = {46},
year = {1990}
}
@inproceedings{feinberg_visual_2007,
abstract = {This paper explores both the design and classroom usage of a visual and interactive programming environment. This environment, named JavaTown, provides a visual representation of the inner workings of object-oriented programs, in which on-screen characters are literally depicted as carrying messages to each other and remembering values. JavaTown was created to aid in the introduction of some of the most elusive concepts in the CS1/CS2 curriculum: object references, parameter passing, variable scope, recursion, and linked lists.},
address = {New York, NY, USA},
annote = {USA},
author = {Feinberg, Dave},
booktitle = {Proceedings of the 38th SIGCSE Technical Symposium on Computer Science Education},
doi = {10.1145/1227310.1227363},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Feinberg - 2007 - A Visual Object-oriented Programming Environment.pdf:pdf},
isbn = {978-1-59593-361-4},
keywords = {education,interactive,metaphor,object-oriented,visual},
pages = {140--144},
publisher = {ACM},
series = {{\{}SIGCSE{\}} '07},
title = {{A Visual Object-oriented Programming Environment}},
url = {http://doi.acm.org/10.1145/1227310.1227363},
year = {2007}
}
@article{Gronau2019c,
abstract = {We recently discussed several limitations of Bayesian leave-one-out cross-validation (LOO) for model selection. Our contribution attracted three thought-provoking commentaries. In this rejoinder, we address each of the commentaries and identify several additional limitations of LOO-based methods such as Bayesian stacking. We focus on differences between LOO-based methods versus approaches that consistently use Bayes' rule for both parameter estimation and model comparison. We conclude that LOO-based methods do not align satisfactorily with the epistemic goal of mathematical psychology. Keywords Bayesian stacking {\textperiodcentered} Bayes factor {\textperiodcentered} Bayesian model averaging {\textperiodcentered} Prequential approach {\textperiodcentered} M-open Bayesian leave-one-out cross-validation (LOO) is increasingly popular for the comparison and selection of quantitative models of cognition and behavior. 1 In a recent article for Computational Brain {\&} Behavior, we outlined several limitations of LOO (Gronau and Wagenmakers this issue). Specifically, three concrete, simple examples illustrated that when a data set of infinite size is perfectly in line with the predictions of a simple model M S and LOO is used to compare M S to a more complex model M C , LOO shows bounded support for M S. As we mentioned, this model selection inconsistency has been known for a long time (e.g., Shao 1993). We also discussed limitations that were unexpected (at least to us). Concretely, for data perfectly 1 Throughout this article, we use the terms model comparison and model selection interchangeably, although it may be argued that there is a subtle difference. This research was supported by a Netherlands Organisation for Scientific Research (NWO) grant to QFG (406.16.528) and to EJW (016.Vici.170.083), as well as an Advanced ERC grant to EJW (743086 UNIFY). R code for reproducing the examples can be found on the OSF project page: https://osf.io/eydtg/ consistent with the simpler model M S , (1) the limiting bound of evidence for M S is often surprisingly modest; (2) the LOO preference for M S may be a nonmonotonic function of the number of observations (meaning that additional observations perfectly consistent with M S may in fact decrease the LOO preference for M S); and (3) contrary to popular belief, the LOO result can depend strongly on the parameter prior distribution, even asymptotically. Our discussion of the limitations of LOO attracted three commentaries. In the first commentary, Vehtari et al. (this issue) claim that we "focus on pathologizing a known and essentially unimportant property of the method; and they fail to discuss the most common issues that arise when using LOO for a real statistical analysis." Furthermore, Vehtari et al. state that we used a version of LOO that is not best practice and they suggest to use LOO-based Bayesian stacking instead (Yao et al. 2018). Vehtari et al. also criticize us for making the assumption that one of the models under consideration is "true" and use this as a springboard to question the usefulness of Bayes factors (e.g., Jeffreys 1961; Kass and Raftery 1995) and Bayesian model averaging (BMA; e.g., Hoeting et al. 1999; Jevons 1874/1913). Finally, Vehtari et al. point out what they believe are more serious limitations of LOO-based methods. The second commentary is by Navarro (this issue) and discusses how the scientific goal of explanation aligns with traditional statistical concerns; Navarro suggests that the model selection literature may focus too heavily on the statistical issues of model choice and too little on the scientific questions of interest. In the third commentary,},
author = {Gronau, Quentin F. and Wagenmakers, Eric-Jan},
doi = {10.1007/s42113-018-0022-4},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gronau, Wagenmakers - 2019 - Rejoinder More Limitations of Bayesian Leave-One-Out Cross-Validation.pdf:pdf},
issn = {2522-0861},
journal = {Computational Brain {\&} Behavior},
keywords = {Bayes factor,Bayesian model averaging,Bayesian stacking,M-open,Prequential approach},
month = {mar},
number = {1},
pages = {35--47},
publisher = {Springer Science and Business Media LLC},
title = {{Rejoinder: More Limitations of Bayesian Leave-One-Out Cross-Validation}},
url = {https://doi.org/10.1007/s42113-018-0022-4},
volume = {2},
year = {2019}
}
@article{BAS2020,
author = {Clyde, Merlise},
journal = {R software package},
number = {version 1.5.5},
publisher = {R package version 1.5.5},
title = {{BAS: Bayesian Variable Selection and Model Averaging using Bayesian Adaptive Sampling}},
url = {https://cran.r-project.org/web/packages/BAS/BAS.pdf},
year = {2020}
}
@book{laplace_1812,
author = {Laplace, Pierre-Simon de (1749-1827)},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Laplace - 1812 - Th{\'{e}}orie Analytique des Probabilit{\'{e}}s.pdf:pdf},
publisher = {Courcier},
title = {{Th{\'{e}}orie Analytique des Probabilit{\'{e}}s}},
year = {1812}
}
@article{Fisher1924,
author = {Fisher, Ronald Aylmer},
journal = {Journal of the Royal Statistical Society},
number = {3},
pages = {442--450},
title = {{The Conditions Under which {\$}\backslashchi{\^{}}2{\$} Measures the Discrepancey Between Observations and Hypothesis}},
volume = {87},
year = {1924}
}
@book{Neal1996,
abstract = {Two features distinguish the Bayesian approach to learning models from data. First, beliefs derived from background knowledge are used to select a prior probability distribution for the model parameters. Second, predictions of future observations are made by integrating the model's predictions with respect to the posterior parameter distribution obtained by updating this prior to take account of the data. For neural network models, both these aspects present diiculties | the prior over network parameters has no obvious relation to our prior knowledge, and integration over the posterior is computationally very demanding. I address the problem by deening classes of prior distributions for network param-eters that reach sensible limits as the size of the network goes to innnity. In this limit, the properties of these priors can be elucidated. Some priors converge to Gaussian processes, in which functions computed by the network may be smooth, Brownian, or fractionally Brownian. Other priors converge to non-Gaussian stable processes. Interesting eeects are obtained by combining priors of both sorts in networks with more than one hidden layer.},
address = {New York},
author = {Neal, Radford M.},
doi = {10.1007/978-1-4612-0745-0},
isbn = {978-0-387-94724-2},
publisher = {Springer Verlag New York},
title = {{Bayesian Learning for Neural Networks}},
url = {http://link.springer.com/10.1007/978-1-4612-0745-0},
year = {1996}
}
@article{Fraser1972,
author = {Fraser, D.A.S.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fraser - 1972 - Bayes, Likelihood of Structural.pdf:pdf},
journal = {The Annals of Mathematical Statistics},
number = {4},
pages = {777--790},
title = {{Bayes, Likelihood of Structural}},
volume = {43},
year = {1972}
}
@article{Stigler2005,
abstract = {Ronald A. Fisher's 1921 article on mathematical statistics (submitted and read in 1921; published in 1922) was arguably the most influential article on that subject in the twentieth century, yet up to that time Fisher was primarily occupied with other pursuits. A number of previously published documents are examined in a new light to argue that the origin of that work owes a considerable (and unacknowledged) debt to a challenge issued in 1916 by Karl Pearson.},
author = {Stigler, Stephen},
doi = {10.1214/088342305000000025},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Stigler - 2005 - Fisher in 1921.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {a,and phrases,fisher,history of statistics,karl pearson,kirstine smith,maxi-,minimum chi square,mum likelihood,r,sufficiency},
number = {1},
pages = {32--49},
title = {{Fisher in 1921}},
url = {http://www.jstor.org/stable/20061159{\%}5Cnpapers2://publication/uuid/41499EB3-3540-444A-8708-8436839ABF8D},
volume = {20},
year = {2005}
}
@article{Meehl1967,
abstract = {Because physical theories typically predict numerical values, an improvement in experimental precision reduces the tolerance range and hence increases corroborability. In most psychological researc...},
author = {Meehl, Paul E.},
journal = {Philosophy of Science},
number = {2},
pages = {103--115},
title = {{Theory testing in psychology and physics: A methodological paradox}},
volume = {34},
year = {1967}
}
@article{KassRaftery1995,
author = {Kass, Robert E and Raftery, Adrian E},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kass, Raftery - 1995 - Bayes factors.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {430},
pages = {773--795},
title = {{Bayes factors}},
volume = {90},
year = {1995}
}
@article{Rouder2012,
abstract = {Bayes factors have been advocated as superior to p-values for assessing statistical evidence in data. Despite the advantages of Bayes factors and the drawbacks of p-values, inference by p-values is still nearly ubiquitous. One impediment to the adoption of Bayes factors is a lack of practical development, particularly a lack of ready-to-use formulas and algorithms. In this paper, we discuss and expand a set of default Bayes factor tests for ANOVA designs. These tests are based on multivariate generalizations of Cauchy priors on standardized effects, and have the desirable properties of being invariant with respect to linear transformations of measurement units. Moreover, these Bayes factors are computationally convenient, and straightforward sampling algorithms are provided. We cover models with fixed, random, and mixed effects, including random interactions, and do so for within-subject, between-subject, and mixed designs. We extend the discussion to regression models with continuous covariates. We also discuss how these Bayes factors may be applied in nonlinear settings, and show how they are useful in differentiating between the power law and the exponential law of skill acquisition. In sum, the current development makes the computation of Bayes factors straightforward for the vast majority of designs in experimental psychology. {\textcopyright} 2012 Elsevier Inc.},
author = {Rouder, Jeffrey N. and Morey, Richard D. and Speckman, Paul L. and Province, Jordan M.},
doi = {10.1016/j.jmp.2012.08.001},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Rouder et al. - 2012 - Default Bayes factors for ANOVA designs.pdf:pdf},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
keywords = {Bayes factor,Bayesian statistics,Linear models,Model selection},
number = {5},
pages = {356--374},
publisher = {Elsevier Inc.},
title = {{Default Bayes factors for ANOVA designs}},
url = {http://dx.doi.org/10.1016/j.jmp.2012.08.001},
volume = {56},
year = {2012}
}
@article{Teacher2017,
author = {Teacher, The Mathematics},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Teacher - 2017 - STATISTICAL.pdf:pdf},
number = {8},
pages = {606--611},
title = {{STATISTICAL}},
volume = {109},
year = {2017}
}
@book{silberbauer_einstieg_2008,
address = {Berlin, Heidelberg},
annote = {DOI: 10.1007/978-3-540-78618-4},
author = {Silberbauer, Christian},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Silberbauer - 2008 - Einstieg in Java und OOP.pdf:pdf},
isbn = {978-3-540-78615-3 978-3-540-78618-4},
publisher = {Springer Berlin Heidelberg},
series = {{\{}eXamen{\}}.press},
title = {{Einstieg in Java und OOP}},
url = {http://link.springer.com/10.1007/978-3-540-78618-4},
year = {2008}
}
@article{SinghXie2011Fraser,
author = {Singh, Kesar and Xie, Minge},
journal = {Statistical Science},
number = {3},
pages = {319--321},
title = {{Discussion of ``Is Bayes Posterior just Quick and Dirty Confidence?''}},
volume = {26},
year = {2011}
}
@article{Fritz2012,
abstract = {The Publication Manual of the American Psychological Association (American Psychological Association, 2001, American Psychological Association, 2010) calls for the reporting of effect sizes and their confidence intervals. Estimates of effect size are useful for determining the practical or theoretical importance of an effect, the relative contributions of factors, and the power of an analysis. We surveyed articles published in 2009 and 2010 in the Journal of Experimental Psychology: General, noting the statistical analyses reported and the associated reporting of effect size estimates. Effect sizes were reported for fewer than half of the analyses; no article reported a confidence interval for an effect size. The most often reported analysis was analysis of variance, and almost half of these reports were not accompanied by effect sizes. Partial $\eta$2 was the most commonly reported effect size estimate for analysis of variance. For t tests, 2/3 of the articles did not report an associated effect size estimate; Cohen's d was the most often reported. We provide a straightforward guide to understanding, selecting, calculating, and interpreting effect sizes for many types of data and to methods for calculating effect size confidence intervals and power analysis.},
author = {Fritz, Catherine O. and Morris, Peter E. and Richler, Jennifer J.},
doi = {10.1037/a0024338},
isbn = {1939-2222; 0022-1015},
issn = {00963445},
journal = {Journal of Experimental Psychology: General},
keywords = {Confidence intervals,Effect size,Eta squared,Statistical interpretation,Statistical reporting},
month = {feb},
number = {1},
pages = {2--18},
pmid = {21823805},
title = {{Effect size estimates: Current use, calculations, and interpretation}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21823805 http://doi.apa.org/getdoi.cfm?doi=10.1037/a0024338},
volume = {141},
year = {2012}
}
@incollection{Picard1980,
author = {Holschuh, Norton},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_5},
pages = {35--45},
publisher = {Springer, New York, NY},
title = {{Randomization and Design: I}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}6 http://link.springer.com/10.1007/978-1-4612-6079-0{\_}5},
year = {1980}
}
@article{Spencer2017,
abstract = {AbstractWhen undertaking quantitative hypothesis testing, social researchers need to decide whether the data with which they are working is suitable for parametric analyses to be used. When conside...},
author = {Spencer, Neil H. and Lay, Margaret and {Kevan de Lopez}, Lindsey},
doi = {10.1080/13645579.2016.1155379},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Spencer, Lay, Kevan de Lopez - 2017 - Normal enough Tools to aid decision making.pdf:pdf},
issn = {1364-5579},
journal = {International Journal of Social Research Methodology},
keywords = {Quantitative data analysis,assumptions,hypothesis testing,normality,robustness},
month = {mar},
number = {2},
pages = {167--179},
publisher = {Routledge},
title = {{Normal enough? Tools to aid decision making}},
url = {https://www.tandfonline.com/doi/full/10.1080/13645579.2016.1155379},
volume = {20},
year = {2017}
}
@article{Castura2010,
author = {Castura, John C.},
doi = {10.1016/j.foodqual.2009.12.002},
issn = {09503293},
journal = {Food Quality and Preference},
number = {3},
pages = {257--258},
publisher = {Elsevier Ltd},
title = {{Equivalence testing: A brief review}},
volume = {21},
year = {2010}
}
@article{Pocock1987,
abstract = {Reports of clinical trials often contain a wealth of data comparing treatments. This can lead to problems in interpretation, particularly when significance testing is used extensively. We examined 45 reports of comparative trials published in the British Medical Journal, the Lancet, or the New England Journal of Medicine to illustrate these statistical problems. The issues we considered included the analysis of multiple end points, the analysis of repeated measurements over time, subgroup analyses, trials of multiple treatments, and the overall number of significance tests in a trial report. Interpretation of large amounts of data is complicated by the common failure to specify in advance the intended size of a trial or statistical stopping rules for interim analyses. In addition, summaries or abstracts of trials tend to emphasize the more statistically significant end points. Overall, the reporting of clinical trials appears to be biased toward an exaggeration of treatment differences. Trials should have a clearer predefined policy for data analysis and reporting. In particular, a limited number of primary treatment comparisons should be specified in advance. The overuse of arbitrary significance levels (for example, P less than 0.05) is detrimental to good scientific reporting, and more emphasis should be given to the magnitude of treatment differences and to estimation methods such as confidence intervals.},
author = {Pocock, Stuart J. and Hughes, Michael D. and Lee, Robert J.},
doi = {10.1056/NEJM198708133170706},
issn = {15334406},
journal = {New England Journal of Medicine},
month = {aug},
number = {7},
pages = {426--432},
pmid = {3614286},
title = {{Statistical Problems in the Reporting of Clinical Trials}},
volume = {317},
year = {1987}
}
@article{Glick1992,
abstract = {Approximately 10?20{\%} of all research and development (R{\&}D) funds are estimated to be spent on questionable studies, which are characterized by misrepresentation of data, inaccurate reporting, and fabrication of experimental results. Routine data audits are capable of decreasing the frequency of questionable R{\&}D studies by half, resulting in a savings of at least {\$}5 to {\$}10 for every audit dollar spent. The primary value of data audit to research is to verify the validity of experimental procedures that lead to positive results, whereas the primary value of data audit to development is to identify problem areas in experimental protocols and procedures. Data auditing is also useful in aiding managers to evaluate and select R{\&}D projects. A regularly scheduled data auditing program increases the reliability of project planning and helps to improve organizational performance in the completion of projects meeting functional expectations on time and within budget. The benefits of data audit to organizations conducting R{\&}D range from the highly focused?reduction of errors and irregularities, to the big picture?enhanced effectiveness in strategic planning and implementation of the plan.$\backslash$nApproximately 10?20{\%} of all research and development (R{\&}D) funds are estimated to be spent on questionable studies, which are characterized by misrepresentation of data, inaccurate reporting, and fabrication of experimental results. Routine data audits are capable of decreasing the frequency of questionable R{\&}D studies by half, resulting in a savings of at least {\$}5 to {\$}10 for every audit dollar spent. The primary value of data audit to research is to verify the validity of experimental procedures that lead to positive results, whereas the primary value of data audit to development is to identify problem areas in experimental protocols and procedures. Data auditing is also useful in aiding managers to evaluate and select R{\&}D projects. A regularly scheduled data auditing program increases the reliability of project planning and helps to improve organizational performance in the completion of projects meeting functional expectations on time and within budget. The benefits of data audit to organizations conducting R{\&}D range from the highly focused?reduction of errors and irregularities, to the big picture?enhanced effectiveness in strategic planning and implementation of the plan.},
author = {Glick, J. Leslie},
doi = {10.1080/08989629208573811},
issn = {15455815},
journal = {Accountability in Research},
keywords = {data audit,research audit,scientific audit},
month = {jan},
number = {3},
pages = {153--168},
publisher = {Taylor {\&} Francis Group},
title = {{Scientific Data Audit—A Key Management Tool}},
url = {https://www.tandfonline.com/doi/full/10.1080/08989629208573811},
volume = {2},
year = {1992}
}
@article{green_how_1991,
abstract = {Numerous rules-of-thumb have been suggested for determining the minimum number of subjects required to conduct multiple regression analyses. These rules-of-thumb are evaluated by comparing their results against those based on power analyses for tests of hypotheses of multiple and partial correlations. The results did not support the use of rules-of-thumb that simply specify some constant (e.g., 100 subjects) as the minimum number of subjects or a minimum ratio of number of subjects (N) to number of predictors (m). Some support was obtained for a rule-of-thumb that N ≥ 50 + 8 m for the multiple correlation and N ≥104 + m for the partial correlation. However, the rule-of-thumb for the multiple correlation yields values too large for N when m ≥ 7, and both rules-of-thumb assume all studies have a medium-size relationship between criterion and predictors. Accordingly, a slightly more complex rule-of thumb is introduced that estimates minimum sample size as function of effect size as well as the number of predictors. It is argued that researchers should use methods to determine sample size that incorporate effect size.},
author = {Green, S B},
doi = {10.1207/s15327906mbr2603_7},
issn = {0027-3171},
journal = {Multivariate Behavioral Research},
month = {jul},
number = {3},
pages = {499--510},
pmid = {26776715},
title = {{How Many Subjects Does It Take To Do A Regression Analysis}},
volume = {26},
year = {1991}
}
@book{VonMises1931,
address = {Berlin},
author = {{Von Mises}, Richard},
publisher = {Springer-Verlag Berlin},
title = {{Wahrscheinlichkeitsrechnung}},
year = {1931}
}
@article{Browne2000,
abstract = {This paper gives a review of cross-validation methods. The original applications in multiple linear regression are considered first. It is shown how predictive accuracy depends on sample size and the number of predictor variables. Both two-sample and single-sample cross-validation indices are investigated. The application of cross-validation methods to the analysis of moment structures is then justified. An equivalence of a single-sample cross-validation index and the Akaike information criterion is pointed out. It is seen that the optimal number of parameters suggested by both single-sample and two-sample cross-validation indices will depend on sample size. {\textcopyright} 2000 Academic Press.},
author = {Browne, Michael W.},
doi = {10.1006/jmps.1999.1279},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
month = {mar},
number = {1},
pages = {108--132},
publisher = {Academic Press},
title = {{Cross-validation methods}},
volume = {44},
year = {2000}
}
@misc{Robert2015,
abstract = {This short note is a self-contained and basic introduction to the Metropolis-Hastings algorithm, this ubiquitous tool used for producing dependent simulations from an arbitrary distribution. The document illustrates the principles of the methodology on simple examples with R codes and provides references to the recent extensions of the method.},
archivePrefix = {arXiv},
arxivId = {1504.01896},
author = {Robert, Christian P.},
booktitle = {WileyStatsRef: Statistics Reference Online},
eprint = {1504.01896},
keywords = {and phrases,bayesian inference,gibbs sampler,hamiltonian monte carlo,hastings algorithm,intractable density,langevin diffusion,markov chains,mcmc meth-,metropolis,ods},
title = {{The Metropolis-Hastings algorithm}},
year = {2015}
}
@article{Barnard1949,
author = {{Barnard G.A.}},
journal = {Journal of the Royal Stastical Society Series B (Methodological)},
pages = {115--139},
title = {{Statistical inference (with Discussion)}},
volume = {11},
year = {1949}
}
@book{moosbrugger_testtheorie_2012,
address = {Berlin Heidelberg},
annote = {Tests und Frageb{\"{o}}gen konstruieren, beurteilen und verstehen sowie Daten analysieren - diese Kompetenzen geh{\"{o}}ren zum Handwerkszeug der Psychologie, aber auch der Sozial- oder Wirtschaftswissenschaften. Die Autoren vermitteln das im Bachelor-Studium erforderliche Grundlagenwissen sowie vertiefende Aspekte f{\"{u}}r den Master-Studiengang - mit Merks{\"{a}}tzen, Zusammenfassungen und Anwendungsbeispielen. Die 2., {\"{u}}berarbeitete Auflage wurde u. a. um Hinweise zur Nutzung von Anwendungssoftware, um Beispieldatens{\"{a}}tze sowie Musteranalysen zum Download erg{\"{a}}nzt Tests und Frageb{\"{o}}gen konstruieren, beurteilen und verstehen sowie Daten analysieren - diese Kompetenzen geh{\"{o}}ren zum Handwerkszeug der Psychologie, aber auch der Sozial- oder Wirtschaftswissenschaften. Die Autoren vermitteln das im Bachelor-Studium erforderliche Grundlagenwissen sowie vertiefende Aspekte f{\"{u}}r den Master-Studiengang - mit Merks{\"{a}}tzen, Zusammenfassungen und Anwendungsbeispielen. Die 2., {\"{u}}berarbeitete Auflage wurde u. a. um Hinweise zur Nutzung von Anwendungssoftware, um Beispieldatens{\"{a}}tze sowie Musteranalysen zum Download erg{\"{a}}nzt
OCLC: 844892841},
edition = {2., aktual},
editor = {Moosbrugger, Helfried and Kelava, Augustin},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2012 - Testtheorie und Fragebogenkonstruktion.pdf:pdf},
isbn = {978-3-642-20071-7 978-3-642-20072-4},
keywords = {Fragebogen,Lehrbuch,Psychological tests,Psychological tests Standards,Psychometrics,Questionnaires,Testkonstruktion,Testtheorie},
publisher = {Springer},
series = {Springer-{\{}Lehrbuch{\}}},
shorttitle = {Testtheorie und {\{}Fragebogenkonstruktion{\}}},
title = {{Testtheorie und Fragebogenkonstruktion}},
year = {2012}
}
@article{Sawilowsky2016,
author = {Sawilowsky, Shlomo},
doi = {10.22237/jmasm/1478001720},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Sawilowsky - 2016 - Rao-Lovric and the Triwizard Point Null Hypothesis Tournament.pdf:pdf},
isbn = {1478001720},
journal = {Journal of Modern Applied Statistical Methods},
number = {2},
pages = {11--12},
title = {{Rao-Lovric and the Triwizard Point Null Hypothesis Tournament}},
url = {http://digitalcommons.wayne.edu/jmasmhttp://digitalcommons.wayne.edu/jmasm/vol15/iss2/4},
volume = {15},
year = {2016}
}
@article{EMAStatisticalPrinciplesForClinicalTrials,
author = {{European Medicines Agency}},
journal = {https://www.ema.europa.eu/en/documents/scientific-guideline/ich-e-9-statistical-principles-clinical-trials-step-5{\_}en.pdf (accessed 01/08/2021)},
title = {{ICH Topic E9 Statistical Principles for Clinical Trials}},
year = {1998}
}
@article{Goddard2016,
author = {Goddard, Scott D. and Johnson, Valen E.},
doi = {10.1111/sjos.12235},
issn = {03036898},
journal = {Scandinavian Journal of Statistics},
keywords = {ANOVA,Bayesian testing,UMPBT,g prior,linear models,t‐test},
month = {dec},
number = {4},
pages = {1162--1177},
publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
title = {{Restricted most powerful Bayesian tests for linear models}},
url = {http://doi.wiley.com/10.1111/sjos.12235},
volume = {43},
year = {2016}
}
@book{Morant1939,
address = {Cambridge},
author = {Morant, G. M.},
publisher = {Cambridge University Press},
title = {{A Bibliography of the Statistical and Other Writings of Karl Pearson (issued by the Biometrika Office, University College London)}},
year = {1939}
}
@article{Ziliak2019,
author = {Ziliak, Stephen T.},
doi = {10.1080/00031305.2018.1514325},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ziliak - 2019 - How Large Are Your G-Values Try Gosset's Guinnessometrics When a Little p Is Not Enough.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {approach to uncertainty,balanced design,economic,external validity,fisher,s p},
number = {sup1},
pages = {281--290},
title = {{How Large Are Your G-Values? Try Gosset's Guinnessometrics When a Little " p" Is Not Enough}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1514325},
volume = {73},
year = {2019}
}
@article{VanErp2019,
abstract = {In linear regression problems with many predictors, penalized regression techniques are often used to guard against overfitting and to select variables relevant for predicting an outcome variable. Recently, Bayesian penalization is becoming increasingly popular in which the prior distribution performs a function similar to that of the penalty term in classical penalization. Specifically, the so-called shrinkage priors in Bayesian penalization aim to shrink small effects to zero while maintaining true large effects. Compared to classical penalization techniques, Bayesian penalization techniques perform similarly or sometimes even better, and they offer additional advantages such as readily available uncertainty estimates, automatic estimation of the penalty parameter, and more flexibility in terms of penalties that can be considered. However, many different shrinkage priors exist and the available, often quite technical, literature primarily focuses on presenting one shrinkage prior and often provides comparisons with only one or two other shrinkage priors. This can make it difficult for researchers to navigate through the many prior options and choose a shrinkage prior for the problem at hand. Therefore, the aim of this paper is to provide a comprehensive overview of the literature on Bayesian penalization. We provide a theoretical and conceptual comparison of nine different shrinkage priors and parametrize the priors, if possible, in terms of scale mixture of normal distributions to facilitate comparisons. We illustrate different characteristics and behaviors of the shrinkage priors and compare their performance in terms of prediction and variable selection in a simulation study. Additionally, we provide two empirical examples to illustrate the application of Bayesian penalization. Finally, an R package bayesreg is available online (https://github.com/sara-vanerp/bayesreg) which allows researchers to perform Bayesian penalized regression with novel shrinkage priors in an easy manner.},
author = {van Erp, Sara and Oberski, Daniel L. and Mulder, Joris},
doi = {10.1016/j.jmp.2018.12.004},
issn = {10960880},
journal = {Journal of Mathematical Psychology},
keywords = {Bayesian,Empirical Bayes,Penalization,Regression,Shrinkage priors},
pages = {31--50},
publisher = {Academic Press Inc.},
title = {{Shrinkage priors for Bayesian penalized regression}},
volume = {89},
year = {2019}
}
@article{Stigler2008b,
abstract = {some history about why p{\textless}5{\%} is often used to determine statistical$\backslash$nsignificance$\backslash$n$\backslash$nalso see "War, enmity..." (CHANCE)},
author = {Stigler, Stephen},
doi = {10.1007/s00144-008-0033-3},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Stigler - 2008 - Fisher and the 5{\%} level.pdf:pdf},
isbn = {0933-2480},
issn = {0933-2480},
journal = {CHANCE},
month = {dec},
number = {4},
pages = {12--12},
publisher = {Springer-Verlag},
title = {{Fisher and the 5{\%} level}},
url = {http://link.springer.com/10.1007/s00144-008-0033-3},
volume = {21},
year = {2008}
}
@article{Good1988,
author = {Good, I.J.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Good - 1988 - The Interface between Statistics and Philosophy of Science.pdf:pdf},
journal = {Statistical Science},
number = {4},
pages = {386--412},
title = {{The Interface between Statistics and Philosophy of Science}},
volume = {3},
year = {1988}
}
@article{Altman1982,
abstract = {The general standard of statistics in medical journals is poor. This paper considers the reasons for this with illustrations of the types of error that are common. The consequences of incorrect statistics in published papers are discussed; these involve scientific and ethical issues. Suggestions are made about ways in which the standard of statistics may be improved. Particular emphasis is given to the necessity for medical journals to have proper statistical refereeing of submitted papers.},
author = {Altman, Douglas G.},
doi = {10.1002/sim.4780010109},
issn = {10970258},
journal = {Statistics in Medicine},
keywords = {Ethics Statistical refereeing,Medical journals,Statistical errors,Statistical guidelines},
number = {1},
pages = {59--71},
pmid = {7187083},
title = {{Statistics in medical journals}},
volume = {1},
year = {1982}
}
@article{yadav_expanding_2017,
abstract = {The increased push for teaching computer science (CS) in schools in the United States requires training a large number of new K-12 teachers. The current efforts to increase the number of CS teachers have predominantly focused on training teachers from other content areas. In order to support these beginning CS teachers, we need to better understand their experiences and challenges encountered in the classroom. This study investigated U.S. CS teachers' perspectives on the demands of teaching computer science and support needed to ensure quality teaching. Results suggested that teachers face a number of challenges, including isolation, lack of adequate computer science background, and limited professional development resources.},
author = {Yadav, Aman and Gretter, Sarah and Hambrusch, Susanne and Sands, Phil},
doi = {10.1080/08993408.2016.1257418},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Yadav et al. - 2017 - Expanding computer science education in schools understanding teacher experiences and challenges.html:html},
issn = {0899-3408},
journal = {Computer Science Education},
keywords = {Computer science education,teacher preparation,teacher professional development},
month = {feb},
number = {4},
pages = {235--254},
shorttitle = {Expanding computer science education in schools},
title = {{Expanding computer science education in schools: understanding teacher experiences and challenges}},
url = {http://dx.doi.org/10.1080/08993408.2016.1257418},
volume = {26},
year = {2017}
}
@article{Hauck1984,
abstract = {The clinical problem of testing for equivalence in comparative bioavailability trials is restated in terms of the proper statistical hypotheses. A simple t-test procedure for these hypotheses has been devloped that is more powerful than the methods based on usual (shortest) and symmetric confidence intervals. In this note, this new procedure is explained and an example is given, including the method for sample size determination. {\textcopyright} 1984 Plenum Publishing Corporation.},
author = {Hauck, Walter W. and Anderson, Sharon},
doi = {10.1007/BF01063612},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hauck, Anderson - 1984 - A new statistical procedure for testing equivalence in two-group comparative bioavailability trials.pdf:pdf},
issn = {0090466X},
journal = {Journal of Pharmacokinetics and Biopharmaceutics},
keywords = {bioavailability,bioequivalence,hypothesis tests,sample size determination},
number = {1},
pages = {83--91},
pmid = {6747820},
publisher = {Kluwer Academic Publishers-Plenum Publishers},
title = {{A new statistical procedure for testing equivalence in two-group comparative bioavailability trials}},
volume = {12},
year = {1984}
}
@article{PEARSON1935,
abstract = {As the originator of the P, 2 test, I should bo glad if you can spare me space for some reply to Mr. Buchanan-Wollaston1. I should like first to state that I am in no way responsible for all the applications which have recently been made of that test, and do not accept the validity of some of the applications which Prof. R. A. Fisher has made of it in his well-known textbook. I am not concerned with his position and leave him to defend it. My own position is as follows:},
author = {Pearson, Karl},
doi = {10.1038/136296a0},
issn = {0028-0836},
journal = {Nature},
keywords = {Humanities and Social Sciences,Science,multidisciplinary},
month = {aug},
number = {3434},
pages = {296--297},
publisher = {Nature Publishing Group},
title = {{Statistical Tests}},
url = {http://www.nature.com/articles/136296a0},
volume = {136},
year = {1935}
}
@article{Cohen1962,
abstract = {One possible reason for the continued neglect of statistical power analysis in research in the behavioral sciences is the inaccessibility of or difficulty with the standard material. A convenient, although not comprehensive, presentation of required sample sizes is provided here. Effect-size indexes and conventional values for these are given for operationally defined small, medium, and large effects. The sample sizes necessary for .80 power to detect effects at these levels are tabled for eight standard statistical tests: (a) the difference between independent means, (b) the significance of a product-moment correlation, (c) the difference between independent rs, (d) the sign test, (e) the difference between independent proportions, (f) chi-square tests for goodness of fit and contingency tables, (g) one-way analysis of variance, and (h) the significance of a multiple or multiple partial correlation., (C) 1992 by the American Psychological Association},
author = {Cohen, Jacob},
doi = {10.1037/h0045186},
isbn = {0096-851X},
issn = {0096851X},
journal = {Journal of Abnormal and Social Psychology},
keywords = {IN SOCIAL PSYCHOLOGICAL RESEARCH,POWER OF,STATISTICS},
number = {3},
pages = {145--153},
pmid = {13880271},
title = {{The statistical power of abnormal-social psychological research: A review}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0045186},
volume = {65},
year = {1962}
}
@article{Stigler2012,
abstract = {Karl Pearson's role in the transformation that took the 19th century statistics of Laplace and Gauss into the modern era of 20th century multivariate analysis is examined from a new point of view. By viewing Pearson's work in the context of a motto he adopted from Charles Darwin, a philosophical theme is identified in Pearson's statistical work, and his three major achievements are briefly described. Some},
author = {Stigler, Stephen M.},
doi = {10.1093/biomet/asr046},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Stigler - 2012 - Studies in the history of probability and statistics, L Karl Pearson and the Rule of Three.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Charles Darwin,Francis Galton,History of statistics,Principal components,Ronald A. Fisher,Spurious correlation},
number = {1},
pages = {1--14},
title = {{Studies in the history of probability and statistics, L: Karl Pearson and the Rule of Three}},
volume = {99},
year = {2012}
}
@article{Wagenmakers2016,
abstract = {The practical advantages of Bayesian inference are demonstrated here through two concrete examples. In the first example, we wish to learn about a criminal's IQ: a problem of parameter estimation. In the second example, we wish to quantify and track support in favor of the null hypothesis that Adam Sandler movies are profitable regardless of their quality: a problem of hypothesis testing. The Bayesian approach unifies both problems within a coherent predictive framework, in which parameters and models that predict the data successfully receive a boost in plausibility, whereas parameters and models that predict poorly suffer a decline. Our examples demonstrate how Bayesian analyses can be more informative, more elegant, and more flexible than the orthodox methodology that remains dominant within the field of psychology.},
author = {Wagenmakers, Eric-Jan and Morey, Richard D. and Lee, Michael D.},
doi = {10.1177/0963721416643289},
issn = {0963-7214},
journal = {Current Directions in Psychological Science},
keywords = {Bayesian inference,hypothesis testing,parameter estimation,prediction,updating},
number = {3},
pages = {169--176},
publisher = {SAGE Publications Inc.},
title = {{Bayesian Benefits for the Pragmatic Researcher}},
volume = {25},
year = {2016}
}
@article{Martino2013,
abstract = {The multiple try Metropolis (MTM) method is a generalization of the classical Metropolis–Hastings algorithm in which the next state of the chain is chosen among a set of samples, according to normalized weights. In the literature, several extensions have been proposed. In this work, we show and remark upon the flexibility of the design of MTM-type methods, fulfilling the detailed balance condition. We discuss several possibilities, show different numerical simulations and discuss the implications of the results},
archivePrefix = {arXiv},
arxivId = {1201.0646},
author = {Martino, Luca and Read, Jesse},
doi = {10.1007/s00180-013-0429-2},
eprint = {1201.0646},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Martino, Read - 2013 - On the flexibility of the design of multiple try Metropolis schemes.pdf:pdf},
isbn = {0034916249192},
issn = {09434062},
journal = {Computational Statistics},
keywords = {MCMC techniques,Metropolis-Hasting method,Multi-point Metropolis algorithm,Multiple try Metropolis algorithm},
number = {6},
pages = {2797--2823},
title = {{On the flexibility of the design of multiple try Metropolis schemes}},
volume = {28},
year = {2013}
}
@article{Cox1958,
author = {Cox, D.R.},
doi = {10.1214/aoms/1177706618},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {2},
pages = {357--372},
pmid = {13602936},
publisher = {Institute of Mathematical Statistics},
title = {{Some problems connected with statistical inference}},
url = {http://projecteuclid.org/euclid.aoms/1177706618},
volume = {29},
year = {1958}
}
@article{Joshi1976,
author = {Joshi, V. M.},
doi = {10.1080/01621459.1976.10480345},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = {jun},
number = {354},
pages = {345--346},
title = {{A Note on Birnbaum's Theory of the Likelihood Principle}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480345},
volume = {71},
year = {1976}
}
@phdthesis{Martin1984,
address = {Iowa},
author = {Martin, Lynn Cindy},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Martin - 1984 - Applications of the distance measures between the prior and posterior distributions.pdf:pdf},
keywords = {Statistics},
school = {Iowa State University},
title = {{Applications of the distance measures between the prior and posterior distributions}},
url = {https://lib.dr.iastate.edu/rtd/8189},
year = {1984}
}
@article{Haario2007,
abstract = {A proper choice of a proposal distribution for Markov chain Monte Carlo methods, for example for the Metropolis-Hastings algorithm, is well known to be a crucial factor for the convergence of the algorithm. In this paper we introduce an adaptive Metropolis (AM) algorithm, where the Gaussian proposal distribution is updated along the process using the full information cumulated so far. Due to the adaptive nature of the process, the AM algorithm is non-Markovian, but we establish here that it has the correct ergodic properties. We also include the results of our numerical tests, which indicate that the AM algorithm competes well with traditional Metropolis-Hastings algorithms, and demonstrate that the AM algorithm is easy to use in practical computation.},
author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
doi = {10.2307/3318737},
issn = {13507265},
journal = {Bernoulli},
keywords = {adaptive markov chain monte,carlo,comparison,convergence,ergodicity,hastings algorithm,markov chain,metropolis,monte carlo},
number = {2},
pages = {223},
title = {{An Adaptive Metropolis Algorithm}},
volume = {7},
year = {2001}
}
@article{Hubbard2019b,
abstract = {Such is the grip of formal methods of statistical inference-that is, frequentist methods for generalizing from sample to population in enumerative studies-in the drawing of scientific inferences that the two are routinely deemed equivalent in the social, management, and biomedical sciences. This, despite the fact that legitimate employment of said methods is difficult to implement on practical grounds alone. But supposing the adoption of these procedures were simple does not get us far; crucially, methods of formal statistical inference are ill-suited to the analysis of much scientific data. Even findings from the claimed gold standard for examination by the latter, randomized controlled trials, can be problematic. Scientific inference is a far broader concept than statistical inference. Its authority derives from the accumulation , over an extensive period of time, of both theoretical and empirical knowledge that has won the (provisional) acceptance of the scholarly community. A major focus of scientific inference can be viewed as the pursuit of significant sameness, meaning replicable and empirically generalizable results among phenomena. Regrettably, the obsession with users of statistical inference to report significant differences in data sets actively thwarts cumulative knowledge development. The manifold problems surrounding the implementation and usefulness of formal methods of statistical inference in advancing science do not speak well of much teaching in methods/statistics classes. Serious reflection on statistics' role in producing viable knowledge is needed. Commendably, the American Statistical Association is committed to addressing this challenge, as further witnessed in this special online, open access issue of The American Statistician.},
author = {Hubbard, Raymond and Haig, Brian D. and Parsa, Rahul A.},
doi = {10.1080/00031305.2018.1464947},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hubbard, Haig, Parsa - 2019 - The Limited Role of Formal Statistical Inference in Scientific Inference.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Analytic studies,Enumerative studies,Observational studies,Randomized controlled trials,Scientific inference,Significant difference,Significant sameness,Statistical inference},
number = {sup1},
pages = {91--98},
publisher = {Taylor {\&} Francis},
title = {{The Limited Role of Formal Statistical Inference in Scientific Inference}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1464947},
volume = {73},
year = {2019}
}
@inproceedings{hovermill_2014,
author = {Hovermill, Jeffrey and Beaudrie, Brian and Boschmans, Barbara},
booktitle = {International Conference on Teaching Statistics (ICOTS9, July, 2014)},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hovermill, Beaudrie, Boschmans - 2014 - Statistical Literacy Requirements for Teachers.pdf:pdf},
title = {{Statistical Literacy Requirements for Teachers}},
year = {2014}
}
@book{Kendall2010,
abstract = {6th ed. Includes various eds. of some vols. Vol. 2A (1999) has imprint: London : Arnold ; New York : Oxford University Press. Vol. 2B (2004) has imprint: London : Arnold ; New York : Oxford University Press [distributor]. v. 1. Distribution theory / Alan Stuart, J. Keith Ord -- v. 2A. Classical inference and the linear model / Alan Stuart, J. Keith Ord, Steven Arnold -- v. 2B. Bayesian inference / Anthony O'Hagan.},
author = {Kendall, Maurice G. (Maurice George) and Stuart, Alan and Ord, J. K. and Arnold, Steven F. and O'Hagan, Anthony.},
edition = {6th},
isbn = {0340529229},
publisher = {John Wiley {\&} Sons},
title = {{Kendall's Advanced Theory of Statistics, Volume 2A, Classical Inference and the Linear Model}},
url = {https://www.wiley.com/en-us/Kendall{\%}27s+Advanced+Theory+of+Statistics{\%}2C+Volume+2A{\%}2C+Classical+Inference+and+the+Linear+Model{\%}2C+6th+Edition-p-9780470689240},
year = {2010}
}
@book{McNemar1949,
address = {New York},
author = {McNemar, Quinn},
publisher = {John Wiley},
title = {{Psychological statistics}},
year = {1949}
}
@article{Conference2016,
author = {Conference, Photovoltaic Specialists and Yablonovitch, E and Kurtz, S R and Liu, H C and Buchanan, M and Chiu, S and Gao, M and Liu, M and Snaith, H J and Johnston, M B and Herz, L M and Bisquert, J and Nakamura, T and Endo, M and Wakamiya, A and Kanemitsu, Y and Gray, J L and Bosch, J J Ten and Herz, L M and Raymond, F and Armin, A and Nagiri, R C R and Burn, P L and Meredith, P},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Conference et al. - 2016 - Re s ear ch r e p o r t s.pdf:pdf},
number = {6280},
pages = {1433--1437},
title = {{Re s ear ch | r e p o r t s}},
volume = {351},
year = {2016}
}
@article{Held2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1712.03032v1},
author = {Held, Leonhard},
eprint = {arXiv:1712.03032v1},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Held - 2017 - p -Values for Credibility.pdf:pdf},
number = {December},
pages = {1--21},
title = {{p -Values for Credibility}},
year = {2017}
}
@article{DeGroot1973,
author = {Degroot, Morris H.},
journal = {Journal of the American Statistical Association},
number = {344},
pages = {966--969},
title = {{Doing What Comes Naturally: Interpreting a Tail Area as a Posterior Probability or as a Likelihood Ratio}},
volume = {68},
year = {1973}
}
@inproceedings{earwood_impact_2016,
abstract = {In computer science education, the programming environment is just as crucial as the classroom environment. Students must be provided with adequate tools for developing an understanding of computer science paradigms. These paradigms include object-oriented programming (OOP) concepts, such as inheritance, polymorphism, and encapsulation. JavelinaCode is a web-based integrated development environment (IDE) for Java programming that provides students with both a static and dynamic visualization of code; the static structure of code at compile time and the dynamic execution of code at run time. The objective of this research is to present the differences between JavelinaCode and similar programming environment tools to demonstrate how JavelinaCode improves students' understanding of OOP concepts. Comparative analysis is conducted between JavelinaCode and programming environment tools, including BlueJ, Jeliot 3, AguiaJ, JIVE, and jGRASP. Each of these tools is evaluated on the basis of time constraints for download and installation, complexity of the download process and tool's interface, and the provision of static and dynamic visualizations. The results of the comparative analysis show that JavelinaCode provides students with a more effective tool for understanding OOP concepts than those considered. This is due to a simplified set up process and user interface and a better integrated set of visualizations. The combination of these factors contributes to a programming environment that emphasizes developing and running code.},
annote = {USA},
author = {Earwood, B and Yang, Jeong and Lee, Young},
booktitle = {2016 {\{}IEEE{\}} {\{}Frontiers{\}} in {\{}Education{\}} {\{}Conference{\}} ({\{}FIE{\}})},
doi = {10.1109/FIE.2016.7757639},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Earwood, Yang, Lee - 2016 - Impact of static and dynamic visualization in improving object-oriented programming concepts.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Earwood, Yang, Lee - 2016 - Impact of static and dynamic visualization in improving object-oriented programming concepts(2).html:html},
keywords = {AguiaJ,BlueJ,Computer science education,Dynamic programming,Internet,JIVE,Java programming,JavelinaCode,Jeliot 3,Linux,OOP concepts,Programming profession,Unified Modeling Language,Visualization,Web-based IDE,Web-based integrated development environment,compile time,data visualisation,dynamic code execution,dynamic code visualization,dynamic visualization,jGRASP,java,object-oriented programming,object-oriented programming concepts,program compilers,programming education,programming environments,run time,static code structure,static code visualization,static visualization,user interface,user interfaces,web-based programming environment},
month = {oct},
pages = {1--5},
title = {{Impact of static and dynamic visualization in improving object-oriented programming concepts}},
year = {2016}
}
@article{Aldrich2008,
author = {Aldrich, John},
journal = {Bayesian Analysis},
keywords = {bayes,dennis lindley,inverse probability,pierre-simon laplace,ronald fisher,theorem,thomas bayes},
number = {1},
pages = {161--170},
title = {{R. A. Fisher on Bayes and Bayes' Theorem}},
volume = {3},
year = {2008}
}
@inproceedings{ramirez-rosales_serious_2016,
abstract = {Nowadays, different serious game projects help the teaching-learning process of Object Oriented Programming (OOP) or Software Engineering (SE) concepts, but none of these projects are doing an integration of both fields for knowledge to children. The research presented in this paper proposes a serious game to help comprehension of basic concepts and attributes of OOP and SE to promote teaching-learning process of these concepts for children older than eight years old through Android devices.},
annote = {Mexico},
author = {Ram{\'{i}}rez-Rosales, S and V{\'{a}}zquez-Reyes, S and Villa-Cisneros, J L and Le{\'{o}}n-Sigg, M De},
booktitle = {2016 4th International Conference in Software Engineering Research and Innovation (CONISOFT)},
doi = {10.1109/CONISOFT.2016.23},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ram{\'{i}}rez-Rosales et al. - 2016 - A Serious Game to Promote Object Oriented Programming and Software Engineering Basic Concepts Learning.html:html},
keywords = {Android (operating system),Android devices,Androids,Computer aided instruction,Computer science education,Games,Humanoid robots,OOP,SE concepts,Software,game projects,object oriented programming,object-oriented programming,serious game,serious games (computing),software engineering,software engineering basic concepts learning,teaching-learning process},
month = {apr},
pages = {97--103},
title = {{A Serious Game to Promote Object Oriented Programming and Software Engineering Basic Concepts Learning}},
year = {2016}
}
@article{Baker2017,
author = {Baker, Monya and Dolgin, Elie},
doi = {10.1038/541269a},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Baker, Dolgin - 2017 - Reproducibility project yields muddy results Gates Foundation demands open access.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
pages = {269--270},
title = {{Reproducibility project yields muddy results: Gates Foundation demands open access}},
volume = {541},
year = {2017}
}
@article{Berger2003,
abstract = {Ronald Fisher advocated testing using p-values, Harold Jeffreys proposed use of objective posterior probabilities of hypotheses and Jerzy Neyman recommended testing with fixed error probabilities. Each was quite critical of the other approaches. Most troubling for statistics and science is that the three approaches can lead to quite different practical conclusions. This article focuses on discussion of the conditional frequentist approach to testing, which is argued to provide the basis for a methodological unification of the approaches of Fisher, Jeffreys and Neyman. The idea is to follow Fisher in using p-values to define the "strength of evidence" in data and to follow his approach of conditioning on strength of evidence; then follow Neyman by computing Type I and Type II error probabilities, but do so conditional on the strength of evidence in the data. The resulting conditional frequentist error probabilities equal the objective posterior probabilities of the hypotheses advocated by Jeffreys.},
author = {Berger, J.O.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Berger - 2003 - Could Fisher, Jeffreys and Neyman Have Agreed on Testing.pdf:pdf},
journal = {Statistical Science},
keywords = {Type I and Type II error probabilities,and phrases: p-values,conditional testing,posterior probabilities of hypotheses},
number = {1},
pages = {1--32},
title = {{Could Fisher, Jeffreys and Neyman Have Agreed on Testing?}},
volume = {18},
year = {2003}
}
@article{Jaeschke1989,
author = {Jaeschke, Roman and Singer, Joel and Guyatt, Gordon H.},
doi = {https://doi.org/10.1016/0197-2456(89)90005-6},
journal = {Controlled Clinical Trials},
number = {4},
pages = {407--415},
title = {{Measurement of health status: Ascertaining the minimal clinically important difference}},
volume = {10},
year = {1989}
}
@article{Monnahan2017,
author = {Monnahan, Cole C. and Thorson, James T. and Branch, Trevor A.},
doi = {10.1111/2041-210X.12681},
editor = {O'Hara, Robert B.},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {Bayesian inference,Markov chain Monte Carlo,Stan,hierarchical modelling,no‐U‐turn sampler},
month = {mar},
number = {3},
pages = {339--348},
publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
title = {{Faster estimation of Bayesian models in ecology using Hamiltonian Monte Carlo}},
url = {http://doi.wiley.com/10.1111/2041-210X.12681},
volume = {8},
year = {2017}
}
@article{OpenScienceFoundation,
author = {{Center for Open Science}},
journal = {https://osf.io/},
title = {{OSF Open Science Foundation}},
url = {https://osf.io/},
year = {2020}
}
@book{Fisher1925,
address = {Edinburgh},
author = {Fisher, Ronald A.},
editor = {and Boyd, Oliver},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1925 - Statistical Methods for Research Workers.pdf:pdf},
publisher = {Oliver and Boyd, Hafner Publishing Company},
title = {{Statistical Methods for Research Workers}},
year = {1925}
}
@article{Jordan1999,
abstract = {This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzman machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simplified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.},
author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
doi = {10.1023/A:1007665907178},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Jordan et al. - 1999 - Introduction to variational methods for graphical models.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Artificial Intelligence,Control,Mechatronics,Natural Language Processing (NLP),Robotics,Simulation and Modeling},
month = {nov},
number = {2},
pages = {183--233},
publisher = {Kluwer Academic Publishers},
title = {{Introduction to variational methods for graphical models}},
volume = {37},
year = {1999}
}
@article{Schwarz1978BayesianInformationCriterion,
abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
author = {Schwarz, G.},
doi = {10.2307/2958889},
issn = {00905364},
journal = {The Annals of Statistics},
number = {2},
pages = {461 -- 464},
publisher = {Institute of Mathematical Statistics},
title = {{Estimating the Dimension of a Model}},
url = {http://www.citeulike.org/user/abhishekseth/article/90008},
volume = {6},
year = {1978}
}
@article{Lakens2013,
abstract = {Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for t-tests and ANOVA's such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between within-and between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow. Keywords: effect sizes, power analysis, cohen's d, eta-squared, sample size planning Effect sizes are the most important outcome of empirical studies. Researchers want to know whether an intervention or experimental manipulation has an effect greater than zero, or (when it is obvious an effect exists) how big the effect is. Researchers are often reminded to report effect sizes, because they are useful for three reasons. First, they allow researchers to present the magnitude of the reported effects in a standardized metric which can be understood regardless of the scale that was used to measure the dependent variable. Such standardized effect sizes allow researchers to communicate the practical significance of their results (what are the practical consequences of the findings for daily life), instead of only reporting the statistical significance (how likely is the pattern of results observed in an experiment, given the assumption that there is no effect in the population). Second, effect sizes allow researchers to draw meta-analytic conclusions by comparing standardized effect sizes across studies. Third, effect sizes from previous studies can be used when planning a new study. An a-priori power analysis can provide an indication of the average sample size a study needs to observe a statistically significant result with a desired likelihood. The aim of this article is to explain how to calculate and report effect sizes for differences between means in between and within-subjects designs in a way that the reported results facilitate cumulative science. There are some reasons to assume that many researchers can improve their understanding of effect sizes. For example, researchers predominantly report the effect size partial eta squared ($\eta$ 2 p), which is provided by statistical software packages such as SPSS. The fact that $\eta$ 2 p is often reported for One-Way ANOVAs (where partial eta squared equals eta squared), indicates that researchers are either very passionate about unnecessary sub-script letters, or rely too much on the effect sizes as they are provided by statistical software packages. This practical primer should be seen as a complementary resource for psychologists who want to learn more about effect sizes (for excellent books that discuss this topic in more detail,. A supplementary spreadsheet is provided to facilitate effect size calculations. Reporting standardized effect sizes for mean differences requires that researchers make a choice about the standardizer of the mean difference, or a choice about how to calculate the proportion of variance explained by an effect. In this article, these choices will be highlighted for Cohen's d and eta squared ($\eta$ 2), two of the most widely used effect sizes in psychological research, with a special focus on the difference between within and between-subjects designs. I point out some caveats for researchers who want to perform power-analyses for within-subjects designs, and provide recommendations regarding the effect sizes that should be reported. Knowledge about the expected size of an effect is important information when planning a study. Researchers typically rely on null hypothesis significance tests to draw conclusions about observed differences between groups of observations. The probability of correctly rejecting the null hypothesis is known as the power of a statistical test (Cohen, 1988). Statistical power depends on the sample size of the study (through its influence on the reliability of the sample values, and specifically the extent to which sample values can be expected to be an approximation of the},
author = {Lakens, Dani{\"{e}}l},
doi = {10.3389/fpsyg.2013.00863},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Lakens - 2013 - Calculating and reporting effect sizes to facilitate cumulative science a practical primer for t-tests and ANOVAs.pdf:pdf},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {Cohen's d, eta-squared,Effect sizes,Power analysis,Sample size planning},
month = {nov},
number = {NOV},
pages = {863},
publisher = {Frontiers},
title = {{Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs}},
url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00863/abstract},
volume = {4},
year = {2013}
}
@inproceedings{Hoffman2012,
author = {Hoffman, Matthew D. and Carpenter, Bob and Gelman, Andrew},
booktitle = {Proceedings of the NIPS Workshop on Probabilistic Programming},
title = {{Stan, scalable software for Bayesian modeling}},
year = {2012}
}
@article{royston_correcting_1989,
abstract = {The Shapiro-Wilk test for non-normality is highly sensitive to the presence of ties due to grouping or rounding of the raw data, and should not be used if the grouping interval exceeds 0.1 standard deviation units. A simple method of modifying Wfor known grouping intervals is presented, and the case of unequal intervals is covered. The modified Wtest may be applied over a wide range of grouping intervals (0-1.5 S.D. units) and sample sizes (7-2000), and is shown to have high power whencompared with a variant of the Pearson $\chi$2test.},
author = {Royston, J P},
doi = {10.1080/00949658908811146},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Royston - 1989 - Correcting the shapiro-wilk-test for ties.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Royston - 1989 - Correcting the shapiro-wilk-test for ties.html:html},
issn = {0094-9655},
journal = {Journal of Statistical Computation and Simulation},
keywords = {Pearson Chi-square test,Shapiro-Wilk W,Sheppard's correction,grouped data,normalising trans-formation,power,rounding,test of normality},
month = {apr},
number = {4},
pages = {237--249},
title = {{Correcting the shapiro-wilk-test for ties}},
url = {http://dx.doi.org/10.1080/00949658908811146},
volume = {31},
year = {1989}
}
@book{Laplace1820,
address = {Paris},
author = {Laplace, Pierre-Simon},
publisher = {Courcier},
title = {{Th{\'{e}}orie Analytique des Probabilit{\'{e}}s (3rd edition)}},
year = {1820}
}
@article{Yates1984,
abstract = {Fisher's exact test, and the approximation to it by the continuity-corrected X2 test, have repeatedly been attacked over the past 40 years, recently with the support of extensive computer exercises. The present paper argues, on commonsense grounds, supported by simple examples, that these attacks are misconceived, and are mainly due to uncritical acceptance of the Neyman-Pearson approach to tests of significance, the use of nominal levels, and refusal to accept the arguments for conditioning on the margins. Two-sided tests have also added to the confusion; it is argued that the best definition of a two-sided probability is twice the observed one-tail probability},
author = {Yates, F.},
doi = {10.2307/2981577},
isbn = {0035-9238},
issn = {00359238},
journal = {Journal of the Royal Statistical Society. Series A (General)},
number = {3},
pages = {426},
pmid = {19237560},
title = {{Test of Significance for 2 × 2 Contingency Tables}},
url = {https://www.jstor.org/stable/10.2307/2981577?origin=crossref http://www.jstor.org/stable/10.2307/2981577?origin=crossref},
volume = {147},
year = {1984}
}
@article{Ioannidis2005a,
abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
archivePrefix = {arXiv},
arxivId = {gr-qc/0208024},
author = {Ioannidis, John P A},
doi = {10.1371/journal.pmed.0020124},
eprint = {0208024},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ioannidis - 2005 - Why most published research findings are false.pdf:pdf},
isbn = {3540239081},
issn = {15491277},
journal = {PLoS Medicine},
number = {8},
pages = {0696--0701},
pmid = {16060722},
primaryClass = {gr-qc},
publisher = {Public Library of Science},
title = {{Why most published research findings are false}},
volume = {2},
year = {2005}
}
@book{Adams1975,
address = {Boston},
author = {Adams, Ernest W.},
doi = {10.1007/978-94-015-7622-2_1},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Adams - 1975 - The Logic of Conditionals.pdf:pdf},
publisher = {Springer Netherlands},
title = {{The Logic of Conditionals}},
year = {1975}
}
@article{Magnusson2020,
abstract = {Recently, new methods for model assessment, based on subsampling and posterior approximations, have been proposed for scaling leave-one-out cross-validation (LOO) to large datasets. Although these methods work well for estimating predictive performance for individual models, they are less powerful in model comparison. We propose an efficient method for estimating differences in predictive performance by combining fast approximate LOO surrogates with exact LOO subsampling using the difference estimator and supply proofs with regards to scaling characteristics. The resulting approach can be orders of magnitude more efficient than previous approaches, as well as being better suited to model comparison.},
archivePrefix = {arXiv},
arxivId = {2001.00980},
author = {Magnusson, M{\aa}ns and Andersen, Michael Riis and Jonasson, Johan and Vehtari, Aki},
eprint = {2001.00980},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Magnusson et al. - 2020 - Leave-One-Out Cross-Validation for Bayesian Model Comparison in Large Data.pdf:pdf},
journal = {arXiv preprint},
title = {{Leave-One-Out Cross-Validation for Bayesian Model Comparison in Large Data}},
url = {http://arxiv.org/abs/2001.00980},
year = {2020}
}
@article{Wrinch1919,
author = {Wrinch, Dorothy and Jeffreys, Harold},
doi = {10.1080/14786441208636005},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Wrinch, Jeffreys - 1919 - LXXV. On some aspects of the theory of probability.pdf:pdf},
isbn = {1478644120863},
issn = {1941-5982},
journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
number = {228},
pages = {715--731},
title = {{LXXV. On some aspects of the theory of probability}},
volume = {38},
year = {1919}
}
@article{Fisher1921,
abstract = {Reproduced with permission of Metron},
author = {Fisher, Ronald Aylmer},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1921 - On the Probable Error of a Coefficient of Correlation Deduced from a Small Sample.pdf:pdf},
journal = {Metron},
number = {1},
pages = {3--32},
title = {{On the "Probable Error" of a Coefficient of Correlation Deduced from a Small Sample.}},
url = {https://digital.library.adelaide.edu.au/dspace/handle/2440/15169},
year = {1921}
}
@article{Rougier2019,
author = {Rougier, Jonathan},
doi = {10.1080/00031305.2018.1502684},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Rougier - 2019 - ipi -Values, Bayes Factors, and Sufficiency.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Embedding model,Exponential tilting,Generalized Li},
number = {sup1},
pages = {148--151},
publisher = {Taylor {\&} Francis},
title = {{{\textless}i{\textgreater}p{\textless}/i{\textgreater} -Values, Bayes Factors, and Sufficiency}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1502684},
volume = {73},
year = {2019}
}
@article{Glauber1963,
author = {Glauber, Roy J.},
doi = {10.1063/1.1703954},
issn = {0022-2488},
journal = {Journal of Mathematical Physics},
keywords = {alloys,fourier analysis,frequency,interactions,ising model,lattices,losses,magnetic fields,markov process,mathematics,oscillations,physics,spin,statistics,susceptibility,temperature,transients,variations},
month = {feb},
number = {2},
pages = {294--307},
title = {{Time‐Dependent Statistics of the Ising Model}},
url = {http://aip.scitation.org/doi/10.1063/1.1703954},
volume = {4},
year = {1963}
}
@book{DeFinetti2017,
address = {Chichester, UK},
author = {de Finetti, Bruno},
doi = {10.1002/9781119286387},
editor = {Mach{\'{i}}, Antonio and Smith, Adrian},
isbn = {9781119286387},
month = {feb},
publisher = {John Wiley {\&} Sons, Ltd},
series = {Wiley Series in Probability and Statistics},
title = {{Theory of Probability}},
url = {http://doi.wiley.com/10.1002/9781119286387},
year = {2017}
}
@article{FisherRonaldAylmerSir1923,
abstract = {Reproduced with permission of Blackwell Publishers},
author = {Fisher, Ronald Aylmer},
doi = {10.2307/2548482},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1923 - Statistical Tests of Agreement Between Observation and Hypothesis.pdf:pdf},
issn = {00130427},
journal = {Economica},
number = {3},
pages = {139--147},
title = {{Statistical Tests of Agreement Between Observation and Hypothesis.}},
url = {https://digital.library.adelaide.edu.au/dspace/handle/2440/15178},
year = {1923}
}
@article{Casella1992,
author = {Casella, George and George, Edward I.},
doi = {10.1080/00031305.1992.10475878},
isbn = {00031305},
issn = {00031305},
journal = {The American Statistician},
number = {3},
pages = {167--174},
pmid = {21105692},
title = {{Explaining the Gibbs Sampler}},
volume = {46},
year = {1992}
}
@article{Pearson1933,
author = {Pearson, E.S.},
journal = {Journal of the Royal Statistical Society},
number = {1},
pages = {21--75},
title = {{A Survey of the Uses of Statistical Method in the Control and Standardization of the Quality of Manufactured Products}},
volume = {96},
year = {1933}
}
@article{Johnson2010,
abstract = {We examine philosophical problems and sampling deficiencies that are associated with current Bayesian hypothesis testing methodology, paying particular attention to objective Bayes methodology. Because the prior densities that are used to define alternative hypotheses in many Bayesian tests assign non-negligible probability to regions of the parameter space that are consistent with null hypotheses, resulting tests provide exponential accumulation of evidence in favour of true alternative hypotheses, but only sublinear accumulation of evidence in favour of true null hypotheses. Thus, it is often impossible for such tests to provide strong evidence in favour of a true null hypothesis, even when moderately large sample sizes have been obtained. We review asymptotic convergence rates of Bayes factors in testing precise null hypotheses and propose two new classes of prior densities that ameliorate the imbalance in convergence rates that is inherited by most Bayesian tests. Using members of these classes, we obtain analytic expressions for Bayes factors in linear models and derive approximations to Bayes factors in large sample settings. Journal compilation {\textcopyright} 2010 Royal Statistical Society.},
author = {Johnson, Valen E. and Rossell, David},
doi = {10.1111/j.1467-9868.2009.00730.x},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Fractional Bayes factor,Intrinsic Bayes factor,Intrinsic prior,Inverse moment density function,Moment density function,Objective Bayes analysis},
month = {mar},
number = {2},
pages = {143--170},
title = {{On the use of non-local prior densities in Bayesian hypothesis tests}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2009.00730.x},
volume = {72},
year = {2010}
}
@article{Fraser2011BayesPosteriorQuickDirtyConfidence,
author = {Fraser, D.A.S.},
journal = {Statistical Science},
number = {3},
pages = {299--316},
title = {{Is Bayes Posterior just Quick and Dirty Confidence?}},
volume = {26},
year = {2011}
}
@article{Lakens2018,
abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
author = {Lakens, Dani{\"{e}}l and Scheel, Anne M. and Isager, Peder M.},
doi = {10.1177/2515245918770963},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Lakens, Scheel, Isager - 2018 - Equivalence Testing for Psychological Research A Tutorial.pdf:pdf},
issn = {2515-2459},
journal = {Advances in Methods and Practices in Psychological Science},
keywords = {equivalence testing,falsification,frequentist,null hypothesis,null-hypothesis significance test,open,power,tost},
number = {2},
pages = {259--269},
title = {{Equivalence Testing for Psychological Research: A Tutorial}},
volume = {1},
year = {2018}
}
@book{hacking_1975,
address = {Cambridge},
author = {Hacking, Ian},
publisher = {Cambridge University Press},
title = {{The Emergence Of Probability - A philosophical study of early ideas about probability, induction and statistical inference}},
year = {1975}
}
@article{peters_developing_2014,
abstract = {This retrospective phenomenological study investigates activities and actions identified by secondary statistics teachers who exhibit robust understandings of variation as deepening their understandings of statistical variation. Using phenomenological methods and a frame of Mezirow's transformation theory, analysis revealed learning factors that include their interests in statistics, motivation to encounter and resolve dilemmas, desires to have an overarching content framework, propensities for critical reflection, and actions on opportunities to engage in statistical learning activities and rationale discourse with more knowledgeable others. The extent to which these teachers embrace these opportunities distinguishes them from other teachers. Results from this study provide some basis for formulating hypotheses about secondary teachers' statistics learning in general by contributing to understanding circumstances that may be conducive to developing deep understandings of statistical content. This study also advances the use of retrospective methods within a theoretical frame for adult learning to investigate teacher learning.},
author = {Peters, Susan A},
doi = {10.1007/s10857-013-9242-7},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Peters - 2014 - Developing understanding of statistical variation secondary statistics teachers' perceptions and recollections of lear.pdf:pdf},
issn = {1386-4416, 1573-1820},
journal = {Journal of Mathematics Teacher Education},
month = {dec},
number = {6},
pages = {539--582},
shorttitle = {Developing understanding of statistical variation},
title = {{Developing understanding of statistical variation: secondary statistics teachers' perceptions and recollections of learning factors}},
url = {https://link.springer.com/article/10.1007/s10857-013-9242-7},
volume = {17},
year = {2014}
}
@incollection{Holschuh1980,
author = {Holschuh, Norton},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_5},
pages = {35--45},
publisher = {Springer, New York, NY},
title = {{Randomization and Design: I}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}5},
year = {1980}
}
@article{GURLAND1962,
author = {Gurland, John and McCullough, Roger S.},
doi = {10.2307/2333975},
issn = {00063444},
journal = {Biometrika},
month = {jan},
number = {3/4},
pages = {403},
publisher = {Narnia},
title = {{Testing Equality of Means after a Preliminary test of Equality of Variances}},
url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/49.3-4.403},
volume = {49},
year = {1962}
}
@article{Kelter2020WIREs,
abstract = {Testing for differences between two groups is one of the scenarios most often faced by scientists across all domains and is particularly important in the medical sciences and psychology. The traditional solution to this problem is rooted inside the Neyman–Pearson theory of null hypothesis significance testing and uniformly most powerful tests. In the last decade, a lot of progress has been made in developing Bayesian versions of the most common parametric and nonparametric two-sample tests, including Student's t-test and the Mann–Whitney U test. In this article, we review the underlying assumptions, models and implications for research practice of these Bayesian two-sample tests and contrast them with the existing frequentist solutions. Also, we show that in general Bayesian and frequentist two-sample tests have different behavior regarding the type I and II error control, which needs to be carefully balanced in practical research. This article is categorized under: Statistical and Graphical Methods of Data Analysis {\textgreater} Bayesian Methods and Theory Statistical and Graphical Methods of Data Analysis {\textgreater} Monte Carlo Methods Statistical and Graphical Methods of Data Analysis {\textgreater} Markov Chain Monte Carlo.},
author = {Kelter, Riko},
doi = {10.1002/wics.1523},
issn = {1939-5108},
journal = {WIREs Computational Statistics},
keywords = {Bayesian two-sample tests,Mann–Whitney U test,Student's t-test,null hypothesis significance testing,testing for differences between two groups},
number = {7},
publisher = {Wiley-Blackwell},
title = {{Bayesian and frequentist testing for differences between two groups with parametric and nonparametric two-sample tests}},
year = {2020}
}
@article{Etz2015,
abstract = {This article brings attention to some historical developments that gave rise to the Bayes factor for testing a point null hypothesis against a composite alternative. In line with current thinking, we find that the conceptual innovation - to assign prior mass to a general law - is due to a series of three articles by Dorothy Wrinch and Sir Harold Jeffreys (1919, 1921, 1923). However, our historical investigation also suggests that in 1932 J. B. S. Haldane made an important contribution to the development of the Bayes factor by proposing the use of a mixture prior comprising a point mass and a continuous probability density. Jeffreys was aware of Haldane's work and it may have inspired him to pursue a more concrete statistical implementation for his conceptual ideas. It thus appears that Haldane may have played a much bigger role in the statistical development of the Bayes factor than has hitherto been assumed.},
archivePrefix = {arXiv},
arxivId = {1511.08180},
author = {Etz, Alexander and Wagenmakers, Eric-Jan},
doi = {10.1214/16-STS599},
eprint = {1511.08180},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Etz, Wagenmakers - 2015 - J. B. S. Haldane's Contribution to the Bayes Factor Hypothesis Test.pdf:pdf},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {History of statistics,Sir Ha,and phrases,evidence,history of statistics,induction,sir},
number = {2},
pages = {313--329},
title = {{J. B. S. Haldane's Contribution to the Bayes Factor Hypothesis Test}},
url = {http://arxiv.org/abs/1511.08180},
volume = {32},
year = {2017}
}
@article{Roberts1996,
abstract = {We develop results on geometric ergodicity of Markov chains and apply these and other recent results in Markov chain theory to multidimensional Hastings and Metropolis algor- ithms. For those based on random walk candidate distributions, we find sufficient con- ditions for moments and moment generating functions to converge at a geometric rate to a prescribed distribution 7t. By phrasing the conditions in terms of the curvature of the densities we show that the results apply to all distributions with positive densities in a large class which encompasses many commonly-used statistical forms. From these results we develop central limit theorems for the Metropolis algorithm. Converse results, show- ing non-geometric convergence rates for chains where the rejection rate is not bounded away from unity, are also given; these show that the negative-definiteness property is not redundant},
author = {Roberts, G. O. and Tweedie, R. L.},
doi = {10.1093/biomet/83.1.95},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Roberts, Tweedie - 1996 - Geometric convergence and central limit theorems for multidimensional Hastings and Metropolis algorithms.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Geometric ergodicity,Gibbs sampling,Hastings algorithm,Irreducible Markov process,Markov chain Monte Carlo,Metropolis algorithm,Posterior distribution},
month = {mar},
number = {1},
pages = {95--110},
publisher = {Oxford University Press},
title = {{Geometric convergence and central limit theorems for multidimensional Hastings and Metropolis algorithms}},
url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/83.1.95},
volume = {83},
year = {1996}
}
@article{Nott2009,
abstract = {A Bayesian approach to variable selection which is based on the expected Kullback-Leibler divergence between the full model and its projection onto a submodel has recently been suggested in the literature. Here we extend this idea by considering projections onto subspaces defined via some form of {\$}L{\_}1{\$} constraint on the parameter in the full model. This leads to Bayesian model selection approaches related to the lasso. In the posterior distribution of the projection there is positive probability that some components are exactly zero and the posterior distribution on the model space induced by the projection allows exploration of model uncertainty. We also consider use of the approach in structured variable selection problems such as ANOVA models where it is desired to incorporate main effects in the presence of interactions. Here we make use of projections related to the non-negative garotte which are able to respect the hierarchical constraints. We also prove a consistency result concerning the posterior distribution on the model induced by the projection, and show that for some projections related to the adaptive lasso and non-negative garotte the posterior distribution concentrates on the true model asymptotically.},
archivePrefix = {arXiv},
arxivId = {0901.4605},
author = {Nott, David and Leng, Chenlei},
eprint = {0901.4605},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Nott, Leng - 2009 - Bayesian projection approaches to variable selection and exploring model uncertainty.pdf:pdf},
journal = {arXix preprint},
month = {jan},
title = {{Bayesian projection approaches to variable selection and exploring model uncertainty}},
url = {http://arxiv.org/abs/0901.4605},
year = {2009}
}
@book{Jeffreys1939,
address = {Oxford},
author = {Jeffreys, Harold},
edition = {1st},
publisher = {The Clarendon Press},
title = {{Theory of Probability}},
year = {1939}
}
@article{al-jabor_qatar_2013,
author = {Al-Jabor, Wadha and Ucar, Pinar},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Al-Jabor, Ucar - 2013 - Qatar Statistic's Authority's initiatives to develop statistical literacy in Qatar.pdf:pdf},
journal = {Statistical Journal of the IAOS},
number = {3},
pages = {147--152},
title = {{Qatar Statistic's Authority's initiatives to develop statistical literacy in Qatar}},
volume = {29},
year = {2013}
}
@article{Pashler2012,
abstract = {lkj this special section had a lot of incisive articles. I read the paper version. Visit http://pps.sagepub.com/content/7/6.toc if wanting to see those articles again.},
author = {Pashler, Harold and Wagenmakers, Eric-Jan},
doi = {10.1177/1745691612465253},
isbn = {1745-6916$\backslash$n1745-6924},
issn = {17456916},
journal = {Perspectives on Psychological Science},
month = {nov},
number = {6},
pages = {528--530},
pmid = {26168108},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Editors' Introduction to the Special Section on Replicability in Psychological Science: A Crisis of Confidence?}},
url = {http://journals.sagepub.com/doi/10.1177/1745691612465253},
volume = {7},
year = {2012}
}
@incollection{Efron2000,
abstract = {This chapter explores in greater depth the dangers of conflating and confusing the criminal and Title IX/Clery-informed responses and prevention approaches, both for institutions of higher education (IHEs) and their students. The chapter demonstrates that both IHEs and lawmakers should look to civil rights and equality-based approaches for preventing and responding to sexual violence and should guard against having those approaches "criminalized" by an overreliance and overconfidence in criminal methods of addressing this violence. In contrast to the equal procedural rights provided to sexual violence victims under Title IX's civil rights approach, the criminal justice system structurally marginalizes all victims of crime, including sexual violence victims, from its procedures and affords them few if any procedural rights. The chapter concludes, schools and government officials must establish systems to coordinate IHE and criminal justice responses to sexual violence, especially when survivors decide to pursue parallel criminal and administrative/civil proceedings.},
address = {Boca Raton},
author = {Cantalupo, Nancy Chi},
booktitle = {The Crisis of Campus Sexual Violence},
chapter = {4},
doi = {10.4324/9781315725604-8},
edition = {1st},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Cantalupo - 2015 - Title IX's Civil Rights Approach and the Criminal Justice System.pdf:pdf},
isbn = {0534167640, 9780534167646},
month = {jan},
pages = {22},
publisher = {CRC Press},
title = {{Title IX's Civil Rights Approach and the Criminal Justice System}},
url = {https://www.taylorfrancis.com/books/e/9781482277494/chapters/10.1201{\%}2F9781482277494-3},
year = {2015}
}
@article{Rao2016,
abstract = {Testing a point (sharp) null hypothesis is arguably the most widely used statistical inferential procedure in many fields of scientific research, nevertheless, the most controversial, and misapprehended. Since 1935 when Buchanan-Wollaston raised the first criticism against hypothesis testing, this foundational field of statistics has drawn increasingly active and stronger opposition, including draconian suggestions that statistical significance testing should be abandoned or even banned. Statisticians should stop ignoring these accumulated and significant anomalies within the current point-null hypotheses paradigm and rebuild healthy foundations of statistical science. The foundation for a paradigm shift in testing statistical hypotheses is suggested, which is testing interval null hypotheses based on implications of the Zero probability paradox. It states that in a real-world research point-null hypothesis of a normal mean has zero probability. This implies that formulated point-null hypothesis of a mean in the context of the simple normal model is almost surely false. Thus, Zero probability paradox points to the root cause of so-called large n problem in significance testing. It discloses that there is no point in searching for a cure under the current point-null paradigm.},
author = {Rao, Calyampudi Radhakrishna and Lovric, Miodrag M.},
doi = {10.22237/jmasm/1478001660},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Rao, Lovric - 2016 - Testing point null hypothesis of a normal mean and the truth 21st Century perspective.pdf:pdf},
issn = {15389472},
journal = {Journal of Modern Applied Statistical Methods},
keywords = {Algebraic numbers,Almost sure false null hypothesis,Inexactification,Lebesgue measure,Paradigm shift in testing statistical hypotheses,Point null hypothesis,Rational numbers,Zero-probability paradox},
number = {2},
pages = {2--21},
publisher = {Wayne State University},
title = {{Testing point null hypothesis of a normal mean and the truth: 21st Century perspective}},
volume = {15},
year = {2016}
}
@article{Haaf2019,
author = {Haaf, Julia M. and Ly, Alexander and Wagenmakers, Eric Jan},
doi = {10.1038/d41586-019-00972-7},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Haaf, Ly, Wagenmakers - 2019 - Retire significance, but still test hypotheses.pdf:pdf},
issn = {14764687},
journal = {Nature},
month = {mar},
number = {7749},
pages = {461},
publisher = {NLM (Medline)},
title = {{Retire significance, but still test hypotheses}},
volume = {567},
year = {2019}
}
@book{Claeskens2008,
abstract = {Choosing a model is central to all statistical work with data. We have seen rapid advances in model fitting and in the theoretical understanding of model selection, yet this book is the first to synthesize research and practice from this active field. Model choice criteria are explained, discussed and compared, including the AIC, BIC, DIC and FIC. The uncertainties involved with model selection are tackled with discussions of frequent and Bayesian methods; model averaging schemes are presented. Real-data examples are complemented by derivations providing deeper insight into the methodology, and instructive exercises build familiarity with the methods. The companion website features Data sets and R-code.},
author = {Claeskens, Gerda and Hjort, Nils Lid},
booktitle = {Model Selection and Model Averaging},
doi = {10.1017/CBO9780511790485},
isbn = {9780511790485},
month = {jan},
pages = {1--312},
publisher = {Cambridge University Press},
title = {{Model selection and model averaging}},
year = {2008}
}
@article{Held2018,
abstract = {The p-value quantifies the discrepancy between the data and a null hypothesis of interest, usually the assumption of no difference or no effect. A Bayesian approach allows the calibration of p-values by transforming them to direct measures of the evidence against the null hypothesis, so-called Bayes factors. We review the available literature in this area and consider two-sided significance tests for a point null hypothesis in more detail. We distinguish simple from local alternative hypotheses and contrast traditional Bayes factors based on the data with Bayes factors based on p-values or test statistics. A well-known finding is that the minimum Bayes factor, the smallest possible Bayes factor within a certain class of alternative hypotheses, provides less evidence against the null hypothesis than the corresponding p-value might suggest. It is less known that the relationship between p-values and minimum Bayes factors also depends on the sample size and on the dimension of the parameter of interest. We i...},
author = {Held, Leonhard and Ott, Manuela},
doi = {10.1146/annurev-statistics-031017-100307},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Held, Ott - 2018 - On p-Values and Bayes Factors.pdf:pdf},
issn = {2326-8298},
journal = {Annual Review of Statistics and Its Application},
keywords = {Bayes factor,evidence,minimum Bayes factor,objective Bayes,p-value,sample size},
number = {1},
pages = {393--419},
publisher = {Annual Reviews},
title = {{On p-Values and Bayes Factors}},
volume = {5},
year = {2018}
}
@inproceedings{Schmidt2009,
address = {New York},
author = {Schmidt, Mikkel N.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
doi = {10.1145/1553374.1553492},
isbn = {9781605585161},
pages = {1--8},
publisher = {ACM Press},
title = {{Function factorization using warped Gaussian processes}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553492},
year = {2009}
}
@article{Rochon2012,
abstract = {BACKGROUND: Student's two-sample t test is generally used for comparing the means of two independent samples, for example, two treatment arms. Under the null hypothesis, the t test assumes that the two samples arise from the same normally distributed population with unknown variance. Adequate control of the Type I error requires that the normality assumption holds, which is often examined by means of a preliminary Shapiro-Wilk test. The following two-stage procedure is widely accepted: If the preliminary test for normality is not significant, the t test is used; if the preliminary test rejects the null hypothesis of normality, a nonparametric test is applied in the main analysis.$\backslash$n$\backslash$nMETHODS: Equally sized samples were drawn from exponential, uniform, and normal distributions. The two-sample t test was conducted if either both samples (Strategy I) or the collapsed set of residuals from both samples (Strategy II) had passed the preliminary Shapiro-Wilk test for normality; otherwise, Mann-Whitney's U test was conducted. By simulation, we separately estimated the conditional Type I error probabilities for the parametric and nonparametric part of the two-stage procedure. Finally, we assessed the overall Type I error rate and the power of the two-stage procedure as a whole.$\backslash$n$\backslash$nRESULTS: Preliminary testing for normality seriously altered the conditional Type I error rates of the subsequent main analysis for both parametric and nonparametric tests. We discuss possible explanations for the observed results, the most important one being the selection mechanism due to the preliminary test. Interestingly, the overall Type I error rate and power of the entire two-stage procedure remained within acceptable limits.$\backslash$n$\backslash$nCONCLUSION: The two-stage procedure might be considered incorrect from a formal perspective; nevertheless, in the investigated examples, this procedure seemed to satisfactorily maintain the nominal significance level and had acceptable power properties.},
author = {Rochon, Justine and Gondan, Matthias and Kieser, Meinhard},
doi = {10.1186/1471-2288-12-81},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Rochon, Gondan, Kieser - 2012 - To test or not to test Preliminary assessment of normality when comparing two independent samples.pdf:pdf},
issn = {14712288},
journal = {BMC Medical Research Methodology},
keywords = {Mann-Whitneys U test,Studentst test,Testing for normality},
title = {{To test or not to test: Preliminary assessment of normality when comparing two independent samples}},
volume = {12},
year = {2012}
}
@book{Held2014,
address = {Berlin, Heidelberg},
author = {Held, Leonhard and {Saban{\'{e}}s Bov{\'{e}}}, Daniel},
doi = {10.1007/978-3-642-37887-4},
isbn = {978-3-642-37886-7},
pages = {375},
publisher = {Springer},
title = {{Applied Statistical Inference}},
year = {2014}
}
@book{Gelman2013BayesianDataAnalysis,
abstract = {Third edition. "Preface This book is intended to have three roles and to serve three associated audiences: an introductory text on Bayesian inference starting from first principles, a graduate text on effective current approaches to Bayesian modeling and computation in statistics and related fields, and a handbook of Bayesian methods in applied statistics for general users of and researchers in applied statistics. Although introductory in its early sections, the book is definitely not elementary in the sense of a first text in statistics. The mathematics used in our book is basic probability and statistics, elementary calculus, and linear algebra. A review of probability notation is given in Chapter 1 along with a more detailed list of topics assumed to have been studied. The practical orientation of the book means that the reader's previous experience in probability, statistics, and linear algebra should ideally have included strong computational components. To write an introductory text alone would leave many readers with only a taste of the conceptual elements but no guidance for venturing into genuine practical applications, beyond those where Bayesian methods agree essentially with standard non-Bayesian analyses. On the other hand, we feel it would be a mistake to present the advanced methods without first introducing the basic concepts from our data-analytic perspective. Furthermore, due to the nature of applied statistics, a text on current Bayesian methodology would be incomplete without a variety of worked examples drawn from real applications. To avoid cluttering the main narrative, there are bibliographic notes at the end of each chapter and references at the end of the book"-- Part I: Fundamentals of Bayesian inference. Probability and inference -- Single-parameter models -- Introduction to multiparameter models -- Asymptotics and connections to non-Bayesian approaches -- Hierarchical models -- Part II: Fundamentals of Bayesian data analysis. Model checking -- Evaluating, comparing, and expanding models -- Modeling accounting for data collection -- Decision analysis -- Part III: Advanced computation. Introduction to Bayesian computation -- Basics of Markov chain simulation -- Computationally efficient Markov chain simulation -- Modal and distributional approximations -- Part IV: Regression models. Introduction to regression models -- Hierarchical linear models -- Generalized linear models -- Models for robust inference -- Models for missing data -- Part V: Nonlinear and nonparametric models. Parametric nonlinear models -- Basis function models -- Gaussian process models -- Finite mixture models -- Dirichlet process models -- A. Standard probability distributions -- B. Outline of proofs of limit theorems -- Computation in R and Stan.},
address = {Boca Raton},
author = {Gelman, Andrew and Carlin, John and Stern, Hal and Dunson, David and Vehtari, Aki and Rubin, Donald},
edition = {3rd},
isbn = {1439840954},
pages = {667},
publisher = {CRC Press/Taylor {\&} Francis},
title = {{Bayesian data analysis}},
year = {2013}
}
@article{gaise_2005,
author = {{American Statistical Association}},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/American Statistical Association - 2005 - Guidelines for Assessment and Instruction in Statistics Education (GAISE) Report, College Repo.pdf:pdf},
number = {July},
pages = {1--141},
title = {{Guidelines for Assessment and Instruction in Statistics Education (GAISE) Report, College Report 2016}},
url = {http://www.amstat.org/education/gaise/GaiseCollege{\_}full.pdf},
year = {2005}
}
@article{Zabell1989b,
author = {Zabell, Sandy},
doi = {10.1214/ss/1177012488},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {R. A. Fisher,history of statistics,inverse probability},
month = {aug},
number = {3},
pages = {247--256},
publisher = {Institute of Mathematical Statistics},
title = {{R. A. Fisher on the History of Inverse Probability}},
url = {http://projecteuclid.org/euclid.ss/1177012488},
volume = {4},
year = {1989}
}
@incollection{Good1953,
address = {Liverpool, U.K.},
author = {Good, I.J.},
booktitle = {Uncertainty and Business Decisions},
editor = {Carter, C.F. and Meredith, G.P. and Shackle, G.L.S.},
publisher = {Liverpool University Press},
title = {{The appropriate mathematical tools for describing and measuring uncertainty}},
year = {1953}
}
@article{Cohen1994,
abstract = {After 4 decades of severe criticism, the ritual of null hypothesis significance testing - mechanical dichotomous decisions around a sacred .05 criterion - still persists. This article reviews the problems with this practice, including its near-universal misinterpretation of p as the probability that H0s false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H0 one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods is suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication.},
author = {Cohen, Jacob},
doi = {10.1037/0003-066X.49.12.997},
issn = {0003066X},
journal = {American Psychologist},
number = {12},
pages = {997--1003},
publisher = {American Psychological Association Inc.},
title = {{The earth is round (p {\textless} .05)}},
url = {/record/1995-12080-001},
volume = {49},
year = {1994}
}
@article{Fisher1921a,
abstract = {In Part I is given a survey of the results of a statistical examination of the yield of the plots of Broadbalk “Wheat field during 67 years. The main features of the comparison of mean yields are well known; the comparative rates of decrement, shown in Section 5, supply a class of facts well worthy of further study. Particularly striking are the relatively slow rates of decrement of plots 2b and 8, compared with plot 7, which would seem to show a permanent advantage in very high nitrogenous dressings, and to emphasise the need for caution in the application of the principle of diminishing returns. The evidence of the influence of potassium sulphate and its substitutes, sodium sulphate and magnesium sulphate, shown in Table V, is also very striking. An unsuspected feature of the changes of mean yields, which precludes the possibility of obtaining from these data true curves of exhaustion has appeared in the slow changes which have taken place in all the plots in a similar manner. In Part II the mathematical methods by which the variation has been analysed has been discussed, partly as a justification of novel procedure, partly, to make clear that the three types of variations found have been genuinely distinguished. In Part III such evidence as is available has been presented, in order to throw light upon the possibility that the changes in mean yield have been caused by variations in the prevalence of weeds at different periods. One point of importance which should be emphasised is that average wheat yields, even over long periods, from different fields or for different seasons cannot approach in accuracy the comparison of plots of the same field in the same seasons. The advantage of the method adopted by Lawes in the permanent experiments which he instituted is very evident. The effects of weather clearly require that the seasons should be identical, unless the series be very long, but the slow changes in mean yield show that even comparatively long series of different years from the same field cannot be accurately compared. Within the same field, however, the slow changes have almost proportional effects, and comparison between the mean yields of neighbouring plots may be made with great accuracy. The only case in which changes in mean yield sensibly affect the comparison of averages is that of plots 17 and 18. In comparing these with plots 3 and 4, 5, 7, and 10, it would be more accurate to confine attention to high yielding periods, at which the disturbing causes are at their minimum. It is believed that the deviations from the smooth curves, which have been freed, for the most part, from the effects of exhaustion and weeds, form statistically homogeneous material for the study of meteorological effects.},
author = {Fisher, R.A},
doi = {10.1017/S0021859600003750},
issn = {0021-8596},
journal = {The Journal of Agricultural Science},
month = {apr},
number = {6},
pages = {107--135},
publisher = {Cambridge University Press},
title = {{Studies in Crop Variation. I. An examination of the yield of dressed grain from broadbalk}},
url = {http://www.journals.cambridge.org/abstract{\_}S0021859600003750},
volume = {XI},
year = {1921}
}
@article{GoutisRobert1998,
author = {Goutis, Constantinos and Robert, Christian P.},
journal = {Biometrika},
number = {1},
pages = {29--37},
title = {{Model choice in generalised linear models: A Bayesian approach via Kullback-Leibler projections}},
url = {https://academic.oup.com/biomet/article-abstract/85/1/29/238940?redirectedFrom=fulltext},
volume = {85},
year = {1998}
}
@article{Perugini2014,
abstract = {An essential first step in planning a confirmatory or a replication study is to determine the sample size necessary to draw statistically reliable inferences using power analysis. A key problem, however, is that what is available is the sample-size estimate of the effect size, and its use can lead to severely underpowered studies when the effect size is overestimated. As a potential remedy, we introduce safeguard power analysis, which uses the uncertainty in the estimate of the effect size to achieve a better likelihood of correctly identifying the population effect size. Using a lower-bound estimate of the effect size, in turn, allows researchers to calculate a sample size for a replication study that helps protect it from being underpowered. We show that in most common instances, compared with nominal power, safeguard power is higher whereas standard power is lower. We additionally recommend the use of safeguard power analysis to evaluate the strength of the evidence provided by the original study.},
author = {Perugini, Marco and Gallucci, Marcello and Costantini, Giulio},
doi = {10.1177/1745691614528519},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Perugini, Gallucci, Costantini - 2014 - Safeguard Power as a Protection Against Imprecise Power Estimates.pdf:pdf},
issn = {1745-6924},
journal = {Perspectives on Psychological Science},
keywords = {power analysis,replicability,safeguard},
month = {may},
number = {3},
pages = {319--32},
pmid = {26173267},
publisher = {SAGE Publications Inc.},
title = {{Safeguard Power as a Protection Against Imprecise Power Estimates.}},
volume = {9},
year = {2014}
}
@book{privitera_statistics_2015,
address = {Los Angeles},
annote = {Introduction to statistics -- Summarizing data : frequency distributions in tables and graphs -- Summarizing data : central tendency -- Summarizing data : variability -- Probability -- Probability, normal distributions, and z scores -- Probability and sampling distributions -- Hypothesis testing : significance, effect size, and power -- Testing means : one-sample and two-independent sample t tests -- Testing means : related samples t test -- Estimation and confidence intervals -- Analysis of variance : one-way between-subjects design -- Analysis of variance : one-way within-subjects (repeated measures) design -- Analysis of variance : two-way between-subjects factorial design -- Correlation -- Linear regression and multiple regression -- Nonparametric tests : chi-square tests -- Nonparametric tests : tests for ordinal data},
author = {Privitera, Gregory J},
edition = {Second edi},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Privitera - 2015 - Statistics for the behavioral sciences.pdf:pdf},
isbn = {978-1-4522-8690-7},
keywords = {Psychology,Social sciences,Statistical methods},
pages = {724},
publisher = {SAGE},
title = {{Statistics for the behavioral sciences}},
year = {2015}
}
@article{Spirtes1998,
abstract = {A linear structural equation model (SEM) without free parameters has two parts: a probability distribution and an associated path diagram corresponding to the causal relations among variables specified by the structural equations and the correlations among the error terms. This article shows how path diagrams can be used to solve a number of important problems in structural equation modeling; for example, How much do sample data underdetermine the correct model specification? Given that there are equivalent models, is it possible to extract the features common to those models? When a modeler draws conclusions about coefficients in an unknown underlying SEM from a multivariate regression, precisely what assumptions are being made about the SEM? The authors explain how the path diagram provides much more than heuristics for special cases; the theory of path diagrams helps to clarify several of the issues just noted.},
author = {Spirtes, Peter and Richardson, Thomas and Meek, Christopher and Scheines, Richard and Glymour, Clark},
doi = {10.1177/0049124198027002003},
issn = {00491241},
journal = {Sociological Methods and Research},
month = {nov},
number = {2},
pages = {182--225},
publisher = {SAGE Publications Inc.},
title = {{Using path diagrams as a structural equation modeling tool}},
url = {http://journals.sagepub.com/doi/10.1177/0049124198027002003},
volume = {27},
year = {1998}
}
@article{Rosenthal2005,
author = {Rosenthal, Jeffrey S.},
journal = {Web archive: http://probability.ca/hastings/ (accessed at 01/03/2021)},
publisher = {http://probability.ca/hastings/},
title = {{W.K. Hastings, Statistician and Developer of the Metropolis-Hastings Algorithm}},
url = {http://probability.ca/hastings/},
year = {2005}
}
@book{James2017,
abstract = {Includes index. An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra. Introduction -- Statistical Learning -- Linear Regression -- Classification -- Resampling Methods -- Linear Model Selection and Regularization -- Moving Beyond Linearity -- Tree-Based Methods -- Support Vector Machines -- Unsupervised Learning.},
author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1007/978-1-4614-7138-7},
isbn = {978-1-4614-7137-0},
publisher = {Springer-Verlag New York},
title = {{An introduction to statistical learning : with applications in R}},
year = {2017}
}
@book{Lesaffre2009,
abstract = {Esthesioneuroblastoma is a rare tumor, which in many cases is diagnosed at an advanced stage with an high recurrence rate and incidence of metastases. Regionary metastases predict a poor prognosis. There is no standard therapy approach for these tumors. The most widly accepted primary therapy is radical craniofacial enbloc resection followed by radiation therapy. Today chemotherapy is getting more important and is administered with curative intention. Multidisciplinary management results in significantly longer survival in advanced tumor stages and recurrence. A clinical staging system as well as histopathological grading according of Hyams could be from importance for selection and timing of the different therapeutic modalities. We present a case of a 34-year-old female patient who was diagnosed with an advanced olfactory neuroblastoma of the upper nasal cavity with bilateral cervical lymph node metastasis (modified Kadish-stage D). Craniofacial resection and bilateral neck dissection was performed, followed by postoperative radiotherapy. Reviewing the recent literature the different therapeutic approaches are compared and discussed.},
author = {Lesaffre, Emmanuel. and Feine, Jocelyne S. and Leroux, Brian G. and Declerck, Dominique},
booktitle = {Statistical and Methodological Aspects of Oral Health Research},
doi = {10.1002/9780470744116},
isbn = {9780470744116},
issn = {13557610},
pages = {1--390},
pmid = {15612608},
publisher = {John Wiley {\&} Sons},
title = {{Statistical and Methodological Aspects of Oral Health Research}},
year = {2009}
}
@article{Hoijtink2019,
abstract = {Learning about hypothesis evaluation using the Bayes factor could enhance psychological research. In contrast to null-hypothesis significance testing it renders the evidence in favor of each of the hypotheses under consideration (it can be used to quantify support for the null-hypothesis) instead of a dichotomous reject/do-not-reject decision; it can straightforwardly be used for the evaluation of multiple hypotheses without having to bother about the proper manner to account for multiple testing; and it allows continuous reevaluation of hypotheses after additional data have been collected (Bayesian updating). This tutorial addresses researchers considering to evaluate their hypotheses by means of the Bayes factor. The focus is completely applied and each topic discussed is illustrated using Bayes factors for the evaluation of hypotheses in the context of an ANOVA model, obtained using the R package bain. Readers can execute all the analyses presented while reading this tutorial if they download bain and the R-codes used. It will be elaborated in a completely nontechnical manner: what the Bayes factor is, how it can be obtained, how Bayes factors should be interpreted, and what can be done with Bayes factors. After reading this tutorial and executing the associated code, researchers will be able to use their own data for the evaluation of hypotheses by means of the Bayes factor, not only in the context of ANOVA models, but also in the context of other statistical models.},
author = {Hoijtink, Herbert and Mulder, Joris and van Lissa, Caspar and Gu, Xin},
doi = {10.1037/met0000201},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hoijtink et al. - 2019 - A Tutorial on Testing Hypotheses Using the Bayes Factor.pdf:pdf},
issn = {1082989X},
journal = {Psychological Methods},
keywords = {Bain,Bayes Factor,Bayesian error probabilities,Informative hypotheses,Posterior probabilities},
number = {5},
pages = {539--556},
pmid = {30742472},
title = {{A Tutorial on Testing Hypotheses Using the Bayes Factor}},
volume = {24},
year = {2019}
}
@book{Douven2015,
abstract = {Conditionals are sentences of the form 'If A, then B', and they play a central role in scientific, logical, and everyday reasoning. They have been in the philosophical limelight for centuries, and more recently, they have been receiving attention from psychologists, linguists, and computer scientists. In spite of this, many key questions concerning conditionals remain unanswered. While most of the work on conditionals has addressed semantical questions - questions about the truth conditions of conditionals - this book focuses on the main epistemological questions that conditionals give rise to, such as: what are the probabilities of conditionals? When is a conditional acceptable or assertable? What do we learn when we receive new conditional information? In answering these questions, this book combines the formal tools of logic and probability theory with the experimental approach of cognitive psychology. It will be of interest to students and researchers in logic, epistemology, and psychology of reasoning.},
author = {Douven, Igor},
booktitle = {The Epistemology of Indicative Conditionals},
doi = {10.1017/cbo9781316275962},
publisher = {Cambridge University Press},
title = {{The Epistemology of Indicative Conditionals}},
year = {2015}
}
@article{Hubbard2004,
abstract = {Confusion over the reporting and interpretation of results of commonly employed classical statistical tests is recorded in a sample of 1,645 papers from 12 psychology journals for the period 1990 through 2002. The confusion arises because researchers mistakenly believe that their interpretation is guided by a single unified theory of statistical inference. But this is not so: classical statistical testing is a nameless amalgamation of the rival and often contradictory approaches developed by Ronald Fisher, on the one hand, and Jerzy Neyman and Egon Pearson, on the other. In particular, there is extensive failure to acknowledge the incompatibility of Fisher's evidential pvalue with the Type I error rate, •, of Neyman–Pearson statistical orthodoxy. The distinction between evidence (p's) and errors (•'s) is not trivial. Rather, it reveals the basic differences underlying Fisher's ideas on significance testing and inductive inference, and Neyman–Pearson views on hypothesis testing and inductive behavior. So complete is this misunderstanding over measures of evidence versus error that it is not viewed as even being a problem among the vast majority of researchers and other relevant parties. These include the APA Task Force on Statistical Inference, and those writing the guidelines concerning statistical testing mandated in APA Publication Manuals. The result is that, despite supplanting Fisher's significance-testing paradigm some fifty years or so ago, recognizable applications of Neyman–Pearson theory are few and far between in psychology's empirical literature. On the other hand, Fisher's influence is ubiquitous. {\textcopyright} 2004, Sage Publications. All rights reserved.},
author = {Hubbard, Raymond},
doi = {10.1177/0959354304043638},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hubbard - 2004 - Alphabet Soup.pdf:pdf},
issn = {0959-3543},
journal = {Theory {\&} Psychology},
keywords = {Fisher,Neyman–Pearson,hybrid statistical model,inductive behavior,inductive inference},
number = {3},
pages = {295--327},
publisher = {Sage PublicationsSage CA: Thousand Oaks, CA},
title = {{Alphabet Soup}},
volume = {14},
year = {2004}
}
@article{Begley2015,
abstract = {Medical and scientific advances are predicated on new knowledge that is robust and reliable and that serves as a solid foundation on which further advances can be built. In biomedical research, we are in the midst of a revolution with the generation of new data and scientific publications at a previously unprecedented rate. However, unfortunately, there is compelling evidence that the majority of these discoveries will not stand the test of time. To a large extent, this reproducibility crisis in basic and preclinical research may be as a result of failure to adhere to good scientific practice and the desperation to publish or perish. This is a multifaceted, multistakeholder problem. No single party is solely responsible, and no single solution will suffice. Here we review the reproducibility problems in basic and preclinical biomedical research, highlight some of the complexities, and discuss potential solutions that may help improve research quality and reproducibility.},
author = {Begley, C. Glenn and Ioannidis, John P.A.},
doi = {10.1161/CIRCRESAHA.114.303819},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Begley, Ioannidis - 2015 - Reproducibility in science Improving the standard for basic and preclinical research.pdf:pdf},
isbn = {1524-4571 (Electronic)$\backslash$r0009-7330 (Linking)},
issn = {15244571},
journal = {Circulation Research},
keywords = {funding,journals,research integrity,universities},
month = {jan},
number = {1},
pages = {116--126},
pmid = {25552691},
title = {{Reproducibility in science: Improving the standard for basic and preclinical research}},
url = {http://circres.ahajournals.org/cgi/doi/10.1161/CIRCRESAHA.114.303819},
volume = {116},
year = {2015}
}
@book{Mayo2018,
abstract = {Mounting failures of replication in social and biological sciences give a new urgency to critically appraising proposed reforms. This book pulls back the cover on disagreements between experts charged with restoring integrity to science. It denies two pervasive views of the role of probability in inference: to assign degrees of belief, and to control error rates in a long run. If statistical consumers are unaware of assumptions behind rival evidence reforms, they can't scrutinize the consequences that affect them (in personalized medicine, psychology, etc.). The book sets sail with a simple tool: if little has been done to rule out flaws in inferring a claim, then it has not passed a severe test. Many methods advocated by data experts do not stand up to severe scrutiny and are in tension with successful strategies for blocking or accounting for cherry picking and selective reporting. Through a series of excursions and exhibits, the philosophy and history of inductive inference come alive. Philosophical tools are put to work to solve problems about science and pseudoscience, induction and falsification.},
address = {Cambridge},
author = {Mayo, Deborah G.},
doi = {10.1017/9781107286184},
isbn = {9781107054134},
pages = {486},
publisher = {Cambridge University Press},
title = {{Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars}},
year = {2018}
}
@article{Brooks1997,
author = {Brooks, S. P. and Dellaportas, P. and Roberts, G. O.},
doi = {10.1080/10618600.1997.10474741},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
month = {sep},
number = {3},
pages = {251--265},
title = {{An Approach to Diagnosing Total Variation Convergence of MCMC Algorithms}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10618600.1997.10474741},
volume = {6},
year = {1997}
}
@incollection{Sprenger2016,
address = {Oxford},
author = {Sprenger, Jan},
booktitle = {Oxford Handbook of Probability and Philosophy},
editor = {H{\'{a}}jek, A. and Hitchcock, C.},
publisher = {Oxford University Press},
title = {{Bayesianism vs. Frequentism in Statistical Inference}},
year = {2016}
}
@book{hannigan2010,
abstract = {SUMMARY: This book is a collaboration from leading figures in statistical education and is designed primarily for academic audiences involved in teaching statistics and mathematics. The book is divided in four sections: (1) Assessment using real-world problems, (2) Assessment statistical thinking, (3) Individual assessment (4) Successful assessment strategies.},
author = {Hannigan, Ailish},
booktitle = {Assessment Methods in Statistical Education: An International Perspective},
doi = {10.1002/9780470710470.ch15},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hannigan - 2010 - Assessment Methods in Statistical Education An International Perspective.pdf:pdf},
isbn = {9780470710470},
keywords = {Encouraging peer learning in an assessment instrum,Encouraging peer learning in assessment instrument,Group projects and assessment,Individual weighting factor (IWF),Peer feedback and self-assessment,Study groups and assessment},
pages = {181--188},
title = {{Assessment Methods in Statistical Education: An International Perspective}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84885717079{\&}partnerID=tZOtx3y1},
year = {2010}
}
@incollection{Berger1996a,
abstract = {In Bayesian model selection or hypothesis testing, it is difficult to develop default Bayes factors, since (improper) noninformative priors cannot typically be used. In developing such default Bayes factors, we feel that it is important to keep several principles in mind. The first is that the default Bayes factor should correspond, in some sense, to an actual Bayes factor with a (sensible) prior, which we call an intrinsic prior. The second principle is that such priors should be properly calibrated across models, in the sense of being “predictively matched.” These notions will be described and illustrated, primarily using examples involving the intrinsic Bayes factor, a recently proposed default Bayes factor. It will be seen that intrinsic Bayes factors seem to correspond to actual Bayes factors with proper priors, at least for nested model scenarios. The corresponding intrinsic priors are specifically given for the normal linear model.},
author = {Berger, James O. and Pericchi, Luis R.},
booktitle = {Modelling and Prediction Honoring Seymour Geisser},
doi = {10.1007/978-1-4612-2414-3_17},
pages = {276--293},
publisher = {Springer New York},
title = {{On The Justification of Default and Intrinsic Bayes Factors}},
url = {https://link.springer.com/chapter/10.1007/978-1-4612-2414-3{\_}17},
year = {1996}
}
@article{Gigerenzer2004,
abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the “null ritual” consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5{\%} significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.},
author = {Gigerenzer, Gerd},
doi = {10.1016/J.SOCEC.2004.09.033},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gigerenzer - 2004 - Mindless statistics.pdf:pdf},
issn = {1053-5357},
journal = {The Journal of Socio-Economics},
number = {5},
pages = {587--606},
publisher = {North-Holland},
title = {{Mindless statistics}},
volume = {33},
year = {2004}
}
@article{Fisher1924,
author = {Fisher, Ronald Aylmer},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1924 - On a Distribution Yielding the Error Functions of Several Well Known Statistics.pdf:pdf},
journal = {Proceedings of the International Congress of Mathematics},
number = {2},
pages = {805--813},
title = {{On a Distribution Yielding the Error Functions of Several Well Known Statistics.}},
url = {https://digital.library.adelaide.edu.au/dspace/handle/2440/15183},
year = {1924}
}
@article{Trafimow2019a,
abstract = {ABSTRACTAlthough the null hypothesis significance testing procedure is problematic, many still favor the use of p-values as indicating the state of evidence against the model used to generate the p...},
author = {Trafimow, David},
doi = {10.1080/13645579.2019.1610592},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Trafimow - 2019 - A taxonomy of model assumptions on which P is based and implications for added benefit in the sciences.pdf:pdf},
issn = {1364-5579},
journal = {International Journal of Social Research Methodology},
keywords = {Theoretical assumptions,auxiliary assumptions,inferential assumptions,model assumptions,p-values,statistical assumptions,taxonomy of assumptions},
month = {nov},
number = {6},
pages = {571--583},
publisher = {Routledge},
title = {{A taxonomy of model assumptions on which P is based and implications for added benefit in the sciences}},
url = {https://www.tandfonline.com/doi/full/10.1080/13645579.2019.1610592},
volume = {22},
year = {2019}
}
@article{Kelter2020d,
abstract = {Objectives: The data presented herein represents the simulated datasets of a recently conducted larger study which investigated the behaviour of Bayesian indices of significance and effect size as alternatives to traditional p-values. The study considered the setting of Student's and Welch's two-sample t-test often used in medical research. It investigated the influence of the sample size, noise, the selected prior hyperparameters and the sensitivity to type I errors. The posterior indices used included the Bayes factor, the region of practical equivalence, the probability of direction, the MAP-based p-value and the e-value in the Full Bayesian Significance Test. The simulation study was conducted in the statistical programming language R. Data description: The R script files for simulation of the datasets used in the study are presented in this article. These script files can both simulate the raw datasets and run the analyses. As researchers may be faced with different effect sizes, noise levels or priors in their domain than the ones studied in the original paper, the scripts extend the original results by allowing to recreate all analyses of interest in different contexts. Therefore, they should be relevant to other researchers.},
author = {Kelter, Riko},
doi = {10.1186/s13104-020-05291-z},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kelter - 2020 - Simulation data for the analysis of Bayesian posterior significance and effect size indices for the two-sample t-test(2).pdf:pdf},
issn = {17560500},
journal = {BMC Research Notes},
keywords = {Bayesian Biostatistics,Bayesian hypothesis testing,Bayesian significance and effect measures,Student's two-sample t-test,Welch's two-sample t-test},
month = {sep},
number = {452},
pmid = {32962722},
publisher = {BioMed Central Ltd},
title = {{Simulation data for the analysis of Bayesian posterior significance and effect size indices for the two-sample t-test to support reproducible medical research}},
volume = {13},
year = {2020}
}
@article{Brard2017,
abstract = {Background Bayesian statistics are an appealing alternative to the traditional frequentist approach to designing, analysing, and reporting of clinical trials, especially in rare diseases. Time-to-event endpoints are widely used in many medical fields. There are additional complexities to designing Bayesian survival trials which arise from the need to specify a model for the survival distribution. The objective of this article was to critically review the use and reporting of Bayesian methods in survival trials. Methods A systematic review of clinical trials using Bayesian survival analyses was performed through PubMed and Web of Science databases. This was complemented by a full text search of the online repositories of pre-selected journals. Cost-effectiveness, dose-finding studies, meta-analyses, and methodological papers using clinical trials were excluded. Results In total, 28 articles met the inclusion criteria, 25 were original reports of clinical trials and 3 were re-analyses of a clinical trial. Most trials were in oncology (n = 25), were randomised controlled (n = 21) phase III trials (n = 13), and half considered a rare disease (n = 13). Bayesian approaches were used for monitoring in 14 trials and for the final analysis only in 14 trials. In the latter case, Bayesian survival analyses were used for the primary analysis in four cases, for the secondary analysis in seven cases, and for the trial re-analysis in three cases. Overall, 12 articles reported fitting Bayesian regression models (semi-parametric, n = 3; parametric, n = 9). Prior distributions were often incompletely reported: 20 articles did not define the prior distribution used for the parameter of interest. Over half of the trials used only non-informative priors for monitoring and the final analysis (n = 12) when it was specified. Indeed, no articles fitting Bayesian regression models placed informative priors on the parameter of interest. The prior for the treatment effect was based on historical data in only four trials. Decision rules were pre-defined in eight cases when trials used Bayesian monitoring, and in only one case when trials adopted a Bayesian approach to the final analysis. Conclusion Few trials implemented a Bayesian survival analysis and few incorporated external data into priors. There is scope to improve the quality of reporting of Bayesian methods in survival trials. Extension of the Consolidated Standards of Reporting Trials statement for reporting Bayesian clinical trials is recommended.},
author = {Brard, Caroline and {Le Teuff}, Gw{\'{e}}na{\"{e}}l and {Le Deley}, Marie-C{\'{e}}cile and Hampson, Lisa V},
doi = {10.1177/1740774516673362},
issn = {1740-7745},
journal = {Clinical Trials: Journal of the Society for Clinical Trials},
keywords = {Bayesian,clinical trial,posterior distribution,prior distribution,survival modelling,systematic review,time-to-event},
month = {feb},
number = {1},
pages = {78--87},
pmid = {27729499},
title = {{Bayesian survival analysis in clinical trials: What methods are used in practice?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27729499 http://journals.sagepub.com/doi/10.1177/1740774516673362},
volume = {14},
year = {2017}
}
@unpublished{kelter2018_rome,
address = {Rome Conference in Mathematics, Academia Tedesca Roma Villa Massimo, Rome},
author = {Kelter, Riko},
title = {{Markov-Chain-Monte-Carlo-Algorithms and ML-Inference: A comparison based on a medical simulation study}},
year = {2018}
}
@misc{SPSS2017,
address = {Armonk, NY},
author = {{IBM Corp.}},
publisher = {IBM Corp.},
title = {{IBM SPSS Statistics for Macintosh}},
year = {2017}
}
@article{Metz2010,
abstract = {Statistics education has become an increasingly important component of the mathematics education of today‟s citizens. In part to address the call for a more statistically literate citizenship, The Guidelines for Assessment and Instruction in Statistics Education (GAISE) were developed in 2005 by the American Statistical Association. These guidelines provide a framework for statistics education towards the end of enabling students to achieve statistical literacy, both for their personal lives and in their careers. In order to achieve statistical literacy by adulthood, statistics education must begin at the elementary school level. However, many elementary school teachers have not had the opportunity to become statistically literate themselves. In addition, they are not equipped pedagogically to provide effective instruction in statistics. This article will discuss statistical concepts that have been identified as necessary for statistical literacy and describe how an undergraduate course in Probability and Statistics for pre- service elementary and middle school teachers was revised and implemented using the GAISE framework, in conjunction with the NCTM Standards for Data Analysis and Probability. The aims of the revised course were to deepen pre-service elementary and middle school teachers‟ conceptual knowledge of statistics; to provide them with opportunities to engage in, design, and implement pedagogical strategies for teaching statistics concepts to children; and, to help them make connections between the statistical concepts they are learning and the statistical concepts they will someday teach to elementary and middle school students.},
author = {Metz, Mary Louise},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Metz - 2010 - Using GAISE and NCTM Standards as Frameworks for Teaching Probability and Statistics to Pre-Service Elementary and Middle.pdf:pdf},
isbn = {10691898 (ISSN)},
issn = {10691898},
journal = {Journal of Statistics Education},
keywords = {pedagogical strategies,preparation of teachers,statistical,statistics education},
number = {3},
pages = {1--27},
title = {{Using GAISE and NCTM Standards as Frameworks for Teaching Probability and Statistics to Pre-Service Elementary and Middle School Mathematics Teachers}},
volume = {18},
year = {2010}
}
@article{FisherRonaldAylmerSir1956,
abstract = {Given a compd., how can we effectively predict its biol. function. It is a fundamentally important problem because the information thus obtained may benefit the understanding of many basic biol. processes and provide useful clues for drug design. In this study, based on the information of chem.-chem. interactions, a novel method was developed that can be used to identify which of the following eleven metabolic pathway classes a query compd. may be involved with: (1) Carbohydrate Metab., (2) Energy Metab., (3) Lipid Metab., (4) Nucleotide Metab., (5) Amino Acid Metab., (6) Metab. of Other Amino Acids, (7) Glycan Biosynthesis and Metab., (8) Metab. of Cofactors and Vitamins, (9) Metab. of Terpenoids and Polyketides, (10) Biosynthesis of Other Secondary Metabolites, (11) Xenobiotics Biodegrdn. and Metab. It was obsd. that the overall success rate obtained by the method via the 5-fold cross-validation test on a benchmark dataset consisting of 3,137 compds. was 77.97{\%}, which is much higher than 10.45{\%}, the corresponding success rate obtained by the random guesses. Besides, to deal with the situation that some compds. may be involved with more than one metabolic pathway class, the method presented here is featured by the capacity able to provide a series of potential metabolic pathway classes ranked according to the descending order of their likelihood for each of the query compds. concerned. Furthermore, our method was also applied to predict 5,549 compds. whose metabolic pathway classes are unknown. Interestingly, the results thus obtained are quite consistent with the deductions from the reports by other investigators. It is anticipated that, with the continuous increase of the chem.-chem. interaction data, the current method will be further enhanced in its power and accuracy, so as to become a useful complementary vehicle in annotating uncharacterized compds. for their biol. functions. A dissertation. [on SciFinder(R)]},
author = {Fisher, Ronald},
doi = {10.1371/journal.pone.0029491},
issn = {1932-6203},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
pages = {56--60},
title = {{On a test of significance in Pearson's Biometrika Tables (No. 11)}},
url = {https://digital.library.adelaide.edu.au/dspace/handle/2440/15270},
year = {1956}
}
@book{Fisher1934statMethodsFifthEdition,
address = {Edinburgh},
archivePrefix = {arXiv},
arxivId = {0-05-002170-2},
author = {Fisher, Ronald Aylmer},
doi = {10.1007/978-3-319-64263-5_25},
edition = {5th},
eprint = {0-05-002170-2},
isbn = {9783319642635},
issn = {15321924},
keywords = {MGUS,Molecular markers,Monoclonal protein,Multiple myeloma,Plasma cell disorder},
pmid = {1756371},
publisher = {Oliver and Boyd LTD.},
title = {{Statistical Methods for Research Workers}},
year = {1934}
}
@article{Cohen1990,
abstract = {This is an account of what I have learned (so far) about the application of statistics to psychology and the other sociobiomedical sciences. It includes the principles "less is more" (fewer variables, more highly targeted issues, sharp rounding off), "simple is better" (graphic representation, unit weighting for linear composites), and "some things you learn aren't so." I have learned to avoid the many misconceptions that surround Fisherian null hypothesis testing. I have also learned the importance of power analysis and the determination of just how big (rather than how statistically significant) are the effects that we study. Finally, I have learned that there is no royal road to statistical induction, that the informed judgment of the investigator is the crucial element in the interpretation of data, and that things take time.},
author = {Cohen, Jacob},
doi = {10.1037/0003-066X.45.12.1304},
issn = {0003066X},
journal = {American Psychologist},
number = {12},
pages = {1304--1312},
publisher = {American Psychological Association Inc.},
title = {{Things I have learned (so far)}},
url = {/record/1991-11596-001},
volume = {45},
year = {1990}
}
@article{Evans1986a,
abstract = {Birnbaum (1962a) argued that the conditionality principle (C) and the sufficiency principle (S) implied the likelihood principle (L); he then argued (Birnbaum 1972) that C and a mathematical equivalence principle M implied L. Evans, Fraser, and Monette (1985a) gave reference details, and this paper gives proof that C alone implies L. The level of support by the profession for L is sharply less than that for S or even for C; thus the paradoxical nature of these results. In this regard, we elaborate on the Monette example (Fraser, Monette, and Ng 1984), which provides a strong case against L. We also examine closely the various proofs linking the principles and find that S and C can each be used operationally to suppress information otherwise deemed relevant. From another viewpoint this says that S and C can each be used in contexts that directly conflict with the original examples and motivations supporting them; the principles can thus be viewed as inappropriately used, or more strongly, as invalid. In either case, the result that C and S imply L or that C implies L can be regarded as noneffective in the context of discriminating applications. A resolution of the apparent anomalies can be obtained by allowing the statistical model to include ingredients additional to those usually present (particularly for subsequent use with conditionality), or alternatively by restricting the application of the principles to contexts where the conflicts would seem not to arise.},
author = {Evans, Michael J. and Fraser, Donald A. S. and Monette, Georges},
doi = {10.2307/3314794},
issn = {03195724},
journal = {Canadian Journal of Statistics},
keywords = {conditionality,content,evidential meaning,likelihood,sufficiency},
month = {sep},
number = {3},
pages = {181--194},
publisher = {Wiley},
title = {{On principles and arguments to likelihood}},
url = {http://doi.wiley.com/10.2307/3314794},
volume = {14},
year = {1986}
}
@book{HernanRobins2020WhatIf,
address = {Boca Raton},
author = {Hern{\'{a}}n, MA and Robins, JM},
publisher = {Chapman {\&} Hall/CRC},
title = {{Causal Inference: What If}},
year = {2020}
}
@article{Yuan2008,
abstract = {Traditionally, the application of Bayesian testing procedures to classical nonparametric settings has been restricted by difficulties associated with prior specification, prohibitively expensive computation, and the absence of sampling densities for data. To overcome these difficulties, we model the sampling distributions of nonparametric test statistics - rather than the sampling distributions of original data - to obtain the Bayes factors required for Bayesian hypothesis tests. We apply this methodology to construct Bayes factors from a wide class of non-parametric test statistics having limiting normal distributions and illustrate these methods with data. Finally, we consider the extension of our methodology to non-parametric test statistics having limiting X2 distributions.},
author = {Yuan, Ying and Johnson, Valen E.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Yuan, Johnson - 2008 - Bayesian hypothesis tests using nonparametric statistics.pdf:pdf},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {Bayes factor,Kruskal-wallis test,Logrank test,Mann-Whitney-Wilcoxon test,Nonparametric hypothesis test,Wilcoxon signed rank test},
number = {3},
pages = {1185--1200},
title = {{Bayesian hypothesis tests using nonparametric statistics}},
volume = {18},
year = {2008}
}
@book{kraehmer2011,
author = {Kr{\"{a}}hmer, Grohmann;},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kr{\"{a}}hmer - 2011 - Statistik in Deutschland 100 Jahre Deutsche Statistische Gesellschaft.pdf:pdf},
isbn = {9783642156342},
pages = {252},
pmid = {16727806},
title = {{Statistik in Deutschland: 100 Jahre Deutsche Statistische Gesellschaft}},
year = {2011}
}
@article{Ly2016a,
abstract = {Our original article provided a relatively detailed summary of Harold Jeffreys's philosophy on statistical hypothesis testing. In response, Robert (2016) maintains that Bayes factors have a number of serious shortcomings. These shortcomings, Robert argues, may be addressed by an alternative approach that conceptualizes model selection as parameter estimation in a mixture model. In a second comment, Chandramouli and Shiffrin (2016) seek to extend Jeffreys's framework by also taking into consideration data distributions that do not originate from either of the models under test. In this rejoinder we argue that Robert's (2016) alternative view on testing has more in common with Jeffreys's Bayes factor than he suggests, as they share the same "shortcomings". On the other hand, we show that the proposition of Chandramouli and Shiffrin (2016) to extend the Bayes factor is in fact further removed from Jeffreys's view on testing than the authors suggest. By elaborating on these points, we hope to clarify our case for Jeffreys's Bayes factors.},
author = {Ly, Alexander and Verhagen, Josine and Wagenmakers, Eric-Jan},
doi = {10.1016/j.jmp.2016.01.003},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ly, Verhagen, Wagenmakers - 2016 - An evaluation of alternative methods for testing hypotheses, from the perspective of Harold Jeffreys.pdf:pdf},
issn = {10960880},
journal = {Journal of Mathematical Psychology},
keywords = {Bayes factors,Induction,Model selection,Replication,Statistical evidence},
pages = {43--55},
publisher = {Elsevier Inc.},
title = {{An evaluation of alternative methods for testing hypotheses, from the perspective of Harold Jeffreys}},
volume = {72},
year = {2016}
}
@article{georgantaki_using_2007,
abstract = {Using Educational Tools for Teaching Object Oriented Design and Programming},
author = {Georgantaki, Stavroula},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Georgantaki - 2007 - Using Educational Tools for Teaching Object Oriented Design and Programming.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Georgantaki - 2007 - Using Educational Tools for Teaching Object Oriented Design and Programming.html:html},
title = {{Using Educational Tools for Teaching Object Oriented Design and Programming}},
url = {http://www.academia.edu/3687655/Using{\_}Educational{\_}Tools{\_}for{\_}Teaching{\_}Object{\_}Oriented{\_}Design{\_}and{\_}Programming},
year = {2007}
}
@incollection{Salmon1988,
address = {Dordrecht},
author = {Salmon, Wesley C.},
booktitle = {Probability and Causality},
doi = {10.1007/978-94-009-3997-4_1},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Salmon - 1988 - Dynamic Rationality Propensity, Probability, and Credence.pdf:pdf},
pages = {3--40},
publisher = {Springer Netherlands},
title = {{Dynamic Rationality: Propensity, Probability, and Credence}},
url = {http://link.springer.com/10.1007/978-94-009-3997-4{\_}1},
year = {1988}
}
@article{Johnson1989,
abstract = {In this and two companion papers, we report on an extended empirical study of the simulated annealing approach to combinatorial optimization proposed by S. Kirkpatrick et al. That study investigated how best to adapt simulated annealing to particular problems and compared its performance to that of more traditional algorithms. This paper (Part I) discusses annealing and our parameterized generic implementation of it, describes how we adapted this generic algorithm to the graph partitioning problem, and reports how well it compared to standard algorithms like the Kernighan-Lin algorithm. (For sparse random graphs, it tended to outperform Kernighan-Lin as the number of vertices become large, even when its much greater running time was taken into account. It did not perform nearly so well, however, on graphs generated with a built-in geometric structure.) We also discuss how we went about optimizing our implementation, and describe the effects of changing the various annealing parameters or varying the basic...},
author = {Johnson, David S. and Aragon, Cecilia R. and McGeoch, Lyle A. and Schevon, Catherine},
doi = {10.1287/opre.37.6.865},
issn = {0030-364X},
journal = {Operations Research},
keywords = {networks/graphs, heuristics: algorithms for graph,simulation, applications: optimization by simulate},
month = {dec},
number = {6},
pages = {865--892},
publisher = { INFORMS },
title = {{Optimization by Simulated Annealing: An Experimental Evaluation; Part I, Graph Partitioning}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.37.6.865},
volume = {37},
year = {1989}
}
@article{Matthews2019a,
abstract = {It is now widely accepted that the techniques of null hypothesis significance testing (NHST) are routinely misused and misinterpreted by researchers seeking insight from data. There is, however, no consensus on acceptable alternatives, leaving researchers with little choice but to continue using NHST, regardless of its failings. I examine the potential for the Analysis of Credibility (AnCred) to resolve this impasse. Using real-life examples, I assess the ability of AnCred to provide researchers with a simple but robust framework for assessing study findings that goes beyond the standard dichotomy of statistical significance/nonsignificance. By extracting more insight from standard summary statistics while offering more protection against inferential fallacies, AnCred may encourage researchers to move toward the post p {\textless} 0.05 era. ARTICLE HISTORY},
author = {Matthews, Robert A. J.},
doi = {10.1080/00031305.2018.1543136},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Matthews - 2019 - Moving Towards the Post ipi {\&}lt 0.05 Era via the Analysis of Credibility.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {analysis of credibility,bayesian inference,hypothesis significance,null},
number = {sup1},
pages = {202--212},
title = {{Moving Towards the Post {\textless}i{\textgreater}p{\textless}/i{\textgreater}  {\textless} 0.05 Era via the Analysis of Credibility}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1543136},
volume = {73},
year = {2019}
}
@article{Student1908a,
author = {Student},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Student - 1908 - The Probable Error of a Mean.pdf:pdf},
journal = {Biometrika},
number = {1},
pages = {1--25},
title = {{The Probable Error of a Mean}},
volume = {6},
year = {1908}
}
@article{Zumbo2016,
author = {Zumbo, Bruno D and Kroc, Edward},
doi = {10.22237/jmasm/1478001780},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Zumbo, Kroc - 2016 - Some Remarks on Rao and Lovric's 'Testing Point Null Hypothesis of a Normal Mean and the Truth 21st Century Perspec.pdf:pdf},
journal = {Journal of Modern Applied Statistical Methods},
number = {2},
pages = {11--2016},
title = {{Some Remarks on Rao and Lovric's 'Testing Point Null Hypothesis of a Normal Mean and the Truth: 21st Century Perspective'}},
url = {http://digitalcommons.wayne.edu/jmasmhttp://digitalcommons.wayne.edu/jmasm/vol15/iss2/5},
volume = {15},
year = {2016}
}
@article{Ly2019,
abstract = {We describe a general method that allows experimenters to quantify the evidence from the data of a direct replication attempt given data already acquired from an original study. These so-called replication Bayes factors are a reconceptualization of the ones introduced by Verhagen and Wagenmakers (Journal of Experimental Psychology: General, 143(4), 1457–1475 2014) for the common t test. This reconceptualization is computationally simpler and generalizes easily to most common experimental designs for which Bayes factors are available.},
author = {Ly, Alexander and Etz, Alexander and Marsman, Maarten and Wagenmakers, Eric Jan},
doi = {10.3758/s13428-018-1092-x},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ly et al. - 2019 - Replication Bayes factors from evidence updating.pdf:pdf},
issn = {15543528},
journal = {Behavior Research Methods},
keywords = {Evidence synthesis,Hypothesis testing,Meta-analysis,Replication},
month = {dec},
number = {6},
pages = {2498--2508},
publisher = {Springer},
title = {{Replication Bayes factors from evidence updating}},
url = {https://link.springer.com/article/10.3758/s13428-018-1092-x},
volume = {51},
year = {2019}
}
@article{Robert2014,
abstract = {This article discusses the dual interpretation of the Jeffreys-Lindley paradox associated with Bayesian posterior probabilities and Bayes factors, both as a differentiation between frequentist and Bayesian statistics and as a pointer to the difficulty of using improper priors while testing. I stress the considerable impact of this paradox on the foundations of both classical and Bayesian statistics. While assessing existing resolutions of the paradox, I focus on a critical viewpoint of the paradox discussed by Spanos in Philosophy of Science. {\textcopyright} 2014 by the Philosophy of Science Association. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1303.5973},
author = {Robert, Christian P.},
doi = {10.1086/675729},
eprint = {1303.5973},
issn = {00318248},
journal = {Philosophy of Science},
number = {2},
pages = {216--232},
publisher = {University of Chicago Press},
title = {{On the Jeffreys-Lindley paradox}},
url = {https://www.journals.uchicago.edu/doi/abs/10.1086/675729},
volume = {81},
year = {2014}
}
@article{Birnbaum1972,
abstract = {A self-contained account is given of the implications among the sufficiency, conditionality and likelihood axioms of statistical evidence for the discrete case. These include a previously unpublished derivation of sufficiency from conditionality. The nondiscrete case is discussed with reference to the same relation, and to the significance of nonunique determination of density and likelihood functions. The writer's current views on this problem area are indicated briefly. {\textcopyright} Taylor {\&} Francis Group, LLC.},
author = {Birnbaum, Allan},
doi = {10.1080/01621459.1972.10481306},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Birnbaum - 1972 - More on concepts of statistical evidence.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
number = {340},
pages = {858--861},
title = {{More on concepts of statistical evidence}},
volume = {67},
year = {1972}
}
@article{Meng1994,
abstract = {Extending work of Rubin, this paper explores a Bayesian counterpart$\backslash$nof the classical p-value, namely, a tail-area probability of a "test$\backslash$nstatistic" under a null hypothesis. The Bayesian formulation, using$\backslash$nposterior predictive$\backslash$n$\backslash$nreplications of the data, allows a "test statistic" to depend on both$\backslash$ndata and unknown (nuisance) parameters and thus permits a direct$\backslash$nmeasure of the discrepancy between sample and population quantities.$\backslash$nThe tail-area probability for a "test statistic" is then found under$\backslash$nthe joint posterior distribution of replicate data and the (nuisance)$\backslash$nparameters, both conditional on the null hypothesis. This posterior$\backslash$npredictive p-value can also be viewed as the posterior mean of a$\backslash$nclassical p-value, averaging over the posterior distribution of (nuisance)$\backslash$nparameters under the null hypothesis, and thus it provides one general$\backslash$nmethod for dealing with nuisance parameters. Two classical examples,$\backslash$nincluding the Behrens-Fisher problem, are used to illustrate the$\backslash$n$\backslash$nposterior predictive p-value and some of its interesting properties,$\backslash$nwhich also reveal a new (Bayesian) interpretation for some classical$\backslash$np-values. An application to multiple-imputation inference is also$\backslash$npresented. A frequency evaluation shows that, in general, if the$\backslash$nreplication is defined by new (nuisance) parameters and new data,$\backslash$nthen the Type I frequentist error of an a-level posterior predictive$\backslash$ntest is often close to but less than a and will never exceed 2a.},
author = {Meng, Xiao-Li},
doi = {10.1214/aos/1176325622},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {{\$}p{\$}-value,62A99,62F03,Bayesian {\$}p{\$}-value,Behrens-Fisher problem,Discrepancy,Type I error,multiple imputation,nuisance parameter,pivot,significance level,tail-area probability,test variable},
number = {3},
pages = {1142--1160},
publisher = {Institute of Mathematical Statistics},
title = {{Posterior Predictive p-Values}},
url = {http://projecteuclid.org/euclid.aos/1176325622},
volume = {22},
year = {1994}
}
@inproceedings{Kelter2018a,
abstract = {The object-oriented programming (OOP) paradigm is quite prominent in German secondary schools. To challenge and overcome possible diiculties in the learning process it is vital for educators to have knowledge about possible (mis-)conceptions. Traditionally, these are gathered by investigating the mental models of students, e.g. towards object-orientation. While on the one side lots of misconceptions could not be reproduced in replication studies, on the other side most often students are asked, while teachers could provide an overview on one or several courses. To tackle both aspects at once, this paper describes the investigation of teachers views on occurring student misconceptions regarding OOP in their lessons. Therefore misconceptions were gathered from literature and were condensed into a survey. The answers of 79 teachers are analysed regarding the frequency with which teachers register misconceptions, which of those are possibly new and by itting linear and quadratic regression models it is investigated, which external factors, such as teaching approach, work experience or educational degree, might inluence the perceived frequency of registered misconceptions. All aspects show promising results for further investigations towards the research of misconceptions in OOP.},
address = {New York, New York, USA},
author = {Kelter, Riko and Kramer, Matthias and Brinda, Torsten},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3279720.3279727},
isbn = {9781450365352},
keywords = {Frequency-analysis of misconceptions in programmin,Misconceptions in object-orientedprogramming,Object-oriented-programming,Regression models for OOP misconceptions},
month = {nov},
pages = {1--10},
publisher = {Association for Computing Machinery},
title = {{Statistical frequency-analysis of misconceptions in object-oriented-programming - Regularized PCR models for frequency analysis across OOP concepts and related factors}},
url = {http://dl.acm.org/citation.cfm?doid=3279720.3279727},
year = {2018}
}
@article{Sutherland2017,
author = {Sutherland, SINCLAIR and Ridgway, J},
journal = {Statistics education research journal.},
number = {1},
pages = {26--30},
title = {{Interactive visualisations and statistical literacy.}},
volume = {16},
year = {2017}
}
@article{Vehtari2012,
abstract = {To date, several methods exist in the statistical literature for model assessment, which purport themselves specifically as Bayesian predictive methods. The decision theoretic assumptions on which these methods are based are not always clearly stated in the original articles, however. The aim of this survey is to provide a unified review of Bayesian predictive model assessment and selection methods, and of methods closely related to them. We review the various assumptions that are made in this context and discuss the connections between different approaches, with an emphasis on how each method approximates the expected utility of using a Bayesian model for the purpose of predicting future data. {\textcopyright} 2013 The author, under a Creative Commons Attribution License.},
author = {Vehtari, Aki and Ojanen, Janne},
doi = {10.1214/14-ss105},
issn = {1935-7516},
journal = {Statistics Surveys},
keywords = {62-02,62C10Bayesian,and phrases,bayesian,cross-validation,decision theory,expected utility,information cri-,information criteria,model,model assessment,model selection,predictive,selection},
pages = {142--228},
title = {{A survey of Bayesian predictive methods for model assessment, selection and comparison}},
volume = {6},
year = {2012}
}
@article{Henderson2013,
abstract = {BACKGROUND: The vast majority of medical interventions introduced into clinical development prove unsafe or ineffective. One prominent explanation for the dismal success rate is flawed preclinical research. We conducted a systematic review of preclinical research guidelines and organized recommendations according to the type of validity threat (internal, construct, or external) or programmatic research activity they primarily address. METHODS AND FINDINGS: We searched MEDLINE, Google Scholar, Google, and the EQUATOR Network website for all preclinical guideline documents published up to April 9, 2013 that addressed the design and conduct of in vivo animal experiments aimed at supporting clinical translation. To be eligible, documents had to provide guidance on the design or execution of preclinical animal experiments and represent the aggregated consensus of four or more investigators. Data from included guidelines were independently extracted by two individuals for discrete recommendations on the design and implementation of preclinical efficacy studies. These recommendations were then organized according to the type of validity threat they addressed. A total of 2,029 citations were identified through our search strategy. From these, we identified 26 guidelines that met our eligibility criteria--most of which were directed at neurological or cerebrovascular drug development. Together, these guidelines offered 55 different recommendations. Some of the most common recommendations included performance of a power calculation to determine sample size, randomized treatment allocation, and characterization of disease phenotype in the animal model prior to experimentation. CONCLUSIONS: By identifying the most recurrent recommendations among preclinical guidelines, we provide a starting point for developing preclinical guidelines in other disease domains. We also provide a basis for the study and evaluation of preclinical research practice. Please see later in the article for the Editors' Summary.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Henderson, Valerie C. and Kimmelman, Jonathan and Fergusson, Dean and Grimshaw, Jeremy M. and Hackam, Dan G.},
doi = {10.1371/journal.pmed.1001489},
editor = {Ioannidis, John PA.},
eprint = {arXiv:1011.1669v3},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Henderson et al. - 2013 - Threats to Validity in the Design and Conduct of Preclinical Efficacy Studies A Systematic Review of Guideline.pdf:pdf},
isbn = {1549-1676},
issn = {15491277},
journal = {PLoS Medicine},
month = {jul},
number = {7},
pages = {e1001489},
pmid = {23935460},
publisher = {Public Library of Science},
title = {{Threats to Validity in the Design and Conduct of Preclinical Efficacy Studies: A Systematic Review of Guidelines for In Vivo Animal Experiments}},
url = {http://dx.plos.org/10.1371/journal.pmed.1001489},
volume = {10},
year = {2013}
}
@article{Kelter2020JORSBayest,
author = {Kelter, Riko},
doi = {https://doi.org/10.5334/jors.290},
journal = {Journal of Open Research Software},
number = {14},
title = {{bayest: An R Package for effect-size targeted Bayesian two-sample t-tests}},
volume = {8},
year = {2020}
}
@book{campe_2003,
author = {Campe, R{\"{u}}diger},
publisher = {Wallstein Verlag},
title = {{Spiel der Wahrscheinlichkeit - Literatur und Berechnung zwischen Pascal und Kleist}},
year = {2003}
}
@article{Cowles1996,
abstract = {A critical issue for users of Markov chain Monte Carlo (MCMC) methods in applications is how to determine when it is safe to stop sampling and use the samples to estimate characteristics of the distribution of interest. Research into methods of computing theoretical convergence bounds holds promise for the future but to date has yielded relatively little of practical use in applied work. Consequently, most MCMC users address the convergence problem by applying diagnostic tools to the output produced by running their samplers. After giving a brief overview of the area, we provide an expository review of 13 convergence diagnostics, describing the theoretical basis and practical implementation of each. We then compare their performance in two simple models and conclude that all of the methods can fail to detect the sorts of convergence failure that they were designed to identify. We thus recommend a combination of strategies aimed at evaluating and accelerating MCMC sampler convergence, including applying diagnostic procedures to a small number of parallel chains, monitoring autocorrelations and cross-correlations, and modifying parametrizations or sampling algorithms appropriately. We emphasize, however, that it is not possible to say with certainty that a finite sample from an MCMC algorithm is representative of an underlying stationary distribution.},
author = {Cowles, Mary Kathryn and Carlin, Bradley P.},
doi = {10.1080/01621459.1996.10476956},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Cowles, Carlin - 1996 - Markov Chain Monte Carlo Convergence Diagnostics A Comparative Review.pdf:pdf},
isbn = {0162-1459},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Autocorrelation,Gibbs sampler,Metropolis-Hastings algorithm},
number = {434},
pages = {883--904},
pmid = {25630846},
title = {{Markov Chain Monte Carlo Convergence Diagnostics: A Comparative Review}},
volume = {91},
year = {1996}
}
@book{Zellner1980a,
address = {Amsterdam},
author = {Zellner, Arnold},
isbn = {0444852700},
pages = {474},
publisher = {Elsevier North-Holland},
title = {{Bayesian analysis in econometrics and statistics : Essays in honor of Harold Jeffreys}},
year = {1980}
}
@incollection{Edwards1993,
address = {Boston, MA},
author = {Edwards, J. H.},
booktitle = {Human Population Genetics: A Centennial Tribute to J.B.S. Haldane},
doi = {10.1007/978-1-4615-2970-5_11},
editor = {Majumder, Partha P.},
pages = {153--164},
publisher = {Springer},
title = {{Haldane and the Analysis of Linkage}},
url = {http://link.springer.com/10.1007/978-1-4615-2970-5{\_}11},
year = {1993}
}
@misc{fisher1938,
address = {Edinburgh},
author = {Fisher, Ronald A.},
booktitle = {The Eugenics review},
doi = {10.1038/144533a0},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1938 - Statistical tables for biological, agricultural and medical research.pdf:pdf},
isbn = {0-02-844720-4},
issn = {0028-0836},
number = {4},
publisher = {Oliver and Boyd},
title = {{Statistical tables for biological, agricultural and medical research}},
volume = {30},
year = {1938}
}
@article{Liu1994,
author = {Liu, J.S. and Wong, W.H. and Kong, A.},
journal = {Biometrika},
number = {1},
pages = {27--40},
title = {{Covariance structure of the Gibbs sampler with applications to the comparisons of estimators and augmentation schemes}},
volume = {81},
year = {1994}
}
@article{Piironen2018,
abstract = {This paper discusses predictive inference and feature selection for generalized linear models with scarce but high-dimensional data. We argue that in many cases one can benefit from a decision theoretically justified two-stage approach: first, construct a possibly non-sparse model that predicts well, and then find a minimal subset of features that characterize the predictions. The model built in the first step is referred to as the $\backslash$emph{\{}reference model{\}} and the operation during the latter step as predictive $\backslash$emph{\{}projection{\}}. The key characteristic of this approach is that it finds an excellent tradeoff between sparsity and predictive accuracy, and the gain comes from utilizing all available information including prior and that coming from the left out features. We review several methods that follow this principle and provide novel methodological contributions. We present a new projection technique that unifies two existing techniques and is both accurate and fast to compute. We also propose a way of evaluating the feature selection process using fast leave-one-out cross-validation that allows for easy and intuitive model size selection. Furthermore, we prove a theorem that helps to understand the conditions under which the projective approach could be beneficial. The benefits are illustrated via several simulated and real world examples.},
archivePrefix = {arXiv},
arxivId = {1810.02406},
author = {Piironen, Juho and Paasiniemi, Markus and Vehtari, Aki},
eprint = {1810.02406},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Piironen, Paasiniemi, Vehtari - 2018 - Projective Inference in High-dimensional Problems Prediction and Feature Selection.pdf:pdf},
journal = {arXiv preprint},
month = {oct},
title = {{Projective Inference in High-dimensional Problems: Prediction and Feature Selection}},
url = {http://arxiv.org/abs/1810.02406},
year = {2018}
}
@article{Birnbaum1962,
abstract = {The concept of conditional experimental frames of reference has a significance for the general theory of statistical inference which has been emphasized by R. A. Fisher, D. R. Cox, J. W. Tukey, and others. This concept is formulated as a principle of conditionality, from which some general consequences are deduced mathematically. These include the likelihood principle, which has not hitherto been very widely accepted, in contrast with the conditionality concept which many statisticians are inclined to accept for purposes of "informative inference." The likelihood principle states that the "evidential meaning" of experimental results is characterized fully by the likelihood function, without other reference to the structure of an experiment, in contrast with standard methods in which significance and confidence levels are based on the complete experimental model. The principal writers supporting the likelihood principle have been Fisher and G. A. Barnard, in addition to Bayesian writers for whom it represents the "directly empirical" part of their standpoint. The likelihood principle suggests certain systematic reinterpretations and revisions of standard methods, including "intrinsic significance and confidence levels" and "intrinsic standard errors," which are developed and illustrated. The close relations between non-Bayesian likelihood methods and Bayesian methods are discussed.},
author = {Birnbaum, Allan},
doi = {10.2307/2281640},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Birnbaum - 1962 - On the Foundations of Statistical Inference (with discussion).pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Birnbaum - 1962 - On the Foundations of Statistical Inference (with discussion)(2).pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {gt{\_}cmc,gts{\_}fiducial},
number = {298},
pages = {269--306},
title = {{On the Foundations of Statistical Inference (with discussion)}},
volume = {57},
year = {1962}
}
@article{Vehtari2015,
abstract = {Importance weighting is a general way to adjust Monte Carlo integration to account for draws from the wrong distribution, but the resulting estimate can be noisy when the importance ratios have a heavy right tail. This routinely occurs when there are aspects of the target distribution that are not well captured by the approximating distribution, in which case more stable estimates can be obtained by modifying extreme importance ratios. We present a new method for stabilizing importance weights using a generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios. The method, which empirically performs better than existing methods for stabilizing importance sampling estimates, includes stabilized effective sample size estimates, Monte Carlo error estimates and convergence diagnostics.},
archivePrefix = {arXiv},
arxivId = {1507.02646},
author = {Vehtari, Aki and Simpson, Daniel and Gelman, Andrew and Yao, Yuling and Gabry, Jonah},
eprint = {1507.02646},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Vehtari et al. - 2015 - Pareto Smoothed Importance Sampling.pdf:pdf},
journal = {arXiv preprint},
keywords = {bayesian computation,importance sampling,monte carlo},
number = {July},
title = {{Pareto Smoothed Importance Sampling}},
url = {http://arxiv.org/abs/1507.02646},
year = {2015}
}
@article{Healy2003,
abstract = {The paper discusses the contribution of R. A. Fisher to the theory and practice of statistics and tries to place these in their historical context.},
author = {Healy, M. J. R.},
doi = {10.1111/1467-9884.00360},
issn = {0039-0526},
journal = {Journal of the Royal Statistical Society: Series D (The Statistician)},
keywords = {Fisher,History,Inference,Statistics},
month = {oct},
number = {3},
pages = {303--310},
publisher = {Wiley/Blackwell (10.1111)},
title = {{R. A. Fisher the statistician}},
url = {http://doi.wiley.com/10.1111/1467-9884.00360 http://www.blackwell-synergy.com/links/doi/10.1111/1467-9884.00360/abs{\%}5CnC:{\%}5CDocuments and Settings{\%}5Cjorn{\%}5CDocuments{\%}5CLitteratur{\%}5CArtikelfiler{\%}5CHealy{\_}RA Fisher the statistician{\_}Statistician{\_}2003.pdf},
volume = {52},
year = {2003}
}
@book{de_vaus_surveys_2002,
address = {London},
annote = {Literaturverz. S. [367] - 373 Previous ed.: London: UCL, 1996
OCLC: 248729262},
author = {{De Vaus}, David A},
edition = {5. ed},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/De Vaus - 2002 - Surveys in social research.pdf:pdf},
isbn = {978-0-415-26857-8 978-0-415-26858-5},
keywords = {Empirische Sozialforschung,Social sciences Research Methodology,Social surveys,Umfrage},
publisher = {Routledge},
series = {Social research today},
title = {{Surveys in social research}},
year = {2002}
}
@techreport{BayarriBerger1997,
abstract = {Measures of surprise refer to quantiications of the degree of incompatibility of data with some hypothesized model H 0 without any reference to alternative models. Traditional measures of surprise have been the p-values, which are however known to grossly overestimate the evidence against H 0. Strict Bayesian analysis calls for an explicit speciication of all possible alternatives to H 0 so Bayesians have not made routine use of measures of surprise. In this report we CRITICALLY REVIEw the proposals that have been made in this regard. We propose new modiications, stress the connections with robust Bayesian analysis and discuss the choice of suitable predictive distributions which allow surprise measures to play their intended role in the presence of nuisance parameters. We recommend either the use of appropriate likelihood-ratio type measures or else the careful calibration of p-values so that they are closer to Bayesian answers.},
address = {Durham, North Carolina, USA},
author = {Bayarri, M J and Berger, James O},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bayarri, Berger - 1997 - Measures of Surprise in Bayesian Analysis.pdf:pdf},
institution = {Institute of Statistics and Decision Sciences},
keywords = {Bayesian p-values,Bayesian robustness,Conditioning,Model checking,Predictive distributions,and phrases Bayes factors},
title = {{Measures of Surprise in Bayesian Analysis}},
year = {1997}
}
@article{Walker1969,
abstract = {Let a random sample of size n be taken from a distribution having a density depending on a real parameter $\theta$, and let $\theta$ have an absolutely continuous prior distribution with density $\pi$($\theta$). We give a rigorous proof that, under suitable regularity conditions, the posterior distribution of $\theta$ will, when n tends to infinity, be asymptotically normal with mean equal to the maximum-likelihood estimator and variance equal to the reciprocal of the second derivative of the logarithm of the likelihood function evaluated at the maximum-likelihood estimator, independently of the form of $\pi$($\theta$).},
author = {Walker, A. M.},
doi = {10.1111/j.2517-6161.1969.tb00767.x},
issn = {00359246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
number = {1},
pages = {80--88},
publisher = {Wiley},
title = {{On the Asymptotic Behaviour of Posterior Distributions}},
volume = {31},
year = {1969}
}
@article{Balzarini2017,
author = {Balzarini, R. N. and Bobson, K. and Chin, K. and Campbell, L.},
journal = {Journal of Experimental Social Psychology},
pages = {191--197},
title = {{Does exposure to erotica reduce attraction and love for romantic partners in men? Independent replications of Kenrick, Gutierres, and Goldberg (1989)}},
volume = {70},
year = {2017}
}
@book{Howell2002,
abstract = {5th ed. CD-ROM contains: Data sets from text. Basic concepts -- Describing and exploring data -- The normal distribution -- Sampling distributions and hypothesis testing -- Basic concepts of probability -- Categorical data and chi-square -- Hypothesis tests applied to means -- Power -- Correlation and regression -- Alternative correlational techniques -- Simple analysis of variance -- Multiple comparisons among treatment means -- Factorial analysis of variance -- Repeated-measures designs -- Multiple regression -- Analyses of variance and covariance as general linear models -- Log-linear analysis -- Resampling and nonparametric approaches to data.},
author = {Howell, David C.},
isbn = {053437770X},
pages = {802},
publisher = {Duxbury/Thomson Learning},
title = {{Statistical methods for psychology}},
url = {https://books.google.com.ua/books/about/Statistical{\_}Methods{\_}for{\_}Psychology.html?id=Kg1-AAAAMAAJ{\&}redir{\_}esc=y},
year = {2002}
}
@article{krieger_sogenannte_2003,
author = {Krieger, Rainer},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Krieger - 2003 - Das sogenannte M{\"{u}}ndliche Eine Befragung von Beurteilten und Beurteilern zur Praxis und Problematik der Bewertung m{\"{u}}n.html:html},
issn = {00062456},
journal = {Bildung und Erziehung},
number = {Heft 1 2003},
pages = {75--92},
shorttitle = {Das sogenannte {\{}M{\"{u}}ndliche{\}}},
title = {{Das sogenannte M{\"{u}}ndliche: Eine Befragung von Beurteilten und Beurteilern zur Praxis und Problematik der Bewertung m{\"{u}}ndlicher Leistungen}},
url = {http://www.digizeitschriften.de/dms/img/?PPN=PPN509215866{\_}0056{\&}DMDID=dmdlog13},
volume = {56},
year = {2003}
}
@article{Day1969,
author = {Day, N.E.},
doi = {10.1093/biomet/56.3.463},
journal = {Biometrika},
month = {dec},
number = {3},
pages = {463--474},
publisher = {Narnia},
title = {{Estimating the components of a mixture of normal distributions}},
url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/56.3.463},
volume = {56},
year = {1969}
}
@incollection{Cochran1980,
author = {Cochran, William G.},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_4},
pages = {17--34},
publisher = {Springer, New York, NY},
title = {{Fisher and the Analysis of Variance}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}4},
year = {1980}
}
@article{Fisher1926,
author = {Fisher, R. A.},
doi = {10.1007/978-1-4612-4380-9_8},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1926 - The Arrangement of Field Experiments.pdf:pdf},
isbn = {978-0-387-94039-7},
journal = {Journal of the Ministry of Agriculture of Great Britain},
pages = {503--513},
pmid = {1833},
title = {{The Arrangement of Field Experiments}},
url = {http://link.springer.com/10.1007/978-1-4612-4380-9{\_}8},
volume = {33},
year = {1926}
}
@article{Pennell2008,
abstract = {In certain biomedical studies, one may anticipate changes in the shape of a response distribution across the levels of an ordinal predictor. For instance, in toxicology studies, skewness and modality might change as dose increases. To address this issue, we propose a Bayesian nonparametric method for testing for distribution changes across an ordinal predictor. Using a dynamic mixture of Dirichlet processes, we allow the response distribution to change flexibly at each level of the predictor. In addition, by assigning mixture priors to the hyperparameters, we can obtain posterior probabilities of no effect of the predictor and identify the lowest dose level for which there is an appreciable change in distribution. The method also provides a natural framework for performing tests across multiple outcomes. We apply our method to data from a genotoxicity experiment. {\textcopyright} 2008, The International Biometric Society.},
author = {Pennell, Michael L. and Dunson, David B.},
doi = {10.1111/j.1541-0420.2007.00885.x},
issn = {0006341X},
journal = {Biometrics},
keywords = {Dirichlet process,Dose response,Nonparametric Bayes,Toxicology,Trend test},
month = {jun},
number = {2},
pages = {413--423},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Nonparametric Bayes Testing of Changes in a Response Distribution with an Ordinal Predictor}},
url = {http://doi.wiley.com/10.1111/j.1541-0420.2007.00885.x},
volume = {64},
year = {2008}
}
@article{VehtariLOOPackage2020,
author = {Vehtari, Aki and Gabry, Jonah and Magnusson, Mans and Yuling, Yao and Andrew, Gelman and B{\"{u}}rkner, Paul-Christian and Goodrich, Ben and Piironen, Juho},
journal = {R package version 2.2.0},
title = {{loo: Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian Models}},
url = {https://cran.r-project.org/web/packages/loo/index.html},
year = {2020}
}
@techreport{ICHE91998,
address = {London, UK},
author = {{ICH E9}},
institution = {International Council for Harmonisation of Technical Requirements for Pharmaceuticals for Human Use (ICH)},
title = {{Statistical principles for clinical trials}},
year = {1998}
}
@book{cramer_sage_2004,
address = {London ; Thousand Oaks, CA},
author = {Cramer, Duncan and Howitt, Dennis},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Cramer, Howitt - 2004 - The Sage dictionary of statistics a practical resource for students in the social sciences.pdf:pdf},
isbn = {978-0-7619-4137-8},
keywords = {Methodology,Social sciences,Statistical methods,Statistics},
publisher = {Sage Publications},
shorttitle = {The {\{}Sage{\}} dictionary of statistics},
title = {{The Sage dictionary of statistics: a practical resource for students in the social sciences}},
year = {2004}
}
@misc{Cole2021,
abstract = {Measures of information and surprise, such as the Shannon information value (S value), quantify the signal present in a stream of noisy data. We illustrate the use of such information measures in the context of interpreting P values as compatibility indices. S values help communicate the limited information supplied by conventional statistics and cast a critical light on cutoffs used to judge and construct those statistics. Misinterpretations of statistics may be reduced by interpreting P values and interval estimates using compatibility concepts and S values instead of "significance"and "confidence.".},
author = {Cole, Stephen R. and Edwards, Jessie K. and Greenland, Sander},
booktitle = {American Journal of Epidemiology},
doi = {10.1093/aje/kwaa136},
issn = {14766256},
keywords = {P value,S value,compatibility,confidence intervals,information,random error,significance tests,statistical inference},
month = {feb},
number = {2},
pages = {191--193},
pmid = {32648906},
publisher = {Oxford University Press},
title = {{Surprise!}},
url = {https://pubmed.ncbi.nlm.nih.gov/32648906/},
volume = {190},
year = {2021}
}
@article{Steel2019,
abstract = {ABSTRACTStatisticians are in general agreement that there are flaws in how science is currently practiced; there is less agreement in how to make repairs. Our prescription for a Post-p {\textless} 0.05 Era is to develop and teach courses that expand our view of what constitutes the domain of statistics and thereby bridge undergraduate statistics coursework and the graduate student experience of applying statistics in research. Such courses can speed up the process of gaining statistical wisdom by giving students insight into the human propensity to make statistical errors, the meaning of a single test within a research project, ways in which p-values work and don't work as expected, the role of statistics in the lifecycle of science, and best practices for statistical communication. The course we have developed follows the story of how we use data to understand the world, leveraging simulation-based approaches to perform customized analyses and evaluate the behavior of statistical procedures. We provide ideas for e...},
author = {Steel, E. Ashley and Liermann, Martin and Guttorp, Peter},
doi = {10.1080/00031305.2018.1505657},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Steel, Liermann, Guttorp - 2019 - Beyond Calculations A Course in Statistical Thinking.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Probabilistic thinking,p-Values,Simulation-based t,p -values,probabilistic thinking,simulation-based,statistical intuition,testing},
number = {sup1},
pages = {392--401},
publisher = {Taylor {\&} Francis},
title = {{Beyond Calculations: A Course in Statistical Thinking}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1505657},
volume = {73},
year = {2019}
}
@article{Brown2017,
abstract = {Teaching is the process of conveying knowledge and skills to learners. It involves preventing misunderstandings or correcting misconceptions that learners have acquired. Thus, effective teaching relies on solid knowledge of the discipline, but also a good grasp of where learners are likely to trip up or misunderstand. In programming, there is much opportunity for misunderstanding, and the penalties are harsh: failing to produce the correct syntax for a program, for example, can completely prevent any progress in learning how to program. Because programming is inherently computer-based, we have an opportunity to automatically observe programming behaviour -- more closely even than an educator in the room at the time. By observing students' programming behaviour, and surveying educators, we can ask: do educators have an accurate understanding of the mistakes that students are likely to make? In this study, we combined two years of the Blackbox dataset (with more than 900 thousand users and almost 100 million compilation events) with a survey of 76 educators to investigate which mistakes students make while learning to program Java, and whether the educators could make an accurate estimate of which mistakes were most common. We find that educators' estimates do not agree with one another or the student data, and discuss the implications of these results.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.07526v1},
author = {Brown, Neil C. C. and Altadmri, Amjad},
doi = {10.1145/2994154},
eprint = {arXiv:1502.07526v1},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Brown, Altadmri - 2017 - Novice Java Programming Mistakes Large-Scale Data vs. Educator Beliefs.pdf:pdf},
isbn = {9781577357384},
issn = {1946-6226},
journal = {Trans. Comput. Educ.},
keywords = {Programming mistakes,blackbox,educators,java},
month = {may},
number = {2},
pages = {7:1----7:21},
pmid = {1000285845},
publisher = {ACM},
title = {{Novice Java Programming Mistakes: Large-Scale Data vs. Educator Beliefs}},
url = {http://dl.acm.org/citation.cfm?doid=3090098.2994154 http://doi.acm.org/10.1145/2994154},
volume = {17},
year = {2017}
}
@article{Kruschke2013,
abstract = {Bayesian estimation for 2 groups provides complete distributions of credible values for the effect size, group means and their difference, standard deviations and their difference, and the normality of the data. The method handles outliers. The decision rule can accept the null value (unlike traditional t tests) when certainty in the estimate is high (unlike Bayesian model comparison using Bayes factors). The method also yields precise estimates of statistical power for various research goals. The software and programs are free and run on Macintosh, Windows, and Linux platforms.},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1037/a0029146},
author = {Kruschke, John K.},
doi = {10.1037/a0029146},
eprint = {/dx.doi.org/10.1037/a0029146},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kruschke - 2013 - Bayesian estimation supersedes the t-test.pdf:pdf},
isbn = {1939-2222 (Electronic) 0022-1015 (Linking)},
issn = {1939-2222},
journal = {Journal of Experimental Psychology: General},
keywords = {Bayes factor,Bayesian statistics,confidence interval,effect size,robust estimation},
number = {2},
pages = {573--603},
pmid = {22774788},
primaryClass = {http:},
title = {{Bayesian estimation supersedes the t-test.}},
volume = {142},
year = {2013}
}
@article{Casella1987,
abstract = {For the one-sided hypothesis testing problem it is shown that it is possible to reconcile Bayesian evidence against H0, expressed in terms of the posterior probability that Ho is true, with frequentist evidence against H0, expressed in terms of the p value. In fact, for many classes of prior distributions it is shown that the infimum of the Bayesian posterior prob- ability of Ho is equal to the p value; in other cases the infimum is less than the p value. The results are in contrast to recent work of Berger and Sellke (1987) in the two-sided (point null) case, where it was found that the p value is much smaller than the Bayesian infimum. Some comments on the point null problem are also given.},
author = {Casella, George and Berger, Roger L.},
doi = {10.1080/01621459.1987.10478396},
isbn = {01621459},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Posterior probability,Prior distribution,p Value},
number = {397},
pages = {106--111},
title = {{Reconciling bayesian and frequentist evidence in the one-sided testing problem}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1987.10478396},
volume = {82},
year = {1987}
}
@article{Altman1991a,
abstract = {This paper reviews changes in the use of statistics in medical journals during the 1980s. Aspects considered are research design, statistical analysis, the presentation of results, medical journal policy (including statistical refereeing), and the misuse of statistics. Despite some notable successes, the misuse of statistics in medical papers remains common.},
author = {Altman, Douglas G.},
doi = {10.1002/sim.4780101206},
journal = {Statistics in Medicine},
number = {12},
pages = {1897--1913},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Statistics in medical journals: Developments in the 1980s}},
volume = {10},
year = {1991}
}
@misc{looPackage2019,
author = {Vehtari, A and J, Gabry and M, Magnusson and Y, Yao and A, Gelman},
publisher = {Comprehensive R Archive Network},
title = {{loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models.}},
url = {https://mc-stan.org/loo},
year = {2019}
}
@article{Pearson1939,
author = {Pearson, E.S.},
doi = {10.1093/biomet/30.3-4.210},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Pearson - 1939 - William Sealy Gosset, 1876-1937.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Pearson - 1939 - William Sealy Gosset, 1876-1937(2).pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = {jan},
number = {3-4},
pages = {210--250},
publisher = {Oxford University Press},
title = {{William Sealy Gosset, 1876-1937}},
url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/30.3-4.210},
volume = {30},
year = {1939}
}
@article{Esteves2019,
abstract = {This paper introduces pragmatic hypotheses and relates this concept to the spiral of scientific evolution. Previous works determined a characterization of logically consistent statistical hypothesis tests and showed that the modal operators obtained from this test can be represented in the hexagon of oppositions. However, despite the importance of precise hypothesis in science, they cannot be accepted by logically consistent tests. Here, we show that this dilemma can be overcome by the use of pragmatic versions of precise hypotheses. These pragmatic versions allow a level of imprecision in the hypothesis that is small relative to other experimental conditions. The introduction of pragmatic hypotheses allows the evolution of scientific theories based on statistical hypothesis testing to be interpreted using the narratological structure of hexagonal spirals, as defined by Pierre Gallais.},
author = {Esteves, Luis Gustavo and Izbicki, Rafael and Stern, J. M. and Stern, Rafael Bassi},
doi = {10.3390/e21090883},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Esteves et al. - 2019 - Pragmatic Hypotheses in the Evolution of Science.pdf:pdf},
issn = {1099-4300},
journal = {Entropy},
keywords = {hypothesis tests,pragmatic hypotheses,precise hypotheses},
number = {9},
pages = {883},
publisher = {MDPI AG},
title = {{Pragmatic Hypotheses in the Evolution of Science}},
url = {https://www.mdpi.com/1099-4300/21/9/883},
volume = {21},
year = {2019}
}
@misc{StanReferenceManual2018,
author = {{Stan Development Team}},
title = {{Stan Modeling Language Users Guide and Reference Manual}},
year = {2018}
}
@article{Greenland2019,
abstract = {The present note explores sources of misplaced criticisms of P-values, such as conflicting definitions of "sig-nificance levels" and "P-values" in authoritative sources, and the consequent misinterpretation of P-values as error probabilities. It then discusses several properties of P-values that have been presented as fatal flaws: That P-values exhibit extreme variation across samples (and thus are "unreliable"), confound effect size with sample size, are sensitive to sample size, and depend on investigator sampling intentions. These properties are often criticized from a likelihood or Bayesian framework, yet they are exactly the properties P-values should exhibit when they are constructed and interpreted correctly within their originating framework. Other common criticisms are that P-values force users to focus on irrelevant hypotheses and overstate evidence against those hypotheses. These problems are not however properties of P-values but are faults of researchers who focus on null hypotheses and overstate evidence based on misperceptions that p = 0.05 represents enough evidence to reject hypotheses. Those problems are easily seen without use of Bayesian concepts by translating the observed P-value p into the Shannon information (S-value or surprisal) −log 2 (p). ARTICLE HISTORY},
author = {Greenland, Sander},
doi = {10.1080/00031305.2018.1529625},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Greenland - 2019 - Valid p-Values Behave Exactly as They Should Some Misleading Criticisms of p-Values and Their Resolution With s-Value.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {compatibility,dichotomania,evidence,information,logworth,nullism,p -values},
number = {sup1},
pages = {106--114},
title = {{Valid p-Values Behave Exactly as They Should: Some Misleading Criticisms of p-Values and Their Resolution With s-Values}},
volume = {73},
year = {2019}
}
@book{Bennett2003,
abstract = {Conditionals are of two basic kinds, often called 'indicative' and 'subjunctive'. This book expounds and evaluates the main literature about each kind. It eventually defends the view of Adams and Edgington that indicatives are devices for expressing subjective probabilities, and the view of Stalnaker and Lewis that subjunctives are statements about close possible worlds. But it also discusses other views, e.g. that indicatives are really material conditionals, and Goodman's approach to subjunctives.},
author = {Bennett, Jonathan},
booktitle = {A Philosophical Guide to Conditionals},
doi = {10.1093/0199258872.001.0001},
isbn = {9780191597046},
keywords = {Adams,Conditionals,Edgington,Goodman,Indicative conditionals,Lewis,Material conditionals,Metaphysics,Philosophy of language,Possible worlds,Probability,Stalnaker,Subjunctive},
month = {nov},
publisher = {Oxford University Press},
title = {{A Philosophical Guide to Conditionals}},
year = {2003}
}
@article{Cheung2016,
abstract = {Big data is a field that has traditionally been dominated by disciplines such as computer science and business, where mainly data-driven analyses have been performed. Psychology, a discipline in which a strong emphasis is placed on behavioral theories and empirical research, has the potential to contribute greatly to the big data movement. However, one challenge to psychologists-and probably the most crucial one-is that most researchers may not have the necessary programming and computational skills to analyze big data. In this study we argue that psychologists can also conduct big data research and that, rather than trying to acquire new programming and computational skills, they should focus on their strengths, such as performing psychometric analyses and testing theories using multivariate analyses to explain phenomena. We propose a split/analyze/meta-analyze approach that allows psychologists to easily analyze big data. Two real datasets are used to demonstrate the proposed procedures in R. A new research agenda related to the analysis of big data in psychology is outlined at the end of the study.},
author = {Cheung, Mike W.-L. and Jak, Suzanne},
doi = {10.3389/fpsyg.2016.00738},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Cheung, Jak - 2016 - Analyzing Big Data in Psychology A SplitAnalyzeMeta-Analyze Approach.pdf:pdf},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {Big data,Meta-analysis,Multilevel model,R platform,Structural equation modeling},
month = {may},
number = {May},
pages = {738},
publisher = {Frontiers Media S.A.},
title = {{Analyzing Big Data in Psychology: A Split/Analyze/Meta-Analyze Approach}},
url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2016.00738/abstract},
volume = {7},
year = {2016}
}
@techreport{Savage1961,
address = {Michigan},
author = {Savage, L.J.},
institution = {Dept. of Statistics, University of Michigan},
title = {{The subjective basis of statistical practice}},
year = {1961}
}
@article{Nickel2006,
author = {Nickel, Gregor},
journal = {Neue Zeitschrift f{\"{u}}r Systematische Theologie und Religionsphilosophie},
pages = {412--429},
title = {{Ethik und Mathematik - Randbemerkungen zu einem prek{\"{a}}ren Verh{\"{a}}ltnis}},
volume = {47},
year = {2006}
}
@inproceedings{partovi_transforming_2014,
abstract = {Code.org first exploded on the CS education scene in Feb 2013 with its first video featuring Mark Zuckerberg and Bill Gates talking about computer science. 10 months later, it launched an Hour of Code campaign that has taken the world by storm. Reaching 10 million students in just 3 days, the Hour of Code became the fastest-spreading service in the history of technology OR education. Code.org founder Hadi Partovi will talk about how he came up with the concepts behind these grassroots campaigns, and how Code.org hopes to harness the reach of the broader CS community to grow computer science education in schools.},
address = {New York, NY, USA},
annote = {USA},
author = {Partovi, Hadi},
booktitle = {Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
doi = {10.1145/2538862.2554793},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Partovi - 2014 - Transforming US Education with Computer Science.pdf:pdf},
isbn = {978-1-4503-2605-6},
keywords = {code.org,computer science,education},
pages = {5--6},
publisher = {ACM},
series = {{\{}SIGCSE{\}} '14},
title = {{Transforming US Education with Computer Science}},
url = {http://doi.acm.org/10.1145/2538862.2554793},
year = {2014}
}
@article{Fisher1923b,
abstract = {The frequency ratio of the allelomorphs of a Mendelian factor is only stable if selection favours the heterozygote: such factors, though occurring rarely, will accumulate in the stock, while those of opposite tendency will be eliminated.},
author = {Fisher, Ronald A.},
doi = {10.1017/S0370164600023993},
issn = {0370-1646},
journal = {Proceedings of the royal society of Edinburgh},
month = {sep},
pages = {321--341},
publisher = {Royal Society of Edinburgh Scotland Foundation},
title = {{XXI.—On the Dominance Ratio}},
url = {http://www.journals.cambridge.org/abstract{\_}S0370164600023993},
volume = {42},
year = {1923}
}
@article{Vehtari2017,
abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
doi = {10.1007/s11222-016-9696-4},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Vehtari, Gelman, Gabry - 2017 - Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC.pdf:pdf},
issn = {0960-3174},
journal = {Statistics and Computing},
keywords = {Artificial Intelligence,Probability and Statistics in Computer Science,Statistical Theory and Methods,Statistics and Computing/Statistics Programs},
number = {5},
pages = {1413--1432},
publisher = {Springer},
title = {{Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC}},
url = {http://link.springer.com/10.1007/s11222-016-9696-4},
volume = {27},
year = {2017}
}
@article{Geman1984,
abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
author = {Geman, Stuart and Geman, Donald},
doi = {10.1109/TPAMI.1984.4767596},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Geman, Geman - 1984 - Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images.pdf:pdf},
isbn = {0934613338},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Annealing,Gibbs distribution,MAP estimate,Markov random field,image restoration,line process,relaxation scene modeling,spatial degradation},
number = {6},
pages = {721--741},
pmid = {22499653},
title = {{Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images}},
volume = {PAMI-6},
year = {1984}
}
@article{Hobbs2007,
author = {Hobbs, Brian P. and Carlin, Bradley P.},
journal = {Journal of Biopharmaceutical Statistics},
number = {1},
pages = {54--80},
title = {{Practical Bayesian design and analysis for drug and device clinical trials}},
volume = {18},
year = {2007}
}
@article{Guttman1967,
abstract = {An attack on the problem of goodness of fit is made by combining a$\backslash$nBayesian and sampling argument; the Bayesian part is effected by$\backslash$nusing the distribution of a future observation, while the sampling$\backslash$nargument concerns itself with the distribution of a "chi-squared$\backslash$nlike" statistic, which measures discrepancies of observed frequencies$\backslash$nfrom those predicted by the distribution of the future observation.$\backslash$nExamples are given for the case of sampling from the binomial, Poisson$\backslash$nand normal distributions. An interesting application arising from$\backslash$nthe above approach is a procedure for estimating the degree of a$\backslash$npolynomial response function.},
author = {Guttman, Irwin},
doi = {10.1111/j.2517-6161.1967.tb00676.x},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
number = {1},
pages = {83--100},
publisher = {Wiley},
title = {{The Use of the Concept of a Future Observation in Goodness-Of-Fit Problems}},
volume = {29},
year = {1967}
}
@article{RobertsRosenthal2007,
author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
journal = {Applied Probability Trust},
keywords = {2000 mathematics subject classification,65c40,computational methods,markov chains,primary 60j10,secondary 60j22},
number = {2},
pages = {458--475},
title = {{Coupling and Ergodicity of Adaptive Markov Chain Monte Carlo Algorithms}},
volume = {44},
year = {2007}
}
@article{Kenakin2014,
abstract = {A pharmacological experiment is typically conducted to: i) test or expand a hypothesis regarding the potential role of a target in the mechanism(s) underlying a disease state using an existing drug or tool compound in normal and/or diseased tissue or animals; or ii) characterize and optimize a new chemical entity (NCE) targeted to modulate a specific disease-associated target to restore homeostasis as a potential drug candidate. Hypothesis testing necessitates an intellectually rigorous, null hypothesis approach that is distinct from a high throughput fishing expedition in search of a hypothesis. In conducting an experiment, the protocol should be transparently defined along with its powering, design, appropriate statistical analysis and consideration of the anticipated outcome (s) before it is initiated. Compound-target interactions often involve the direct study of phenotype(s) unique to the target at the cell, tissue or animal/human level. However, in vivo studies are often compromised by a lack of sufficient information on the compound pharmacokinetics necessary to ensure target engagement and also by the context-free analysis of ubiquitous cellular signaling pathways downstream from the target. The use of single tool compounds/drugs at one concentration in engineered cell lines frequently results in reductionistic data that have no physiologically relevance. This overview, focused on trends in the peer-reviewed literature, discusses the execution and reporting of experiments and the criteria recommended for the physiologically-relevant assessment of target engagement to identify viable new drug targets and facilitate the advancement of translational studies. {\textcopyright} 2013 Elsevier Inc.},
author = {Kenakin, Terry and Bylund, David B. and Toews, Myron L. and Mullane, Kevin and Winquist, Raymond J. and Williams, Michael},
doi = {10.1016/j.bcp.2013.10.024},
isbn = {1873-2968 (Electronic)$\backslash$n0006-2952 (Linking)},
issn = {00062952},
journal = {Biochemical Pharmacology},
keywords = {Drug discovery,Pharmacology,Receptors,Systems biology,Target engagement},
month = {jan},
number = {1},
pages = {64--77},
pmid = {24269285},
title = {{Replicated, replicable and relevant-target engagement and pharmacological experimentation in the 21st century}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24269285 http://linkinghub.elsevier.com/retrieve/pii/S0006295213007089},
volume = {87},
year = {2014}
}
@article{rausch_personality_2016,
abstract = {This study examined personality similarity between teachers and their students and its impact on teacher judgement of student achievement in the domains of reading comprehension and mathematics. Personality similarity was quantified through intraclass correlations between personality characteristics of 409 dyads of German teachers and their students. This similarity index was combined with teachers' global and task-specific judgements of student achievement. Personality similarity has a significant effect on global judgement in both domains under study. Students who are similar to their teacher are judged more positively than students who are dissimilar, even when students' test performance is controlled. This effect could not be verified for task-specific judgements. Results indicate that impact of potential sympathy bias in social judgements differs between different types of judgement. That is, global judgements are more likely to be biased than more specific judgements. Theoretical and educational relevance of the findings are discussed.},
author = {Rausch, Tobias and Karing, Constance and D{\"{o}}rfler, Tobias and Artelt, Cordula},
doi = {10.1080/01443410.2014.998629},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Rausch et al. - 2016 - Personality similarity between teachers and their students influences teacher judgement of student achievement.html:html},
issn = {0144-3410},
journal = {Educational Psychology},
keywords = {judgement bias,personality similarity,teacher judgement accuracy},
month = {may},
number = {5},
pages = {863--878},
title = {{Personality similarity between teachers and their students influences teacher judgement of student achievement}},
url = {http://dx.doi.org/10.1080/01443410.2014.998629},
volume = {36},
year = {2016}
}
@article{Gannon2019,
abstract = {This article argues that researchers do not need to completely abandon the p-value, the best-known significance index, but should instead stop using significance levels that do not depend on sample sizes. A testing procedure is developed using a mixture of frequentist and Bayesian tools, with a significance level that is a function of sample size, obtained from a generalized form of the Neyman-Pearson Lemma that minimizes a linear combination of $\alpha$, the probability of rejecting a true null hypothesis, and $\beta$, the probability of failing to reject a false null, instead of fixing $\alpha$ and minimizing $\beta$. The resulting hypothesis tests do not violate the Likelihood Principle and do not require any constraints on the dimensionalities of the sample space and parameter space. The procedure includes an ordering of the entire sample space and uses predictive probability (density) functions, allowing for testing of both simple and compound hypotheses. Accessible examples are presented to highlight specific characteristics of the new tests. ARTICLE HISTORY},
author = {Gannon, Mark Andrew and {de Bragan{\c{c}}a Pereira}, Carlos Alberto and Polpo, Adriano},
doi = {10.1080/00031305.2018.1518268},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gannon, de Bragan{\c{c}}a Pereira, Polpo - 2019 - Blending Bayesian and Classical Tools to Define Optimal Sample-Size-Dependent Significance.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
number = {sup1},
pages = {213--222},
title = {{Blending Bayesian and Classical Tools to Define Optimal Sample-Size-Dependent Significance Levels}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1518268},
volume = {73},
year = {2019}
}
@article{Berger2001,
abstract = {Testing the fit of data to a parametric model can be done by embedding the parametric model in a nonparametric alternative and computing the Bayes factor of the parametric model to the nonparametric alternative. Doing so by specifying the nonparametric alternative via a Polya tree process is particularly attractive, from both theoretical and methodological perspectives. Among the benefits is a degree of computational simplicity that even allows for robustness analyses to be implemented. Default (nonsubjective) versions of this analysis are developed herein, in the sense that recommended choices are provided for the (many) features of the Polya tree process that need to be specified. Considerable discussion of these features is also provided to assist those who might be interested in subjective choices. A variety of examples involving location–scale models are studied. Finally, it is shown that the resulting procedure can be viewed as a conditional frequentist test, resulting in data-dependent reported error probabilities that have a real frequentist interpretation (as opposed to p values) in even small sample situations. {\textcopyright} 2001 American Statistical Association.},
author = {Berger, James O. and Guglielmi, Alessandra},
doi = {10.1198/016214501750333045},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Berger, Guglielmi - 2001 - Bayesian and conditional frequentist testing of a parametric model versus nonparametric alternatives.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Bayes factor,Bayesian robustness,Conditional frequentist type I and type II error p,Noninformative priors,Nonparametric bayes,Polya tree process,Testing fit},
month = {mar},
number = {453},
pages = {174--184},
publisher = {Taylor {\&} Francis},
title = {{Bayesian and conditional frequentist testing of a parametric model versus nonparametric alternatives}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=uasa20},
volume = {96},
year = {2001}
}
@inproceedings{sharma_game-theme_2015,
abstract = {Due to complex mathematical nature of computer science, it is considered a complex and arduous subject by college students. Due to a gradual decrease in computer science students, in spite of a growing demand for computer science professionals, it is crucial to find a way to attract computer science students by making the concepts even more fascinating and absorbing. The aim of this paper is to develop game theme based instructional modules for computer science students that motivates and engages students while contributing to their learning outcomes. Game theme based instructional modules are designed to encourage faculty to teach and motivate students to learn the concepts of object oriented programming using interactive, graphical, game-like examples. This paper discusses the design parameters and implementation of the module and describes a case study of adopting the module in an existing class. The results of the study demonstrate the effectiveness of the instructional module and the possibility to include it in the existing curriculum with minimum alterations to the existing established course material. The instructional modules act as a supplement to an existing course and enable faculty to explore teaching with a game-theme materials and helping students to be more motivated and engaged in class.},
annote = {USA},
author = {Sharma, S and Stigall, J and Rajeev, S},
booktitle = {2015 International Conference on Computational Science and Computational Intelligence (CSCI)},
doi = {10.1109/CSCI.2015.35},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Sharma, Stigall, Rajeev - 2015 - Game-Theme Based Instructional Module for Teaching Object Oriented Programming.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Sharma, Stigall, Rajeev - 2015 - Game-Theme Based Instructional Module for Teaching Object Oriented Programming.html:html},
keywords = {Computer science education,Courseware,Games,Software,Solid modeling,Three-dimensional displays,Virtual reality,assessment,computer science,computer science students,education,game-theme based instructional module,instructional module,learning outcomes,object oriented programming teaching,object-oriented programming,serious games (computing),teaching},
month = {dec},
pages = {252--257},
title = {{Game-Theme Based Instructional Module for Teaching Object Oriented Programming}},
year = {2015}
}
@misc{Rubin1984,
abstract = {A common reaction among applied statisticians is that the Bayesian statistician's energies in an applied problem must be directed at the a priori elicitation of one model specification from which an optimal design and all inferences follow automatically by applying Bayes's theorem to calculate conditional distributions of unknowns given knowns. I feel, however, that the applied Bayesian statistician's tool-kit should be more extensive and include tools that may be usefully labeled frequency calculations. Three types of Bayesianly justifiable and relevant frequency calculations are presented using examples to convey their use for the applied statistician.},
author = {Rubin, Donald B.},
booktitle = {The Annals of Statistics},
pages = {1151--1172},
publisher = {Institute of Mathematical Statistics},
title = {{Bayesianly Justifiable and Relevant Frequency Calculations for the Applies Statistician}},
volume = {12},
year = {1984}
}
@article{benjaminRedefineStatisticalSignificance,
abstract = {We propose to change the default P-value threshold for statistical significance from 0.05 to 0.005 for claims of new discoveries.},
author = {Benjamin, Daniel J. and Berger, J.O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, Eric-Jan and Berk, Richard and Bollen, Kenneth A. and Brembs, Bj{\"{o}}rn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and {De Boeck}, Paul and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and {Hua Ho}, Teck and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P. A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and McCarthy, Michael and Moore, Don A. and Morgan, Stephen L. and Munaf{\'{o}}, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Sch{\"{o}}nbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and {Van Zandt}, Trisha and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
doi = {10.1038/s41562-017-0189-z},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Benjamin, Berger - 2018 - Redefine statistical significance.pdf:pdf},
issn = {2397-3374},
journal = {Nature Human Behaviour},
keywords = {Human behaviour,Statistics},
number = {1},
pages = {6--10},
publisher = {Nature Publishing Group},
title = {{Redefine statistical significance}},
volume = {2},
year = {2018}
}
@book{heyde_2001,
address = {New York, Berlin, Heidelberg},
author = {Heyde, C.C. and Seneta, E.},
publisher = {Springer-Verlag},
title = {{Statisticians Of The Centuries}},
year = {2001}
}
@book{BovensHartmann2003,
address = {Oxford},
author = {Bovens, Luc and Hartmann, Stephan},
publisher = {Oxford University Press},
title = {{Bayesian Epistemology}},
year = {2003}
}
@article{Kalbfleisch1975,
abstract = {Ancillary statistics are divided into two logically diatinct types: those determined by the experimental design and those determined by the mathematical modelling of the problem. It ie pointed out that, in the class of inference problems where our purpose is to gain information or insight into the nature of a chance set-up, a weakened conditionality principle when applied first removes the possibility of deriving the likelihood principle. Since to some extent a conditionality principle must be applied in experiment definition, it is argued that this is a necessary first step if full acceptance of the likelihood axiom is to be avoided. {\textcopyright} 1975 Oxford University Press.},
author = {Kalbfleisch, John D.},
doi = {10.1093/biomet/62.2.251},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kalbfleisch - 1975 - Sufficiency and conditionality.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Ancillary statistics,Conditionality,Likelihood,Principles of inference,Sufficiency},
number = {2},
pages = {251--259},
title = {{Sufficiency and conditionality}},
volume = {62},
year = {1975}
}
@article{Campbell2011,
abstract = {Bayesian statistical methodology has been used for more than 10 years in medical device premarket submissions to the U.S. Food and Drug Administration (FDA). A complete list of the publicly available information associated with these FDA applications is presented. In addition to the increasing number of Bayesian methodological papers in the statistical journals, a number of successful Bayesian clinical trials in the biomedical journals have been recently reported. Some challenges that require more methodological development are discussed. The promise of using Bayesian methods for incorporation of prior information as well as for conducting adaptive trials is great.},
author = {Campbell, Gregory},
doi = {10.1080/10543406.2011.589638},
isbn = {9780470863695},
issn = {10543406},
journal = {Journal of Biopharmaceutical Statistics},
keywords = {Adaptive designs,Bayesian hierarchical models,Implants,Noninferiority,Software},
month = {sep},
number = {5},
pages = {871--887},
pmid = {21830920},
publisher = {Taylor {\&} Francis Group},
title = {{Bayesian statistics in medical devices: Innovation sparked by the FDA}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10543406.2011.589638},
volume = {21},
year = {2011}
}
@article{Johnson2019,
abstract = {This article examines the evidence contained in t statistics that are marginally significant in 5{\%} tests. The bases for evaluating evidence are likelihood ratios and integrated likelihood ratios, computed under a variety of assumptions regarding the alternative hypotheses in null hypothesis significance tests. Likelihood ratios and integrated likelihood ratios provide a useful measure of the evidence in favor of competing hypotheses because they can be interpreted as representing the ratio of the probabilities that each hypothesis assigns to observed data. When they are either very large or very small, they suggest that one hypothesis is much better than the other in predicting observed data. If they are close to 1.0, then both hypotheses provide approximately equally valid explanations for observed data. I find that p-values that are close to 0.05 (i.e., that are "marginally significant") correspond to integrated likelihood ratios that are bounded by approximately 7 in two-sided tests, and by approximately 4 in one-sided tests. The modest magnitude of integrated likelihood ratios corresponding to p-values close to 0.05 clearly suggests that higher standards of evidence are needed to support claims of novel discoveries and new effects. ARTICLE HISTORY},
author = {Johnson, Valen E.},
doi = {10.1080/00031305.2018.1518788},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Johnson - 2019 - Evidence From Marginally Significant iti Statistics.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {bayes factor,hypothesis test,posterior odds,prior odds,significance level,uniformly},
number = {sup1},
pages = {129--134},
title = {{Evidence From Marginally Significant {\textless}i{\textgreater}t{\textless}/i{\textgreater} Statistics}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1518788},
volume = {73},
year = {2019}
}
@book{Cureton,
author = {Cureton, Edward E. (Edward Eugene) and D'Agostino, Ralph B.},
isbn = {0805815465},
title = {{Factor analysis : an applied approach}}
}
@book{Gelman2006,
address = {Cambridge},
author = {Gelman, Andrew and Hill, Jennifer},
doi = {10.1017/CBO9780511790942},
isbn = {9780511790942},
publisher = {Cambridge University Press},
title = {{Data Analysis Using Regression and Multilevel/Hierarchical Models}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511790942},
year = {2006}
}
@article{Jeffreys1936,
abstract = {In a previous paper (afterwards referred to as Paper I) tests have been given for the significance of some quantities found statistically. The results are given in the form P ( q | $\theta$ h )/ P (˜ q |$\theta$ h ); here h denotes the previous knowledge and $\theta$ the experimental evidence used, while q is the hypothesis that all the variations outstanding can be attributed to accidental error or random variation, and ˜ q the hypothesis that at least part of them is systematic. It has been supposed in the analysis that q and ˜ q are equally probable on the information h ; but if they are not, the only alteration is that the ratios evaluated now represent},
author = {Jeffreys, Harold},
doi = {10.1017/s0305004100019125},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Jeffreys - 1936 - Further significance tests.pdf:pdf},
issn = {0305-0041},
journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
month = {oct},
number = {03},
pages = {416--445},
publisher = {Cambridge University Press},
title = {{Further significance tests}},
url = {https://www.cambridge.org/core/product/identifier/S0305004100019125/type/journal{\_}article},
volume = {32},
year = {1936}
}
@article{cowie_exploring_2017-1,
abstract = {A number of trends are converging to drive the need for more informed teacher data use. These include advocacy for formative assessment and the need for teachers to account for student learning. In this context, assessment literacy and data literacy have emerged as a focus in research and professional development. Problematically, research signals that developing assessment/data literacy is challenging with evidence that teachers may not have mastered relevant aspects of mathematics and statistics. This paper reports lecturer, student teacher and school leader views of the role and requirements of data literacy using data from a larger study into how to foster student teacher mathematical thinking for the breadth of teacher professional work. Data were generated via interviews, surveys and document analysis. Findings suggest a concern and opportunity to develop assessment/data literacy as this calls on mathematical and statistical understanding.},
author = {Cowie, Bronwen and Cooper, Beverley},
doi = {10.1080/0969594X.2016.1225668},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Cowie, Cooper - 2017 - Exploring the challenge of developing student teacher data literacy.html:html},
issn = {0969-594X},
journal = {Assessment in Education: Principles, Policy {\&} Practice},
keywords = {Academic Achievement,Administrator Attitudes,Data,Foreign Countries,Formative Evaluation,Interviews,Mathematical Logic,Mathematics Education,Preservice Teachers,Statistics,Student Teacher Attitudes,Student Teachers,Teacher Education Programs,Technological Literacy,Thinking Skills,teacher education},
number = {2},
pages = {147--163},
title = {{Exploring the challenge of developing student teacher data literacy}},
volume = {24},
year = {2017}
}
@article{Holmes2015,
abstract = {In this article we describe Bayesian nonparametric procedures for two-sample hypothesis testing. Namely, given two sets of samples y (1) iid ∼ F (1) and y (2) iid ∼ F (2) , with F (1) , F (2) unknown, we wish to evaluate the evidence for the null hypothesis H0 : F (1) ≡ F (2) versus the alternative H1 : F (1) = F (2). Our method is based upon a nonparametric P{\'{o}}lya tree prior centered either subjectively or using an empirical procedure. We show that the P{\'{o}}lya tree prior leads to an analytic expression for the marginal likelihood under the two hypotheses and hence an explicit measure of the probability of the null Pr(H0|{\{}y (1) , y (2) {\}}).},
author = {Holmes, Chris C and Caron, Fran{\c{c}}ois and Griffin, Jim E and Stephens, David A},
doi = {10.1214/14-BA914},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Holmes et al. - 2015 - Two-sample Bayesian Nonparametric Hypothesis Testing.pdf:pdf},
journal = {Bayesian Analysis},
keywords = {Bayesian nonparametrics,P?lya tree,hypothesis testing},
number = {2},
pages = {297--320},
title = {{Two-sample Bayesian Nonparametric Hypothesis Testing}},
volume = {10},
year = {2015}
}
@article{Weaver1948,
author = {Weaver, W.},
journal = {Scientific Monthly},
number = {6},
pages = {390--392},
title = {{Probability, rarity, interest and surprise}},
volume = {67},
year = {1948}
}
@article{Altekar2004,
author = {Altekar, G. and Dwarkadas, S. and Huelsenbeck, J. P. and Ronquist, F.},
doi = {10.1093/bioinformatics/btg427},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Altekar et al. - 2004 - Parallel Metropolis coupled Markov chain Monte Carlo for Bayesian phylogenetic inference.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
month = {feb},
number = {3},
pages = {407--415},
publisher = {Oxford University Press},
title = {{Parallel Metropolis coupled Markov chain Monte Carlo for Bayesian phylogenetic inference}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btg427},
volume = {20},
year = {2004}
}
@inproceedings{holland_avoiding_1997,
abstract = {This paper identifies and describes a number of misconceptions observed in students learning about object technology. It identifies simple, concrete, measures course designers and teachers can take to avoid these misconceptions arising. The context for this work centres on an introductory undergraduate course and a postgraduate course. Both these courses are taught by distance education. These courses both use Smalltalk as an introduction to object technology. More particularly, the undergraduate course uses Smalltalk as a first programming language.Distance education can limit the amount and speed of individual feedback that can be given in the early stages of learning. For this reason, particular attention has been paid to characterizing measures for avoiding elementary misconceptions seen in beginning learners. At the same time we also address some misconceptions observed in postgraduate students. The pedagogical issues discussed are of particular importance when devising an extended series of examples for teaching or assessment, or when designing a visual microworld to be used for teaching purposes.},
address = {New York, NY, USA},
author = {Holland, Simon and Griffiths, Robert and Woodman, Mark},
booktitle = {Proceedings of the Twenty-eighth SIGCSE Technical Symposium on Computer Science Education},
doi = {10.1145/268084.268132},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Holland, Griffiths, Woodman - 1997 - Avoiding Object Misconceptions.pdf:pdf},
isbn = {978-0-89791-889-3},
pages = {131--134},
publisher = {ACM},
series = {{\{}SIGCSE{\}} '97},
title = {{Avoiding Object Misconceptions}},
url = {http://doi.acm.org/10.1145/268084.268132},
year = {1997}
}
@article{Fisher1934a,
abstract = {In a previous paper H. Jeffreys put forward a method of obtaining the distribution a priori of the precision constant of a hypothetical normal distribution, by means of the principle that if three independent observations are made in succession, from a continuous distribution of any form, the probability that the third observation shall fall between the first two must be one-third (p. 48): "Two measures are made. What is the probability that the third observation will lie between them? The answer is easily seen to be one-third." This proposition, in the form in which Jeffreys states it as the foundation for his deductions, is ambiguous, and may bear one of two distinct meanings, one true and the other demonstrably false. The proposition may mean.},
author = {Fisher, R. A.},
doi = {10.1098/rspa.1934.0134},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1934 - Probability Likelihood and Quantity of Information in the Logic of Uncertain Inference.pdf:pdf},
issn = {1364-5021},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
month = {aug},
number = {856},
pages = {1--8},
publisher = {The Royal Society},
title = {{Probability Likelihood and Quantity of Information in the Logic of Uncertain Inference}},
url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1934.0134},
volume = {146},
year = {1934}
}
@book{Billingsley1968,
address = {Boulder, Colorado},
author = {Billingsley, Patrick.},
edition = {1st},
isbn = {9780471197454},
pages = {277},
publisher = {Wiley},
title = {{Convergence of probability measures}},
year = {1968}
}
@article{Burkner2018,
author = {B{\"{u}}rkner, Paul-Christian},
doi = {10.32614/RJ-2018-017},
journal = {The R Journal},
number = {1},
pages = {395--411},
title = {{Advanced Bayesian Multilevel Modeling with the R Package brms}},
volume = {10},
year = {2018}
}
@article{Pashler2012,
author = {Wagenmakers, Eric-Jan and Pashler, Harold},
doi = {10.1177/1745691612465253},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Wagenmakers, Pashler - 2012 - Editors' Introduction to the Special Section on Replicability in Psychological Science A Crisis of Confi.pdf:pdf},
journal = {Perspectives on Psychological Science},
number = {6},
pages = {528--530},
title = {{Editors' Introduction to the Special Section on Replicability in Psychological Science: A Crisis of Confidence?}},
volume = {7},
year = {2012}
}
@book{Gelman2007,
abstract = {"Data Analysis Using Regression and Multilevel/Hierarchical Models is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout."--Publisher description. Why? -- Concepts and methods from basic probability and statistics -- Linear regression: the basics -- Linear regression: before and after fitting the model -- Logistic regression -- Generalized linear models -- Simulation for checking statistical procedures and model fits -- Causal inference using regression on the treatment variable -- Causal inference using more advanced models -- Multilevel structures -- Multilevel linear models: the basics -- Multilevel linear models: varying slopes, non-nested models, and other complexities -- Multilevel logistic regression -- Multilevel generalized linear models -- Multilevel modeling Bugs and R: the basics -- Fitting multilevel linear and generalized linear models in Bugs and R -- Likelihood and Bayesian inference and computation -- Debugging and speeding convergence -- Sample size and power calculations -- Understanding and summarizing the fitted models -- Analysis of variance -- Causal inference using multilevel models -- Model checking and comparison -- Missing-data imputation -- Six quick tips to improve your regression modeling -- Statistical graphics for research and presentation -- Software.},
author = {Gelman, Andrew. and Hill, Jennifer},
isbn = {9780511790942},
pages = {625},
publisher = {Cambridge University Press},
title = {{Data analysis using regression and multilevel/hierarchical models}},
year = {2007}
}
@misc{Goodrich2012,
author = {Goodrich, Benjamin King and Wawro, Gregory and Katznelson, Ira},
title = {{Designing Quantitative Historical Social Inquiry: An Introduction to Stan}},
url = {https://papers.ssrn.com/sol3/papers.cfm?abstract{\_}id=2105531},
year = {2012}
}
@article{Fisher1935,
author = {Fisher, R.A.},
journal = {Journal of the Royal Statistical Society},
number = {1},
pages = {39--82},
title = {{The Logic of Inductive Inference}},
volume = {98},
year = {1935}
}
@article{Hennecke2019,
author = {Hennecke, M.},
doi = {doi: 10.1037/mot0000104},
journal = {Motivation Science},
pages = {1--13},
title = {{What doesn't kill you,{\ldots}: Means for avoidance goal pursuit are less enjoyable than means for approach goal pursuit.}},
volume = {5},
year = {2019}
}
@article{Dasgupta2003,
author = {Dasgupta, Sanjoy and Gupta, Anupam},
doi = {10.1002/rsa.10073},
file = {:Users/riko/Downloads/jl.pdf:pdf},
journal = {Random Structures {\&} Algorithms},
number = {1},
pages = {60--65},
title = {{An Elementary Proof of a Theorem of Johnson and Lindenstrauss}},
volume = {22},
year = {2003}
}
@article{Lindley1998,
author = {Lindley, Dennis V},
file = {:Users/riko/Downloads/1028905932-1.pdf:pdf},
journal = {Statistical Science},
keywords = {1996,a recent paper in,and hsu,and its ensuing discussion,and phrases,bioequivalence,coherence,conjugacy,expected loss,hereinafter re-,loss,sample size,this journal {\v{z}} berger,utility},
number = {2},
pages = {136--141},
title = {{Decision Analysis and Bioequivalence Trials}},
volume = {13},
year = {1998}
}
@article{Pearson1904,
author = {Pearson, Karl},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Pearson - 1904 - On the Theory of Contingency and Its Relation to Association and Normal Correlation.pdf:pdf},
journal = {Drapers' Company Research Memoirs Biometric Series I},
pages = {46},
title = {{On the Theory of Contingency and Its Relation to Association and Normal Correlation}},
year = {1904}
}
@article{Barndorff-Nielse1976,
author = {Barndorff-Nielsen, O.},
journal = {Journal of the Royal Statistical Society, Series B},
pages = {103--131},
title = {{Plausibility Inference}},
url = {https://www.jstor.org/stable/2985016},
volume = {38},
year = {1976}
}
@article{Gabry2019,
author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
doi = {10.1111/rssa.12378},
issn = {09641998},
journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
keywords = {Bayesian data analysis,Statistical graphics,Statistical workflow},
month = {feb},
number = {2},
pages = {389--402},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Visualization in Bayesian workflow}},
url = {http://doi.wiley.com/10.1111/rssa.12378},
volume = {182},
year = {2019}
}
@article{Mayo1992,
abstract = {I document some of the main evidence showing that E. S. Pearson rejected the key features of the behavioral-decision philosophy that became associated with the Neyman-Pearson Theory of statistics (NPT). I argue that NPT principles arose not out of behavioral aims, where the concern is solely with behaving correctly sufficiently often in some long run, but out of the epistemological aim of learning about causes of experimental results (e.g., distinguishing genuine from spurious effects). The view Pearson did hold gives a deeper understanding of NPT tests than their typical formulation as 'accept-reject routines', against which criticisms of NPT are really directed. The 'Pearsonian' view that emerges suggests how NPT tests may avoid these criticisms while still retaining what is central to these methods: the control of error probabilities. {\textcopyright} 1992 Kluwer Academic Publishers.},
author = {Mayo, Deborah G.},
doi = {10.1007/BF00485352},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Mayo - 1992 - Did Pearson reject the Neyman-Pearson philosophy of statistics.pdf:pdf},
issn = {00397857},
journal = {Synthese},
keywords = {Epistemology,Logic,Metaphysics,Philosophy of Language,Philosophy of Science},
month = {feb},
number = {2},
pages = {233--262},
publisher = {Kluwer Academic Publishers},
title = {{Did Pearson reject the Neyman-Pearson philosophy of statistics?}},
url = {https://link.springer.com/article/10.1007/BF00485352},
volume = {90},
year = {1992}
}
@book{Bloom1972,
address = {New York},
author = {Bloom, Benjamin S and Engelhart, Max D and Furst, Edward J and Hill, Walker H and Krathwohl, David R},
booktitle = {The Classification of Educational Goals},
number = {Handbook I - Cognitive Domain},
publisher = {David McKay},
title = {{Taxonomy Of Educational Objectives}},
volume = {I},
year = {1972}
}
@article{Amrhein2019,
abstract = {Statistical inference often fails to replicate. One reason is that many results may be selected for drawing inference because some threshold of a statistic like the P-value was crossed, leading to biased reported effect sizes. Nonetheless, considerable non-replication is to be expected even without selective reporting, and generalizations from single studies are rarely if ever warranted. Honestly reported results must vary from replication to replication because of varying assumption violations and random variation; excessive agreement itself would suggest deeper problems, such as failure to publish results in conflict with group expectations or desires. A general perception of a “replication crisis” may thus reflect failure to recognize that statistical tests not only test hypotheses, but countless assumptions and the entire environment in which research takes place. Because of all the uncertain and unknown assumptions that underpin statistical inferences, we should treat inferential statistics as highly unstable local descriptions of relations between assumptions and data, rather than as providing generalizable inferences about hypotheses or models. And that means we should treat statistical results as being much more incomplete and uncertain than is currently the norm. Acknowledging this uncertainty could help reduce the allure of selective reporting: Since a small P-value could be large in a replication study, and a large P-value could be small, there is simply no need to selectively report studies based on statistical results. Rather than focusing our study reports on uncertain conclusions, we should thus focus on describing accurately how the study was conducted, what problems occurred, what data were obtained, what analysis methods were used and why, and what output those methods produced.},
author = {Amrhein, Valentin and Trafimow, David and Greenland, Sander},
doi = {10.1080/00031305.2018.1543137},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Amrhein, Trafimow, Greenland - 2019 - Inferential Statistics as Descriptive Statistics There Is No Replication Crisis if We Don't Expe.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {auxiliary hypotheses,confidence interval,hypothesis test,p -value},
number = {sup1},
pages = {262--270},
title = {{Inferential Statistics as Descriptive Statistics: There Is No Replication Crisis if We Don't Expect Replication}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1543137},
volume = {73},
year = {2019}
}
@article{Johnson1991,
abstract = {This is the second in a series of three papers that empirically examine the competitiveness of simulated annealing in certain well-studied domains of combinatorial optimization. Simulated annealing is a randomized technique proposed by S. Kirkpatrick, C. D. Gelatt and M. P. Vecchi for improving local optimization algorithms. Here we report on experiments at adapting simulated annealing to graph coloring and number partitioning, two problems for which local optimization had not previously been thought suitable. For graph coloring, we report on three simulated annealing schemes, all of which can dominate traditional techniques for certain types of graphs, at least when large amounts of computing time are available. For number partitioning, simulated annealing is not competitive with the differencing algorithm of N. Karmarkar and R. M. Karp, except on relatively small instances. Moreover, if running time is taken into account, natural annealing schemes cannot even outperform multiple random runs of the local...},
author = {Johnson, David S. and Aragon, Cecilia R. and McGeoch, Lyle A. and Schevon, Catherine},
doi = {10.1287/opre.39.3.378},
issn = {0030-364X},
journal = {Operations Research},
keywords = {applications: optimization by simulated annealing,combinatorics: number partitioning heuristics,heuristics: graph coloring heuristics,mathematics,networks/graphs,simulation},
month = {jun},
number = {3},
pages = {378--406},
publisher = { INFORMS },
title = {{Optimization by Simulated Annealing: An Experimental Evaluation; Part II, Graph Coloring and Number Partitioning}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.39.3.378},
volume = {39},
year = {1991}
}
@article{Freedman2015,
abstract = {Low reproducibility rates within life science research undermine cumulative knowledge production and contribute to both delays and costs of therapeutic drug development. An analysis of past studies indicates that the cumulative (total) prevalence of irreproducible preclinical research exceeds 50{\%}, resulting in approximately US{\$}28,000,000,000 (US{\$}28B)/year spent on preclinical research that is not reproducible—in the United States alone. We outline a framework for solutions and a plan for long-term improvements in reproducibility rates that will help to accelerate the discovery of life-saving therapies and cures.},
author = {Freedman, Leonard P. and Cockburn, Iain M. and Simcoe, Timothy S.},
doi = {10.1371/journal.pbio.1002165},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Freedman, Cockburn, Simcoe - 2015 - The Economics of Reproducibility in Preclinical Research.pdf:pdf},
issn = {1545-7885},
journal = {PLOS Biology},
month = {jun},
number = {6},
pages = {e1002165},
publisher = {Public Library of Science},
title = {{The Economics of Reproducibility in Preclinical Research}},
url = {http://dx.plos.org/10.1371/journal.pbio.1002165},
volume = {13},
year = {2015}
}
@inproceedings{Kelter2019a,
abstract = {Teaching and learning programming is a challenge. Although several learning and programming environments have been proposed for classes, there seems to be more dissent than consensus as to which tools are preferable over others. This paper investigates teachers' perspectives on popular learning and programming environments used in secondary computer science education in Germany. The environments investigated are: BlueJ, Scratch, Greenfoot, Eclipse, MIT App Inventor, Processing IDE, and Alice. Based on prior research, a catalogue of environment features supporting the learning processes of students was constructed. Using these criteria, an online-survey was conducted with computer science teachers in North Rhine-Westphalia, Germany. In the survey, the participating teachers evaluated the selected tools' adequacy for teaching object-oriented programming. The findings support the results of prior research conducted with students, stressing the importance of a simple and user-friendly graphical user interface (GUI) as well as the option to visualise classes and objects. Contrary to prior studies, the results show that teachers do not see the editor as equally important, as students do, and that there is no consensus about the role of the area of application for choosing an integrated development environment (IDE). Student-friendly debugging messages as well as a step-by-step execution of programs were identified as important features. Although no tool excelled for every criterion, the clear favourite was BlueJ.},
author = {Kelter, Riko and Kramer, Matthias and Brinda, Torsten},
booktitle = {IFIP Advances in Information and Communication Technology},
doi = {10.1007/978-3-030-23513-0_5},
isbn = {9783030235123},
issn = {1868422X},
keywords = {Educational programming environments,Object-oriented-programming,Teaching and learning programming},
month = {jun},
pages = {47--55},
publisher = {Springer New York LLC},
title = {{Teachers' Perspectives on Learning and Programming Environments for Secondary Education}},
volume = {524},
year = {2019}
}
@article{Evans2018,
abstract = {The "law of practice"-a simple nonlinear function describing the relationship between mean response time (RT) and practice- has provided a practically and theoretically useful way of quantifying the speed-up that characterizes skill acquisition. Early work favored a power law, but this was shown to be an artifact of biases caused by averaging over participants who are individually better described by an exponential law. However, both power and exponential functions make the strong assumption that the speedup always proceeds at a steadily decreasing rate, even though there are sometimes clear exceptions. We propose a new law that can both accommodate an initial delay resulting in a slower-faster-slower rate of learning, with either power or exponential forms as limiting cases, and which can account for not only mean RT but also the effect of practice on the entire distribution of RT. We evaluate this proposal with data from a broad array of tasks using hierarchical Bayesian modeling, which pools data across participants while minimizing averaging artifacts, and using inference procedures that take into account differences in flexibility among laws. In a clear majority of paradigms our results supported a delayed exponential law.},
author = {Evans, Nathan J. and Brown, Scott D. and Mewhort, Douglas J.K. and Heathcote, Andrew},
doi = {10.1037/rev0000105},
issn = {0033295X},
journal = {Psychological Review},
keywords = {Bayesian hierarchical models,Law of practice,Learning,Skill acquisition},
month = {jul},
number = {4},
pages = {592--605},
pmid = {29952624},
publisher = {American Psychological Association Inc.},
title = {{Refining the law of practice}},
url = {/record/2018-30695-005},
volume = {125},
year = {2018}
}
@article{Zabell1989,
abstract = {R. A. Fisher's account of the decline of inverse probability methods during the latter half of the 19th century identifies Boole, Venn and Chrystal as the key figures in this change. Careful examination of these and other writings of the period, however, reveals a different and much more complex picture. Contrary to Fisher's account, inverse methods--at least in modified form--remained theoretically respectable until the 1920's, when the work of Fisher and then Neyman caused their eclipse for the next quarter century.},
author = {Zabell, Sandy},
doi = {10.1214/ss/1177012491},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Zabell - 1989 - R. A. Fisher on the History of Inverse Probability.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {R. A. Fisher,history of statistics,inverse probability},
month = {aug},
number = {3},
pages = {247--256},
publisher = {Institute of Mathematical Statistics},
title = {{R. A. Fisher on the History of Inverse Probability}},
url = {http://projecteuclid.org/euclid.ss/1177012488 http://www.jstor.org/stable/2245634{\%}5Cnpapers2://publication/uuid/E2BC6E95-77D6-4228-B02A-000D51F23D38},
volume = {4},
year = {1989}
}
@article{VanDoorn2019,
abstract = {Despite the increasing popularity of Bayesian inference in empirical research, few practical guidelines provide detailed refile:///Users/tim/Documents/Academia/Papers with my name on them/BayesianGuidelinesFinal(2).pdfcommendations for how to apply Bayesian procedures and interpret the results. Here we offer specific guidelines for four different stages of Bayesian statistical reasoning in a research setting: planning the analysis, executing the analysis, interpreting the results, and reporting the results. The guidelines for each stage are illustrated with a running example. Although the guidelines are geared toward analyses performed with the open-source statistical software JASP, most guidelines extend to Bayesian inference in general.},
archivePrefix = {arXiv},
arxivId = {10.31234/osf.io/yqxfr},
author = {van Doorn, Johnny and van den Bergh, Don and Bohm, Udo and Dablander, Fabian and Derks, Koen and Draws, Tim and Evans, Nathan J and Gronau, Quentin Frederik and Hinne, Max and Kucharsk{\'{y}}, Simon and Ly, Alexander and Marsman, Maarten and Matzke, Dora and Raj, Akash and Sarafoglou, Alexandra and Stefan, Angelika and Voelkel, Jan G and Wagenmakers, Eric-Jan},
doi = {10.31234/osf.io/yqxfr},
eprint = {osf.io/yqxfr},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/van Doorn et al. - 2019 - The JASP Guidelines for Conducting and Reporting a Bayesian Analysis.pdf:pdf},
journal = {psyarxiv preprint, https://psyarxiv.com/yqxfr},
primaryClass = {10.31234},
title = {{The JASP Guidelines for Conducting and Reporting a Bayesian Analysis}},
year = {2019}
}
@book{Kruschke2015,
abstract = {There is an explosion of interest in Bayesian statistics, primarily because recently created computational methods have finally made Bayesian analysis obtainable to a wide audience. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan provides an accessible approach to Bayesian data analysis, as material is explained clearly with concrete examples. The book begins with the basics, including essential concepts of probability and random sampling, and gradually progresses to advanced hierarchical modeling methods for realistic data. Included are step-by-step instructions on how to conduct Bayesian data analyses in the popular and free software R and WinBugs. This book is intended for first-year graduate students or advanced undergraduates. It provides a bridge between undergraduate training and modern Bayesian methods for data analysis, which is becoming the accepted research standard. Knowledge of algebra and basic calculus is a prerequisite.},
address = {Oxford},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kruschke, John K.},
booktitle = {Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan},
doi = {10.1016/B978-0-12-405888-0.09999-2},
edition = {2nd},
eprint = {arXiv:1011.1669v3},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kruschke - 2015 - Doing Bayesian data analysis A tutorial with R, JAGS, and Stan, second edition.pdf:pdf},
isbn = {9780124058880},
issn = {1385-4046},
pages = {1--759},
pmid = {15003161},
publisher = {Academic Press},
title = {{Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan}},
year = {2015}
}
@article{Gelman2019,
abstract = {The new book by philosopher Deborah Mayo is relevant to data science for topical reasons, as she takes various controversial positions regarding hypothesis testing and statistical practice, and also as an entry point to thinking about the philosophy of statistics. The present article is a slightly expanded version of a series of informal reviews and comments on Mayo's book. We hope this discussion will introduce people to Mayo's ideas along with other perspectives on the topics she addresses.},
archivePrefix = {arXiv},
arxivId = {1905.08876},
author = {Gelman, Andrew and Haig, Brian and Hennig, Christian and Owen, Art and Cousins, Robert and Young, Stan and Robert, Christian and Yanofsky, Corey and Wagenmakers, Eric-Jan and Kenett, Ron and Lakeland, Daniel},
eprint = {1905.08876},
journal = {arXiv preprint, http://arxiv.org/abs/1905.08876},
title = {{Many perspectives on Deborah Mayo's "Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars"}},
url = {http://arxiv.org/abs/1905.08876},
year = {2019}
}
@article{Zehna1966,
author = {Zehna, P.W. (1966)},
doi = {10.1214/aoms/1177699475},
isbn = {0003-4851},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {jun},
number = {37},
pages = {744},
publisher = {Institute of Mathematical Statistics},
title = {{Invariance of Maximum Likelihood Estimators}},
url = {http://projecteuclid.org/euclid.aoms/1177699475},
volume = {3},
year = {1966}
}
@article{Martini1984,
abstract = {A procedure for model selection is presented which chooses the model that gives the best prediction of the future observation. The predictive distributions associated with each model are compared by means of the logarithmic utility function. For two nested normal linear models, the choice criterion is the product of the posterior odds ratio and a factor depending on the design point of the future observation. Taking appropriate mean values, a criterion is obtained which is independent of the particular design point. This average criterion differs from the ones proposed by Akaike, Schwarz and others in that it adjusts the likelihood ratio statistic by taking into account not only the difference in dimensionality, but also the estimated distance of the two models.},
author = {Martini, A. San and Spezzaferri, F.},
doi = {10.2307/2345515},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
pages = {296--303},
publisher = {WileyRoyal Statistical Society},
title = {{A Predictive Model Selection Criterion}},
url = {https://www.jstor.org/stable/2345515},
volume = {46},
year = {1984}
}
@inproceedings{eckerdal_relationship_2011,
abstract = {Phenomenographic research studies have identified different understandings of the concepts class and object by novice programmers. Aspects of understanding include a focus on artefacts of text, syntax and structure (text), as active agents in a program (action) and as models of an external reality (model). We explore the hypothesis that these aspects of conceptual understanding form a hierarchy in which mastery of the text aspect is a necessary precondition for understanding objects as active agents and the action aspect is a precondition for model understandings. We use empirical data from the final examination of an introductory programming course to test the relationship between the text and action aspects. Our findings do not support the hypothesis of a hierarchy but rather suggest that text and action understandings develop in parallel.},
address = {New York, NY, USA},
author = {Eckerdal, Anna and Laakso, Mikko-Jussi and Lopez, Mike and Sarkar, Amitrajit},
booktitle = {Proceedings of the 16th Annual Joint Conference on Innovation and Technology in Computer Science Education},
doi = {10.1145/1999747.1999760},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Eckerdal et al. - 2011 - Relationship Between Text and Action Conceptions of Programming A Phenomenographic and Quantitative Perspective.pdf:pdf},
isbn = {978-1-4503-0697-3},
keywords = {critical aspects,hierarchy,novice programming,phenomenographic outcome space,text and action},
pages = {33--37},
publisher = {ACM},
series = {{\{}ITiCSE{\}} '11},
shorttitle = {Relationship {\{}Between{\}} {\{}Text{\}} and {\{}Action{\}} {\{}Concep}},
title = {{Relationship Between Text and Action Conceptions of Programming: A Phenomenographic and Quantitative Perspective}},
url = {http://doi.acm.org/10.1145/1999747.1999760},
year = {2011}
}
@article{Zellner1987,
author = {Zellner, Arnold},
doi = {10.1214/ss/1177013241},
issn = {08834237},
journal = {Statistical Science},
number = {3},
pages = {339--341},
publisher = {Institute of Mathematical Statistics},
title = {{[Testing precise hypotheses]: Comment}},
url = {https://projecteuclid.org/euclid.ss/1177013241},
volume = {2},
year = {1987}
}
@inproceedings{mohammed_cloud-based_2016,
abstract = {The Java programming language is widely used in industry and business. Therefore, academic institutions worldwide include Java learning as a basic part of their Computer Science and Engineering curricula. At the same time, smart devices have become popular among university learners. This research tries to take advantage of this fact to promote Java learning. The main problem is that we cannot compile Java programs on smart devices due to the technical limitations of such devices. This research aims to leverage cloud computing, the availability, prevalence and affordability of smart devices and the ever-growing market of Android devices to provide users with text editors to create and modify Java programs and save them to a server. Users can also compile and execute created programs. A web-based version of the application is also provided for users who do not use Android devices that can be accessed via a browser on a PC or Smart device. The system uses an existing online compiler. The developed cloud-based compiler can be integrated into a smart multimedia learning system for learning the Java programming language.},
annote = {Nigeria {\&} Japan},
author = {Mohammed, T Y and Hamada, M},
booktitle = {2016 15th {\{}International{\}} {\{}Conference{\}} on {\{}Information{\}} {\{}Technology{\}} {\{}Based{\}} {\{}Higher{\}} {\{}Education{\}} and {\{}Training{\}} ({\{}ITHET{\}})},
doi = {10.1109/ITHET.2016.7760742},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Mohammed, Hamada - 2016 - A cloud-based Java compiler for smart devices.html:html},
keywords = {Android devices,Androids,Cloud Compiler,Computer Science and Engineering curriculum,Computer aided instruction,Computer science education,Humanoid robots,Java learning,Java program compiling,Java program modification,Java programming,Java programming language learning,Learning Technologies,Multimedia System,Operating systems,Program processors,Servers,Web-based application version,cloud computing,cloud-based Java compiler,java,mobile learning,object-oriented programming,online compiler,program compilers,program execution,smart devices,smart multimedia learning system,smart phones,text editor},
month = {sep},
pages = {1--6},
title = {{A cloud-based Java compiler for smart devices}},
year = {2016}
}
@book{Berzuini2012,
address = {Chichester, UK},
author = {Berzuini, C and Dawid, P and Bernardinell, L and VanderWeele, Tyler J and Hern{\'{a}}n, Miguel A},
booktitle = {Causality: Statistical Perspectives and Applications},
editor = {Berzuini, Carlo and Dawid, Philip and Bernadinell, Luisa},
isbn = {9780470665565},
publisher = {Wiley},
title = {{Causality: Statistical Perspectives and Applications}},
year = {2012}
}
@article{yotongyos_undergraduate_2015,
abstract = {Statistical literacy has been recognized as essential knowledge that all citizens need to possess in today's information-driven society. Undergraduate students require statistical knowhow and skills to apply in their studies and everyday's life. The aim of this study was to assess the level of statistical literacy among undergraduate students in Thailand. The two element model of statistical literacy by Gal (2004), knowledge component (comprised of five cognitive elements: literacy skills, statistical knowledge, mathematical knowledge, context knowledge, and critical questions) and a dispositional component (comprised of two elements: critical stance, and beliefs and attitudes), was used. A survey was administered to 103 undergraduate students of Faculty of Education, Chulalongkorn University. The results revealed that the undergraduate students had moderate level of overall statistical literacy, knowledge component and a dispositional component. In the knowledge component, the students had high level of literacy skills and mathematical knowledge, moderate level of statistical knowledge and critical questions, and low level of context knowledge. Additionally, in the dispositional component, the students had moderate level of critical stance, and beliefs and attitudes. The practical implications for educators to improve students' statistical literacy were discussed.},
author = {Yotongyos, Marayat and Traiwichitkhun, Duangkamol and Kaemkate, Wannee},
doi = {10.1016/j.sbspro.2015.04.328},
issn = {1877-0428},
journal = {Procedia - Social and Behavioral Sciences},
keywords = {Attitude toward Statistics,Statistical Literac,Statistics Knowledge},
month = {jun},
number = {Supplement C},
pages = {2731--2734},
series = {The {\{}Proceedings{\}} of 6th {\{}World{\}} {\{}Conference{\}} on educational {\{}Sciences{\}}},
shorttitle = {Undergraduate {\{}Students{\}}' {\{}Statistical{\}} {\{}Literacy{\}}},
title = {{Undergraduate Students' Statistical Literacy: A Survey Study}},
url = {http://www.sciencedirect.com/science/article/pii/S1877042815025884},
volume = {191},
year = {2015}
}
@article{Hubbard1997,
abstract = {Because the widespread use of statistical significance testing has deleterious consequences for the development of a cumulative knowledge base, the American Psychological Association's Board of Scientific Affairs is in the process of appointing a Task Force whose charge includes the possibility of phasing out such testing in textbooks and journal articles. Just how popular is significance testing in psychology? This issue is examined in the present historical study, which uses data from randomly selected issues of the Journal of Applied Psychology for the period 1917-94. Results indicate that the practice of significance testing, at one time of restricted usage, has expanded to the point that it is virtually synonymous with empirical analysis. The data also lend support to Gigerenzer and Murray's (1987) allegation that an inference revolution occurred in psychology during the period 1940-55. Unfortunately, it is concluded that the ubiquity of significance testing constitutes a classic example of the overadoption of a methodology. {\textcopyright} 1997, Sage Publications. All rights reserved.},
author = {Hubbard, Raymond and Parsa, Rahul A. and Luthy, Michael R.},
doi = {10.1177/0959354397074006},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hubbard, Parsa, Luthy - 1997 - The Spread of Statistical Significance Testing in Psychology.pdf:pdf},
issn = {0959-3543},
journal = {Theory {\&} Psychology},
keywords = {inference revolution,p values,replication,significance testing,teaching statistics},
month = {aug},
number = {4},
pages = {545--554},
publisher = {Sage PublicationsSage CA: Thousand Oaks, CA},
title = {{The Spread of Statistical Significance Testing in Psychology}},
url = {http://journals.sagepub.com/doi/10.1177/0959354397074006},
volume = {7},
year = {1997}
}
@incollection{Liu2004,
address = {New York},
author = {Liu, Jun S.},
booktitle = {Monte Carlo Strategies in Scientific Computing},
doi = {10.1007/978-0-387-76371-2_9},
editor = {Liu, Jun S.},
pages = {183--203},
publisher = {Springer Verlag New York},
title = {{Molecular Dynamics and Hybrid Monte Carlo}},
url = {http://link.springer.com/10.1007/978-0-387-76371-2{\_}9},
year = {2004}
}
@article{Newton2012,
abstract = {A stochastic Markov chain model for metastatic progression is developed for primary lung cancer based on a network construction of metastatic sites with dynamics modeled as an ensemble of random walkers on the network. We calculate a transition matrix, with entries (transition probabilities) interpreted as random variables, and use it to construct a circular bi-directional network of primary and metastatic locations based on postmortem tissue analysis of 3827 autopsies on untreated patients documenting all primary tumor locations and metastatic sites from this population. The resulting 50 potential metastatic sites are connected by directed edges with distributed weightings, where the site connections and weightings are obtained by calculating the entries of an ensemble of transition matrices so that the steady-state distribution obtained from the long-time limit of the Markov chain dynamical system corresponds to the ensemble metastatic distribution obtained from the autopsy data set. We condition our search for a transition matrix on an initial distribution of metastatic tumors obtained from the data set. Through an iterative numerical search procedure, we adjust the entries of a sequence of approximations until a transition matrix with the correct steady-state is found (up to a numerical threshold). Since this constrained linear optimization problem is underdetermined, we characterize the statistical variance of the ensemble of transition matrices calculated using the means and variances of their singular value distributions as a diagnostic tool. We interpret the ensemble averaged transition probabilities as (approximately) normally distributed random variables. The model allows us to simulate and quantify disease progression pathways and timescales of progression from the lung position to other sites and we highlight several key findings based on the model.},
author = {Newton, Paul K. and Mason, Jeremy and Bethel, Kelly and Bazhenova, Lyudmila A. and Nieva, Jorge and Kuhn, Peter},
doi = {10.1371/journal.pone.0034637},
editor = {Ermentrout, Bard},
isbn = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
month = {apr},
number = {4},
pages = {e34637},
pmid = {22558094},
title = {{A stochastic markov chain model to describe lung cancer growth and metastasis}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22558094 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3338733 http://dx.plos.org/10.1371/journal.pone.0034637},
volume = {7},
year = {2012}
}
@article{RStan2020,
author = {{Stan Development Team}},
journal = {R package version 2.19.3},
title = {{RStan: the R interface to Stan}},
url = {http://mc-stan.org/},
year = {2020}
}
@book{kuchlin_einfuhrung_2005,
address = {Berlin},
annote = {Literaturverz. S. [445] - 448
OCLC: 76686604},
author = {K{\"{u}}chlin, Wolfgang and Weber, Andreas},
edition = {3., {\"{u}}berar},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/K{\"{u}}chlin, Weber - 2005 - Einf{\"{u}}hrung in die Informatik.pdf:pdf},
isbn = {978-3-540-20958-4},
keywords = {Electronic data processing,Java (Computer program language),Java 2,Objektorientierte Programmierung,java},
publisher = {Springer},
series = {{\{}eXamen{\}}.press},
shorttitle = {Einf{\"{u}}hrung in die {\{}Informatik{\}}},
title = {{Einf{\"{u}}hrung in die Informatik}},
year = {2005}
}
@book{Oksendal1998,
abstract = {Fifth edition. This book gives an introduction to the basic theory of stochastic calculus and its applications. Examples are given throughout the text, in order to motivate and illustrate the theory and show its importance for many applications in e.g. economics, biology and physics. The basic idea of the presentation is to start from some basic results (without proofs) of the easier cases and develop the theory from there, and to concentrate on the proofs of the easier case (which nevertheless are often sufficiently general for many purposes) in order to be able to reach quickly the parts of the theory which is most important for the applications. The new feature of this 5th edition is an extra chapter on applications to mathematical finance. Introduction -- Some Mathematical Preliminaries -- Ito Integrals -- Ito Processes and the Ito Formula -- Stochastic Differential Equations -- The Filtering Problem -- Diffusions: Basic Problems -- Other Topics in Diffusion Theory -- Applications to Boundary Value Problems -- Applications to Optimal Stopping -- Application to Stochastic Control -- Application to Mathematical Finance -- Appendix A: Normal Random Variables -- Appendix B: Conditional Expectations -- Appendix C: Uniform Integrability and Martingale Convergence -- Solutions and Additional Hints to Some of the Exercises -- Bibliography -- List of Frequently Used Notation and Symbols -- Index.},
author = {{\O}ksendal, Bernt.},
isbn = {9783662036204},
pages = {324},
publisher = {Springer Berlin Heidelberg},
title = {{Stochastic Differential Equations : an Introduction with Applications}},
year = {1998}
}
@article{engel_statistical_2017,
author = {Engel, JOACHIM},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Engel - 2017 - Statistical literacy for active citizenship A call for data science education.pdf:pdf},
journal = {Statistics Education Research Journal},
number = {1},
pages = {44--49},
shorttitle = {Statistical literacy for active citizenship},
title = {{Statistical literacy for active citizenship: A call for data science education}},
volume = {16},
year = {2017}
}
@book{Feller1968,
address = {New York},
author = {Feller, W.},
edition = {Vol. I, 3r},
publisher = {John Wiley {\&} Sons},
title = {{An Introduction to Probability Theory and its Applications}},
year = {1968}
}
@article{Xinogalos2012a,
abstract = {Programming microworlds are being used for introducing students to programming for many years. Although many professors and school teachers report positive results from using programming microwords, these results are usually based on anecdotal evidence rather than rigorous empirical evaluation. A question that has not been answered yet with certainty is whether the knowledge acquired in the context of a programming microworld is transferred to a conventional programming language. In an attempt to investigate this issue we used a specially designed middle-term exam and a questionnaire at the end of an undergraduate Object-Oriented Programming (OOP) course. The course uses the programming microworld objectKarel for a short introduction to the most fundamental OOP concepts, and then the environment BlueJ and Java for presenting the real thing. The analysis of students' replies in the questionnaire shows that the introduction to OOP with the microworld helps the vast majority of students to comprehend fundamental concepts of OOP and what is more important is that this knowledge is transferred to Java afterwards. These results are supported by the comparative analysis of students' performance in written exams that took place in the context of two distinct offerings of the course, prior and after the use of the microworld in it. (Contains 3 figures and 4 tables.)},
author = {Xinogalos, Stelios},
doi = {10.2190/EC.47.3.b},
isbn = {0735-6331},
issn = {0735-6331},
journal = {Journal of Educational Computing Research},
month = {oct},
number = {3},
pages = {251--277},
title = {{An Evaluation of Knowledge Transfer from Microworld Programming to Conventional Programming}},
url = {http://journals.sagepub.com/doi/10.2190/EC.47.3.b},
volume = {47},
year = {2012}
}
@article{Aarts2015,
abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47{\%} of original effect sizes were in the 95{\%} confidence interval of the replication effect size; 39{\%} of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68{\%} with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
author = {Aarts, Alexander A. and Anderson, Joanna E. and Anderson, Christopher J. and Attridge, Peter R. and Attwood, Angela and Axt, Jordan and Babel, Molly and Bahn{\'{i}}k, {\v{S}}t{\v{e}}p{\'{a}}n and Baranski, Erica and Barnett-Cowan, Michael and Bartmess, Elizabeth and Beer, Jennifer and Bell, Raoul and Bentley, Heather and Beyan, Leah and Binion, Grace and Borsboom, Denny and Bosch, Annick and Bosco, Frank A. and Bowman, Sara D. and Brandt, Mark J. and Braswell, Erin and Brohmer, Hilmar and Brown, Benjamin T. and Brown, Kristina and Br{\"{u}}ning, Jovita and Calhoun-Sauls, Ann and Callahan, Shannon P. and Chagnon, Elizabeth and Chandler, Jesse and Chartier, Christopher R. and Cheung, Felix and Christopherson, Cody D. and Cillessen, Linda and Clay, Russ and Cleary, Hayley and Cloud, Mark D. and Conn, Michael and Cohoon, Johanna and Columbus, Simon and Cordes, Andreas and Costantini, Giulio and Alvarez, Leslie D.Cramblet and Cremata, Ed and Crusius, Jan and DeCoster, Jamie and DeGaetano, Michelle A. and Penna, Nicol{\'{a}}s Delia and {Den Bezemer}, Bobby and Deserno, Marie K. and Devitt, Olivia and Dewitte, Laura and Dobolyi, David G. and Dodson, Geneva T. and Donnellan, M. Brent and Donohue, Ryan and Dore, Rebecca A. and Dorrough, Angela and Dreber, Anna and Dugas, Michelle and Dunn, Elizabeth W. and Easey, Kayleigh and Eboigbe, Sylvia and Eggleston, Casey and Embley, Jo and Epskamp, Sacha and Errington, Timothy M. and Estel, Vivien and Farach, Frank J. and Feather, Jenelle and Fedor, Anna and Fern{\'{a}}ndez-Castilla, Bel{\'{e}}n and Fiedler, Susann and Field, James G. and Fitneva, Stanka A. and Flagan, Taru and Forest, Amanda L. and Forsell, Eskil and Foster, Joshua D. and Frank, Michael C. and Frazier, Rebecca S. and Fuchs, Heather and Gable, Philip and Galak, Jeff and Galliani, Elisa Maria and Gampa, Anup and Garcia, Sara and Gazarian, Douglas and Gilbert, Elizabeth and Giner-Sorolla, Roger and Gl{\"{o}}ckner, Andreas and Goellner, Lars and Goh, Jin X. and Goldberg, Rebecca and Goodbourn, Patrick T. and Gordon-McKeon, Shauna and Gorges, Bryan and Gorges, Jessie and Goss, Justin and Graham, Jesse and Grange, James A. and Gray, Jeremy and Hartgerink, Chris and Hartshorne, Joshua and Hasselman, Fred and Hayes, Timothy and Heikensten, Emma and Henninger, Felix and Hodsoll, John and Holubar, Taylor and Hoogendoorn, Gea and Humphries, Denise J. and Hung, Cathy O.Y. and Immelman, Nathali and Irsik, Vanessa C. and Jahn, Georg and J{\"{a}}kel, Frank and Jekel, Marc and Johannesson, Magnus and Johnson, Larissa G. and Johnson, David J. and Johnson, Kate M. and Johnston, William J. and Jonas, Kai and Joy-Gaba, Jennifer A. and Kappes, Heather Barry and Kelso, Kim and Kidwell, Mallory C. and Kim, Seung Kyung and Kirkhart, Matthew and Kleinberg, Bennett and Kne{\v{z}}evi{\'{c}}, Goran and Kolorz, Franziska Maria and Kossakowski, Jolanda J. and Krause, Robert Wilhelm and Krijnen, Job and Kuhlmann, Tim and Kunkels, Yoram K. and Kyc, Megan M. and Lai, Calvin K. and Laique, Aamir and Lakens, Dani{\"{e}}l and Lane, Kristin A. and Lassetter, Bethany and Lazarevi{\'{c}}, Ljiljana B. and {Le Bel}, Etienne P. and Lee, Key Jung and Lee, Minha and Lemm, Kristi and Levitan, Carmel A. and Lewis, Melissa and Lin, Lin and Lin, Stephanie and Lippold, Matthias and Loureiro, Darren and Luteijn, Ilse and MacKinnon, Sean and Mainard, Heather N. and Marigold, Denise C. and Martin, Daniel P. and Martinez, Tylar and Masicampo, E. J. and Matacotta, Josh and Mathur, Maya and May, Michael and Mechin, Nicole and Mehta, Pranjal and Meixner, Johannes and Melinger, Alissa and Miller, Jeremy K. and Miller, Mallorie and Moore, Katherine and M{\"{o}}schl, Marcus and Motyl, Matt and M{\"{u}}ller, Stephanie M. and Munafo, Marcus and Neijenhuijs, Koen I. and Nervi, Taylor and Nicolas, Gandalf and Nilsonne, Gustav and Nosek, Brian A. and Nuijten, Mich{\`{e}}le B. and Olsson, Catherine and Osborne, Colleen and Ostkamp, Lutz and Pavel, Misha and Penton-Voak, Ian S. and Perna, Olivia and Pernet, Cyril and Perugini, Marco and Pipitone, R. Nathan and Pitts, Michael and Plessow, Franziska and Prenoveau, Jason M. and Rahal, Rima Maria and Ratliff, Kate A. and Reinhard, David and Renkewitz, Frank and Ricker, Ashley A. and Rigney, Anastasia and Rivers, Andrew M. and Roebke, Mark and Rutchick, Abraham M. and Ryan, Robert S. and Sahin, Onur and Saide, Anondah and Sandstrom, Gillian M. and Santos, David and Saxe, Rebecca and Schlegelmilch, Ren{\'{e}} and Schmidt, Kathleen and Scholz, Sabine and Seibel, Larissa and Selterman, Dylan Faulkner and Shaki, Samuel and Simpson, William B. and Sinclair, H. Colleen and Skorinko, Jeanine L.M. and Slowik, Agnieszka and Snyder, Joel S. and Soderberg, Courtney and Sonnleitner, Carina and Spencer, Nick and Spies, Jeffrey R. and Steegen, Sara and Stieger, Stefan and Strohminger, Nina and Sullivan, Gavin B. and Talhelm, Thomas and Tapia, Megan and {Te Dorsthorst}, Anniek and Thomae, Manuela and Thomas, Sarah L. and Tio, Pia and Traets, Frits and Tsang, Steve and Tuerlinckx, Francis and Turchan, Paul and Val{\'{a}}{\v{s}}ek, Milan and {Van't Veer}, Anna E. and {Van Aert}, Robbie and {Van Assen}, Marcel and {Van Bork}, Riet and {Van De Ven}, Mathijs and {Van Den Bergh}, Don and {Van Der Hulst}, Marije and {Van Dooren}, Roel and {Van Doorn}, Johnny and {Van Renswoude}, Daan R. and {Van Rijn}, Hedderik and Vanpaemel, Wolf and Echeverr{\'{i}}a, Alejandro V{\'{a}}squez and Vazquez, Melissa and Velez, Natalia and Vermue, Marieke and Verschoor, Mark and Vianello, Michelangelo and Voracek, Martin and Vuu, Gina and Wagenmakers, Eric Jan and Weerdmeester, Joanneke and Welsh, Ashlee and Westgate, Erin C. and Wissink, Joeri and Wood, Michael and Woods, Andy and Wright, Emily and Wu, Sining and Zeelenberg, Marcel and Zuni, Kellylynn},
doi = {10.1126/science.aac4716},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Aarts et al. - 2015 - Estimating the reproducibility of psychological science.pdf:pdf},
issn = {0036-8075},
journal = {Science},
month = {aug},
number = {6251},
pages = {aac4716},
publisher = {American Association for the Advancement of Science},
title = {{Estimating the reproducibility of psychological science}},
url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aac4716},
volume = {349},
year = {2015}
}
@article{Verdinelli1995,
abstract = {We present a simple method for computing Bayes factors. The method derives from observing that in general, a Bayes factor can be written as the product of a quantity called the Savage–Dickey density ratio and a correction factor; both terms are easily estimated from posterior simulation. In some cases it is possible to do these computations without ever evaluating the likelihood. {\textcopyright} 1995 Taylor {\&} Francis Group, LLC.},
author = {Verdinelli, Isabella and Wasserman, Larry},
doi = {10.1080/01621459.1995.10476554},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Bayes factor,Hypothesis testing,Markov chain Monte Carlo},
number = {430},
pages = {614--618},
title = {{Computing Bayes factors using a generalization of the Savage-Dickey density ratio}},
volume = {90},
year = {1995}
}
@book{Jackman2009a,
address = {Chichester, UK},
author = {Jackman, Simon},
doi = {10.1002/9780470686621},
isbn = {9780470686621},
month = {oct},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Bayesian Analysis for the Social Sciences}},
url = {http://doi.wiley.com/10.1002/9780470686621},
year = {2009}
}
@book{snedecor1937,
address = {Ames, Iowa},
author = {Snedecor, George W.},
edition = {1st},
publisher = {Collegiate Press},
title = {{Statistical Methods}},
year = {1937}
}
@incollection{Jeffreys1980,
address = {Amsterdam, The Netherlands},
author = {Jeffreys, Harold},
booktitle = {Bayesian Analysis in Econometrics and Statistics : Essays in Honor of Harold Jeffreys},
editor = {Zellner, Arnold and Kadane, Joseph B.},
pages = {451--453},
publisher = {North-Holland Publishing Company},
title = {{Some General Points in Probability Theory}},
year = {1980}
}
@book{Porter2006,
address = {Princeton},
author = {Porter, Theodore M.},
isbn = {0-691-12635-6},
publisher = {Princeton University Press},
title = {{Karl Pearson: The Scientific Life in a Statistical Age}},
year = {2006}
}
@article{USFoodAndDrugAdministrationBioequivalence2001,
author = {{U.S. Food and Drug Administration Center for Drug Evaluation and Research}},
journal = {Web archive: https://www.fda.gov/regulatory-information/search-fda-guidance-documents/statistical-approaches-establishing-bioequivalence (accessed 01/03/2021)},
title = {{Guidance for industry: Statistical approaches to establishing bioequivalence}},
url = {https://www.fda.gov/regulatory-information/search-fda-guidance-documents/statistical-approaches-establishing-bioequivalence},
year = {2001}
}
@article{Wang2016,
abstract = {In this paper, we propose an explicit closed-form Bayes factor for the problem of two-sample hypothesis testing. The proposed approach can be regarded as a Bayesian version of the pooled-variance t-statistic and has various appealing properties in practical applications. It relies on data only through the t-statistic and can thus be calculated by using an Excel spreadsheet or a pocket calculator. It avoids several undesirable paradoxes, which may be encountered by the previous Bayesian approach of Gonen et al. (2005). Specifically, the proposed approach can be easily taught in an introductory statistics course with an emphasis on Bayesian thinking. Simulated and real data examples are provided for illustrative purposes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.02568v1},
author = {Wang, Min and Liu, Guangying},
doi = {10.1080/00031305.2015.1093027},
eprint = {arXiv:1509.02568v1},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Liu - 2016 - A Simple Two-Sample Bayesian t-Test for Hypothesis Testing.pdf:pdf},
issn = {15372731},
journal = {American Statistician},
keywords = {Bayes factor,Pooled-variance t-statistic,Posterior probability,Prior elicitation,Teaching elementary statistics},
number = {2},
pages = {195--201},
title = {{A Simple Two-Sample Bayesian t-Test for Hypothesis Testing}},
volume = {70},
year = {2016}
}
@article{LeCam1953,
author = {{Le Cam}, L.},
journal = {University of California Publications in Statistics},
pages = {277--330},
title = {{On some asymptotic properties of maximum-likelihood estimates and related Bayes estimates}},
volume = {1},
year = {1953}
}
@article{Stigler2007,
abstract = {At a superficial level, the idea of maximum likelihood must be prehistoric: early hunters and gatherers may not have used the words ``method of maximum likelihood'' to describe their choice of where and how to hunt and gather, but it is hard to believe they would have been surprised if their method had been described in those terms. It seems a simple, even unassailable idea: Who would rise to argue in favor of a method of minimum likelihood, or even mediocre likelihood? And yet the mathematical history of the topic shows this ``simple idea'' is really anything but simple. Joseph Louis Lagrange, Daniel Bernoulli, Leonard Euler, Pierre Simon Laplace and Carl Friedrich Gauss are only some of those who explored the topic, not always in ways we would sanction today. In this article, that history is reviewed from back well before Fisher to the time of Lucien Le Cam's dissertation. In the process Fisher's unpublished 1930 characterization of conditions for the consistency and efficiency of maximum likelihood estimates is presented, and the mathematical basis of his three proofs discussed. In particular, Fisher's derivation of the information inequality is seen to be derived from his work on the analysis of variance, and his later approach via estimating functions was derived from Euler's Relation for homogeneous functions. The reaction to Fisher's work is reviewed, and some lessons drawn.},
archivePrefix = {arXiv},
arxivId = {0804.2996},
author = {Stigler, Stephen M.},
doi = {10.1214/07-STS249},
eprint = {0804.2996},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Stigler - 2007 - The Epic Story of Maximum Likelihood.pdf:pdf},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {a,abraham wald,and phrases,cial philosopher-scientist herbert spencer,ef-,ficiency,fisher,harold hotelling,history of statistics,jerzy neyman,karl pearson,maximum likelihood,one eve-,r,sufficiency,superefficiency},
number = {4},
pages = {598--620},
title = {{The Epic Story of Maximum Likelihood}},
url = {http://projecteuclid.org/euclid.ss/1207580174},
volume = {22},
year = {2007}
}
@incollection{Hinkley1980b,
author = {Hinkley, David},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_1},
pages = {1--5},
publisher = {Springer, New York, NY},
title = {{R.A. Fisher: Some Introductory Remarks}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}1},
year = {1980}
}
@article{Douven2012,
abstract = {Some of the information we receive comes to us in an explicitly conditional form. It is an open question how to model the accommodation of such information in a Bayesian framework. This paper presents data suggesting that there may be no strictly Bayesian account of updating on conditionals. Specifically, the data seem to indicate that such updating at least sometimes proceeds on the basis of explanatory considerations, which famously have no home in standard Bayesian epistemology. The paper also proposes a still broadly Bayesian model of updating on conditionals that explicitly factors in explanation. The model is shown to have clear empirical content and thus to be open to empirical testing. {\textcopyright} 2012 Blackwell Publishing Ltd.},
author = {Douven, Igor},
doi = {10.1111/j.1468-0017.2012.01443.x},
issn = {02681064},
journal = {Mind and Language},
month = {jun},
number = {3},
pages = {239--263},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Learning Conditional Information}},
volume = {27},
year = {2012}
}
@article{Kelter2021BehaviorResearchMethodsFBSTPackage,
author = {Kelter, Riko},
doi = {https://doi.org/10.3758/s13428-021-01613-6},
journal = {Behavior Research Methods},
title = {{fbst: An R package for the Full Bayesian Significance Test for testing a sharp null hypothesis against its alternative via the e-value}},
volume = {in press},
year = {2021}
}
@article{Weber2012,
author = {Weber, R. and Popova, L.},
doi = {10.1080/19312458.2012.703834},
journal = {Communication Methods and Measures},
number = {3},
pages = {190--213},
title = {{Testing equivalence in communication research: theory and application}},
volume = {6},
year = {2012}
}
@article{Wetzels2011,
abstract = {Statistical inference in psychology has traditionally relied heavily on p-value significance testing. This approach to drawing conclusions from data, however, has been widely criticized, and two types of remedies have been advocated. The first proposal is to supplement p values with complementary measures of evidence, such as effect sizes. The second is to replace inference with Bayesian measures of evidence, such as the Bayes factor. The authors provide a practical comparison of p values, effect sizes, and default Bayes factors as measures of statistical evidence, using 855 recently published t tests in psychology. The comparison yields two main results. First, although p values and default Bayes factors almost always agree about what hypothesis is better supported by the data, the measures often disagree about the strength of this support; for 70{\%} of the data sets for which the p value falls between .01 and .05, the default Bayes factor indicates that the evidence is only anecdotal. Second, effect sizes can provide additional evidence to p values and default Bayes factors. The authors conclude that the Bayesian approach is comparatively prudent, preventing researchers from overestimating the evidence in favor of an effect.},
author = {Wetzels, Ruud and Matzke, Dora and Lee, Michael D. and Rouder, Jeffrey N. and Iverson, Geoffrey J. and Wagenmakers, Eric-Jan},
doi = {10.1177/1745691611406923},
isbn = {1745-6916},
issn = {17456916},
journal = {Perspectives on Psychological Science},
keywords = {Bayes factor experimental,Effect size,Hypothesis testing,P value,T test},
number = {3},
pages = {291--298},
pmid = {26168519},
title = {{Statistical evidence in experimental psychology: An empirical comparison using 855 t tests}},
volume = {6},
year = {2011}
}
@article{fisher_1951,
author = {Fisher, Ronald A.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1951 - Contributions to Mathematical Statistics by R.A. Fisher.pdf:pdf},
journal = {Biometrika},
number = {1},
pages = {257--259},
title = {{Contributions to Mathematical Statistics by R.A. Fisher}},
volume = {38},
year = {1951}
}
@misc{BANOVA2019,
author = {Dong, Chen and Wedel, Michel},
publisher = {Comprehensive R Archive Network},
title = {{BANOVA: Hierarchical Bayesian ANOVA Models}},
url = {https://cran.r-project.org/package=BANOVA},
year = {2019}
}
@article{TannerWong1987,
author = {Tanner, Martin; and Wong, Wing Hung},
doi = {10.2307/2289463},
isbn = {0162-1459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
number = {398},
pages = {528--540},
title = {{The Calculation of Posterior Distributions by Data Augmentation}},
volume = {82},
year = {1987}
}
@book{Edwards1954,
address = {New York},
author = {Edwards, Allen L.},
publisher = {Rinehart},
title = {{Statistical methods for the behavioral sciences}},
year = {1954}
}
@inproceedings{hubwieser_global_2015,
abstract = {In two special issues of the ACM journal "Transactions on Computing Education" (TOCE), 14 extensive case studies about the various situations of Computer Science Education (CSE) in K-12 schools in 12 countries (respectively states) were collected. During the work at the ITiCSE 2015, we have performed a deductive qualitative text analysis on these case studies in order to extract the most useful information. As a category system, we applied some selected categories of the Darmstadt Model that was developed by the working group "Computer Science/Informatics in Secondary Schools" at the ITiCSE 2011. Based on the coding results, we summarized information about the different fields of Computing Education at schools, the intended goals and competencies, the taught content, the applied programming languages and tools and the different forms of assessment and teacher education. Despite the limitations of the analyzed articles, representing just snapshots of complex situations from the specific viewpoint of the respective authors, we were able to collect some interesting results.},
address = {New York, NY, USA},
annote = {Germany, Norway, Slovakia, Lithau},
author = {Hubwieser, Peter and Giannakos, Michail N and Berges, Marc and Brinda, Torsten and Diethelm, Ira and Magenheim, Johannes and Pal, Yogendra and Jackova, Jana and Jasute, Egle},
booktitle = {Proceedings of the 2015 ITiCSE on Working Group Reports},
doi = {10.1145/2858796.2858799},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hubwieser et al. - 2015 - A Global Snapshot of Computer Science Education in K-12 Schools.pdf:pdf},
isbn = {978-1-4503-4146-2},
keywords = {competencies,computing education,teacher education},
pages = {65--83},
publisher = {ACM},
series = {{\{}ITICSE{\}}-{\{}WGR{\}} '15},
title = {{A Global Snapshot of Computer Science Education in K-12 Schools}},
url = {http://doi.acm.org/10.1145/2858796.2858799},
year = {2015}
}
@book{alasuutari_sage_2008,
address = {Los Angeles},
annote = {OCLC: ocn170955260},
editor = {Alasuutari, Pertti and Bickman, Leonard and Brannen, Julia},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2008 - The SAGE handbook of social research methods.pdf:pdf},
isbn = {978-1-4129-1992-0},
keywords = {Research Methodology,Social sciences},
publisher = {SAGE},
title = {{The SAGE handbook of social research methods}},
year = {2008}
}
@incollection{CarotaParmigiani1996,
address = {London},
author = {Carota, C. and Parmigiani, G.},
booktitle = {Bayesian Statistics (Vol. 5)},
editor = {Bernado, J.M. and Berger, J.O. and Dawid, A.P. and Smith, A.F.M.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Carota, Carota, Parmigiani - 1996 - On Bayes Factors for Nonparametric Alternatives.pdf:pdf},
pages = {507--511},
publisher = {Oxford University Press},
title = {{On Bayes Factors for Nonparametric Alternatives}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.38.8115},
year = {1996}
}
@book{lawless2014,
author = {Lawless, Jerald F.},
editor = {Lawless, Jerald F.},
publisher = {Chapman {\&} Hall/CRC},
title = {{Statistics in Action - A Canadian Outlook}},
year = {2014}
}
@article{Bancroft1944,
author = {Bancroft, T. A.},
doi = {10.1214/aoms/1177731284},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bancroft - 1944 - On Biases in Estimation Due to the Use of Preliminary Tests of Significance.pdf:pdf},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {jun},
number = {2},
pages = {190--204},
publisher = {Institute of Mathematical Statistics},
title = {{On Biases in Estimation Due to the Use of Preliminary Tests of Significance}},
volume = {15},
year = {1944}
}
@article{Cauchemez2004,
abstract = {We propose a transmission model to estimate the main characteristics of influenza transmission in households. The model details the risks of infection in the household and in the community at the individual scale. Heterogeneity among subjects is investigated considering both individual susceptibility and infectiousness. The model was applied to a data set consisting of the follow-up of influenza symptoms in 334 households during 15 days after an index case visited a general practitioner with virologically confirmed influenza.Estimating the parameters of the transmission model was challenging because a large part of the infectious process was not observed: only the dates when new cases were detected were observed. For each case, the data were augmented with the unobserved dates of the start and the end of the infectious period. The transmission model was included in a 3-levels hierarchical structure: (i) the observation level ensured that the augmented data were consistent with the observed data, (ii) the transmission level described the underlying epidemic process, (iii) the prior level specified the distribution of the parameters. From a Bayesian perspective, the joint posterior distribution of model parameters and augmented data was explored by Markov chain Monte Carlo (MCMC) sampling.The mean duration of influenza infectious period was estimated at 3.8 days (95 per cent credible interval, 95 per cent CI [3.1,4.6]) with a standard deviation of 2.0 days (95 per cent CI [1.1,2.8]). The instantaneous risk of influenza transmission between an infective and a susceptible within a household was found to decrease with the size of the household, and established at 0.32 person day(-1) (95 per cent CI [0.26,0.39]); the instantaneous risk of infection from the community was 0.0056 day(-1) (95 per cent CI [0.0029,0.0087]). Focusing on the differences in transmission between children (less than 15 years old) and adults, we estimated that the former were more likely to transmit than adults (posterior probability larger than 99 per cent), but that the mean duration of the infectious period was similar in children (3.6 days, 95 per cent CI [2.3,5.2]) and adults (3.9 days, 95 per cent CI [3.2,4.9]). The posterior probability that children had a larger community risk was 76 per cent and the posterior probability that they were more susceptible than adults was 79 per cent.},
archivePrefix = {arXiv},
arxivId = {0910.0139},
author = {Cauchemez, Simon and Carrat, F. and Viboud, C. and Valleron, A. J. and Bo{\"{e}}lle, P. Y.},
doi = {10.1002/sim.1912},
eprint = {0910.0139},
isbn = {0277-6715 (Print)$\backslash$r0277-6715 (Linking)},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {Bayesian inference,Household transmission,Infectious diseases,Influenza,Markov chain Monte Carlo methods},
month = {nov},
number = {22},
pages = {3469--3487},
pmid = {15505892},
publisher = {Wiley-Blackwell},
title = {{A Bayesian MCMC approach to study transmission of influenza: Application to household longitudinal data}},
url = {http://doi.wiley.com/10.1002/sim.1912},
volume = {23},
year = {2004}
}
@article{MacKay2016,
author = {MacKay, Jock},
doi = {10.1111/insr.12154},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/MacKay - 2016 - Discussion Acquiring Statistical Literacy and Thinking(4).pdf:pdf},
issn = {17515823},
journal = {International Statistical Review},
number = {2},
pages = {189--194},
title = {{Discussion: Acquiring Statistical Literacy and Thinking}},
volume = {84},
year = {2016}
}
@techreport{Neal1997a,
abstract = {One way to sample from a distribution is to sample uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal `slice' defined by the current vertical position. Variations on such `slice sampling' methods can easily be implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling, and may be more efficient than easily-constructed versions of the Metropolis algorithm. Slice sampling is therefore attractive in routine Markov chain Monte Carlo applications, and for use by software that automatically generates a Markov chain sampler from a model specification. One can also easily devise overrelaxed versions of slice sampling, which sometimes greatly improve sampling efficiency by suppressing random walk behaviour. Random walks can also be avoided in some slice sampling schemes that simultaneously update all variables.},
address = {Toronto},
author = {Neal, Radford M.},
booktitle = {Technical Report No. 9722},
doi = {10.1.1.48.886},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Neal - 1997 - {\{}Markov chain Monte Carlo{\}} Methods Based on `Slicing' the Density Function.pdf:pdf},
institution = {Department of Statistics, University of Toronto},
isbn = {9781109363869},
pages = {1--27},
title = {{Markov chain Monte Carlo Methods Based on 'Slicing' the Density Function}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.48.886{\&}rep=rep1{\&}type=pdf},
year = {1997}
}
@inproceedings{Magnusson2019,
address = {Long Beach, California},
author = {Magnusson, Mans and Andersen, Michael Riis and Jonasson, Johan and Vehtari, Aki},
booktitle = {Proceedings of the 36th International Conference on Machine Learning},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Magnusson et al. - 2019 - Bayesian Leave-One-Out Cross-Validation for Large Data.pdf:pdf},
title = {{Bayesian Leave-One-Out Cross-Validation for Large Data}},
year = {2019}
}
@incollection{Nickel2002,
address = {Thoverton},
author = {Nickel, Gregor},
booktitle = {Between Chance and Choice. Interdisciplinary Perspectives on Determinism},
editor = {Atmanspacher, H. and Bishop, R.},
pages = {33--49},
publisher = {Imprint Academic},
title = {{Perspectives on scientific determinism}},
year = {2002}
}
@book{Salmon1966,
address = {Pittsburgh},
author = {Salmon, Wesley C.},
isbn = {0822951185},
publisher = {University of Pittsburgh Press},
title = {{The Foundations of Scientific Inference}},
url = {https://books.google.de/books/about/The{\_}Foundations{\_}of{\_}Scientific{\_}Inference.html?id=PnKwqcndEC4C{\&}redir{\_}esc=y},
year = {1966}
}
@book{meyer_unterrichtsentwicklung_2015,
address = {Berlin},
annote = {Literaturverzeichnis: Seite 193-204 "Alle Schulformen" - Cover "Mit Materialien auf CD-ROM" - Cover
OCLC: 904021573},
author = {Meyer, Hilbert},
edition = {1. Auflage},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Meyer - 2015 - Unterrichtsentwicklung.pdf:pdf},
isbn = {978-3-589-22473-9},
keywords = {Qualit{\"{a}}tsmanagement,Schulentwicklung,Unterrichtsbeurteilung,Unterrichtsmethode,Unterrichtsorganisation},
publisher = {Cornelsen},
title = {{Unterrichtsentwicklung}},
year = {2015}
}
@article{Berger1996b,
author = {Berger, Roger L and Hsu, Jason C and Berger, Roger L and Hsu, Jason C},
file = {:Users/riko/Downloads/2246021.pdf:pdf},
journal = {Statistical Science},
number = {4},
pages = {283--302},
title = {{Bioequivalence Trials, Intersection-Union Tests and Equivalence Confidence Sets}},
volume = {11},
year = {1996}
}
@article{Ragonis2005a,
author = {Ragonis, Noa and Ben-Ari, Mordechai},
doi = {10.1080/08993400500224310},
isbn = {0899-3408, 1744-5175},
journal = {Computer Science Education},
language = {en},
number = {3},
pages = {203--221},
title = {{A long-term investigation of the comprehension of OOP concepts by novices}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08993400500224310},
volume = {15},
year = {2005}
}
@misc{Stigler2013,
author = {Stigler, Stephen},
booktitle = {American Statistician},
doi = {10.1080/00031305.2012.747448},
issn = {00031305},
month = {feb},
number = {1},
pages = {6--7},
title = {{Comment: Bayesian inference: The Rodney Dangerfield of statistics?}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.2012.747448},
volume = {67},
year = {2013}
}
@article{Spiegelhalter2002,
abstract = {We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure pD for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general pD approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the 'hat' matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding pD to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.},
author = {Spiegelhalter, David J. and Best, Nicola G. and Carlin, Bradley P. and van der Linde, Angelika},
doi = {10.1111/1467-9868.00353},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Bayesian model comparison,Decision theory,Deviance information criterion,Effective number of parameters,Hierarchical models,Information theory,Leverage,Markov chain Monte Carlo methods,Model dimension},
month = {oct},
number = {4},
pages = {583--639},
title = {{Bayesian measures of model complexity and fit}},
url = {http://doi.wiley.com/10.1111/1467-9868.00353},
volume = {64},
year = {2002}
}
@article{Dunson2008,
abstract = {We consider Bayesian inference about collections of unknown distributions subject to a partial stochastic ordering. To address problems in testing of equalities between groups and estimation of group-specific distributions, we propose classes of restricted dependent Dirichlet process priors. These priors have full support in the space of stochastically ordered distributions, and can be used for collections of unknown mixture distributions to obtain a flexible class of mixture models. Theoretical properties are discussed, efficient methods are developed for posterior computation using Markov chain Monte Carlo simulation and the methods are illustrated using data from a study of DNA damage and repair. {\textcopyright} 2008 U.S. Government/Department of Health and Human Services.},
author = {Dunson, D. B. and Peddada, S. D.},
doi = {10.1093/biomet/asn043},
issn = {0006-3444},
journal = {Biometrika},
keywords = {Dependent Dirichlet process,Hypothesis testing,Mixture model,Nonparametric Bayes inference,Order restriction},
month = {nov},
number = {4},
pages = {859--874},
publisher = {Oxford Academic},
title = {{Bayesian nonparametric inference on stochastic ordering}},
url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/asn043},
volume = {95},
year = {2008}
}
@article{Schoenbrodt2017,
author = {Sch{\"{o}}nbrodt, Felix D. and Wagenmakers, Eric-Jan and Zehetleitner, Michael and Perugini, Marco},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Sch{\"{o}}nbrodt et al. - 2017 - Sequential Hypothesis Testing With Bayes Factors Efficiently Testing Mean Differences.pdf:pdf},
journal = {Psychological Methods.},
keywords = {about the,and hypotheses,bayes factor,e ciency,for example,for this endeavor,hypothesis testing,idence of competing theories,is to increase knowledge,optional stopping,scientists have to weigh,sequential designs,the ev-,the goal of science,world},
number = {2},
pages = {322--339},
title = {{Sequential Hypothesis Testing With Bayes Factors: Efficiently Testing Mean Differences}},
url = {doi: 10.1037/met0000061},
volume = {22},
year = {2017}
}
@article{Fisher1932a,
abstract = {{\textless}p{\textgreater}Logicians have long distinguished two modes of human reasoning, under the respective names of deductive and inductive reasoning. In deductive reasoning we attempt to argue from a hypothesis to its necessary consequences, which may be verifiable by observation; that is, to argue from the general to the particular. In inductive reasoning we attempt to argue from the particular, which is typically a body of observational material, to the general, which is typically a theory applicable to future experience. In statistical language we are attempting to argue from the sample to the population, from which it was drawn. Since recent statistical work has shown that this type of argument can be carried out with exactitude in a usefully large class of cases(2, 3), by means of conceptions somewhat different from those of the classical theory of probability, it may be useful briefly to restate the logical and mathematical distinctions which have to be drawn.{\textless}/p{\textgreater}},
author = {Fisher, R. A.},
doi = {10.1017/S0305004100010094},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1932 - Inverse probability and the use of Likelihood.pdf:pdf},
issn = {0305-0041},
journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
month = {jul},
number = {3},
pages = {257--261},
publisher = {Cambridge University Press},
title = {{Inverse probability and the use of Likelihood}},
url = {https://www.cambridge.org/core/product/identifier/S0305004100010094/type/journal{\_}article},
volume = {28},
year = {1932}
}
@article{Gronau2019b,
abstract = {Cross-validation (CV) is increasingly popular as a generic method to adjudicate between mathematical models of cognition and behavior. In order to measure model generalizability, CV quantifies out-of-sample predictive performance, and the CV preference goes to the model that predicted the out-of-sample data best. The advantages of CV include theoretic simplicity and practical feasibility. Despite its prominence, however, the limitations of CV are often underappreciated. Here, we demonstrate the limitations of a particular form of CV-Bayesian leave-one-out cross-validation or LOO-with three concrete examples. In each example, a data set of infinite size is perfectly in line with the predictions of a simple model (i.e., a general law or invariance). Nevertheless, LOO shows bounded and relatively modest support for the simple model. We conclude that CV is not a panacea for model selection.},
author = {Gronau, Quentin F. and Wagenmakers, Eric-Jan},
doi = {10.1007/s42113-018-0011-7},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gronau, Wagenmakers - 2019 - Limitations of Bayesian Leave-One-Out Cross-Validation for Model Selection.pdf:pdf},
issn = {2522-0861},
journal = {Computational Brain {\&} Behavior},
keywords = {Bounded support,Consistency,Evidence,Generalizability,Induction,Principle of parsimony},
month = {mar},
number = {1},
pages = {1--11},
publisher = {Springer Science and Business Media LLC},
title = {{Limitations of Bayesian Leave-One-Out Cross-Validation for Model Selection}},
url = {https://doi.org/10.1007/s42113-018-0011-7},
volume = {2},
year = {2019}
}
@article{Shannon1948,
author = {Shannon, C E},
doi = {10.1002/j.1538-7305.1948.tb01338.x},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Shannon - Unknown - A Mathematical Theory of Communication.pdf:pdf},
issn = {15387305},
journal = {Bell System Technical Journal},
number = {3},
pages = {379--423},
title = {{A Mathematical Theory of Communication}},
volume = {27},
year = {1948}
}
@article{hubwieser_how_2015,
abstract = {Aiming to collect various concepts, approaches, and strategies for improving computer science education in K-12 schools, we edited this second special issue of the ACM TOCE journal. Our intention was to collect a set of case studies from different countries that would describe all relevant aspects of specific implementations of Computer Science Education in K-12 schools. By this, we want to deliver well-founded arguments and rich material to the critical discussion about the state and the goals of K-12 computer science education, and also provide visions for the future of this research area. In this editorial, we explain our intention and report some details about the genesis of these special issues. Following, we give a short summary of the Darmstadt Model, which was suggested to serve as a structuring principle of the case studies. The next part of the editorial presents a short description of the five extended case studies from India, Korea, NRW/Germany, Finland, and USA that are selected to be included in this second issue. In order to give some perspectives for the future, we propose a set of open research questions of the field, partly derived from the Darmstadt Model, partly stimulated by a look on large-scale investigations like PISA.},
annote = {Germany, Israel, Norway},
author = {Hubwieser, Peter and Armoni, Michal and Giannakos, Michail N},
doi = {10.1145/2729983},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hubwieser, Armoni, Giannakos - 2015 - How to Implement Rigorous Computer Science Education in K-12 Schools Some Answers and Many Questio.pdf:pdf},
issn = {1946-6226},
journal = {Trans. Comput. Educ.},
keywords = {CS education,Darmstadt Model,K-12 education,Schools,curricula,research questions},
month = {apr},
number = {2},
pages = {5:1----5:12},
shorttitle = {How to {\{}Implement{\}} {\{}Rigorous{\}} {\{}Computer{\}} {\{}Science{\}}},
title = {{How to Implement Rigorous Computer Science Education in K-12 Schools? Some Answers and Many Questions}},
url = {http://doi.acm.org/10.1145/2729983},
volume = {15},
year = {2015}
}
@inproceedings{govender_are_2012,
address = {New York, NY, USA},
annote = {South Africa},
author = {Govender, Desmond Wesley and Govender, Irene},
booktitle = {Proceedings of the 17th ACM Annual Conference on Innovation and Technology in Computer Science Education},
doi = {10.1145/2325296.2325411},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Govender, Govender - 2012 - Are Students Learning Object Oriented Programming in an Object Oriented Programming Course Student Voices.pdf:pdf},
isbn = {978-1-4503-1246-2},
keywords = {object oriented approach,object oriented programming,teaching strategy},
pages = {395},
publisher = {ACM},
series = {{\{}ITiCSE{\}} '12},
shorttitle = {Are {\{}Students{\}} {\{}Learning{\}} {\{}Object{\}} {\{}Oriented{\}} {\{}Pro}},
title = {{Are Students Learning Object Oriented Programming in an Object Oriented Programming Course?: Student Voices}},
url = {http://doi.acm.org/10.1145/2325296.2325411},
year = {2012}
}
@book{Popper1959,
address = {London, New York},
author = {Popper, Karl},
booktitle = {The Logic of Scientific Discovery},
doi = {10.4324/9780203994627},
isbn = {0203994620},
publisher = {Routledge},
title = {{The Logic of Scientific Discovery}},
year = {1959}
}
@article{Cumming2001,
abstract = {Reform of statistical practice in the social and behavioral sciences requires wider use of confidence intervals (CIs), effect size measures, and meta-analysis. The authors discuss four reasons for promoting use of CIs: They (a) are readily interpretable, (b) are linked to familiar statistical significance tests, (c) can encourage meta-analytic thinking, and (d) give information about precision. The authors discuss calculation of CIs for a basic stan- dardized effect size measure, Cohen's $\delta$ (also known as Cohen's d), and contrast these with the familiar CIs for original score means. CIs for $\delta$ require use of noncentral t distri- butions, which the authors apply also to statistical power and simple meta-analysis of standardized effect sizes. They provide the ESCI graphical software, which runs under Microsoft Excel, to illustrate the discussion.Wider use of CIs for $\delta$ and other effect size measures should help promote highly desirable reform of statistical practice in the social sciences.},
author = {Cumming, Geoff and Finch, Sue},
doi = {10.1177/0013164401614002},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Cumming, Finch - 2001 - A Primer on the Understanding, Use, and Calculation of Confidence Intervals that are Based on Central and Noncen.pdf:pdf},
issn = {0013-1644},
journal = {Educational and Psychological Measurement},
number = {4},
pages = {532--574},
publisher = {SAGE Publications},
title = {{A Primer on the Understanding, Use, and Calculation of Confidence Intervals that are Based on Central and Noncentral Distributions}},
volume = {61},
year = {2001}
}
@article{Freidlin2003,
abstract = {Paired data arises in a wide variety of applications where often the underlying distribution of the paired differences is unknown. When the differences are normally distributed, the t-test is optimum. On the other hand, if the differences are not normal, the t-test can have substantially less power than the appropriate optimum test, which depends on the unknown distribution. In textbooks, when the normality of the differences is questionable, typically the non-parametric Wilcoxon signed rank test is suggested. An adaptive procedure that uses the Shapiro-Wilk test of normality to decide whether to use the t-test or the Wilcoxon signed rank test has been employed in several studies. Faced with data from heavy tails, the U.S. Environmental Protection Agency (EPA) introduced another approach: it applies both the sign and t-tests to the paired differences, the alternative hypothesis is accepted if either test is significant. This paper investigates the statistical properties of a currently used adaptive test, the EPA's method and suggests an alternative technique. The new procedure is easy to use and generally has higher empirical power, especially when the differences are heavy-tailed, than currently used methods.},
author = {Freidlin, B. and Miao, W. and Gastwirth, J. L.},
doi = {10.1002/bimj.200390056},
issn = {03233847},
journal = {Biometrical Journal},
keywords = {Adaptive tests,Paired data,Power robustness,Shapiro-Wilk test},
month = {oct},
number = {7},
pages = {887--900},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{On the Use of the Shapiro-Wilk Test in Two-Stage Adaptive Inference for Paired Data from Moderate to Very Heavy Tailed Distributions}},
volume = {45},
year = {2003}
}
@article{Wald1949,
abstract = {paper},
author = {Wald, Abraham},
doi = {10.1214/aoms/1177730030},
isbn = {0003486X},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {jun},
number = {2},
pages = {165--205},
pmid = {21567699},
publisher = {Institute of Mathematical Statistics},
title = {{Statistical Decision Functions}},
url = {http://projecteuclid.org/euclid.aoms/1177730030},
volume = {20},
year = {1949}
}
@misc{Baccaglini2010,
abstract = {A growing number of articles are emerging in the medical and statistics literature that describe epidemiologic and statistical flaws of research studies. Many examples of these deficiencies are encountered in the oral, craniofacial, and dental literature. However, only a handful of methodologic articles have been published in the oral literature warning investigators of potential errors that may arise early in the study and that can irreparably bias the final results. In this study, we briefly review some of the most common pitfalls that our team of epidemiologists and statisticians has identified during the review of submitted or published manuscripts and research grant applications. We use practical examples from the oral medicine and dental literature to illustrate potential shortcomings in the design and analysis of research studies, and how these deficiencies may affect the results and their interpretation. A good study design is essential, because errors in the analysis can be corrected if the design was sound, but flaws in study design can lead to data that are not salvageable. We recommend consultation with an epidemiologist or a statistician during the planning phase of a research study to optimize study efficiency, minimize potential sources of bias, and document the analytic plan.},
author = {Baccaglini, L and Shuster, J J and Cheng, J and Theriaque, D W and Schoenbach, V J and Tomar, S L and Poole, C},
booktitle = {Oral Diseases},
doi = {10.1111/j.1601-0825.2009.01634.x},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Baccaglini et al. - 2010 - Design and statistical analysis of oral medicine studies Common pitfalls.pdf:pdf},
isbn = {1354-523X},
issn = {1354523X},
keywords = {Bias,Epidemiology,Guideline,Methods,Public health,Statistics},
month = {apr},
number = {3},
pages = {233--241},
pmid = {19874532},
publisher = {NIH Public Access},
title = {{Design and statistical analysis of oral medicine studies: Common pitfalls}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19874532 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2879019},
volume = {16},
year = {2010}
}
@article{Kruschke2018b,
abstract = {This article explains the foundational concepts of Bayesian data analysis using virtually no mathematical notation. Bayesian ideas already match your intuitions from everyday reasoning and from traditional data analysis. Simple examples of Bayesian data analysis are presented that illustrate how the information delivered by a Bayesian analysis can be directly interpreted. Bayesian approaches to null-value assessment are discussed. The article clarifies misconceptions about Bayesian methods that newcomers might have acquired elsewhere. We discuss prior distributions and explain how they are not a liability but an important asset. We discuss the relation of Bayesian data analysis to Bayesian models of mind, and we briefly discuss what methodological problems Bayesian data analysis is not meant to solve. After you have read this article, you should have a clear sense of how Bayesian data analysis works and the sort of information it delivers, and why that information is so intuitive and useful for drawing conclusions from data.},
author = {Kruschke, John K. and Liddell, T.M.},
doi = {10.3758/s13423-017-1272-1},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kruschke, Liddell - 2018 - Bayesian data analysis for newcomers.pdf:pdf},
issn = {15315320},
journal = {Psychonomic Bulletin and Review},
keywords = {Bayes factor,Bayesian analysis,Bayesian model,Confidence interval,Highest density interval,Null hypothesis significance test,Region of practical equivalence,Replication crisis,p value},
number = {1},
pages = {155--177},
publisher = {Springer New York LLC},
title = {{Bayesian data analysis for newcomers}},
volume = {25},
year = {2018}
}
@article{Kelter2021StatisticsInBiosciences,
author = {Kelter, Riko},
journal = {Statistics in Biosciences},
title = {{A new Bayesian two-sample t-test and solution to the Behrens-Fisher problem based on Gaussian mixture distributions}},
volume = {(in press)},
year = {2021}
}
@article{Amari2009,
abstract = {A divergence measure between two probability distributions or positive arrays (positive measures) is a useful tool for solving optimization problems in optimization, signal processing, machine learning, and statistical inference. The Csisz{\'{a}}r f-divergence is a unique class of divergences having information monotonicity, from which the dual $\alpha$ geometrical structure with the Fisher metric is derived. The Bregman divergence is another class of divergences that gives a dually flat geometrical structure different from the $\alpha$-structure in general. Csisz{\'{a}}r gave an axiomatic characterization of divergences related to inference problems. The Kullback-Leibler divergence is proved to belong to both classes, and this is the only such one in the space of probability distributions. This paper proves that the $\alpha$-divergences constitute a unique class belonging to both classes when the space of positive measures or positive arrays is considered. They are the canonical divergences derived from the dually flat geometrical structure of the space of positive measures. {\textcopyright} 2009 IEEE.},
author = {Amari, Shun Ichi},
doi = {10.1109/TIT.2009.2030485},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Bregman divergence,Canonical divergence,Dually flat structure,Fisher information,Information geometry,Information monotonicity,f-divergence},
number = {11},
pages = {4925--4931},
publisher = {IEEE Press},
title = {$\alpha$-divergence is unique, belonging to both f-divergence and bregman divergence classes},
volume = {55},
year = {2009}
}
@book{Royall1997,
address = {London},
author = {Royall, Richard},
publisher = {Chapman and Hall},
title = {{Statistical Evidence: A likelihood paradigm for statistical evidence}},
year = {1997}
}
@article{Gelfand1990b,
abstract = {The use of the Gibbs sampler as a method for calculating Bayesian marginal posterior and predictive densities is reviewed and illustrated with a range of normal data models, including variance components, unordered and ordered means, hierarchical growth curves, and missing data in a crossover trial. In all cases the approach is straightforward to specify distributionally and to implement computationally, with output readily adapted for required inference summaries.},
author = {Gelfand, Alan E. and Hills, Susan E. and Racine-Poon, Amy and Smith, Adrian F.M.},
doi = {10.1080/01621459.1990.10474968},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Density estimation,Hierarchical models,Marginalization,Missing data,Nonlinear parameters,Order-restricted inference,Variance components},
number = {412},
pages = {972--985},
title = {{Illustration of Bayesian inference in normal data models using Gibbs sampling}},
volume = {85},
year = {1990}
}
@article{FDABayes2010,
address = {Rockville},
author = {{U.S. Food and Drug Administration Center for Devices and Radiological Health}},
institution = {U.S. Food and Drug Administration},
journal = {https://www.fda.gov/regulatory-information/search-fda-guidance-documents/guidance-use-bayesian-statistics-medical-device-clinical-trials (accessed 16/08/2021)},
title = {{Guidance for the Use of Bayesian Statistics in Medical Device Clinical Trials}},
year = {2010}
}
@article{Kruschke2012,
abstract = {The use of Bayesian methods for data analysis is creating a revolution in fields ranging from genetics to marketing. Yet, results of our literature review, including more than 10,000 articles published in 15 journals from January 2001 and December 2010, indicate that Bayesian approaches are essentially absent from the organizational sciences. Our article introduces organizational science researchers to Bayesian methods and describes why and how they should be used. We use multiple linear regression as the framework to offer a step-by-step demonstration, including the use of software, regarding how to implement Bayesian methods. We explain and illustrate how to determine the prior distribution, compute the posterior distribution, possibly accept the null value, and produce a write-up describing the entire Bayesian process, including graphs, results, and their interpretation. We also offer a summary of the advantages of using Bayesian analysis and examples of how specific published research based on frequentist analysis-based approaches failed to benefit from the advantages offered by a Bayesian approach and how using Bayesian analyses would have led to richer and, in some cases, different substantive conclusions. We hope that our article will serve as a catalyst for the adoption of Bayesian methods in organizational science research.},
author = {Kruschke, John K. and Aguinis, Herman and Joo, Harry},
doi = {10.1177/1094428112457829},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kruschke, Aguinis, Joo - 2012 - The Time Has Come Bayesian Methods for Data Analysis in the Organizational Sciences.pdf:pdf},
isbn = {1094-4281 1552-7425},
issn = {10944281},
journal = {Organizational Research Methods},
keywords = {Monte Carlo,bootstrapping),computer simulation procedures (e.g.,multilevel research,quantitative research},
number = {4},
pages = {722--752},
title = {{The Time Has Come: Bayesian Methods for Data Analysis in the Organizational Sciences}},
volume = {15},
year = {2012}
}
@article{Makowski2019,
abstract = {Turmoil has engulfed psychological science. Causes and consequences of the reproducibility crisis are in dispute. With the hope of addressing some of its aspects, Bayesian methods are gaining increasing attention in psychological science. Some of their advantages, as opposed to the frequentist framework, are the ability to describe parameters in probabilistic terms and explicitly incorporate prior knowledge about them into the model. These issues are crucial in particular regarding the current debate about statistical significance. Bayesian methods are not necessarily the only remedy against incorrect interpretations or wrong conclusions, but there is an increasing agreement that they are one of the keys to avoid such fallacies. Nevertheless, its flexible nature is its power and weakness, for there is no agreement about what indices of “significance” should be computed or reported. This lack of a consensual index or guidelines, such as the frequentist p-value, further contributes to the unnecessary opacity that many non-familiar readers perceive in Bayesian statistics. Thus, this study describes and compares several Bayesian indices, provide intuitive visual representation of their “behavior” in relationship with common sources of variance such as sample size, magnitude of effects and also frequentist significance. The results contribute to the development of an intuitive understanding of the values that researchers report, allowing to draw sensible recommendations for Bayesian statistics description, critical for the standardization of scientific reporting.},
author = {Makowski, Dominique and Ben-Shachar, Mattan S. and Chen, S. H. Annabel and L{\"{u}}decke, Daniel},
doi = {10.3389/fpsyg.2019.02767},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Makowski et al. - 2019 - Indices of Effect Existence and Significance in the Bayesian Framework.pdf:pdf},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {Bayes Factors,Bayesian,NHST,p-value,significance},
pages = {2767},
publisher = {Frontiers},
title = {{Indices of Effect Existence and Significance in the Bayesian Framework}},
volume = {10},
year = {2019}
}
@article{Fisher1929,
abstract = {Reproduced with permission of the Royal Society.},
author = {Fisher, R.A.},
doi = {10.1098/rspa.1929.0151},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1929 - Tests of Significance in Harmonic Analysis.pdf:pdf},
issn = {1364-5021},
journal = {Proceedings of the Royal Society of London, Series A},
number = {1929},
pages = {54--59},
title = {{Tests of Significance in Harmonic Analysis}},
url = {https://digital.library.adelaide.edu.au/dspace/handle/2440/15201},
volume = {125},
year = {1929}
}
@article{Locascio2019,
abstract = {AbstractThe author has previously proposed results blind manuscript evaluation (RBME) as a method of ameliorating often cited problems of statistical inference and scientific publication, notably publication bias, overuse/misuse of null hypothesis significance testing (NHST), and irreproducibility of reported scientific results. In RBME, manuscripts submitted to scientific journals are assessed for suitability for publication without regard to their reported results. Criteria for publication are based exclusively on the substantive importance of the research question addressed in the study, conveyed in the Introduction section of the manuscript, and the quality of the methodology, as reported in the Methods section. Practically, this policy is implemented by a two stage process whereby the editor initially distributes only the Introduction and Methods sections of a submitted manuscript to reviewers and a provisional decision regarding acceptance is made, followed by a second stage in which the complete ma...},
author = {Locascio, Joseph J.},
doi = {10.1080/00031305.2018.1505658},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Locascio - 2019 - The Impact of Results Blind Science Publishing on Statistical Consultation and Collaboration.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Publishing,Publication bias,Results-Blind,Signific,publication bias,publishing,results-blind,significance},
number = {sup1},
pages = {346--351},
publisher = {Taylor {\&} Francis},
title = {{The Impact of Results Blind Science Publishing on Statistical Consultation and Collaboration}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1505658},
volume = {73},
year = {2019}
}
@article{xinogalos_introduction_2006,
abstract = {The objects-first strategy to teaching programming has prevailed over the imperative-first and functional-first strategies during the last decade. However, the objects-first strategy has created added difficulties to both the teaching and learning of programming. In an attempt to confront these difficulties and support the objects-first strategy we developed a novel programming environment, objectKarel, which uses the language Karel++. The design of objectKarel was based on the results of the extended research that has been carried out about novice programmers. What differentiates it from analogous environments is the fact that it combines features that have been used solely in them: incorporated e-lessons and hands-on activities; an easy to use structure editor for developing/editing programs; program animation; explanatory visualization; highly informative and friendly error messages; recordability. In this paper, we present the didactic rationale that dictated the design of objectKarel and the features of the environment, including the e-lessons. In addition, we present the results from the use of objectKarel in the classroom and the results of the students' assessment of the environment.},
author = {Xinogalos, Stelios and Satratzemi, Maya and Dagdilelis, Vassilios},
doi = {10.1016/j.compedu.2004.09.005},
issn = {0360-1315},
journal = {Comput. Educ.},
keywords = {pedagogical issues,programming and programming languages,teaching/learning strategies},
month = {sep},
number = {2},
pages = {148--171},
shorttitle = {An {\{}Introduction{\}} to {\{}Object{\}}-oriented {\{}Programmin}},
title = {{An Introduction to Object-oriented Programming with a Didactic Microworld: ObjectKarel}},
url = {http://dx.doi.org/10.1016/j.compedu.2004.09.005},
volume = {47},
year = {2006}
}
@article{Fricker2019,
abstract = {ABSTRACTIn this article, we assess the 31 articles published in Basic and Applied Social Psychology (BASP) in 2016, which is one full year after the BASP editors banned the use of inferential statistics. We discuss how the authors collected their data, how they reported and summarized their data, and how they used their data to reach conclusions. We found multiple instances of authors overstating conclusions beyond what the data would support if statistical significance had been considered. Readers would be largely unable to recognize this because the necessary information to do so was not readily available.},
author = {Fricker, Ronald D. and Burke, Katherine and Han, Xiaoyan and Woodall, William H.},
doi = {10.1080/00031305.2018.1537892},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fricker et al. - 2019 - Assessing the Statistical Analyses Used in iBasic and Applied Social Psychologyi After Their ipi -Value Ban.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {effect size,inference ban,nhst,psychology,statistical},
number = {sup1},
pages = {374--384},
title = {{Assessing the Statistical Analyses Used in {\textless}i{\textgreater}Basic and Applied Social Psychology{\textless}/i{\textgreater} After Their {\textless}i{\textgreater}p{\textless}/i{\textgreater} -Value Ban}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1537892},
volume = {73},
year = {2019}
}
@article{Hastings1970,
abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
archivePrefix = {arXiv},
arxivId = {5744249209},
author = {Hastings, W. K.},
doi = {10.1093/biomet/57.1.97},
eprint = {5744249209},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hastings - 1970 - Monte carlo sampling methods using Markov chains and their applications.pdf:pdf},
isbn = {00063444},
issn = {00063444},
journal = {Biometrika},
month = {apr},
number = {1},
pages = {97--109},
pmid = {18855289},
title = {{Monte carlo sampling methods using Markov chains and their applications}},
url = {http://biomet.oxfordjournals.org/cgi/doi/10.1093/biomet/57.1.97{\%}0Ahttps://academic.oup.com/biomet/article-abstract/57/1/97/2721936},
volume = {57},
year = {1970}
}
@article{Stigler1991,
author = {Stigler, S. M.},
doi = {10.1214/ss/1177011943},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Stigler - 1991 - Stochastic Simulation in the Nineteenth Century.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and extend access to,is collaborating with jstor,preserve,statistical science,titute of mathematical statistics,to digitize},
number = {1},
pages = {89--97},
title = {{Stochastic Simulation in the Nineteenth Century}},
volume = {6},
year = {1991}
}
@article{Neal2003,
abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can sample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal "slice" defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such "slice sampling" methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by "overrelaxation," and for multivariate slice sampling by "reflection" from the edges of the slice. CR - Copyright {\&}{\#}169; 2003 Institute of Mathematical Statistics},
archivePrefix = {arXiv},
arxivId = {1003.3201v1},
author = {Neal, Radford M.},
doi = {10.1214/aos/1056562461},
eprint = {1003.3201v1},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Neal - 2003 - Slice sampling.pdf:pdf},
isbn = {00905364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Gibbs sampling,Markov chain Monte Carlo,Metropolis algorithm,adaptive methods,auxiliary variables,dynamical methods,overrelaxation},
month = {jun},
number = {3},
pages = {743--748},
pmid = {22393278},
publisher = {Institute of Mathematical Statistics},
title = {{Slice sampling}},
url = {http://projecteuclid.org/euclid.aos/1056562461},
volume = {31},
year = {2003}
}
@incollection{Bernado1999,
address = {Valencia},
author = {Bernado, J.M.},
booktitle = {Bayesian Statistics (Vol. 6)},
editor = {Bernado, J.M. and Berger, J.O. and Dawid, A.P. and Smith, A.F.M.},
pages = {101--130 (with discussion)},
publisher = {Oxford University Press},
title = {{Nested hypothesis testing: the Bayesian reference criterion}},
year = {1999}
}
@book{hazzan_guide_2014,
address = {London},
annote = {DOI: 10.1007/978-1-4471-6630-6},
author = {Hazzan, Orit and Lapidot, Tami and Ragonis, Noa},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hazzan, Lapidot, Ragonis - 2014 - Guide to Teaching Computer Science.pdf:pdf},
isbn = {978-1-4471-6629-0 978-1-4471-6630-6},
publisher = {Springer London},
title = {{Guide to Teaching Computer Science}},
url = {http://link.springer.com/10.1007/978-1-4471-6630-6},
year = {2014}
}
@misc{VanderWeele2016,
abstract = {This article provides an overview of recent developments in mediation analysis, that is, analyses used to assess the relative magnitude of different pathways and mechanisms by which an exposure may affect an outcome. Traditional approaches to mediation in the biomedical and social sciences are described. Attention is given to the confounding assumptions required for a causal interpretation of direct and indirect effect estimates. Methods from the causal inference literature to conduct mediation in the presence of exposure-mediator interactions, binary outcomes, binary mediators, and case-control study designs are presented. Sensitivity analysis techniques for unmeasured confounding and measurement error are introduced. Discussion is given to extensions to time-to-event outcomes and multiple mediators. Further flexible modeling strategies arising from the precise counterfactual definitions of direct and indirect effects are also described. The focus throughout is on methodology that is easily implementable in practice across a broad range of potential applications.},
author = {VanderWeele, Tyler J.},
booktitle = {Annual Review of Public Health},
doi = {10.1146/annurev-publhealth-032315-021402},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/VanderWeele - 2016 - Mediation Analysis A Practitioner's Guide.pdf:pdf},
issn = {15452093},
keywords = {Direct effects,Indirect effects,Mechanism,Pathway analysis},
month = {mar},
pages = {17--32},
pmid = {26653405},
publisher = {Annual Reviews Inc.},
title = {{Mediation Analysis: A Practitioner's Guide}},
url = {https://pubmed.ncbi.nlm.nih.gov/26653405/},
volume = {37},
year = {2016}
}
@book{Lakatos1970,
abstract = {Two books have been particularly influential in contemporary philosophy of science: Karl R. Popper's Logic of Scientific Discovery, and Thomas S. Kuhn's Structure of Scientific Revolutions. Both agree upon the importance of revolutions in science, but differ about the role of criticism in science's revolutionary growth. This volume arose out of a symposium on Kuhn's work, with Popper in the chair, at an international colloquium held in London in 1965. The book begins with Kuhn's statement of his position followed by seven essays offering criticism and analysis, and finally by Kuhn's reply. The book will interest senior undergraduates and graduate students of the philosophy and history of science, as well as professional philosophers, philosophically inclined scientists, and some psychologists and sociologists.},
address = {Cambridge},
author = {Lakatos, Imre and Musgrave, Alan},
booktitle = {Criticism and the Growth of Knowledge: Proceedings of the International Colloquium in the Philosophy of Science, London, 1965 Volume 4},
doi = {10.1017/CBO9781139171434},
editor = {Lakatos, Imre and Musgrave, Alan},
isbn = {9781139171434},
month = {sep},
pages = {1--282},
publisher = {Cambridge University Press},
title = {{Criticism and the growth of knowledge}},
url = {https://www.cambridge.org/core/product/identifier/9781139171434/type/book},
year = {1970}
}
@inproceedings{fienberg_RAFisherAnAppreciation,
address = {Hemsbach},
author = {Fienberg, S.E. and Hinkley, D.V.},
booktitle = {Lecture Notes in Statistics},
editor = {Berger, S. and Fienberg, S. and Gani, J and Krickeberg, K. and Olkin, I. and Singer, B.},
publisher = {Springer-Verlag},
title = {{R.A. Fisher - An Appreciation}},
year = {1990}
}
@article{Maurer2019,
abstract = {Longstanding concerns with the role and interpretation of p-values in statistical practice prompted the American Statistical Association (ASA) to make a statement on p-values. The ASA statement spurred a flurry of responses and discussions by statisticians, with many wondering about the steps necessary to expand the adoption of these principles. Introductory statistics classrooms are key locations to introduce and emphasize the nuance related to p-values; in part because they engrain appropriate analysis choices at the earliest stages of statistics education, and also because they reach the broadest group of students. We propose a framework for statistics departments to conduct a content audit for p-value principles in their introductory curriculum. We then discuss the process and results from applying this course audit framework within our own statistics department. We also recommend meeting with client departments as a complement to the course audit. Discussions about analyses and practices common to particular fields can help to evaluate if our service courses are meeting the needs of client departments and to identify what is needed in our introductory courses to combat the misunderstanding and future misuse of p-values. ARTICLE HISTORY},
author = {Maurer, Karsten and Hudiburgh, Lynette and Werwinski, Lisa and Bailer, John},
doi = {10.1080/00031305.2018.1537890},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Maurer et al. - 2019 - Content Audit for ipi -value Principles in Introductory Statistics.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {audit,curriculum,education},
number = {sup1},
pages = {385--391},
title = {{Content Audit for {\textless}i{\textgreater}p{\textless}/i{\textgreater} -value Principles in Introductory Statistics}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1537890},
volume = {73},
year = {2019}
}
@book{Carlin2009,
address = {Boca Raton},
author = {Carlin, B.P. and Louis, T.A.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Carlin, Louis - 2009 - Bayesian Methods for Data Analysis.pdf:pdf},
month = {sep},
publisher = {Chapman {\&} Hall, CRC Press},
title = {{Bayesian Methods for Data Analysis}},
year = {2009}
}
@article{John2012,
abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
doi = {10.1177/0956797611430953},
eprint = {arXiv:1011.1669v3},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/John, Loewenstein, Prelec - 2012 - Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling.pdf:pdf},
isbn = {1011770956},
issn = {14679280},
journal = {Psychological Science},
keywords = {disclosure,judgment,methodology,professional standards},
number = {5},
pages = {524--532},
pmid = {22508865},
title = {{Measuring the Prevalence of Questionable Research Practices With Incentives for Truth Telling}},
volume = {23},
year = {2012}
}
@article{Gordon2010,
author = {Gordon, Sue and Nicholas, Jackie},
journal = {International Journal of Innovation in Science and Mathematics Education (formerly CAL-laborate International)},
number = {1},
shorttitle = {Teaching with examples and statistical literacy},
title = {{Teaching with examples and statistical literacy: views from teachers in statistics service courses}},
volume = {18},
year = {2010}
}
@article{Knapp2001,
abstract = {Thompson (1999a and elsewhere) has taken strong positions on a variety of methodological issues. In this article, the authors critique some of those positions and provide alternative views for each. {\textcopyright} 2001 Taylor {\&} Francis Group, LLC.},
author = {Knapp, Thomas R. and Sawilowsky, Shlomo S.},
doi = {10.1080/00220970109599498},
file = {:Users/riko/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/dissertation/papers/KnappSawilowsky2001.pdf:pdf},
issn = {19400683},
journal = {Journal of Experimental Education},
keywords = {Context specificity,Editorial policy,Effect sizes,Function weights,Reliability,Statistical significance,Stepwise},
number = {1},
pages = {65--79},
publisher = { Taylor {\&} Francis Group },
title = {{Constructive criticisms of methodological and editorial practices}},
url = {https://www.tandfonline.com/doi/abs/10.1080/00220970109599498},
volume = {70},
year = {2001}
}
@phdthesis{sabbag_examining_2016,
author = {Sabbag, Anelise Guimaraes},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Sabbag - 2016 - Examining the relationship between statistical literacy and statistical reasoning.pdf:pdf},
school = {University of Minnesota},
title = {{Examining the relationship between statistical literacy and statistical reasoning}},
year = {2016}
}
@article{Hoenig2001a,
abstract = {It is well known that statistical power calculations can be valuable in planning an experiment. There is also a large literature advocating that power calculations be made whenever one performs a statistical test of a hypothesis and one obtains a statistically nonsignificant result. Advocates of such post-experiment power calculations claim the calculations should be used to aid in the interpretation of the experimental results. This approach, which appears in various forms, is fundamentally flawed. We document that the problem is extensive and present arguments to demonstrate the flaw in the logic.},
author = {Hoenig, John M. and Heisey, Dennis M.},
doi = {10.1198/000313001300339897},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hoenig, Heisey - 2001 - The abuse of power The pervasive fallacy of power calculations for data analysis.pdf:pdf},
issn = {00031305},
journal = {American Statistician},
keywords = {Bioequivalence testing,Burden of proof,Observed power,Retrospective power analysis,Statistical power,Type II error},
month = {feb},
number = {1},
pages = {19--24},
publisher = {Taylor {\&} Francis},
title = {{The abuse of power: The pervasive fallacy of power calculations for data analysis}},
url = {https://www.tandfonline.com/doi/abs/10.1198/000313001300339897},
volume = {55},
year = {2001}
}
@article{Sellke2001a,
abstract = {P values are the most commonly used tool to measure evidence against a hypothesis or hypothesized model. Unfortunately, they are often incorrectly viewed as an error probability for rejection of the hypothesis or, even worse, as the posterior probability that the hypothesis is true. The fact that these interpretations can be completely misleading when testing precise hypotheses is first reviewed, through consideration of two revealing simulations. Then two calibrations of a p value are developed, the first being interpretable as odds and the second as either a (conditional) frequentist error probability or as the posterior probability of the hypothesis.},
author = {Sellke, Thomas and Bayarri, M. J. and Berger, James O.},
doi = {10.1198/000313001300339950},
issn = {00031305},
journal = {American Statistician},
keywords = {Bayes factors,Bayesian robustness,Conditional frequentist error probabilities,Odds},
month = {feb},
number = {1},
pages = {62--71},
publisher = {Taylor {\&} Francis},
title = {{Calibration of p values for testing precise null hypotheses}},
url = {https://www.tandfonline.com/doi/abs/10.1198/000313001300339950},
volume = {55},
year = {2001}
}
@book{Kolmogorov1950,
address = {New York},
author = {Kolmogorov, A.N.},
publisher = {Chelsea Pub. Co.},
title = {{Foundations of the Theory of Probability}},
year = {1950}
}
@inproceedings{astrachan_resolved:_2005,
abstract = {The participants will use a debate format with a provocative thesis to explore the pedagogical approach known as "objects early" or "objects first." By arguing in the affirmative, Elliot Koffman and Stuart Reges will point out concerns that have been raised about the approach. By arguing in the negative, Kim Bruce and Michael K{\"{o}}lling will describe schools that are succeeding with the approach and ways to address significant concerns. Owen Astrachan as moderator will ensure that the debate remains civil and will provide some humorous and possibly even insightful commentary on the evidence presented by both sides.},
address = {New York, NY, USA},
annote = {UK {\&} USA},
author = {Astrachan, Owen and Bruce, Kim and Koffman, Elliot and K{\"{o}}lling, Michael and Reges, Stuart},
booktitle = {Proceedings of the 36th SIGCSE Technical Symposium on Computer Science Education},
doi = {10.1145/1047344.1047359},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Astrachan et al. - 2005 - Resolved Objects Early Has Failed.pdf:pdf},
isbn = {978-1-58113-997-6},
keywords = {CS1,object oriented programming,objects first},
pages = {451--452},
publisher = {ACM},
series = {{\{}SIGCSE{\}} '05},
shorttitle = {Resolved},
title = {{Resolved: Objects Early Has Failed}},
url = {http://doi.acm.org/10.1145/1047344.1047359},
year = {2005}
}
@book{Lindquist1953,
address = {Boston},
author = {Lindquist, E.F.},
publisher = {Houghton Mifflin},
title = {{Design and analysis of experiments in psychology and education}},
year = {1953}
}
@article{Chib1995,
author = {Chib, Siddhartha and Greenberg, Edward},
doi = {10.1080/00031305.1995.10476177},
issn = {0003-1305},
journal = {The American Statistician},
month = {nov},
number = {4},
pages = {327--335},
title = {{Understanding the Metropolis-Hastings Algorithm}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.1995.10476177},
volume = {49},
year = {1995}
}
@inproceedings{sanders_student_2008,
abstract = {In this paper, we present the results of an experiment in which we sought to elicit students' understanding of object-oriented (OO) concepts using concept maps. Our analysis confirmed earlier research indicating that students do not have a firm grasp on the distinction between "class" and "instance." Unlike earlier research, we found that our students generally connect classes with both data and behavior. Students rarely included any mention of the hardware/software context of programs, their users, or their real-world domains. Students do mention inheritance, but not encapsulation or abstraction. And the picture they draw of OO is a static one: we found nothing that could be construed as referring to interaction among objects in a program. We then discuss the implications for teaching introductory OO programming.},
address = {New York, NY, USA},
author = {Sanders, Kate and Boustedt, Jonas and Eckerdal, Anna and McCartney, Robert and Mostr{\"{o}}m, Jan Erik and Thomas, Lynda and Zander, Carol},
booktitle = {Proceedings of the 39th SIGCSE Technical Symposium on Computer Science Education},
doi = {10.1145/1352135.1352251},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Sanders et al. - 2008 - Student Understanding of Object-oriented Programming As Expressed in Concept Maps.pdf:pdf},
isbn = {978-1-59593-799-5},
keywords = {CS1,concept maps,empirical research,object-oriented},
pages = {332--336},
publisher = {ACM},
series = {{\{}SIGCSE{\}} '08},
title = {{Student Understanding of Object-oriented Programming As Expressed in Concept Maps}},
url = {http://doi.acm.org/10.1145/1352135.1352251},
year = {2008}
}
@article{Fisher1935,
abstract = {I have called my paper "The logic of inductive inference." It might just as well have been called "On making sense of figures." For everyone who does habitually attempt the difficult task of making sense of figures is, in fact, essaying a logical process of the kind we call inductive, in that he is attempting to draw inferences from the particular to the general; or, as we more usually say in statistics, from the sample to the population. Such inferences we recognize to be uncertain inferences, but it does not follow from this that they are not mathematically rigorous inferences. In the theory of probability we are habituated to statements which may be entirely rigorous, involving the concept of probability, which, if translated into verifiable observations, have the character of uncertain statements.},
author = {Fisher, Ronald Aylmer},
doi = {10.2307/2342435},
isbn = {0952-8385},
issn = {09528385},
journal = {Journal of the Royal Statistical Society},
number = {1},
pages = {39},
title = {{The Logic of Inductive Inference}},
url = {https://www.jstor.org/stable/10.2307/2342435?origin=crossref http://www.jstor.org/stable/10.2307/2342435?origin=crossref},
volume = {98},
year = {1935}
}
@article{Kordsmeyer2017,
author = {Kordsmeyer, Tobias and Penke, Lars},
journal = {Evolution and Human Behavior},
pages = {704--713},
title = {{The association of three indicators of developmental instability with mating success in humans}},
volume = {38},
year = {2017}
}
@article{Kuhnast2008,
abstract = {BACKGROUND Although non-normal data are widespread in biomedical research, parametric tests unnecessarily predominate in statistical analyses. METHODS We surveyed five biomedical journals and - for all studies which contain at least the unpaired t-test or the non-parametric Wilcoxon-Mann-Whitney test - investigated the relationship between the choice of a statistical test and other variables such as type of journal, sample size, randomization, sponsoring etc. RESULTS The non-parametric Wilcoxon-Mann-Whitney was used in 30{\%} of the studies. In a multivariable logistic regression the type of journal, the test object, the scale of measurement and the statistical software were significant. The non-parametric test was more common in case of non-continuous data, in high-impact journals, in studies in humans, and when the statistical software is specified, in particular when SPSS was used.},
author = {K{\"{u}}hnast, Corinna and Neuh{\"{a}}user, Markus},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/K{\"{u}}hnast, Neuh{\"{a}}user - 2008 - A note on the use of the non-parametric Wilcoxon-Mann-Whitney test in the analysis of medical studies.pdf:pdf},
issn = {1612-3174},
journal = {German medical science : GMS e-journal},
month = {apr},
pages = {Doc02},
pmid = {19675730},
publisher = {German Medical Science},
title = {{A note on the use of the non-parametric Wilcoxon-Mann-Whitney test in the analysis of medical studies.}},
volume = {6},
year = {2008}
}
@book{Berger1988a,
abstract = {The likelihood principle (LP) is a normative principle for evaluating statistical inference procedures. The LP can be proved from arguably self-evident premises; indeed, it can be proved to be logically equivalent to these premises. This chapter attempts to prove a precise version of the LP, with a number of caveats; and briefly mentions some alternative versions. The importance of the likelihood principle is that it discusses if the comparison is not relevant. LP does rule out many specific inferences. It allows categorizing methods of statistical inference in a very natural and powerful way: a way, which is more abstract and more general than the usual ways of classifying statistical theories. The LP also captures some of the most attractive features of Bayesianism, while leaving open the question of whether a subjective prior should be. Since it provides a lot of common ground between factions of Bayesians, the LP is a good, irenic starting point for agreement between factions of philosophers of statistics. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
address = {Hayward, California},
author = {Berger, J.O. and Wolpert, Robert L.},
booktitle = {Lecture Notes - Monograph Series},
editor = {Gupta, Shanti S.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Berger, Wolpert - 1988 - The Likelihood Principle.pdf:pdf},
isbn = {0940600137},
pages = {vii--208},
publisher = {Institute of Mathematical Statistics},
title = {{The Likelihood Principle}},
year = {1988}
}
@article{asaStatement2016PressRelease,
abstract = {The American Statistical Association (ASA) has released a " Statement on Statistical Significance and P-Values " with six principles underlying the proper use and interpretation of the p-value [http://amstat.tandfonline.com/doi/abs/10.1080/00031305.2016.1154108{\#}.Vt2XIOaE2MN]. The ASA releases this guidance on p-values to improve the conduct and interpretation of quantitative science and inform the growing emphasis on reproducibility of science research. The statement also notes that the increased quantification of scientific research and a proliferation of large, complex data sets has expanded the scope for statistics and the importance of appropriately chosen techniques, properly conducted analyses, and correct interpretation. Good statistical practice is an essential component of good scientific practice, the statement observes, and such practice " emphasizes principles of good study design and conduct, a variety of numerical and graphical summaries of data, understanding of the phenomenon under study, interpretation of results in context, complete reporting and proper logical and quantitative understanding of what data summaries mean. " " The p-value was never intended to be a substitute for scientific reasoning, " said Ron Wasserstein, the ASA's executive director. " Well-reasoned statistical arguments contain much more than the value of a single number and whether that number exceeds an arbitrary threshold. The ASA statement is intended to steer research into a 'post p{\textless}0.05 era.' "},
author = {{American Statistical Association}},
doi = {10.1080/00031305.2016.1154108.Vt2XIOaE2MN},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/American Statistical Association - 2016 - American Statistical Association Press Release for the Statement on Statistical Significance a.pdf:pdf},
title = {{American Statistical Association Press Release for the Statement on Statistical Significance and P-Values}},
url = {bit.ly/2mw2mXF},
year = {2016}
}
@article{Wasserstein2019,
author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
doi = {10.1080/00031305.2019.1583913},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Wasserstein, Schirm, Lazar - 2019 - Moving to a World Beyond p0.05.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
number = {sup1},
pages = {1--19},
title = {{Moving to a World Beyond "p{\textless}0.05"}},
volume = {73},
year = {2019}
}
@book{fincher_computer_2004,
abstract = {This volume provides an overview of how to approach computer science education research from a pragmatic perspective.},
address = {London; New York},
annote = {OCLC: 61730895},
author = {Fincher, Sally and Petre, Marian},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fincher, Petre - 2004 - Computer science education research.pdf:pdf},
isbn = {978-0-203-01754-8 978-90-265-1969-7},
publisher = {RoutledgeFalmer},
title = {{Computer science education research}},
url = {http://public.eblib.com/choice/publicfullrecord.aspx?p=243141},
year = {2004}
}
@techreport{Birnbaum1964,
address = {New York},
author = {Birnbaum, Allan},
institution = {Courant Institute of Mathematical Science},
pages = {Tech. Rep. IMM--NYU 332},
title = {{The anomalous concept of statistical evidence}},
year = {1964}
}
@book{LeCam1990,
address = {New York, NY},
author = {{Le Cam}, Lucien and {Lo Yang}, Grace},
doi = {10.1007/978-1-4684-0377-0},
isbn = {978-1-4684-0379-4},
publisher = {Springer US},
series = {Springer Series in Statistics},
title = {{Asymptotics in Statistics}},
url = {http://link.springer.com/10.1007/978-1-4684-0377-0},
year = {1990}
}
@article{Banks1996a,
abstract = {Irving John Good was born in London on December 9, 1916. He attended the Haberdashers' “secondary” School, distinguishing himself as a mathematical prodigy, and then entered Jesus College at Cambridge University in 1935. He studied under G. H. Hardy and A. S. Besicovitch, obtaining his Ph.D. in 1941, and was the Cambridgeshire chess champion in 1939. Then he was called into World War II service as a cryptanalyst at Bletchley Park, working partly as the main statistician in teams led by Alan Turing and, later, by the British chess champion C. H. O'D. Alexander and by M. H. A. Newman. The work employed early electromagnetic and electronic computers and applied Bayesian statistics relevant to reading the two main secret ciphers used by the German Army and Navy, providing crucial intelligence to the Allies. After the war, Good taught briey at Manchester University and made a few suggestions for the electronic computer project. He was then drawn back into classified work for the British government. During that time he obtained an Sc.D. from Cambridge and a D.Sc. from Oxford. In 1967 he came to the United States, becoming a University Distinguished Professor at Virginia Polytechnic Institute. Officially he retired in 1994, but in practice he can be found at work late in the day when the snow isn't deep. {\textcopyright} 1996 Applied Probability Trust.},
author = {Banks, David L.},
doi = {10.1214/ss/1032209661},
issn = {0883-4237},
journal = {Statistical Science},
number = {1},
pages = {1--19},
publisher = {Institute of Mathematical Statistics},
title = {{A conversation with I. J. Good}},
url = {https://projecteuclid.org/journals/statistical-science/volume-11/issue-1/A-conversation-with-I-J-Good/10.1214/ss/1032209661.full},
volume = {11},
year = {1996}
}
@book{Berger1985,
abstract = {Second edition. In this new edition the author has added substantial material on Bayesian analysis, including lengthy new sections on such important topics as empirical and hierarchical Bayes analysis, Bayesian calculation, Bayesian communication, and group decision making. With these changes, the book can be used as a self-contained introduction to Bayesian analysis. In addition, much of the decision-theoretic portion of the text was updated, including new sections covering such modern topics as minimax multivariate (Stein) estimation. 1 Basic Concepts -- 2 Utility and Loss -- 3 Prior Information and Subjective Probability -- 4 Bayesian Analysis -- 5 Minimax Analysis -- 6 Invariance -- 7 Preposterior and Sequential Analysis -- 8 Complete and Essentially Complete Classes -- Appendix 1 Common Statistical Densities -- I Continuous -- II Discrete -- Appendix 2 Supplement to Chapter 4 -- II Development of (4.121) and (4.122) -- III Verification of Formula (4.123) -- Appendix 3 Technical Arguments from Chapter 7 -- I Verification of Formula (7.8) -- II Verification of Formula (7.10) -- Notation and Abbreviations -- Author Index.},
address = {New York},
author = {Berger, J.O.},
isbn = {9781441930743},
pages = {618},
publisher = {Springer},
title = {{Statistical Decision Theory and Bayesian Analysis}},
year = {1985}
}
@article{wasserstein2016,
abstract = {Additional reading: http://www.nature.com/news/statisticians-issue-warning-over-misuse-of-p-values-1.19503},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
doi = {10.1080/00031305.2016.1154108},
eprint = {1011.1669},
isbn = {0003-1305 1537-2731},
issn = {0003-1305},
journal = {The American Statistician},
number = {2},
pages = {129--133},
pmid = {25246403},
publisher = {Taylor {\&} Francis},
title = {{The ASA's Statement on p-Values: Context, Process, and Purpose}},
volume = {70},
year = {2016}
}
@article{Escobar1995,
abstract = {We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings density for estimation and are exemplified by special cases where data are modeled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior, and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established a general for class of normal mixture model},
author = {Escobar, Michael D. and West, Mike},
doi = {10.1080/01621459.1995.10476550},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Kernel estimation,Mixtures of Dirichlet processes,Multimodality,Normal mixtures,Posterior sampling,Smoothing parameter estimation},
number = {430},
pages = {577--588},
title = {{Bayesian density estimation and inference using mixtures}},
volume = {90},
year = {1995}
}
@book{Humphrey2016,
address = {Oxford},
author = {Humphrey, Paul},
publisher = {Oxford University Press},
title = {{Oxford Handbook of the Philosophy of Science}},
year = {2016}
}
@article{Lockhart2014,
author = {Lockhart, Richard and Taylor, Jonathan and Tibshirani, Ryan J. and Tibshirani, Robert},
doi = {10.1214/13-AOS1175},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Lockhart et al. - 2014 - A significance test for the lasso.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {{\$}p{\$}-value,Lasso,least angle regression,significance test},
month = {apr},
number = {2},
pages = {413--468},
publisher = {Institute of Mathematical Statistics},
title = {{A significance test for the lasso}},
url = {http://projecteuclid.org/euclid.aos/1400592161},
volume = {42},
year = {2014}
}
@article{Rao1992,
abstract = {Before the beginning of this century, statistics meant observed data$\backslash$nand descriptive summary figures, such as means, variances, indices,$\backslash$netc., computed from data. With the introduction of the $\chi${\^{}}2 test$\backslash$nfor goodness of fit (specification) by Karl Pearson (1900) and the$\backslash$nt test by Gosset (Student, 1908) for drawing inference on the mean$\backslash$nof a normal population, statistics started acquiring new meaning$\backslash$nas a method of processing data to determine the amount of uncertainty$\backslash$nin various generalizations we may make from observed data (sample)$\backslash$nto the source of the data (population). The major steps that led$\backslash$nto the establishment and recognition of statistics as a separate$\backslash$nscientific discipline and an inevitable tool in improving natural$\backslash$nknowledge were made by R. A. Fisher during the decade 1915-1925.$\backslash$nMost of the concepts and methods introduced by Fisher are fundamental$\backslash$nand continue to provide the framework for the discussion of statistical$\backslash$ntheory. Fisher's work is monumental, both in richness and variety$\backslash$nof ideas, and provided the inspiration for phenomenal developments$\backslash$nin statistical methodology for applications in all areas of human$\backslash$nendeavor during the last 75 years. Some of Fisher's pioneering works$\backslash$nhave raised bitter controversies that still continue. These controversies$\backslash$nhave indeed helped in highlighting the intrinsic difficulties in$\backslash$ninductive reasoning and seeking refinements in statistical methodology.},
archivePrefix = {arXiv},
arxivId = {arXiv:1807.00179v1},
author = {Rao, C. Radhakrishna},
doi = {10.1287/isre.l080.0215},
eprint = {arXiv:1807.00179v1},
isbn = {08834237},
issn = {08834237},
journal = {Statistical Science},
keywords = {Ancillary statistics,Bayes theorem,F-test,Fisher information,Fisher optimal scores,confound- ing,confounding,consistency,efficiency,factorial experiments,fiducial proba- bility,likelihood,local control,maximum likelihood,nonparametric tests,randomization,regression,replication,roots of determinantal equation,sufficiency},
month = {feb},
number = {1},
pages = {34--48},
publisher = {Institute of Mathematical Statistics},
title = {{R . A . Fisher : The Founder of Modern Statistics}},
url = {http://projecteuclid.org/euclid.ss/1177011442 http://www.jstor.org/stable/2245989},
volume = {7},
year = {1992}
}
@inproceedings{spigariol_pedagogical_2016,
abstract = {Since years, the prevalent programming paradigm in professional world of software development is the object-oriented one. However, the inertia of traditional forms of programming coupled with outdated learnings in the field, often leading to object programming languages used with a procedural approach and then waste their potential and fall into few robust software. Assuming the influence of university careers systems in the dynamics of software development industry, it is essential to address the educational processes that occur in their area. This paper presents some pedagogical choices about how to teach programming under the paradigm of OOP and use of an educational software, called Wollok, which was designed by university teachers to accompany and sustain this process. It consists in a development environment that includes a new programming language based on the classic Smalltalk ideas with more modern languages own characteristics. Its simplicity allows focus on the main concepts of paradigm - objects, messages, polymorphism- and gradually incorporate more complexity - classes, inheritance, mixins-according to the progress of the learning process.},
author = {Spigariol, L},
booktitle = {2016 {\{}IEEE{\}} {\{}Congreso{\}} {\{}Argentino{\}} de {\{}Ciencias{\}} de la {\{}Inform{\}} {\#}225;tica y {\{}Desarrollos{\}} de {\{}Investigaci{\}} {\#}243;n ({\{}CACIDI{\}})},
doi = {10.1109/CACIDI.2016.7785976},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Spigariol - 2016 - A pedagogical proposal for teaching object-oriented programming Implementation through the educational software Wollo.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Spigariol - 2016 - A pedagogical proposal for teaching object-oriented programming Implementation through the educational software Woll.html:html},
keywords = {Computer aided instruction,Computer languages,Computer science education,OOP paradigm,Programming profession,Proposals,Software,Wollok,Wollok educational software,education,educational technology,learning process,object oriented programming,object programming languages,object-oriented programming,software development},
month = {nov},
pages = {1--6},
shorttitle = {A pedagogical proposal for teaching object-oriente},
title = {{A pedagogical proposal for teaching object-oriented programming: Implementation through the educational software Wollok}},
year = {2016}
}
@article{Vats2018,
abstract = {Gelman and Rubin's (1992) convergence diagnostic is one of the most popular methods for terminating a Markov chain Monte Carlo (MCMC) sampler. Since the seminal paper, researchers have developed sophisticated methods of variance estimation for Monte Carlo averages. We show that this class of estimators find immediate use in the Gelman-Rubin statistic, a connection not established in the literature before. We incorporate these estimators to upgrade both the univariate and multivariate Gelman-Rubin statistics, leading to increased stability in MCMC termination time. An immediate advantage is that our new Gelman-Rubin statistic can be calculated for a single chain. In addition, we establish a relationship between the Gelman-Rubin statistic and effective sample size. Leveraging this relationship, we develop a principled cutoff criterion for the Gelman-Rubin statistic. Finally, we demonstrate the utility of our improved diagnostic via examples.},
archivePrefix = {arXiv},
arxivId = {1812.09384},
author = {Vats, Dootika and Knudson, Christina},
eprint = {1812.09384},
pages = {1--22},
title = {{Revisiting the Gelman-Rubin Diagnostic}},
url = {http://arxiv.org/abs/1812.09384},
year = {2018}
}
@article{Ferguson1974,
abstract = {Methods of generating prior distributions on spaces of probability measures for use in Bayesian nonparametric inference are reviewed with special emphasis on the Dirichlet processes, the tailfree processes, and processes neutral to the right. Some applications are given.},
author = {Ferguson, Thomas S.},
doi = {10.1214/aos/1176342752},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {Bayesian nonparametric inference,Dirichlet process,adaptive investment,adaptive sampling with recall,characterization of distributions,neutral to the right,prior distributions,tailfree processes},
month = {jul},
number = {4},
pages = {615--629},
publisher = {Institute of Mathematical Statistics},
title = {{Prior Distributions on Spaces of Probability Measures}},
url = {https://projecteuclid.org/euclid.aos/1176342752},
volume = {2},
year = {1974}
}
@incollection{Grossman2011,
address = {Amsterdam},
author = {Grossman, Jason},
booktitle = {Philosophy of Statistics},
chapter = {7},
editor = {Bandyopadhyay, Prasanta S. and Forster, Malcolm R.},
pages = {553--580},
publisher = {Elsevier North-Holland},
title = {{The likelihood principle}},
year = {2011}
}
@article{Fisher1955,
abstract = {The attempt to reinterpret the common tests of significance used in scientific research as though they constituted some kind of acceptance procedure and led to "decisions" in Wald's sense, originated in several misapprehensions and has led, apparently, to several more.},
author = {Fisher, Ronald},
doi = {10.2307/2983785},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1955 - Statistical methods and scientific induction.pdf:pdf},
isbn = {00359246},
issn = {00359246},
journal = {Journal of the Royal Stastical Society Series B (Methodological)},
number = {1},
pages = {69--78},
title = {{Statistical methods and scientific induction}},
volume = {17},
year = {1955}
}
@article{Neville2006,
abstract = {OBJECTIVE To assess the frequency of statistical errors in the dermatology literature. DESIGN Original studies published in the Archives of Dermatology and the Journal of the American Academy of Dermatology from January through December 2003 were analyzed for correctness of statistical methods and reporting of the results. RESULTS Of 364 studies published, 155 included statistical analysis. Of these, 59 (38.1{\%}) contained errors in the methods or omissions in reporting of the statistical results. Fourteen percent of the articles with statistical analysis contained errors in the methods used (considered to be more significant errors), 26.5{\%} contained errors in the presentation of the results, and 2.6{\%} contained errors in both. CONCLUSIONS The misuse of statistical methods is prevalent in the dermatology literature, and the appropriate use of these methods is an integral component of all studies. Readers should critically analyze the methods and results of studies published in the dermatology literature.},
author = {Neville, Julie A. and Lang, Wei and Fleischer, Alan B.},
doi = {10.1001/archderm.142.6.737},
issn = {0003-987X},
journal = {Archives of Dermatology},
month = {jun},
number = {6},
pages = {737--40},
pmid = {16785376},
title = {{Errors in the Archives of Dermatology and the Journal of the American Academy of Dermatology From January Through December 2003}},
volume = {142},
year = {2006}
}
@inproceedings{xinogalos_long-term_2009,
abstract = {In this paper we present important results from a long-term evaluation of an ldquoobject-oriented design and programmingrdquo course. In its last form the course is based on the combined use of the microworld objectKarel and the environment BlueJ, while some important modifications on the original teaching approach based on BlueJ have been made.},
author = {Xinogalos, S and Satratzemi, M},
booktitle = {2009 Ninth IEEE International Conference on Advanced Learning Technologies},
doi = {10.1109/ICALT.2009.131},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Xinogalos, Satratzemi - 2009 - A Long-Term Evaluation and Reformation of an Object Oriented Design and Programming Course.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Xinogalos, Satratzemi - 2009 - A Long-Term Evaluation and Reformation of an Object Oriented Design and Programming Course.html:html},
keywords = {BlueJ environment,Books,Computer science education,Course Design and Evaluation,Debugging,Genetic programming,Graphical user interfaces,Guidelines,Informatics,Programming Microworlds,Technology management,education,educational courses,educational programming environments,java,long-term evaluation,microworld objectKarel,object oriented design-programming course,object oriented programming,object-oriented programming,teaching approach},
month = {jul},
pages = {64--66},
title = {{A Long-Term Evaluation and Reformation of an Object Oriented Design and Programming Course}},
year = {2009}
}
@article{Rubin1987,
author = {Rubin, D.B.},
journal = {Journal of the American Statistical Association},
pages = {543--546},
title = {{A noniterative sampling-importance resampling alternative to the data augmen-tation algorithm for creating a few imputations when fractions of missing information are modest: The SIR algorithm}},
volume = {82},
year = {1987}
}
@phdthesis{Xu2013,
abstract = {Statistics},
author = {Xu, Xiaojin},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Xu - 2014 - Methods in Hypothesis Testing , Markov Chain Monte Carlo and Neuroimaging Data Analysis.pdf:pdf},
keywords = {Statistics,Thesis or Dissertation},
month = {sep},
title = {{Methods in Hypothesis Testing , Markov Chain Monte Carlo and Neuroimaging Data Analysis}},
url = {https://dash.harvard.edu/handle/1/11108711},
year = {2014}
}
@article{Tendeiro2019,
abstract = {Null hypothesis significance testing (NHST) has been under scrutiny for decades. The literature shows overwhelming evidence of a large range of problems affecting NHST. One of the proposed alternatives to NHST is using Bayes factors instead of p values. Here we denote the method of using Bayes factors to test point null models as "null hypothesis Bayesian testing" (NHBT). In this article we offer a wide overview of potential issues (limitations or sources of misinterpretation) with NHBT which is currently missing in the literature. We illustrate many of the shortcomings of NHBT by means of reproducible examples. The article concludes with a discussion of NHBT in particular and testing in general. In particular, we argue that posterior model probabilities should be given more emphasis than Bayes factors, because only the former provide direct answers to the most common research questions under consideration.},
author = {Tendeiro, Jorge N. and Kiers, Henk A.L.},
doi = {10.1037/met0000221},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Tendeiro, Kiers - 2019 - A Review of Issues About Null Hypothesis Bayesian Testing.pdf:pdf},
issn = {1082989X},
journal = {Psychological Methods},
keywords = {Bayes factors,Byes statistics,Null hypothesis Bayesian testing,Null hypothesis significance testing,p values},
number = {6},
pages = {774--795},
title = {{A Review of Issues About Null Hypothesis Bayesian Testing}},
volume = {24},
year = {2019}
}
@article{Morey2016,
abstract = {A core aspect of science is using data to assess the degree to which data provide evidence for competing claims, hypotheses, or theories. Evidence is by definition something that should change the credibility of a claim in a reasonable person's mind. However, common statistics, such as significance testing and confidence intervals have no interface with concepts of belief, and thus it is unclear how they relate to statistical evidence. We explore the concept of statistical evidence, and how it can be quantified using the Bayes factor. We also discuss the philosophical issues inherent in the use of the Bayes factor.},
author = {Morey, Richard D. and Romeijn, J.W. and Rouder, J.N.},
doi = {10.1016/j.jmp.2015.11.001},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Morey, Romeijn, Rouder - 2016 - The philosophy of Bayes factors and the quantification of statistical evidence.pdf:pdf},
isbn = {0022-2496},
issn = {10960880},
journal = {Journal of Mathematical Psychology},
keywords = {Bayes factor,Hypothesis testing},
pages = {6--18},
title = {{The philosophy of Bayes factors and the quantification of statistical evidence}},
volume = {72},
year = {2016}
}
@book{Robert2010,
abstract = {Computational techniques based on simulation have now become an essential part of the statistician's toolbox. It is thus crucial to provide statisticians with a practical understanding of those methods, and there is no better way to develop intuition and skills for simulation than to use simulation to solve statistical problems. Introducing Monte Carlo Methods with R covers the main tools used in statistical simulation from a programmer's point of view, explaining the R implementation of each simulation technique and providing the output for better understanding and comparison. While this book constitutes a comprehensive treatment of simulation methods, the theoretical justification of those methods has been considerably reduced, compared with Robert and Casella (2004). Similarly, the more exploratory and less stable solutions are not covered here. This book does not require a preliminary exposure to the R programming language or to Monte Carlo methods, nor an advanced mathematical background. While many examples are set within a Bayesian framework, advanced expertise in Bayesian statistics is not required. The book covers basic random generation algorithms, Monte Carlo techniques for integration and optimization, convergence diagnoses, Markov chain Monte Carlo methods, including Metropolis {\{}Hastings and Gibbs algorithms, and adaptive algorithms. All chapters include exercises and all R programs are available as an R package called mcsm. The book appeals to anyone with a practical interest in simulation methods but no previous exposure. It is meant to be useful for students and practitioners in areas such as statistics, signal processing, communications engineering, control theory, econometrics, finance and more. The programming parts are introduced progressively to be accessible to any reader. Christian P. Robert is Professor of Statistics at Université Paris Dauphine, and Head of the Statistics Laboratory of CREST, both in Paris, France. He has authored more than 150 papers in applied probability, Bayesian statistics and simulation methods. He is a fellow of the Institute of Mathematical Statistics and the recipient of an IMS Medallion. He has authored eight other books, including The Bayesian Choice which received the ISBA DeGroot Prize in 2004, Monte Carlo Statistical Methods with George Casella, and Bayesian Core with Jean-Michel Marin. He has served as Joint Editor of the Journal of the Royal Statistical Society Series B, as well as an associate editor for most major statistical journals, and was the 2008 ISBA President. George Casella is Distinguished Professor in the Department of Statistics at the University of Florida. He is active in both theoretical and applied statistics, is a fellow of the Institute of Mathematical Statistics and the American Statistical Association, and a Foreign Member of the Spanish Royal Academy of Sciences. He has served as Theory and Methods Editor of the Journal of the American Statistical Association, as Executive Editor of Statistical Science, and as Joint Editor of the Journal of the Royal Statistical Society Series B. In addition to books with Christian Robert, he has written Variance Components, 1992, with S.R. Searle and C.E. McCulloch; Statistical Inference, Second Edition, 2001, with Roger Berger; and Theory of Point Estimation, Second Edition, 1998, with Erich Lehmann. His latest book is Statistical Design 2008. Basic R programming -- Random variable generation -- Monte Carlo integration -- Controling and accelerating convergence -- Monte Carlo Optimization -- Metropolis-Hastings algorithms -- Gibbs samplers -- Convergence Monitoring for MCMC algorithms.}},
author = {Robert, Christian P. and Casella, George.},
isbn = {1441915753},
pages = {283},
publisher = {Springer},
title = {{Introducing Monte Carlo methods with R}},
year = {2010}
}
@article{Meyners2012EquivTestingReview,
author = {Meyners, Michael},
doi = {http://dx.doi.org/10.1016/j.foodqual.2012.05.003},
journal = {Food Quality and Preference},
pages = {231--245},
title = {{Equivalence tests - A review}},
volume = {26},
year = {2012}
}
@misc{UCIMachineLearningRepository,
address = {Irvine, CA},
author = {Dua, D. and Graff, C.},
publisher = {University of California, School of Information and Computer Science},
title = {{UCI Machine Learning Repository}},
url = {http://archive.ics.uci.edu/ml},
year = {2019}
}
@article{Newton2014,
abstract = {The notion of entropy is used to compare the complexity associated with 12 common cancers based on metastatic tumor distribution autopsy data. We characterize power-law distributions, entropy, and Kullback-Liebler divergence associated with each primary cancer as compared with data for all cancer types aggregated. We then correlate entropy values with other measures of complexity associated with Markov chain dynamical systems models of progression. The Markov transition matrix associated with each cancer is associated with a directed graph model where nodes are anatomical locations where a metastatic tumor could develop, and edge weightings are transition probabilities of progression from site to site. The steady-state distribution corresponds to the autopsy data distribution. Entropy correlates well with the overall complexity of the reduced directed graph structure for each cancer and with a measure of systemic interconnectedness of the graph, called graph conductance. The models suggest that grouping cancers according to their entropy values, with skin, breast, kidney, and lung cancers being prototypical high entropy cancers, stomach, uterine, pancreatic and ovarian being mid-level entropy cancers, and colorectal, cervical, bladder, and prostate cancers being prototypical low entropy cancers, provides a potentially useful framework for viewing metastatic cancer in terms of predictability, complexity, and metastatic potential.},
author = {Newton, Paul K and Mason, Jeremy and Hurt, Brian and Bethel, Kelly and Bazhenova, Lyudmila and Nieva, Jorge and Kuhn, Peter},
doi = {10.1038/srep07558},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Newton et al. - 2014 - Entropy, complexity, and Markov diagrams for random walk cancer models.pdf:pdf},
isbn = {2045-2322 (Electronic)$\backslash$r2045-2322 (Linking)},
issn = {20452322},
journal = {Scientific Reports},
month = {dec},
pages = {7558},
pmid = {25523357},
publisher = {Nature Publishing Group},
title = {{Entropy, complexity, and Markov diagrams for random walk cancer models}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25523357 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4894412},
volume = {4},
year = {2014}
}
@article{Manski2019,
abstract = {A central objective of empirical research on treatment response is to inform treatment choice. Unfortunately, researchers commonly use concepts of statistical inference whose foundations are distant from the problem of treatment choice. It has been particularly common to use hypothesis tests to compare treatments. Wald's development of statistical decision theory provides a coherent frequentist framework for use of sample data on treatment response to make treatment decisions. A body of recent research applies statistical decision theory to characterize uniformly satisfactory treatment choices, in the sense of maximum loss relative to optimal decisions (also known as maximum regret). This article describes the basic ideas and findings, which provide an appealing practical alternative to use of hypothesis tests. For simplicity, the article focuses on medical treatment with evidence from classical randomized clinical trials. The ideas apply generally, encompassing use of observational data and treatment choice in nonmedical contexts. ARTICLE HISTORY},
author = {Manski, Charles F.},
doi = {10.1080/00031305.2018.1513377},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Manski - 2019 - Treatment Choice With Trial Data Statistical Decision Theory Should Supplant Hypothesis Testing.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {analysis of treatment,medical decisions,minimax regret,randomized,response},
number = {sup1},
pages = {296--304},
title = {{Treatment Choice With Trial Data: Statistical Decision Theory Should Supplant Hypothesis Testing}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1513377},
volume = {73},
year = {2019}
}
@inproceedings{wong_use_2014,
abstract = {The purpose of this study is to identify and evaluate how a computer game could be applied as a tool for learning Object-Oriented programming in computer science courses. The study aims to reduce the complexity of learning object-oriented programming for the students and provide efficient object-oriented design learning environment for them.},
annote = {Malaysia},
author = {Wong, Y S and Yatim, M H Mohamad and Tan, W H},
booktitle = {2014 IEEE Global Engineering Education Conference (EDUCON)},
doi = {10.1109/EDUCON.2014.6826059},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Wong, Yatim, Tan - 2014 - Use computer game to learn Object-Oriented programming in computer science courses.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Wong, Yatim, Tan - 2014 - Use computer game to learn Object-Oriented programming in computer science courses.html:html},
keywords = {Computational modeling,Computer aided instruction,Computer science education,Computers,Educational institutions,Games,Object oriented modeling,computer game,computer games,computer science courses,educational courses,higher education,learning,object oriented programming,object-oriented design learning environment,object-oriented programming,teaching},
month = {apr},
pages = {9--16},
title = {{Use computer game to learn Object-Oriented programming in computer science courses}},
year = {2014}
}
@book{Lehmann2021Volume2,
address = {New York},
author = {Lehmann, E.L. and Romano, Joseph P.},
edition = {4th},
publisher = {Springer International Publishing},
title = {{Testing Statistical Hypotheses - Volume II}},
year = {2021}
}
@book{Marin2014,
address = {New York},
author = {Marin, Jean-Michel and Robert, Christian},
isbn = {9781493950492},
publisher = {Springer},
title = {{Bayesian Essentials With R}},
year = {2014}
}
@book{GorrooChurn2016,
author = {Gorroochurn, Prakash},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gorroochurn - 2016 - Classic Topics on The history of Modern Mathematical statistics.pdf:pdf},
isbn = {9781119127925},
pages = {1--779},
title = {{Classic Topics on The history of Modern Mathematical statistics}},
year = {2016}
}
@article{Ioannidis2016,
abstract = {John Ioannidis argues that problem base, context placement, information gain, pragmatism, patient centeredness, value for money, feasibility, and transparency define useful clinical research. He suggests most clinical research is not useful and reform is overdue.},
author = {Ioannidis, John P.A.},
doi = {10.1371/journal.pmed.1002049},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ioannidis - 2016 - Why Most Clinical Research Is Not Useful.pdf:pdf},
isbn = {1549-1676 (Electronic)$\backslash$r1549-1277 (Linking)},
issn = {15491676},
journal = {PLoS Medicine},
number = {6},
pmid = {27328301},
publisher = {Public Library of Science},
title = {{Why Most Clinical Research Is Not Useful}},
volume = {13},
year = {2016}
}
@article{Robert2008a,
abstract = {Published exactly seventy years ago, Jeffreys's Theory of Probability (1939) has had a unique impact on the Bayesian community and is now considered to be one of the main classics in Bayesian Statistics as well as the initiator of the objective Bayes school. In particular, its advances on the derivation of noninformative priors as well as on the scaling of Bayes factors have had a lasting impact on the field. However, the book reflects the characteristics of the time, especially in terms of mathematical rigor. In this paper we point out the fundamental aspects of this reference work, especially the thorough coverage of testing problems and the construction of both estimation and testing noninformative priors based on functional divergences. Our major aim here is to help modern readers in navigating in this difficult text and in concentrating on passages that are still relevant today.},
archivePrefix = {arXiv},
arxivId = {0804.3173},
author = {Robert, Christian P. and Chopin, Nicolas and Rousseau, Judith},
doi = {10.1214/09-STS284},
eprint = {0804.3173},
journal = {Statistical Science},
keywords = {-finit,Bayesian foundations,and phrases,bayes fac-,bayesian foundations,goodness of fit,jeffreys,kullback divergence,noninformative prior,p -values,s prior,tests,tor,$\sigma$ -finite measure},
number = {2},
pages = {141--172},
title = {{Harold Jeffreys's Theory of Probability Revisited}},
url = {http://arxiv.org/abs/0804.3173{\%}0Ahttp://dx.doi.org/10.1214/09-STS284},
volume = {24},
year = {2008}
}
@book{Lehmann2011,
abstract = {Classical statistical theory-hypothesis testing, estimation, and the design of experiments and sample surveys-is mainly the creation of two men: Ronald A. Fisher (1890-1962) and Jerzy Neyman (1894-1981). Their contributions sometimes complemented each other, sometimes occurred in parallel, and, particularly at later stages, often were in strong opposition. The two men would not be pleased to see their names linked in this way, since throughout most of their working lives they detested each other. Nevertheless, they worked on the same problems, and through their combined efforts created a new d. Classical statistical theory{\{}u2014{\}}hypothesis testing, estimation, and the design of experiments and sample surveys{\{}u2014{\}}is mainly the creation of two men:� Ronald A. Fisher (1890-1962) and Jerzy Neyman (1894-1981).� Their contributions sometimes complemented each other, sometimes occurred in parallel, and, particularly at later stages, often were in strong opposition.� The two men would not be pleased to see their names linked in this way, since throughout most of their working lives they detested each other.� Nevertheless, they worked on the same problems, and through their combined efforts created a new discipline. This new book by E.L. Lehmann, himself a student of Neyman{\{}u2019{\}}s, explores the relationship between Neyman and Fisher, as well as their interactions with other influential statisticians, and the statistical history they helped create together. Lehmann uses direct correspondence and original papers to recreate an historical account of the creation of the Neyman-Pearson Theory as well as Fisher{\{}u2019{\}}s dissent, and other important statistical theories. 1) Introduction -- 2) Fisher's Testing Methodology -- 3) The Neyman-Pearson Theory -- 4) Fisher's Dissent -- 5) The Design of Experiments and Sample Surveys -- 6) Estimation -- 7) Epilog -- Appendix -- References -- Name Index -- Subject Index.},
address = {New York},
author = {Lehmann, E. L.},
isbn = {9781441994998},
pages = {115},
publisher = {Springer},
title = {{Fisher, Neyman, and the creation of classical statistics}},
year = {2011}
}
@article{Rasch2011,
abstract = {Traditionally, when applying the two-sample t test, some pre-testing occurs. That is, the theory-based assumptions of normal distributions as well as of homogeneity of the variances are often tested in applied sciences in advance of the tried-for t test. But this paper shows that such pre-testing leads to unknown final type-I- and type-II-risks if the respective statistical tests are performed using the same set of observations. In order to get an impression of the extension of the resulting misinterpreted risks, some theoretical deductions are given and, in particular, a systematic simulation study is done. As a result, we propose that it is preferable to apply no pre-tests for the t test and no t test at all, but instead to use the Welch-test as a standard test: its power comes close to that of the t test when the variances are homogeneous, and for unequal variances and skewness values {\{}pipe{\}}$\gamma$1{\{}pipe{\}}{\textless} 3, it keeps the so called 20{\%} robustness whereas the t test as well as Wilcoxon's U test cannot be recommended for most cases. {\textcopyright} 2009 Springer-Verlag.},
author = {Rasch, Dieter and Kubinger, Klaus D. and Moder, Karl},
doi = {10.1007/s00362-009-0224-x},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Rasch, Kubinger, Moder - 2011 - The two-sample t test Pre-testing its assumptions does not pay off.pdf:pdf},
issn = {09325026},
journal = {Statistical Papers},
keywords = {Pre-tests,Two-sample t test,Welch-test,Wilcoxon-U test},
month = {feb},
number = {1},
pages = {219--231},
publisher = {Springer-Verlag},
title = {{The two-sample t test: Pre-testing its assumptions does not pay off}},
volume = {52},
year = {2011}
}
@article{Kamary2014,
abstract = {We consider a novel paradigm for Bayesian testing of hypotheses and Bayesian model comparison. Our alternative to the traditional construction of posterior probabilities that a given hypothesis is true or that the data originates from a specific model is to consider the models under comparison as components of a mixture model. We therefore replace the original testing problem with an estimation one that focus on the probability weight of a given model within a mixture model. We analyze the sensitivity on the resulting posterior distribution on the weights of various prior modeling on the weights. We stress that a major appeal in using this novel perspective is that generic improper priors are acceptable, while not putting convergence in jeopardy. Among other features, this allows for a resolution of the Lindley-Jeffreys paradox. When using a reference Beta B(a,a) prior on the mixture weights, we note that the sensitivity of the posterior estimations of the weights to the choice of a vanishes with the sample size increasing and avocate the default choice a=0.5, derived from Rousseau and Mengersen (2011). Another feature of this easily implemented alternative to the classical Bayesian solution is that the speeds of convergence of the posterior mean of the weight and of the corresponding posterior probability are quite similar.},
archivePrefix = {arXiv},
arxivId = {1412.2044},
author = {Kamary, Kaniav and Mengersen, Kerrie and Robert, Christian P. and Rousseau, Judith},
doi = {10.16373/j.cnki.ahr.150049},
eprint = {1412.2044},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kamary et al. - 2014 - Testing hypotheses via a mixture estimation model.pdf:pdf},
isbn = {0894-0282},
issn = {00237205},
journal = {arXiv preprint, https://arxiv.org/abs/1412.2044},
keywords = {and phrases,bayesian anal-,beta prior,improper prior,mixture estimation,mixture model,testing statistical hypotheses,ysis},
pages = {1--37},
pmid = {22352717},
title = {{Testing hypotheses via a mixture estimation model}},
year = {2014}
}
@article{Schuirmann1981,
author = {Schuirmann, D.J.},
journal = {Biometrics},
number = {617},
title = {{On hypothesis testing to determine if the mean of a normal distribution is contained in a known interval}},
volume = {37},
year = {1981}
}
@article{Hoffman2014,
abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size {\{}$\backslash$epsilon{\}} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter {\{}$\backslash$epsilon{\}} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
archivePrefix = {arXiv},
arxivId = {1111.4246},
author = {Hoffman, Matthew D. and Gelman, Andrew},
doi = {10.1190/1.3627885},
eprint = {1111.4246},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {adaptive monte carlo,bayesian inference,dual averaging,hamiltonian monte carlo,markov chain monte carlo},
pages = {1351--1381},
pmid = {1107811},
title = {{The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo}},
url = {http://arxiv.org/abs/1111.4246},
volume = {15},
year = {2014}
}
@misc{MitchellBeauchamp1988,
author = {Mitchell, T.J. and Beauchamp, J.J.},
booktitle = {Journal of the American Statistical Association},
doi = {10.2307/2290131},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Mitchell, Beauchamp - 1988 - Bayesian Variable Selection in Linear Regression.pdf:pdf},
issn = {01621459},
number = {404},
pages = {1023--1032},
title = {{Bayesian Variable Selection in Linear Regression}},
volume = {83},
year = {1988}
}
@book{grant_2015,
address = {New York; Heidelberg; Dordrecht; London; Birkh{\"{a}}user},
author = {Grant, Hardy and Kleiner, Israel},
publisher = {Springer Verlag Birkh{\"{a}}user Science},
title = {{Turning points in the history of mathematics}},
year = {2015}
}
@article{Fisher1922b,
abstract = {Reproduced with permission of Blackwell Publishers},
author = {Fisher, R. A.},
doi = {10.2307/2340521},
isbn = {09528385},
issn = {09528385},
journal = {Journal of the Royal Statistical Society},
number = {1},
pages = {87},
pmid = {1000270032},
title = {{On the Interpretation of $\chi$ 2 from Contingency Tables, and the Calculation of P}},
url = {http://www.jstor.org/stable/2340521?origin=crossref},
volume = {85},
year = {1922}
}
@inproceedings{kelter2018_koli,
address = {New York},
author = {Kelter, Riko and Kramer, Matthias and Brinda, Torsten},
booktitle = {The 18th Koli Calling International Conference on Computing Education Research (Koli Calling '18),},
doi = {https://doi.org/10.1145/3279720.3279727},
publisher = {ACM Press},
title = {{Statistical Frequency-Analysis of Misconceptions In Object-Oriented-Programming: Regularized PCR Models for Frequency Analysis across OOP Concepts and related Factors}},
year = {2018}
}
@article{Stone1976,
author = {Stone, Mervyn},
doi = {10.1080/01621459.1976.10481490},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Stone - 1976 - Strong inconsistency from uniform priors.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
number = {353},
pages = {114--116},
title = {{Strong inconsistency from uniform priors}},
volume = {71},
year = {1976}
}
@article{Loftus1991,
author = {Loftus, Geoffrey R.},
doi = {10.1037/029395},
issn = {00107549},
journal = {Contemporary Psychology: A Journal of Reviews},
month = {feb},
number = {2},
pages = {102--105},
title = {{On the Tyranny of Hypothesis Testing in the Social Sciences}},
url = {http://access.portico.org/stable?au=phzm0jg8k},
volume = {36},
year = {1991}
}
@inproceedings{Plummer2003,
author = {Plummer, Martyn},
booktitle = {Proceedings of the 3rd International Workshop on Distributed Statistical Computing (DSC 2003)},
doi = {10.1111/j.2517-6161.1996.tb02070.x},
title = {{JAGS: A Program for Analysis of Bayesian Graphical Models Using Gibbs Sampling}},
year = {2003}
}
@article{Andrieu2008,
abstract = {Abstract We review adaptive Markov chain Monte Carlo algorithms ( MCMC ) as a mean to optimise their performance. Using simple toy examples we review their theoretical underpinnings, and in particular show why adaptive MCMC algorithms might fail when ... $\backslash$n},
author = {Andrieu, Christophe and Thoms, Johannes},
doi = {10.1007/s11222-008-9110-y},
isbn = {0960-3174},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Adaptive MCMC,Controlled Markov chain,MCMC,Stochastic approximation},
number = {4},
pages = {343--373},
title = {{A tutorial on adaptive MCMC}},
volume = {18},
year = {2008}
}
@article{letheby_statistical_2013,
author = {Letheby, Robert},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Letheby - 2013 - Statistical literacy Bringing concepts to life–recent experiences of the Australian Bureau of Statistics.pdf:pdf},
journal = {Statistical Journal of the IAOS},
number = {3},
pages = {153--157},
shorttitle = {Statistical literacy},
title = {{Statistical literacy: Bringing concepts to life–recent experiences of the Australian Bureau of Statistics}},
volume = {29},
year = {2013}
}
@book{Ruschendorf2014,
author = {R{\"{u}}schendorf, Ludger},
isbn = {9783642419966},
publisher = {Springer},
title = {{Mathematische Statistik}},
year = {2014}
}
@article{Mestek2008,
abstract = {Pedometer-determined physical activity (PA) is inversely related to body composition in middle-aged adults; however, researchers have not established such a relationship in college students. Objective and Participants: In this study, the authors attempted to characterize PA and examine its relationship with body composition in undergraduate college students (N = 88). Methods: The authors measured the BC of 44 women (M age = 21 ± 1 year, M body mass index [BMI] = 23.9 ± 4 kg/m2) and 44 men (M age = 22 ± 1 year, BMI = 26.9 ± 0.9 kg/m2); participants also wore a pedometer for 7 days and completed a PA questionnaire. Results: Men averaged significantly more steps/day (10,027 ± 3,535) than did women (8,610 ± 2,252). For women only, the authors observed significant correlations between steps/day and body composition variables. Men reported engaging in vigorous PA significantly more often than did women. Conclusions: These findings indicate that men engage in PA more often but that PA is related to body composition only in women. In addition, there is better agreement between pedometer-measured and self-reported PA in college-aged men than women. Copyright {\textcopyright} 2008 Heldref Publications.},
author = {Mestek, Michael L. and Plaisance, Eric and Grandjean, Peter},
doi = {10.3200/JACH.57.1.39-44},
issn = {07448481},
journal = {Journal of American College Health},
keywords = {Body composition,College health,Obesity,Pedometer,Physical activity},
month = {jul},
number = {1},
pages = {39--44},
pmid = {18682344},
title = {{The relationship between pedometer-determined and self-reported physical activity and body composition variables in college-aged men and women}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18682344 http://www.tandfonline.com/doi/abs/10.3200/JACH.57.1.39-44},
volume = {57},
year = {2008}
}
@misc{msw_nrw_schulwesen_2017,
author = {NRW, M S W},
month = {mar},
publisher = {Ministerium f{\"{u}}r Schule und Weiterbildung des Landes Nordrhein-Westfalen V{\"{o}}lklinger Str. 49 40221 D{\"{u}}sseldorf},
title = {{Das Schulwesen in Nordrhein-Westfalen aus quantitativer Sicht, Statistische {\"{U}}bersicht Nr. 395, 1. Auflage}},
url = {https://www.schulministerium.nrw.de/docs/bp/Ministerium/Service/Schulstatistik/Amtliche-Schuldaten/Quantita{\_}2016.pdf},
year = {2017}
}
@techreport{Florens1996,
abstract = {An encompassing test between two models is based on the idea that the first model is able to explain the inference obtained by the second model. In a Bayesian Framework, the posterior distribution of the second model will then be compared to the posterior distribution built in hte first model trough a distribution on the parameter of the second model conditionally on the parameter of the first model. Such a strategy is used to test a parametric model against a non parametric one. This strategy is in particular justified by the inadequacy of usual tests as posterior odds. But the implementation of encompassing tests can only be made thanks to simulation techniques which intensively use representations of Dirichlet Measures.},
address = {Louvain},
author = {Florens, J.P. and Richard, J.F. and Rolin, J.M.},
booktitle = {Technical Report 96.08},
institution = {Universit{\'{e}} Catholique de Louvain},
publisher = {Institut de Statistique},
title = {{Bayesian Encompassing Specification Tests of a Parametric Model Against a Non Parametric Alternative}},
year = {1996}
}
@article{Good1956,
author = {Good, I.J.},
journal = {Annals of Mathematical Statistics},
number = {4},
pages = {1130--1135},
title = {{The surprise index for the multivariate normal distribution}},
volume = {27},
year = {1956}
}
@article{Tarran2017,
author = {Tarran, Brian},
doi = {10.1111/j.1740-9713.2017.01003.x},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Tarran - 2017 - How to measure statistical literacy(4).pdf:pdf},
issn = {17409713},
journal = {Significance},
number = {1},
pages = {42--43},
title = {{How to measure statistical literacy?}},
volume = {14},
year = {2017}
}
@article{Begley2013,
abstract = {C. Glenn Begley explains how to recognize the preclinical papers in which the data won't stand up.},
author = {Begley, C. Glenn},
doi = {10.1038/497433a},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
month = {may},
number = {7450},
pages = {433--434},
pmid = {23698428},
title = {{Six red flags for suspect work.}},
url = {http://www.nature.com/articles/497433a},
volume = {497},
year = {2013}
}
@article{Linde2020,
author = {Linde, Maximilian and van Ravenzwaaij, Don},
journal = {arXiv preprint: arXiv:1910.11616v1},
title = {{baymedr: An R Package for the Calculation of Bayes Factors for Equivalence, Non-Inferiority, and Superiority Designs}},
year = {2020}
}
@article{Kruschke2012a,
abstract = {The use of Bayesian methods for data analysis is creating a revolution in fields ranging from genetics to marketing. Yet, results of our literature review, including more than 10,000 articles published in 15 journals from January 2001 and December 2010, indicate that Bayesian approaches are essentially absent from the organizational sciences. Our article introduces organizational science researchers to Bayesian methods and describes why and how they should be used. We use multiple linear regression as the framework to offer a step-by-step demonstration, including the use of software, regarding how to implement Bayesian methods. We explain and illustrate how to determine the prior distribution, compute the posterior distribution, possibly accept the null value, and produce a write-up describing the entire Bayesian process, including graphs, results, and their interpretation. We also offer a summary of the advantages of using Bayesian analysis and examples of how specific published research based on frequentist analysis-based approaches failed to benefit from the advantages offered by a Bayesian approach and how using Bayesian analyses would have led to richer and, in some cases, different substantive conclusions. We hope that our article will serve as a catalyst for the adoption of Bayesian methods in organizational science research. {\textcopyright} The Author(s) 2012.},
author = {Kruschke, John K. and Aguinis, Herman and Joo, Harry},
doi = {10.1177/1094428112457829},
issn = {15527425},
journal = {Organizational Research Methods},
keywords = {Monte Carlo,bootstrapping),computer simulation procedures (e.g.,multilevel research,quantitative research},
number = {4},
pages = {722--752},
publisher = {SAGE Publications Inc.},
title = {{The Time Has Come: Bayesian Methods for Data Analysis in the Organizational Sciences}},
volume = {15},
year = {2012}
}
@article{VanDoorn2017,
abstract = {Bayesian inference for rank-order problems is frustrated by the absence of an explicit likelihood function. This hurdle can be overcome by assuming a latent normal representation that is consistent with the ordinal information in the data: the observed ranks are conceptualized as an impoverished reflection of an underlying continuous scale, and inference concerns the parameters that govern the latent representation. We apply this generic data-augmentation method to obtain Bayes factors for three popular rank-based tests: the rank sum test, the signed rank test, and Spearman's {\$}\backslashrho{\_}s{\$}.},
archivePrefix = {arXiv},
arxivId = {1712.06941},
author = {van Doorn, Johnny and Ly, Alexander and Marsman, Maarten and Wagenmakers, Eric-Jan},
eprint = {1712.06941},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/van Doorn et al. - 2017 - Bayesian Rank-Based Hypothesis Testing for the Rank Sum Test, the Signed Rank Test, and Spearman's rho.pdf:pdf},
keywords = {1018xe amsterdam,b,bayes factors,correspondence may be addressed,data augmentation,department,e-mail,in part by a,j,lands,latent normal,netherlands organization of scientific,nl,of psychological methods,the nether-,this work was supported,to johnny van doorn,university of amsterdam,uva,valckeniersstraat 59,vandoorn,vici grant from the},
pages = {1--33},
title = {{Bayesian Rank-Based Hypothesis Testing for the Rank Sum Test, the Signed Rank Test, and Spearman's rho}},
year = {2017}
}
@article{Ishwaran2005,
abstract = {Variable selection in the linear regression model takes many apparent faces from both frequentist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theoretically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure's ability to reduce model uncertainty in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainty. {\textcopyright} Institute of Mathematical Statistics, 2005.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0505633v1},
author = {Ishwaran, Hemant and Rao, J. Sunil},
doi = {10.1214/009053604000001147},
eprint = {0505633v1},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ishwaran, Rao - 2005 - Spike and slab variable selection Frequentist and bayesian strategies.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Generalized ridge regression,Hypervariance,Model averaging,Model uncertainty,Ordinary least squares,Penalization,Rescaling,Shrinkage,Stochastic variable selection,Zcut},
number = {2},
pages = {730--773},
primaryClass = {arXiv:math},
title = {{Spike and slab variable selection: Frequentist and bayesian strategies}},
volume = {33},
year = {2005}
}
@article{Kirkwood1981,
author = {Kirkwood, Thomas B. L.},
doi = {10.2307/2530573},
issn = {0006341X},
journal = {Biometrics},
number = {3},
pages = {589--594},
title = {{Bioequivalence Testing - A Need to Rethink}},
volume = {37},
year = {1981}
}
@book{Protter2005,
abstract = {It has been 13 years since the first edition of Stochastic Integration and Differential Equations, A New Approach appeared, and in those years many other texts on the same subject have been published, often with connections to applications, especially mathematical finance. Yet in spite of the apparent simplicity of approach, none of these books has used the functional analytic method of presenting semimartingales and stochastic integration. Thus a 2nd edition seems worthwhile and timely, though we will no longer call it "a new approach." The new edition has several significant changes, most prominently the addition of exercises for solution. These are intended to supplement the text, but lemmas needed in a proof are never relegated to the exercises! Many of the exercises have been tested by graduate students at Purdue and Cornell Universities. Chap. 3 has been nearly completely redone, with a new, more intuitive and simultaneously elementary proof of the fundamental Doob-Meyer decomposition theorem, the more general version of the Girsanov theorem due to Lenglart, the Kazamaki-Novikov criteria for exponential local martingales to be martingales, and a modern treatment of compensators. Chap. 4 treats sigma martingales (important in finance theory) and gives a more comprehensive treatment of martingale representation, including both the Jacod-Yor theory and Emery's examples of martingales that actually have martingale representation (thus going beyond the standard cases of Brownian motion and the compensated Poisson process). New topics added include an introduction to the theory of the expansion of filtrations, and an elementary treatment of the Burkholder-Gundy-Fefferman martingale inequalities. Last, there are of course small changes throughout the book.},
address = {Berlin, Heidelberg},
author = {Protter, Philip E.},
booktitle = {Springer},
doi = {10.1007/BF02925506},
isbn = {3540003134},
issn = {09607692},
number = {1},
pages = {415},
pmid = {17245898},
publisher = {Springer Berlin Heidelberg},
series = {Stochastic Modelling and Applied Probability},
title = {{Stochastic Integration and Differential Equations}},
url = {http://link.springer.com/10.1007/978-3-662-10061-5},
volume = {21},
year = {2004}
}
@book{hacking_1990,
address = {Cambridge},
author = {Hacking, Ian},
publisher = {Cambridge University Press},
title = {{The Taming Of Chance}},
year = {1990}
}
@book{Lehmann2021,
address = {New York},
author = {Lehmann, E.L. and Romano, Joseph P.},
edition = {4th},
publisher = {Springer International Publishing},
title = {{Testing Statistical Hypotheses - Volume I}},
year = {2021}
}
@inproceedings{CrossII2008,
abstract = {Inheritance and polymorphism are important OOP topics in CS1 and CS2. While these concepts are generally straightforward, there are subtleties that may leave students confused. For example, accessibility of a field in an object is generally determined by the declaring type and access modifiers of the field, and the context in the executing program from which the field is referenced. The dynamic object viewers in jGRASP have been expanded to allow the user to: (1) change the declared type of the object reference to any compatible type and (2) change the accessibility context of the object reference. These options enable the user to explore accessibility and visibility relationships by experimenting with any object on the workbench or in the debugger. Symbols, color, and text are used in the viewer to indicate inheritance relationships, accessibility, and visibility of fields and methods. Initial classroom use has demonstrated the potential for these new viewer features as an aid to students who are learning about inheritance and polymorphism.},
author = {{Cross  II}, James H and Hendrix, T Dean and Umphress, David A and Barowski, Larry A},
booktitle = {Proceedings of the 13th Annual Conference on Innovation and Technology in Computer Science Education},
doi = {10.1145/1384271.1384300},
isbn = {978-1-60558-078-4},
keywords = {Visualization,inheritance,jGRASP,object viewers,object-oriented programming,polymorphism},
pages = {103--108},
publisher = {ACM},
title = {{Exploring Accessibility and Visibility Relationships in Java}},
url = {http://doi.acm.org/10.1145/1384271.1384300},
year = {2008}
}
@book{Kolmogorov1956,
address = {Providence, RI},
author = {Kolmogorov, A.N.},
edition = {2nd},
publisher = {AMS Chelsea Publishing},
title = {{Foundations of the theory of probability}},
year = {1956}
}
@incollection{Sprenger2016a,
address = {Oxford},
author = {Sprenger, Jan},
booktitle = {Oxford Handbook of the Philosophy of Science},
doi = {10.1093/oxfordhb/9780199368815.013.10},
editor = {Humphreys, Paul},
publisher = {Oxford University Press},
title = {{Confirmation and Induction}},
url = {http://oxfordhandbooks.com/view/10.1093/oxfordhb/9780199368815.001.0001/oxfordhb-9780199368815-e-10},
year = {2016}
}
@book{Moosbrugger2012,
address = {Berlin Heidelberg},
annote = {Literaturangaben

Tests und Frageb{\"{o}}gen konstruieren, beurteilen und verstehen sowie Daten analysieren - diese Kompetenzen geh{\"{o}}ren zum Handwerkszeug der Psychologie, aber auch der Sozial- oder Wirtschaftswissenschaften. Die Autoren vermitteln das im Bachelor-Studium erforderliche Grundlagenwissen sowie vertiefende Aspekte f{\"{u}}r den Master-Studiengang - mit Merks{\"{a}}tzen, Zusammenfassungen und Anwendungsbeispielen. Die 2., {\"{u}}berarbeitete Auflage wurde u. a. um Hinweise zur Nutzung von Anwendungssoftware, um Beispieldatens{\"{a}}tze sowie Musteranalysen zum Download erg{\"{a}}nzt Tests und Frageb{\"{o}}gen konstruieren, beurteilen und verstehen sowie Daten analysieren - diese Kompetenzen geh{\"{o}}ren zum Handwerkszeug der Psychologie, aber auch der Sozial- oder Wirtschaftswissenschaften. Die Autoren vermitteln das im Bachelor-Studium erforderliche Grundlagenwissen sowie vertiefende Aspekte f{\"{u}}r den Master-Studiengang - mit Merks{\"{a}}tzen, Zusammenfassungen und Anwendungsbeispielen. Die 2., {\"{u}}berarbeitete Auflage wurde u. a. um Hinweise zur Nutzung von Anwendungssoftware, um Beispieldatens{\"{a}}tze sowie Musteranalysen zum Download erg{\"{a}}nzt},
author = {Moosbrugger, Helfried and Kelava, Augustin},
booktitle = {Springer-Lehrbuch},
edition = {2., aktual},
isbn = {978-3-642-20071-7 978-3-642-20072-4},
keywords = {Fragebogen,Lehrbuch,Psychological tests,Psychological tests Standards,Psychometrics,Questionnaires,Testkonstruktion,Testtheorie},
language = {ger},
pages = {439},
publisher = {Springer},
shorttitle = {Testtheorie und Fragebogenkonstruktion},
title = {{Testtheorie und Fragebogenkonstruktion: mit 66 Abbildungen und 41 Tabellen}},
year = {2012}
}
@article{Morey2016c,
abstract = {Interval estimates – estimates of parameters that include an allowance for sampling uncertainty – have long been touted as a key component of statistical analyses. There are several kinds of interval estimates, but the most popular are confidence intervals (CIs): intervals that contain the true parameter value in some known proportion of repeated samples, on average. The width of confidence intervals is thought to index the precision of an estimate; CIs are thought to be a guide to which parameter values are plausible or reasonable; and the confidence coefficient of the interval (e.g., 95 {\%}) is thought to index the plausibility that the true parameter is included in the interval. We show in a number of examples that CIs do not necessarily have any of these properties, and can lead to unjustified or arbitrary inferences. For this reason, we caution against relying upon confidence interval theory to justify interval estimates, and suggest that other theories of interval estimation should be used instead.},
author = {Morey, Richard D. and Hoekstra, Rink and Rouder, Jeffrey N. and Lee, Michael D. and Wagenmakers, Eric-Jan},
doi = {10.3758/s13423-015-0947-8},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Morey et al. - 2016 - The fallacy of placing confidence in confidence intervals.pdf:pdf},
issn = {1069-9384},
journal = {Psychonomic Bulletin {\&} Review},
keywords = {Bayesian inference and parameter estimation,Bayesian statistics,Statistical inference,Statistics},
number = {1},
pages = {103--123},
publisher = {Springer New York LLC},
title = {{The fallacy of placing confidence in confidence intervals}},
volume = {23},
year = {2016}
}
@incollection{Raftery1996,
address = {London},
author = {Raftery, A.E.},
booktitle = {Markov Chain Monte Carlo in Practice},
editor = {Gilks, W.R. and Spiegelhalter, D.J. and Richardson, S.},
pages = {163--188},
publisher = {Chapman {\&} Hall},
title = {{Hypothesis testing and model selection}},
year = {1996}
}
@article{Krueger2019,
author = {Krueger, Joachim I. and Heck, Patrick R.},
doi = {10.1080/00031305.2018.1470033},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Krueger, Heck - 2019 - Putting the iPi -Value in its Place.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Bayes' theorem,Inference,Null hypotheses,Statistical significance testing,bayes,inference,null hypotheses,p -values,p-values,statistical significance testing,theorem},
number = {sup1},
pages = {122--128},
publisher = {Taylor {\&} Francis},
title = {{Putting the {\textless}i{\textgreater}P{\textless}/i{\textgreater} -Value in its Place}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1470033},
volume = {73},
year = {2019}
}
@article{Sugden2013,
abstract = {MOTIVATION Validation and reproducibility of results is a central and pressing issue in genomics. Several recent embarrassing incidents involving the irreproducibility of high-profile studies have illustrated the importance of this issue and the need for rigorous methods for the assessment of reproducibility. RESULTS Here, we describe an existing statistical model that is very well suited to this problem. We explain its utility for assessing the reproducibility of validation experiments, and apply it to a genome-scale study of adenosine deaminase acting on RNA (ADAR)-mediated RNA editing in Drosophila. We also introduce a statistical method for planning validation experiments that will obtain the tightest reproducibility confidence limits, which, for a fixed total number of experiments, returns the optimal number of replicates for the study. AVAILABILITY Downloadable software and a web service for both the analysis of data from a reproducibility study and for the optimal design of these studies is provided at http://ccmbweb.ccv.brown.edu/reproducibility.html .},
author = {Sugden, Lauren A. and Tackett, Michael R. and Savva, Yiannis A. and Thompson, William A. and Lawrence, Charles E.},
doi = {10.1093/bioinformatics/btt508},
isbn = {1367-4811 (Electronic)$\backslash$r1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
month = {nov},
number = {22},
pages = {2844--2851},
pmid = {24048353},
title = {{Assessing the validity and reproducibility of genome-scale predictions}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btt508},
volume = {29},
year = {2013}
}
@book{Kleijn2020,
address = {Amsterdam},
author = {Kleijn, B.J.K.},
publisher = {Springer},
title = {{The frequentist theory of Bayesian statistics}},
year = {2020}
}
@book{eckey_deskriptive_2008,
abstract = {Verlagstext: Dieses Lehrbuch vermittelt anwendungsorientiert die Verfahren der Deskriptiven Statistik, wie sie in den Wirtschafts- und Sozialwissenschaften an Universit{\"{a}}ten und Fachhochschulen gelehrt werden. Anhand zahlreicher Beispiele werden die statistischen Methoden nicht nur anschaulich dargestellt, sondern ihre Ergebnisse auch ausf{\"{u}}hrlich interpretiert. Somit eignet sich das Buch hervorragend als Begleitlekt{\"{u}}re und zum selbstst{\"{a}}ndigen Nacharbeiten einer Vorlesung oder auch zum gezielten Nachschlagen bestimmter Fragestellungen. Das Lehrbuch empfiehlt sich auch f{\"{u}}r Praktiker, beispielsweise aus der Markt- und Meinungsforschung und dem Controlling, die sich Grundlagen aneignen oder {\"{u}}ber die Interpretation von bestimmten Kennzahlen informieren wollen},
address = {Wiesbaden},
annote = {Literaturverz. S. [275] - 277 Bis 3. Aufl. u.d.T.: Eckey, Hans-Friedrich: Statistik
OCLC: 244627736},
author = {Eckey, Hans-Friedrich and Kosfeld, Reinhold and T{\"{u}}rck, Matthias},
edition = {5., {\"{u}}berar},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Eckey, Kosfeld, T{\"{u}}rck - 2008 - Deskriptive Statistik Grundlagen - Methoden - Beispiele.pdf:pdf},
isbn = {978-3-8349-0859-9},
publisher = {Gabler},
series = {Lehrbuch},
shorttitle = {Deskriptive {\{}Statistik{\}}},
title = {{Deskriptive Statistik: Grundlagen - Methoden - Beispiele}},
year = {2008}
}
@article{Richardson1993,
abstract = {Risk factors used in epidemiology are often measured with error which can seriously affect the assessment of the relation between risk factors and disease outcome. In this paper, a Bayesian perspective on measurement error problems in epidemiology is taken and it is shown how the information available in this setting can be structured in terms of conditional independence models. The modeling of common designs used in the presence of measurement error (validation group, repeated measures, ancillary data) is described The authors indicate how Bayesian estimation can be carried out in these settings using Gibbs sampling, a sampling technique which is being increasingly referred to in statistical and biomedical applications. The method is illustrated by analyzing a design with two measunng instruments and no validation group. Am J Epidemiol 1993; 138.430-42 {\textcopyright} 1993 by The Johns Hopkins University School of Hygiene and Public Health.},
author = {Richardson, Sylvia and Gilks, Walter R.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Richardson, Gilks - 1993 - A bayesian approach to measurement error problems in epidemiology using conditional independence models.pdf:pdf},
journal = {American Journal of Epidemiology},
keywords = {Bayesian method,Biometry,Epidemiologic methods,Monte Carlo method},
number = {6},
pages = {430--442},
title = {{A Bayesian approach to measurement error problems in epidemiology using conditional independence models}},
volume = {138},
year = {1993}
}
@article{McShane2018,
author = {McShane, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/McShane et al. - Unknown - Abandon Statistical Significance.pdf:pdf},
journal = {The American Statistician},
title = {{Abandon Statistical Significance}}
}
@book{bortz_statistik_2010,
abstract = {Der neue BORTZ-SCHUSTER rechnet (sich) f{\"{u}}r Sie! Denn er wurde in der 7. Auflage komplett {\"{u}}berarbeitet, erg{\"{a}}nzt und didaktisch verbessert ; jetzt mit brandneuen Online-Zusatzmaterialien! An Bew{\"{a}}hrtem wurde festgehalten: Dieses Lehrbuch ist DIE Grundlage im Bachelorstudium, denn es ist alles Pr{\"{u}}fungsrelevante drin, von Elementarstatistik {\"{u}}ber varianzanalytische Methoden bis zu multivariaten Methoden und {\"{U}}bungsaufgaben helfen bei der Pr{\"{u}}fungsvorbereitung. Es ist weiterhin auch DAS Nachschlagewerk im Masterstudium und in der Forschung: Beispiele aus dem psychologischen Forschungsalltag helfen beim Nachvollziehen von Berechnungen und ein Glossar liefert schnell die wichtigsten Begriffsdefinitionen. NEUES wurde sinnvoll erg{\"{a}}nzt, die Didaktik optimiert: Die neue Auflage ist neu strukturiert, k{\"{u}}rzere Kapitel sorgen f{\"{u}}r einen klareren Aufbau. Die Grundlagen werden f{\"{u}}r Einsteiger verst{\"{a}}ndlicher und ausf{\"{u}}hrlicher dargestellt. Markierungen kennzeichnen Vertiefungskapitel. Hinweise zu EDV-Anwendungen sind systematisch in K{\"{a}}sten hervorgehoben. Und besonders interessant f{\"{u}}r Anwender und die Pr{\"{u}}fungsvorbereitung: Eine neue begleitende Website enth{\"{a}}lt Anleitungen f{\"{u}}r die Berechnung der im Buch enthaltenen Beispiele mit Statistikprogrammen (SPSS-Syntax) sowie Lerntools f{\"{u}}r Studierende und Materialien (Abbildungen und Folien) f{\"{u}}r Dozenten. Der BORTZ-SCHUSTER ist und bleibt ein unerl{\"{a}}ssliches Statistik-Lehrbuch f{\"{u}}r Studierende der Psychologie und Sozialwissenschaften, f{\"{u}}r Wissenschaftler und f{\"{u}}r Anwender},
address = {Berlin Heidelberg},
annote = {Literaturverzeichnis: Seite 603-636 Mit 70 Abbildungen und 163 Tabellen - Titelblatt
OCLC: 845714518},
author = {Bortz, J{\"{u}}rgen and Schuster, Christof},
edition = {7., vollst},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bortz, Schuster - 2010 - Statistik f{\"{u}}r Human- und Sozialwissenschaftler.pdf:pdf},
isbn = {978-3-642-12769-4},
keywords = {Humanwissenschaften,Mathematical sociology,Psychological tests,Social sciences Statistics,Sociology Statistical methods,Sozialwissenschaften,Statistik},
publisher = {Springer},
series = {Springer-{\{}Lehrbuch{\}}},
title = {{Statistik f{\"{u}}r Human- und Sozialwissenschaftler}},
year = {2010}
}
@inproceedings{ragonis_mis_2017,
abstract = {The paper presents research conducted with high school (HS) students (N=86) learning object-oriented programming (OOP) and computer science HS teachers (N=48). The focus was on students' and teachers' understanding of the this reference. Proper conceptualization of this indicates an understanding of objects in general and of the current object, and it involves various aspects of programming variants. Students' preferences as to the use of the this reference were also examined. Findings revealed a lack of understanding of both the implication and the implementation of this; only 45{\%} of the students expressed understanding of when we must use this; only 60{\%} expressed understanding of when not to use this, and only 24{\%} expressed clear understanding in their definition of this. Even correct answers do not necessarily indicate conceptual understanding, rather a repetition of definitions or programming habits, or a reliance on operative aspects of the implementation. The teachers expressed a considerable lack of clarity in accurately characterizing the correctness of students' answers.},
address = {New York, NY, USA},
author = {Ragonis, Noa and Shmallo, Ronit},
booktitle = {Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education},
doi = {10.1145/3017680.3017715},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ragonis, Shmallo - 2017 - On the (Mis) Understanding of the This Reference.pdf:pdf},
isbn = {978-1-4503-4698-6},
keywords = {Computer science education,object-oriented programming,this reference},
pages = {489--494},
publisher = {ACM},
series = {{\{}SIGCSE{\}} '17},
title = {{On the (Mis) Understanding of the "This" Reference}},
url = {http://doi.acm.org/10.1145/3017680.3017715},
year = {2017}
}
@inproceedings{Dawid1977,
address = {Grenoble},
author = {Dawid, A.P.},
booktitle = {Proceedings of the European Meeting of Statisticians},
isbn = {0720407516},
publisher = {North-Holland Pub. Co.},
title = {{Recent Developments in Statistics}},
year = {1977}
}
@phdthesis{ziegler_reconceptualizing_2014,
author = {Ziegler, Laura Ann},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ziegler - 2014 - Reconceptualizing statistical literacy Developing an assessment for the modern introductory statistics course.pdf:pdf},
school = {University of Minnesota},
shorttitle = {Reconceptualizing statistical literacy},
title = {{Reconceptualizing statistical literacy: Developing an assessment for the modern introductory statistics course}},
year = {2014}
}
@book{Salsburg2001,
address = {New York},
author = {Salsburg, David},
publisher = {Henry Holt and Company},
title = {{The Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century: How Statisticians Revolutionized Science in the 20th Century}},
year = {2001}
}
@article{Neymann1928,
author = {Neyman, Author J and Pearson, E S},
journal = {Biometrika},
number = {1},
pages = {175--240},
title = {{On the Use and Interpretation of Certain Test Criteria for Purposes of Statistical Inference : Part I}},
volume = {20},
year = {1928}
}
@article{Fisher1958NatureOfProbability,
author = {Fisher, Ronald Aylmer},
journal = {Centennial Review},
number = {2},
pages = {261--274},
title = {{The nature of probability}},
year = {1958}
}
@article{Bai2011,
abstract = {The efficiency of Markov chain Monte Carlo (MCMC) algorithms can vary dramatically with the choice of simulation parameters. Adaptive MCMC (AMCMC) algorithms allow the automatic tuning of the parameters while the simulation is in progress. A multimodal target distribution may call for regional adaptation of Metropolis–Hastings samplers so that the proposal distribution varies across regions in the sample space. Establishing such a partition is not straightforward and, in many instances, the learning required for its specification takes place gradually, as the simulation proceeds. In the case in which the target distribution is approximated by a mixture of Gaussians, we propose an adaptation process for the partition. It involves fitting the mixture using the available samples via an online EM algorithm and, based on the current mixture parameters, constructing the regional adaptive algorithm with online recursion (RAPTOR). The method is compared with other regional AMCMC samplers and is tested on simulate...},
author = {Bai, Yan and Craiu, Radu V. and {Di Narzo}, Antonio F.},
doi = {10.1198/jcgs.2010.09035},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Adaptive MCMC,Mixture model,Online EM},
month = {jan},
number = {1},
pages = {63--79},
publisher = {Taylor {\&} Francis},
title = {{Divide and Conquer: A Mixture-Based Approach to Regional Adaptation for MCMC}},
url = {http://www.tandfonline.com/doi/abs/10.1198/jcgs.2010.09035},
volume = {20},
year = {2011}
}
@article{Naaman2016JeffreysLindleyParadox,
author = {Naaman, Michael},
doi = {10.1214/16-EJS1146},
journal = {Electronic Journal of Statistics},
number = {1},
pages = {1526--1550},
title = {{Almost sure hypothesis testing and a resolution of the Jeffreys-Lindley paradox}},
volume = {10},
year = {2016}
}
@book{Sprenger2019,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
author = {Sprenger, Jan and Hartmann, Stephan},
booktitle = {Bayesian Philosophy of Science},
doi = {10.1093/oso/9780199672110.001.0001},
month = {aug},
publisher = {Oxford University Press},
title = {{Bayesian Philosophy of Science}},
year = {2019}
}
@article{Bond2012,
abstract = {Although statistics education research has focused on students' learning and conceptual understanding of statistics, researchers have only recently begun investigating students' perceptions of statistics. The term perception describes the overlap between cognitive and non-cognitive factors. In this mixed-methods study, undergraduate students provided their perceptions of statistics and completed the "Survey of Students' Attitudes Toward Statistics-36" (SATS-36). The qualitative data suggest students had basic knowledge of what the word statistics meant, but with varying depths of understanding and conceptualization of statistics. Quantitative analysis also examined the relationship between students' perceptions of statistics and attitudes toward statistics. We found no significant difference in mean pre- or post-SATS scores across conceptualization and content knowledge categories. The implications of these findings for education and research are discussed. (Contains 6 tables and 6 figures.)},
author = {Bond, Marjorie E. and Perkins, Susan N. and Ramirez, Caroline},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bond, Perkins, Ramirez - 2012 - Students' perceptions of statistics An exploration of attitudes, conceptualizations, and content knowled.pdf:pdf},
issn = {15701824},
journal = {Statistics Education Research Journal},
keywords = {Conception of statistics,Sats-36,Statistics education research,Student attitudes},
number = {2},
pages = {6--25},
title = {{Students' perceptions of statistics: An exploration of attitudes, conceptualizations, and content knowledge of statistics}},
volume = {11},
year = {2012}
}
@misc{Stat2DataRPackage,
author = {Cannon, Ann and Cobb, George and Hartlaub, Bradley and Legler, Julie and Lock, Robin and Moore, Thomas and Rossman, Allan and Witmer, Jeffrey},
title = {{Stat2Data: Datasets for Stat2}},
year = {2019}
}
@article{Savage1962,
abstract = {Accessed: 25-12-2017 20:48 UTC JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.},
author = {Savage, L J and Barnard, George and Cornfield, Jerome and Bross, Irwin and Box, George E P and Good, I J and Lindley, D V and Clunies-Ross, C W and Pratt, John W and Levene, Howard and Goldman, Thomas and Dempster, A P and Kempthorne, Oscar and Birnbaum, Allan},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Savage et al. - 1962 - On the Foundations of Statistical Inference Discussion.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {298},
pages = {307--326},
title = {{On the Foundations of Statistical Inference: Discussion}},
volume = {57},
year = {1962}
}
@inproceedings{fuchs_improving_2016,
abstract = {In this paper we present a newly developed online learning platform which introduces gamification elements into software engineering education. Starting from assumptions based on cognitive load theory we present the design of an online gamification-based training system to be used in software engineering contexts. Students can voluntarily solve challenges for which they may earn credits. These small problems serve as assessments; the approach follows the assessment for learning paradigm in that assessments provide formative feedback to enhance the learning experience. The combination of formative assessment and gamification is new to software engineering education. We describe system design as well as the different types of challenges in detail. We also provide several examples for actual challenges used in an object-oriented programming introduction using Java.},
author = {Fuchs, M and Wolff, C},
booktitle = {2016 {\{}IEEE{\}} {\{}Global{\}} {\{}Engineering{\}} {\{}Education{\}} {\{}Conference{\}} ({\{}EDUCON{\}})},
doi = {10.1109/EDUCON.2016.7474653},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fuchs, Wolff - 2016 - Improving programming education through gameful, formative feedback.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fuchs, Wolff - 2016 - Improving programming education through gameful, formative feedback.html:html},
keywords = {Computer aided instruction,Computer science education,Context,Games,Internet,Programming profession,assessment for learning,cognition,cognitive laod theory,cognitive load theory,computer games,education,formative feedback assessment,gamification,gamification-based training system,java,learning,object-oriented programming,online learning platform,online web-based self-assessment Introduction (He,programming education,software engineering,software engineering education},
month = {apr},
pages = {860--867},
title = {{Improving programming education through gameful, formative feedback}},
year = {2016}
}
@article{Kelter2021MathPsy,
author = {Kelter, Riko},
doi = {10.1016/j.jmp.2020.102474},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
publisher = {Academic Press},
title = {{Bayesian model selection in the M-open setting — Approximate posterior inference and subsampling for efficient large-scale leave-one-out cross-validation via the difference estimator}},
volume = {100},
year = {2021}
}
@article{OHagan2019,
abstract = {Expert opinion and judgment enter into the practice of statistical inference and decision-making in numerous ways. Indeed, there is essentially no aspect of scientific investigation in which judgment is not required. Judgment is necessarily subjective, but should be made as carefully, as objectively, and as scientifically as possible. Elicitation of expert knowledge concerning an uncertain quantity expresses that knowledge in the form of a (subjective) probability distribution for the quantity. Such distributions play an important role in statistical inference (for example as prior distributions in a Bayesian analysis) and in evidence-based decision-making (for example as expressions of uncertainty regarding inputs to a decision model). This article sets out a number of practices through which elicitation can be made as rigorous and scientific as possible. One such practice is to follow a recognized protocol that is designed to address and minimize the cognitive biases that experts are prone to when making probabilistic judgments. We review the leading protocols in the field, and contrast their different approaches to dealing with these biases through the medium of a detailed case study employing the SHELF protocol.},
author = {O'Hagan, Anthony},
doi = {10.1080/00031305.2018.1518265},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/O'Hagan - 2019 - Expert Knowledge Elicitation Subjective but Scientific.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {cognitive bias,elaboration,elicitation protocol,facilitator,heuristics},
number = {sup1},
pages = {69--81},
title = {{Expert Knowledge Elicitation: Subjective but Scientific}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1518265},
volume = {73},
year = {2019}
}
@article{Yang2019,
author = {Yang, Jinyoung and Levi, Evgeny and Craiu, Radu V and Rosenthal, Jeffrey S},
journal = {Journal of Computational and Graphical Statistics},
number = {2},
pages = {276--289},
title = {{Adaptive Component-wise Multiple-Try Metropolis Sampling}},
volume = {28},
year = {2019}
}
@article{Gebski2003a,
author = {Gebski, Val J and Keech, Anthony C},
doi = {10.5694/J.1326-5377.2003.TB05139.X},
issn = {1326-5377},
journal = {Medical Journal of Australia},
keywords = {Statistics,epidemiology and research design},
month = {feb},
number = {4},
pages = {182--184},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Statistical methods in clinical trials}},
volume = {178},
year = {2003}
}
@article{Siebert2015,
abstract = {To explore increasing concerns about scientific misconduct and data irreproducibility in some areas of science, we interviewed a number of senior biomedical researchers. These interviews revealed a perceived decline in trust in the scientific enterprise, in large part because the quantity of new data exceeds the field's ability to process it appropriately. This phenomenon-which is termed 'overflow' in social science-has important implications for the integrity of modern biomedical science.},
author = {Siebert, Sabina and Machesky, Laura M and Insall, Robert H},
doi = {10.7554/eLife.10825},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Siebert, Machesky, Insall - 2015 - Overflow in science and its implications for trust.pdf:pdf},
isbn = {10.7554/eLife.10825},
issn = {2050084X},
journal = {eLife},
keywords = {overflow,point of view,reproducibility,scientific conduct,trust in science},
month = {sep},
number = {e10825},
pmid = {26365552},
publisher = {eLife Sciences Publications, Ltd},
title = {{Overflow in science and its implications for trust}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26365552 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4563216},
volume = {4},
year = {2015}
}
@misc{Olsen2003,
abstract = {The use of statistics in scientific and medical journals has been subjected to considerable review in recent years. Many journals have published systematic reviews of statistical meth- ods (4–8, 12). These reviews indicate room for improvement. Typically, at least half of the published scientific articles that use statistical methods contain statistical errors. Common er- rors include failing to document the statistical methods used or using an improper method to test a statistical hypothesis. This study analyzes articles in Infection and Immunity for appropriateness of statistical analysis and reporting. I reviewed all 141 articles from two issues, January 2002 (volume 70, no. 1) and July 2002 (volume 70, no. 7); listed the statistical anal- yses used; and identified errors in analysis and reporting. Er- rors were defined with respect to the current instructions to authors (2), accepted statistical practice (1, 11), and profes- sional judgment. The statistical errors identified in Infection and Immunity are comparable to those found in similar journals: 54{\%} of the articles reviewed contained errors of analysis (20{\%}), reporting (22{\%}), or both (12{\%}). The most common analysis errors are failure to adjust or account for multiple comparisons (27 stud- ies), reporting a conclusion based on observation without con- ducting a statistical test (20 studies), and use of statistical tests that assume a normal distribution on data that follow a skewed distribution (at least 11 studies). The most common reporting errors are unlabeled or inappropriate error bars or measures of variability (15 studies) and failure to describe the statistical tests performed (12 studies). These errors are discussed more fully below, with examples and suggestions for improvement},
author = {Olsen, Cara H},
booktitle = {Infection and Immunity},
doi = {10.1128/IAI.71.12.6689-6692.2003},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Olsen - 2003 - Review of the Use of Statistics in Infection and Immunity.pdf:pdf},
issn = {00199567},
month = {dec},
number = {12},
pages = {6689--6692},
pmid = {14638751},
publisher = {American Society for Microbiology Journals},
title = {{Review of the Use of Statistics in Infection and Immunity}},
volume = {71},
year = {2003}
}
@article{Burnham2004,
abstract = {The model selection literature has been generally poor at reflecting the deep foundations of the Akaike information criterion (AIC) and at making appropriate comparisons to the Bayesian information criterion (BIC). There is a clear philosophy, a sound criterion based in information theory, and a rigorous statistical foundation for AIC. AIC can be justified as Bayesian using a "savvy" prior on models that is a function of sample size and the number of model parameters. Furthermore, BIC can be derived as a non-Bayesian result. Therefore, arguments about using AIC versus BIC for model selection cannot be from a Bayes versus frequentist perspective. The philosophical context of what is assumed about reality, approximating models, and the intent of model-based inference should determine whether AIC or BIC is used. Various facets of such multimodel inference are presented here, particularly methods of model averaging.},
author = {Burnham, Kenneth P. and Anderson, David R.},
doi = {10.1177/0049124104268644},
issn = {0049-1241},
journal = {Sociological Methods {\&} Research},
keywords = {AIC,BIC,Model averaging,Model selection,Multimodel inference},
month = {nov},
number = {2},
pages = {261--304},
publisher = {Sage PublicationsSage CA: Thousand Oaks, CA},
title = {{Multimodel Inference}},
url = {http://journals.sagepub.com/doi/10.1177/0049124104268644},
volume = {33},
year = {2004}
}
@article{Christensen2005,
abstract = {This article presents a simple example that illustrates the key differences and similarities between the Fisherian, Neyman-Pearson, and Bayesian approaches to testing. Implications for more complex situations are also discussed.},
author = {Christensen, Ronald},
doi = {10.1198/000313005X20871},
isbn = {0003-1305},
issn = {00031305},
journal = {American Statistician},
keywords = {Confidence,Lindley's paradox,Most powerful test,P values,Significance tests},
month = {may},
number = {2},
pages = {121--126},
pmid = {29378013},
title = {{Testing Fisher, Neyman, Pearson, and Bayes}},
url = {http://www.tandfonline.com/doi/abs/10.1198/000313005X20871},
volume = {59},
year = {2005}
}
@book{Ghosal2017,
abstract = {Explosive growth in computing power has made Bayesian methods for infinite-dimensional models - Bayesian nonparametrics - a nearly universal framework for inference, finding practical use in numerous subject areas. Written by leading researchers, this authoritative text draws on theoretical advances of the past twenty years to synthesize all aspects of Bayesian nonparametrics, from prior construction to computation and large sample behavior of posteriors. Because understanding the behavior of posteriors is critical to selecting priors that work, the large sample theory is developed systematically, illustrated by various examples of model and prior combinations. Precise sufficient conditions are given, with complete proofs, that ensure desirable posterior properties and behavior. Each chapter ends with historical notes and numerous exercises to deepen and consolidate the reader's understanding, making the book valuable for both graduate students and researchers in statistics and machine learning, as well as in application areas such as econometrics and biostatistics.},
author = {Ghosal, Subhashis and van der Vaart, Aad},
booktitle = {Fundamentals of Nonparametric Bayesian Inference},
doi = {10.1017/9781139029834},
file = {:Users/riko/Downloads/(Cambridge series in statistical and probabilistic mathematics) Ghoshal, Subhashis{\_} Vaart, Aad W. van der - Fundamentals of nonparametric Bayesian inference-Cambridge University Press (2017).pdf:pdf},
isbn = {9781139029834},
pages = {1--646},
title = {{Fundamentals of nonparametric Bayesian inference}},
year = {2017}
}
@article{Granero2020,
author = {Granero, Roser and Treasure, Janet and Claes, Laurence and Favaro, Angela and Jim{\'{e}}nez‐Murcia, Susana and Karwautz, Andreas and Grange, Daniel Le and Tchanturia, Kate and Fern{\'{a}}ndez‐Aranda, Fernando},
doi = {10.1002/erv.2782},
issn = {1072-4133},
journal = {European Eating Disorders Review},
month = {sep},
number = {5},
pages = {483--491},
publisher = {John Wiley and Sons Ltd},
title = {{Null hypothesis significance tests, a misleading approach to scientific knowledge: Some implications for eating disorders research}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/erv.2782},
volume = {28},
year = {2020}
}
@article{Scheffe1942,
author = {Scheffe, Henry},
doi = {10.2307/2302663},
issn = {00029890},
journal = {The American Mathematical Monthly},
month = {feb},
number = {2},
pages = {99},
title = {{An Inverse Problem in Correlation Theory}},
url = {http://www.jstor.org/stable/2302663?origin=crossref},
volume = {49},
year = {1942}
}
@article{Rocke1984,
author = {Rocke, D. M.},
journal = {Biometrics},
pages = {225--230},
title = {{On testing for bioequivalence}},
volume = {40},
year = {1984}
}
@book{Fruhwirth-Schnatter2006,
abstract = {"The prominence of finite mixture modelling is greater than ever. Many important statistical topics like clustering data, outlier treatment, or dealing with unobserved heterogeneity involve finite mixture models in some way or other. The area of potential applications goes beyond simple data analysis and extends to regression analysis and to non-linear time series analysis using Markov switching models. For more than the hundred years since Karl Pearson showed in 1894 how to estimate the five parameters of a mixture of two normal distributions using the method of moments, statistical inference for finite mixture models has been a challenge to everybody who deals with them. In the past ten years, very powerful computational tools emerged for dealing with these models which combine a Bayesian approach with recent Monte simulation techniques based on Markov chains. This book reviews these techniques and covers the most recent advances in the field, among them bridge sampling techniques and reversible jump Markov chain Monte Carlo methods. It is the first time that the Bayesian perspective of finite mixture modelling is systematically presented in book form. It is argued that the Bayesian approach provides much insight in this context and is easily implemented in practice. Although the main focus is on Bayesian inference, the author reviews several frequentist techniques, especially selecting the number of components of a finite mixture model, and discusses some of their shortcomings compared to the Bayesian approach. The aim of this book is to impart the finite mixture and Markov switching approach to statistical modelling to a wide-ranging community. This includes not only statisticians, but also biologists, economists, engineers, financial agents, market researcher, medical researchers or any other frequent user of statistical models. This book should help newcomers to the field to understand how finite mixture and Markov switching models are formulated, what structures they imply on the data, what they could be used for, and how they are estimated. Researchers familiar with the subject also will profit from reading this book. The presentation is rather informal without abandoning mathematical correctness. Previous notions of Bayesian inference and Monte Carlo simulation are useful but not needed. Sylvia Fruhwirth-Schnatter is Professor of Applied Statistics and Econometrics at the Department of Applied Statistics of the Johannes Kepler University in Linz, Austria. She received her Ph. D. in mathematics from the University of Technology in Vienna in 1988. She has published in many leading journals in applied statistics and econometrics on topics such as Bayesian inference, finite mixture models, Markov switching models, state space models, and their application in marketing, economics and finance."--Publisher's website. 1. Finite mixture modeling -- 2. Statistical inference for a finite mixture model with known number of components -- 3. Practical Bayesian inference for a finite mixture model with known number of components -- 4. Statistical inference for finite mixture models under model specification uncertainty -- 5. Computational tools for Bayesian inference for finite mixtures models under model specification uncertainty -- 6. Finite mixture models with normal components -- 7. Data analysis based on finite mixtures -- 8. Finite mixtures of regression models -- 9. Finite mixture models with nonnormal components -- 10. Finite Markov mixture modeling -- 11. Statistical inference for Markov switching models -- 12. Nonlinear time series analysis based on Markov switching models -- 13. Switching state space models.},
address = {New York},
author = {Fr{\"{u}}hwirth-Schnatter, Sylvia},
isbn = {9781441921949},
pages = {492},
publisher = {Springer},
title = {{Finite mixture and Markov switching models}},
year = {2006}
}
@inproceedings{jonsson_using_2015,
abstract = {In this paper we report on an experiment conducted in an attempt to improve further the learning environment in a basic campus course on Object-Oriented Programming and Design given to first-year engineering students studying Computer Science and Engineering. This course has for years had the same traditional set-up that is common also in other engineering schools around the world including lectures, mandatory programming assignments, and a final written exam. What we did was to substitute the lectures for in-class sessions based on a variant of the teaching method known as “flipping the classroom” combined with certain elements of Peer Discussion and Just-in-time teaching. To make all this work, we introduced a web-based MOOC tool into the course. To be able to investigate the quantitative effects of our experiment, we had an experiment class consisting of 70 students taking the course in this new way and a control class of 57 students taking the course in the traditional way. On the final written exam, which was identical for the two classes and marked the same way, 81{\%} of the students in the experiment class passed compared to 60{\%} in the control class. Moreover, the share of students passing with good grades was 58{\%} in the experiment class compared to 32{\%} in the control class. So, not only did the share of students passing the course increase by a third, but also the share of students passing with good grades almost doubled.},
author = {Jonsson, H},
booktitle = {2015 {\{}IEEE{\}} {\{}Frontiers{\}} in {\{}Education{\}} {\{}Conference{\}} ({\{}FIE{\}})},
doi = {10.1109/FIE.2015.7344221},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Jonsson - 2015 - Using flipped classroom, peer discussion, and just-in-time teaching to increase learning in a programming course.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Jonsson - 2015 - Using flipped classroom, peer discussion, and just-in-time teaching to increase learning in a programming course.html:html},
keywords = {Computer aided instruction,Computer science education,Computers,Internet,Programming profession,Videos,Web-based MOOC tool,active learning,basic campus course,blended learning,computer science,control class,education,educational courses,engineering education,first-year engineering students,flipped classroom,just-in-time teaching,learning environment,mandatory programming assignments,object oriented programming,object-oriented design,object-oriented programming,peer discussion,programming,programming course,teaching},
month = {oct},
pages = {1--9},
title = {{Using flipped classroom, peer discussion, and just-in-time teaching to increase learning in a programming course}},
year = {2015}
}
@book{Glymour1980,
address = {Princeton},
author = {Glymour, C.},
publisher = {Princeton University Press},
title = {{Theory of Evidence}},
year = {1980}
}
@article{Dawid1970,
abstract = {1. Introduction. Suppose that we wish to make inferences about an unknown real parameter $\theta$, and that we have a random sample of observations (Xl, X2, {\ldots}, Xn) from a distribution which depends on $\theta$.},
author = {Dawid, A. P.},
doi = {10.1017/S0305004100045953},
issn = {14698064},
journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
number = {3},
pages = {625--633},
publisher = {Cambridge University Press},
title = {{On the limiting normality of posterior distributions}},
url = {https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/abs/on-the-limiting-normality-of-posterior-distributions/6AF5E26887F51F2AB837D5F9633A8B41},
volume = {67},
year = {1970}
}
@article{Schervish1992,
author = {Schervish, Mark J. and Carlin, Bradley P.},
doi = {10.1080/10618600.1992.10477008},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
month = {jun},
number = {2},
pages = {111--127},
title = {{On the Convergence of Successive Substitution Sampling}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10618600.1992.10477008},
volume = {1},
year = {1992}
}
@article{Ishwaran1999,
author = {Ishwaran, Hemant},
doi = {10.1080/10618600.1999.10474849},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
month = {dec},
number = {4},
pages = {779--799},
title = {{Applications of Hybrid Monte Carlo to Bayesian Generalized Linear Models: Quasicomplete Separation and Neural Networks}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10618600.1999.10474849},
volume = {8},
year = {1999}
}
@article{Mahl2012a,
abstract = {Markov models are mathematical models that can be used to describe disease progression and evaluate the cost–effectiveness of medical interventions. Markov models allow projecting clinical and economic outcomes into the future and are therefore frequently used to estimate long-term outcomes of medical interventions. The purpose of this paper is to demonstrate its use in dentistry, using the example of resin-bonded bridges to replace missing teeth, and to review the literature. We used literature data and a four-state Markov model to project long-term outcomes of resin-bonded bridges over a time horizon of 60 years. In addition, the literature was searched in PubMed Medline for research articles on the application of Markov models in dentistry.},
author = {Mahl, Dominik and Marinello, Carlo P and Sendi, Pedram},
doi = {10.1586/erp.12.47},
issn = {1473-7167},
journal = {Expert Review of Pharmacoeconomics {\&} Outcomes Research},
keywords = {Markov models,cost–effectiveness analysis,dentistry,prosthodontics,resin-bonded bridges},
month = {oct},
number = {5},
pages = {623--629},
publisher = {Taylor {\&} Francis},
title = {{Markov models in dentistry: application to resin-bonded bridges and review of the literature}},
url = {http://www.tandfonline.com/doi/full/10.1586/erp.12.47},
volume = {12},
year = {2012}
}
@book{Humbert2006,
address = {Wiesbaden},
annote = {Literaturverzeichnis: Seite [247] - 271},
author = {Humbert, Ludger},
booktitle = {Leitf{\"{a}}den der Informatik},
edition = {2., {\"{u}}berar},
isbn = {978-3-8351-0112-8},
keywords = {Informatikunterricht},
language = {ger},
pages = {284},
publisher = {Teubner},
shorttitle = {Didaktik der Informatik},
title = {{Didaktik der Informatik: mit praxiserprobtem Unterrichtsmaterial}},
year = {2006}
}
@article{Sawilowsky2003,
abstract = {The main purpose of this article is to contest the propositions that (1) hypothesis tests should be abandoned in favor of confidence intervals, and (2) science has not benefited from hypothesis testing. The minor purpose is to propose (1) descriptive statistics, graphics, and effect sizes do not obviate the need for hypothesis testing, (2) significance testing (reporting p values and leaving it to the reader to determine significance) is subjective and outside the realm of the scientific method, and (3) Bayesian and qualitative methods should be used for Bayesian and qualitative research studies, respectively. Copyright {\textcopyright} 2003 JMASM, Inc.},
author = {Sawilowsky, S.},
doi = {10.22237/jmasm/1067645940},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Sawilowsky - 2003 - Deconstructing arguments from the case against hypothesis testing.pdf:pdf},
issn = {15389472},
journal = {Journal of Modern Applied Statistical Methods},
keywords = {Bayes,Bracketed intervals,Effect size,Hypothesis testing,Qualitative,Significance testing},
month = {nov},
number = {2},
pages = {467--474},
publisher = {Wayne State University},
title = {{Deconstructing arguments from the case against hypothesis testing}},
volume = {2},
year = {2003}
}
@article{FraserEvansMonette1986Discussion,
author = {Kalbfleisch, John D. and Berger, James and Dawid, A.P. and Sprott, D.A.},
journal = {The Canadian Journal of Statistics},
number = {3},
pages = {194--199},
title = {{On Principles and Arguments to Likelihood: Discussion}},
volume = {14},
year = {1986}
}
@article{Chartier2008,
author = {Chartier, Sylvain and Faulkner, Andrew},
doi = {10.20982/tqmp.04.2.p065},
issn = {1913-4126},
journal = {Tutorials in Quantitative Methods for Psychology},
month = {sep},
number = {2},
pages = {65--78},
title = {{General Linear Models: An Integrated Approach to Statistics}},
url = {http://www.tqmp.org/RegularArticles/vol04-2/p065},
volume = {4},
year = {2008}
}
@article{Gaver1987,
author = {Gaver, D.P. and O'Muircheartaigh, I.G.},
journal = {Technometrics},
keywords = {empirical bayes,poisson process,reliability,robustness},
number = {1},
pages = {1--15},
title = {{Robust Empirical Bayes Analyses of Event Rates}},
volume = {29},
year = {1987}
}
@book{hesse_diagnostik_2011,
abstract = {Wie k{\"{o}}nnen Lehrkr{\"{a}}fte Lernvoraussetzungen und Lernerfolge von Sch{\"{u}}lerinnen und Sch{\"{u}}lern explizit diagnostizieren? Wie erkennen sie Hochbegabung und Lernschwierigkeiten? Theoretische Grundlagen der p{\"{a}}dagogisch-psychologischen Diagnostik werden erkl{\"{a}}rt, schulpraxisbezogene Diagnoseanl{\"{a}}sse bearbeitet. Es geht vor allem darum, ein Denkger{\"{u}}st bei Lehrkr{\"{a}}ften zu entwickeln, um so ihre diagnostische T{\"{a}}tigkeit zu optimieren. Der Band eignet sich f{\"{u}}r die Lehreraus- und Weiterbildung, als Seminarlekt{\"{u}}re und zum Selbststudium},
address = {Opladen Farmington Hills},
annote = {Literaturverzeichnis: Seite 305-319
OCLC: 846471997},
author = {Hesse, Ingrid and Latzko, Brigitte},
edition = {2. Auflage},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hesse, Latzko - 2011 - Diagnostik f{\"{u}}r Lehrkr{\"{a}}fte.pdf:pdf},
isbn = {978-3-8252-3088-3},
keywords = {Begabungsdiagnostik,Educational psychology,Educational tests and measurements,Grundschule,Lehrbuch,Lehrer,Lehrerbildung,P{\"{a}}dagogische Diagnostik,Sch{\"{u}}lerbeurteilung},
number = {3088},
publisher = {Verlag Barbara Budrich},
series = {{\{}UTB{\}} {\{}P{\"{a}}dagogik{\}}},
title = {{Diagnostik f{\"{u}}r Lehrkr{\"{a}}fte}},
year = {2011}
}
@inproceedings{eckerdal_novice_2005,
abstract = {Problems with understanding concepts, so called misconceptions, have been investigated and reported in a number of studies regarding object-oriented programming [4], [3]. In a first programming course using an object-oriented language, it is of great importance that students get a good understanding of central concepts like object and class at an early stage of their education. We have, with a phenomenographic research approach, performed a study with first year university students, investigating what an understanding of the concepts object and class includes from a student perspective. By applying variation theory [8] to our results we are able to pin-point what the students need to be able to discern in order to gain a "rich" understanding of these concepts.},
address = {New York, NY, USA},
author = {Eckerdal, Anna and Thun{\'{e}}, Michael},
booktitle = {Proceedings of the 10th Annual SIGCSE Conference on Innovation and Technology in Computer Science Education},
doi = {10.1145/1067445.1067473},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Eckerdal, Thun{\'{e}} - 2005 - Novice Java Programmers' Conceptions of Object and Class, and Variation Theory.pdf:pdf},
isbn = {978-1-59593-024-8},
keywords = {conceptions,misconceptions,phenomenography,variation theory},
pages = {89--93},
publisher = {ACM},
series = {{\{}ITiCSE{\}} '05},
title = {{Novice Java Programmers' Conceptions of "Object" and "Class", and Variation Theory}},
url = {http://doi.acm.org/10.1145/1067445.1067473},
year = {2005}
}
@article{pearson_criterion_1900,
author = {Pearson, Karl},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Pearson - 1900 - On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is.pdf:pdf},
journal = {Philosophical Magazine},
series = {Series 5. 50 (302)},
title = {{On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling}},
year = {1900}
}
@article{Dickey1970,
author = {Dickey, James M. and Lientz, B. P.},
doi = {10.1214/AOMS/1177697203},
issn = {0003-4851},
journal = {Annals of Mathematical Statistics},
number = {1},
pages = {214--226},
publisher = {Institute of Mathematical Statistics},
title = {{The Weighted Likelihood Ratio, Sharp Hypotheses about Chances, the Order of a Markov Chain}},
volume = {41},
year = {1970}
}
@article{USFoodAndDrugAdministrationClinicalTrialTypes,
author = {{U.S. Food and Drug Administration}},
journal = {Web archive: https://www.fda.gov/patients/clinical-trials-what-patients-need-know/what-are-different-types-clinical-research (accessed 01/03/2021)},
title = {{What Are the Different Types of Clinical Research?}},
url = {https://www.fda.gov/patients/clinical-trials-what-patients-need-know/what-are-different-types-clinical-research},
year = {2019}
}
@article{Hastings1970a,
author = {Hastings, W. K.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hastings - 1970 - Monte Carlo Sampling Methods Using Markov Chains and Their Applications(2).pdf:pdf},
journal = {Biometrika},
number = {1},
pages = {97--109},
title = {{Monte Carlo Sampling Methods Using Markov Chains and Their Applications}},
volume = {57},
year = {1970}
}
@book{Hastie2017,
abstract = {Second edition. 1. Introduction -- 2. Overview of supervised learning -- 3. Linear methods for regression -- 4. Linear methods for classification -- 5. Basis expansions and regularization -- 6. Kernel smoothing methods -- 7. Model assessment and selection -- 8. Model inference and averaging -- 9. Additive models, trees, and related methods -- 10. Boosting and additive trees -- 11. Neural networks -- 12. Support vector machines and flexible discriminants -- 13. Prototype methods and nearest-neighbors -- 14. Unsupervised learning -- 15. Random forests -- 16. Ensemble learning -- 17. Undirected graphical models -- 18. High-dimensional problems: p{\textgreater}{\textgreater} N.},
address = {New York},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H. (Jerome H.)},
doi = {10.1007/978-0-387-84858-7},
edition = {2nd},
isbn = {0387848576},
pages = {745},
publisher = {Springer-Verlag New York},
title = {{The Elements of Statistical Learning : Data mining, Inference, and Prediction}},
year = {2017}
}
@article{Fisher1937,
author = {Fisher, R.A.},
doi = {10.1111/j.1469-1809.1937.tb02154.x},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1937 - On a point raised by M. S. Bartlett on fiducial probability.pdf:pdf},
issn = {20501420},
journal = {Annals of Eugenics},
month = {jun},
number = {4},
pages = {370--375},
publisher = {Wiley/Blackwell (10.1111)},
title = {{On a point raised by M. S. Bartlett on fiducial probability}},
url = {http://doi.wiley.com/10.1111/j.1469-1809.1937.tb02154.x},
volume = {7},
year = {1937}
}
@article{Yates1964,
author = {Yates, F.},
doi = {10.2307/2528402},
isbn = {0006341X},
issn = {0006341X},
journal = {Biometrics},
month = {jun},
number = {2},
pages = {343},
title = {{Fiducial Probability, Recognisable Sub-Sets and Behrens}},
url = {https://www.jstor.org/stable/2528402?origin=crossref},
volume = {20},
year = {1964}
}
@article{Kadane1987,
author = {Kadane, Joseph B.},
doi = {10.1214/ss/1177013244},
issn = {08834237},
journal = {Statistical Science},
number = {3},
pages = {347--348},
publisher = {Institute of Mathematical Statistics},
title = {{[Testing precise hypotheses]: Comment}},
url = {https://projecteuclid.org/euclid.ss/1177013244},
volume = {2},
year = {1987}
}
@article{Deaton2018,
abstract = {Randomized Controlled Trials (RCTs) are increasingly popular in the social sciences, not only in medicine. We argue that the lay public, and sometimes researchers, put too much trust in RCTs over other methods of investigation. Contrary to frequent claims in the applied literature, randomization does not equalize everything other than the treatment in the treatment and control groups, it does not automatically deliver a precise estimate of the average treatment effect (ATE), and it does not relieve us of the need to think about (observed or unobserved) covariates. Finding out whether an estimate was generated by chance is more difficult than commonly believed. At best, an RCT yields an unbiased estimate, but this property is of limited practical value. Even then, estimates apply only to the sample selected for the trial, often no more than a convenience sample, and justification is required to extend the results to other groups, including any population to which the trial sample belongs, or to any individual, including an individual in the trial. Demanding ‘external validity' is unhelpful because it expects too much of an RCT while undervaluing its potential contribution. RCTs do indeed require minimal assumptions and can operate with little prior knowledge. This is an advantage when persuading distrustful audiences, but it is a disadvantage for cumulative scientific progress, where prior knowledge should be built upon, not discarded. RCTs can play a role in building scientific knowledge and useful predictions but they can only do so as part of a cumulative program, combining with other methods, including conceptual and theoretical development, to discover not ‘what works' but ‘why things work'.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.09326},
author = {Deaton, Angus and Cartwright, Nancy},
doi = {10.1016/j.socscimed.2017.12.005},
eprint = {arXiv:1603.09326},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Deaton, Cartwright - 2018 - Understanding and misunderstanding randomized controlled trials.pdf:pdf},
isbn = {1096-8644},
issn = {18735347},
journal = {Social Science and Medicine},
keywords = {Balance,Bias,Economic development,External validity,Health,Precision,RCTs,Transportation of results},
month = {aug},
pages = {2--21},
pmid = {20607699},
publisher = {Pergamon},
title = {{Understanding and misunderstanding randomized controlled trials}},
url = {https://www.sciencedirect.com/science/article/pii/S0277953617307359},
volume = {210},
year = {2018}
}
@article{Chandramouli2019,
abstract = {The three examples Gronau and Wagenmakers (Computational Brain and Behavior, 2018; hereafter denoted G{\&}W) use to demonstrate the limitations of Bayesian forms of leave-one-out cross validation (let us term this LOOCV) for model selection have several important properties: The true model instance is among the model classes being compared; the smaller, simpler model is a point hypothesis that in fact generates the data; the larger class contains the smaller. As G{\&}W admit, there is a good deal of prior history pointing to the limitations of cross validation and LOOCV when used in such situations (e.g., Bernardo and Smith 1994). We do not wish to rehash this literature trail, but rather give a conceptual overview of methodology that allows discussion of the ways that various methods of model selection align with scientific practice and scientific inference, and give our recommendation for the simplest approach that matches statistical inference to the needs of science. The methods include minimum description length (MDL) as reported by Gr{\"{u}}nwald (2007); Bayesian model selection (BMS) as reported by Kass and Raftery (Journal of the American Statistical Association, 90, 773-795, 1995); and LOOCV as reported by Browne (Journal of Mathematical Psychology, 44, 108-132, 2000) and Gelman et al. (Statistics and Computing, 24, 997-1016, 2014). In this commentary, we shall restrict the focus to forms of BMS and LOOCV. In addition, in these days of BBig Data,{\^{}} one wants inference procedures that will give reasonable answers as the amount of data grows large, one focus of the article by G{\&}W. We discuss how the various inference procedures fare when the data grow large. At the most conceptual level, our commentary is motivated by the desire to have statistics serve science not science serve statistics. To help make this happen, we propose Bayesian inference be carried out with the following components. A chief motivation for these proposals is the assumption that the Btrue{\^{}} model is never among and is always more complex than any instance in the models being considered: 1) Discretize all instance hypotheses and probabilities into sufficiently small intervals (multidimensional volumes). 2) Represent all model classes as combinations of instance intervals. 3) Approximate each instance interval by a point hypothesis within the interval. 4) Start Bayesian inference by finding the posterior probability of every one of these point hypotheses (see Gelman and Carlin 2017). 5) When desired and desirable, form a posterior probability for each class by summing the posterior probabilities for the instances in the class, and use those class posteriors to make class inferences. 6) Carry out model class comparisons for model classes that do not overlap. To make this proposal generally applicable , one must allow certain types of shared instances that are Bdistinguishable{\^{}} to be separated into distinct hypotheses. We start with terminology: a model instance is fully specified-it has all parameter values defined and predicts a fully specified distribution of outcomes for the experiment in question. A model class is a collection of model instances, often defined by a given parametric form (e.g., a seven degree polynomial). Different model classes sometimes overlap by sharing instances. This is commonly seen when model classes are nested hierarchically (e.g., a two degree polynomial is nested in a seven degree polynomial), in which case, all the instances in the smaller class are also in the larger class. It is of course possible for model classes to overlap in non-nested fashion; e.g., class 1 for probability of success, p, might specify p to lie in (0, .7) while class 2 might specify p to lie in (.4,},
author = {Chandramouli, Suyog H. and Shiffrin, Richard M.},
doi = {10.1007/s42113-018-0017-1},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Chandramouli, Shiffrin - 2019 - Commentary on Gronau and Wagenmakers.pdf:pdf},
issn = {2522-0861},
journal = {Computational Brain {\&} Behavior},
keywords = {Bayes Factor,Cross-Validation,Model Selection,Overlapping Model Classes},
month = {mar},
number = {1},
pages = {12--21},
publisher = {Springer Science and Business Media LLC},
title = {{Commentary on Gronau and Wagenmakers}},
url = {https://doi.org/10.1007/s42113-018-0017-1},
volume = {2},
year = {2019}
}
@article{Bickel1969,
abstract = {This paper deals with the asymptotic theory of Bayes solutions in (i) Estimation (ii) Testing when hypothesis and alternative are separated at least by an indifference region, under the assumption that the observations are independent and indentically distributed. The estimation results which are partial generalizations of results of LeCam begin with a proof of the convergence of the normalized posterior density to the appropriate normal density in a strong sense. From this result we derive the asymptotic efficiency of Bayes estimates obtained from smooth loss functions and in particular of the posterior mean. The last two theorems of this section deal with asymptotic expansions for the posterior risk in such estimation problems. The section on testing contains a limit theorem for the n-th root of the posterior risk under weak conditions on the prior and the loss function. Finally we discuss generalizations and some open problems. {\textcopyright} 1969 Springer-Verlag.},
author = {Bickel, P. J. and Yahav, J. A.},
doi = {10.1007/BF00531650},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bickel, Yahav - 1969 - Some contributions to the asymptotic theory of Bayes solutions.pdf:pdf},
issn = {00443719},
journal = {Zeitschrift f{\"{u}}r Wahrscheinlichkeitstheorie und Verwandte Gebiete},
keywords = {Economics,Finance,Insurance,Management,Mathematical and Computational Biology,Mathematical and Computational Physics,Operations Research/Decision Theory,Probability Theory and Stochastic Processes,Quantitative Finance,Statistics for Business,Theoretical},
month = {dec},
number = {4},
pages = {257--276},
publisher = {Springer-Verlag},
title = {{Some contributions to the asymptotic theory of Bayes solutions}},
url = {https://link.springer.com/article/10.1007/BF00531650},
volume = {11},
year = {1969}
}
@article{Bergh2019ANOVAJasp,
address = {Amsterdam},
author = {Bergh, Don Van Den and Doorn, Johnny Van and Marsman, Maarten and Gupta, Komarlu Narendra and Sarafoglou, Alexandra and Jan, G and Stefan, Angelika and Ly, Alexander and Hinne, Max},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bergh et al. - 2019 - A Tutorial on Conducting and Interpreting a Bayesian ANOVA in JASP.pdf:pdf},
institution = {University of Amsterdam},
journal = {psyarxiv preprint, https://psyarxiv.com/spreb},
title = {{A Tutorial on Conducting and Interpreting a Bayesian ANOVA in JASP}},
year = {2019}
}
@article{carter_classroom_2017,
author = {Carter, Jackie and Brown, Mark and Simpson, KATHRYN},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Carter, Brown, Simpson - 2017 - From the Classroom to the workplace how social science students are doing data analysis for real.pdf:pdf},
journal = {Statistics Education Research Journal},
number = {1},
pages = {80--101},
shorttitle = {From the {\{}Classroom{\}} to the workplace},
title = {{From the Classroom to the workplace: how social science students are doing data analysis for real}},
volume = {16},
year = {2017}
}
@article{RichardsonGreen1997,
author = {Richardson, Sylvia and Green, Peter},
journal = {Journal of the Royal Statistical Society Series B (Methodological)},
number = {4},
pages = {731--792},
title = {{On Bayesian Analysis of Mixtures with an Unknown Number of Components}},
volume = {59},
year = {1997}
}
@inproceedings{grover_measuring_2017,
abstract = {Programming in block-based environments is a key element of introductory computer science (CS) curricula in K-12 settings. Past research conducted in the context of text-based programming points to several challenges related to novice learners' understanding of foundational programming constructs such as variables, loops, and expressions. This research aims to develop assessment items for measuring student understanding in introductory CS classrooms in middle school using a principled approach for assessment design. This paper describes the design of assessments items that were piloted with 100 6th, 7th, 8th graders who had completed an introductory programming course using Scratch. The results and follow-up cognitive thinkalouds indicate that students are generally unfamiliar with the use of variables, and harbor misconceptions about them. They also have trouble with other aspects of introductory programming such as how loops work, and how the Boolean operators work. These findings point to the need for pedagogy that combines popular constructionist activities with those that target conceptual learning, along with better professional development to support teachers' conceptual learning of these foundational constructs.},
address = {New York, NY, USA},
author = {Grover, Shuchi and Basu, Satabdi},
booktitle = {Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education},
doi = {10.1145/3017680.3017723},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Grover, Basu - 2017 - Measuring Student Learning in Introductory Block-Based Programming Examining Misconceptions of Loops, Variables, a.pdf:pdf},
isbn = {978-1-4503-4698-6},
keywords = {K-12 CS education,assessments,block based programming,boolean logic,introductory programming,loops,middle school CS,novice misconceptions,variables},
pages = {267--272},
publisher = {ACM},
series = {{\{}SIGCSE{\}} '17},
shorttitle = {Measuring {\{}Student{\}} {\{}Learning{\}} in {\{}Introductory{\}} {\{}}},
title = {{Measuring Student Learning in Introductory Block-Based Programming: Examining Misconceptions of Loops, Variables, and Boolean Logic}},
url = {http://doi.acm.org/10.1145/3017680.3017723},
year = {2017}
}
@article{Bartholomew1967,
author = {Bartholomew, D.J.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bartholomew - 1967 - Hypothesis Testing When the Sample Size is Treated as a Random.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
number = {1},
pages = {53--82},
title = {{Hypothesis Testing When the Sample Size is Treated as a Random}},
volume = {29},
year = {1967}
}
@article{Freedman1983,
abstract = {We describe a new method of formulating stopping rules for clinical trials, one that incorporates opinion on what difference is clinically important. We compare the method with conventional group sequential designs and illustrate it by application to a study of Pancuronium Bromide for prevention of haemorrhage in pre-term infants.},
author = {Freedman, L. S. and Lowe, D. and Macaskill, P.},
doi = {10.1002/sim.4780020210},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Freedman, Lowe, Macaskill - 1983 - Stopping rules for clinical trials.pdf:pdf},
issn = {10970258},
journal = {Statistics in Medicine},
keywords = {Clinical trials,Group sequential designs,Intraventricular haemorrhage,Pre‐term infants,Stopping rules},
number = {2},
pages = {167--174},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Stopping rules for clinical trials}},
volume = {2},
year = {1983}
}
@article{Parolo2015,
abstract = {The exponential growth in the number of scientific papers makes it increasingly difficult for researchers to keep track of all the publications relevant to their work. Consequently, the attention that can be devoted to individual papers, measured by their citation counts, is bound to decay rapidly. In this work we make a thorough study of the life-cycle of papers in different disciplines. Typically, the citation rate of a paper increases up to a few years after its publication, reaches a peak and then decreases rapidly. This decay can be described by an exponential or a power law behavior, as in ultradiffusive processes, with exponential fitting better than power law for the majority of cases. The decay is also becoming faster over the years, signaling that nowadays papers are forgotten more quickly. However, when time is counted in terms of the number of published papers, the rate of decay of citations is fairly independent of the period considered. This indicates that the attention of scholars depends on the number of published items, and not on real time.},
archivePrefix = {arXiv},
arxivId = {1503.01881},
author = {Parolo, Pietro Della Briotta and Pan, Raj Kumar and Ghosh, Rumi and Huberman, Bernardo A. and Kaski, Kimmo and Fortunato, Santo},
doi = {10.1016/j.joi.2015.07.006},
eprint = {1503.01881},
isbn = {1751-1577},
issn = {18755879},
journal = {Journal of Informetrics},
keywords = {Citation count,Decay of attention,Time evolution},
month = {oct},
number = {4},
pages = {734--745},
publisher = {Elsevier},
title = {{Attention decay in science}},
url = {https://www.sciencedirect.com/science/article/pii/S1751157715200442?via{\%}3Dihub},
volume = {9},
year = {2015}
}
@article{Matthews2018,
author = {Matthews, Robert A J},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Matthews - 2018 - Beyond ‘ significance ' principles and practice of the Analysis of Credibility Subject Category Subject Areas.pdf:pdf},
title = {{Beyond ‘ significance ': principles and practice of the Analysis of Credibility Subject Category : Subject Areas :}},
year = {2018}
}
@article{Good1992,
abstract = {Various compromises that have occurred between Bayesian and non-Bayesian methods are reviewed. (A citation is provided that discusses the inevitability of compromises within the Bayesian approach.) One example deals with the masses of elementary particles, but no knowledge of physics will be assumed. {\textcopyright} 1992 Taylor {\&} Francis Group, LLC.},
author = {Good, I.J.},
doi = {10.1080/01621459.1992.10475256},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Animals as informal,Bayesians,Fine structure constant,Hierarchical Bayes,Inexactification,Maximum Likelihood,P values,Relativistic,Standardized,Type II},
number = {419},
pages = {597--606},
title = {{The Bayes/non-Bayes compromise: A brief review}},
volume = {87},
year = {1992}
}
@book{Hastie2015,
abstract = {"A Chapman {\&} Hall book." Discover New Methods for Dealing with High-Dimensional Data A sparse statistical model has only a small number of nonzero parameters or weights; therefore, it is much easier to estimate and interpret than a dense model. Statistical Learning with Sparsity: The Lasso and Generalizations presents methods that exploit sparsity to help recover the underlying signal in a set of data. Top experts in this rapidly evolving field, the authors describe the lasso for linear regression and a simple coordinate descent algorithm for its computation. They discuss the application of ℓ1 penalties to generalized linear models and support vector machines, cover generalized penalties such as the elastic net and group lasso, and review numerical methods for optimization. They also present statistical inference methods for fitted (lasso) models, including the bootstrap, Bayesian methods, and recently developed approaches. In addition, the book examines matrix decomposition, sparse multivariate analysis, graphical models, and compressed sensing. It concludes with a survey of theoretical results for the lasso. In this age of big data, the number of features measured on a person or object can be large and might be larger than the number of observations. This book shows how the sparsity assumption allows us to tackle these problems and extract useful and reproducible patterns from big datasets. Data analysts, computer scientists, and theorists will appreciate this thorough and up-to-date treatment of sparse statistical modeling. 1. Introduction -- 2. The lasso for linear models -- 3. Generalized linear models -- 4. Generalizations of the lasso penalty -- 5. Optimization methods -- 6. Statistical inference -- 7. Matrix decompositions, approximations, and completion -- 8. Sparse multivariate methods -- 9. Graphs and model selection -- 10. Signal approximation and compressed sensing -- 11. Theoretical results for the lasso.},
address = {New York},
author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
doi = {https://doi.org/10.1201/b18401},
edition = {1st},
isbn = {1498712169},
pages = {351},
publisher = {Chapman and Hall/CRC},
title = {{Statistical learning with Sparsity : the lasso and generalizations}},
year = {2015}
}
@article{Celeux2018a,
abstract = {This chapter surveys the most standard Monte Carlo methods available for simulating from a posterior distribution associated with a mixture and conducts some experiments about the robustness of the Gibbs sampler in high dimensional Gaussian settings. This is a chapter prepared for the forthcoming 'Handbook of Mixture Analysis'.},
archivePrefix = {arXiv},
arxivId = {1812.07240},
author = {Celeux, Gilles and Kamary, Kaniav and Malsiner-Walli, Gertraud and Marin, Jean-Michel and Robert, Christian P.},
eprint = {1812.07240},
title = {{Computational Solutions for Bayesian Inference in Mixture Models}},
url = {http://arxiv.org/abs/1812.07240},
year = {2018}
}
@article{Hansen1943,
author = {Hansen, Morris H. and Hurwitz, William N.},
doi = {10.1214/AOMS/1177731356},
issn = {0003-4851},
journal = {Annals of Mathematical Statistics},
number = {4},
pages = {333--362},
publisher = {Institute of Mathematical Statistics},
title = {{On the Theory of Sampling from Finite Populations}},
url = {https://projecteuclid.org/euclid.aoms/1177731356},
volume = {14},
year = {1943}
}
@article{Kelter2019Bayesanova,
author = {Kelter, Riko},
journal = {Comprehensive R Archive Network},
number = {https://cran.r-project.org/web/packages/bayesanova/index.html},
title = {{bayesanova - Bayesian Inference in the Analysis of Variance via Markov Chain Monte Carlo in Gaussian Mixture Models}},
url = {https://cran.r-project.org/web/packages/bayesanova/index.html},
year = {2019}
}
@article{schield_gaise_2017,
author = {Schield, Milo},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Schield - 2017 - Gaise 2016 promotes Statistical Literacy.pdf:pdf},
journal = {Statistics Education Research Journal},
number = {1},
title = {{Gaise 2016 promotes Statistical Literacy}},
volume = {17},
year = {2017}
}
@book{Fisher1958,
address = {New York},
author = {Fisher, R.A.},
edition = {12th},
publisher = {Hafner},
title = {{Statistical Methods for Research Workers}},
year = {1958}
}
@inproceedings{teif_partonomy_2006,
abstract = {The study presented in this paper explores junior high school students' comprehension of basic OO concepts. It summarizes major (mis)conceptions demonstrated by the students in two main categories: confusion of (1) taxonomic and (2) partonomic hierarchies with classes, objects and their interrelations.},
address = {New York, NY, USA},
author = {Teif, Mariana and Hazzan, Orit},
booktitle = {Working Group Reports on ITiCSE on Innovation and Technology in Computer Science Education},
doi = {10.1145/1189215.1189170},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Teif, Hazzan - 2006 - Partonomy and Taxonomy in Object-oriented Thinking Junior High School Students' Perceptions of Object-oriented Bas.pdf:pdf},
isbn = {978-1-59593-603-5},
keywords = {junior high school students' perceptions,object-oriented programming,partonomy,taxonomy},
pages = {55--60},
publisher = {ACM},
series = {{\{}ITiCSE{\}}-{\{}WGR{\}} '06},
shorttitle = {Partonomy and {\{}Taxonomy{\}} in {\{}Object{\}}-oriented {\{}Thi}},
title = {{Partonomy and Taxonomy in Object-oriented Thinking: Junior High School Students' Perceptions of Object-oriented Basic Concepts}},
url = {http://doi.acm.org/10.1145/1189215.1189170},
year = {2006}
}
@article{gould_data_2017,
abstract = {Past definitions of statistical literacy should be updated in order to account for the greatly amplified role that data now play in our lives. Experience working with high-school students in an innovative data science curriculum has shown that teaching statistical literacy, augmented by data literacy, can begin early.},
author = {Gould, Robert},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gould - 2017 - Data Literacy is Statistical Literacy.pdf:pdf},
journal = {Statistics Education Research Journal},
keywords = {Data literacy,Data science,Statistical literacy},
number = {1},
pages = {22--25},
title = {{Data Literacy is Statistical Literacy}},
url = {https://iase-web.org/documents/SERJ/SERJ16(1){\_}Gould.pdf},
volume = {16},
year = {2017}
}
@article{Kucukelbir2015,
abstract = {Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult to automate. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI). The user only provides a Bayesian model and a dataset; nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We implement ADVI in Stan (code available now), a probabilistic programming framework. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.},
archivePrefix = {arXiv},
arxivId = {1506.03431},
author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
eprint = {1506.03431},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kucukelbir et al. - 2015 - Automatic Variational Inference in Stan.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
month = {jun},
pages = {568--576},
publisher = {Neural information processing systems foundation},
title = {{Automatic Variational Inference in Stan}},
url = {http://arxiv.org/abs/1506.03431},
volume = {2015-Janua},
year = {2015}
}
@article{Lindley1957,
author = {Lindley, D.V.},
journal = {Biometrika},
number = {1},
pages = {187--192},
title = {{A Statistical Paradox}},
volume = {44},
year = {1957}
}
@article{Robinson2019,
abstract = {There is a crisis in the foundations of statistical inference. I believe that this crisis will eventually be resolved by regarding the subjective Bayesian paradigm as ideal in principle but often using standard procedures which are not subjective Bayesian for well-defined standard circumstances. As a step toward this resolution, this article looks at the question of what properties statistical inferences might reasonably be expected to have and argues that the use of p-values should be restricted to pure significance testing. The value judgments presented are supported by a range of examples.},
author = {Robinson, Geoffrey K.},
doi = {10.1080/00031305.2017.1415971},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Robinson - 2019 - What Properties Might Statistical Inferences Reasonably be Expected to Have—Crisis and Resolution in Statistical Inf.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Bayesian inference,Neyman–Pearson hypothesis testing,Objectivity,p-Value},
month = {jul},
number = {3},
pages = {243--252},
publisher = {American Statistical Association},
title = {{What Properties Might Statistical Inferences Reasonably be Expected to Have?—Crisis and Resolution in Statistical Inference}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1415971},
volume = {73},
year = {2019}
}
@book{OHaganForster2010,
address = {New York},
author = {O'Hagan, Anthony and Forster, Jonathan},
publisher = {Wiley Blackwell},
title = {{Kendall's Advanced Theory of Statistics 2B: Bayesian Inference}},
year = {2010}
}
@article{garfield_statistical_nodate,
author = {Garfield, Joan},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hovermill, Beaudrie, Boschmans - 2014 - Statistical Literacy Requirements for Teachers.pdf:pdf},
title = {{Statistical Literacy Requirements for Teachers}}
}
@article{Brooks1998,
author = {Brooks, Stephen P. and Roberts, Gareth O.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Brooks, Roberts - 1998 - Assessing Convergence of Markov Chain Monte Carlo Algorithms.pdf:pdf},
journal = {Statistics and Computing},
number = {8},
pages = {319--335},
title = {{Assessing Convergence of Markov Chain Monte Carlo Algorithms}},
year = {1998}
}
@inproceedings{Bandyopadhyay2009,
abstract = {Dental research gives rise to data with potentially complex correlation structure. Assessments of dental caries yield a binary outcome indicating the presence or absence of caries experience for each surface of each tooth in a subject's mouth. In addition to this nesting, caries outcome exhibit spatial structure among neighboring teeth. We develop a Bayesian multivariate model for spatial binary data using random effects autologistic regression that controls for the correlation within tooth surfaces and spatial correlation among neighboring teeth. Using a sample from a clinical study conducted at the Medical University of South Carolina, we compare this autologistic model with covariates to alternative models to demonstrate the improvement in predictions and also to assess the effects of covariates on caries experience.},
author = {Bandyopadhyay, Dipankar and Reich, Brian J and Slate, Elizabeth H},
booktitle = {Statistics in Medicine},
doi = {10.1002/sim.3647},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bandyopadhyay, Reich, Slate - 2009 - Bayesian modeling of multivariate spatial binary data with applications to dental caries.pdf:pdf},
isbn = {0277-6715},
issn = {02776715},
keywords = {Autologistic,Binary,Caries,MCMC,Spatial,Winbugs},
month = {dec},
number = {28},
pages = {3492--3508},
pmid = {19902498},
publisher = {NIH Public Access},
title = {{Bayesian modeling of multivariate spatial binary data with applications to dental caries}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19902498 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2796302},
volume = {28},
year = {2009}
}
@article{ragonis_long-term_2005,
author = {Ragonis, Noa and Ben-Ari, Mordechai},
doi = {10.1080/08993400500224310},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ragonis, Ben-Ari - 2005 - A long-term investigation of the comprehension of {\{}OOP{\}} concepts by novices.pdf:pdf},
issn = {0899-3408, 1744-5175},
journal = {Computer Science Education},
month = {sep},
number = {3},
pages = {201--221},
title = {{A Long-Term Investigation of the Comprehension of OOP Concepts by Novices}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08993400500224310},
volume = {15},
year = {2005}
}
@article{Dupuis2003,
abstract = {The variable selection method proposed in the paper is based on the evaluation of the Kullback-Leibler distance between the full (or encompassing) model and its submodels. The Bayesian implementation of the method does not require a separate prior modeling on the submodels since the corresponding parameters for the submodels are defined as the Kullback-Leibler projections of the full model parameters. The result of the selection procedure is the submodel with the smallest number of covariates which is at an acceptable distance of the full model. We introduce the notion of explanatory power of a model and scale the maximal acceptable distance in terms of the explanatory power of the full model. Moreover, an additivity property between embedded submodels shows that our selection procedure is equivalent to select the submodel with the smallest number of covariates which has a sufficient explanatory power. We illustrate the performances of this method on a breast cancer dataset. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
author = {Dupuis, J{\'{e}}rome A. and Robert, Christian P.},
doi = {10.1016/S0378-3758(02)00286-0},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Additivity property,Entropy,Kullback-Leibler distance,Logit model,Transitivity},
month = {feb},
number = {1-2},
pages = {77--94},
publisher = {Elsevier},
title = {{Variable selection in qualitative models via an entropic explanatory power}},
volume = {111},
year = {2003}
}
@article{good_1979,
author = {Good, I.J.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Good - 1979 - Studies in the History of Probability and Statistics XXXVII. Turing's Statistical Work in World War II.pdf:pdf},
journal = {Biometrika},
pages = {393--396},
title = {{Studies in the History of Probability and Statistics: XXXVII. Turing's Statistical Work in World War II}},
volume = {66},
year = {1979}
}
@article{Lavine1992,
author = {Lavine, Michael},
journal = {The Annals of Statistics},
pages = {1222--1235},
title = {{Some aspects of Polya tree distributions for statistical modelling}},
volume = {20},
year = {1992}
}
@misc{Donaldson1966,
abstract = {An empirical investigation of the power of the F-test when the underlying distribution is nonnormal and the within-cell variances are not homogeneous. It is shown that even for small samples the analysis-of-variance F-test is valid, although the usual assumptions of normal distribution and homogeneous error variances are seriously violated. The test is shown to be valid for both Type I and Type II error. An analysis of the correlation between the numerator and denominator of the null hypothesis F-ratio demonstrates that the distribution of F is little affected by the shape of the underlying distribution.},
author = {Donaldson, T. S.},
booktitle = {The RAND Corporation Research Memoranda},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Donaldson - 1966 - Power of the F-test for nonnormal distributions and unequal error variances.pdf:pdf},
pages = {56},
title = {{Power of the F-test for nonnormal distributions and unequal error variances}},
volume = {Memorandum},
year = {1966}
}
@inproceedings{Verma1990,
address = {Cambridge, MA},
author = {Verma, Thomas and Pearl, Judea},
booktitle = {Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence},
editor = {Bonissone, P. and Henrion, M. and Kanal, L.N. and Lemmer, J.F.},
pages = {255--270},
publisher = {Elsevier Science Publishers, B.V.},
title = {{Equivalence and synthesis of causal models}},
year = {1990}
}
@article{Fisher1933,
abstract = {Not Available},
author = {Fisher, R. A.},
doi = {10.1098/rspa.1933.0021},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1933 - The Concepts of Inverse Probability and Fiducial Probability Referring to Unknown Parameters.pdf:pdf},
issn = {1364-5021},
journal = {Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character},
month = {feb},
number = {838},
pages = {343--348},
publisher = {The Royal Society},
title = {{The Concepts of Inverse Probability and Fiducial Probability Referring to Unknown Parameters}},
url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1933.0021 http://www.jstor.org/stable/93984},
volume = {139},
year = {1933}
}
@article{Buehler1959,
author = {Buehler, Robert J.},
journal = {The Annals of Mathematical Statistics},
number = {4},
pages = {845--863},
title = {{Some Validity Criteria for Statistical Inferences}},
volume = {30},
year = {1959}
}
@article{Zeedyk2005,
author = {Zeedyk, M. Suzanne},
doi = {10.2304/plat.2005.5.2.97},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Zeedyk - 2005 - Detective Work on Statistics Street teaching statistics through humorous analogy.pdf:pdf},
issn = {1475-7257},
journal = {Psychology Learning {\&} Teaching},
number = {2},
pages = {97},
title = {{Detective Work on Statistics Street: teaching statistics through humorous analogy}},
volume = {5},
year = {2005}
}
@article{Society2017,
author = {Gilks, W. R. and And, N. G. Best and Tan, K. K. C.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gilks, And, Tan - 1995 - Adaptive Rejection Metropolis Sampling within Gibbs Sampling.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
keywords = {accelerated life tests,age-specific failure rate,asymptotic theory,censored data,conditional inference,hazard function,life table,limit estimate,medical applications,product,regression,reliability,theory,two-sample rank tests},
number = {4},
pages = {455--472},
title = {{Adaptive Rejection Metropolis Sampling within Gibbs Sampling}},
volume = {44},
year = {1995}
}
@article{Kelter2020,
author = {Kelter, Riko},
doi = {10.1080/15366367.2019.1689761},
journal = {Measurement: Interdisciplinary Research and Perspectives},
number = {2},
pages = {101--119},
title = {{Bayesian survival analysis in STAN for improved measuring of uncertainty in parameter estimates}},
volume = {18},
year = {2020}
}
@article{Greenland2016a,
abstract = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so-and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
doi = {10.1007/s10654-016-0149-3},
eprint = {1011.1669},
isbn = {1573-7284 (Electronic)$\backslash$r0393-2990 (Linking)},
issn = {15737284},
journal = {European Journal of Epidemiology},
keywords = {Confidence intervals,Hypothesis testing,Null testing,P value,Power,Significance tests,Statistical testing},
number = {4},
pages = {337--350},
pmid = {27209009},
title = {{Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations}},
volume = {31},
year = {2016}
}
@article{Morrison2002,
abstract = {The debate between the Mendelians and the (largely Darwinian) biometricians has been referred to by R. A. Fisher as ‘one of the most needless controversies in the history of science' and by David Hull as ‘an explicable embarrassment'. The literature on this topic consists mainly of explaining why the controversy occurred and what factors prevented it from being resolved. Regrettably, little or no mention is made of the issues that figured in its resolution. This paper deals with the latter topic and in doing so reorients the focus of the debate as one between Karl Pearson and R. A. Fisher rather than between the biometricians and the Mendelians. One reason for this reorientation is that Pearson's own work in 1904 and 1909 suggested that Mendelism and biometry could, to some extent, be made compatible, yet he remained steadfast in his rejection of Mendelism. The interesting question then is why Fisher, who was also a proponent of biometric methods, was able to synthesise the two traditions in a way that Pearson either could not or would not. My answer to this question involves an analysis of the ways in which different kinds of assumptions were used in modelling Mendelian populations. I argue that it is these assumptions, which lay behind the statistical techniques of Pearson and Fisher, that can be isolated as the source of Pearson's rejection of Mendelism and Fisher's success in the synthesis.},
author = {Morrison, Margaret},
doi = {10.1093/bjps/53.1.39},
isbn = {0007-0882},
issn = {00070882},
journal = {British Journal for the Philosophy of Science},
number = {1},
pages = {39--68},
publisher = {Oxford University PressThe British Society for the Philosophy of Science},
title = {{Modelling populations: Pearson and Fisher on Mendelism and biometry}},
url = {https://www.jstor.org/stable/3541640},
volume = {53},
year = {2002}
}
@article{Metropolis1949,
author = {Metropolis, Nicholas and Ulam, S.},
doi = {10.2307/2280232},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Metropolis, Ulam - 1949 - The Monte Carlo Method.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
month = {sep},
number = {247},
pages = {335},
title = {{The Monte Carlo Method}},
url = {https://www.jstor.org/stable/2280232?origin=crossref},
volume = {44},
year = {1949}
}
@book{Borel1962,
address = {Mineola, New York},
author = {Borel, E.},
publisher = {Dover Publications},
title = {{Probabilities and life}},
year = {1962}
}
@book{petersen_2017,
address = {Wiesbaden},
author = {Petersen, Christian},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Petersen - 2017 - Naturwissenschaften im Fokus I - Geschichtliche Entwicklung, Grundbegriffe, Mathematik.pdf:pdf},
isbn = {9783658152970},
publisher = {Springer Fachmedien Wiesbaden GmbH 2017},
title = {{Naturwissenschaften im Fokus I - Geschichtliche Entwicklung, Grundbegriffe, Mathematik}},
year = {2017}
}
@inproceedings{kramer_way_2016,
abstract = {Modeling and assessing competencies in computer science education is still an up to date topic. Especially regarding object-oriented programming the research data leaves room for further investigations. Based on an existing competency structure model, a first version of a test has been developed, to assess students' abilities to recognize elements of object-oriented programming in given source code. The results lead into the direction that novices tend to search for keywords while more experienced individuals recognize patterns via code structure. Relying on these results, ideas for further item types are presented.},
address = {New York, NY, USA},
annote = {Germany},
author = {Kramer, Matthias and Tobinski, David A and Brinda, Torsten},
booktitle = {Proceedings of the 16th Koli Calling International Conference on Computing Education Research},
doi = {10.1145/2999541.2999544},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kramer, Tobinski, Brinda - 2016 - On the Way to a Test Instrument for Object-oriented Programming Competencies.pdf:pdf},
isbn = {978-1-4503-4770-9},
keywords = {assessment,classical test theory,competency modeling,item response theory,object-oriented programming},
pages = {145--149},
publisher = {ACM},
series = {Koli {\{}Calling{\}} '16},
title = {{On the Way to a Test Instrument for Object-oriented Programming Competencies}},
url = {http://doi.acm.org/10.1145/2999541.2999544},
year = {2016}
}
@book{Watanabe2009,
abstract = {This article introduces the book, “algebraic geometry and statistical learning theory. ” A parametric model in statistics or a learning machine in information science is called singular if it is not identifiable or if its Fisher information matrix is not positive definite. Although a lot of statistical models and learning machines are singular, their statistical prop- erties have been left unknown. In this book, an algebraic geometrical method is established on which we can construct new statistical theory for singular models. Four main formulas are proved. Firstly, we show that any log likelihood function can be represented by a common standard form, based on resolution of singularities. Secondly, the asymptotic behavior of the Bayes marginal likelihood is derived by the zeta function theory. Thirdly, the asymptotic expansions of Bayes generalization and training errors are proved, which enable us to make a widely applicable information criterion for singular models. And lastly, the symmetry of the generalization and training errors in the maximum a posteriori method is proved. In this book, algebraic geometry is explained for non-mathematicians, and the concrete, applicable, and nontrivial formulas are introduced. Also it is theoretically shown that, in sin- gular models, Bayes estimation is more appropriate than one point estimation, even asymp- totically},
address = {Cambridge},
author = {Watanabe, Sumio},
booktitle = {Algebraic Geometry and Statistical Learning Theory},
doi = {10.1017/cbo9780511800474},
publisher = {Cambridge University Press},
title = {{Algebraic Geometry and Statistical Learning Theory}},
year = {2009}
}
@article{Kruschke2018a,
author = {Kruschke, John K.},
doi = {10.1177/2515245918771304},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kruschke - 2018 - Rejecting or Accepting Parameter Values in Bayesian Estimation.pdf:pdf},
journal = {Advances in Methods and Practices in Psychological Science},
keywords = {1,16,17,18,bayes factor,bayesian,credible interval,equivalence testing,hypothesis testing,in everyday life and,in science,meta-analysis,open materials,people often gather,received 12,revision accepted 3},
pages = {270--280},
title = {{Rejecting or Accepting Parameter Values in Bayesian Estimation}},
volume = {1(2)},
year = {2018}
}
@article{Zimmerman2011,
abstract = {There is no formal and generally accepted procedure for choosing an appropriate significance test for sample data when the assumption of normality is doubtful. Various tests of normality that have been proposed over the years have been found to have limited usefulness, and sometimes a preliminary test makes the situation worse. The present paper investigates a specific and easily applied rule for choosing between a parametric and non-parametric test, the Student t test and the Wilcoxon-Mann-Whitney test, that does not require a preliminary significance test of normality. Simulations reveal that the rule, which can be applied to sample data automatically by computer software, protects the Type I error rate and increases power for various sample sizes, significance levels, and non-normal distribution shapes. Limitations of the procedure in the case of heterogeneity of variance are discussed.},
author = {Zimmerman, Donald W.},
doi = {10.1348/000711010X524739},
issn = {00071102},
journal = {British Journal of Mathematical and Statistical Psychology},
month = {nov},
number = {3},
pages = {388--409},
pmid = {21973093},
title = {{A simple and effective decision rule for choosing a significance test to protect against non-normality}},
volume = {64},
year = {2011}
}
@article{Makowski2019a,
author = {Makowski, Dominique and Ben-Shachar, Mattan and L{\"{u}}decke, Daniel},
doi = {10.21105/joss.01541},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Makowski, Ben-Shachar, L{\"{u}}decke - 2019 - bayestestR Describing Effects and their Uncertainty, Existence and Significance within the Baye.pdf:pdf},
journal = {Journal of Open Source Software},
number = {40},
pages = {1541},
title = {{bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework}},
volume = {4},
year = {2019}
}
@article{Hathaway1985,
author = {Hathaway, Richard J.},
doi = {10.1214/aos/1176349557},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hathaway - 1985 - A Constrained Formulation of Maximum-Likelihood Estimation for Normal Mixture Distributions.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {Consistency,maximum-likelihood estimation,normal mixture densities},
month = {jun},
number = {2},
pages = {795--800},
publisher = {Institute of Mathematical Statistics},
title = {{A Constrained Formulation of Maximum-Likelihood Estimation for Normal Mixture Distributions}},
url = {http://projecteuclid.org/euclid.aos/1176349557},
volume = {13},
year = {1985}
}
@article{Good1983,
author = {Good, I.J.},
journal = {Journal of Statistical Computation and Simulation},
number = {1},
pages = {69--71},
title = {{Antisurprise}},
volume = {17},
year = {1983}
}
@article{Marino2014,
abstract = {Descriptive, exploratory, and inferential statistics are necessary components of hypothesis-driven biomedical research. Despite the ubiquitous need for these tools, the emphasis on statistical methods in pharmacology has become dominated by inferential methods often chosen more by the availability of user-friendly software than by any understanding of the data set or the critical assumptions of the statistical tests. Such frank misuse of statistical methodology and the quest to reach the mystical $\alpha$ {\textless} 0.05 criteria has hampered research via the publication of incorrect analysis driven by rudimentary statistical training. Perhaps more critically, a poor understanding of statistical tools limits the conclusions that may be drawn from a study by divorcing the investigator from their own data. The net result is a decrease in quality and confidence in research findings, fueling recent controversies over the reproducibility of high profile findings and effects that appear to diminish over time. The recent development of "omics" approaches leading to the production of massive higher dimensional data sets has amplified these issues making it clear that new approaches are needed to appropriately and effectively mine this type of data. Unfortunately, statistical education in the field has not kept pace. This commentary provides a foundation for an intuitive understanding of statistics that fosters an exploratory approach and an appreciation for the assumptions of various statistical tests that hopefully will increase the correct use of statistics, the application of exploratory data analysis, and the use of statistical study design, with the goal of increasing reproducibility and confidence in the literature. {\textcopyright} 2013 Published by Elsevier Inc.},
author = {Marino, Michael J.},
doi = {10.1016/j.bcp.2013.05.017},
isbn = {2156520852},
issn = {00062952},
journal = {Biochemical Pharmacology},
keywords = {Exploratory data analysis,Non-parametric,Parametric,Power analysis,Statistical design},
month = {jan},
number = {1},
pages = {78--92},
pmid = {23747488},
title = {{The use and misuse of statistical methodologies in pharmacology research}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23747488 http://linkinghub.elsevier.com/retrieve/pii/S0006295213003432},
volume = {87},
year = {2014}
}
@article{Neyman1938,
author = {Neyman, Jerzy and Pearson, Egon Sharpe},
journal = {Statistical Research Memoirs},
number = {2},
pages = {25--57},
title = {{Contributions to the theory of testing statistical hypotheses, Parts II, III}},
year = {1938}
}
@article{FisherRonaldAylmerSir1960a,
abstract = {Reproduced with permission of the Journal of the Operations Research Society of Japan.},
author = {Fisher, R A},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1960 - Scientific thought and the refinement of human reasoning.pdf:pdf},
journal = {Journal of Op. Res. Soc. of Japan},
keywords = {Journal article},
pages = {1--10},
title = {{Scientific thought and the refinement of human reasoning}},
url = {https://digital.library.adelaide.edu.au/dspace/handle/2440/15278},
volume = {3},
year = {1960}
}
@article{dagostino_suggestion_1990,
abstract = {For testing that an underlying population is normally distributed the skewness and kurtosis statistics, $\backslash${\$}{\{}\backslashtextbackslash{\}}sqrt{\{}b{\_}1{\}}{\$} and b2, and the D'Agostino-Pearson K2 statistic that combines these two statistics have been shown to be powerful and informative tests. Their use, however, has not been as prevalent as their usefulness. We review these tests and show how readily available and popular statistical software can be used to implement them. Their relationship to deviations from linearity in normal probability plotting is also presented.},
author = {D'Agostino, Ralph B and Belanger, Albert},
doi = {10.2307/2684359},
issn = {0003-1305},
journal = {The American Statistician},
number = {4},
pages = {316--321},
title = {{A Suggestion for Using Powerful and Informative Tests of Normality}},
url = {http://www.jstor.org/stable/2684359},
volume = {44},
year = {1990}
}
@article{Horton2009,
author = {Horton, Richard},
doi = {10.1016/S0140-6736(09)61309-X},
issn = {01406736},
journal = {The Lancet},
number = {9685},
pages = {176},
pmid = {19616703},
publisher = {Elsevier},
title = {{Phase 0 trials: a platform for drug development?}},
volume = {374},
year = {2009}
}
@book{miles_applying_2000,
abstract = {This book takes a fresh look at applying regression analysis in the behavioural sciences by introducing the reader to regression analysis through a simple model-building approach. The authors start with the basics and begin by re-visiting the mean, and the standard deviation, with which most readers will already be familiar, and show that they can be thought of a least squares model. The book then shows that this least squares model is actually a special case of a regression analysis and can be extended to deal with first one, and then more than one independent variable. Extending the model from the mean to a regression analysis provides a powerful, but simple, way of thinking about what students believe are the more complex aspects of regression analysis. The authors gradually extend the model to include aspects of regression analysis such as non-linear regression, logistic regression, and moderator and mediator analysis. These approaches are often presented in terms that are too mathematical for non-statistically inclined students to deal with. Throughout the book maintains a conceptual, non-mathematical focus. Most equations are placed in an appendix, where a detailed explanation is given, to avoid disrupting the flow of the main text. This book will be indispensable for anyone using regression and correlation from undergraduates doing projects to postgraduate and researchers.},
address = {London ; Thousand Oaks, Calif},
author = {Miles, Jeremy and Shevlin, Mark},
edition = {1 edition},
isbn = {978-0-7619-6230-4},
month = {dec},
publisher = {Sage Publications Ltd},
shorttitle = {Applying {\{}Regression{\}} and {\{}Correlation{\}}},
title = {{Applying Regression and Correlation: A Guide for Students and Researchers}},
year = {2000}
}
@book{Altman1991,
abstract = {1st ed. Primarily aimed at medical researchers, this book aims to provide an understanding of the basic principles that underlie research design, data analysis and the interpretation of results, and to enable the reader to carry out a wide range of statistical analyses.},
address = {Boca Raton},
author = {Altman, Douglas G.},
pages = {611},
publisher = {Chapman and Hall},
title = {{Practical statistics for medical research}},
year = {1991}
}
@article{Kelter2020fbstPackage,
author = {Kelter, Riko},
journal = {Comprehensive R Archive Network},
number = {https://cran.r-project.org/web/packages/fbst/index.html},
title = {{fbst: The Full Bayesian Significance Test and the e-Value}},
url = {https://cran.r-project.org/web/packages/fbst/index.html},
year = {2020}
}
@book{Pearl2016,
address = {Chichester, UK},
author = {Pearl, Judea and Glymour, Madelyn and Jewell, Nicholas P.},
pages = {156},
publisher = {Wiley},
title = {{Causal Inference in Statistics: A Primer}},
year = {2016}
}
@book{Carnap1962,
abstract = {Second edition.},
address = {Chicago},
author = {Carnap, Rudolf},
edition = {2nd},
isbn = {0226093433},
publisher = {University of Chicago Press},
title = {{Logical Foundations of Probability}},
year = {1962}
}
@book{brueckler_2018,
address = {Berlin},
author = {Br{\"{u}}ckler, Franka Miriam},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Br{\"{u}}ckler - 2018 - Geschichte der Mathematik kompakt - Das Wichtigste aus Analysis, Wahrscheinlichkeitstheorie, angewandter Mathematik,.pdf:pdf},
isbn = {9783662555736},
publisher = {Springer-Verlag GmbH Deutschland},
title = {{Geschichte der Mathematik kompakt - Das Wichtigste aus Analysis, Wahrscheinlichkeitstheorie, angewandter Mathematik, Topologie und Mengenlehre}},
year = {2018}
}
@article{Topolinski2012,
abstract = {The omnipresent abstract symbol for time progression and regression is clockwise versus counterclockwise rotation. It was tested whether merely executing and seeing clockwise (vs. counterclockwise)...},
author = {Topolinski, Sascha and Sparenberg, Peggy},
doi = {10.1177/1948550611419266},
issn = {1948-5506},
journal = {Social Psychological and Personality Science},
keywords = {embodiment,mere exposure,novelty,openness to experience,time},
number = {3},
pages = {308--314},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Turning the Hands of Time}},
url = {http://journals.sagepub.com/doi/10.1177/1948550611419266},
volume = {3},
year = {2012}
}
@article{Guo2016,
abstract = {This paper aims to review state-of-the-art Bayesian-inference-based methods applied to functional magnetic resonance imaging (fMRI) data. Particularly, we focus on one specific long-standing challenge in the computational modeling of fMRI datasets: how to effectively explore typical functional interactions from fMRI time series and the corresponding boundaries of temporal segments. Bayesian inference is a method of statistical inference which has been shown to be a powerful tool to encode dependence relationships among the variables with uncertainty. Here we provide an introduction to a group of Bayesian-inference-based methods for fMRI data analysis, which were designed to detect magnitude or functional connectivity change points and to infer their functional interaction patterns based on corresponding temporal boundaries. We also provide a comparison of three popular Bayesian models, that is, Bayesian Magnitude Change Point Model (BMCPM), Bayesian Connectivity Change Point Model (BCCPM), and Dynamic Bayesian Variable Partition Model (DBVPM), and give a summary of their applications. We envision that more delicate Bayesian inference models will be emerging and play increasingly important roles in modeling brain functions in the years to come.},
author = {Guo, Xuan and Liu, Bing and Chen, Le and Chen, Guantao and Pan, Yi and Zhang, Jing},
doi = {10.1155/2016/3279050},
issn = {17486718},
journal = {Computational and Mathematical Methods in Medicine},
month = {mar},
pages = {1--9},
pmid = {27034708},
publisher = {Hindawi},
title = {{Bayesian inference for functional dynamics exploring in fMRI data}},
url = {http://www.hindawi.com/journals/cmmm/2016/3279050/},
volume = {2016},
year = {2016}
}
@article{Goodman2019a,
author = {Goodman, Steven N.},
doi = {10.1080/00031305.2018.1558111},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Goodman - 2019 - Why is Getting Rid of iPi -Values So Hard Musings on Science and Statistics.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {inference,p -values,reproducible,research,scientific inference,statistical},
number = {sup1},
pages = {26--30},
title = {{Why is Getting Rid of {\textless}i{\textgreater}P{\textless}/i{\textgreater} -Values So Hard? Musings on Science and Statistics}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1558111},
volume = {73},
year = {2019}
}
@book{Schubert2011,
address = {Heidelberg},
annote = {Literaturverzeichnis: Seite [377] - 398},
author = {Schubert, Sigrid and Schwill, Andreas},
edition = {2. Auflage},
isbn = {978-3-8274-2652-9},
keywords = {Fachdidaktik,Informatikunterricht,Mathematics,computer science},
language = {ger},
pages = {417},
publisher = {Spektrum, Akademischer Verlag},
title = {{Didaktik der Informatik}},
year = {2011}
}
@article{VanRavenzwaaij2019,
abstract = {Background: In clinical trials, study designs may focus on assessment of superiority, equivalence, or non-inferiority, of a new medicine or treatment as compared to a control. Typically, evidence in each of these paradigms is quantified with a variant of the null hypothesis significance test. A null hypothesis is assumed (null effect, inferior by a specific amount, inferior by a specific amount and superior by a specific amount, for superiority, non-inferiority, and equivalence respectively), after which the probabilities of obtaining data more extreme than those observed under these null hypotheses are quantified by p-values. Although ubiquitous in clinical testing, the null hypothesis significance test can lead to a number of difficulties in interpretation of the results of the statistical evidence. Methods: We advocate quantifying evidence instead by means of Bayes factors and highlight how these can be calculated for different types of research design. Results: We illustrate Bayes factors in practice with reanalyses of data from existing published studies. Conclusions: Bayes factors for superiority, non-inferiority, and equivalence designs allow for explicit quantification of evidence in favor of the null hypothesis. They also allow for interim testing without the need to employ explicit corrections for multiple testing.},
author = {{Van Ravenzwaaij}, Don and Monden, Rei and Tendeiro, Jorge N. and Ioannidis, John P.A.},
doi = {10.1186/s12874-019-0699-7},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Van Ravenzwaaij et al. - 2019 - Bayes factors for superiority, non-inferiority, and equivalence designs.pdf:pdf},
issn = {14712288},
journal = {BMC Medical Research Methodology},
keywords = {Bayes factors,Clinical trials,Non-inferiority designs,Statistical inference},
number = {71},
pages = {1--12},
pmid = {30925900},
publisher = {BMC Medical Research Methodology},
title = {{Bayes factors for superiority, non-inferiority, and equivalence designs}},
volume = {19},
year = {2019}
}
@article{Gorard2016,
abstract = {This brief paper introduces a new approach to assessing the trustworthiness of research comparisons when expressed numerically. The ‘number needed to disturb' a research finding would be the number...},
author = {Gorard, Stephen and Gorard, Jonathan},
doi = {10.1080/13645579.2015.1091235},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gorard, Gorard - 2016 - What to do instead of significance testing Calculating the ‘number of counterfactual cases needed to disturb.pdf:pdf},
issn = {1364-5579},
journal = {International Journal of Social Research Methodology},
keywords = {Significance testing,attrition,evaluation,reliability,sampling},
month = {jul},
number = {4},
pages = {481--490},
publisher = {Routledge},
title = {{What to do instead of significance testing? Calculating the ‘number of counterfactual cases needed to disturb a finding'}},
url = {https://www.tandfonline.com/doi/full/10.1080/13645579.2015.1091235},
volume = {19},
year = {2016}
}
@misc{Stata2019,
address = {College Station},
author = {StataCorp.},
publisher = {TX: StataCorp LLC.},
title = {{Stata Statistical Software}},
year = {2019}
}
@article{Fisher1923,
abstract = {It is not infrequently assumed that varieties of cultivated plants differ not only in their suitability to different climatic and soil conditions, but in their response to different manures. Since the experimental error of field experiments is often underestimated, this supposition affords a means of explaining discrepancies between the results of manurial experiments conducted with different varieties; in the absence of experimental evidence adequate to prove or disprove the supposed differences between varieties in their response to manures such explanations cannot be definitely set aside, although we very often suspect that the discrepancies are in reality due to the normal errors of field experiments.},
author = {Fisher, R. A. and Mackenzie, W. A.},
doi = {10.1017/S0021859600003592},
isbn = {1469-5146},
issn = {14695146},
journal = {The Journal of Agricultural Science},
month = {jul},
number = {3},
pages = {311--320},
publisher = {Cambridge University Press},
title = {{Studies in crop variation: II. The manurial response of different potato varieties}},
url = {http://www.journals.cambridge.org/abstract{\_}S0021859600003592},
volume = {13},
year = {1923}
}
@article{Dienes2011,
abstract = {Researchers are often confused about what can be inferred from significance tests. One problem occurs when people apply Bayesian intuitions to significance testing—two approaches that must be firmly separated. This article presents some common situations in which the approaches come to different conclusions; you can see where your intuitions initially lie. The situations include multiple testing, deciding when to stop running participants, and when a theory was thought of relative to finding out results. The interpretation of nonsignificant results has also been persistently problematic in a way that Bayesian inference can clarify. The Bayesian and orthodox approaches are placed in the context of different notions of rationality, and I accuse myself and others as having been irrational in the way we have been using statistics on a key notion of rationality. The reader is shown how to apply Bayesian inference in practice, using free online software, to allow more coherent inferences from data.},
author = {Dienes, Zoltan},
doi = {10.1177/1745691611406920},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Dienes - 2011 - Bayesian Versus Orthodox Statistics Which Side Are You On.pdf:pdf},
isbn = {1745-6916},
issn = {1745-6916},
journal = {Perspectives on Psychological Science},
keywords = {bayes,evidence,from having a rigorous,have benefited enormously,likelihood principle,procedure for extracting inferences,psychology and other disciplines,significance testing,statistical inference},
number = {3},
pages = {274--290},
pmid = {26168518},
title = {{Bayesian Versus Orthodox Statistics: Which Side Are You On?}},
url = {http://journals.sagepub.com/doi/10.1177/1745691611406920},
volume = {6},
year = {2011}
}
@article{Arandjelovic2019,
abstract = {{\textless}p{\textgreater} The usefulness of the statistic known as the {\textless}italic{\textgreater}p{\textless}/italic{\textgreater} -value, as a means of quantifying the strength of evidence for the presence of an effect from empirical data has long been questioned in the statistical community. In recent years, there has been a notable increase in the awareness of both fundamental and practical limitations of the statistic within the target research fields and especially biomedicine. In this article, I analyse the recently published article (Colquhoun 2017 {\textless}italic{\textgreater}R. Soc. open sci.{\textless}/italic{\textgreater} {\textless}bold{\textgreater}4{\textless}/bold{\textgreater} , 171085 ( {\textless}ext-link ext-link-type="uri" href="http://dx.doi.org/10.1098/rsos.171085"{\textgreater}doi:10.1098/rsos.171085{\textless}/ext-link{\textgreater} )) which, in summary, argues that with a better understanding and thus more appropriate use of the statistic, many of the aforementioned limitations can be addressed. In particular, I demonstrate that the (often implicit) premises of this counterargument are questionable, in some cases arguably inconsistent, and that therefore the counterargument provides little if any justification for the continued use of the {\textless}italic{\textgreater}p{\textless}/italic{\textgreater} -value. Additionally, my analysis should help researchers seeking to interpret their empirical data by illustrating the nuanced nature and the multiplicity of statistical, methodological and epistemological issues which must be considered in this process. {\textless}/p{\textgreater}},
author = {Arandjelovi{\'{c}}, Ognjen},
doi = {10.1098/rsos.181519},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Arandjelovi{\'{c}} - 2019 - A more principled use of the ipi -value Not so fast a critique of Colquhoun's argument.pdf:pdf},
issn = {2054-5703},
journal = {Royal Society Open Science},
keywords = {Bayesian,Empirical,Evidence,Frequentist,Statistics},
month = {may},
number = {5},
pages = {181519},
publisher = {Royal Society Publishing},
title = {{A more principled use of the {\textless}i{\textgreater}p{\textless}/i{\textgreater} -value? Not so fast: a critique of Colquhoun's argument}},
url = {https://royalsocietypublishing.org/doi/10.1098/rsos.181519},
volume = {6},
year = {2019}
}
@article{Kirkpatrick1983,
author = {Kirkpatrick, S. and Gelatt, C.D. and Vecchi, M.P.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kirkpatrick, Gelatt, Vecchi - 1983 - Optimization by Simulated Annealing.pdf:pdf},
journal = {Science},
number = {4598},
pages = {671--680},
title = {{Optimization by Simulated Annealing}},
url = {http://links.jstor.org/sici?sici=0036-8075{\%}2819830513{\%}293{\%}3A220{\%}3A4598{\%}3C671{\%}3AOBSA{\%}3E2.0.CO{\%}3B2-8},
volume = {220},
year = {1983}
}
@inproceedings{gross_evaluating_2005,
abstract = {Educators have developed a myriad of tools to help novices learn to program. Unfortunately, too little is known about the educational impact of these environments, or even how to assess this impact. In this paper we summarize a representative collection of the assessments of novice programming environments, present a rubric for evaluating the quality of such assessments, and demonstrate the application of our rubric to the summarized works. The intent is that such an evaluative framework will help inform future efforts in assessing novice programming environments.},
address = {New York, NY, USA},
author = {Gross, Paul and Powers, Kris},
booktitle = {Proceedings of the First International Workshop on Computing Education Research},
doi = {10.1145/1089786.1089796},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gross, Powers - 2005 - Evaluating Assessments of Novice Programming Environments.pdf:pdf},
isbn = {978-1-59593-043-9},
keywords = {Evaluation,assessment,novice programmers,novice programming environments},
pages = {99--110},
publisher = {ACM},
series = {{\{}ICER{\}} '05},
title = {{Evaluating Assessments of Novice Programming Environments}},
url = {http://doi.acm.org/10.1145/1089786.1089796},
year = {2005}
}
@book{Achinstein2001,
address = {Oxford},
author = {Achinstein, P.},
publisher = {Oxford University Press},
title = {{The Book of Evidence}},
year = {2001}
}
@article{Rothman2016,
abstract = {In the marketplace of scientific results, the preferred currency by which results have been valued has been statistical signifi-cance, expressed either as a dichotomous label or by the under-lying p-value, which may be given as a number or an inequality. Like other modern currencies, the value of this one is not inher-ent but derived from widely held assumptions and expectations. Indeed, reliance on statistical significance is as misplaced as faith in some dubious paper monies. At the risk of stretching the analogy, I suggest that a version of Gresham's Law has been operating, allowing statistical significance to force out of circu-lation better ways to analyze data, and leaving us with results that are, all too often, astonishingly misleading. As the ASA statement (ASA 2016) indicates, a fundamen-tal flaw of relying on statistical significance for inference is the need to dichotomize all results into those that are significant or not significant. This practice degrades vast efforts to collect and analyze quantitative data into a mere label. Furthermore, if ever there were a false dichotomy, it is the dichotomy between sig-nificant and not. The label is assigned by an arbitrary rule and inevitably has less information than the p-value from which it derives. Moreover, the p-value itself is a handicapped approach to interpretation because it doesn't measure effect size. Instead, it blends together information on estimated effect size and the precision of that estimate (Lang, Rothman, and Cann 1998). Al-though p-values and confidence intervals are closely related, a confidence interval, in contrast to a p-value, expresses sepa-rately both effect size and precision (Poole 2001). This advan-tage of confidence intervals illustrates that it usually takes two numbers to measure two distinct characteristics. Unfortunately, all too often we have seen the reported confidence interval used merely to determine if the null value lies within it or not, debas-ing the confidence interval into a label, a surrogate significance test (Cumming 2012). The correspondence between results that are statistically significant and those that are truly important is far too low to be useful. Consequently, scientists have embraced and even avidly pursued meaningless differences solely because they are statis-tically significant, and have ignored important effects because they failed to pass the screen of statistical significance. These are pernicious problems, and not just in the metaphorical sense. It is a safe bet that people have suffered or died because scien-tists (and editors, regulators, journalists and others) have used significance tests to interpret results, and have consequently Online discussion of the ASA Statement on Statistical Significance and P-Values, The American Statistician, 70. Kenneth J. Rothman, RTI International– Health Solutions, 3040 East Cornwallis Road, Research Triangle Park, NC 27709-2194 (Email: krothman@rti.org). failed to identify the most beneficial courses of action (Hauer 2004; Schmidt and Rothman 2014). How do we fix this problem? The reliance on statistical sig-nificance testing is ingrained in the social system of many sci-ences, and therefore reflexive on the part of many members of those social systems, making it difficult to counter. Nonethe-less, we can and should advise today's students of statistics that they should avoid statistical significance testing, and embrace estimation instead. Those who have tried offering this advice know it can be challenging. Students all too often fear that their success will be measured by publications and grants that are evaluated by reviewers who esteem statistical significance. De-spite such inertia, in epidemiology there has been an encourag-ing trend toward reporting confidence intervals to supplement or even supplant statistical significance and p-values, toward using confidence intervals to measure effect size and to gauge precision rather than to test null hypotheses, and toward avoid-ing the fallacy of considering every statistically non-significant result as if it were evidence for a null relation. Real change will take the concerted effort of experts to en-lighten working scientists, journalists, editors and the public at large that statistical significance has been a harmful concept, and that the estimates of meaningful effect measures is a much more fruitful research aim than the testing of null hypotheses. This statement of the ASA does not go nearly far enough toward that end, but it is a welcome start and a hopeful sign.},
author = {Rothman, Kenneth J.},
doi = {10.1007/s10654-016-0158-2},
issn = {0393-2990},
journal = {European Journal of Epidemiology},
number = {5},
pages = {443--444},
publisher = {Springer Netherlands},
title = {{Disengaging from statistical significance}},
volume = {31},
year = {2016}
}
@article{Myung2003,
abstract = {In this paper, I provide a tutorial exposition on maximum likelihood estimation (MLE). The intended audience of this tutorial are researchers who practice mathematical modeling of cognition but are unfamiliar with the estimation method. Unlike least-squares estimation which is primarily a descriptive tool, MLE is a preferred method of parameter estimation in statistics and is an indispensable tool for many statistical modeling techniques, in particular in non-linear modeling with non-normal data. The purpose of this paper is to provide a good conceptual explanation of the method with illustrative examples so the reader can have a grasp of some of the basic principles. {\textcopyright} 2003 Elsevier Science (USA). All rights reserved.},
author = {Myung, In Jae},
doi = {10.1016/S0022-2496(02)00028-7},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
month = {feb},
number = {1},
pages = {90--100},
publisher = {Academic Press Inc.},
title = {{Tutorial on maximum likelihood estimation}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249602000287},
volume = {47},
year = {2003}
}
@misc{Ashby2006,
abstract = {This review examines the state of Bayesian thinking as Statistics in Medicine was launched in 1982, reflecting particularly on its applicability and uses in medical research. It then looks at each subsequent five-year epoch, with a focus on papers appearing in Statistics in Medicine, putting these in the context of major developments in Bayesian thinking and computation with reference to important books, landmark meetings and seminal papers. It charts the growth of Bayesian statistics as it is applied to medicine and makes predictions for the future. From sparse beginnings, where Bayesian statistics was barely mentioned, Bayesian statistics has now permeated all the major areas of medical statistics, including clinical trials, epidemiology, meta-analyses and evidence synthesis, spatial modelling, longitudinal modelling, survival modelling, molecular genetics and decision-making in respect of new technologies.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Ashby, Deborah},
booktitle = {Statistics in Medicine},
doi = {10.1002/sim.2672},
eprint = {NIHMS150003},
isbn = {2007090091480},
issn = {02776715},
keywords = {Bayesian,Clinical trials,Longitudinal modelling,Medical statistics,Review,Spatial modelling},
month = {nov},
number = {21},
pages = {3589--3631},
pmid = {16947924},
title = {{Bayesian statistics in medicine: A 25 year review}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16947924 http://doi.wiley.com/10.1002/sim.2672},
volume = {25},
year = {2006}
}
@book{McElreath2020,
abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds readers' knowledge of and confidence in statistical modeling. Reflecting the need for even minor programming in today's model-based statistics, the book pushes readers to perform step-by-step calculations that are usually automated. This unique computational approach ensures that readers understand enough of the details to make reasonable choices and interpretations in their own modeling work. The text presents generalized linear multilevel models from a Bayesian perspective, relying on a simple logical interpretation of Bayesian probability and maximum entropy. It covers from the basics of regression to multilevel models. The author also discusses measurement error, missing data, and Gaussian process models for spatial and network autocorrelation. By using complete R code examples throughout, this book provides a practical foundation for performing statistical inference. Designed for both PhD students and seasoned professionals in the natural and social sciences, it prepares them for more advanced or specialized statistical modeling. Web Resource The book is accompanied by an R package (rethinking) that is available on the author's website and GitHub. The two core functions (map and map2stan) of this package allow a variety of statistical models to be constructed from standard model formulas.},
address = {Leipzig},
author = {McElreath, Richard},
doi = {10.1201/9781315372495},
edition = {2nd},
isbn = {9781315362618},
publisher = {CRC Press},
title = {{Statistical rethinking: A Bayesian course with examples in R and Stan}},
year = {2020}
}
@article{Geyer1992a,
author = {Geyer, Charles J.},
doi = {10.1214/ss/1177011137},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Gibbs sampler,Markov chain,Metropolis-Hastings algorithm,Monte Carlo,central limit theorem,variance estimation},
month = {nov},
number = {4},
pages = {473--483},
publisher = {Institute of Mathematical Statistics},
title = {{Practical Markov Chain Monte Carlo}},
url = {http://projecteuclid.org/euclid.ss/1177011137},
volume = {7},
year = {1992}
}
@article{Giere1977,
author = {Giere, Ronald N.},
doi = {10.1007/BF00485688},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Giere - 1977 - Allan Birnbaum's conception of statistical evidence.pdf:pdf},
issn = {00397857},
journal = {Synthese},
keywords = {Epistemology,Logic,Metaphysics,Philosophy of Language,Philosophy of Science},
month = {sep},
number = {1},
pages = {5--13},
publisher = {Kluwer Academic Publishers},
title = {{Allan Birnbaum's conception of statistical evidence}},
url = {https://link.springer.com/article/10.1007/BF00485688},
volume = {36},
year = {1977}
}
@book{Popper2005,
address = {London},
author = {Popper, Karl},
publisher = {Routledge},
title = {{The Logic Of Scientific Discovery}},
year = {2005}
}
@article{Campbell2018,
abstract = {In response to growing concern about the reliability and reproducibility of published science, researchers have proposed adopting measures of greater statistical stringency, including suggestions to require larger sample sizes and to lower the highly criticized p{\textless}0.05 significance threshold. While pros and cons are vigorously debated, there has been little to no modeling of how adopting these measures might affect what type of science is published. In this paper, we develop a novel optimality model that, given current incentives to publish, predicts a researcher's most rational use of resources in terms of the number of studies to undertake, the statistical power to devote to each study, and the desirable pre-study odds to pursue. We then develop a methodology that allows one to estimate the reliability of published research by considering a distribution of preferred research strategies. Using this approach, we investigate the merits of adopting measures of `greater statistical stringency' with the goal of informing the ongoing debate.},
archivePrefix = {arXiv},
arxivId = {1803.06053},
author = {Campbell, Harlan and Gustafson, Paul},
doi = {10.1080/00031305.2018.1555101},
eprint = {1803.06053},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Campbell, Gustafson - 2018 - The world of research has gone berserk modeling the consequences of requiring greater statistical stringenc.pdf:pdf},
title = {{The world of research has gone berserk: modeling the consequences of requiring "greater statistical stringency" for scientific publication}},
url = {http://arxiv.org/abs/1803.06053},
volume = {1305},
year = {2018}
}
@article{Gelman2013a,
abstract = {View full textDownload full textRelated var addthis{\_}config = {\{} ui{\_}cobrand: "Taylor {\&} Francis Online", services{\_}compact: "citeulike,netvibes,twitter,technorati,delicious,linkedin,facebook,stumbleupon,digg,google,more", pubid: "ra-4dff56cd6bb1830b" {\}}; Add to shortlist Link Permalink http://dx.doi.org/10.1080/00031305.2012.752409 Download Citation Recommend to: A friend},
archivePrefix = {arXiv},
arxivId = {1210.7225},
author = {Gelman, Andrew and Robert, Christian P.},
doi = {10.1080/00031305.2012.752409},
eprint = {1210.7225},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gelman, Robert - 2013 - Rejoinder The anti-Bayesian moment and its passing.pdf:pdf},
isbn = {0003-1305},
issn = {00031305},
journal = {American Statistician},
number = {1},
pages = {16--17},
title = {{Rejoinder: The anti-Bayesian moment and its passing}},
volume = {67},
year = {2013}
}
@article{Roberts2008,
author = {Roberts, Gareth O and Rosenthal, Jeffrey S},
journal = {Journal of Computational and Graphical Statistics},
number = {2},
pages = {349--367},
title = {{Examples of Adaptive MCMC}},
volume = {18},
year = {2009}
}
@book{Ravetz1996,
abstract = {Originally published: Oxford : Clarendon Press, 1971. Pt. I. The Varieties of Scientific Experience -- 1. 'What is Science?' -- 2. Social Problems of Industrialized Science -- Pt. II. The Achievement of Scientific Knowledge -- 3. Science as Craftsman's Work -- 4. Scientific Inquiry: Problem-Solving on Artificial Objects -- 5. Methods -- 6. Facts and their Evolution -- 7. The Special Character of Scientific Knowledge -- Pt. Ill. Social Aspects of Scientific Activity -- 8. The Protection of Property -- 9. The Management of Novelty -- 10. Quality Control in Science -- 11. Ethics in Scientific Activity -- Pt. IV. Science in the Modern World -- 12. Technical Problems -- 13. Practical Problems -- 14. Immature and Ineffective Fields of Inquiry -- Pt. V. Conclusion: The Future of Science.},
author = {Ravetz, Jerome R.},
isbn = {1560008512},
pages = {449},
publisher = {Transaction Publishers},
title = {{Scientific Knowledge and Its Social Problems}},
year = {1995}
}
@article{UnitedStatesFoodandDrugAdministrationCenterforDrugEvaluationandResearchandCenterforBiologicsEvaluationandResearch,
author = {{U.S. Food and Drug Administration Center for Drug Evaluation and Research and Center for Biologics Evaluation and Research}},
journal = {Web archive: https://www.fda.gov/media/78504/download (accessed 01/03/2021)},
title = {{Non-inferiority clinical trials to establish effectiveness: Guidance for industry}},
url = {https://www.fda.gov/media/78504/download},
year = {2016}
}
@misc{thuringer_ministerium_fur_bildung_wissenschaft_und_kultur_kernlehrplan_2012,
author = {{Wissenschaft und Kultur}, Th{\"{u}}ringer Ministerium f{\"{u}}r Bildung},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Wissenschaft und Kultur - 2012 - Kernlehrplan des Landes Th{\"{u}}ringen f{\"{u}}r die Gymnasiale Oberstufe.pdf:pdf},
publisher = {Th{\"{u}}ringer Ministerium f{\"{u}}r Bildung, Wissenschaft und Kultur},
title = {{Kernlehrplan des Landes Th{\"{u}}ringen f{\"{u}}r die Gymnasiale Oberstufe}},
year = {2012}
}
@techreport{Mills2018,
author = {Mills, Jeffrey A.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Mills - 2018 - Objective Bayesian Hypothesis Testing.pdf:pdf},
institution = {University of Cincinnati},
title = {{Objective Bayesian Precise Hypothesis Testing}},
year = {2018}
}
@article{Blume2019,
abstract = {ABSTRACTSecond generation p-values preserve the simplicity that has made p-values popular while resolving critical flaws that promote misinterpretation of data, distraction by trivial effects, and unreproducible assessments of data. The second-generation p-value (SGPV) is an extension that formally accounts for scientific relevance by using a composite null hypothesis that captures null and scientifically trivial effects. Because the majority of spurious findings are small effects that are technically nonnull but practically indistinguishable from the null, the second-generation approach greatly reduces the likelihood of a false discovery. SGPVs promote transparency, rigor and reproducibility of scientific results by a priori identifying which candidate hypotheses are practically meaningful and by providing a more reliable statistical summary of when the data are compatible with the candidate hypotheses or null hypotheses, or when the data are inconclusive. We illustrate the importance of these advances u...},
author = {Blume, Jeffrey D. and Greevy, Robert A. and Welty, Valerie F. and Smith, Jeffrey R. and Dupont, William D.},
doi = {10.1080/00031305.2018.1537893},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Blume et al. - 2019 - An Introduction to Second-Generation ipi -Values.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {hypothesis,likelihood ratios,null,p -value,statistical evidence},
number = {sup1},
pages = {157--167},
title = {{An Introduction to Second-Generation {\textless}i{\textgreater}p{\textless}/i{\textgreater} -Values}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1537893},
volume = {73},
year = {2019}
}
@article{Cressie1980,
abstract = {This paper reviews what is currently known about the behaviour of the t-statistic when one is no longer sampling from a normal distribution. Suppose Y is a batch of data on which the t-test is performed. Briefly then, heavy-tailed components of Y give a lighttailed t, positive correlation among Y gives a heavy-tailed t, and positively skewed components of Y give a negatively skewed t. The emphasis is on understanding why one gets this type of behaviour, although some numerical tables are presented to illustrate the conclusions.},
author = {Cressie, Noel},
doi = {10.1111/j.1467-842X.1980.tb01161.x},
issn = {1467842X},
journal = {Australian Journal of Statistics},
month = {jun},
number = {2},
pages = {143--153},
publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
title = {{Relaxing Assumptions in the one-sample t-test}},
volume = {22},
year = {1980}
}
@article{Fisher1955a,
author = {Fisher, Ronald},
journal = {Journal of the Royal Statistical Society, Series B},
number = {1},
pages = {69--78},
title = {{{\{}S{\}}tatistical {\{}M{\}}ethods and {\{}S{\}}cientific {\{}I{\}}nduction}},
url = {https://www.jstor.org/stable/2983785?seq=1{\#}metadata{\_}info{\_}tab{\_}contents http://links.jstor.org/sici?sici=0035-9246(1955)17:1{\%}3C69:SMASI{\%}3E2.0.CO;2-M},
volume = {17},
year = {1955}
}
@article{Meyners2007,
author = {Meyners, Michael},
journal = {Food Quality and Preference},
pages = {541--547},
title = {{Least equivalent allowable difference in equivalence testing.}},
volume = {18},
year = {2007}
}
@article{Ioannidis2005,
abstract = {Context Controversy and uncertainty ensue when the results of clinical research on the effectiveness of interventions are subsequently contradicted. Controversies are most prominent when high-impact research is involved.$\backslash$nObjectives To understand how frequently highly cited studies are contradicted or find effects that are stronger than in other similar studies and to discern whether specific characteristics are associated with such refutation over time.$\backslash$nDesign All original clinical research studies published in 3 major general clinical journals or high-impact-factor specialty journals in 1990-2003 and cited more than 1000 times in the literature were examined.$\backslash$nMain Outcome Measure The results of highly cited articles were compared against subsequent studies of comparable or larger sample size and similar or better controlled designs. The same analysis was also performed comparatively for matched studies that were not so highly cited.$\backslash$nResults Of 49 highly cited original clinical research studies, 45 claimed that the intervention was effective. Of these, 7 (16{\%}) were contradicted by subsequent studies, 7 others (16{\%}) had found effects that were stronger than those of subsequent studies, 20 (44{\%}) were replicated, and 11 (24{\%}) remained largely unchallenged. Five of 6 highly-cited nonrandomized studies had been contradicted or had found stronger effects vs 9 of 39 randomized controlled trials (P = .008). Among randomized trials, studies with contradicted or stronger effects were smaller (P = .009) than replicated or unchallenged studies although there was no statistically significant difference in their early or overall citation impact. Matched control studies did not have a significantly different share of refuted results than highly cited studies, but they included more studies with “negative” results.$\backslash$nConclusions Contradiction and initially stronger effects are not unusual in highly cited research of clinical interventions and their outcomes. The extent to which high citations may provoke contradictions and vice versa needs more study. Controversies are most common with highly cited nonrandomized studies, but even the most highly cited randomized trials may be challenged and refuted over time, especially small ones.},
author = {Ioannidis, John P.A.},
doi = {10.1001/jama.294.2.218},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ioannidis - 2005 - Contradicted and initially stronger effects in highly cited clinical research.pdf:pdf},
isbn = {0098-7484},
issn = {00987484},
journal = {Journal of the American Medical Association},
keywords = {clinical research},
month = {jul},
number = {2},
pages = {218--228},
pmid = {16014596},
publisher = {American Medical Association},
title = {{Contradicted and initially stronger effects in highly cited clinical research}},
volume = {294},
year = {2005}
}
@book{Reid1982,
abstract = {Includes index. Jerzy Neyman received the National Medal of Science "for laying the foundations of modern statistics and devising tests and procedures that have become essential parts of the knowledge of every statistician." Until his death in 1981 at the age of 87, Neyman was vigorously involved in the concerns and controversies of the day, a scientist whose personality and activity were integral parts of his contribution to science. Through conversations with Neyman as well as the author's access to his personal and professional letters and papers, this book presents the life story of the statistician.},
address = {New York},
author = {Reid, Constance.},
isbn = {9783540907473},
pages = {298},
publisher = {Springer-Verlag},
title = {{Neyman - From Life}},
year = {1982}
}
@article{Eva2020,
abstract = {In this article, we address a major outstanding question of probabilistic Bayesian epistemology: how should a rational Bayesian agent update their beliefs upon learning an indicative conditional? A number of authors have recently contended that this question is fundamentally underdetermined by Bayesian norms, and hence that there is no single update procedure that rational agents are obliged to follow upon learning an indicative conditional. Here we resist this trend and argue that a core set of widely accepted Bayesian norms is sufficient to identify a normatively privileged updating procedure for this kind of learning. Along the way, we justify a privileged formalization of the notion of ‘epistemic conservativity', offer a new analysis of the Judy Benjamin problem, and emphasize the distinction between interpreting the content of new evidence and updating one's beliefs on the basis of that content.},
author = {Eva, Benjamin and Hartmann, Stephan and Rad, Soroush Rafiee},
doi = {10.1093/mind/fzz025},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Eva, Hartmann, Rad - 2020 - Learning from Conditionals.pdf:pdf},
issn = {0026-4423},
journal = {Mind},
number = {514},
pages = {461--508},
publisher = {Oxford University Press},
title = {{Learning from Conditionals}},
volume = {129},
year = {2020}
}
@article{Gelman2013,
abstract = {The missionary zeal of many Bayesians of old has been matched, in the other direction, by a view among some theoreticians that Bayesian methods are absurd-not merely misguided but obviously wrong in principle. We consider several examples, beginning with Feller's classic text on probability theory and continuing with more recent cases such as the perceived Bayesian nature of the so-called doomsday argument. We analyze in this note the intellectual background behind various misconceptions about Bayesian statistics, without aiming at a complete historical coverage of the reasons for this dismissal.},
archivePrefix = {arXiv},
arxivId = {1006.5366},
author = {Gelman, Andrew and Robert, Christian P.},
doi = {10.1080/00031305.2013.760987},
eprint = {1006.5366},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gelman, Robert - 2013 - Not only defended but also applied The perceived absurdity of Bayesian inference.pdf:pdf},
isbn = {0003-1305},
issn = {00031305},
journal = {American Statistician},
keywords = {Bogosity,Doomsdsay argument,Foundations,Frequentist,Laplace law of succession},
number = {1},
pages = {1--5},
pmid = {85654693},
title = {{"Not only defended but also applied": The perceived absurdity of Bayesian inference}},
volume = {67},
year = {2013}
}
@article{sutherland_interactive_2017,
author = {Sutherland, SINCLAIR and Ridgway, J},
journal = {Statistics education research journal.},
number = {1},
pages = {26--30},
title = {{Interactive visualisations and statistical literacy.}},
volume = {16},
year = {2017}
}
@article{Cook2018,
author = {Cook, J.A. and Julious, S.A. and Sones, W. and Hampson, L.V. and Hewitt, C. and Berlin, J.A. and Ashby, D. and Emsley, R. and Fergusson, D.A. and Walters, S.J. and Wilson, E.C.F. and MacLennan, G. and Stallard, N. and Rothwell, J.C. and Bland, M. and Brown, L. and Ramsay, C.R. and Cook, A. and Armstrong, D. and Altman, D. and Vale, L.D.},
doi = {10.1136/bmj.k3750},
journal = {Trials},
number = {1},
pages = {1--6},
title = {{DELTA 2 guidance on choosing the target difference and undertaking and reporting the sample size calculation for a randomised controlled trial}},
volume = {19},
year = {2018}
}
@article{Pashler2012a,
abstract = {We discuss three arguments voiced by scientists who view the current outpouring of concern about replicability as overblown. The first idea is that the adoption of a low alpha level (e.g., 5{\%}) puts reasonable bounds on the rate at which errors can enter the published literature, making false-positive effects rare enough to be considered a minor issue. This, we point out, rests on statistical misunderstanding: The alpha level imposes no limit on the rate at which errors may arise in the literature (Ioannidis, 2005b). Second, some argue that whereas direct replication attempts are uncommon, conceptual replication attempts are common--providing an even better test of the validity of a phenomenon. We contend that performing conceptual rather than direct replication attempts interacts insidiously with publication bias, opening the door to literatures that appear to confirm the reality of phenomena that in fact do not exist. Finally, we discuss the argument that errors will eventually be pruned out of the literature if the field would just show a bit of patience. We contend that there are no plausible concrete scenarios to back up such forecasts and that what is needed is not patience, but rather systematic reforms in scientific practice.},
author = {Pashler, Harold and Harris, Christine R.},
doi = {10.1177/1745691612463401},
isbn = {1745-6916},
issn = {17456916},
journal = {Perspectives on Psychological Science},
keywords = {publication bias,replication},
number = {6},
pages = {531--536},
pmid = {26168109},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Is the Replicability Crisis Overblown? Three Arguments Examined}},
url = {http://journals.sagepub.com/doi/10.1177/1745691612463401},
volume = {7},
year = {2012}
}
@article{Barker1969,
author = {Barker, A. A.},
journal = {Australian Journal of Physics},
pages = {119--133},
title = {{Monte Carlo calculations of the radial distribution functions for a proton-electron plasma}},
volume = {18},
year = {1969}
}
@article{Senn2011,
author = {Senn, Stephen},
journal = {RMM},
number = {Special Topic: Statistical Science and Philosophy of Science},
pages = {48--66},
title = {{You May Believe You Are a Bayesian But You Are Probably Wrong}},
volume = {2},
year = {2011}
}
@book{kruger_methoden_2014,
address = {Berlin, Heidelberg},
annote = {DOI: 10.1007/978-3-642-37827-0},
author = {Kr{\"{u}}ger, Dirk and Parchmann, Ilka and Schecker, Horst},
editor = {Kr{\"{u}}ger, Dirk and Parchmann, Ilka and Schecker, Horst},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kr{\"{u}}ger, Parchmann, Schecker - 2014 - Methoden in der naturwissenschaftsdidaktischen Forschung.pdf:pdf},
isbn = {978-3-642-37826-3 978-3-642-37827-0},
publisher = {Springer Berlin Heidelberg},
title = {{Methoden in der naturwissenschaftsdidaktischen Forschung}},
url = {http://link.springer.com/10.1007/978-3-642-37827-0},
year = {2014}
}
@article{Geisser1979,
abstract = {This article offers a synthesis of Bayesian and sample-reuse approaches to the problem of high structure model selection geared to prediction. Similar methods are used for low structure models. Nested and nonnested paradigms are discussed and examples given. {\textcopyright} 1979, Taylor {\&} Francis Group, LLC.},
author = {Geisser, Seymour and Eddy, William F.},
doi = {10.1080/01621459.1979.10481632},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Data analysis,Hypothesis testing,Prediction,Predictive sample reuse,Quasi-Bayes criterion,Quasi-likelihood criterion},
number = {365},
pages = {153--160},
title = {{A predictive approach to model selection}},
volume = {74},
year = {1979}
}
@techreport{Viele1996,
address = {Kentucky},
author = {Viele, K.},
institution = {University of Kentucky, Dept. of Statistics},
title = {{A Nonparametric Bayesian Method for Evaluating Fit in Hierarchical Curve Models}},
year = {1996}
}
@article{Good1989,
abstract = {ISSN: 0094-9655 (Print) 1563-5163 (Online) Journal homepage: http://www.tandfonline.com/loi/gscs20 There are several possible definitions of a surprise index: the subject is reviewed by Good (1984188). Weaver (1948) proposed the following (first) definition: Suppose that an experiment has n possible mutually exclusive outcomes having proba-bilities p,,p,, . . . ,p,, and suppose that the ith one occurs. Then Weaver's surprise index is i., =p/pi where p = {\~{}} , (p j) =Ip: (the repeat rate or Gini's index of homogeneity). When 3,, is large one has reason to be surprised and perhaps to question or even to reject one's model of the experiment. In this respect A,, and the other surprise indexes, serve much the same purpose as a P-value (compare Good, 1981b). Let us compare A, with a P-value in more detail. It may be recalled that a surprise index depends on how one has decided to categorize the possible outcomes of an experiment. This categorization might even depend on the observed outcome. For example, if you are dealt four cards from an ordinary pack of playing cards, and if you happen to get the four kings, you might regard this outcome as belonging either to (i) the set of four cards all of the same denomination, or (ii) as four royal cards all of the same denomination, and for both categorizations you might decide to lump together all the non-occurring outcomes into a single category. (I mention in passing that I do not agree at all with Weaver's comment, on his page 390, that "no one particular hand [of cards] has any right to be called a surprising event".) These two methods of categoriza-tion will give different values for A,, and yet other values for i., will be derived if},
author = {Good, I.J.},
doi = {10.1080/00949658908811160},
issn = {0094-9655},
journal = {Journal of Statistical Computation and Simulation},
month = {may},
number = {1-2},
pages = {90--92},
publisher = {Informa UK Limited},
title = {{C332. Surprise indexes and p-values}},
volume = {32},
year = {1989}
}
@incollection{Rosenthal2014,
author = {Rosenthal, Jeffrey S},
booktitle = {Statistics in Action: A Canadian Outlook},
editor = {Lawless, Jerald F.},
pages = {93--108},
publisher = {Chapman and Hall/CRC},
title = {{Optimizing and Adapting the Metropolis Algorithm}},
year = {2014}
}
@incollection{Mensch1980,
author = {Mensch, Roy},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_8},
pages = {67--74},
publisher = {Springer, New York, NY},
title = {{Fisher and the Method of Moments}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}8},
year = {1980}
}
@article{Kang2019,
abstract = {Big data is now widely used in many fields and is also widely applied to the integration of disciplines. Traditional methods of safety psychology are not well suited for analyzing psychological states, especially in the management of human factors in industrial production. Also, big data now becomes a new way to excavate related insight by analyzing a large amount of psychological data. So, this paper is to propose the concept of big data of safety psychology (BDSP) and to illustrate the challenges of applying big data in safety psychology. First, this paper puts forward the concept of BDSP and analyzes the difference between BDSP and traditional sample data. Subsequently, this paper summarizes the classification standard and basic characteristic of BDSP, explores the framework of BDSP and then constructs a three-dimensional structure of BDSP. Lastly, this paper discusses the challenges of using BDSP. This study is of great help to safety practitioners to solve psychological issues in the safety domain, and points out one of the research trends of human factor in industrial safety.},
author = {Kang, Liangguo and Wu, Chao and Wang, Bing},
doi = {10.3389/fpsyg.2019.01596},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kang, Wu, Wang - 2019 - Principles, Approaches and Challenges of Applying Big Data in Safety Psychology Research.pdf:pdf},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {Big data,Big data of safety psychology,Industrial safety,Multidisciplinary psychology,Organizational psychology},
month = {jul},
number = {July},
pages = {1596},
publisher = {Frontiers Media S.A.},
title = {{Principles, Approaches and Challenges of Applying Big Data in Safety Psychology Research}},
url = {https://www.frontiersin.org/article/10.3389/fpsyg.2019.01596/full},
volume = {10},
year = {2019}
}
@incollection{DasGupta1980,
author = {{Das Gupta}, Somesh},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_16},
pages = {161--170},
publisher = {Springer, New York, NY},
title = {{Discriminant Analysis}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}16},
year = {1980}
}
@book{field_2012_R,
address = {London},
author = {Field, Andy P and Miles, Jeremy and Field, Zo{\"{e}}},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Field, Miles, Field - 2012 - Discovering Statistics Using R.pdf:pdf},
pages = {957},
publisher = {SAGE Publications Ltd},
title = {{Discovering Statistics Using R}},
year = {2012}
}
@article{Livingston2004,
abstract = {Statistics is the study of populations, how they relate to one another and what effect sampling has in terms of representing the original population. Use of statistical tools has been facilitated by modern computer technology; however, given the ease in which results are obtained, it is easy to overlook potentially incorrect use of the various statistical tests. Statistical tools are invaluable for investigators because they make it possible to determine if scientific results are important. When a test is used, it is important to know that the result of any statistical test is valid to avoid erroneous conclusions. To do so, the investigator must have a basic understanding of the test's assumptions and limitations. Modern statistical packages often include a variety of results relating to the test's applicability to the data analyzed. Not uncommonly, biologists are unfamiliar with these analyses. This review intends to improve the reader's understanding of t-tests by providing a history of Student's t-test, and its assumptions, applications, and limitations. Part 1 of the series ("The Mean and Standard Deviation: What Does It All Mean?") reviewed basic aspects of distributions, measures of central tendency, and dispersion assessment. Small sample size effects on accurate estimation of a population mean and group comparisons for continuous data are presented in this review. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
author = {Livingston, Edward H},
doi = {10.1016/j.jss.2004.02.003},
issn = {00224804},
journal = {Journal of Surgical Research},
number = {1},
pages = {58--65},
publisher = {Academic Press},
title = {{Who was student and why do we care so much about his t-test?}},
volume = {118},
year = {2004}
}
@article{xinogalos_microworlds_2017,
abstract = {Teaching and learning programming constitutes a challenge. Although several teaching approaches and programming tools have been proposed, it seems that they have limited impact on classroom practice. This article investigates students' perceptions on five educational programming environments that are widely used and the features that any introductory programming environment should have. The environments investigated are: BlueJ; objectKarel; Scratch; Alice; and MIT App inventor. These environments were studied and used by experienced undergraduate students of Informatics in the context of a fourth year course. The main features of the environments and the way of presenting them to students, as well as the assignments in the context of the course are presented, in order to help the reader realize what experience was gained by the students that evaluated the environments. Based on a questionnaire filled in by students interesting conclusions were drawn. Students identified the main features of the environments and evaluated them positively, although problems were identified. An introductory programming environment should engage students through the development of programs connected to their interests, such as games and mobile apps. Moreover, an ideal introductory programming environment should provide a simple and user-friendly Graphical User Interface (GUI) that supports visualization of objects and classes, includes a puzzle-like editor for program development, reports simple and understandable error messages in natural language, and finally the ability to execute the program in a step by step manner. Although no single environment fulfils all these features, it seems that the most successful environment is Scratch.},
author = {Xinogalos, Stelios and Satratzemi, Maya and Malliarakis, Christos},
doi = {10.1007/s10639-015-9433-1},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Xinogalos, Satratzemi, Malliarakis - 2017 - Microworlds, games, animations, mobile apps, puzzle editors and more What is important for a.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Xinogalos, Satratzemi, Malliarakis - 2017 - Microworlds, games, animations, mobile apps, puzzle editors and more What is important for.html:html},
issn = {1360-2357, 1573-7608},
journal = {Education and Information Technologies},
month = {jan},
number = {1},
pages = {145--176},
shorttitle = {Microworlds, games, animations, mobile apps, puzzl},
title = {{Microworlds, games, animations, mobile apps, puzzle editors and more: What is important for an introductory programming environment?}},
url = {https://link.springer.com/article/10.1007/s10639-015-9433-1},
volume = {22},
year = {2017}
}
@book{Pearson1990,
abstract = {This volume is an insightful biographical account of the life and work of William Sealy Gosset, a highly influential figure in the development of modern statistical thinking. Based on his statistical correspondence, the book provides unique insight into the ideas discussed by Gosset and his peers, while reflecting the profound influence he was to have in areas such as the statistics of small samples. This volume will appeal to all statisticians interested in the development of statistics in the first four decades of this century. Machine derived contents note: 1. Introduction -- 2. Background -- 3. William Sealy Gosset -- 4. Karl Pearson -- 5. Ronald A. Fisher -- 6. Egon S. Pearson.},
author = {Pearson, E S and Plackett, Robert Lewis and Barnard, George Alfred},
isbn = {9780198522270},
pages = {142},
publisher = {Clarendon Press},
title = {{''Student'': a statistical biography of William Sealy Gosset: based on writings by E. S. Pearson}},
url = {https://global.oup.com/academic/product/student-9780198522270?lang=en{\&}cc=ru},
year = {1990}
}
@article{Liang2008,
abstract = {Zellner's g prior remains a popular conventional prior for use in Bayesian variable selection, despite several undesirable consistency issues. In this article we study mixtures of g priors as an alternative to default g priors that resolve many of the problems with the original formulation while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture g priors and provide real and simulated examples to compare the mixture formulation with fixed g priors, empirical Bayes approaches, and other default procedures. {\textcopyright} 2008 American Statistical Association.},
author = {Liang, Feng and Paulo, Rui and Molina, German and Clyde, Merlise A. and Berger, J.O.},
doi = {10.1198/016214507000001337},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {AIC,BIC,Bayesian model averaging,Cauchy,Empirical bayes,Gaussian hypergeometric functions,Model selection,Zellner-Siow priors},
month = {mar},
number = {481},
pages = {410--423},
publisher = {Taylor {\&} Francis},
title = {{Mixtures of g priors for Bayesian variable selection}},
volume = {103},
year = {2008}
}
@article{Fisher1915,
abstract = {508 Distribution of the Correlation Coeffeients of Samples In the second of these two papers the more difficult problem of the frequency distribution of the correlation coefficient is attempted. For samples of 2 the frequency},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fisher, Ronald Aylmer},
doi = {10.2307/2331838},
eprint = {arXiv:1011.1669v3},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1915 - Frequency Distribution of the Values of the Correlation Coefficient in Samples from an Indefinitely Large Population.pdf:pdf},
isbn = {0006-3444},
issn = {00063444},
journal = {Biometrika},
number = {4},
pages = {507},
pmid = {25246403},
title = {{Frequency Distribution of the Values of the Correlation Coefficient in Samples from an Indefinitely Large Population}},
url = {https://digital.library.adelaide.edu.au/dspace/handle/2440/15166 http://www.jstor.org/stable/2331838?origin=crossref},
volume = {10},
year = {1915}
}
@article{Begley2012,
abstract = {C. Glenn Begley and Lee M. Ellis propose how methods, publications and incentives must change if patients are to benefit.},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/9907372v1},
author = {Begley, C. Glenn and Ellis, Lee M.},
doi = {10.1038/483531a},
eprint = {9907372v1},
isbn = {1476-4687; 0028-0836},
issn = {00280836},
journal = {Nature},
month = {mar},
number = {7391},
pages = {531--533},
pmid = {22460880},
primaryClass = {arXiv:cond-mat},
title = {{Drug development: Raise standards for preclinical cancer research}},
url = {http://www.nature.com/articles/483531a},
volume = {483},
year = {2012}
}
@phdthesis{Ehlert2012,
author = {Ehlert, Albrecht},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ehlert - 2012 - Empirische {\{}Studie{\}} {\{}Unterschiede{\}} im {\{}Lernerfolg{\}} und {\{}Unterschiede{\}} im subjektiven {\{}Erleben{\}} des {\{}Unterrichts{\}} von.html:html},
language = {German},
publisher = {Freie Universit{\"{a}}t Berlin, Freie Universit{\"{a}}t Berlin, Germany},
shorttitle = {Empirische Studie},
title = {{Empirische Studie: Unterschiede im Lernerfolg und Unterschiede im subjektiven Erleben des Unterrichts von Sch{\"{u}}lerinnen und Sch{\"{u}}lern im Informatik-Anfangsunterricht (11. Klasse Berufliches Gymnasium) in Abh{\"{a}}ngigkeit von der zeitlichen Reihenfolge der Theme}},
url = {http://www.diss.fu-berlin.de/diss/receive/FUDISS{\_}thesis{\_}000000035764},
year = {2012}
}
@article{Gelman2019a,
abstract = {Debate abounds about how to describe weaknesses in statistics. Andrew Gelman has no confidence in the term "confidence interval," but Sander Greenland doesn't find "uncertainty interval" any better and argues instead for "compatibility interval"},
author = {Gelman, Andrew and Greenland, Sander},
doi = {10.1136/bmj.l5381},
issn = {17561833},
journal = {The BMJ},
month = {sep},
pmid = {31506269},
publisher = {BMJ Publishing Group},
title = {{Are confidence intervals better termed "uncertainty intervals"?}},
url = {https://www.bmj.com/content/366/bmj.l5381 https://www.bmj.com/content/366/bmj.l5381.abstract},
volume = {366},
year = {2019}
}
@article{bilgin_opening_2017,
author = {BILGIN, AYSE AYSIN BOMBACI and COADY, CARMEL and GEIGER, VINCENT and CAVANAGH, MICHAEL and MULLIGAN, JOANNE and PETOCZ, PETER},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/BILGIN et al. - 2017 - Opening real science Evaluation of an online module on Statistical Literacy for pre-service primary teachers.pdf:pdf},
journal = {Statistics Education Research Journal},
number = {1},
shorttitle = {{\{}OPENING{\}} {\{}REAL{\}} {\{}SCIENCE{\}}},
title = {{Opening real science: Evaluation of an online module on Statistical Literacy for pre-service primary teachers}},
volume = {17},
year = {2017}
}
@article{Cumming2014,
abstract = {We need to make substantial changes to how we conduct research. First, in response to heightened concern that our published research literature is incomplete and untrustworthy, we need new requirements to ensure research integrity. These include prespecification of studies whenever possible, avoidance of selection and other inappropriate data-analytic practices, complete reporting, and encouragement of replication. Second, in response to renewed recognition of the severe flaws of null-hypothesis significance testing (NHST), we need to shift from reliance on NHST to estimation and other preferred techniques. The new statistics refers to recommended practices, including estimation based on effect sizes, confidence intervals, and meta-analysis. The techniques are not new, but adopting them widely would be new for many researchers, as well as highly beneficial. This article explains why the new statistics are important and offers guidance for their use. It describes an eight-step new-statistics strategy for research with integrity, which starts with formulation of research questions in estimation terms, has no place for NHST, and is aimed at building a cumulative quantitative discipline.},
author = {Cumming, Geoff},
doi = {10.1177/0956797613504966},
issn = {14679280},
journal = {Psychological Science},
keywords = {estimation,meta-analysis,replication,research integrity,research methods,statistical analysis,the new statistics},
number = {1},
pages = {7--29},
title = {{The New Statistics: Why and How}},
volume = {25},
year = {2014}
}
@article{Lehmann1993,
abstract = {The Fisher and Neyman-Pearson approaches to testing statistical hypotheses are compared with respect to their attitudes to the interpretation of the outcome, to power, to conditioning, and to the use of fixed significance levels. It is argued that despite basic philosophical differences, in their main practical aspects the two theories are complementary rather than contradictory and that a unified approach is possible that combines the best features of both. As applications, the controversies about the Behrens-Fisher problem and the comparison of two binomials (2 × 2 tables) are considered from the present point of view.},
author = {Lehmann, E L},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Lehmann - 1993 - The Fisher, Neymann-Pearson Theories of Testing Hypotheses One Theory or Two.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {behrens-fisher problem,conditioning,p-value,power,significance level},
number = {424},
pages = {1242--1249},
title = {{The Fisher, Neymann-Pearson Theories of Testing Hypotheses: One Theory or Two?}},
volume = {88},
year = {1993}
}
@article{Gassiat2018,
author = {Gassiat, Elisabeth},
journal = {Handbook of Mixture Analysis},
number = {2},
pages = {1--19},
title = {{Mixtures of Nonparametric Components and Hidden Markov Models}},
url = {https://www.math.u-psud.fr/{~}gassiat/chaptergassiat2.pdf},
volume = {1},
year = {2018}
}
@book{paradies_diagnostizieren_2009,
address = {Berlin},
annote = {Literaturverz. S. 186 - 189
OCLC: 552411052},
author = {Paradies, Liane and Linser, Hans J{\"{u}}rgen and Greving, Johannes},
edition = {3. Aufl},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Paradies, Linser, Greving - 2009 - Diagnostizieren, Fordern und F{\"{o}}rdern.pdf:pdf},
isbn = {978-3-589-22167-7},
keywords = {Lernerfolgsmessung,Schulleistungsmessung,Schulp{\"{a}}dagogik},
publisher = {Cornelsen Scriptor},
title = {{Diagnostizieren, Fordern und F{\"{o}}rdern}},
year = {2009}
}
@article{Rouder2014,
abstract = {Optional stopping refers to the practice of peeking at data and then, based on the results, deciding whether or not to continue an experiment. In the context of ordinary significance-testing analysis, optional stopping is discouraged, because it necessarily leads to increased type I error rates over nominal values. This article addresses whether optional stopping is problematic for Bayesian inference with Bayes factors. Statisticians who developed Bayesian methods thought not, but this wisdom has been challenged by recent simulation results of Yu, Sprenger, Thomas, and Dougherty (2013) and Sanborn and Hills (2013). In this article, I show through simulation that the interpretation of Bayesian quantities does not depend on the stopping rule. Researchers using Bayesian methods may employ optional stopping in their own research and may provide Bayesian analysis of secondary data regardless of the employed stopping rule. I emphasize here the proper interpretation of Bayesian quantities as measures of subjective belief on theoretical positions, the difference between frequentist and Bayesian interpretations, and the difficulty of using frequentist intuition to conceptualize the Bayesian approach.},
author = {Rouder, Jeffrey N.},
doi = {10.3758/s13423-014-0595-4},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Rouder - 2014 - Optional stopping no problem for Bayesians.pdf:pdf},
issn = {15315320},
journal = {Psychonomic Bulletin {\&} Review},
month = {apr},
number = {2},
pages = {301--308},
title = {{Optional stopping: no problem for Bayesians}},
volume = {21},
year = {2014}
}
@article{BarnardJenkinsWinsten1962,
author = {Barnard, G.A. and Jenkins, G.M. and Winsten, C.B.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Barnard, Jenkins, Winsten - 1962 - Likelihood Inference and Time Series.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series A (General)},
number = {3},
pages = {321--372},
title = {{Likelihood Inference and Time Series}},
volume = {125},
year = {1962}
}
@article{Held2019,
abstract = {{\textless}p{\textgreater} The concept of intrinsic credibility has been recently introduced to check the credibility of ‘out of the blue' findings without any prior support. A significant result is deemed intrinsically credible if it is in conflict with a sceptical prior derived from the very same data that would make the effect just non-significant. In this paper, I propose to use Bayesian prior-predictive tail probabilities to assess intrinsic credibility. For the standard 5{\%} significance level, this leads to a new {\textless}italic{\textgreater}p{\textless}/italic{\textgreater} -value threshold that is remarkably close to the recently proposed {\textless}italic{\textgreater}p{\textless}/italic{\textgreater} {\textless} 0.005 standard. I also introduce the credibility ratio, the ratio of the upper to the lower limit (or {\textless}italic{\textgreater}vice versa{\textless}/italic{\textgreater} ) of a confidence interval for a significant effect size. I show that the credibility ratio has to be smaller than 5.8 such that a significant finding is also intrinsically credible. Finally, a {\textless}italic{\textgreater}p{\textless}/italic{\textgreater} -value for intrinsic credibility is proposed that is a simple function of the ordinary {\textless}italic{\textgreater}p{\textless}/italic{\textgreater} -value and has a direct frequentist interpretation in terms of the probability of replicating an effect. An application to data from the Open Science Collaboration study on the reproducibility of psychological science suggests that intrinsic credibility of the original experiment is better suited to predict the success of a replication experiment than standard significance. {\textless}/p{\textgreater}},
author = {Held, Leonhard},
doi = {10.1098/rsos.181534},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Held - 2019 - The assessment of intrinsic credibility and a new argument for ipi {\&}lt 0.005.pdf:pdf},
issn = {2054-5703},
journal = {Royal Society Open Science},
keywords = {Confidence interval,Intrinsic credibility,P-value,Prior-data conflict,Replication,Significance test},
month = {mar},
number = {3},
pages = {181534},
publisher = {Royal Society Publishing},
title = {{The assessment of intrinsic credibility and a new argument for {\textless}i{\textgreater}p{\textless}/i{\textgreater} {\textless} 0.005}},
url = {https://royalsocietypublishing.org/doi/10.1098/rsos.181534},
volume = {6},
year = {2019}
}
@incollection{Good1981,
address = {Virginia},
author = {Good, I.J.},
booktitle = {Philosophy in Economics},
editor = {Pitt, Joseph C.},
file = {:Users/riko/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/dissertation/papers/Good1981.pdf:pdf},
pages = {149--174},
publisher = {Springer Netherlands},
title = {{Some Logic and History of Hypothesis Testing}},
year = {1981}
}
@article{Shuster2005,
abstract = {In this article, primarily we look at a case study, where prior to conducting the major efficacy analysis, one performs a diagnostic test for assumptions, and acts upon the result if the diagnostic test rejects the assumptions. Specifically, we show by an example that a hybrid approach of using a diagnostic test for equality of variance in a two-sample t-test situation can adversely affect, rather than protect, the operating characteristics of the study. If this kind of hybrid approach fails in such a simple setting, analysts should be very cautious about using hybrid approaches in more complex analyses of efficacy. Secondarily, we present rationale as to why the classical tests (or slightly modified versions) can be viewed as asymptotically non-parametric, and can actually be more robust against failure of assumptions than rank tests. Readers are cautioned that this illustration is limited to efficacy analysis, and is not meant as a criticism of other analyses, such as modelling or exploratory ones.},
author = {Shuster, Jonathan J.},
doi = {10.1002/sim.2175},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {Assumptions,Clinical trial,Equal variance,Permutation test,Rerandomization},
month = {aug},
number = {16},
pages = {2431--2438},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Diagnostics for assumptions in moderate to large simple clinical trials: Do they really help?}},
volume = {24},
year = {2005}
}
@article{Green2017,
abstract = {Mixture models have been around for over 150 years, as an intuitively simple and practical tool for enriching the collection of probability distributions available for modelling data. In this chapter we describe the basic ideas of the subject, present several alternative representations and perspectives on these models, and discuss some of the elements of inference about the unknowns in the models. Our focus is on the simplest set-up, of finite mixture models, but we discuss also how various simplifying assumptions can be relaxed to generate the rich landscape of modelling and inference ideas traversed in the rest of this book.},
archivePrefix = {arXiv},
arxivId = {1705.01505},
author = {Green, Peter J.},
eprint = {1705.01505},
isbn = {9780134838342},
pages = {1--16},
title = {{Introduction to finite mixtures}},
url = {http://arxiv.org/abs/1705.01505},
year = {2017}
}
@article{Rogers1993,
abstract = {Equivalency testing, a statistical method often used in biostatistics to determine the equivalence of 2 experimental drugs, is introduced to social scientists. Examples of equivalency testing are offered, and the usefulness of the method to the social scientist is discussed.},
author = {Rogers, James L. and Howard, Kenneth I. and Vessey, John T.},
doi = {10.1037/0033-2909.113.3.553},
issn = {00332909},
journal = {Psychological Bulletin},
keywords = {Antidepressive Agents,Cognitive Behavioral Therapy,Combined Modality Therapy,Comparative Study,Female,Humans,J L Rogers,J T Vessey,K I Howard,MEDLINE,MMPI,Male,Mental Disorders / diagnosis,Mental Disorders / drug therapy*,Mental Disorders / therapy,Models,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,P.H.S.,PubMed Abstract,Research Support,Social Sciences,Theoretical,Tricyclic / therapeutic use,U.S. Gov't,doi:10.1037/0033-2909.113.3.553,pmid:8316613},
number = {3},
pages = {553--565},
pmid = {8316613},
publisher = {American Psychological Association Inc.},
title = {{Using significance tests to evaluate equivalence between two experimental groups}},
volume = {113},
year = {1993}
}
@book{VanderWeele2015,
address = {New York},
author = {VanderWeele, Tyler J.},
publisher = {Oxford University Press},
title = {{Explanation in Causal Inference: Methods for Mediation and Interaction}},
year = {2015}
}
@article{Kruschke2010,
abstract = {Although Bayesian models of mind have attracted great interest from cognitive scientists, Bayesian methods for data analysis have not. This article reviews several advantages of Bayesian data analysis over traditional null-hypothesis significance testing. Bayesian methods provide tremendous flexibility for data analytic models and yield rich information about parameters that can be used cumulatively across progressive experiments. Because Bayesian statistical methods can be applied to any data, regardless of the type of cognitive model (Bayesian or otherwise) that motivated the data collection, Bayesian methods for data analysis will continue to be appropriate even if Bayesian models of mind lose their appeal. {\textcopyright} 2010 Elsevier Ltd.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kruschke, John K.},
doi = {10.1016/j.tics.2010.05.001},
eprint = {arXiv:1011.1669v3},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kruschke - 2010 - What to believe Bayesian methods for data analysis.pdf:pdf},
isbn = {1879-307X (Electronic)$\backslash$n1364-6613 (Linking)},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {7},
pages = {293--300},
pmid = {20542462},
publisher = {Elsevier Ltd},
title = {{What to believe: Bayesian methods for data analysis}},
url = {http://dx.doi.org/10.1016/j.tics.2010.05.001},
volume = {14},
year = {2010}
}
@book{Fisher1932,
address = {Edinburgh},
author = {Fisher, R.A.},
edition = {4th},
pages = {307},
publisher = {Oliver and Boyd},
title = {{Statistical methods for research workers}},
url = {https://trove.nla.gov.au/work/10809098?q{\&}sort=holdings+desc{\&}{\_}=1541507074322{\&}versionId=50609123},
year = {1932}
}
@article{Edgington1995,
author = {Edgington, Dorothy},
journal = {Mind},
number = {414},
pages = {235--329},
title = {{On Conditionals}},
volume = {104},
year = {1995}
}
@article{Carvalho2009,
abstract = {This paper presents a general, fully Bayesian framework for sparse supervised-learning problems based on the horseshoe prior. The horseshoe prior is a member of the family of multivariate scale mixtures of normals, and is therefore closely related to widely used approaches for sparse Bayesian learning, including, among others, Laplacian priors (e.g. the LASSO) and Student-t priors (e.g. the relevance vector machine). The advantages of the horseshoe are its robustness at handling unknown sparsity and large outlying signals. These properties are justified theoretically via a representation theorem and accompanied by comprehensive empirical experiments that compare its performance to benchmark alternatives. {\textcopyright} 2009 by the authors.},
author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Carvalho, Polson, Scott - 2009 - Handling sparsity via the horseshoe.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {73--80},
title = {{Handling sparsity via the horseshoe}},
volume = {5},
year = {2009}
}
@article{Fiedler2016,
abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes.Weconclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
author = {Fiedler, Klaus and Schwarz, Norbert},
doi = {10.1177/1948550615612150},
isbn = {8224406113},
issn = {19485514},
journal = {Social Psychological and Personality Science},
keywords = {ethics/morality,language,research methods,research practices,survey methodology},
month = {jan},
number = {1},
pages = {45--52},
pmid = {25869851},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Questionable Research Practices Revisited}},
url = {http://journals.sagepub.com/doi/10.1177/1948550615612150},
volume = {7},
year = {2016}
}
@article{Burman1989,
abstract = {Concepts of $\nu$-fold cross-validation and repeated learning-testing methods have been introduced here. In many problems, these methods are computationally much less expensive than ordinary cross-validation and can be used in its place. A comparative study of these three methods has been carried out in detail.},
author = {Burman, Prabir},
doi = {10.2307/2336116},
issn = {00063444},
journal = {Biometrika},
month = {sep},
number = {3},
pages = {503},
publisher = {JSTOR},
title = {{A Comparative Study of Ordinary Cross-Validation, v-Fold Cross-Validation and the Repeated Learning-Testing Methods}},
url = {https://www.jstor.org/stable/2336116},
volume = {76},
year = {1989}
}
@article{Natanegara2014,
author = {Natanegara, Fanni and Neuenschwander, Beat and Seaman, John W. and Kinnersley, Nelson and Heilmann, Cory R. and Ohlssen, David and Rochester, George},
doi = {10.1002/pst.1595},
issn = {15391604},
journal = {Pharmaceutical Statistics},
month = {jan},
number = {1},
pages = {3--12},
title = {{The current state of Bayesian methods in medical product development: survey results and recommendations from the DIA Bayesian Scientific Working Group}},
url = {http://doi.wiley.com/10.1002/pst.1595},
volume = {13},
year = {2014}
}
@article{Student1908,
author = {Student},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Student - 1908 - The Probable Error of a Correlation Coefficient.pdf:pdf},
journal = {Biometrika},
number = {1},
pages = {302--319},
title = {{The Probable Error of a Correlation Coefficient}},
volume = {6},
year = {1908}
}
@article{Hitchcock2003,
abstract = {The Metropolis-Hastings algorithm is an extremely popular Markov chain Monte Carlo technique among statisticians. This article explores the history of the algorithm, highlighting key personalities and events in its development. We relate reasons for the delay in the acceptance of the algorithm and reasons for its recent popularity.},
author = {Hitchcock, David B.},
doi = {10.1198/0003130032413},
issn = {15372731},
journal = {American Statistician},
keywords = {Biography,Markov chain,Monte Carlo method,Simulation,Statistical computing},
number = {4},
pages = {254--257},
title = {{A History of the Metropolis-Hastings algorithm}},
volume = {57},
year = {2003}
}
@article{Matthews2018a,
abstract = {The inferential inadequacies of statistical significance testing are now widely recognized. There is, however, no consensus on how to move research into a 'post p {\textless} 0.05' era. We present a potential route forward via the Analysis of Credibility, a novel methodology that allows researchers to go beyond the simplistic dichotomy of significance testing and extract more insight from new findings. Using standard summary statistics, AnCred assesses the credibility of significant and non-significant findings on the basis of their evidential weight, and in the context of existing knowledge. The outcome is expressed in quantitative terms of direct relevance to the substantive research question, providing greater protection against misinterpretation. Worked examples are given to illustrate how AnCred extracts additional insight from the outcome of typical research study designs. Its ability to cast light on the use of p-values, the interpretation of non-significant findings and the so-called 'replication crisis' is also discussed.},
author = {Matthews, Robert A J},
doi = {10.1098/rsos.171047},
issn = {2054-5703},
journal = {Royal Society open science},
keywords = {Bayesian methods,credibility,replication crisis,significance testing,statistical inference},
month = {jan},
number = {1},
pages = {171047},
pmid = {29410818},
title = {{Beyond 'significance': principles and practice of the Analysis of Credibility.}},
url = {http://rsos.royalsocietypublishing.org/lookup/doi/10.1098/rsos.171047 http://www.ncbi.nlm.nih.gov/pubmed/29410818 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5792895},
volume = {5},
year = {2018}
}
@book{Lovric2010,
author = {Lovric, M.},
file = {:Users/riko/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/dissertation/papers/Miodrag Lovric - International encyclopedia of statistical science-Springer  (2010).pdf:pdf},
publisher = {Springer},
title = {{International Encyclopedia of Statistical Science}},
year = {2010}
}
@article{Helwig2017,
abstract = {Regression models are commonly used in psychological research. In most studies, re- gression coefficients are estimated via maximum likelihood (ML) estimation. It is well-known that ML estimates have desirable large sample properties, but are prone to overfitting in small to mod- erate sized samples. In this paper, we discuss the benefits of using penalized regression, which is a form of penalized likelihood (PL) estimation. Informally, PL estimation can be understood as in- troducing bias to estimators for the purpose of reducing their variance, with the ultimate goal of providing better solutions. We focus on the Gaussian regression model, where ML and PL estima- tion reduce to ordinary least squares (OLS) and penalized least squares (PLS) estimation, respec- tively. We cover classic OLS and stepwise regression, as well as three popular penalized regression approaches: ridge regression, the lasso, and the elastic net. We compare the different penalties (or biases) imposed by each method, and discuss the resulting features each penalty encourages in the solution. To demonstrate the methods, we use an example where the goal is to predict a student's math exam performance from 30 potential predictors. Using a step-by-step tutorial with R code, we demonstrate how to (i) load and prepare the data for analysis, (ii) fit the OLS, stepwise, ridge, lasso, and elastic net models, (iii) extract and compare the model fitting results, and (iv) evaluate the performance of each method. Our example reveals that penalized regression methods can pro- duce more accurate and more interpretable results than the classic OLS and stepwise regression solutions.},
author = {Helwig, Nathaniel E.},
doi = {10.20982/tqmp.13.1.p001},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Helwig - 2017 - Adding bias to reduce variance in psychological results A tutorial on penalized regression.pdf:pdf},
isbn = {0000000329071},
issn = {2292-1354},
journal = {The Quantitative Methods for Psychology},
keywords = {elastic net,lasso,ordinary least squares,penalized least squares,ridge,tools},
number = {1},
pages = {1--19},
title = {{Adding bias to reduce variance in psychological results: A tutorial on penalized regression}},
volume = {13},
year = {2017}
}
@article{Berger1997,
abstract = {In this paper, we show that the conditional frequentist method of testing a precise hypothesis can be made virtually equivalent to Bayesian testing. The conditioning strategy proposed by Berger, Brown and Wolpert in 1994, for the simple versus simple case, is generalized to testing a precise null hypothesis versus a composite alternative hypothesis. Using this strategy, both the conditional frequentist and the Bayesian will report the same error probabilities upon rejecting or accepting. This is of considerable interest because it is often perceived that Bayesian and frequentist testing are incompatible in this situation. That they are compatible, when conditional frequentist testing is allowed, is a strong indication that the "wrong" frequentist tests are currently being used for postexperimental assessment of accuracy. The new unified testing procedure is discussed and illustrated in several common testing situations.},
author = {Berger, J.O. and Boukai, B. and Wang, Y.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Berger, Boukai, Wang - 1997 - Unified Frequentist and Bayesian Testing of a Precise Hypothesis.pdf:pdf},
journal = {Statistical Science},
keywords = {and phrases: Bayes factor,composite hy-pothesis,conditional test,error probabilities,likelihood ratio},
number = {3},
pages = {133--160},
title = {{Unified Frequentist and Bayesian Testing of a Precise Hypothesis}},
volume = {12},
year = {1997}
}
@article{cohen_local_2014,
annote = {Israel},
author = {Cohen, Avi and Haberman, Bruria and Stauber, Dalit},
doi = {10.1145/2614512.2614526},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Cohen, Haberman, Stauber - 2014 - Local and Global Perspectives of Education—thoughts About Including Computer Science in Internationa.pdf:pdf},
issn = {2153-2184},
journal = {ACM Inroads},
keywords = {Chamsa- an international comparative model,Community-oriented education,globalized education,international standards and exams},
month = {jun},
number = {2},
pages = {43--49},
title = {{Local and Global Perspectives of Education—thoughts About Including Computer Science in International Exams}},
url = {http://doi.acm.org/10.1145/2614512.2614526},
volume = {5},
year = {2014}
}
@article{Sprenger2013LindleyParadox,
author = {Sprenger, Jan},
journal = {Philosophy of Science},
number = {5},
pages = {733--744},
title = {{Testing a Precise Null Hypothesis: The Case of Lindley's Paradox}},
volume = {80},
year = {2013}
}
@book{Moore2012,
abstract = {Ninth edition. ODL{\_}FdSEW{\_}2019/2020. Looking at data--distributions -- Looking at data--relationships -- Producing data -- Probability: the study of randomness -- Sampling distributions -- Introduction to inference -- Inference for means -- Inference for proportions -- Inference for categorical data -- Inference for regression -- Multiple regression -- One-way analysis of variance -- Two-way analysis of variance -- Logistic regression -- Nonparametric tests -- Bootstrap methods and permutation tests -- Statistics for quality: control and capability.},
address = {New York},
author = {Moore, David S. and McCabe, George P. and Craig, Bruce A.},
edition = {9th},
isbn = {1319013384},
publisher = {W. H. Freeman},
title = {{Introduction to the practice of statistics}},
year = {2012}
}
@article{Peskun1973,
abstract = {The sampling method proposed by Metropolis et ai. (1953) requires the simulation of a Markov chain with a specified pi as its stationary distribution. Hastings (1970) outlined a general procedure for constructing and simulating such a Markov chain. The matrix P of transition probabilities is constructed using a defined symmetric function s{\^{}} and an arbitrary transition matrix Q - Here, for a given Q , the relative merits of the two simple choices for s {\{}i suggested by Hastings (1970) are discussed. The optimum choice for 8 ti is shown to be one of these. For the other choice, those 0 are given which are known to make the sampling method based on P asymptotically lees precise than independent sampling.}},
author = {Peskun, P. H.},
doi = {10.1093/biomet/60.3.607},
issn = {00063444},
journal = {Biometrika},
keywords = {Markov chain method of sampling,Monte-Carlo estimation,Simulation,Variance reduction},
number = {3},
pages = {607--612},
title = {{Optimum Monte-Carlo sampling using Markov chains}},
volume = {60},
year = {1973}
}
@article{Vehtari2014,
abstract = {The future predictive performance of a Bayesian model can be estimated using Bayesian cross-validation. In this article, we consider Gaussian latent variable models where the integration over the latent values is approximated using the Laplace method or expectation propagation (EP). We study the properties of several Bayesian leave-one-out (LOO) cross-validation approximations that in most cases can be computed with a small additional cost after forming the posterior approximation given the full data. Our main objective is to assess the accuracy of the approximative LOO cross-validation estimators. That is, for each method (Laplace and EP) we compare the approximate fast computation with the exact brute force LOO computation. Secondarily, we evaluate the accuracy of the Laplace and EP approximations themselves against a ground truth established through extensive Markov chain Monte Carlo simulation. Our empirical results show that the approach based upon a Gaussian approximation to the LOO marginal distribution (the so-called cavity distribution) gives the most accurate and reliable results among the fast methods.},
archivePrefix = {arXiv},
arxivId = {1412.7461},
author = {Vehtari, Aki and Mononen, Tommi and Tolvanen, Ville and Sivula, Tuomas and Winther, Ole},
eprint = {1412.7461},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Vehtari et al. - 2014 - Bayesian leave-one-out cross-validation approximations for Gaussian latent variable models.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Expectation propagation,Gaussian latent variable model,Laplace approximation,Leave-one-out cross-validation,Predictive performance},
month = {dec},
pages = {1--38},
publisher = {Microtome Publishing},
title = {{Bayesian leave-one-out cross-validation approximations for Gaussian latent variable models}},
url = {http://arxiv.org/abs/1412.7461},
volume = {17},
year = {2014}
}
@article{Jeffreys1935,
abstract = {It often happens that when two sets of data obtained by observation give slightly different estimates of the true value we wish to know whether the difference is significant. The usual procedure is to say that it is significant if it exceeds a certain rather arbitrary multiple of the standard error; but this is not very satisfactory, and it seems worth while to see whether any precise criterion can be obtained by a thorough application of the theory of probability.},
author = {Jeffreys, Harold},
doi = {10.1017/S030500410001330X},
isbn = {1469-8064},
issn = {14698064},
journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
number = {2},
pages = {203--222},
pmid = {1000182805},
title = {{Some Tests of Significance, Treated by the Theory of Probability}},
volume = {31},
year = {1935}
}
@book{katz_2009,
address = {Boston},
author = {Katz, Victor J.},
publisher = {Addison Wesley},
title = {{A History Of Mathematics - An Introduction}},
year = {2009}
}
@article{Aldrich2006,
abstract = {The paper considers the statistical work of the physicist Harold Jeffreys. In 1933-4 Jeffreys had a controversy with R.A. Fisher, the leading statistician of the time. Prior to the encounter, Jeffreys had worked on probability as the basis for scientific inference and had used methods from the theory of errors in astronomy and seismology. He had also started to rework the theory of errors on the basis of his theory of probability. After the encounter Jeffreys produced a full-scale Bayesian treatment of statistics in the form of his Theory of Probability.},
author = {Aldrich, John},
doi = {10.1111/j.1751-5823.2005.tb00150.x},
journal = {International Statistical Review},
number = {3},
pages = {289--307},
title = {{The Statistical Education of Harold Jeffreys}},
volume = {73},
year = {2006}
}
@article{Hjort1990,
abstract = {in models for life history data and choose to concentrate on A instead of B. Thus a natural task to undertake is the   estimation of A in },
author = {Hjort, Nils Lid},
doi = {10.1214/aos/1176347749},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {Beta processes,Cox regression,Levy process,censoring,cumulative hazard,nonparametric Bayes,time-discrete,time-inhomogeneous},
month = {sep},
number = {3},
pages = {1259--1294},
publisher = {Institute of Mathematical Statistics},
title = {{Nonparametric Bayes Estimators Based on Beta Processes in Models for Life History Data}},
url = {https://projecteuclid.org/euclid.aos/1176347749},
volume = {18},
year = {1990}
}
@article{Geyer1992,
author = {Geyer, Charles J.},
doi = {10.1214/ss/1177011137},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Geyer - 1992 - Practical Markov Chain Monte Carlo.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Gibbs sampler,Markov chain,Metropolis-Hastings algorithm,Monte Carlo,central limit theorem,variance estimation},
month = {nov},
number = {4},
pages = {473--483},
publisher = {Institute of Mathematical Statistics},
title = {{Practical Markov Chain Monte Carlo}},
url = {http://projecteuclid.org/euclid.ss/1177011137},
volume = {7},
year = {1992}
}
@article{Kelter2021JournalOfStatCompAndSim,
abstract = {Student's two-sample t-test is often used in medical research like randomized controlled trials. To control type I errors, normality of the observed data needs to be assessed. In practice, a two-st...},
author = {Kelter, Riko},
doi = {10.1080/00949655.2021.1925278},
issn = {0094-9655},
journal = {Journal of Statistical Computation and Simulation},
keywords = {BF = Bayes factor,Bayesian two-sample tests,JZS prior= Jeffreys–Zellner–Siow prior,Mann–Whitney's U test,NHST = null hypothesis significance testing,Student's t-test,preliminary testing for normality,type I and II error rates},
month = {may},
number = {online first},
pages = {1--23},
publisher = {Taylor {\&} Francis},
title = {{Type I and II error rates of Bayesian two-sample tests under preliminary assessment of normality in balanced and unbalanced designs and its influence on the reproducibility of medical research}},
url = {https://www.tandfonline.com/doi/full/10.1080/00949655.2021.1925278},
year = {2021}
}
@article{McKeigue2010,
abstract = {The 'Mendelian randomization' approach uses genotype as an instrumental variable to distinguish between causal and non-causal explanations of biomarker-disease associations. Classical methods for instrumental variable analysis are limited to linear or probit models without latent variables or missing data, rely on asymptotic approximations that are not valid for weak instruments and focus on estimation rather than hypothesis testing. We describe a Bayesian approach that overcomes these limitations, using the JAGS program to compute the log-likelihood ratio (lod score) between causal and non-causal explanations of a biomarker-disease association. To demonstrate the approach, we examined the relationship of plasma urate levels to metabolic syndrome in the ORCADES study of a Scottish population isolate, using genotype at six single-nucleotide polymorphisms in the urate transporter gene SLC2A9 as an instrumental variable. In models that allow for intra-individual variability in urate levels, the lod score favouring a non-causal over a causal explanation was 2.34. In models that do not allow for intra-individual variability, the weight of evidence against a causal explanation was weaker (lod score 1.38). We demonstrate the ability to test one of the key assumptions of instrumental variable analysis--that the effects of the instrument on outcome are mediated only through the intermediate variable--by constructing a test for residual effects of genotype on outcome, similar to the tests of 'overidentifying restrictions' developed for classical instrumental variable analysis. The Bayesian approach described here is flexible enough to deal with any instrumental variable problem, and does not rely on asymptotic approximations that may not be valid for weak instruments. The approach can easily be extended to combine information from different study designs. Statistical power calculations show that instrumental variable analysis with genetic instruments will typically require combining information from moderately large cohort and cross-sectional studies of biomarkers with information from very large genetic case-control studies.},
author = {McKeigue, Paul M and Campbell, Harry and Wild, Sarah and Vitart, Veronique and Hayward, Caroline and Rudan, Igor and Wright, Alan F and Wilson, James F},
doi = {10.1093/ije/dyp397},
issn = {1464-3685},
journal = {International Journal of Epidemiology},
month = {jun},
number = {3},
pages = {907--918},
pmid = {20348110},
title = {{Bayesian methods for instrumental variable analysis with genetic instruments (‘Mendelian randomization'): example with urate transporter SLC2A9 as an instrumental variable for effect of urate levels on metabolic syndrome}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20348110 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2878456 https://academic.oup.com/ije/article-lookup/doi/10.1093/ije/dyp397},
volume = {39},
year = {2010}
}
@article{gordon_teaching_2010,
author = {Gordon, Sue and Nicholas, Jackie},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gordon, Nicholas - 2010 - Teaching with examples and statistical literacy views from teachers in statistics service courses.pdf:pdf},
journal = {International Journal of Innovation in Science and Mathematics Education (formerly CAL-laborate International)},
number = {1},
shorttitle = {Teaching with examples and statistical literacy},
title = {{Teaching with examples and statistical literacy: views from teachers in statistics service courses}},
volume = {18},
year = {2010}
}
@article{Kennedy-Shaffer2019,
author = {Kennedy-Shaffer, Lee},
doi = {10.1080/00031305.2018.1537891},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kennedy-Shaffer - 2019 - Before ipi {\&}lt 0.05 to Beyond ipi {\&}lt 0.05 Using History to Contextualize ipi -Values and Significance Testing.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {education,foundational,hypothesis testing,inference,issues,probability},
number = {sup1},
pages = {82--90},
title = {{Before {\textless}i{\textgreater}p{\textless}/i{\textgreater}  {\textless} 0.05 to Beyond {\textless}i{\textgreater}p{\textless}/i{\textgreater}  {\textless} 0.05: Using History to Contextualize {\textless}i{\textgreater}p{\textless}/i{\textgreater} -Values and Significance Testing}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1537891},
volume = {73},
year = {2019}
}
@article{Efron1998,
abstract = {Fisher is the single most important figure in 20th century statistics. This talk examines his influence on modern statistical thinking, trying to predict how Fisherian we can expect the 21st century to be. Fisher's philosophy is characterized as a series of shrewd compromises between the Bayesian and frequentist viewpoints, augmented by some unique characteristics that are particularly useful in applied problems. Several current research topics are examined with an eye toward Fisherian influence, or the lack of it, and what this portends for future statistical developments. Based on the 1996 Fisher lecture, the article closely follows the text of that talk.},
author = {Efron, Bradley},
doi = {10.1214/ss/1028905930},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Efron - 1998 - R. A. Fisher in the 21st century (Invited paper presented at the 1996 R. A. Fisher Lecture).pdf:pdf},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Bayes,bootstrap,confidence intervals,empirical Bayes,fiducial,frequentist,model selection,statistical inference},
pages = {95--122},
title = {{R. A. Fisher in the 21st century (Invited paper presented at the 1996 R. A. Fisher Lecture)}},
url = {http://projecteuclid.org/euclid.ss/1028905930},
volume = {13},
year = {1998}
}
@book{Karatzas1991,
abstract = {2nd ed. "Springer study edition"--Cover.},
author = {Karatzas, Ioannis. and Shreve, Steven E.},
isbn = {9780387976556},
pages = {470},
publisher = {Springer-Verlag},
title = {{Brownian motion and stochastic calculus}},
year = {1991}
}
@article{Lee2001,
abstract = {Many theories of skill acquisition have had considerable success in addressing the fine details of learning in relatively simple tasks, but can they scale up to complex tasks that are more typical of human learning in the real world? Some theories argue for scalability by making the implicit assumption that complex tasks consist of many smaller parts, which are learned according to basic learning principles. Surprisingly, there has been rather sparse empirical testing of this crucial assumption. In this article, we examine this assumption directly by decomposing the learning in the Kanfer-Ackerman Air-Traffic Controller Task (Ackerman, 1988) from the learning at the global level all the way down to the learning at the keystroke level. First, we reanalyze the data from Ackerman (1988) and show that the learning in this complex task does indeed reflect the learning of smaller parts at the keystroke level. Second, in a follow-up eye-tracking experiment, we show that a large portion of the learning at the keystroke level reflects the learning even at a lower, i.e., attentional level. {\textcopyright} 2001 Academic Press.},
author = {Lee, Frank J. and Anderson, John R.},
doi = {10.1006/cogp.2000.0747},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Anderson - 2001 - Does Learning a Complex Task Have to Be Complex A Study in Learning Decomposition.pdf:pdf},
issn = {00100285},
journal = {Cognitive Psychology},
month = {may},
number = {3},
pages = {267--316},
publisher = {Academic Press Inc.},
title = {{Does Learning a Complex Task Have to Be Complex?: A Study in Learning Decomposition}},
volume = {42},
year = {2001}
}
@article{Wagenmakers2020,
abstract = {A frequentist confidence interval can be constructed by inverting a hypothesis test, such that the interval contains only parameter values that would not have been rejected by the test. We show how a similar definition can be employed to construct a Bayesian support interval. Consistent with Carnap's theory of corroboration, the support interval contains only parameter values that receive at least some minimum amount of support from the data. The support interval is not subject to Lindley's paradox and provides an evidence-based perspective on inference that differs from the belief-based perspective that forms the basis of the standard Bayesian credible interval.},
author = {Wagenmakers, Eric-Jan and Gronau, Quentin F. and Dablander, Fabian and Etz, Alexander},
doi = {10.1007/s10670-019-00209-z},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Wagenmakers et al. - 2020 - The Support Interval.pdf:pdf},
journal = {Erkenntnis},
publisher = {Springer Netherlands},
title = {{The Support Interval}},
url = {https://doi.org/10.1007/s10670-019-00209-z},
year = {2020}
}
@article{kaila_redesigning_2016,
abstract = {Educational technology offers several potential benefits for programming education. Still, to facilitate the technology properly, integration into a course must be carefully designed. In this article, we present a redesign of an object-oriented university-level programming course. In the redesign, a collaborative education tool was utilized to enhance active learning, facilitate communication between students and teachers, and remodel the evaluation procedure by utilizing automatically assessed tasks. The redesign was based on the best practices found in our own earlier research and that of the research community, with a focus on facilitating active learning methods and student collaboration. The redesign was evaluated by comparing two instances of the redesigned course against two instances using the old methodology. The drop-out rate decreased statistically significantly in the redesigned course instances. Moreover, there was a trend toward higher grade averages in the redesigned instances. Based on the results, we can conclude that the utilization of educational technology has a highly positive effect on student performance. Still, making major changes to course methodology does not come without certain difficulties. Hence, we also present our experiences and suggestions for the course redesign to help other educators and researchers perform similar design changes.},
annote = {Finland},
author = {Kaila, Erkki and Kurvinen, Einari and Lokkila, Erno and Laakso, Mikko-Jussi},
doi = {10.1145/2906362},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kaila et al. - 2016 - Redesigning an Object-Oriented Programming Course.pdf:pdf},
issn = {1946-6226},
journal = {Trans. Comput. Educ.},
keywords = {course methodology,course redesign,object-oriented programming,programming education},
month = {aug},
number = {4},
pages = {18:1----18:21},
title = {{Redesigning an Object-Oriented Programming Course}},
url = {http://doi.acm.org/10.1145/2906362},
volume = {16},
year = {2016}
}
@article{Vovk1993,
abstract = {A simple example shows that the classical theory of probability implies more than one can deduce via Kolmogorov's calculus of probability. Developing Dawid's ideas I propose a new calculus of probability which is free from this drawback. This calculus naturally leads to a new interpretation of probability. I argue that attempts to create a general empirical theory of probability should be abandoned and we should content ourselves with the logic of probability establishing relations between probabilistic theories and observations. My approach to the logic of probability is based on a variant of Ville's principle of the excluded gambling strategy. In addition to the classical theory of probability this approach is applied to the probabilistic theories provided by the problem of testing validity of probability forecasts and by statistical models.},
author = {Vovk, V. G.},
doi = {10.1111/j.2517-6161.1993.tb01904.x},
issn = {00359246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
keywords = {bayes factors,central limit theorem,prequential framework,principle of excluded gambling strategy,p‐values,strong law of large numbers},
month = {jan},
number = {2},
pages = {317--341},
publisher = {Wiley},
title = {{A Logic of Probability, with Application to the Foundations of Statistics}},
url = {http://doi.wiley.com/10.1111/j.2517-6161.1993.tb01904.x},
volume = {55},
year = {1993}
}
@article{Wrinch1923,
author = {Wrinch, Dorothy and Jeffreys, Harold},
doi = {10.1080/14786442308634125},
isbn = {1478644230},
issn = {1941-5982},
journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
number = {266},
pages = {368--374},
title = {{XXXVII. On certain fundamental principles of scientific inquiry (Second Paper)}},
url = {https://www.tandfonline.com/doi/full/10.1080/14786442308634125},
volume = {45},
year = {1923}
}
@phdthesis{Laarhoven1988,
address = {Rotterdam},
author = {van Laarhoven, P.J.M.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Laarhoven - 1988 - Theoretical and computational aspects of simulated annealing.pdf:pdf},
school = {Erasmus Universiteit Rotterdam},
title = {{Theoretical and computational aspects of simulated annealing}},
type = {PhD thesis},
url = {https://research.tue.nl/en/publications/theoretical-and-computational-aspects-of-simulated-annealing},
year = {1988}
}
@article{Cook2014,
author = {Cook, J.A. and Hislop, J.A. and Adewuyi, T.E. and Harrild, K.A. and Altman, D.G. and Ramsay, D.G. and Fraser, C. and Buckley, B. and Fayers, P. and Harvey, I. and Briggs, A.H. and Norrie, J.D. and Fergusson, D. and Ford, I. and Vale, L.D.},
doi = {10.3310/hta18280},
journal = {Health Technology Assessment},
number = {28},
pages = {1--172},
title = {{Assessing methods to specify the target difference for a randomised controlled trial: DELTA (Difference ELicitation in TriAls) review}},
volume = {18},
year = {2014}
}
@book{Fisher1925SMRWI,
address = {Edinburgh},
author = {Fisher, R.A.},
publisher = {Oliver and Boyd},
title = {{Statistical Methods for Research Workers}},
year = {1925}
}
@article{Kelter2020b,
author = {Kelter, Riko},
doi = {https://doi.org/10.1080/15366367.2020.1742561},
journal = {Measurement: Interdisciplinary Research and Perspectives},
number = {4},
pages = {242--250},
title = {{Statistical Rethinking: A Bayesian Course with examples in R and STAN (2nd ed.)}},
volume = {18},
year = {2020}
}
@article{Hendriksen2020,
abstract = {It is often claimed that Bayesian methods, in particular Bayes factor methods for hypothesis testing, can deal with optional stopping. We first give an overview, using only most elementary probability theory, of three different mathematical meanings that various authors give to this claim: stopping rule independence, posterior calibration and (semi-) frequentist robustness to optional stopping. We then prove theorems to the effect that - while their practical implications are sometimes debatable - these claims do indeed hold in a general measure-theoretic setting. The novelty here is that we allow for nonintegrable measures based on improper priors, which leads to particularly strong results for the practically important case of models satisfying a group invariance (such as location or scale). When equipped with the right Haar prior, calibration and semi-frequentist robustness to optional stopping hold uniformly irrespective of the value of the underlying nuisance parameter, as long as the stopping rule satisfies a certain intuitive property.},
archivePrefix = {arXiv},
arxivId = {1807.09077},
author = {Hendriksen, Allard and de Heide, Rianne and Gr{\"{u}}nwald, Peter},
doi = {10.1214/20-ba1234},
eprint = {1807.09077},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {Bayes factors,Bayesian testing,group invariance,optional stopping,right Haar prior},
month = {jul},
title = {{Optional Stopping with Bayes Factors: A Categorization and Extension of Folklore Results, with an Application to Invariant Situations}},
url = {https://projecteuclid.org/euclid.ba/1597716128},
volume = {in press},
year = {2020}
}
@book{stillwell_2002,
address = {New York},
author = {Stillwell, John},
publisher = {Springer Verlag},
title = {{Mathematics and Its History}},
year = {2002}
}
@article{Wrinch1923a,
author = {Wrinch, Dorothy and Jeffreys, Harold},
doi = {10.1080/14786442308634218},
isbn = {1478644230},
issn = {1941-5982},
journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
number = {271},
pages = {1--22},
title = {{I. The theory of mensuration}},
volume = {46},
year = {1923}
}
@article{Neyman1933,
abstract = {The problem of testing statistical hypotheses is an old one. Its origin is usually connected with the name of Thomas Bayes, who gave the well-known theorem on the probabilities a posteriori of the possible “causes" of a given event. Since then it has been discussed by many writers of whom we shall here mention two only, Bertrand and Borel, whose differing views serve well to illustrate the point from which we shall approach the subject. Bertrand put into statistical form a variety of hypotheses, as for example the hypothesis that a given group of stars with relatively small angular distances between them as seen from the earth, form a “system” or group in space. His method of attack, which is that in common use, consisted essentially in calculating the probability, P, that a certain character, x , of the observed facts would arise if the hypothesis tested were true. If P were very small, this would generally be considered as an indication that the hypothesis, H, was probably false, and vice versa . Bertrand expressed the pessimistic view that no test of this kind could give reliable results. Borel, however, in a later discussion, considered that the method described could be applied with success provided that the character, x , of the observed facts were properly chosen—were, in fact, a character which he terms “en quelque sorte remarquable.”},
author = {Neyman, Jerzy and Pearson, Egon Sharpe},
doi = {10.1098/RSTA.1933.0009},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Neyman, Pearson - 1933 - On the problem of the most efficient tests of statistical hypotheses.pdf:pdf},
issn = {0264-3952},
journal = {Phil. Trans. R. Soc. Lond. A},
month = {feb},
number = {694-706},
pages = {289--337},
publisher = {The Royal Society},
title = {{On the problem of the most efficient tests of statistical hypotheses}},
url = {http://rsta.royalsocietypublishing.org/content/231/694-706/289},
volume = {231},
year = {1933}
}
@article{Zabell1989a,
author = {Zabell, Sandy L.},
doi = {10.1007/BF01236567},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Zabell - 1989 - The rule of succession.pdf:pdf},
issn = {01650106},
journal = {Erkenntnis},
number = {2-3},
pages = {283--321},
title = {{The rule of succession}},
volume = {31},
year = {1989}
}
@article{Dawid2015,
abstract = {We present an overview of the decision-theoretic framework of statistical causality, which is well suited for formulating and solving problems of determining the effects of applied causes. The approach is described in detail, and it is related to and contrasted with other current formulations, such as structural equation models and potential responses. Topics and applications covered include confounding, the effect of treatment on the treated, instrumental variables, and dynamic treatment strategies.},
author = {Dawid, A. Philip},
doi = {10.1146/annurev-statistics-010814-020105},
issn = {2326-8298},
journal = {Annual Review of Statistics and Its Application},
keywords = {Conditional independence,Confounding,Directed acyclic graph,Dynamic treatment strategy,Effect of treatment on the treated,Moralization},
number = {1},
pages = {273--303},
publisher = {Annual Reviews Inc.},
title = {{Statistical Causality from a Decision-Theoretic Perspective}},
volume = {2},
year = {2015}
}
@article{quiceno_design_2017,
abstract = {Object-oriented programming (OOP) is a programming paradigm in which programs are organized as cooperative collections of object. Learning the object-oriented approach is however difficult for novice students, mostly because the concepts of object, class and inheritance are abstract in nature. The main objective of this article is to present the design and validation of a learning environment for learning the concepts of object-oriented programming. The learning environment is based on Polya's heuristic method for solving problems. The validation process was performed using a pretest and post-test applied to two groups: control and experimental. The results given in the validation demonstrate evidence that experimental group recorded a higher level of consolidation of the concepts of object-oriented programming.},
author = {Quiceno, A J Olier and Salgado, A A Gomez and Pineres, M F Caro},
doi = {10.1109/TLA.2017.7827913},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Quiceno, Salgado, Pineres - 2017 - Design and Implementation of a Teaching Tool for Introduction to object-oriented programming.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Quiceno, Salgado, Pineres - 2017 - Design and Implementation of a Teaching Tool for Introduction to object-oriented programming.html:html},
issn = {1548-0992},
journal = {IEEE Latin America Transactions},
keywords = {Academic Performance,Computer aided instruction,Constructivist Scenario "La Ronda del Sin{\'{u}}",Greenfoot,IEEE transactions,Media,OOP,Polya heuristic method,Process control,Silicon compounds,class concept,control group,cooperative object collections,education,experimental group,inheritance,inheritance concept,object concept,object oriented programming,object-oriented approach learning,object-oriented programming,posttest,pretest,teaching,teaching tool design,teaching tool implementation,validation process},
month = {jan},
number = {1},
pages = {97--102},
title = {{Design and Implementation of a Teaching Tool for Introduction to object-oriented programming}},
volume = {15},
year = {2017}
}
@article{Tierney1994a,
author = {Tierney, Luke},
doi = {10.1214/aos/1176325750},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {62-04,Gibbs sampler,Metropolis-Hastings algorithm,Monte Carlo,variance reduction},
month = {dec},
number = {4},
pages = {1701--1728},
publisher = {Institute of Mathematical Statistics},
title = {{Markov Chains for Exploring Posterior Distributions}},
url = {http://projecteuclid.org/euclid.aos/1176325750},
volume = {22},
year = {1994}
}
@article{Mather1951,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Mather, Kenneth},
doi = {10.1080/01621459.1951.10500767},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
number = {253},
pages = {51--54},
pmid = {25246403},
title = {{R. A. Fisher's Statistical Methods for Research Workers: An Appreciation}},
volume = {46},
year = {1951}
}
@article{Wagenmakers2015,
author = {Wagenmakers, Eric-Jan and Beek, T.F. and Rotteveel, M. and Gierholz, A. and Matzke, D. and Steingroever, H. and Ly, Alexander and Verhagen, Josine and Selker, Ravi and Sasiadek, Adam and Gronau, Quentin F. and Love, Jonathan and Pinto, Yair},
journal = {Frontiers in Psychology},
title = {{Turning the hands of time again: A purely confirmatory replication study and a Bayesian analysis}},
volume = {6},
year = {2015}
}
@article{Wasserman2000,
abstract = {This paper reviews the Bayesian approach to model selection and model averaging. In this review, I emphasize objective Bayesian methods based on noninformative priors. I will also discuss implementation details, approximations, and relationships to other methods. {\textcopyright} 2000 Academic Press.},
author = {Wasserman, Larry},
doi = {10.1006/jmps.1999.1278},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
keywords = {AIC; Bayes factors; BIC; consistency; default Baye},
month = {mar},
number = {1},
pages = {92--107},
publisher = {Academic Press},
title = {{Bayesian model selection and model averaging}},
volume = {44},
year = {2000}
}
@article{smith_metal_1975,
author = {Smith, R J and Bryant, R G},
issn = {0006-291X},
journal = {Biochemical and Biophysical Research Communications},
keywords = {Animals,Binding Sites,Cadmium,Carbonic Anhydrases,Cattle,Humans,Hydrogen-Ion Concentration,Magnetic Resonance Spectroscopy,Mercury,Protein Binding,Protein Conformation,Zinc},
month = {oct},
number = {4},
pages = {1281--1286},
pmid = {3},
shorttitle = {Metal substitutions incarbonic anhydrase},
title = {{Metal substitutions incarbonic anhydrase: a halide ion probe study}},
volume = {66},
year = {1975}
}
@article{uysal_interviews_2014,
abstract = {Different methods, strategies, or tools have been proposed for teaching Object Oriented Programming (OOP). However, it is still difficult to introduce OOP to novice learners. The problem may be not only adopting a method or language, but also use of},
author = {Uysal, Murat Pasa},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Uysal - 2014 - Interviews With College Students Evaluating Computer Programming Environments For Introductory Courses.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Uysal - 2014 - Interviews With College Students Evaluating Computer Programming Environments For Introductory Courses.html:html},
journal = {Journal of College Teaching {\&} Learning (TLC)},
shorttitle = {Interviews {\{}With{\}} {\{}College{\}} {\{}Students{\}}},
title = {{Interviews With College Students: Evaluating Computer Programming Environments For Introductory Courses}},
url = {http://www.academia.edu/6850593/Interviews{\_}With{\_}College{\_}Students{\_}Evaluating{\_}Computer{\_}Programming{\_}Environments{\_}For{\_}Introductory{\_}Courses},
volume = {11},
year = {2014}
}
@inproceedings{santos_developing_2012,
abstract = {Teaching programming in context, i.e. having exercises dealing with artifacts of a familiar domain (e.g. images, card games), has demonstrated convincing results in terms of raising student retention and interest in CS. This session addresses the development of contexts for Java by means of AGUIA/J plugins.},
address = {New York, NY, USA},
annote = {Portugal},
author = {Santos, Andr{\'{e}} L},
booktitle = {Proceedings of the 17th ACM Annual Conference on Innovation and Technology in Computer Science Education},
doi = {10.1145/2325296.2325394},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Santos - 2012 - Developing Contexts for Teaching Java Using AGUIAJ.pdf:pdf},
isbn = {978-1-4503-1246-2},
keywords = {contextualized programming,java,object-oriented programming},
pages = {378},
publisher = {ACM},
series = {{\{}ITiCSE{\}} '12},
title = {{Developing Contexts for Teaching Java Using AGUIA/J}},
url = {http://doi.acm.org/10.1145/2325296.2325394},
year = {2012}
}
@article{Betancourt2017,
abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
archivePrefix = {arXiv},
arxivId = {1701.02434},
author = {Betancourt, Michael},
doi = {10.2307/1126509},
eprint = {1701.02434},
isbn = {9788578110796},
issn = {00093920},
journal = {arXiv preprint, https://arxiv.org/abs/1701.02434},
pmid = {25246403},
title = {{A Conceptual Introduction to Hamiltonian Monte Carlo}},
url = {http://arxiv.org/abs/1701.02434},
year = {2017}
}
@incollection{Good1988SurpriseIndex,
address = {New York},
author = {Good, I.J.},
booktitle = {Encyclopedia of Statistical Sciences},
editor = {Kotz, S. and Johnson, N.L. and Reid, C.B.},
publisher = {John Wiley {\&} Sons},
title = {{Surprise index}},
volume = {7},
year = {1988}
}
@article{Micceri1989,
author = {Micceri, Theodore},
doi = {10.1037/0033-2909.105.1.156},
issn = {1939-1455},
journal = {Psychological Bulletin},
number = {1},
pages = {156--166},
title = {{The unicorn, the normal curve, and other improbable creatures.}},
volume = {105},
year = {1989}
}
@article{Box1980,
author = {Box, George E.P.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Box - 1980 - Sampling and Bayes ' Inference in Scientific Modelling and Robustness.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series A (General)},
keywords = {bayes theorem,inference,iterative learning,model building,sampling},
number = {4},
pages = {383--430},
title = {{Sampling and Bayes' Inference in Scientific Modelling and Robustness}},
volume = {143},
year = {1980}
}
@article{KelterStern2020,
archivePrefix = {arXiv},
arxivId = {https://arxiv.org/abs/2006.03334},
author = {Kelter, Riko and Stern, J.M.},
eprint = {/arxiv.org/abs/2006.03334},
journal = {arXiv preprint, https://arxiv.org/abs/2006.03334},
primaryClass = {https:},
title = {{The Full Bayesian Significance Test and the e-value - Foundations, theory and application in the cognitive sciences}},
year = {2020}
}
@article{BergerBrownWolpert1994,
author = {Berger, J.O. and Brown, L.D and Wolpert, R.L.},
doi = {10.1214/aos/1176348654},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Berger, Brown, Wolpert - 1994 - A Unified Conditional Frequentist and Bayesian Test for fixed and sequential Hypothesis Testing.pdf:pdf},
issn = {00905364},
journal = {The Annals of Statistics},
number = {4},
pages = {1787--1807},
title = {{A Unified Conditional Frequentist and Bayesian Test for fixed and sequential Hypothesis Testing}},
volume = {22},
year = {1994}
}
@article{Ashby2000,
abstract = {We review two recent trends: the emergence of evidence-based medicine and the growing use of Bayesian statistics in medical applications. Evidence-based medicine requires an integrated assessment of the available evidence, and associated uncertainty, but there is also an emphasis on decision-making, for individual patients, or at other points in the health-care system. This demands consideration of the values and costs associated with potential outcomes. We argue that the natural statistical framework for evidence-based medicine is a Bayesian approach to decision-making that incorporates an integrated summary of the available evidence and associated uncertainty with assessment of utilities. We outline a practical agenda for further development.},
author = {Ashby, Deborah and Smith, Adrian F.M.},
doi = {10.1002/1097-0258(20001215)19:23<3291::AID-SIM627>3.0.CO;2-T},
isbn = {0277-6715},
issn = {02776715},
journal = {Statistics in Medicine},
number = {23},
pages = {3291--3305},
pmid = {11113960},
title = {{Evidence-based medicine as Bayesian decision-making}},
volume = {19},
year = {2000}
}
@book{Brooks2011,
abstract = {Foundations, methodology, and algorithms. Introduction to Markov chain Monte Carlo / Charles J. Geyer ; A short history of MCMC : subjective recollections from incomplete data / Christian Robert and George Casella ; Reversible jump MCMC / Yanan Fan and Scott A. Sisson ; Optimal proposal distributions and adaptive MCMC / Jeffrey S. Rosenthal ; MCMC using Hamiltonian dynamics / Radford M. Neal ; Inference from simulations and monitoring convergence / Andrew Gelman and Kenneth Shirley ; Implementing MCMC : estimating with confidence / James M. Flegal and Galin L. Jones ; Perfection within reach : exact MCMC sampling / Radu V. Craiu and Xiao-Li Meng ; Spatial point processes / Mark Huber ; The data augmentation algorithm : theory and methodology / James P. Hobert ; Importance sampling, simulated tempering, and umbrella sampling / Charles J. Geyer ; Likelihood-free MCMC / Scott A. Sisson and Yanan Fan -- Applications and case studies. MCMC in the analysis of genetic data on related individuals / Elizabeth Thompson ; An MCMC-based analysis of a multilevel model for functional MRI data / Brian Caffo [and others] ; Partially collapsed Gibbs sampling and path-adaptive metropolis-Hastings in high-energy astrophysics / David A. van Dyk and Taeyoung Park ; Posterior exploration for computationally intensive forward models / David Higdon [and others] ; Statistical ecology / Ruth King ; Gaussian random field models for spatial data / Murali Haran ; Modeling preference changes via a hidden Markov item response theory model / Jong Hee Park ; Parallel Bayesian MCMC imputation for multiple distributed lag models : a case study in environmental epidemiology / Biran Caffo [and others] ; MCMC for state-space models / Paul Fearnhead ; MCMC in educational research / Roy Levy, Robert J. Mislevy, and John T. Behrens -- Applications of MCMC in fisheries science / Russell B. Millar ; Model comparison and simulation for hierarchical models : analyzing rural-urban migration in Thailand / Filiz Garip and Bruce Western.},
address = {Boca Raton},
author = {Brooks, Steve},
isbn = {1420079417},
pages = {592},
publisher = {CRC Press,Taylor {\&} Francis},
title = {{Handbook of Markov chain Monte Carlo}},
year = {2011}
}
@article{Huisman2017,
abstract = {An agent who receives information in the form of an indicative conditional statement and who trusts her source will modify her credences to bring them in line with the conditional. I will argue that the agent, upon the acquisition of such information, should, in general, expand her prior credence function to an indeterminate posterior one; that is, to a set of credence functions. Two different ways the agent might interpret the conditional will be presented, and the properties of the resulting indeterminate posteriors compared. The cause of the expansion from a single prior credence function to a set of credence functions forming the indeterminate posterior one will be explained. The expansion undermines the Bayesian dogma that the result of assimilating new information into a determinate prior credence functions is always a determinate posterior one.},
author = {Huisman, Leendert M.},
doi = {10.1007/s10670-016-9833-7},
issn = {15728420},
journal = {Erkenntnis},
keywords = {Epistemology,Ethics,Logic,Ontology,Philosophy,general},
number = {3},
pages = {583--601},
publisher = {Springer Netherlands},
title = {{Learning from Simple Indicative Conditionals}},
volume = {82},
year = {2017}
}
@article{UnitedStatesFoodandDrugAdministrationCenterforVeterinaryMedicine2016,
author = {{U.S. Food and Drug Administration Center for Veterinary Medicine}},
journal = {Web archive: https://www.fda.gov/regulatory-information/search-fda-guidance-documents/cvm-gfi-224-vich-gl52-bioequivalence-blood-level-bioequivalence-study (accessed 01/03/2021)},
title = {{Guidance for industry: Bioequivalence: Blood level bioequivalence study VICH GL52}},
year = {2016}
}
@article{Baker2016,
abstract = {A Nature survey lifts the lid on how researchers view the ‘crisis' rocking science and what they think will help.},
author = {Baker, Monya and Penny, Dan},
doi = {10.1038/533452A},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Baker, Penny - 2016 - Is there a reproducibility crisis.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
number = {7604},
pages = {452--454},
pmid = {27225100},
title = {{Is there a reproducibility crisis?}},
volume = {533},
year = {2016}
}
@article{Benjamin2019,
abstract = {Researchers commonly use p-values to answer the question: How strongly does the evidence favor the alternative hypothesis relative to the null hypothesis? p-Values themselves do not directly answer this question and are often misinterpreted in ways that lead to overstating the evidence against the null hypothesis. Even in the "post p {\textless} 0.05 era, " however, it is quite possible that p-values will continue to be widely reported and used to assess the strength of evidence (if for no other reason than the widespread availability and use of statistical software that routinely produces p-values and thereby implicitly advocates for their use). If so, the potential for misinterpretation will persist. In this article, we recommend three practices that would help researchers more accurately interpret p-values. Each of the three recommended practices involves interpreting p-values in light of their corresponding "Bayes factor bound, " which is the largest odds in favor of the alternative hypothesis relative to the null hypothesis that is consistent with the observed data. The Bayes factor bound generally indicates that a given p-value provides weaker evidence against the null hypothesis than typically assumed. We therefore believe that our recommendations can guard against some of the most harmful p-value misinterpretations. In research communities that are deeply attached to reliance on "p {\textless} 0.05, " our recommendations will serve as initial steps away from this attachment. We emphasize that our recommendations are intended merely as initial, temporary steps and that many further steps will need to be taken to reach the ultimate destination: a holistic interpretation of statistical evidence that fully conforms to the principles laid out in the ASA statement on statistical significance and p-values. ARTICLE HISTORY},
author = {Benjamin, Daniel J. and Berger, J.O.},
doi = {10.1080/00031305.2018.1543135},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Benjamin, Berger - 2019 - Three Recommendations for Improving the Use of p-Values.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {bayes factor,p -value,post-experimental odds},
number = {sup1},
pages = {186--191},
title = {{Three Recommendations for Improving the Use of p-Values}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1543135},
volume = {73},
year = {2019}
}
@article{Fraser2019,
abstract = {This article has two objectives. The first and narrower is to formalize the p-value function, which records all possible p-values, each corresponding to a value for whatever the scalar parameter of interest is for the problem at hand, and to show how this p-value function directly provides full inference information for any corresponding user or scientist. The p-value function provides familiar inference objects: significance levels, confidence intervals, critical values for fixed-level tests, and the power function at all values of the parameter of interest. It thus gives an immediate accurate and visual summary of inference information for the parameter of interest. We show that the p-value function of the key scalar interest parameter records the statistical position of the observed data relative to that parameter, and we then describe an accurate approximation to that p-value function which is readily constructed. ARTICLE HISTORY},
author = {Fraser, D. A. S.},
doi = {10.1080/00031305.2018.1556735},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fraser - 2019 - The ipi -value Function and Statistical Inference.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {accept,ancillarity,box,conditioning,cox,decision or judgment,reject},
number = {sup1},
pages = {135--147},
title = {{The {\textless}i{\textgreater}p{\textless}/i{\textgreater} -value Function and Statistical Inference}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1556735},
volume = {73},
year = {2019}
}
@book{Guilford1950,
address = {New York},
author = {Guilford, J.P.},
edition = {2nd},
publisher = {McGraw-Hill},
title = {{Fundamental statistics in psychology and education}},
year = {1950}
}
@inproceedings{Brown1986,
address = {Hayward, California},
author = {Brown, L.D.},
booktitle = {IMS lecture notes - Monograph Series 6},
title = {{Foundations of Exponential Families}},
year = {1986}
}
@book{poetzsch-heffter_konzepte_2009,
address = {Berlin, Heidelberg},
annote = {DOI: 10.1007/978-3-540-89471-1},
author = {Poetzsch-Heffter, Arnd},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Poetzsch-Heffter - 2009 - Konzepte objektorientierter Programmierung.pdf:pdf},
isbn = {978-3-540-89470-4 978-3-540-89471-1},
publisher = {Springer Berlin Heidelberg},
series = {{\{}eXamen{\}}.press},
title = {{Konzepte objektorientierter Programmierung}},
url = {http://link.springer.com/10.1007/978-3-540-89471-1},
year = {2009}
}
@article{Goodman2019,
abstract = {When the editors of Basic and Applied Social Psychology effectively banned the use of null hypothesis significance testing (NHST) from articles published in their journal, it set off a fire-storm of discussions both supporting the decision and defending the utility of NHST in scientific research. At the heart of NHST is the p-value which is the probability of obtaining an effect equal to or more extreme than the one observed in the sample data, given the null hypothesis and other model assumptions. Although this is conceptually different from the probability of the null hypothesis being true, given the sample, p-values nonetheless can provide evidential information, toward making an inference about a parameter. Applying a 10,000-case simulation described in this article, the authors found that p-values' inferential signals to either reject or not reject a null hypothesis about the mean ($\alpha$ = 0.05) were consistent for almost 70{\%} of the cases with the parameter's true location for the sampled-from population. Success increases if a hybrid decision criterion, minimum effect size plus p-value (MESP), is used. Here, rejecting the null also requires the difference of the observed statistic from the exact null to be meaningfully large or practically significant, in the researcher's judgment and experience. The simulation compares performances of several methods: from p-value and/or effect size-based, to confidence-interval based, under various conditions of true location of the mean, test power, and comparative sizes of the meaningful distance and population variability. For any inference procedure that outputs a binary indicator, like flagging whether a p-value is significant, the output of one single experiment is not sufficient evidence for a definitive conclusion. Yet, if a tool like MESP generates a relatively reliable signal and is used knowledgeably as part of a research process, it can provide useful information. ARTICLE HISTORY},
author = {Goodman, William M. and Spruill, Susan E. and Komaroff, Eugene},
doi = {10.1080/00031305.2018.1564697},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Goodman, Spruill, Komaroff - 2019 - A Proposed Hybrid Effect Size Plus ipi -Value Criterion Empirical Evidence Supporting its Use.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {mesp,minimum effect size,nhst,plus p -value criterion,statistical evidence},
number = {sup1},
pages = {168--185},
title = {{A Proposed Hybrid Effect Size Plus {\textless}i{\textgreater}p{\textless}/i{\textgreater} -Value Criterion: Empirical Evidence Supporting its Use}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1564697},
volume = {73},
year = {2019}
}
@article{Rouder2009,
abstract = {Progress in science often comes from discovering invariances in relationships among variables; these invariances often correspond to null hypotheses. As is commonly known, it is not possible to state evidence for the null hypothesis in conventional significance testing. Here we highlight a Bayes factor alternative to the conventional t test that will allow researchers to express preference for either the null hypothesis or the alternative. The Bayes factor has a natural and straightforward interpretation, is based on reasonable assumptions, and has better properties than other methods of inference that have been advocated in the psychological literature. To facilitate use of the Bayes factor, we provide an easy-to-use, Web-based program that performs the necessary calculations.},
author = {Rouder, Jeffrey N. and Speckman, Paul L. and Sun, Dongchu and Morey, Richard D. and Iverson, Geoffrey},
doi = {10.3758/PBR.16.2.225},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Rouder et al. - 2009 - Bayesian t tests for accepting and rejecting the null hypothesis.pdf:pdf},
isbn = {1069-9384(Print)},
issn = {10699384},
journal = {Psychonomic Bulletin and Review},
number = {2},
pages = {225--237},
pmid = {19293088},
title = {{Bayesian t tests for accepting and rejecting the null hypothesis}},
volume = {16},
year = {2009}
}
@article{Evans1986,
abstract = {Birnbaum (1962a) argued that the conditionality principle (C) and the sufficiency principle (S) implied the likelihood principle (L); he then argued (Birnbaum 1972) that C and a mathematical equivalence principle M implied L. Evans, Fraser, and Monette (1985a) gave reference details, and this paper gives proof that C alone implies L. The level of support by the profession for L is sharply less than that for S or even for C; thus the paradoxical nature of these results. In this regard, we elaborate on the Monette example (Fraser, Monette, and Ng 1984), which provides a strong case against L. We also examine closely the various proofs linking the principles and find that S and C can each be used operationally to suppress information otherwise deemed relevant. From another viewpoint this says that S and C can each be used in contexts that directly conflict with the original examples and motivations supporting them; the principles can thus be viewed as inappropriately used, or more strongly, as invalid. In either case, the result that C and S imply L or that C implies L can be regarded as noneffective in the context of discriminating applications. A resolution of the apparent anomalies can be obtained by allowing the statistical model to include ingredients additional to those usually present (particularly for subsequent use with conditionality), or alternatively by restricting the application of the principles to contexts where the conflicts would seem not to arise.},
author = {Evans, Michael J. and Fraser, Donald A. S. and Monette, Georges},
doi = {10.2307/3314794},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Evans, Fraser, Monette - 1986 - On principles and arguments to likelihood.pdf:pdf},
issn = {03195724},
journal = {Canadian Journal of Statistics},
keywords = {and phrases,conditionality,content,evi-,likelihood,principles,sufficiency},
number = {3},
pages = {181--194},
title = {{On principles and arguments to likelihood}},
volume = {14},
year = {1986}
}
@inproceedings{sanders_checklists_2007,
abstract = {In this paper, we begin by considering object-oriented programming concepts and typical novice misconceptions as identified in the literature. We then present the results of a close examination of student programs, in an objects-first CS1 course, in which we find concrete evidence of students learning these concepts while also displaying some of these misconceptions. This leads to the development of two checklists that educators can use when designing or grading student programs.},
address = {New York, NY, USA},
author = {Sanders, Kate and Thomas, Lynda},
booktitle = {Proceedings of the 12th Annual SIGCSE Conference on Innovation and Technology in Computer Science Education},
doi = {10.1145/1268784.1268834},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Sanders, Thomas - 2007 - Checklists for Grading Object-oriented CS1 Programs Concepts and Misconceptions.pdf:pdf},
isbn = {978-1-59593-610-3},
keywords = {CS1,assessment,empirical research,misconceptions,object-oriented concepts},
pages = {166--170},
publisher = {ACM},
series = {{\{}ITiCSE{\}} '07},
shorttitle = {Checklists for {\{}Grading{\}} {\{}Object{\}}-oriented {\{}CS{\}}1 {\{}}},
title = {{Checklists for Grading Object-oriented CS1 Programs: Concepts and Misconceptions}},
url = {http://doi.acm.org/10.1145/1268784.1268834},
year = {2007}
}
@book{McGrayne2011,
address = {Devon, Pennsylvania},
author = {McGrayne, Sharon Bertsch},
publisher = {Yale University Press},
title = {{The Theory that would not die: how Bayes' rule cracked the enigma code, hunted down Russian submarines, and emerged triumphant from two centuries of controversy}},
year = {2011}
}
@article{BayesFactorPackage,
author = {Morey, Richard D. and Rouder, Jeffrey N.},
journal = {R package version 0.9.12-4.2},
title = {{BayesFactor: Computation of Bayes Factors for Common Designs}},
url = {https://cran.r-project.org/package=BayesFactor},
year = {2018}
}
@incollection{Runger1980,
author = {Runger, George},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_11},
pages = {95--100},
publisher = {Springer, New York, NY},
title = {{Some Numerical Illustrations of Fisher's Theory of Statistical Estimation}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}11},
year = {1980}
}
@article{Hubbard2000,
abstract = {The historical growth in the popularity of statistical significance testing is examined using a random sample of annual data from 12 American Psychological Association (APA) journals. The results replicate and extend the findings of Hubbard, Parsa, and Luthy, who used data from only the Journal of Applied Psychology. The results also confirm Gigerenzer and Murray's allegation that an inference revolution occurred in psychology between 1940 and 1955. An assessment of the future prospects for statistical significance testing is offered. It is concluded that replication with extension research, and its connections with meta-analysis, is a betler vehicle for developing a cumulative knowledge base in the discipline than statistical significance testing. It is conceded, however, that statistical significance testing is likely here to stay.},
author = {Hubbard, Raymond and Ryan, Patricia A.},
doi = {10.1177/00131640021970808},
issn = {00131644},
journal = {Educational and Psychological Measurement},
number = {5},
pages = {661--681},
title = {{The historical growth of statistical significance testing in psychology - And its future prospects}},
volume = {60},
year = {2000}
}
@article{Andrews2013,
abstract = {Within the last few years, Bayesian methods of data analysis in psychology have proliferated. In this paper, we briefly review the history or the Bayesian approach to statistics, and consider the implications that Bayesian methods have for the theory and practice of data analysis in psychology.},
author = {Andrews, Mark and Baguley, Thom},
doi = {10.1111/bmsp.12004},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Andrews, Baguley - 2013 - Prior approval The growth of Bayesian methods in psychology.pdf:pdf},
isbn = {2044-8317 (Electronic)$\backslash$r0007-1102 (Linking)},
issn = {00071102},
journal = {British Journal of Mathematical and Statistical Psychology},
number = {1},
pages = {1--7},
pmid = {23330865},
title = {{Prior approval: The growth of Bayesian methods in psychology}},
volume = {66},
year = {2013}
}
@article{Bayes1763,
abstract = {10.1098/rstl.1763.0053},
author = {Bayes, Mr. and Price, Mr.},
doi = {10.1098/rstl.1763.0053},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bayes, Price - 1763 - An Essay Towards Solving a Problem in the Doctrine of Chances. By the Late Rev. Mr. Bayes, F. R. S. Communicated b.pdf:pdf},
isbn = {02607085},
issn = {0261-0523},
journal = {Philosophical Transactions of the Royal Society of London},
number = {0},
pages = {370--418},
pmid = {4789},
title = {{An Essay Towards Solving a Problem in the Doctrine of Chances. By the Late Rev. Mr. Bayes, F. R. S. Communicated by Mr. Price, in a Letter to John Canton, A. M. F. R. S.}},
volume = {53},
year = {1763}
}
@article{Pratt1965,
author = {Pratt, John W.},
doi = {10.1111/j.2517-6161.1965.tb01486.x},
issn = {00359246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
month = {jul},
number = {2},
pages = {169--192},
publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
title = {{Bayesian Interpretation of Standard Inference Statements}},
url = {http://doi.wiley.com/10.1111/j.2517-6161.1965.tb01486.x},
volume = {27},
year = {1965}
}
@book{Box1978,
abstract = {p. 444-5: Fisher writing in 1956: I think there is a parallelism in the nature of scientific controversy between continental drift in the last 80 years or so, and organic evolution about 100 years earlier. Each idea as it originated was necessarily speculative, and not accompanied certainly by sufficiently cogent evidence to carry final conviction. There were, however, many suggestive pointers. In consequence of this natural situation both questions have been argued with imperfect facts, incorrect theories, and often incompetent reasoning, over a long period during which many people have committed themselves to impossible positions, and many more, fearing to burn their fingers have enclosed themselves in p. 445: towers not of ivory, but of very solid wood. In the period 1800-1850, although geological specimens were being collected and described, experiments in plant hybridization carried out, the classification of animals and plants greatly improved, and embryological studies at least had attracted attention, yet so unwilling are ordinary men to run the risk of contemptuous ridicule, no one of consequence attempted to revive what had been left as speculative and almost poetical, adeas by Buffon, Erasmus Darwin and Lamarck. Darwin worked on the problem almost secretly from 1838, and only published in 1859 because he was forced to. After that the ice came down like Niagara, but of course the new idea was still ill-understood and ill-expounded for at least the next 50 years, during which a few subordinate causes of error had been removed by special research. I think a lot of geologists must be timidly peering out fof their holes on hearing the strange news that geophysicists are talking about continental drift, and I have wondered how many scientific discoveries of importance have been left unmade for lack of the quality called moral courage.},
address = {New York},
author = {Box, Joan Fisher},
isbn = {0471093009},
keywords = {ECOLOGICAL GENETICS,HISTORY OF SCIENCE,PHILOSOPHY,SELECTION,STATISTICS},
pages = {1--512},
publisher = {Wiley},
title = {{R.A. Fisher. The Life of a Scientist}},
url = {https://books.google.de/books/about/R{\_}A{\_}Fisher{\_}the{\_}life{\_}of{\_}a{\_}scientist.html?id=JOfuAAAAMAAJ{\&}redir{\_}esc=y},
year = {1978}
}
@article{Hand1998,
abstract = {Data mining is a new discipline lying at the interface of statistics, database technology, pattern recognition, machine learning, and other areas. It is concerned with the secondary analysis of large databases in order to find previously unsuspected relationships which are of interest or value to the database owners. New problems arise, partly as a consequence of the sheer size of the data sets involved, and partly because of issues of pattern matching. However, since statistics provides the intellectual glue underlying the effort, it is important for statisticians to become involved. There are very real opportunities for statisticians to make significant contributions. {\textcopyright} 1998 Taylor {\&} Francis Group, LLC.},
author = {Hand, David J.},
doi = {10.1080/00031305.1998.10480549},
issn = {15372731},
journal = {American Statistician},
keywords = {Databases,Exploratory data analysis,Knowledge discovery},
number = {2},
pages = {112--118},
title = {{Data mining: Statistics and more?}},
volume = {52},
year = {1998}
}
@book{gelman_2015,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
booktitle = {CEUR Workshop Proceedings},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gelman et al. - 2015 - Bayesian data analysis.pdf:pdf},
isbn = {9788578110796},
issn = {16130073},
keywords = {Mobile,Named entity disambiguation,Natural language processing,News,Recommender system},
number = {9},
pages = {33--36},
pmid = {25246403},
publisher = {CRC Press},
title = {{Bayesian data analysis}},
volume = {1542},
year = {2015}
}
@article{Jacobucci2016,
author = {Jacobucci, Ross and Grimm, Kevin J. and McArdle, John J.},
doi = {10.1080/10705511.2016.1154793},
issn = {1070-5511},
journal = {Structural Equation Modeling: A Multidisciplinary Journal},
keywords = {factor analysis,lasso,penalization,regularization,ridge,shrinkage,structural equation modeling},
month = {jul},
number = {4},
pages = {555--566},
publisher = {Psychology Press Ltd},
title = {{Regularized Structural Equation Modeling}},
url = {http://www.tandfonline.com/doi/full/10.1080/10705511.2016.1154793},
volume = {23},
year = {2016}
}
@article{Vehtari2019,
abstract = {This article is an invited discussion of the article by Gronau and Wagenmakers (2018) that can be found at https://dx.doi.org/10.1007/s42113-018-0011-7.},
archivePrefix = {arXiv},
arxivId = {1810.05374},
author = {Vehtari, Aki and Simpson, Daniel P. and Yao, Yuling and Gelman, Andrew},
doi = {10.1007/s42113-018-0020-6},
eprint = {1810.05374},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Vehtari et al. - 2019 - Limitations of “Limitations of Bayesian Leave-one-out Cross-Validation for Model Selection”.pdf:pdf},
issn = {2522-0861},
journal = {Computational Brain {\&} Behavior},
keywords = {M-closed,M-open,Principle of complexity,Reality,Statistical convenience},
month = {mar},
number = {1},
pages = {22--27},
publisher = {Springer Science and Business Media LLC},
title = {{Limitations of “Limitations of Bayesian Leave-one-out Cross-Validation for Model Selection”}},
url = {https://doi.org/10.1007/s42113-018-0020-6},
volume = {2},
year = {2019}
}
@inproceedings{craig_looking_2014,
abstract = {There is a growing awareness of the importance of including computing education in the curriculum of secondary schools in countries like the United States of America, the United Kingdom, New Zealand, and South Korea. Consequently, we have seen serious efforts to introduce computing education to the core curriculum and/or to improve it. Recent reports (such as Wilson et al. 2010; Hubwieser et al. 2011) reveal that computing education faces problems regarding its lack of exposure as well as a lack of motivators for students to follow this line of study. Although students use computers for many tasks both at home and at school, many of them never quite understand what computer science is and how it relates to algorithmic thinking and problem solving. This panel will bring together leaders in computing education from Australia, Germany, Greece, Israel and Norway to describe the state of computing education in each of their countries. Issues raised will include how high school computer education is conducted in that country, how teachers are skilled /accredited, the challenges that are being faced today and how these challenges are being addressed. Panellists will suggest lessons other countries may find of value from their way of doing things. An important issue is how to recruit female students in to computer education at high school level and how to encourage them to continue in the discipline to university. The problem is exacerbated because computer education is still not included as a compulsory subject in the regular curriculum of high schools in all of these countries},
address = {New York, NY, USA},
annote = {Australia {\&} Norway},
author = {Craig, Annemieke and Lang, Catherine and Giannakos, Michail N and Kleiner, Carsten and Gal-Ezer, Judith},
booktitle = {Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
doi = {10.1145/2538862.2538871},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Craig et al. - 2014 - Looking Outside What Can Be Learnt from Computing Education Around the World.pdf:pdf},
isbn = {978-1-4503-2605-6},
keywords = {computer,education,international},
pages = {371--372},
publisher = {ACM},
series = {{\{}SIGCSE{\}} '14},
shorttitle = {Looking {\{}Outside{\}}},
title = {{Looking Outside: What Can Be Learnt from Computing Education Around the World?}},
url = {http://doi.acm.org/10.1145/2538862.2538871},
year = {2014}
}
@book{Hollander2014,
address = {New Jersey},
author = {Hollander, Myles and Wolfe, Douglas A. and Chicken, Eric},
booktitle = {Wiley Series in Probability and Statistics},
edition = {3rd},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hollander, Wolfe, Chicken - 2014 - Nonparametric Statistical Methods.pdf:pdf},
isbn = {9780470387375},
publisher = {Wiley},
title = {{Nonparametric Statistical Methods}},
year = {2014}
}
@book{Ghosh2003,
abstract = {, except for brief excerpts in connection with reviews or scholarly analysis. Use in connection with any form of information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed is forbidden. The use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.},
author = {Ghosh, J.K. and Ramamoorthi, R.V.},
booktitle = {Bayesian Nonparametrics},
doi = {10.1007/b97842},
publisher = {Springer-Verlag},
title = {{Bayesian Nonparametrics}},
year = {2003}
}
@article{Declerck2008,
abstract = {Abstract – Objectives: The aim of the present study was to examine the prevalence and severity of caries experience in the primary dentition of preschool children and to assess the association of disease distribution with oral hygiene levels, reported oral health behaviours and socio-demographic factors. Methods: Study samples comprised 1250 3-year-old and 1283 5-year-old pre-school children from four distinct geographical areas in Flanders. Information on oral hygiene and dietary habits, oral health behaviours and socio-demographic variables was collected using questionnaires completed by the parents. Clinical examinations were performed using standardized criteria. Caries experience was recorded at the level of cavitation (d3 level). Simple as well as multivariable logistic regression analyses were performed in order to identify factors associated with prevalence and severity of caries experience. Results: Visible plaque was present in 31{\%} of 3-year-olds and 37{\%} of 5-year-olds. In 3-year-olds, 7{\%} presented with caries experience while this was the case in 31{\%} of 5-year-olds. Multivariable logistic regression revealed significant associations, in 3-year-olds, of caries experience with presence of dental plaque (OR = 7.93; 95{\%} CI: 2.56–24.55) and reported consumption of sugared drinks at night (OR = 7.96; 95{\%} CI: 1.57–40.51). In 5-year-olds, significant associations were seen with age (OR = 7.79; 95{\%} CI: 2.38–25.43), gender (OR = 0.37 with 95{\%} CI: 0.19–0.71 for girls), presence of visible dental plaque (OR = 3.36; 95{\%} CI: 1.64–6.89) and reported habit of having sugar-containing drinks in between meals (OR = 2.60 with 95{\%} CI: 1.16–5.84 and OR = 3.18 with 95{\%} CI: 1.39–7.28, respectively for 1×/day and {\textgreater} 1×/day versus not every day). In 5-year-olds with caries experience (30.8{\%} of total sample), the severity of disease was further analysed (d3mft between 1 and 4 versus d3mft 5 or higher). Multivariable analyses showed a significant association with gender [girls more likely to have higher disease levels; OR = 4.67 (95{\%} CI: 1.65–13.21)] and with presence of plaque (OR = 3.91 with 95{\%} CI: 1.23–12.42). Conclusions: Presence of visible plaque accumulation and reported consumption of sugared drinks were associated with prevalence of caries experience in Flemish preschool children. Severity of disease was associated with gender and with presence of plaque. Results underline the importance of plaque control and diet management from very young age on.},
author = {Declerck, Dominique and Leroy, Roos and Martens, Luc and Lesaffre, Emmanuel and Garcia-Zattera, Maria Jos{\'{e}} and Broucke, Stephan Vanden and Debyser, Martine and Hoppenbrouwers, Karel},
doi = {10.1111/j.1600-0528.2007.00385.x},
isbn = {1600-0528 (Electronic)$\backslash$r0301-5661 (Linking)},
issn = {03015661},
journal = {Community Dentistry and Oral Epidemiology},
keywords = {Caries experience,Preschool children,Primary dentition},
month = {apr},
number = {2},
pages = {168--178},
pmid = {18333881},
title = {{Factors associated with prevalence and severity of caries experience in preschool children}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18333881},
volume = {36},
year = {2008}
}
@article{Gordon2018,
abstract = {Phase and polarisation of coherent light are highly perturbed by interaction with microstructural changes in pre-malignant tissue, holding promise for label-free early cancer detection in endoscopically accessible tissues such as the gastrointestinal tract. Flexible optical fibres used in conventional diagnostic endoscopy scramble phase and polarisation, restricting clinicians instead to low-contrast amplitude-only imaging. Here, we unscramble phase and polarisation images by exploiting the near-diagonal multi-core fibre (MCF) transmission matrix to create a novel parallelised fibre characterisation architecture, scalable to arbitrary MCFs without additional experimental overhead. Our flexible MCF holographic endoscope produces full-field en-face images of amplitude, quantitative phase and resolved polarimetric properties using a low-cost laser diode and camera. We demonstrate that recovered phase enables computational re-focusing at working distances up to 1mm over a field-of-view up to 750{\$}\backslashtimes{\$}750 {\$}\backslashmu m{\^{}}2{\$}. Furthermore, we demonstrate that the spatial distribution of phase and polarisation information enables label-free visualisation of early tumours in oesophageal mouse issue that are not identifiable using conventional amplitude-only information, a milestone towards future application for early cancer detection in endoscopy.},
archivePrefix = {arXiv},
arxivId = {1811.03977},
author = {Gordon, George S. D. and Joseph, James and Alcolea, Maria P. and Sawyer, Travis and Macfaden, Alexander J. and Williams, Calum and Fitzpatrick, Catherine R. M. and Jones, Philip H. and di Pietro, Massimiliano and Fitzgerald, Rebecca C. and Wilkinson, Timothy D. and Bohndiek, Sarah E.},
eprint = {1811.03977},
journal = {arxiv preprint, https://arxiv.org/abs/1811.03977},
title = {{Quantitative phase and polarisation endoscopy applied to detection of early oesophageal tumourigenesis}},
url = {http://arxiv.org/abs/1811.03977},
year = {2018}
}
@inproceedings{al-tahat_impact_2016,
abstract = {Alice is an incredibly fun 3D programming environment that allows users to manipulate objects in a 3D world in order to create program animated movies. This paper discusses the impact of adopting Alice on female students' attitude and performance in an introductory computer programming course in Java language. The target population of this research is first year computing students at Arab Open University - Jordan branch. Quasi-experiment was conducted in this research, where two classes were chosen one of which used Alice, and the other class has not used it. Data analysis showed that using Alice had a positive impact on female students' performance and attitude towards computer programming.},
annote = {Jordan},
author = {Al-Tahat, K and Taha, N and Hasan, B and Shawar, B A},
booktitle = {2016 {\{}SAI{\}} {\{}Computing{\}} {\{}Conference{\}} ({\{}SAI{\}})},
doi = {10.1109/SAI.2016.7556080},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Al-Tahat et al. - 2016 - The impact of a 3D visual tool on female students attitude and performance in computer programming.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Al-Tahat et al. - 2016 - The impact of a 3D visual tool on female students attitude and performance in computer programming.html:html},
keywords = {3D programming environment,3D visual tool,Alice,Arab Open University,Attitude,Computer Programming,Computer aided instruction,Computer science education,Computers,Female,Gender,Jordan,Performance,Programming profession,Three-dimensional displays,animated movies,computer programming course,data analysis,education,educational courses,human factors,java,java language,object oriented programming},
month = {jul},
pages = {864--867},
title = {{The impact of a 3D visual tool on female students attitude and performance in computer programming}},
year = {2016}
}
@article{Robert2008,
abstract = {We attempt to trace the history and development of Markov chain Monte Carlo (MCMC) from its early inception in the late 1940s through its use today. We see how the earlier stages of Monte Carlo (MC, not MCMC) research have led to the algorithms currently in use. More importantly, we see how the development of this methodology has not only changed our solutions to problems, but has changed the way we think about problems.},
archivePrefix = {arXiv},
arxivId = {0808.2902},
author = {Robert, Christian and Casella, George},
doi = {10.1214/10-STS351},
eprint = {0808.2902},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Robert, Casella - 2008 - A Short History of Markov Chain Monte Carlo Subjective Recollections from Incomplete Data.pdf:pdf},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
number = {1},
pages = {102--115},
title = {{A Short History of Markov Chain Monte Carlo: Subjective Recollections from Incomplete Data}},
volume = {26},
year = {2008}
}
@article{Basu1975,
author = {Basu, D.},
doi = {10.1007/978-1-4612-3894-2},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Basu - 1975 - Statistical Information and Likelihood (with discussion).pdf:pdf},
journal = {Sankhya: The Indian Journal of Statistics, Series A},
number = {1},
pages = {1--71},
title = {{Statistical Information and Likelihood (with discussion)}},
volume = {37},
year = {1975}
}
@article{DaSilva2015,
abstract = {This work addresses an important issue regarding the performance of simultaneous test procedures: the construction of multiple tests that at the same time are optimal from a statistical perspective and that also yield logically-consistent results that are easy to communicate to practitioners of statistical methods. For instance, if hypothesis A implies hypothesis B, is it possible to create optimal testing procedures that reject A whenever they reject B Unfortunately, several standard testing procedures fail in having such logical consistency. Although this has been deeply investigated under a frequentist perspective, the literature lacks analyses under a Bayesian paradigm. In this work, we contribute to the discussion by investigating three rational relationships under a Bayesian decision-theoretic standpoint: coherence, invertibility and union consonance. We characterize and illustrate through simple examples optimal Bayes tests that fulfill each of these requisites separately. We also explore how far one can go by putting these requirements together. We show that although fairly intuitive tests satisfy both coherence and invertibility, no Bayesian testing scheme meets the desiderata as a whole, strengthening the understanding that logical consistency cannot be combined with statistical optimality in general. Finally, we associate Bayesian hypothesis testing with Bayes point estimation procedures. We prove the performance of logically-consistent hypothesis testing by means of a Bayes point estimator to be optimal only under very restrictive conditions.},
author = {da Silva, Gustavo and Esteves, Luis and Fossaluza, Victor and Izbicki, Rafael and Wechsler, Sergio},
doi = {10.3390/e17106534},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/da Silva et al. - 2015 - A Bayesian Decision-Theoretic Approach to Logically-Consistent Hypothesis Testing.pdf:pdf},
issn = {1099-4300},
journal = {Entropy},
keywords = {Bayes tests,Decision theory,Logical consistency,Loss functions,Multiple hypothesis testing},
number = {12},
pages = {6534--6559},
publisher = {MDPI AG},
title = {{A Bayesian Decision-Theoretic Approach to Logically-Consistent Hypothesis Testing}},
url = {http://www.mdpi.com/1099-4300/17/10/6534},
volume = {17},
year = {2015}
}
@article{Branscum2008,
abstract = {A common goal in meta-analysis is estimation of a single effect measure using data from several studies that are each designed to address the same scientific inquiry. Because studies are typically conducted in geographically disperse locations, recent developments in the statistical analysis of meta-analytic data involve the use of random effects models that account for study-to-study variability attributable to differences in environments, demographics, genetics, and other sources that lead to heterogeneity in populations. Stemming from asymptotic theory, study-specific summary statistics are modeled according to normal distributions with means representing latent true effect measures. A parametric approach subsequently models these latent measures using a normal distribution, which is strictly a convenient modeling assumption absent of theoretical justification. To eliminate the influence of overly restrictive parametric models on inferences, we consider a broader class of random effects distributions. We develop a novel hierarchical Bayesian nonparametric Polya tree mixture (PTM) model. We present methodology for testing the PTM versus a normal random effects model. These methods provide researchers a straightforward approach for conducting a sensitivity analysis of the normality assumption for random effects. An application involving meta-analysis of epidemiologic studies designed to characterize the association between alcohol consumption and breast cancer is presented, which together with results from simulated data highlight the performance of PTMs in the presence of nonnormality of effect measures in the source population. {\textcopyright} 2008, The International Biometric Society.},
author = {Branscum, Adam J. and Hanson, Timothy E.},
doi = {10.1111/j.1541-0420.2007.00946.x},
issn = {0006341X},
journal = {Biometrics},
keywords = {Bayesian nonparametrics,Epidemiologic studies,Robustness},
month = {sep},
number = {3},
pages = {825--833},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Bayesian Nonparametric Meta-Analysis Using Polya Tree Mixture Models}},
url = {http://doi.wiley.com/10.1111/j.1541-0420.2007.00946.x},
volume = {64},
year = {2008}
}
@article{Good1985,
author = {Good, I.J.},
journal = {Journal of Statistical Computation and Simulation},
number = {1},
pages = {88--89},
title = {{A new measure of surprise}},
volume = {21},
year = {1985}
}
@article{Kelter2019,
author = {Kelter, Riko},
institution = {Department of Mathematics, University of Siegen},
journal = {arXiv preprint},
keywords = {arXiv preprint},
mendeley-tags = {arXiv preprint},
title = {{A new Bayesian two-sample t-test for effect size estimation under uncertainty based on a two-component Gaussian mixture with known allocations and the region of practical equivalence}},
url = {https://arxiv.org/abs/1906.07524},
year = {2019}
}
@book{tabak_2005,
address = {New York},
author = {Tabak, John},
publisher = {Checkmark Books},
title = {{Probability {\&} Statistics}},
year = {2005}
}
@article{Society2018,
author = {Society, Royal Statistical},
number = {1},
pages = {102--118},
title = {{Problems Arising in the Analysis of a Series of Similar Experiments Author ( s ): W . G . Cochran Source : Supplement to the Journal of the Royal Statistical Society , Vol . 4 , No . 1 ( 1937 ), pp . Published by : Wiley for the Royal Statistical Society }},
volume = {4},
year = {2018}
}
@article{Navarro2019,
abstract = {Discussions of model selection in the psychological literature typically frame the issues as a question of statistical inference, with the goal being to determine which model makes the best predictions about data.Within this setting, advocates of leaveone- out cross-validation and Bayes factors disagree on precisely which prediction problem model selection questions should aim to answer. In this comment, I discuss some of these issues from a scientific perspective. What goal does model selection serve when all models are known to be systematically wrong? How might “toy problems” tell a misleading story? How does the scientific goal of explanation align with (or differ from) traditional statistical concerns? I do not offer answers to these questions, but hope to highlight the reasons why psychological researchers cannot avoid asking them.},
author = {Navarro, Danielle J.},
doi = {10.1007/s42113-018-0019-z},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Navarro - 2019 - Between the Devil and the Deep Blue Sea Tensions Between Scientific Judgement and Statistical Model Selection.pdf:pdf},
issn = {2522-0861},
journal = {Computational Brain {\&} Behavior},
keywords = {Model selection,Science,Statistics},
month = {mar},
number = {1},
pages = {28--34},
publisher = {Springer Science and Business Media LLC},
title = {{Between the Devil and the Deep Blue Sea: Tensions Between Scientific Judgement and Statistical Model Selection}},
url = {https://doi.org/10.1007/s42113-018-0019-z},
volume = {2},
year = {2019}
}
@article{Stigler2006,
abstract = {In hommage to Bernard Bru, the story is told of the crucial influence Karl Pearson had on Ronald Fisher through a timely and perspective editorial reply to a hasty and insufficiently considered short submission by Fisher},
author = {Stigler, Stephen M.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Stigler - 2006 - How Ronald Fisher became a mathematical statistician.pdf:pdf},
issn = {1950-6821},
journal = {Mathematics and Social Sciences},
keywords = {Fiducial inference,Maximum likelihood,Parameter,Sufficiency},
number = {4},
pages = {23--30},
title = {{How Ronald Fisher became a mathematical statistician}},
url = {http://msh.revues.org/3631?file=1},
volume = {44},
year = {2006}
}
@article{Berger1987,
abstract = {The problem of testing a point null hypothesis (or a "small interval" null hypothesis) is considered. Of interest is the relationship between the P value (or observed significance level) and conditional and Bayesian measures of evidence against the null hypothesis. Although one might presume that a small P value indicates the presence of strong evidence against the null, such is not necessarily the case. Expanding on earlier work especially Edwards, Lindman, and Savage (1963) and Dickey (1977), it is shown that actual evidence against a null (as measured, say, by posterior probability or comparative likelihood) can differ by an order of magnitude from the P value. For instance, data that yield a P value of .05, when testing a normal mean, result in a posterior probability of the null of at least .30 for any objective prior distribution. ("Objective" here means that equal prior weight is given the two hypotheses and that the prior is symmetric and nonincreasing away from the null; other definitions of "objective" will be seen to yield qualitatively similar results.) The overall conclusion is that P values can be highly misleading measures of the evidence provided by the data against the null hypothesis.},
author = {Berger, J.O. and Sellke, Thomas},
doi = {10.1080/01621459.1987.10478397},
isbn = {01621459},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Bayes factor,P values,Point null hypothesis,Posterior probability,Weighted likelihood ratio},
number = {397},
pages = {112--122},
pmid = {10047091},
title = {{Testing a point null hypothesis: The irreconcilability of P values and evidence}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1987.10478397},
volume = {82},
year = {1987}
}
@article{Savage1976,
abstract = {Fisher's contributions to statistics are surveyed. His background, skills, temperament, and style of thought and writing are sketched. His mathematical and methodological contributions are outlined. More attention is given to the technical concepts he introduced ...},
archivePrefix = {arXiv},
arxivId = {arXiv:1306.3979v1},
author = {Savage, Leonard J.},
doi = {10.1214/aos/1176343456},
eprint = {arXiv:1306.3979v1},
isbn = {00905364},
issn = {2168-8966},
journal = {Annals of Statistics},
keywords = {62-03 Statistical inference R. A. Fisher R. A. Fis},
month = {may},
number = {3},
pages = {441--500},
pmid = {369},
publisher = {Institute of Mathematical Statistics},
title = {{On Rereading R. A. Fisher}},
url = {http://projecteuclid.org/euclid.aos/1176343456},
volume = {4},
year = {1976}
}
@article{Godambe1979,
author = {Godambe, V.P.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Godambe - 1979 - On Birnbaum's Axiom of the Mathematically Equivalent Experiments.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B. (Methodological)},
keywords = {axiom of relabelling,conditionalrry principle,experiment,likelihood,principle,suficiency principle},
number = {1},
pages = {107--110},
title = {{On Birnbaum's Axiom of the Mathematically Equivalent Experiments}},
volume = {41},
year = {1979}
}
@article{Westlake1976,
author = {Westlake, W.J.},
journal = {Biometrics},
number = {4},
pages = {741--744},
title = {{Symmetrical confidence intervals for bioequivalence trials}},
volume = {32},
year = {1976}
}
@article{Farooq2014,
abstract = {Computer programming is the core of computer science curriculum. Several programming languages have been used to teach the first course in computer programming, and such languages are referred to as first programming language (FPL). The pool of programming languages has been evolving with the development of new languages, and from this pool different languages have been used as FPL at different times. Though the selection of an appropriate FPL is very important, yet it has been a controversial issue in the presence of many choices. Many efforts have been made for designing a good FPL, however, there is no ample way to evaluate and compare the existing languages so as to find the most suitable FPL. In this article, we have proposed a framework to evaluate the existing imperative, and object oriented languages for their suitability as an appropriate FPL. Furthermore, based on the proposed framework we have devised a customizable scoring function to compute a quantitative suitability score for a language, which reflects its conformance to the proposed framework. Lastly, we have also evaluated the conformance of the widely used FPLs to the proposed framework, and have also computed their suitability scores.},
author = {Farooq, Muhammad Shoaib and Khan, Sher Afzal and Ahmad, Farooq and Islam, Saeed and Abid, Adnan},
doi = {10.1371/journal.pone.0088941},
isbn = {1932-6203},
journal = {PLoS ONE},
number = {2},
title = {{An Evaluation Framework and Comparative Analysis of the Widely Used First Programming Languages}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3933420/},
volume = {9},
year = {2014}
}
@book{Jackman2009,
abstract = {Bayesian methods are increasingly being used in the social sciences, as the problems encountered lend themselves so naturally to the subjective qualities of Bayesian methodology. This book provides an accessible introduction to Bayesian methods, tailored specifically for social science students. It contains lots of real examples from political science, psychology, sociology, and economics, exercises in all chapters, and detailed descriptions of all the key concepts, without assuming any background in statistics beyond a first course. It features examples of how to implement the methods using W. Bayesian Analysis for the Social Sciences; Contents; List of Figures; List of Tables; Preface; Acknowledgments; Introduction; Part I Introducing Bayesian Analysis; 1 The foundations of Bayesian inference; 2 Getting started: Bayesian analysis for simple models; Part II Simulation Based Bayesian Analysis; 3 Monte Carlo methods; 4 Markov chains; 5 Markov chain Monte Carlo; 6 Implementing Markov chain Monte Carlo; Part III Advanced Applications in the Social Sciences; 7 Hierarchical Statistical Models; 8 Bayesian analysis of choice making; 9 Bayesian approaches to measurement; Part IV Appendices.},
author = {Jackman, Simon},
isbn = {9780470686621},
pages = {564},
publisher = {Wiley},
title = {{Bayesian analysis for the social sciences}},
year = {2009}
}
@techreport{Brooks1998b,
author = {Brooks, Stephen and Gelman, Andrew},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Brooks, Gelman - 1998 - Some issues in Monitoring Convergence of Iterative Simulations.pdf:pdf},
institution = {University of Bristol},
issn = {01672991},
keywords = {conver-,gence diagnosis,importance sampling is,inference,iterations approaches in nity,markov chain monte carlo,one way of seeing,this approximate nature of},
title = {{Some issues in Monitoring Convergence of Iterative Simulations}},
year = {1998}
}
@article{Laplace1774,
address = {Paris},
author = {Laplace, Pierre-Simon},
journal = {M{\'{e}}moires de l'Acad{\'{e}}mie royale des sciences de Paris (Savants {\'{e}}trangers)},
pages = {621--656},
title = {{M{\'{e}}moire sur la probabilit{\'{e}} des causes par les {\'{e}}v{\'{e}}nements}},
volume = {6},
year = {1774}
}
@book{Ravetz1973,
author = {Ravetz, Jerome R.},
isbn = {0140600256},
pages = {449},
publisher = {Penguin Books},
title = {{Scientific Knowledge and Its Social Problems}},
year = {1995}
}
@article{George1997,
abstract = {This paper describes and compares various hierarchical mixture prior formulations of variable selection uncertainty in normal linear regression models. These include the nonconjugate SSVS formulation of George and McCulloch (1993), as well as conjugate formulations which allow for analytical simplification. Hyperparameter settings which base selection on practical significance, and the implications of using mixtures with point priors are discussed. Computational methods for posterior evaluation and exploration are considered. Rapid updating methods are seen to provide feasible methods for exhaustive evaluation using Gray Code sequencing in moderately sized problems, and fast Markov Chain Monte Carlo exploration in large problems. Estimation of normalization constants is seen to provide improved posterior estimates of individual model probabilities and the total visited probability. Various procedures are illustrated on simulated sample problems and on a real problem concerning the construction of financial index tracking portfolios.},
author = {George, Edward I. and McCulloch, Robert E.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/George, McCulloch - 1997 - Approaches for bayesian variable selection.pdf:pdf},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {Conjugate prior,Gibbs sampling,Gray Code,Hierarchical models,Markov chain Monte Carlo,Metropolis-Hastings algorithms,Normal mixtures,Normalization constant,Regression,Simulation},
number = {2},
pages = {339--373},
title = {{Approaches for bayesian variable selection}},
volume = {7},
year = {1997}
}
@book{Reichert2005,
abstract = {Der Autor J. Nievergelt hat vor etwa 15 Jahren eine immer noch gut einsetzbare Einf{\"{u}}hrung in "Datenstrukturen und Algorithmen" geschrieben (BA 8/87), die sich auch f{\"{u}}r den Informatikunterricht in der Sekundarstufe II eignet. Das vorliegende Buch steht in der Tradition dieses fr{\"{u}}heren Werkes. Die Autoren gehen davon aus, dass Computer heute unseren Alltag pr{\"{a}}gen und deshalb die Grundlagen der Informations- und Kommunikationstechnologien, mit anderen Worten ein fundiertes Verst{\"{a}}ndnis f{\"{u}}r die Funktionsweise von Computern und Softwaresystemen zur Allgemeinbildung geh{\"{o}}rt. Gest{\"{u}}tzt auf den programmierbaren Marienk{\"{a}}fer Kara (einem als endlichen Automaten implementierten Roboter) f{\"{u}}hren sie in spielerischer Weise in grundlegende Algorithmen der Informatik ein. Aber auch komplexere Themen wie z.B. die Koordination nebenl{\"{a}}ufiger Prozesse oder die Modellierung zweidimensionaler Turingmaschinen werden ausf{\"{u}}hrlich ber{\"{u}}cksichtigt. Mit oft verbl{\"{u}}ffenden Beispielen wird die Bedeutung dieser Konzepte im Alltag demonstriert. Die Programmierumgebung Kara steht im Internet kostenfrei zur Verf{\"{u}}gung. (1 S) (Klaus Barckow)},
address = {Berlin Heidelberg},
annote = {Literaturverzeichnis: Seite [149] - 150},
author = {Reichert, Raimond and Nievergelt, J{\"{u}}rg and Hartmann, Werner},
booktitle = {eXamen.press},
edition = {Zweite, {\"{u}}b},
isbn = {978-3-540-23819-5},
keywords = {Informatikunterricht,Kara},
language = {ger},
pages = {152},
publisher = {Springer},
shorttitle = {Programmieren mit Kara},
title = {{Programmieren mit Kara: ein spielerischer Zugang zur Informatik}},
year = {2005}
}
@article{Gandenberger2015,
author = {Gandenberger, Greg},
journal = {The British Journal for the Philosophy of Science},
number = {3},
pages = {475--503},
title = {{A New Proof of the Likelihood Principle}},
volume = {66},
year = {2015}
}
@article{epple_2000,
author = {Epple, Moritz},
journal = {Mathematische Semesterberichte},
number = {1},
pages = {131--163},
title = {{Genies, Ideen, Institutionen, mathematische Werkst{\"{a}}tten: Formen der Mathematikgeschichte - Ein metahistorischer Essay}},
volume = {47},
year = {2000}
}
@misc{Barnard1945,
abstract = {IF an experiment yields results in the form of a 2 × 2 table: nullwhere m and n have been fixed in advance, a test for deciding whether there is evidence of association between the attributes A and B and the mutually exclusive and exhaustive attributes P and not-P has been given},
author = {{Barnard G.A.}},
booktitle = {Nature},
pages = {177},
title = {{A new test for 2 x 2 tables}},
volume = {156},
year = {1945}
}
@article{sentance_computing_2017,
author = {Sentance, Sue and Csizmadia, Andrew},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Sentance, Csizmadia - 2017 - Computing in the curriculum Challenges and strategies from a teacher's perspective.pdf:pdf},
journal = {Education and Information Technologies},
number = {2},
pages = {469--495},
shorttitle = {Computing in the curriculum},
title = {{Computing in the curriculum: Challenges and strategies from a teacher's perspective}},
volume = {22},
year = {2017}
}
@article{Heide2020,
archivePrefix = {arXiv},
arxivId = {arXiv:1708.08278v4},
author = {Heide, Rianne De and Gr{\"{u}}nwald, Peter D},
eprint = {arXiv:1708.08278v4},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Heide, Gr{\"{u}}nwald - 2020 - Why optional stopping is a problem for Bayesians.pdf:pdf},
journal = {arXiv preprint},
number = {April},
pages = {1--31},
title = {{Why optional stopping is a problem for Bayesians}},
year = {2020}
}
@article{Fisher1930,
abstract = {ABSTRACT I know only one case in mathematics of a doctrine which has been accepted and developed by the most eminent men of their time, and is now perhaps accepted by men now living, which at the same time has appeared to a succession of sound writers to be fundamentally false and devoid of foundation. Yet that is quite exactly the position in respect of inverse probability. Bayes, who seems to have first attempted to apply the notion of probability, not only to effects in relation to their causes but also to causes in relation to their effects, invented a theory, and evidently doubted its soundness, for he did not publish it during his life. It was posthumously published by Price, who seems to have felt no doubt of its soundness. It and its applications must have made great headway during the next 20 years, for Laplace takes for granted in a highly generalised form what Bayes tentatively wished to postulate in a special case.},
author = {Fisher, R. A.},
doi = {10.1017/S0305004100016297},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1930 - Inverse Probability.pdf:pdf},
isbn = {0305004100016},
issn = {14698064},
journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
number = {4},
pages = {528--535},
title = {{Inverse Probability}},
volume = {26},
year = {1930}
}
@article{FisherRonaldAylmerSir1960,
abstract = {Reproduced with permission of the Journal of the Operations Research Society of Japan.},
author = {Fisher, Ronald Aylmer},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1960 - Scientific thought and the refinement of human reasoning.pdf:pdf},
journal = {Journal of Op. Res. Soc. of Japan},
pages = {1--10},
title = {{Scientific thought and the refinement of human reasoning}},
url = {https://digital.library.adelaide.edu.au/dspace/handle/2440/15278},
volume = {3},
year = {1960}
}
@article{Hamra2013,
abstract = {Markov Chain Monte Carlo (MCMC) methods are increasingly popular among epidemiologists. The reason for this may in part be that MCMC offers an appealing approach to handling some difficult types of analyses. Additionally, MCMC methods are those most commonly used for Bayesian analysis. However, epidemiologists are still largely unfamiliar with MCMC. They may lack familiarity either with he implementation of MCMC or with interpretation of the resultant output. As with tutorials outlining the calculus behind maximum likelihood in previous decades, a simple description of the machinery of MCMC is needed. We provide an introduction to conducting analyses with MCMC, and show that, given the same data and under certain model specifications, the results of an MCMC simulation match those of methods based on standard maximum-likelihood estimation (MLE). In addition, we highlight examples of instances in which MCMC approaches to data analysis provide a clear advantage over MLE. We hope that this brief tutorial will encourage epidemiologists to consider MCMC approaches as part of their analytic tool-kit.},
author = {Hamra, Ghassan and MacLehose, Richard and Richardson, David},
doi = {10.1093/ije/dyt043},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hamra, MacLehose, Richardson - 2013 - Markov chain monte carlo An introduction for epidemiologists.pdf:pdf},
isbn = {0300-5771},
issn = {03005771},
journal = {International Journal of Epidemiology},
month = {apr},
number = {2},
pages = {627--634},
pmid = {23569196},
publisher = {Oxford University Press},
title = {{Markov chain monte carlo: An introduction for epidemiologists}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23569196 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3619958},
volume = {42},
year = {2013}
}
@article{Bartlett1933,
author = {Bartlett, M. S.},
doi = {10.1098/rspa.1933.0136},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bartlett - 1933 - Probability and Chance in the Theory of Statistics.pdf:pdf},
issn = {1364-5021},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
month = {sep},
number = {845},
pages = {518--534},
title = {{Probability and Chance in the Theory of Statistics}},
url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1933.0136},
volume = {141},
year = {1933}
}
@article{McNeish2015,
abstract = {Ordinary least squares and stepwise selection are widespread in behavioral science research; however, these methods are well known to encounter overfitting problems such that R(2) and regression coefficients may be inflated while standard errors and p values may be deflated, ultimately reducing both the parsimony of the model and the generalizability of conclusions. More optimal methods for selecting predictors and estimating regression coefficients such as regularization methods (e.g., Lasso) have existed for decades, are widely implemented in other disciplines, and are available in mainstream software, yet, these methods are essentially invisible in the behavioral science literature while the use of sub optimal methods continues to proliferate. This paper discusses potential issues with standard statistical models, provides an introduction to regularization with specific details on both Lasso and its related predecessor ridge regression, provides an example analysis and code for running a Lasso analysis in R and SAS, and discusses limitations and related methods.},
author = {McNeish, Daniel M.},
doi = {10.1080/00273171.2015.1036965},
issn = {0027-3171},
journal = {Multivariate Behavioral Research},
keywords = {lasso,overfitting,regression,regularization},
month = {sep},
number = {5},
pages = {471--484},
pmid = {26610247},
publisher = {Psychology Press Ltd},
title = {{Using Lasso for Predictor Selection and to Assuage Overfitting: A Method Long Overlooked in Behavioral Sciences}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26610247 http://www.tandfonline.com/doi/full/10.1080/00273171.2015.1036965},
volume = {50},
year = {2015}
}
@article{Zieba2014,
abstract = {In this paper, we present boosted SVM dedicated to solve imbalanced data problems. Proposed solution combines the benefits of using ensemble classifiers for uneven data together with cost-sensitive support vectors machines. Further, we present oracle-based approach for extracting decision rules from the boosted SVM. In the next step we examine the quality of the proposed method by comparing the performance with other algorithms which deal with imbalanced data. Finally, boosted SVM is used for medical application of predicting post-operative life expectancy in the lung cancer patients. {\textcopyright} 2013 Elsevier B.V.},
author = {Zieba, Maciej and Tomczak, Jakub M. and Lubicz, Marek and {\'{S}}wiatek, Jerzy},
doi = {10.1016/j.asoc.2013.07.016},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Boosted SVM,Decision rules,Imbalanced data,Post-operative life expectancy prediction},
month = {jan},
number = {PART A},
pages = {99--108},
publisher = {Elsevier},
title = {{Boosted SVM for extracting rules from imbalanced data in application to prediction of the post-operative life expectancy in the lung cancer patients}},
volume = {14},
year = {2014}
}
@incollection{PhillipsSmith1996,
address = {London},
author = {Phillips, D.M. and Smith, A.F.M.},
booktitle = {Markov-Chain-Monte-Carlo in Practice},
editor = {Gilks, W. R. and Richardson, S. and Spiegelhalter, D. J.},
pages = {215--240},
publisher = {Chapman {\&} Hall},
title = {{Bayesian model comparison via jump diffusions}},
year = {1996}
}
@article{Trafimow2019,
abstract = {ABSTRACTThe American Statistical Association's Symposium on Statistical Inference (SSI) included a session on how editorial practices should change in a universe no longer dominated by null hypothesis significance testing (NHST). The underlying assumptions were first, that NHST is problematic; and second, that editorial practices really should change. The present article is based on my talk in this session, and on these assumptions. Consistent with the spirit of the SSI, my focus is not on what reviewers and editors should not do (e.g., NHST) but rather on what they should do, with an emphasis on changes that are not obvious. The recommended changes include a wider consideration of the nature of the contribution than submitted manuscripts usually receive; a greater tolerance of ambiguity; more of an emphasis on the thinking and execution of the study, with a decreased emphasis on the findings; replacing NHST with the a priori procedure; and a call for reviewers and editors to recognize that there are many...},
author = {Trafimow, David},
doi = {10.1080/00031305.2018.1537888},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Trafimow - 2019 - Five Nonobvious Changes in Editorial Practice for Editors and Reviewers to Consider When Evaluating Submissions in a P.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {a priori procedure,editorial,editorial changes,practices},
number = {sup1},
pages = {340--345},
title = {{Five Nonobvious Changes in Editorial Practice for Editors and Reviewers to Consider When Evaluating Submissions in a Post {\textless}i{\textgreater}p{\textless}/i{\textgreater}  {\textless} 0.05 Universe}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1537888},
volume = {73},
year = {2019}
}
@techreport{Coro2017,
address = {Pisa},
author = {Coro, Gianpaolo},
institution = {Institute of Science and Technology, University of Pisa},
title = {{A Lightweight Guide on Gibbs Sampling and JAGS}},
year = {2017}
}
@book{Deck2006,
abstract = {Literaturverz. S. 241 - 243. Dieses Buch behandelt stochastische Integrale bezüglich der Brownschen Bewegung, den daraus resultierenden Itôschen Differentialkalkül und einige Anwendungen. Ausgezeichnet für Anwender: Der Autor minimiert die mathematischen Voraussetzungen, und er entwickelt den Itô-Kalkül im ersten Schritt ohne Martingale. Zwei Besonderheiten, die dafür sorgen, dass tiefer liegende stochastische Methoden zunächst nicht benötigt werden. Erst im zweiten Schritt thematisiert das Buch die engen Beziehungen zur Martingaltheorie und zur Browschen Bewegung. Anwendungen auf stochastische Differentialgleichungen und Optionspreistheorie runden den Text ab.},
author = {Deck, Thomas.},
isbn = {3540253920},
publisher = {Springer},
title = {{Der Ito-Kalk{\"{u}}l : Einf{\"{u}}hrung und Anwendungen}},
year = {2006}
}
@book{pearson_grammarOfScience1892,
author = {Pearson, Karl},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Pearson - 1892 - The Grammar of Science.pdf:pdf},
publisher = {Walter Scott},
title = {{The Grammar of Science}},
year = {1892}
}
@article{Kelter2020BMCJasp,
abstract = {Although null hypothesis significance testing (NHST) is the agreed gold standard in medical decision making and the most widespread inferential framework used in medical research, it has several drawbacks. Bayesian methods can complement or even replace frequentist NHST, but these methods have been underutilised mainly due to a lack of easy-to-use software. JASP is an open-source software for common operating systems, which has recently been developed to make Bayesian inference more accessible to researchers, including the most common tests, an intuitive graphical user interface and publication-ready output plots. This article provides a non-technical introduction to Bayesian hypothesis testing in JASP by comparing traditional tests and statistical methods with their Bayesian counterparts. The comparison shows the strengths and limitations of JASP for frequentist NHST and Bayesian inference. Specifically, Bayesian hypothesis testing via Bayes factors can complement and even replace NHST in most situations in JASP. While p-values can only reject the null hypothesis, the Bayes factor can state evidence for both the null and the alternative hypothesis, making confirmation of hypotheses possible. Also, effect sizes can be precisely estimated in the Bayesian paradigm via JASP. Bayesian inference has not been widely used by now due to the dearth of accessible software. Medical decision making can be complemented by Bayesian hypothesis testing in JASP, providing richer information than single p-values and thus strengthening the credibility of an analysis. Through an easy point-and-click interface researchers used to other graphical statistical packages like SPSS can seemlessly transition to JASP and benefit from the listed advantages with only few limitations.},
author = {Kelter, Riko},
doi = {10.1186/s12874-020-00980-6},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kelter - 2020 - Bayesian alternatives to null hypothesis significance testing in biomedical research a non-technical introduction to Bay.pdf:pdf},
issn = {1471-2288},
journal = {BMC Medical Research Methodology},
keywords = {Health Sciences,Medicine,Statistical Theory and Methods,Statistics for Life Sciences,Theory of Medicine/Bioethics},
number = {1},
publisher = {BioMed Central},
title = {{Bayesian alternatives to null hypothesis significance testing in biomedical research: a non-technical introduction to Bayesian inference with JASP}},
volume = {20},
year = {2020}
}
@article{pearson_1907,
author = {Pearson, Karl},
doi = {10.1080/14786440709463611},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Pearson - 1907 - On the Influence of Past Experience on Future Expectation.pdf:pdf},
isbn = {1478644070946},
issn = {1941-5982},
journal = {Philosophical Magazine Series 6},
number = {75},
pages = {365--378},
title = {{On the Influence of Past Experience on Future Expectation}},
url = {http://www.tandfonline.com/doi/abs/10.1080/14786440709463611},
volume = {13},
year = {1907}
}
@book{Pierce2002,
abstract = {A type system is a syntactic method for automatically checking the absence of certain erroneous behaviors by classifying program phrases according to the kinds of values they compute. The study of type systems-and of programming languages from a type-theoretic perspective-has important applications in software engineering, language design, high-performance compilers, and security.{\textless}br {\textless}br This text provides a comprehensive introduction both to type systems in computer science and to the basic theory of programming languages. The approach is pragmatic and operational; each new concept is motivated by programming examples and the more theoretical sections are driven by the needs of implementations. Each chapter is accompanied by numerous exercises and solutions, as well as a running implementation, available via the Web. Dependencies between chapters are explicitly identified, allowing readers to choose a variety of paths through the material.{\textless}br {\textless}br The core topics include the untyped lambda-calculus, simple type systems, type reconstruction, universal and existential polymorphism, subtyping, bounded quantification, recursive types, kinds, and type operators. Extended case studies develop a variety of approaches to modeling the features of object-oriented languages.},
author = {Pierce, Benjamin},
booktitle = {ACM SIGPLAN Notices},
doi = {10.1145/360271.360273},
isbn = {0262162091},
issn = {03621340},
number = {8},
pages = {20--30},
publisher = {MIT Press},
title = {{Types and Programming Languages}},
url = {http://portal.acm.org/citation.cfm?doid=360271.360273},
volume = {35},
year = {2000}
}
@incollection{Gaerdenfors1995,
address = {Oxford},
author = {G{\"{a}}rdenfors, Peter and Rott, Hans},
booktitle = {Handbook of Logic in Artificial Intelligence and Logic Programming, Volume 4: Epistemic and Temporal Logics},
pages = {35--132},
publisher = {Oxford University Press},
title = {{Belief Revision}},
year = {1995}
}
@article{Kruschke2015a,
abstract = {Bayesian data analysis involves describing data by meaningful mathematical models, and allocating credibility to parameter values that are consistent with the data and with prior knowledge. The Bayesian approach is ideally sui ted for constructing hierarchical models, which are useful for data structures with multiple levels, such as data from individuals who are members of groups which in turn are in higher-level organizations. Hierarchical models have parameters that meaningfully describe the data at their multiple levels and connect information within and across levels. Bayesian methods are very flexible and straightforward for estimating parameters of complex hierarchical models (and simpler models too). We provide an introduction to the ideas of hierarc hical models and to the Bayesian estimation of their parameters, illustrated with two extended examples. One example considers baseball batting averages of individual players grouped by fielding position. A second example uses a hierarchical extension of a cognitive process model to examine individual differences in attention allocation of people who have eating disorders. We conclude by discussing Bayesian model comparison as a case of hierarchical modeling.},
author = {Kruschke, John K. and Vanpaemel, W},
doi = {10.1093/oxfordhb/9780199957996.013.13},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kruschke, Vanpaemel - 2015 - Bayesian estimation in hierarchical models.pdf:pdf},
isbn = {9780199957996},
journal = {The Oxford Handbook of Computational and Mathematical Psychology},
keywords = {icle},
number = {May 2016},
pages = {279--299},
title = {{Bayesian estimation in hierarchical models}},
year = {2015}
}
@article{Halpin2006,
abstract = {The application of statistical testing in psychological research over the period of 1940-1960 is examined in order to address psychologists' reconciliation of the extant controversy between the Fisher and Neyman-Pearson approaches. Textbooks of psychological statistics and the psychological journal literature are reviewed to examine the presence of what Gigerenzer (1993) called a hybrid model of statistical testing. Such a model is present in the textbooks, although the mathematically incomplete character of this model precludes the appearance of a similarly hybridized approach to statistical testing in the research literature. The implications of this hybrid model for psychological research and the statistical testing controversy are discussed.},
author = {Halpin, Peter F. and Stam, Henderikus J.},
doi = {10.2307/20445367},
issn = {00029556},
journal = {The American Journal of Psychology},
number = {4},
pages = {625--653},
publisher = {University of Illinois Press},
title = {{Inductive Inference or Inductive Behavior: Fisher and Neyman: Pearson Approaches to Statistical Testing in Psychological Research (1940-1960)}},
volume = {119},
year = {2006}
}
@article{Gelman2015,
abstract = {Stan is a free and open-source C++ program that performs Bayesian inference or optimization for arbitrary user-specified models and can be called from the command line, R, Python, Matlab, or Julia and has great promise for fitting large and complex statistical models in many areas of application. We discuss Stan from users' and developers' perspectives and illustrate with a simple but nontrivial nonlinear regression example.},
author = {Gelman, Andrew and Lee, Daniel and Guo, Jiqiang},
doi = {10.3102/1076998615606113},
journal = {Journal of Educational and Behavioral Statistics},
month = {oct},
number = {5},
pages = {530--543},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Stan: A Probabilistic Programming Language for Bayesian Inference}},
url = {http://journals.sagepub.com/doi/10.3102/1076998615606113},
volume = {40},
year = {2015}
}
@article{Lakens2014,
abstract = {{\textcopyright} 2014 John Wiley {\&} Sons, Ltd. Running studies with high statistical power, while effect size estimates in psychology are often inaccurate, leads to a practical challenge when designing an experiment. This challenge can be addressed by performing sequential analyses while the data collection is still in progress. At an interim analysis, data collection can be stopped whenever the results are convincing enough to conclude that an effect is present, more data can be collected, or the study can be terminated whenever it is extremely unlikely that the predicted effect will be observed if data collection would be continued. Such interim analyses can be performed while controlling the Type 1 error rate. Sequential analyses can greatly improve the efficiency with which data are collected. Additional flexibility is provided by adaptive designs where sample sizes are increased on the basis of the observed effect size. The need for pre-registration, ways to prevent experimenter bias, and a comparison between Bayesian approaches and null-hypothesis significance testing (NHST) are discussed. Sequential analyses, which are widely used in large-scale medical trials, provide an efficient way to perform high-powered informative experiments. I hope this introduction will provide a practical primer that allows researchers to incorporate sequential analyses in their research.},
author = {Lakens, Dani"el},
doi = {10.1002/ejsp.2023},
issn = {10990992},
journal = {European Journal of Social Psychology},
number = {7},
pages = {701--710},
title = {{Performing high-powered studies efficiently with sequential analyses}},
volume = {44},
year = {2014}
}
@book{Megginson1998,
address = {New York, NY},
author = {Megginson, Robert E.},
doi = {10.1007/978-1-4612-0603-3},
isbn = {978-1-4612-6835-2},
publisher = {Springer New York},
series = {Graduate Texts in Mathematics},
title = {{An Introduction to Banach Space Theory}},
volume = {183},
year = {1998}
}
@incollection{Geisser1980,
address = {New York},
author = {Geisser, Seymour},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_7},
editor = {Fienberg, Stephen E. and Hinkley, David V.},
pages = {59--66},
publisher = {Springer Verlag New York},
title = {{Basic Theory of the 1922 Mathematical Statistics Paper}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}7},
year = {1980}
}
@article{Pearson1900a,
abstract = {The object of this paper is to investigate a criterion of the probability on any theory of an observed system of errors, and to apply it to the determination of goodness of fit in the case of frequency curves.},
author = {Pearson, Karl},
doi = {10.1080/14786440009463897},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Pearson - 1900 - On the criterion that a given system of deviations from the probable in the case of a correlated system of variables(2).pdf:pdf},
isbn = {1941-5982},
issn = {1941-5982},
journal = {Philosophical Magazine Series 5},
number = {302},
pages = {157--175},
title = {{On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling}},
url = {http://www.tandfonline.com/doi/abs/10.1080/14786440009463897},
volume = {50},
year = {1900}
}
@article{Berger1987a,
abstract = {Testing of precise (point or small interval) hypotheses is reviewed, with special emphasis placed on exploring the dramatic conflict between conditional measures (Bayes factors and posterior probabilities) and the classical P-value (or observed significance level). This conflict is highlighted by finding lower bounds on the conditional measures over wide classes of priors, in normal and binomial situations, lower bounds, which are much larger than the P-value; this leads to the recommendation of several alternatives to P-values. Results are also given concerning the validity of approximating an interval null by a point null. The overall discussion features critical examination of issues such as the probability of objective testing and the possibility of testing from confidence sets. {\textcopyright} 1987, Institute of Mathematical Statistics. All Rights Reserved.},
author = {Berger, J.O. and Delampady, Mohan},
doi = {10.1214/ss/1177013238},
issn = {08834237},
journal = {Statistical Science},
keywords = {Bayes factor,Binomial tests,Jeffreys's paradox,Objectivity,P-value,Point null hypothesis,Posterior probability,Robust bayesian analysis,Scientific communication,$\chi$2 tests},
number = {3},
pages = {317--335},
publisher = {Institute of Mathematical Statistics},
title = {{Testing precise hypotheses}},
volume = {2},
year = {1987}
}
@article{FDAAdaptiveBayesianDesignsForClinicalTrials2019,
author = {{U.S. Food and Drug Administration Center for Drug Evaluation and Research and Center for Biologics Evaluation and Research}},
institution = {U.S. Department of Health and Human Services},
journal = {Web archive: https://www.fda.gov/media/78495/download (accessed 01/03/2021)},
title = {{Adaptive Designs for Clinical Trials of Drugs and Biologics: Guidance for Industry}},
year = {2019}
}
@inproceedings{Fisher1924a,
abstract = {This paper was written to call attention to the fact that many recently solved problems of distribution involve only a single family of distributions, that of X2, z, and t. It is concerned to bring out the mathematical basis of these, their mutual relationships, and the principal applications then known. It is reprinted with minor adjustments of notation.},
author = {Fisher, Ronald Aylmer},
booktitle = {Proceedings of the International Congress of Mathematics},
doi = {10.15288/jsad.2011.72.199},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1924 - On a Distribution Yielding the Error Functions of Several Well Known Statistics.pdf:pdf},
issn = {1937-1888},
pages = {805--813},
title = {{On a distribution yielding the error functions of several well known statistics}},
url = {https://digital.library.adelaide.edu.au/dspace/handle/2440/15183},
year = {1924}
}
@article{Wilkinson1977,
author = {Wilkinson, G.N.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Wilkinson - 1977 - On Resolving the Controvery in Statistical Inference.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
number = {2},
pages = {119--171},
title = {{On Resolving the Controvery in Statistical Inference}},
volume = {39},
year = {1977}
}
@article{Dawid1984,
author = {Dawid, A.P.},
journal = {Journal of the Royal Stastical Society Series A},
number = {Part 2},
pages = {278--292},
title = {{Statistical Theory: The Prequential Approach}},
volume = {147},
year = {1984}
}
@book{Wilcox2005,
abstract = {This revised book provides a thorough explanation of the foundation of robust methods, incorporating the latest updates on R and S-Plus, robust ANOVA (Analysis of Variance) and regression. It guides advanced students and other professionals through the basic strategies used for developing practical solutions to problems, and provides a brief background on the foundations of modern methods, placing the new methods in historical context. Author Rand Wilcox includes chapter exercises and many real-world examples that illustrate how various methods perform in different situations. Introduction to Robust Estimation and Hypothesis Testing, Second Edition, focuses on the practical applications of modern, robust methods which can greatly enhance our chances of detecting true differences among groups and true associations among variables.},
author = {Wilcox, Rand},
booktitle = {Technometrics},
doi = {10.1198/tech.2005.s334},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Wilcox - 2005 - Introduction to Robust Estimation and Hypothesis Testing.pdf:pdf},
isbn = {9780123869838},
issn = {0040-1706},
number = {4},
pages = {525--526},
pmid = {5268247},
title = {{Introduction to Robust Estimation and Hypothesis Testing}},
volume = {47},
year = {2005}
}
@article{Fisher1948,
author = {Fisher, R.A.},
journal = {Annales de l'I.H.P.},
pages = {191--213},
title = {{Conclusions fiduciaires}},
volume = {3},
year = {1948}
}
@incollection{Pearson1966,
address = {London},
author = {Pearson, Egon Sharpe},
booktitle = {Research Papers in Statistics (Festschrift for J. Neyman)},
editor = {David, F.},
publisher = {Wiley},
title = {{The Neyman-Pearson story}},
year = {1966}
}
@article{Duane1987,
abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.},
author = {Duane, Simon and Kennedy, A.D. and Pendleton, Brian J. and Roweth, Duncan},
doi = {10.1016/0370-2693(87)91197-X},
issn = {0370-2693},
journal = {Physics Letters B},
month = {sep},
number = {2},
pages = {216--222},
publisher = {North-Holland},
title = {{Hybrid Monte Carlo}},
url = {https://www.sciencedirect.com/science/article/pii/037026938791197X},
volume = {195},
year = {1987}
}
@book{Pearl2009,
abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 5,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.},
address = {New York},
author = {Pearl, Judea},
booktitle = {Causality: Models, Reasoning, and Inference, Second Edition},
doi = {10.1017/CBO9780511803161},
isbn = {9780511803161},
pages = {1--464},
publisher = {Cambridge University Press},
title = {{Causality: Models, reasoning, and inference, second edition}},
year = {2009}
}
@article{Sandve2013,
abstract = {a},
author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
doi = {10.1371/journal.pcbi.1003285},
editor = {Bourne, Philip E.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Sandve et al. - 2013 - Ten Simple Rules for Reproducible Computational Research.pdf:pdf},
isbn = {1553-7358 (Electronic)$\backslash$r1553-734X (Linking)},
issn = {1553734X},
journal = {PLoS Computational Biology},
month = {oct},
number = {10},
pages = {e1003285},
pmid = {24204232},
publisher = {Public Library of Science},
title = {{Ten Simple Rules for Reproducible Computational Research}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1003285},
volume = {9},
year = {2013}
}
@inproceedings{Nayak1996,
address = {New York},
author = {Nayak, Abhaya C and Pagnucco, Maurice and Foo, Norman Y and Peppas, Pavlos},
booktitle = {ECAI},
pages = {75--79},
publisher = {Wiley},
title = {{Learning From Conditionals: Judy Benjamin's Other Problems.}},
year = {1996}
}
@inproceedings{yang_enhancing_2015,
abstract = {While Object-Oriented programming in Java has been widely adopted as an introductory programming course in Computer Science, it is considered difficult to teach and learn. Studies have identified that the difficulty is from the underlying Object-Oriented concepts and principles. To help student programmers better understand the structure of a program and the concepts of Object-Oriented design, visualizations in various formats have been applied to programming environments. This paper presents a web-based interactive educational programming environment, JavlinaCode, and its unique design principles. JavlinaCode is designed for teaching object-oriented programming in Java. It aims to enhance student programmers' programming skill and to help them understand object-oriented design concepts. It provides integrated static and dynamic visualizations: the static state of a Java program in an UML class diagram and the dynamic run-time state of the program execution. With the synchronized multi-view real time visualization along with source code, JavelinaCode is highly expected to reduce student programmers' cognitive workload in Java programming and to enhance comprehension of the objectoriented programming and design concepts.},
annote = {USA},
author = {Yang, J and Lee, Y and Hicks, D and Chang, K H},
booktitle = {2015 IEEE Frontiers in Education Conference (FIE)},
doi = {10.1109/FIE.2015.7344152},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2015 - Enhancing object-oriented programming education using static and dynamic visualization.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2015 - Enhancing object-oriented programming education using static and dynamic visualization.html:html},
keywords = {Computer science education,Data visualization,Java programming,JavlinaCode,Programming profession,UML class diagram,Unified Modeling Language,Visualization,Web-based interactive educational programming env,computer science,dynamic visualization,introductory programming course,java,object-oriented design concepts,object-oriented programming,object-oriented programming education,program execution,programming environments,source code,static visualization,student programmer programming skill,student programmers,synchronized multiview real time visualization,teaching},
month = {oct},
pages = {1--5},
title = {{Enhancing object-oriented programming education using static and dynamic visualization}},
year = {2015}
}
@article{Anderson1986,
author = {Anderson, Herbert L.},
journal = {Los Alamos Science},
pages = {96--108},
title = {{Metropolis, Monte Carlo, and the MANIAC}},
volume = {14},
year = {1986}
}
@book{Lehmann1998,
abstract = {Second edition / E.L. Lehmann, George Casella. This second, much enlarged edition by Lehmann and Casella of Lehmann's classic text on point estimation maintains the outlook and general style of the first edition. All of the topics are updated. An entirely new chapter on Bayesian and hierarchical Bayesian approaches is provided, and there is much new material on simultaneous estimation. Each chapter concludes with a Notes section which contains suggestions for further study. The book is a companion volume to the second edition of Lehmann's "Testing Statistical Hypotheses". E.L. Lehmann is Professor Emeritus at the University of California, Berkeley. He is a member of the National Academy of Sciences and the American Academy of Arts and Sciences, and the recipient of honorary degrees from the University of Leiden, The Netherlands, and the University of Chicago. George Casella is the Liberty Hyde Bailey Professor of Biological Statistics in The College of Agriculture and Life Sciences at Cornell University. Casella has served as associate editor of The American Statistician, Statistical Science and JASA. He is currently the Theory and Methods Editor of JASA. Casella has authored two other textbooks (Statistical Inference, 1990, with Roger Berger and Variance Components, 1992, with Shayle A. Searle and Charles McCulloch). He is a fellow of the IMS and ASA, and an elected fellow of the ISI. Also available: E.L. Lehmann, Testing Statistical Hypotheses Second Edition, Springer- Verlag New York, Inc., ISBN 0-387-94919-4. Cover -- Table of Contents -- List of Tables -- List of Figures -- List of Examples -- Table of Notation -- 1. Preparations -- 2. Unbiasedness -- 3. Equivariance -- 4. Average Risk Optimality -- 5. Minimaxity and Admissibility -- 6. Asymptotic Optimality -- References -- Author Index.},
address = {New York},
author = {Lehmann, E. L. (Erich Leo) and Casella, George},
isbn = {0387985026},
pages = {589},
publisher = {Springer},
title = {{Theory of point estimation.}},
year = {1998}
}
@incollection{Gelfand1996,
address = {London},
author = {Gelfand, A.},
booktitle = {Markov Chain Monte Carlo in Practice},
chapter = {9},
pages = {145--161},
publisher = {Chapman {\&} Hall},
title = {{Model determination using sampling-based methods}},
year = {1996}
}
@inproceedings{mussai_animation_2012,
abstract = {An animation as an illustrate tool for learning concepts in oop was created. It based on previous research that recognized misconceptions in inheritance and Our research checked how viewing the animation impacted on the CS students' understanding the main concepts of oop. The main finding was that viewing the animation improved the CS students' performance, especially the concept: casting. It especially helped them to build proper programming modules.},
address = {New York, NY, USA},
author = {Mussai, Yael and Liberman, Neomi},
booktitle = {Proceedings of the 17th ACM Annual Conference on Innovation and Technology in Computer Science Education},
doi = {10.1145/2325296.2325403},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Mussai, Liberman - 2012 - An Animation As an Illustrate Tool for Learning Concepts in Oop.pdf:pdf},
isbn = {978-1-4503-1246-2},
keywords = {Animation,casting.,inheritance,misconception,object oriented programming,polymorphism},
pages = {386},
publisher = {ACM},
series = {{\{}ITiCSE{\}} '12},
title = {{An Animation As an Illustrate Tool for Learning Concepts in Oop}},
url = {http://doi.acm.org/10.1145/2325296.2325403},
year = {2012}
}
@article{Besag1974,
abstract = {The formulation of conditional probability models for finite systems of spatially interacting random variables is examined. A simple alternative proof of the Hammersley-Clifford theorem is presented and the theorem is then used to construct specific spatial schemes on and off the lattice. Particular emphasis is placed upon practical applications of the models in plant ecology when the variates are binary or Gaussian. Some aspects of infinite lattice Gaussian processes are discussed. Methods of statistical analysis for lattice schemes are proposed, including a very flexible coding technique. The methods are illustrated by two numerical examples. It is maintained throughout that the conditional probability approach to the specification and analysis of spatial interaction is more attractive than the alternative joint probability approach. Keywords},
author = {Besag, Julian},
doi = {10.1111/j.2517-6161.1974.tb00999.x},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
number = {2},
pages = {192--225},
title = {{Spatial Interaction and the Statistical Analysis of Lattice Systems}},
volume = {36},
year = {1974}
}
@article{Turing1942,
archivePrefix = {arXiv},
arxivId = {1505.04714},
author = {Turing, Alan M.},
eprint = {1505.04714},
journal = {National Archives of the UK},
number = {HW 25/37},
title = {{The Applications of Probability to Cryptography}},
url = {http://arxiv.org/abs/1505.04714},
year = {1942}
}
@article{Hubbard2019a,
author = {Hubbard, Raymond},
doi = {10.1080/00031305.2018.1497540},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hubbard - 2019 - Will the ASA's Efforts to Improve Statistical Practice be Successful Some Evidence to the Contrary.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Citation analysis,Null hypothesis significance testing,Statistical practice,Teaching statistics,citation analysis,hypothesis significance,null,p- values,p-Values,statistical,testing},
number = {sup1},
pages = {31--35},
publisher = {Taylor {\&} Francis},
title = {{Will the ASA's Efforts to Improve Statistical Practice be Successful? Some Evidence to the Contrary}},
volume = {73},
year = {2019}
}
@article{A.J.Matthews2001,
abstract = {There is a growing awareness of Bayesian methods within the medical research community, and increasing discussion of their potential applications. This interest has, however, so far failed to convert into the routine use of such methods by working clinicians. I argue that attempts to encourage the use of Bayesian methods by highlighting the deficiencies of conventional (frequentist) inference have not succeeded because these deficiencies typically have minor practical consequences, while their more serious effects can usually be explained away by appeal to other statistical issues. As a result, Bayesian methods have not appeared to offer practical pay-offs big enough to justify the cost of acquiring the necessary expertise. In an attempt to remove this "cost-benefit" hurdle, I outline a simple Bayesian technique that can be used alongside frequentist methods to address an issue of routine practical concern to working clinicians: the credibility of new research findings. {\textcopyright} 2001 Elsevier Science B.V.},
author = {Matthews, Robert},
doi = {10.1016/S0378-3758(00)00232-9},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Matthews - 2001 - Why should clinicians care about Bayesian methods.pdf:pdf},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
month = {mar},
number = {1},
pages = {43--58},
publisher = {North-Holland},
title = {{Why should clinicians care about Bayesian methods?}},
url = {https://www.sciencedirect.com/science/article/pii/S0378375800002329?via{\%}3Dihub},
volume = {94},
year = {2001}
}
@article{EvansWagenmakers2020,
author = {Evans, Nathan J and Wagenmakers, Eric-Jan},
doi = {10.1093/brain/awz096},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Evans, Wagenmakers - 2019 - Theoretically meaningful models can answer clinically relevant questions.pdf:pdf},
issn = {14602156},
journal = {Brain},
number = {5},
pages = {1170--1172},
pmid = {31032845},
title = {{Theoretically meaningful models can answer clinically relevant questions}},
volume = {142},
year = {2019}
}
@article{Diaconis2009,
abstract = {The use of simulation for high-dimensional intractable computa-tions has revolutionized applied mathematics. Designing, improving and un-derstanding the new tools leads to (and leans on) fascinating mathematics, from representation theory through micro-local analysis.},
author = {Diaconis, Persi},
doi = {10.1090/S0273-0979-08-01238-X},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Diaconis - 2009 - the Markov Chain Monte Carlo Revolution.pdf:pdf},
isbn = {0273-0979},
issn = {0273-0979},
journal = {Bulletin of the American Mathematical Society},
number = {208},
pages = {179--205},
pmid = {263433700002},
title = {{The Markov Chain Monte Carlo Revolution}},
volume = {46},
year = {2009}
}
@article{Gubernatis2005,
abstract = {The 1953 publication, “Equation of State Calculations by Very Fast Computing Machines” by N. Metropolis, A. W. Rosenbluth and M. N. Rosenbluth, and M. Teller and E. Teller [J. Chem. Phys.21, 1087 (1953)] marked the beginning of the use of the Monte Carlo method for solving problems in the physical sciences. The method described in this publication subsequently became known as the Metropolis algorithm, undoubtedly the most famous and most widely used Monte Carlo algorithm ever published. As none of the authors made subsequent use of the algorithm, they became unknown to the large simulation physics community that grew from this publication and their roles in its development became the subject of mystery and legend. At a conference marking the 50th anniversary of the 1953 publication, Marshall Rosenbluth gave his recollections of the algorithm's development. The present paper describes the algorithm, reconstructs the historical context in which it was developed, and summarizes Marshall's recollections.},
author = {Gubernatis, J. E.},
doi = {10.1063/1.1887186},
isbn = {1070664X},
issn = {1070664X},
journal = {Physics of Plasmas},
number = {5},
pages = {1--5},
title = {{Marshall Rosenbluth and the Metropolis algorithm}},
volume = {12},
year = {2005}
}
@article{Fisher1925a,
abstract = {ABSTRACT It has been pointed out to me that some of the statistical ideas employed in the following investigation have never received a strictly logical definition and analysis. The idea of a frequency curve, for example, evidently implies an infinite hypothetical population distributed in a definite manner; but equally evidently the idea of an infinite hypothetical population requires a more precise logical specification than is contained in that phrase. The same may be said of the intimately connected idea of random sampling. These ideas have grown up in the minds of practical statisticians and lie at the basis especially of recent work; there can be no question of their pragmatic value. It was no part of my original intention to deal with the logical bases of these ideas, but some comments which Dr Burnside has kindly made have convinced me that it may be desirable to set out for criticism the manner in which I believe the logical foundations of these ideas may be established.},
archivePrefix = {arXiv},
arxivId = {0-05-002170-2},
author = {Fisher, Ronald Aylmer},
doi = {10.1017/S0305004100009580},
eprint = {0-05-002170-2},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1925 - Theory of Statistical Estimation(2).pdf:pdf},
isbn = {0305004100009},
issn = {14698064},
journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
month = {jul},
number = {05},
pages = {700},
pmid = {1756371},
publisher = {Cambridge University Press},
title = {{Theory of Statistical Estimation}},
url = {http://www.journals.cambridge.org/abstract{\_}S0305004100009580},
volume = {22},
year = {1925}
}
@article{Akaike1974,
abstract = {The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximurn likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AlC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples. {\textcopyright} 1974, IEEE. All rights reserved.},
author = {Akaike, Hirotugu},
doi = {10.1109/TAC.1974.1100705},
issn = {15582523},
journal = {IEEE Transactions on Automatic Control},
number = {6},
pages = {716--723},
title = {{A New Look at the Statistical Model Identification}},
volume = {19},
year = {1974}
}
@article{fienberg2006,
abstract = {While Bayes' theorem has a 250-year history, and the method of inverse probability that flowed from it dominated statistical thinking into the twentieth century, the adjective “Bayesian” was not part of the statistical lexicon until relatively recently. This paper provides an overview of key Bayesian developments, beginning with Bayes' posthumously published 1763 paper and continuing up through approximately 1970, including the period of time when “Bayesian” emerged as the label of choice for those who advocated Bayesian methods.},
author = {Fienberg, Stephen E.},
doi = {10.1214/06-BA101},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fienberg - 2006 - When did Bayesian inference become Bayesian.pdf:pdf},
isbn = {1936-0975},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Bayes theorem,Classical statistical methods,Frequentist methods,Inverse probability,Neo-bayesian revival,Stiglers law of eponymy,Subjective probability},
number = {1 A},
pages = {1--40},
title = {{When did Bayesian inference become "Bayesian"?}},
volume = {1},
year = {2006}
}
@article{Tiku1971,
author = {Tiku, M.L.},
journal = {Journal of the American Statistical Association},
number = {336},
title = {{Power Function of the F-test Under Non-Normal Situations}},
volume = {66},
year = {1971}
}
@article{hubwieser_perspectives_2014,
abstract = {In view of the recent developments in many countries, for example, in the USA and in the UK, it appears that computer science education (CSE) in primary or secondary schools (K-12) has reached a significant turning point, shifting its focus from ICT-oriented to rigorous computer science concepts. The goal of this special issue is to offer a publication platform for soundly based in-depth experiences that have been made around the world with concepts, approaches, or initiatives that aim at supporting this shift. For this purpose, the article format was kept as large as possible, enabling the authors to explain many facets of their concepts and experiences in detail. Regarding the structure of the articles, we had encouraged the authors to lean on the Darmstadt Model, a category system that was developed to support the development, improvement, and investigation of K-12 CSE across regional or national boundaries. This model could serve as a unifying framework that might provide a proper structure for a well-founded critical discussion about the future of K-12 CSE. Curriculum designers or policy stakeholders, who have to decide, which approach an upcoming national initiative should follow, could benefit from this discussion as well as researchers who are investigating K12 CSE in any regard. With this goal in mind, we have selected six extensive and two short case studies from the UK, New Zealand, USA/Israel, France, Sweden, Georgia (USA), Russia, and Italy that provide an in-depth analysis of K-12 CSE in their respective country or state.},
annote = {R, Germany, Israel, Norway, Austria},
author = {Hubwieser, Peter and Armoni, Michal and Giannakos, Michail N and Mittermeir, Roland T},
doi = {10.1145/2602482},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hubwieser et al. - 2014 - Perspectives and Visions of Computer Science Education in Primary and Secondary (K-12) Schools.pdf:pdf},
issn = {1946-6226},
journal = {Trans. Comput. Educ.},
keywords = {CS education,K-12 education,Schools,curricula},
month = {jun},
number = {2},
pages = {7:1----7:9},
title = {{Perspectives and Visions of Computer Science Education in Primary and Secondary (K-12) Schools}},
url = {http://doi.acm.org/10.1145/2602482},
volume = {14},
year = {2014}
}
@article{Held2018a,
abstract = {The p-value quantifies the discrepancy between the data and a null hypothesis of interest, usually the assumption of no difference or no effect. A Bayesian approach allows the calibration of p-valu...},
author = {Held, Leonhard and Ott, Manuela},
doi = {10.1146/annurev-statistics-031017-100307},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Held, Ott - 2018 - On p-Values and Bayes Factors(2).pdf:pdf},
issn = {2326-8298},
journal = {Annual Review of Statistics and Its Application},
keywords = {Bayes factor,evidence,minimum Bayes factor,objective Bayes,p-value,sample size},
number = {1},
pages = {393--419},
publisher = {Annual Reviews},
title = {{On p-Values and Bayes Factors}},
url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-031017-100307},
volume = {5},
year = {2018}
}
@article{Hodges1954,
abstract = {The distinction between statistical significance and material significance in hypotheses testing is discussed. Modifications of the customary tests, in order to test for the absence of material signif},
author = {Hodges, J. L. and Lehmann, E. L.},
doi = {10.1111/j.2517-6161.1954.tb00169.x},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hodges, Lehmann - 1954 - Testing the Approximate Validity of Statistical Hypotheses.pdf:pdf},
issn = {00359246},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
number = {2},
pages = {261--268},
publisher = {Wiley},
title = {{Testing the Approximate Validity of Statistical Hypotheses}},
volume = {16},
year = {1954}
}
@article{laplaceMemoireSurLesProbabilites1778,
author = {Laplace, Pierre-Simon de (1749-1827)},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Laplace - 1778 - M{\'{e}}moire Sur Les probabilit{\'{e}}s.pdf:pdf},
journal = {M{\'{e}}moires de l'Acad{\'{e}}mie des Sciences de Paris - Oeuvres compl{\`{e}}tes},
pages = {227--332},
title = {{M{\'{e}}moire Sur Les probabilit{\'{e}}s}},
volume = {9},
year = {1778}
}
@article{Kopycka-Kedzierawski2006,
abstract = {OBJECTIVES: Markov modeling is a useful mathematical procedure for calculating probabilities of disease prognosis. Increasingly, Markov models are being applied in medical and health services research and also in social sciences research. The purpose of our study was to use the Markov process to determine time-dependent transition probabilities for caries-free children to convert to a caries-active state and to assess the impact of salivary mutans streptococci (MS) levels on caries status.$\backslash$n$\backslash$nMETHODS: Our analysis was based on data obtained from a 6-year longitudinal study of risk factors associated with caries onset in children.$\backslash$n$\backslash$nRESULTS: Based on a two-state Markov model, the probability that a caries-free child would convert to a caries-active state during the study ranged between 0.0046 and 0.0471. The highest probability of converting from a caries-free state to a caries-active state was 0.0471 at age 8.5 years.$\backslash$n$\backslash$nCONCLUSIONS: In addition to standard statistical methods of analyzing longitudinal caries data, Markov models show promise for use in the analysis of caries risk.},
author = {Kopycka-Kedzierawski, D. T. and Billings, R. J.},
doi = {10.1111/j.1600-0528.2006.00268.x},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kopycka-Kedzierawski, Billings - 2006 - Application of nonhomogenous Markov models for analyzing longitudinal caries risk.pdf:pdf},
isbn = {0301-5661 (Print)$\backslash$r0301-5661 (Linking)},
issn = {03015661},
journal = {Community Dentistry and Oral Epidemiology},
keywords = {Caries risk,Longitudinal data,Mutans streptococci,Nonhomogenous Markov models},
month = {apr},
number = {2},
pages = {123--129},
pmid = {16515676},
publisher = {Wiley/Blackwell (10.1111)},
title = {{Application of nonhomogenous Markov models for analyzing longitudinal caries risk}},
url = {http://doi.wiley.com/10.1111/j.1600-0528.2006.00268.x},
volume = {34},
year = {2006}
}
@article{Lindley1956,
abstract = {A measure is introduced of the information provided by an experiment. The measure is derived from the work of Shannon [10] and involves the knowledge prior to performing the experiment, expressed through a prior probability distribution over the parameter space. The measure is used to compare some pairs of experiments without reference to prior distributions; this method of comparison is contrasted with the methods discussed by Blackwell. Finally, the measure is applied to provide a solution to some problems of experimental design, where the object of experimentation is not to reach decisions but rather to gain knowledge about the world.},
author = {Lindley, D. V.},
doi = {10.1214/aoms/1177728069},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {dec},
number = {4},
pages = {986--1005},
publisher = {Institute of Mathematical Statistics},
title = {{On a Measure of the Information Provided by an Experiment}},
url = {http://projecteuclid.org/euclid.aoms/1177728069},
volume = {27},
year = {1956}
}
@article{Hunter2001,
abstract = {An overemphasis on creativity for evaluating research has lead to a serious devaluation of replication studies. However, a total sample size of N = 153,669 is needed to estimate a causal effect to two digits, which is quite rare for a single study. The only way to get accurate estimation is to average across replications. If the average sample size were as high as N = 200, over 700 replication studies would be needed. Scientific replications are more problematic than pure statistical replications, and so even more replications are needed to achieve reasonable accuracy.},
author = {John, E Hunter},
doi = {10.1086/jcr.2001.28.issue-1},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/John - 2001 - The desperate need for replications.pdf:pdf},
issn = {0093-5301},
journal = {Journal of Consumer Research},
keywords = {Creativity,Market research,Methods,Sample size,Studies},
month = {jun},
number = {1},
pages = {149},
publisher = {Oxford University Press},
title = {{The desperate need for replications}},
url = {https://academic.oup.com/jcr/article-lookup/doi/10.1086/321953 http://proquest.umi.com/pqdweb?did=710083531{\&}Fmt=7{\&}clientId=12010{\&}RQT=309{\&}VName=PQD},
volume = {28},
year = {2001}
}
@article{Neyman1937,
abstract = {We shall distinguish two aspects of the problems of estimation . (i) the practical and (ii) the theoretical. The practical aspect may be described as follows: (i a ) The statistician is concerned with a population, $\pi$, which for some reason or other cannot be studied exhaustively. It is only possible to draw a sample from this population which may be studied in detail and used to form an opinion the values of certain constants describing the properties of the population $\pi$. For example, it may be desired to calculate approximately the mean of a certain character possessed by the individuals forming the population $\pi$, etc.},
author = {Neyman, J.},
doi = {10.1098/rsta.1937.0005},
isbn = {0080-4614},
issn = {1364-503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
number = {767},
pages = {333--380},
pmid = {91337},
publisher = {Royal Society},
title = {{Outline of a Theory of Statistical Estimation Based on the Classical Theory of Probability}},
url = {https://www.jstor.org/stable/91337 http://rsta.royalsocietypublishing.org/cgi/doi/10.1098/rsta.1937.0005},
volume = {236},
year = {1937}
}
@article{Jones2017,
author = {Jones, Julie Scott and Goldring, John E},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Jones, Goldring - 2017 - Telling Stories , Landing Planes and Getting Them Moving a Holistic Approach To Developi Ng Students ' Stati.pdf:pdf},
journal = {Statistics Education Research Journal},
keywords = {1,social science,statistical literacy amongst uk,statistics education research,tcp model,threshold concepts,troublesome knowledge},
number = {1},
pages = {1--19},
title = {{Telling Stories , Landing Planes and Getting Them Moving ; a Holistic Approach To Developi Ng Students ' Statist Ical Literacy}},
volume = {16},
year = {2017}
}
@book{Dudley1989,
address = {Belmont},
author = {Dudley, R.M.},
publisher = {Wadsworth {\&} Brooks-Cole},
title = {{Real Analysis and Probability}},
year = {1989}
}
@inproceedings{patino_program_2015,
abstract = {In this paper we want to describe advances reached in the game design and implementation related to Serious Game: Program with Ixquic. We present a work in progress about this videogame. Program with Ixquic, has two main purposes: developing programming skills for every user that interacts with this videogame, where virtual scenes created in 2D dimensions presents Object-Oriented Programming concepts through the development of examples and exercises in Java Programming Language.},
annote = {Mexico},
author = {Patino, T and Ramos, C},
booktitle = {2015 7th International Conference on Games and Virtual Worlds for Serious Applications (VS-Games)},
doi = {10.1109/VS-GAMES.2015.7295781},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Patino, Ramos - 2015 - Program with Ixquic How to Learn Object-Oriented Programming with a Game.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Patino, Ramos - 2015 - Program with Ixquic How to Learn Object-Oriented Programming with a Game.html:html},
keywords = {2D dimensions,Computer aided instruction,Computer science education,Games,Java programming language,Programming profession,Training,Virtual reality,Visualization,game design,java,object oriented programming,object-oriented programming,object-oriented programming concepts,program with ixquic,serious game,serious games (computing),videogame,virtual scenes},
month = {sep},
pages = {1--2},
shorttitle = {Program with {\{}Ixquic{\}}},
title = {{Program with Ixquic: How to Learn Object-Oriented Programming with a Game}},
year = {2015}
}
@article{Stein1962,
author = {Stein, Charles},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Stein - 1962 - A Remark on the Likelihood Principle.pdf:pdf},
journal = {Journal of the Royal Statistical Society Series A (General)},
number = {4},
pages = {565--568},
title = {{A Remark on the Likelihood Principle}},
volume = {125},
year = {1962}
}
@article{Fraser2017,
author = {Fraser, D.A.S.},
journal = {Annual Review of Statistics and Its Application},
number = {1},
pages = {1--14},
title = {{p-Values: The Insight to Modern Statistical Inference}},
volume = {4},
year = {2017}
}
@article{Dronamraju2015,
abstract = {J.B.S. Haldane made important contributions to several sciences although he did not possess an academic qualification in any branch of science. A classical scholar, who grew up in a scientific household in Oxford, Haldane was taught the principles of scientific experimentation from his childhood by his father, the distinguished physiologist John Scott Haldane. Collaborating with his father, Haldane contributed to respiratory physiology but soon switched to genetics, especially population genetics. He investigated mathematically the dynamics of selection - mutation balance in populations - concluding that it is mutation that determines the course of evolution. Besides genetics, Haldane was noted for his important contributions to enzyme kinetics, origin of life, biometry, cybernetics, cosmology and deep sea diving, among others.},
author = {Dronamraju, Krishna},
doi = {10.1016/j.mrrev.2015.05.002},
issn = {13835742},
journal = {Mutation Research},
keywords = {History of Genetics,India,J.B.S. Haldane,Mutation,Physiology,Selection},
month = {jul},
pages = {1--6},
pmid = {26281764},
title = {{J.B.S. Haldane as I knew him, with a brief account of his contribution to mutation research}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26281764 https://linkinghub.elsevier.com/retrieve/pii/S1383574215000289},
volume = {765},
year = {2015}
}
@article{Colquhoun2017,
abstract = {We wish to answer this question: If you observe a 'significant' p-value after doing a single unbiased experiment, what is the probability that your result is a false positive? The weak evidence provided by p-values between 0.01 and 0.05 is explored by exact calculations of false positive risks. When you observe p = 0.05, the odds in favour of there being a real effect (given by the likelihood ratio) are about 3 : 1. This is far weaker evidence than the odds of 19 to 1 that might, wrongly, be inferred from the p-value. And if you want to limit the false positive risk to 5{\%}, you would have to assume that you were 87{\%} sure that there was a real effect before the experiment was done. If you observe p = 0.001 in a well-powered experiment, it gives a likelihood ratio of almost 100 : 1 odds on there being a real effect. That would usually be regarded as conclusive. But the false positive risk would still be 8{\%} if the prior probability of a real effect were only 0.1. And, in this case, if you wanted to achieve a false positive risk of 5{\%} you would need to observe p = 0.00045. It is recommended that the terms 'significant' and 'non-significant' should never be used. Rather, p-values should be supplemented by specifying the prior probability that would be needed to produce a specified (e.g. 5{\%}) false positive risk. It may also be helpful to specify the minimum false positive risk associated with the observed p-value. Despite decades of warnings, many areas of science still insist on labelling a result of p {\textless} 0.05 as 'statistically significant'. This practice must contribute to the lack of reproducibility in some areas of science. This is before you get to the many other well-known problems, like multiple comparisons, lack of randomization and p-hacking. Precise inductive inference is impossible and replication is the only way to be sure. Science is endangered by statistical misunderstanding, and by senior people who impose perverse incentives on scientists.},
author = {Colquhoun, David},
doi = {10.1098/rsos.171085},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Colquhoun - 2017 - The reproducibility of research and the misinterpretation of p-values.pdf:pdf},
issn = {20545703},
journal = {Royal Society Open Science},
keywords = {False positive risk,Null hypothesis tests,Reproducibility,Significance tests,Statistics},
number = {12},
pmid = {29308247},
publisher = {The Royal Society},
title = {{The reproducibility of research and the misinterpretation of p-values}},
volume = {4},
year = {2017}
}
@article{Diebold2012,
abstract = {I investigate the origins of the now-ubiquitous term ”Big Data," in industry and academics, in computer science and statistics/econometrics. Credit for coining the term must be shared. In particular, John Mashey and others at Silicon Graphics produced highly relevant (unpublished, non-academic) work in the mid-1990s. The first significant academic references (independent of each other and of Silicon Graphics) appear to be Weiss and Indurkhya (1998) in computer science and Diebold (2000) in statistics /econometrics. Douglas Laney of Gartner also produced insightful work (again unpublished and non-academic) slightly later. Big Data the term is now firmly entrenched, Big Data the phenomenon continues unabated, and Big Data the discipline is emerging.},
author = {Diebold, Francis X.},
doi = {10.2139/ssrn.2152421},
journal = {SSRN Electronic Journal},
keywords = {Francis X. Diebold,Massive data,On the Origin(s) and Development of the Term 'Big,SSRN,computing,econometrics,statistics},
month = {oct},
publisher = {Elsevier BV},
title = {{On the Origin(s) and Development of the Term 'Big Data'}},
url = {https://papers.ssrn.com/abstract=2152421},
year = {2012}
}
@article{batanero2002,
author = {Batanero, Carmen},
doi = {10.1111/j.1751-5823.2002.tb00340.x},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Batanero - 2002 - Discussion The Role of Models in Understanding and Improving Statistical Literacy(2).pdf:pdf},
issn = {0306-7734},
journal = {International Statistical Review},
number = {1},
pages = {37--40},
title = {{Discussion: The Role of Models in Understanding and Improving Statistical Literacy}},
url = {http://doi.wiley.com/10.1111/j.1751-5823.2002.tb00340.x},
volume = {70},
year = {2002}
}
@article{Fisher1937DesignOfExperiments2nd,
address = {Edinburgh; London},
author = {{R.A. Fisher}},
edition = {2nd},
pages = {260},
publisher = {Oliver and Boyd},
title = {{The design of experiments}},
url = {https://trove.nla.gov.au/work/7546506?q{\&}sort=holdings+desc{\&}{\_}=1541503725214{\&}versionId=19867660},
year = {1937}
}
@book{jeffreys1961,
abstract = {This paper argues that probability is not an objective phenomenon that can be identified with either the configurational properties of sequences, or the dynamic properties of sources that generate sequences. Instead, it is proposed that probability is a function of subjective as well as objective conditions. This is explained by formulating a notion of probability that is a modification of Laplace's classical enunciation. This definition is then used to explain why probability is strongly associated with disordered sequences, and is also used to throw light on a number of problems in probability theory.},
address = {Oxford},
author = {Jeffreys, Harold},
booktitle = {Oxford Classic Texts in the Physical Sciences},
edition = {3rd},
isbn = {0-19-850368-7},
issn = {00070882},
publisher = {Oxford University Press},
title = {{Theory of Probability}},
year = {1961}
}
@inproceedings{Schuirmann1987b,
author = {Schuirmann, D.J.},
booktitle = {ASA Proceedings of the Biopharmaceutical Section},
pages = {137--142},
title = {{A compromise test for equivalence of average bioavailability}},
year = {1987}
}
@article{Gelman1992,
author = {Gelman, Andrew and Rubin, Donald B.},
doi = {10.1214/ss/1177011136},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gelman, Rubin - 1992 - Inference from Iterative Simulation Using Multiple Sequences.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Bayesian inference,ECM,EM,Gibbs sampler,Metropolis algorithm,SIR,convergence of stochastic processes,importance sampling,multiple imputation,random-effects model},
number = {4},
pages = {457--472},
publisher = {Institute of Mathematical Statistics},
title = {{Inference from Iterative Simulation Using Multiple Sequences}},
url = {http://projecteuclid.org/euclid.ss/1177011136},
volume = {7},
year = {1992}
}
@inproceedings{Xinogalos2007,
author = {Xinogalos, Stelios and Satratzemi, Maya and Dagdilelis, Vassilios},
booktitle = {Proceedings of the 12th Annual SIGCSE Conference on Innovation and Technology in Computer Science Education},
doi = {10.1145/1268784.1268914},
isbn = {978-1-59593-610-3},
keywords = {java,object-oriented programming (OOP)},
pages = {345--345},
publisher = {ACM},
shorttitle = {Teaching Java with BlueJ},
title = {{Teaching Java with BlueJ: A Two-year Experience}},
url = {http://doi.acm.org/10.1145/1268784.1268914},
year = {2007}
}
@article{Goodrich2020,
author = {Goodrich, B. and Gabry, J. and Ali, I. and Brilleman, S.},
journal = {R package version 2.19.3},
title = {{rstanarm: Bayesian applied regression modeling via Stan}},
url = {https://cran.r-project.org/web/packages/rstanarm/index.html},
year = {2020}
}
@article{Kiefer1956,
author = {Kiefer, J. and Wolfowitz, J.},
doi = {10.1214/aoms/1177728066},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kiefer, Wolfowitz - 1956 - Consistency of the Maximum Likelihood Estimator in the Presence of Infinitely Many Incidental Parameters.pdf:pdf},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {dec},
number = {4},
pages = {887--906},
publisher = {Institute of Mathematical Statistics},
title = {{Consistency of the Maximum Likelihood Estimator in the Presence of Infinitely Many Incidental Parameters}},
url = {http://projecteuclid.org/euclid.aoms/1177728066},
volume = {27},
year = {1956}
}
@article{Uffink1995,
abstract = {The principle of maximum entropy is a general method to assign values to probability distributions on the basis of partial information. This principle, introduced by Jaynes in 1957, forms an extension of the classical principle of insufficient reason. It has been further generalized, both in mathematical formulation and in intended scope, into the principle of maximum relative entropy or of minimum information. It has been claimed that these principles are singled out as unique methods of statistical inference that agree with certain compelling consistency requirements. This paper reviews these consistency arguments and the surrounding controversy. It is shown that the uniqueness proofs are flawed, or rest on unreasonably strong assumptions. A more general class of inference rules, maximizing the so-called R{\'{e}}nyi entropies, is exhibited which also fulfill the reasonable part of the consistency assumptions. {\textcopyright} 1996.},
author = {Uffink, Jos},
doi = {10.1016/1355-2198(95)00015-1},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Uffink - 1995 - Can the maximum entropy principle be explained as a consistency requirement.pdf:pdf},
issn = {13552198},
journal = {Studies in History and Philosophy of Modern Physics},
number = {3},
pages = {223--261},
publisher = {Pergamon},
title = {{Can the maximum entropy principle be explained as a consistency requirement?}},
volume = {26},
year = {1995}
}
@article{Myrvold2019,
abstract = {Richard Pettigrew has recently advanced a justification of the Principle of Indifference on the basis of a principle that he calls “cognitive conservatism,” or “extreme epistemic conservatism.” However, the credences based on the Principle of Indifference, as Pettigrew formulates it, violate another desideratum, namely, that learning from experience be possible. If it is accepted that learning from experience should be possible, this provides grounds for rejecting cognitive conservatism. Another set of criteria considered by Pettigrew, which involves a weighted mean of worst-case and best-case accuracy, affords some learning, but not the sort that one would expect. This raises the question of whether accuracy-based considerations can be adapted to justify credence functions that permit induction.},
author = {Myrvold, Wayne C.},
doi = {10.1007/s10670-018-9972-0},
issn = {15728420},
journal = {Erkenntnis},
keywords = {Epistemology,Ethics,Logic,Ontology,Philosophy,general},
number = {3},
pages = {577--584},
publisher = {Springer Netherlands},
title = {{Learning is a Risky Business}},
volume = {84},
year = {2019}
}
@article{Adjerid2018,
author = {Adjerid, Idris and Kelley, Ken},
doi = {10.1037/amp0000190},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Adjerid, Kelley - 2018 - Big Data in Psychology A Framework for Research Advancement.pdf:pdf},
journal = {American Psychologist},
keywords = {big data,data science,instrumentation,machine learning},
number = {7},
pages = {899--917},
title = {{Big Data in Psychology: A Framework for Research Advancement}},
url = {http://dx.doi.org/10.1037/amp0000190},
volume = {73},
year = {2018}
}
@book{RaiffaSchlaifer1961,
address = {Mass.},
author = {Raiffa, Howard and Schlaifer, Robert},
publisher = {Graduate School of Business Administration, Harvard University},
title = {{Applied Statistical Decision Theory}},
year = {1961}
}
@article{McGonigle2014,
abstract = {Animal models have historically played a critical role in the exploration and characterization of disease pathophysiology, target identification, and in the in vivo evaluation of novel therapeutic agents and treatments. In the wake of numerous clinical trial failures of new chemical entities (NCEs) with promising preclinical profiles, animal models in all therapeutic areas have been increasingly criticized for their limited ability to predict NCE efficacy, safety and toxicity in humans. The present review discusses some of the challenges associated with the evaluation and predictive validation of animal models, as well as methodological flaws in both preclinical and clinical study designs that may contribute to the current translational failure rate. The testing of disease hypotheses and NCEs in multiple disease models necessitates evaluation of pharmacokinetic/pharmacodynamic (PK/PD) relationships and the earlier development of validated disease-associated biomarkers to assess target engagement and NCE efficacy. Additionally, the transparent integration of efficacy and safety data derived from animal models into the hierarchical data sets generated preclinically is essential in order to derive a level of predictive utility consistent with the degree of validation and inherent limitations of current animal models. The predictive value of an animal model is thus only as useful as the context in which it is interpreted. Finally, rather than dismissing animal models as not very useful in the drug discovery process, additional resources, like those successfully used in the preclinical PK assessment used for the selection of lead NCEs, must be focused on improving existing and developing new animal models. {\textcopyright} 2013 Elsevier Inc.},
author = {McGonigle, Paul and Ruggeri, Bruce},
doi = {10.1016/j.bcp.2013.08.006},
isbn = {0006-2952},
issn = {00062952},
journal = {Biochemical Pharmacology},
keywords = {Animal models,Hierarchical data integration,Transgenic,Translational models,Validation},
month = {jan},
number = {1},
pages = {162--171},
pmid = {23954708},
title = {{Animal models of human disease: Challenges in enabling translation}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23954708 http://linkinghub.elsevier.com/retrieve/pii/S0006295213004929},
volume = {87},
year = {2014}
}
@article{Madigan1994,
abstract = {We consider the problem of model selection and accounting for model uncertainty in high-dimensional contingency tables, motivated by expert system applications. The approach most used currently is a stepwise strategy guided by tests based on approximate asymptotic P values leading to the selection of a single model; inference is then conditional on the selected model. The sampling properties of such a strategy are complex, and the failure to take account of model uncertainty leads to underestimation of uncertainty about quantities of interest. In principle, a panacea is provided by the standard Bayesian formalism that averages the posterior distributions of the quantity of interest under each of the models, weighted by their posterior model probabilities. Furthermore, this approach is optimal in the sense of maximizing predictive ability. But this has not been used in practice, because computing the posterior model probabilities is hard and the number of models is very large (often greater than 1011). We argue that the standard Bayesian formalism is unsatisfactory and propose an alternative Bayesian approach that, we contend, takes full account of the true model uncertainty by averaging over a much smaller set of models. An efficient search algorithm is developed for finding these models. We consider two classes of graphical models that arise in expert systems: the recursive causal models and the decomposable log-linear models. For each of these, we develop efficient ways of computing exact Bayes factors and hence posterior model probabilities. For the decomposable log-linear models, this is based on properties of chordal graphs and hyper-Markov prior distributions and the resultant calculations can be carried out locally. The end product is an overall strategy for model selection and accounting for model uncertainty that searches efficiently through the very large classes of models involved. Three examples are given. The first two concern data sets that have been analyzed by several authors in the context of model selection. The third addresses a urological diagnostic problem. In each example, our model averaging approach provides better out-of-sample predictive performance than any single model that might reasonably have been selected. {\textcopyright} 1994 Taylor {\&} Francis Group, LLC.},
author = {Madigan, David and Raftery, Adrian E.},
doi = {10.1080/01621459.1994.10476894},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Madigan, Raftery - 1994 - Model selection and accounting for model uncertainty in graphical models using occam's window.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Chordal graph,Contingency table,Decomposable log-linear model,Expert system,Hyper-Markov distribution,Recursive causal model},
number = {428},
pages = {1535--1546},
title = {{Model selection and accounting for model uncertainty in graphical models using occam's window}},
volume = {89},
year = {1994}
}
@article{Shalloway2014,
abstract = {Many disparate definitions of Bayesian credible intervals and regions are in use, which can lead to ambiguous presentation of results. It is particularly unsatisfactory when intervals are specified that do not match the one-sided character of the evidence. We suggest that a sensible resolution is to use the parameterization-independent region that maximizes the information gain between the initial prior and posterior distributions, as assessed by their Kullback-Leibler divergence, subject to the constraint on included posterior probability. This turns out to be equivalent to the relative surprise region previously defined by Evans (1997), and thus provides information theoretic support for its use. We also show that this region is the constrained optimizer over the posterior measure of any strictly monotonic function of the likelihood, which explains its many optimal properties, and that it is guaranteed to be consistent with the sidedness of the evidence. Because all of its equivalent derivations depend on the evidence as well as on the posterior distribution, we suggest that it be called the evidentiary credible region.},
author = {Shalloway, David},
doi = {10.1214/14-BA883},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Shalloway - 2014 - The evidentiary credible region.pdf:pdf},
issn = {19316690},
journal = {Bayesian Analysis},
keywords = {Credible interval,Credible region,Highest posterior density,Information gain,Kullback-leibler,Parameterization invariance,Relative surprise region},
number = {4},
pages = {909--922},
publisher = {International Society for Bayesian Analysis},
title = {{The evidentiary credible region}},
volume = {9},
year = {2014}
}
@article{Wetzels2009,
abstract = {We propose a sampling-based Bayesian t test that allows researchers to quantify the statistical evidence in favor of the null hypothesis. This Savage-Dickey (SD) t test is inspired by the Jeffreys-Zellner-Siow (JZS) t test recently proposed by Rouder, Speckman, Sun, Morey, and Iverson (2009). The SD test retains the key concepts of the JZS test but is applicable to a wider range of statistical problems. The SD test allows researchers to test order restrictions and applies to two-sample situations in which the different groups do not share the same variance. ? 2009 The Psychonomic Society, Inc.},
author = {Wetzels, Ruud and Raaijmakers, Jeroen G.W. and Jakab, Em{\"{o}}ke and Wagenmakers, Eric-Jan},
doi = {10.3758/PBR.16.4.752},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Wetzels et al. - 2009 - How to quantify support for and against the null hypothesis A flexible WinBUGS implementation of a default Bayes.pdf:pdf},
issn = {10699384},
journal = {Psychonomic Bulletin and Review},
number = {4},
pages = {752--760},
title = {{How to quantify support for and against the null hypothesis: A flexible WinBUGS implementation of a default Bayesian t test}},
volume = {16},
year = {2009}
}
@article{Buehler1963,
author = {Buehler, Robert J. and Feddersen, Alan P.},
doi = {10.1214/aoms/1177704034},
isbn = {00034851},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {sep},
number = {3},
pages = {1098--1100},
publisher = {Institute of Mathematical Statistics},
title = {{Note on a conditional property of student's t}},
url = {http://projecteuclid.org/euclid.aoms/1177704034 http://f3.tiera.ru/2/M{\_}Mathematics/MV{\_}Probability/MVsa{\_}Statistics and applications/Rojo J. (eds.) Optimality.. The second Erich L. Lehmann symposium (IMS, 2006)(ISBN 094060065X)(359s){\_}MVsa{\_}.pdf{\%}5Cnhttp://www},
volume = {34},
year = {1963}
}
@article{Paddock2003,
author = {Paddock, S.M. and Ruggeri, F. and Lavine, M. and West, M.},
journal = {Statistica Sinica},
number = {2},
pages = {443--460},
title = {{Randomized Polya tree models for nonparametric Bayesian inference}},
volume = {13},
year = {2003}
}
@article{Schucany2006,
abstract = {ISSN: 0361-0926 (Print) 1532-415X (Online) Journal homepage: http://www.tandfonline.com/loi/lsta20 One of the most basic topics in many introductory statistical methods texts is inference for a population mean, . The primary tool for confidence intervals and tests is the Student t sampling distribution. Although the derivation requires independent identically distributed normal random variables with constant variance, 2 , most authors reassure the readers about some robustness to the normality and constant variance assumptions. Some point out that if one is concerned about assumptions, one may statistically test these prior to reliance on the Student t. Most software packages provide optional test results for both (a) the Gaussian assumption and (b) homogeneity of variance. Many textbooks advise only informal graphical assessments, such as certain scatterplots for independence, others for constant variance, and normal quantile–quantile plots for the adequacy of the Gaussian model. We concur with this recommendation. As convincing evidence against formal tests of (a), such as the Shapiro–Wilk, we offer a simulation study of the tails of the resulting conditional sampling distributions of the Studentized mean. We analyze the results of systematically screening all samples from normal, uniform, exponential, and Cauchy populations. This pretest does not correct the erroneous significance levels and makes matters worse for the exponential. In practice, we conclude that graphical diagnostics are better than a formal pretest. Furthermore, rank or permutation methods are recommended for exact validity in the symmetric case.},
author = {Schucany, William R. and {Tony Ng}, H. K.},
doi = {10.1080/03610920600853308},
issn = {03610926},
journal = {Communications in Statistics - Theory and Methods},
keywords = {Adaptive procedure,Monte Carlo simulation,Nonparametric,Permutation,Pretest,Robustness,Shapiro-Wilk},
month = {dec},
number = {12},
pages = {2275--2286},
publisher = {Taylor {\&} Francis Group},
title = {{Preliminary goodness-of-fit tests for normality do not validate the one-sample student t}},
volume = {35},
year = {2006}
}
@article{VanDoorn2020,
abstract = {Bayesian inference for rank-order problems is frustrated by the absence of an explicit likelihood function. This hurdle can be overcome by assuming a latent normal representation that is consistent with the ordinal information in the data: the observed ranks are conceptualized as an impoverished reflection of an underlying continuous scale, and inference concerns the parameters that govern the latent representation. We apply this generic data-augmentation method to obtain Bayes factors for three popular rank-based tests: the rank sum test, the signed rank test, and Spearman's {\$}\backslashrho{\_}s{\$}.},
archivePrefix = {arXiv},
arxivId = {1712.06941},
author = {van Doorn, Johnny and Ly, Alexander and Marsman, Maarten and Wagenmakers, Eric-Jan},
doi = {10.1080/02664763.2019.1709053},
eprint = {1712.06941},
issn = {0266-4763},
journal = {Journal of Applied Statistics},
keywords = {Bayes factors,data augmentation,latent normal,semi-parametrics,two-sample},
number = {16},
pages = {2984--3006},
publisher = {Taylor {\&} Francis},
title = {{Bayesian Rank-Based Hypothesis Testing for the Rank Sum Test, the Signed Rank Test, and Spearman's rho}},
url = {http://arxiv.org/abs/1712.06941},
volume = {47},
year = {2020}
}
@article{Doob1949,
author = {Doob, J.L.},
doi = {https://mathscinet.ams.org/mathscinet-getitem?mr=0033460},
journal = {Colloques Internationaux Du Centre National de La Recherche Scientifique, No. 13. Centre National de la Recherche Scientifique, Paris},
pages = {23--27},
title = {{Le Calcul des Probabilit{\'{e}}s et ses Applications}},
volume = {13},
year = {1949}
}
@article{Gubernatis2005a,
abstract = {The 1953 publication, “Equation of State Calculations by Very Fast Computing Machines” by N. Metropolis, A. W. Rosenbluth and M. N. Rosenbluth, and M. Teller and E. Teller [J. Chem. Phys. 21, 1087 (1953)] marked the beginning of the use of the Monte Carlo method for solving problems in the physical sciences. The method described in this publication subsequently became known as the Metropolis algorithm, undoubtedly the most famous and most widely used Monte Carlo algorithm ever published. As none of the authors made subsequent use of the algorithm, they became unknown to the large simulation physics community that grew from this publication and their roles in its development became the subject of mystery and legend. At a conference marking the 50th anniversary of the 1953 publication, Marshall Rosenbluth gave his recollections of the algorithm's development. The present paper describes the algorithm, reconstructs the historical context in which it was developed, and summarizes Marshall's recollections.},
author = {Gubernatis, J. E.},
doi = {10.1063/1.1887186},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gubernatis - 2005 - Marshall Rosenbluth and the Metropolis algorithm.pdf:pdf},
issn = {1070-664X},
journal = {Physics of Plasmas},
keywords = {Monte Carlo methods,plasma simulation},
month = {may},
number = {5},
pages = {057303},
publisher = {American Institute of Physics},
title = {{Marshall Rosenbluth and the Metropolis algorithm}},
url = {http://aip.scitation.org/doi/10.1063/1.1887186},
volume = {12},
year = {2005}
}
@article{Lin2004,
abstract = {A finite mixture model using the multivariate t distribution has been shown as a robust extension of normal mixtures. In this paper, we present a Bayesian approach for inference about parameters of t-mixture models. The specifications of prior distributions are weakly informative to avoid causing nonintegrable posterior distributions. We present two efficient EM-type algorithms for computing the joint posterior mode with the observed data and an incomplete future vector as the sample. Markov chain Monte Carlo sampling schemes are also developed to obtain the target posterior distribution of parameters. The advantages of Bayesian approach over the maximum likelihood method are demonstrated via a set of real data.},
author = {Lin, Tsung I. and Lee, Jack C. and Ni, Huey F.},
doi = {10.1023/B:STCO.0000021410.33077.10},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {ECM,ECME,MCMC,Maximum a posteriori,Maximum likelihood estimation,t mixture model},
number = {2},
pages = {119--130},
title = {{Bayesian analysis of mixture modelling using the multivariate t distribution}},
volume = {14},
year = {2004}
}
@article{Kelter2021,
abstract = {Hypothesis testing is an essential statistical method in experimental psychology and the cognitive sciences. The problems of traditional null hypothesis significance testing (NHST) have been discussed widely, and among the proposed solutions to the replication problems caused by the inappropriate use of significance tests and p-values is a shift toward Bayesian data analysis. However, Bayesian hypothesis testing is concerned with various posterior indices for significance and the size of an effect. This complicates Bayesian hypothesis testing in practice, as the availability of multiple Bayesian alternatives to the traditional p-value causes confusion which one to select and why. In this paper, various Bayesian posterior indices which have been proposed in the literature are compared and their benefits and limitations are discussed. The comparison shows that conceptually not all proposed Bayesian alternatives to NHST and p-values are beneficial, and the usefulness of some indices strongly depends on the study design and research goal. However, the comparison also reveals that there exist at least two candidates among the available Bayesian posterior indices which have appealing theoretical properties and are widely underused in the cognitive sciences.},
author = {Kelter, Riko},
doi = {10.1080/00273171.2021.1967716},
issn = {0027-3171},
journal = {Multivariate Behavioral Research},
keywords = {Bayes factor,Bayesian hypothesis testing,Bayesian posterior indices,MAP-based p-value,ROPE,e-value,equivalence testing,probability of direction (PD)},
number = {in press},
pages = {1--29},
publisher = {Informa UK Limited},
title = {{How to Choose between Different Bayesian Posterior Indices for Hypothesis Testing in Practice}},
url = {https://www.tandfonline.com/doi/full/10.1080/00273171.2021.1967716},
year = {2021}
}
@book{McNemar1955,
address = {New York},
author = {McNemar, Quinn},
edition = {2nd},
publisher = {John Wiley},
title = {{Psychological statistics}},
year = {1955}
}
@article{DawidMusio2021,
archivePrefix = {arXiv},
arxivId = {https://arxiv.org/abs/2104.00119},
author = {Dawid, A.P. and Musio, M.},
eprint = {/arxiv.org/abs/2104.00119},
journal = {Annual Review of Statistics and its Application},
primaryClass = {https:},
title = {{Effects of Causes and Causes of Effects}},
volume = {(in press)},
year = {2021}
}
@article{Tierney1994,
author = {Tierney, L.},
journal = {The Annals of Statistics},
number = {4},
pages = {1701--1762},
title = {{Markov Chains for Exploring Posterior Distributions}},
volume = {22},
year = {1994}
}
@book{Samaniego2010,
abstract = {This monograph contributes to the area of comparative statistical inference. Attention is restricted to the important subfield of statistical estimation. The book is intended for an audience having a solid grounding in probability and statistics at the level of the year-long undergraduate course taken by statistics and mathematics majors. The necessary background on Decision Theory and the frequentist and Bayesian approaches to estimation is presented and carefully discussed in Chapters 13. The threshold problem - identifying the boundary between Bayes estimators which tend to outperform standard frequentist estimators and Bayes estimators which dont - is formulated in an analytically tractable way in Chapter 4. The formulation includes a specific (decision-theory based) criterion for comparing estimators. The centerpiece of the monograph is Chapter 5 in which, under quite general conditions, an explicit solution to the threshold is obtained for the problem of estimating a scalar parameter under squared error loss. The six chapters that follow address a variety of other contexts in which the threshold problem can be productively treated. Included are treatments of the Bayesian consensus problem, the threshold problem for estimation problems involving of multi-dimensional parameters and/or asymmetric loss, the estimation of nonidentifiable parameters, empirical Bayes methods for combining data from similar experiments and linear Bayes methods for combining data from related experiments. The final chapter provides an overview of the monographs highlights and a discussion of areas and problems in need of further research. F. J. Samaniego is a Distinguished Professor of Statistics at the University of California, Davis. He served as Theory and Methods Editor of the Journal of the American Statistical Association (2003-05), was the 2004 recipient of the Davis Prize for Undergraduate Teaching and Scholarly Achievement, and is an elected Fellow of the ASA, the IMS and the RSS and an elected Member of the ISI.},
address = {New York, NY},
author = {Samaniego, Francisco J.},
doi = {10.1007/978-1-4419-5941-6},
isbn = {9781441959409},
issn = {0172-7397},
pages = {135--156},
pmid = {15772297},
publisher = {Springer New York},
series = {Springer Series in Statistics},
title = {{A comparison of the Bayesian and frequentist approaches to estimation}},
url = {http://link.springer.com/10.1007/978-1-4419-5941-6},
year = {2010}
}
@incollection{David1966,
address = {London},
author = {David, F},
publisher = {Wiley},
title = {{Research papers in statistics (Festschrift for J. Neyman)}},
url = {https://www.worldcat.org/title/research-papers-in-statistics-festschrift-for-j-neyman/oclc/1222306},
year = {1966}
}
@article{Winquist2014,
abstract = {Pharmacology is an integrative discipline that originated from activities, now nearly 7000 years old, to identify therapeutics from natural product sources. Research in the 19th Century that focused on the Law of Mass Action (LMA) demonstrated that compound effects were dose-/concentration-dependent eventually leading to the receptor concept, now a century old, that remains the key to understanding disease causality and drug action. As pharmacology evolved in the 20th Century through successive biochemical, molecular and genomic eras, the precision in understanding receptor function at the molecular level increased and while providing important insights, led to an overtly reductionistic emphasis. This resulted in the generation of data lacking physiological context that ignored the LMA and was not integrated at the tissue/whole organism level. As reductionism became a primary focus in biomedical research, it led to the fall of pharmacology. However, concerns regarding the disconnect between basic research efforts and the approval of new drugs to treat 21st Century disease tsunamis, e.g., neurodegeneration, metabolic syndrome, etc. has led to the reemergence of pharmacology, its rise, often in the semantic guise of systems biology. Against a background of limited training in pharmacology, this has resulted in issues in experimental replication with a bioinformatics emphasis that often has a limited relationship to reality. The integration of newer technologies within a pharmacological context where research is driven by testable hypotheses rather than technology, together with renewed efforts in teaching pharmacology, is anticipated to improve the focus and relevance of biomedical research and lead to novel therapeutics that will contain health care costs. {\textcopyright} 2013 Elsevier Inc.},
author = {Winquist, Raymond J. and Mullane, Kevin and Williams, Michael},
doi = {10.1016/j.bcp.2013.09.011},
isbn = {00062952},
issn = {00062952},
journal = {Biochemical Pharmacology},
keywords = {Drug discovery,Pharmacology,Receptors,Systems biology},
month = {jan},
number = {1},
pages = {4--24},
pmid = {24070656},
title = {{The fall and rise of pharmacology - (Re-)defining the discipline?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24070656 http://linkinghub.elsevier.com/retrieve/pii/S0006295213006011},
volume = {87},
year = {2014}
}
@article{Johnson2013,
abstract = {Recent advances in Bayesian hypothesis testing have led to the development of uniformly most powerful Bayesian tests, which represent an objective, default class of Bayesian hypothesis tests that have the same rejection regions as classical significance tests. Based on the correspondence between these two classes of tests, it is possible to equate the size of classical hypothesis tests with evidence thresholds in Bayesian tests, and to equate P values with Bayes factors. An examination of these connections suggest that recent concerns over the lack of reproducibility of scientific studies can be attributed largely to the conduct of significance tests at unjustifiably high levels of significance. To correct this problem, evidence thresholds required for the declaration of a significant finding should be increased to 25-50:1, and to 100-200:1 for the declaration of a highly significant finding. In terms of classical hypothesis tests, these evidence standards mandate the conduct of tests at the 0.005 or 0.001 level of significance.},
author = {Johnson, V. E.},
doi = {10.1073/pnas.1313476110},
isbn = {1313476110},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {48},
pages = {19313--19317},
pmid = {24218581},
title = {{Revised standards for statistical evidence}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1313476110},
volume = {110},
year = {2013}
}
@article{Cheung2018,
author = {Cheung, Mike W.L. and Jak, Suzanne},
doi = {10.1027/2151-2604/a000348},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Cheung, Jak - 2018 - Challenges of Big Data Analyses and Applications in Psychology.pdf:pdf},
isbn = {1009213725},
issn = {21512604},
journal = {Zeitschrift f{\"{u}}r Psychologie / Journal of Psychology},
number = {4},
pages = {209--211},
title = {{Challenges of Big Data Analyses and Applications in Psychology}},
volume = {226},
year = {2018}
}
@article{Burkner2017,
author = {B{\"{u}}rkner, Paul-Christian},
doi = {10.18637/jss.v080.i01},
journal = {Journal of Statistical Software},
number = {1},
pages = {1--28},
title = {{brms: An R Package for Bayesian Multilevel Models Using Stan}},
volume = {80},
year = {2017}
}
@book{benjamin_s._bloom_taxonomy_1972,
address = {New York},
author = {{Benjamin S. Bloom} and Engelhart, Max D and Furst, Edward J and Hill, Walker H and Krathwohl, David R},
number = {Handbook I - Cognitive Domain},
publisher = {David McKay},
series = {The {\{}Classification{\}} of {\{}Educational{\}} {\{}Goals{\}}},
title = {{Taxonomy Of Educational Objectives}},
volume = {I},
year = {1972}
}
@article{Soper1913,
author = {Soper, Author H E},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Soper - 1913 - On the Probable Error of the Correlation Coefficient to a Second Approximation.pdf:pdf},
number = {1},
pages = {91--115},
title = {{On the Probable Error of the Correlation Coefficient to a Second Approximation}},
volume = {9},
year = {1913}
}
@article{Serlin1985,
author = {Serlin, Ronald C and Lapsley, Daniel K},
journal = {American Psychologist},
number = {1},
pages = {73--83},
title = {{Rationality in Psychological Research}},
volume = {40},
year = {1985}
}
@article{McShane2019,
abstract = {We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm—and the p-value thresholds intrinsic to it—as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to “ban” p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
archivePrefix = {arXiv},
arxivId = {1709.07588},
author = {McShane, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
doi = {10.1080/00031305.2018.1527253},
eprint = {1709.07588},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/McShane et al. - 2019 - Abandon Statistical Significance.pdf:pdf},
issn = {15372731},
journal = {American Statistician},
keywords = {Null hypothesis significance testing,Replication,Sociology of science,Statistical significance,p-Value},
month = {mar},
number = {sup1},
pages = {235--245},
publisher = {American Statistical Association},
title = {{Abandon Statistical Significance}},
volume = {73},
year = {2019}
}
@article{Mauldin1992,
author = {Mauldin, R. Daniel and Sudderth, William D. and Williams, S.C.},
journal = {The Annals of Statistics},
number = {3},
pages = {1203--1221},
title = {{Polya Trees and Random Distributions}},
volume = {20},
year = {1992}
}
@incollection{DasGupta1980a,
author = {{Das Gupta}, Somesh},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_3},
pages = {9--16},
publisher = {Springer, New York, NY},
title = {{Distribution of the Correlation Coefficient}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}3},
year = {1980}
}
@article{Pearson1974,
author = {Pearson, E S},
journal = {Int. Stat. Review},
number = {1},
pages = {5--8},
title = {{Memories of the impact of {\{}F{\}}isher's work in the 1920s}},
volume = {42},
year = {1974}
}
@book{Bremaud2020,
address = {Cham, Switzerland},
author = {Br{\'{e}}maud, Pierre},
file = {:Users/riko/Library/Mobile Documents/com{\~{}}apple{\~{}}CloudDocs/dissertation/papers/Pierre Br{\'{e}}maud - Probability Theory and Stochastic Processes-Springer (2020).pdf:pdf},
publisher = {Springer Nature Switzerland},
title = {{Probability Theory and Stochastic Processes}},
year = {2020}
}
@inproceedings{Berger1984,
address = {Valencia},
author = {Berger, J.O.},
booktitle = {Bayesian Statistics II - Proceedings of the Second Valencia International Meeting},
editor = {Bernardo, J.M. and DeGroot, M.H. and Lindley, D. and Smith, A.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Berger - 1984 - In defense of the likelihood principle axiomatics and coherency.pdf:pdf},
pages = {33--67},
publisher = {Elsevier},
title = {{In defense of the likelihood principle: axiomatics and coherency}},
year = {1984}
}
@article{Kendall1982,
author = {Kendall, D.G. and Bartlett, M.S. and Thornton, L.},
journal = {Biographical Memoirs of Fellows of the Royal Society},
pages = {379--412},
title = {{Jerzy Neyman}},
volume = {28},
year = {1982}
}
@article{Fisher1939,
author = {Fisher, R.A.},
doi = {10.1111/j.1469-1809.1939.tb02192.x},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1939 - “STUDENT”.pdf:pdf},
issn = {20501420},
journal = {Annals of Eugenics},
month = {jan},
number = {1},
pages = {1--9},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{“STUDENT”}},
url = {http://doi.wiley.com/10.1111/j.1469-1809.1939.tb02192.x},
volume = {9},
year = {1939}
}
@article{Karabatsos2006,
abstract = {This article examines a Bayesian nonparametric approach to model selection and model testing, which is based on concepts from Bayesian decision theory and information theory. The approach can be used to evaluate the predictive-utility of any model that is either probabilistic or deterministic, with that model analyzed under either the Bayesian or classical-frequentist approach to statistical inference. Conditional on an observed set of data, generated from some unknown true sampling density, the approach identifies the "best" model as the one that predicts a sampling density that explains the most information about the true density. Furthermore, in the approach, the decision is to reject a model when it does not explain enough information about the true density (according to a straightforward calibration of the Kullback-Leibler divergence measure). The posterior estimate of the true density is based on a Bayesian nonparametric prior that can give positive support to the entire space of sampling densities (defined on some sample space). This article also discusses the theoretical and practical advantages of the Bayesian nonparametric approach over all other types of model selection procedures, and over any model testing procedure that depends on interpreting a p -value. Finally, the Bayesian nonparametric approach is illustrated on four real data sets, in the comparison and testing of order-constrained models, cognitive models, models of choice-behavior, and a test of a general psychometric model. {\textcopyright} 2005 Elsevier Inc. All rights reserved.},
author = {Karabatsos, George},
doi = {10.1016/j.jmp.2005.07.003},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
month = {apr},
number = {2},
pages = {123--148},
publisher = {Academic Press},
title = {{Bayesian nonparametric model selection and model testing}},
volume = {50},
year = {2006}
}
@article{Halsey2019,
abstract = {The p -value has long been the figurehead of statistical analysis in biology, but its position is under threat. p is now widely recognized as providing quite limited information about our data, and as being easily misinterpreted. Many biologists are aware of p 's frailties, but less clear about how they might change the way they analyse their data in response. This article highlights and summarizes four broad statistical approaches that augment or replace the p -value, and that are relatively straightforward to apply. First, you can augment your p -value with information about how confident you are in it, how likely it is that you will get a similar p -value in a replicate study, or the probability that a statistically significant finding is in fact a false positive. Second, you can enhance the information provided by frequentist statistics with a focus on effect sizes and a quantified confidence that those effect sizes are accurate. Third, you can augment or substitute p -values with the Bayes factor to inform on the relative levels of evidence for the null and alternative hypotheses; this approach is particularly appropriate for studies where you wish to keep collecting data until clear evidence for or against your hypothesis has accrued. Finally, specifically where you are using multiple variables to predict an outcome through model building, Akaike information criteria can take the place of the p -value, providing quantified information on what model is best. Hopefully, this quick-and-easy guide to some simple yet powerful statistical options will support biologists in adopting new approaches where they feel that the p -value alone is not doing their data justice.},
author = {Halsey, Lewis G.},
doi = {10.1098/rsbl.2019.0174},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Halsey - 2019 - The reign of the ipi -value is over what alternative analyses could we employ to fill the power vacuum(2).pdf:pdf},
issn = {1744-9561},
journal = {Biology Letters},
keywords = {AIC,Bayesian,Confidence intervals,Effect size,Statistical analysis},
month = {may},
number = {5},
pages = {20190174},
publisher = {Royal Society Publishing},
title = {{The reign of the p-value is over: what alternative analyses could we employ to fill the power vacuum?}},
url = {https://royalsocietypublishing.org/doi/10.1098/rsbl.2019.0174},
volume = {15},
year = {2019}
}
@article{Dongarra2000,
author = {Dongarra, J. and Sullivan, F.},
doi = {10.1109/MCISE.2000.814652},
issn = {1521-9615},
journal = {Computing in Science {\&} Engineering},
month = {jan},
number = {1},
pages = {22--23},
title = {{Guest Editors Introduction to the top 10 algorithms}},
url = {http://ieeexplore.ieee.org/document/814652/},
volume = {2},
year = {2000}
}
@article{Stigler1996,
abstract = {I wish to place before you this evening an unusual proposition, namely, that Mathematical Statistics began in 1933 . There are two reasons I describe this as an" unusual" proposition: first, it is unusual because of its precision in specifying a single yearclaims in intellectual ... $\backslash$n},
author = {Stigler, Stephen M.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Stigler - 1996 - The History of Statistics in 1933.pdf:pdf},
isbn = {0674403401},
issn = {08834237},
journal = {Statistical Science},
number = {3},
pages = {244--252},
pmid = {13784460},
title = {{The History of Statistics in 1933}},
volume = {11},
year = {1996}
}
@article{Richey2010,
abstract = {{\{}T{\}}his paper describes the story of the evolution of {\{}M{\}}arkov chain {\{}M{\}}onte {\{}C{\}}arlo ({\{}MCMC{\}}) methods from the perspective of the four seminal papers. {\{}I{\}}n the 50 years since their discovery by physicists at {\{}L{\}}os {\{}A{\}}lamos right after {\{}WWII{\}}, {\{}MCMC{\}} methods have revolutionized modern computational statistics and applied mathematics. {\{}T{\}}heir development is a fascinating interplay between physics, applied mathematics, computer science, and statistics.},
archivePrefix = {arXiv},
arxivId = {0712.0689},
author = {Richey, Matthew},
doi = {10.4169/000298910X485923},
eprint = {0712.0689},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Richey - 2010 - The evolution of Markov chain Monte Carlo methods.pdf:pdf},
isbn = {00029890$\backslash$n19300972},
issn = {00029890},
journal = {American Mathematical Monthly},
number = {5},
pages = {383--413},
pmid = {178037200001},
title = {{The evolution of Markov chain Monte Carlo methods}},
volume = {117},
year = {2010}
}
@article{Savage1982,
author = {Savage, Leonard J.},
doi = {10.1080/00031305.1982.10482802},
issn = {0003-1305},
journal = {The American Statistician},
month = {may},
number = {2},
pages = {121--121},
publisher = {Taylor {\&} Francis Group},
title = {{The Writings of Leonard Jimmie Savage — A Memorial Selection}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.1982.10482802},
volume = {36},
year = {1982}
}
@article{Bensmail1997,
author = {Bensmail, Halima and Celeux, Gilles and Raftery, Adrian E. and Robert, Christian P.},
doi = {10.1023/A:1018510926151},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bensmail et al. - 1997 - Inference in model-based cluster analysis.pdf:pdf},
journal = {Statistics and Computing},
number = {1},
pages = {1--10},
publisher = {Kluwer Academic Publishers},
title = {{Inference in model-based cluster analysis}},
url = {http://link.springer.com/10.1023/A:1018510926151},
volume = {7},
year = {1997}
}
@article{Wright1921,
author = {Wright, S.},
journal = {Journal of Agricultural Research},
pages = {557--585},
title = {{Correlation and Causation}},
volume = {20},
year = {1921}
}
@article{Fraser2018,
abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64{\%} of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42{\%} had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51{\%} had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution},
author = {Fraser, Hannah and Parker, Timothy H. and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
doi = {10.17605/OSF.IO/AJYQG},
editor = {Wicherts, Jelte M.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fraser et al. - 2018 - Questionable Research Practices in Ecology and Evolution.pdf:pdf},
isbn = {1111111111},
issn = {1932-6203},
journal = {Pre Print},
keywords = {Ecology and Evolutionary Biology,HARKing,Life Sciences,cherry,ecology,evolutionary biology,hacking,open science,p,picking,replicability,reproducibility,transparency},
month = {jul},
number = {7},
pages = {e0200303},
publisher = {Public Library of Science},
title = {{Questionable Research Practices in Ecology and Evolution}},
url = {http://dx.plos.org/10.1371/journal.pone.0200303 https://osf.io/ajyqg/},
volume = {13},
year = {2018}
}
@article{Loveland2011,
author = {Loveland, Jennifer L},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Loveland - 2011 - Mathematical Justifcation of Introductory Hypothesis Tests and Development of Reference Materials.pdf:pdf},
title = {{Mathematical Justifcation of Introductory Hypothesis Tests and Development of Reference Materials}},
year = {2011}
}
@article{Lewis1976,
author = {Lewis, David},
doi = {10.2307/2184045},
issn = {00318108},
journal = {The Philosophical Review},
number = {3},
pages = {297},
title = {{Probabilities of Conditionals and Conditional Probabilities}},
volume = {85},
year = {1976}
}
@book{Fisher1956,
address = {London},
author = {Fisher, Ronald Aylmer},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1956 - Statistical Methods and Scientific Inference.pdf:pdf},
publisher = {Oliver and Boyd},
title = {{Statistical Methods and Scientific Inference}},
year = {1956}
}
@incollection{Achinstein2010,
address = {Cambridge},
author = {Achinstein, P.},
booktitle = {Error and Inference: Recent Exchanges on Experimental Reasoning, Reliability, and the Objectivity and Rationality of Science},
editor = {Mayo, D. and Spanos, A.},
pages = {170--188},
publisher = {Cambridge University Press},
title = {{Mill's Sins or Mayo's Errors?}},
year = {2010}
}
@article{Cassidy2019,
abstract = {{\textless}p{\textgreater}Null-hypothesis significance testing (NHST) is commonly used in psychology; however, it is widely acknowledged that NHST is not well understood by either psychology professors or psychology students. In the current study, we investigated whether introduction-to-psychology textbooks accurately define and explain statistical significance. We examined 30 introductory-psychology textbooks, including the best-selling books from the United States and Canada, and found that 89{\%} incorrectly defined or explained statistical significance. Incorrect definitions and explanations were most often consistent with the odds-against-chance fallacy. These results suggest that it is common for introduction-to-psychology students to be taught incorrect interpretations of statistical significance. We hope that our results will create awareness among authors of introductory-psychology books and provide the impetus for corrective action. To help with classroom instruction, we provide slides that correctly describe NHST and may be useful for introductory-psychology instructors.{\textless}/p{\textgreater}},
author = {Cassidy, Scott A. and Dimova, Ralitza and Gigu{\`{e}}re, Benjamin and Spence, Jeffrey R. and Stanley, David J.},
doi = {10.1177/2515245919858072},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Cassidy et al. - 2019 - Failing Grade 89{\%} of Introduction-to-Psychology Textbooks That Define or Explain Statistical Significance Do So.pdf:pdf},
issn = {2515-2459},
journal = {Advances in Methods and Practices in Psychological Science},
keywords = {null-hypothesis significance testing,open data,open materials,p values,quantitative literacy,quantitative psychology,statistical inference},
month = {sep},
number = {3},
pages = {233--239},
publisher = {SAGE Publications},
title = {{Failing Grade: 89{\%} of Introduction-to-Psychology Textbooks That Define or Explain Statistical Significance Do So Incorrectly}},
url = {http://journals.sagepub.com/doi/10.1177/2515245919858072},
volume = {2},
year = {2019}
}
@article{Neyman1933a,
abstract = {In a recent paper we have discussed certain general principles underlying the determination of the most efficient tests of statistical hypotheses, but the method of approach did not involve any detailed consideration of the question of a priori probability. We propose now to consider more fully the bearing of the earlier results on this question and in particular to discuss what statements of value to the statistician in reaching his final judgment can be made from an analysis of observed data, which would not be modified by any change in the probabilities a priori. In dealing with the problem of statistical estimation, R. A. Fisher has shown how, under certain conditions, what may be described as rules of behaviour can be employed which will lead to results independent of these probabilities; in this connection he has discussed the important conception of what he terms fiducial limits. But the testing of statistical hypotheses cannot be treated as a problem in estimation, and it is necessary to discuss afresh in what sense tests can be employed which are independent of a priori probability laws.},
author = {Neyman, J. and Pearson, E. S. and Yule, G. U.},
doi = {10.1017/S030500410001152X},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Neyman, Pearson, Yule - 1933 - The testing of statistical hypotheses in relation to probabilities a priori.pdf:pdf},
issn = {0305-0041},
journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
number = {04},
pages = {492},
publisher = {Cambridge University Press},
title = {{The testing of statistical hypotheses in relation to probabilities a priori}},
url = {http://www.journals.cambridge.org/abstract{\_}S030500410001152X},
volume = {29},
year = {1933}
}
@article{Wilcox1998,
abstract = {Hundreds of articles in statistical journals have pointed out that standard analysis of variance, Pearson product -moment correlations, and least squares regression can be highly misleading and can have relatively low power even under very small departures from normality. In practical terms, psychology journals are littered with nonsignificant results that would have been significant if a more modern method had been used. Modern robust techniques, developed during the past 30 years, provide very effective methods for dealing with nonnormality, and they compete very well with conventional procedures when standard assumptions are met. In addition, modern methods provide accurate confidence intervals for a much broader range of situations, they provide more effective methods for detecting and studying outliers, and they can be used to get a deeper understanding of how variables are related. This article outlines and illustrates these results.},
author = {Wilcox, Rand R.},
doi = {10.1037/0003-066X.53.3.300},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Wilcox - 1998 - How Many Discoveries Have Been Lost by Ignoring Modern Statistical Methods.pdf:pdf},
issn = {0003066X},
journal = {American Psychologist},
number = {3},
pages = {300--314},
title = {{How Many Discoveries Have Been Lost by Ignoring Modern Statistical Methods?}},
volume = {53},
year = {1998}
}
@book{adams_2009,
author = {Adams, William J.},
edition = {2nd},
publisher = {American Mathematical Society},
title = {{The Life and Times of the Central Limit Theorem}},
year = {2009}
}
@article{Kruschke2018,
author = {Kruschke, John K. and Liddell, T.M.},
doi = {10.3758/s13423-016-1221-4},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kruschke, Liddell - 2018 - The Bayesian New Statistics Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesia.pdf:pdf},
journal = {Psychonomic Bulletin and Review},
keywords = {Bayes factor,Bayesian inference,Confidence interval,Credible interval,Effect size,Equivalence testing,Highest density interval,Meta-analysis,Null hypothesis significance testing,Power analysis,Randomized controlled trial,Region of practical equivalence,bayes factor,bayesian,confidence interval,credible,effect size,equivalence,highest density interval,inference,interval,meta-analysis,null hypothesis significance testing,power analysis,region of practical},
pages = {178--206},
publisher = {Psychonomic Bulletin {\&} Review},
title = {{The Bayesian New Statistics : Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective}},
volume = {25},
year = {2018}
}
@book{Leimkuhler2004,
address = {Cambridge},
author = {Leimkuhler, Benedict and Reich, Sebastian},
booktitle = {Geometry And Topology},
isbn = {9780521772907},
publisher = {Cambridge University Press},
title = {{Simulating Hamiltonian Dynamics}},
year = {2004}
}
@article{Schooler2014,
abstract = {Independent replication of studies before publication may reveal sources of unreliable results, says Jonathan W. Schooler.},
author = {Schooler, Jonathan W.},
doi = {10.1038/515009a},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Schooler - 2014 - Metascience could rescue the 'replication crisis'.pdf:pdf},
isbn = {0028-0836},
issn = {14764687},
journal = {Nature},
number = {7525},
pages = {9},
pmid = {25373639},
title = {{Metascience could rescue the 'replication crisis'}},
volume = {515},
year = {2014}
}
@article{Sanborn2014,
abstract = {Null hypothesis significance testing (NHST) is the most commonly used statistical methodology in psychology. The probability of achieving a value as extreme or more extreme than the statistic obtained from the data is evaluated, and if it is low enough, the null hypothesis is rejected. However, because common experimental practice often clashes with the assumptions underlying NHST, these calculated probabilities are often incorrect. Most commonly, experimenters use tests that assume that sample sizes are fixed in advance of data collection but then use the data to determine when to stop; in the limit, experimenters can use data monitoring to guarantee that the null hypothesis will be rejected. Bayesian hypothesis testing (BHT) provides a solution to these ills because the stopping rule used is irrelevant to the calculation of a Bayes factor. In addition, there are strong mathematical guarantees on the frequentist properties of BHT that are comforting for researchers concerned that stopping rules could influence the Bayes factors produced. Here, we show that these guaranteed bounds have limited scope and often do not apply in psychological research. Specifically, we quantitatively demonstrate the impact of optional stopping on the resulting Bayes factors in two common situations: (1) when the truth is a combination of the hypotheses, such as in a heterogeneous population, and (2) when a hypothesis is composite-taking multiple parameter values-such as the alternative hypothesis in a t-test. We found that, for these situations, while the Bayesian interpretation remains correct regardless of the stopping rule used, the choice of stopping rule can, in some situations, greatly increase the chance of experimenters finding evidence in the direction they desire. We suggest ways to control these frequentist implications of stopping rules on BHT. {\textcopyright} 2013 Psychonomic Society, Inc.},
author = {Sanborn, Adam N. and Hills, Thomas T.},
doi = {10.3758/s13423-013-0518-9},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Sanborn, Hills - 2014 - The frequentist implications of optional stopping on Bayesian hypothesis tests.pdf:pdf},
issn = {15315320},
journal = {Psychonomic Bulletin and Review},
keywords = {Bayesian statistics,Model selection,Statistical inference},
number = {2},
pages = {283--300},
pmid = {24101570},
title = {{The frequentist implications of optional stopping on Bayesian hypothesis tests}},
volume = {21},
year = {2014}
}
@article{Yates1951,
author = {Yates, F.},
doi = {10.1080/01621459.1951.10500764},
isbn = {01621459},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
number = {253},
pages = {19--34},
pmid = {25506333},
title = {{The Influence of Statistical Methods for Research Workers on the Development of the Science of Statistics}},
volume = {46},
year = {1951}
}
@article{kultus_bildungsstandard_2004,
author = {f{\"{u}}r Kultus, Ministerium and Jugend and {des Landes Baden-W{\"{u}}rttemberg}, Stuttgart},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kultus, Jugend, des Landes Baden-W{\"{u}}rttemberg - 2004 - Bildungsstandard Informatik Gymnasium.pdf:pdf},
title = {{Bildungsstandard Informatik Gymnasium}},
url = {http://www.bildung-staerkt-menschen.de/service/downloads/Bildungsstandards/Gym/Gym{\_}Inf{\_}wb{\_}bs.pdf},
year = {2004}
}
@book{Neyman1956,
abstract = {Editor: 1945/46- J. Neyman.},
author = {Neyman, Jerzy},
issn = {0097-0433},
keywords = {Asymptotic normality,Asymptotic similarity,Consistent estimate,Sufficiency},
publisher = {University of California Press},
title = {{Proceedings of the Berkeley Symposium on Mathematical Statistics and Probability.}},
url = {https://projecteuclid.org/euclid.bsmsp/1200501652},
year = {1956}
}
@inproceedings{LeCam1956,
address = {Berkeley},
author = {{Le Cam}, L.},
booktitle = {Proc. Third Berkeley Symp. on Math. Statist. and Prob.},
pages = {129--156},
publisher = {Univ. of Calif. Press},
title = {{On the Asymptotic Theory of Estimation and Testing Hypotheses}},
year = {1956}
}
@article{Fabius1964,
author = {Fabius, J.},
doi = {10.1214/aoms/1177703584},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {jun},
number = {2},
pages = {846--856},
publisher = {Institute of Mathematical Statistics},
title = {{Asymptotic Behavior of Bayes' Estimates}},
url = {https://projecteuclid.org/euclid.aoms/1177703584},
volume = {35},
year = {1964}
}
@book{Jeffreys1941,
address = {Oxford},
author = {Jeffreys, Harold},
edition = {2nd},
publisher = {The Clarendon Press},
title = {{Theory of Probability}},
year = {1948}
}
@article{Neuroskeptic2012,
abstract = {In the spirit of Dante Alighieri's Inferno, this paper takes a humorous look at the fate that awaits scientists who sin against best practice.},
author = {Neuroskeptic},
doi = {10.1177/1745691612459519},
isbn = {1745-6916$\backslash$n1745-6924},
issn = {17456916},
journal = {Perspectives on Psychological Science},
month = {nov},
number = {6},
pages = {643--644},
pmid = {26168124},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{The Nine Circles of Scientific Hell}},
url = {http://journals.sagepub.com/doi/10.1177/1745691612459519},
volume = {7},
year = {2012}
}
@article{Matthews2019,
abstract = {It is now widely accepted that the techniques of null hypothesis significance testing (NHST) are routinely misused and misinterpreted by researchers seeking insight from data. There is, however, no consensus on acceptable alternatives, leaving researchers with little choice but to continue using NHST, regardless of its failings. I examine the potential for the Analysis of Credibility (AnCred) to resolve this impasse. Using real-life examples, I assess the ability of AnCred to provide researchers with a simple but robust framework for assessing study findings that goes beyond the standard dichotomy of statistical significance/nonsignificance. By extracting more insight from standard summary statistics while offering more protection against inferential fallacies, AnCred may encourage researchers to move toward the post p {\textless} 0.05 era. ARTICLE HISTORY},
author = {Matthews, Robert A. J.},
doi = {10.1080/00031305.2018.1543136},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Matthews - 2019 - Moving Towards the Post ipi {\&}lt 0.05 Era via the Analysis of Credibility.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {analysis of credibility,bayesian inference,hypothesis significance,null},
number = {sup1},
pages = {202--212},
title = {{Moving Towards the Post {\textless}i{\textgreater}p{\textless}/i{\textgreater}  {\textless} 0.05 Era via the Analysis of Credibility}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1543136},
volume = {73},
year = {2019}
}
@inproceedings{mciver_evaluating_2002,
author = {McIver, Linda},
booktitle = {Fourteenth Annual Workshop of the Psychology of Programming Interest Group (PPIG 2002), Brunel University, Middlesex, UK},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/McIver - 2002 - Evaluating languages and environments for novice programmers.pdf:pdf},
title = {{Evaluating languages and environments for novice programmers}},
url = {http://www.ppig.org/papers/14th-mciver.pdf},
year = {2002}
}
@article{Westlake1972,
author = {Westlake, Wilfred J.},
doi = {10.1002/jps.2600610845},
issn = {00223549},
journal = {Journal of Pharmaceutical Sciences},
keywords = {Bioavailability trials,Clinical equivalence of drug formulations—use of c,Confidence intervals—analysis of comparative bioav,Drug formulations,comparative—analysis,comparison—use of confidence in,use},
month = {aug},
number = {8},
pages = {1340--1341},
title = {{Use of Confidence Intervals in Analysis of Comparative Bioavailability Trials}},
volume = {61},
year = {1972}
}
@article{RusticusEva2016,
author = {Rusticus, S.A. and Eva, K.W.},
doi = {10.1007/s10459-015-9633},
journal = {Practical Assessment, Research {\&} Evaluation},
number = {7},
pages = {1--6},
title = {{Defining equivalence in medical education evaluation and research: does a distribution-based approach work?}},
volume = {16},
year = {2016}
}
@article{Fisher1912,
author = {Fisher, Ronald Aylmer},
doi = {10.2307/2246266},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1912 - On an Absolute Criterion for Fitting Frequency Curves.pdf:pdf},
issn = {08834237},
journal = {Messenger of Mathematics},
pages = {155--160},
title = {{On an Absolute Criterion for Fitting Frequency Curves}},
volume = {41},
year = {1912}
}
@article{Ruberg2019,
abstract = {ABSTRACTThe cost and time of pharmaceutical drug development continue to grow at rates that many say are unsustainable. These trends have enormous impact on what treatments get to patients, when they get them and how they are used. The statistical framework for supporting decisions in regulated clinical development of new medicines has followed a traditional path of frequentist methodology. Trials using hypothesis tests of “no treatment effect” are done routinely, and the p-value {\textless} 0.05 is often the determinant of what constitutes a “successful” trial. Many drugs fail in clinical development, adding to the cost of new medicines, and some evidence points blame at the deficiencies of the frequentist paradigm. An unknown number effective medicines may have been abandoned because trials were declared “unsuccessful” due to a p-value exceeding 0.05. Recently, the Bayesian paradigm has shown utility in the clinical drug development process for its probability-based inference. We argue for a Bayesian approach tha...},
author = {Ruberg, Stephen J. and Harrell, Frank E. and Gamalo-Siebers, Margaret and LaVange, Lisa and {Jack Lee}, J. and Price, Karen and Peck, Carl},
doi = {10.1080/00031305.2019.1566091},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ruberg et al. - 2019 - Inference and Decision Making for 21st-Century Drug Development and Approval.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {bayesian statistics,clinical,p -value,prior and posterior,probability,trial},
number = {sup1},
pages = {319--327},
title = {{Inference and Decision Making for 21st-Century Drug Development and Approval}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1566091},
volume = {73},
year = {2019}
}
@book{Casella2002a,
abstract = {2nd ed. Probability theory -- Transformations and expectations -- Common families of distributions -- Multiple random variables -- Properties of a random sample -- Principles of data reduction -- Point estimation -- Hypothesis testing -- Interval estimation -- Asymptotic evaluations -- Analysis of variance and regression -- Regression models.},
address = {Stamford, Connecticut},
author = {Casella, George. and Berger, Roger L.},
isbn = {9780534243128},
pages = {660},
publisher = {Thomson Learning},
title = {{Statistical inference}},
year = {2002}
}
@article{Creutz1979,
author = {Creutz, Michael},
doi = {10.1103/PhysRevLett.43.553},
issn = {0031-9007},
journal = {Physical Review Letters},
month = {aug},
number = {8},
pages = {553--556},
publisher = {American Physical Society},
title = {{Confinement and the Critical Dimensionality of Space-Time}},
url = {https://link.aps.org/doi/10.1103/PhysRevLett.43.553},
volume = {43},
year = {1979}
}
@article{Fraser1969,
author = {Fraser, D.A.S.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fraser - 1969 - The Structure of Inference.pdf:pdf},
journal = {Biometrika},
number = {2},
pages = {453--456},
title = {{The Structure of Inference}},
volume = {56},
year = {1969}
}
@article{VanDeSchoot2017,
abstract = {Although the statistical tools most often used by researchers in the field of psychology over the last 25 years are based on frequentist statistics, it is often claimed that the alternative Bayesian approach to statistics is gaining in popularity. In the current article, we investigated this claim by performing the very first systematic review of Bayesian psychological articles published between 1990 and 2015 (n 1,579). We aim to provide a thorough presentation of the role Bayesian statistics plays in psychology. This historical assessment allows us to identify trends and see how Bayesian methods have been integrated into psychological research in the context of different statistical frameworks (e.g., hypothesis testing, cognitive models, IRT, SEM, etc.). We also describe take-home messages and provide "big-picture" recommendations to the field as Bayesian statistics becomes more popular. Our review indicated that Bayesian statistics is used in a variety of contexts across subfields of psychology and related disciplines. There are many different reasons why one might choose to use Bayes (e.g., the use of priors, estimating otherwise intractable models, modeling uncertainty, etc.). We found in this review that the use of Bayes has increased and broadened in the sense that this methodology can be used in a flexible manner to tackle many different forms of questions. We hope this presentation opens the door for a larger discussion regarding the current state of Bayesian statistics, as well as future trends. Translational Abstract Over 250 years ago, Bayes (or Price, or Laplace) introduced a method to take prior knowledge into account in data analysis. Although these ideas and Bayes's theorem have been longstanding within the fields of mathematics and statistics, these tools have not been at the forefront of modern-day applied psychological research. It was frequentist statistics (i.e., p values and null hypothesis testing; developed by Fisher, Neyman, and Pearson long after Bayes's theorem), which has dominated the field of Psychology throughout the 21st century. However, it is often claimed by 'Bayesians' that the alternative Bayesian approach to statistics is gaining in popularity. In the current article, we investigated this claim by performing the very first systematic review of Bayesian psychological articles published between 1990 and 2015 (n 1,579). Our findings showed that there was some merit in this thought. In fact, the use of Bayesian methods in applied Psychological work has steadily increased since the nineties and is currently taking flight. It was clear in this review that Bayesian statistics is used in a variety of contexts across subfields of Psychology and related disciplines. This is an exciting time, where we can watch the field of applied statistics change more than ever before. The way in which researchers think about and answer substantive inquiries is slowly taking on a new philosophical meaning that now incorporates previous knowledge and opinions into the estimation process. We hope this presentation opens the door for a larger discussion regarding the current state of Bayesian statistics, as well as future trends.},
author = {{Van De Schoot}, Rens and Winter, Sonja D and Ryan, Ois{\'{i}}n and Zondervan-Zwijnenburg, Mari{\"{e}}lle and Depaoli, Sarah},
doi = {10.1037/met0000100.supp},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Van De Schoot et al. - 2017 - A Systematic Review of Bayesian Articles in Psychology The Last 25 Years.pdf:pdf},
journal = {Psychological Methods},
keywords = {Bayes's theorem,MCMC-methods,posterior,prior,systematic review},
number = {2},
pages = {217--239},
title = {{A Systematic Review of Bayesian Articles in Psychology: The Last 25 Years}},
url = {http://dx.doi.org/10.1037/met0000100.supphttp://dx.doi.org/10.1037/met0000100},
volume = {22},
year = {2017}
}
@article{Edgeworth1908,
author = {Edgeworth, F. Y.},
doi = {10.2307/2339461},
issn = {09528385},
journal = {Journal of the Royal Statistical Society},
month = {jun},
number = {2},
pages = {381},
publisher = {WileyRoyal Statistical Society},
title = {{On the Probable Errors of Frequency-Constants}},
url = {https://www.jstor.org/stable/2339461?origin=crossref},
volume = {71},
year = {1908}
}
@book{Lindley1972,
address = {Philadelphia},
author = {Lindley, D.V.},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Bayesian statistics, A Review}},
year = {1972}
}
@article{Schoder2006,
abstract = {BACKGROUND: Statistical methodology has become an increasingly important topic in dermatological research. Adequacy of the statistical procedure depends among others on distributional assumptions. In dermatological articles, the choice between parametric and nonparametric methods is often based on preliminary goodness-of-fit tests. AIM: For the special case of the assumption of normally distributed data, the Kolmogorov-Smirnov test is the most popular choice. We investigated the performance of this test on four types of non-normal data, representing the majority of real data in dermatological research. METHODS: Simulations were run to assess the performance of the Kolmogorov-Smirnov test, depending on sample size and severity of violations of normality. RESULTS: The Kolmogorov-Smirnov test performs badly on data with single outliers, 10{\%} outliers and skewed data at sample sizes {\textless} 100, whereas normality is rejected to an acceptable degree for Likert-type data. CONCLUSION: Preliminary testing for normality is not recommended for small-to-moderate sample sizes.},
author = {Schoder, V. and Himmelmann, A. and Wilhelm, K. P.},
doi = {10.1111/j.1365-2230.2006.02206.x},
issn = {03076938},
journal = {Clinical and Experimental Dermatology},
month = {nov},
number = {6},
pages = {757--761},
publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
title = {{Preliminary testing for normality: Some statistical aspects of a common concept}},
volume = {31},
year = {2006}
}
@book{dauben_2002,
address = {Basel, Boston, Berlin},
author = {Dauben, Joseph W. and Scriba, Christoph J.},
publisher = {Birkh{\"{a}}user Verlag},
title = {{Writing the History of Mathematics: Its Historical Development}},
year = {2002}
}
@article{Fisher1926,
abstract = {Reproduced with permission of H.M.S.O.},
author = {Fisher, Ronald Aylmer},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1926 - The Arrangement of Field Experiments.pdf:pdf},
journal = {Journal of the Ministry of Agriculture of Great Britain},
keywords = {Journal article},
number = {33},
pages = {501--513},
title = {{The Arrangement of Field Experiments.}},
url = {https://digital.library.adelaide.edu.au/dspace/handle/2440/15191},
year = {1926}
}
@article{RorgerEckhardt1987,
author = {Eckhardt, Rorger},
issn = {0028-0836},
journal = {Los Alamos Science},
pages = {131--136},
title = {{Stan Ulam, John Von Neumann, and the Monte Carlo Method}},
volume = {15},
year = {1987}
}
@incollection{Box1980,
author = {Box, Joan Fisher},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_2},
pages = {6--8},
publisher = {Springer, New York, NY},
title = {{Fisher: The Early Years}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}2},
year = {1980}
}
@article{Olejnik2003,
abstract = {The editorial policies of several prominent educational and psychological journals require that researchers report some measure of effect size along with tests for statistical significance. In analysis of variance contexts, this requirement might be met by using eta squared or omega squared statistics. Current procedures for computing these measures of effect often do not consider the effect that design features of the study have on the size of these statistics. Because research-design features can have a large effect on the estimated proportion of explained variance, the use of partial eta or omega squared can be misleading. The present article provides formulas for computing generalized eta and omega squared statistics, which provide estimates of effect size that are comparable across a variety of research designs.},
author = {Olejnik, Stephen and Algina, James},
doi = {10.1037/1082-989X.8.4.434},
isbn = {1082-989X},
issn = {1939-1463},
journal = {Psychological Methods},
number = {4},
pages = {434--447},
pmid = {14664681},
title = {{Generalized Eta and Omega Squared Statistics: Measures of Effect Size for Some Common Research Designs.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.8.4.434},
volume = {8},
year = {2003}
}
@book{Jeffreys1931,
address = {Cambridge},
author = {Jeffreys, Harold},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Jeffreys - 1931 - Scientific Inference.pdf:pdf},
isbn = {978-0-521-18078-8},
pmid = {2848},
publisher = {Cambridge University Press},
title = {{Scientific Inference}},
year = {1931}
}
@article{Murdoch2008,
abstract = {P-values are taught in introductory statistics classes in a way that confuses many of the students, leading to common misconceptions about their meaning. In this article, we argue that p-values should be taught through simulation, emphasizing that p-values are random variables. By means of elementary examples we illustrate how to teach students valid interpretations of p-values and give them a deeper understanding of hypothesis testing.},
author = {Murdoch, Duncan J and Tsai, Yu Ling and Adcock, James},
doi = {10.1198/000313008X332421},
isbn = {0003-1305},
issn = {00031305},
journal = {American Statistician},
keywords = {Empirical cumulative distribution function (ECDF),Histograms,Hypothesis testing,Teaching statistics},
month = {aug},
number = {3},
pages = {242--245},
title = {{P-values are random variables}},
url = {http://www.tandfonline.com/doi/abs/10.1198/000313008X332421},
volume = {62},
year = {2008}
}
@inproceedings{Flores2014,
address = {New York, New York, USA},
author = {Flores, Pamela and Medinilla, Nelson and Pamplona, Sonia},
booktitle = {Proceedings of the 14th Koli Calling International Conference on Computing Education Research},
doi = {10.1145/2674683.2674697},
isbn = {978-1-4503-3065-7},
keywords = {information hiding,qualitative research,software design},
pages = {61--70},
publisher = {ACM Press},
title = {{What Do Software Design Students Understand About Information Hiding?: A Qualitative Case Study}},
url = {http://dl.acm.org/citation.cfm?doid=2674683.2674697 http://doi.acm.org/10.1145/2674683.2674697},
year = {2014}
}
@article{Cousineau2005,
author = {Cousineau, Denis},
doi = {10.20982/tqmp.01.1.p001},
issn = {1913-4126},
journal = {Tutorials in Quantitative Methods for Psychology},
month = {sep},
number = {1},
pages = {1--3},
title = {{The rise of quantitative methods in psychology}},
url = {http://www.tqmp.org/RegularArticles/vol01-1/p001},
volume = {1},
year = {2005}
}
@article{Juzek2019,
abstract = {The Two-One-Sided T-test procedure (TOST) is used to show that two samples are equivalent or similar, in contrast to classical statistical tests which check for dissimilarity. The TOST relies on a parameter called delta, which has to be set by the researcher using their intuition. Doing so can be difficult, because of complex interactions of relevant parameters. In this article we present a method to set delta, which is established and validated through extensive simulations based on real data sets from linguistics and other sciences. The presented method is shown to be sound and reliable, but we cannot exclude deviant early model behaviour (N?10) and deviant late model behaviour (N{\textgreater}100,000).},
author = {Juzek, Tom S. and Kizach, Johannes},
doi = {10.1558/jrds.39002},
issn = {2052-417X},
journal = {Journal of Research Design and Statistics in Linguistics and Communication Science},
keywords = {Language-Learning Situations,Likert scale,Mixed methods,Rasch Analysis,TOST,Validity,acceptability,competence,corpus linguistics,data simulation,experiment,factor analysis,judgment communication science,language,linguistics,marginal,measurement,quantitative analysis,readability indices research design statistics,sided t,similarity testing,statistical methods,t,test,test validation,tests,translation,twoone},
month = {aug},
number = {1-2},
pages = {153--169},
publisher = {Equinox Publishing},
title = {{How to Set Delta in the Two-One-Sided T-tests Procedure (TOST)}},
url = {https://journal.equinoxpub.com/JRDS/article/view/15494},
volume = {5},
year = {2019}
}
@inproceedings{Lindley1975,
author = {Lindley, D V},
booktitle = {Advances in Applied Probability 7(Supplement: Proceedings of the Conference on Directions for Mathematical Statistics)},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Lindley - 1975 - The Future of Statistics A Bayesian 21st Century.pdf:pdf},
pages = {106--115},
title = {{The Future of Statistics: A Bayesian 21st Century}},
url = {https://www.jstor.org/stable/1426315},
volume = {7},
year = {1975}
}
@article{Rose2019,
abstract = {Stepwise regression building procedures are commonly used applied statistical tools, despite their well-known drawbacks. While many of their limitations have been widely discussed in the literature, other aspects of the use of individual statistical fit measures, especially in high-dimensional stepwise regression settings, have not. Giving primacy to individual fit, as is done with p-values and R 2 , when group fit may be the larger concern, can lead to misguided decision making. One of the most consequential uses of stepwise regression is in health care, where these tools allocate hundreds of billions of dollars to health plans enrolling individuals with different predicted health care costs. The main goal of this "risk adjustment" system is to convey incentives to health plans such that they provide health care services fairly, a component of which is not to discriminate in access or care for persons or groups likely to be expensive. We address some specific limitations of p-values and R 2 for high-dimensional stepwise regression in this policy problem through an illustrated example by additionally considering a group-level fairness metric. ARTICLE HISTORY},
author = {Rose, Sherri and McGuire, Thomas G.},
doi = {10.1080/00031305.2018.1518269},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Rose, McGuire - 2019 - Limitations of iPi -Values and iRi -squared for Stepwise Regression Building A Fairness Demonstration in Health P.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {adjustment,fairness,health policy,p -value,regression,risk,stepwise},
number = {sup1},
pages = {152--156},
title = {{Limitations of {\textless}i{\textgreater}P{\textless}/i{\textgreater} -Values and {\textless}i{\textgreater}R{\textless}/i{\textgreater} -squared for Stepwise Regression Building: A Fairness Demonstration in Health Policy Risk Adjustment}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1518269},
volume = {73},
year = {2019}
}
@article{Betensky2019,
abstract = {It is widely recognized by statisticians, though not as widely by other researchers, that the p-value cannot be interpreted in isolation, but rather must be considered in the context of certain features of the design and substantive application, such as sample size and meaningful effect size. I consider the setting of the normal mean and highlight the information contained in the p-value in conjunction with the sample size and meaningful effect size. The p-value and sample size jointly yield 95{\%} confidence bounds for the effect of interest, which can be compared to the predetermined meaningful effect size to make inferences about the true effect. I provide simple examples to demonstrate that although the p-value is calculated under the null hypothesis, and thus seemingly may be divorced from the features of the study from which it arises, its interpretation as a measure of evidence requires its contextualization within the study. This implies that any proposal for improved use of the p-value as a measure of the strength of evidence cannot simply be a change to the threshold for significance. ARTICLE HISTORY},
author = {Betensky, Rebecca A.},
doi = {10.1080/00031305.2018.1529624},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Betensky - 2019 - The ipi -Value Requires Context, Not a Threshold.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {effect size,sample size,statistical significance},
number = {sup1},
pages = {115--117},
title = {{The {\textless}i{\textgreater}p{\textless}/i{\textgreater} -Value Requires Context, Not a Threshold}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1529624},
volume = {73},
year = {2019}
}
@article{Ly2016,
abstract = {Harold Jeffreys pioneered the development of default Bayes factor hypothesis tests for standard statistical problems. Using Jeffreys's Bayes factor hypothesis tests, researchers can grade the decisiveness of the evidence that the data provide for a point null hypothesis H0versus a composite alternative hypothesis H1. Consequently, Jeffreys's tests are of considerable theoretical and practical relevance for empirical researchers in general and for experimental psychologists in particular. To highlight this relevance and to facilitate the interpretation and use of Jeffreys's Bayes factor tests we focus on two common inferential scenarios: testing the nullity of a normal mean (i.e., the Bayesian equivalent of the t-test) and testing the nullity of a correlation. For both Bayes factor tests, we explain their development, we extend them to one-sided problems, and we apply them to concrete examples from experimental psychology.},
author = {Ly, Alexander and Verhagen, Josine and Wagenmakers, Eric-Jan},
doi = {10.1016/j.jmp.2015.06.004},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ly, Verhagen, Wagenmakers - 2016 - Harold Jeffreys's default Bayes factor hypothesis tests Explanation, extension, and application in ps.pdf:pdf},
isbn = {0022-2496},
issn = {10960880},
journal = {Journal of Mathematical Psychology},
keywords = {Bayes factors,Harold Jeffreys,Model selection},
pages = {19--32},
title = {{Harold Jeffreys's default Bayes factor hypothesis tests: Explanation, extension, and application in psychology}},
url = {http://www.ejwagenmakers.com/2016/LyEtAl2016JMP.pdf},
volume = {72},
year = {2016}
}
@article{Berkson1946,
author = {Berkson, Joseph},
doi = {10.1093/ije/dyu022},
issn = {14643685},
journal = {Biometrics},
number = {3},
pages = {47--53},
pmid = {24585734},
title = {{Limitations of the application of fourfold table analysis to hospital data}},
volume = {2},
year = {1946}
}
@misc{Halperin2018,
abstract = {Exercise and sport sciences continue to grow as a collective set of disciplines by investigating a broad array of basic and applied research questions. Despite the progress, there is room for improvement. A number of problems pertaining to reliability and validity of research practices hinder advancement and the potential impact of the field. These problems include: 1) inadequate validation of surrogate outcomes, 2) too few longitudinal and 3) replication studies, 4) limited reporting of null or trivial results, and 5) insufficient scientific transparency. The purpose of this review is to discuss these problems as they pertain to exercise and sport sciences based on their treatment in other disciplines, namely psychology and medicine, and propose a number of solutions and recommendations},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Halperin, Israel and Vigotsky, Andrew D. and Foster, Carl and Pyne, David B.},
booktitle = {International Journal of Sports Physiology and Performance},
doi = {10.1123/ijspp.2017-0322},
eprint = {1111.6189v1},
isbn = {8128562452},
issn = {15550265},
keywords = {Methodology,Null results,Replication},
month = {feb},
number = {2},
pages = {127--134},
pmid = {24911322},
publisher = {Human KineticsChampaign, Illinois, USA},
title = {{Strengthening the practice of exercise and sport-science research}},
url = {https://journals.humankinetics.com/doi/10.1123/ijspp.2017-0322},
volume = {13},
year = {2018}
}
@misc{Cox1977,
abstract = {The main object of the paper is to give a general review of the nature and importance of significance tests. Such tests are regarded as procedures for measuring the consistency of data with a null hypothesis by the calculation of a p-value (tail area). A distinction is drawn between several kinds of null hypothesis. The ways of deriving tests, namely via the so-called absolute test, via implicit consideration of alternatives and via explicit consideration of alternatives are reviewed. Some of the difficulties of multidimensional alternatives are outlined and the importance of the diagnostic ability of a test is stressed. Brief examples include tests of distributional form including multivariate normality. The effect of modifying statistical analysis in the light of the data is discussed, four main cases being distinguished. Then a number of more technical aspects of significance tests are outlined, including the role of two-sided tests, the role of the continuity correction, Bayesian tests and the use of tests in the comparison of alternative models. Finally the circumstances are reviewed under which significance tests can provide the main summary of a statistical analysis.},
author = {Cox, D.R. and Spj{\o}tvoll, Emil and Johansen, S{\o}ren and van Zwet, Willem R. and Bithell, J. F. and Barndorff-Nielsen, Ole and Keuls, M.},
booktitle = {Scandinavian Journal of Statistics},
doi = {10.2307/4615652},
pages = {49--70},
publisher = {WileyBoard of the Foundation of the Scandinavian Journal of Statistics},
title = {{The Role of Significance Tests [with Discussion and Reply]}},
url = {https://www.jstor.org/stable/4615652},
volume = {4},
year = {1977}
}
@inproceedings{Grove1997,
abstract = {Conditioning is the generally agreed-upon method for updating probability distribu­ tions when one learns that an event is cer­ tainly true. But it has been argued that we need other rules, in particular the rule of cross-entropy minimization, to handle up­ dates that involve uncertain information. In this paper we reexamine such a case: van Fraassen's Judy Benjamin problem [1987], which in essence asks how one might update given the value of a conditional probability. We argue that-contrary to the suggestions in the literature-it is possible to use simple conditionalization in this case, and thereby obtain answers that agree fully with intu­ ition. This contrasts with proposals such as cross-entropy, which are easier to apply but can give unsatisfactory answers. Based on the lessons from this example, we speculate on some general philosophical issues concern­ ing probability update.},
address = {San Francisco},
author = {Grove, Adam J. and Halpern, Joseph Y.},
booktitle = {Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Grove - 1997 - Probability Update Conditioning vs. Cross-Entropy.pdf:pdf},
publisher = {Morgan Kaufmann},
title = {{Probability Update: Conditioning vs. Cross-Entropy}},
year = {1997}
}
@article{Beribisky2019,
abstract = {Researchers often need to consider the practical significance of a relationship. For example, interpreting the magnitude of an effect size or establishing bounds in equivalence testing requires knowledge of the meaningfulness of a relationship. However, there has been little research exploring the degree of relationship among variables (e.g., correlation, mean difference) necessary for an association to be interpreted as meaningful or practically significant. In this study, we presented statistically trained and untrained participants with a collection of figures that displayed varying degrees of mean difference between groups or correlations among variables and participants indicated whether or not each relationship was meaningful. The results suggest that statistically trained and untrained participants differ in their qualification of a meaningful relationship, and that there is significant variability in how large a relationship must be before it is labeled meaningful. The results also shed some light on what degree of relationship is considered meaningful by individuals in a context-free setting.},
author = {Beribisky, Nataly and Davidson, Heather and Cribbie, Robert A.},
doi = {10.7717/peerj.6853},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Beribisky, Davidson, Cribbie - 2019 - Exploring perceptions of meaningfulness in visual representations of bivariate relationships.pdf:pdf},
issn = {21678359},
journal = {PeerJ},
keywords = {Effect sizes,Overlapping histograms,Practical significance,Scatterplots},
number = {5},
pages = {e6853},
publisher = {PeerJ Inc.},
title = {{Exploring perceptions of meaningfulness in visual representations of bivariate relationships}},
volume = {2019},
year = {2019}
}
@misc{msw_kernlehrplan_2014,
author = {MSW},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/MSW - 2014 - Kernlehrplan f{\"{u}}r die gymnasiale Oberstufe NRW.pdf:pdf},
publisher = {Ministerium f{\"{u}}r Schule und Weiterbildung des Landes Nordrhein-Westfalen},
title = {{Kernlehrplan f{\"{u}}r die gymnasiale Oberstufe NRW}},
url = {www.schulministerium.nrw.de},
year = {2014}
}
@incollection{Clifford1990,
abstract = {Ames and Janes provide a theoretical framework that explains alcohol and/or drug problems among workers (I). Existing studies of occupational risk factors for alcohol and drug problems across multiple occupations and industries provide mixed findings with respect to Ames and Janes' framework. In a preliminary study, the relationships between occupational characteristics and measures of alcohol and drug problems were investigated among a sample of workers from a variety of occupations and industry settings. Some support was found for all of the major elements of Ames and Janes' framework: normative regulation of drinking, quality and organization of work, workplace factors, and drinking subcultures. [ABSTRACT FROM AUTHOR]},
address = {Oxford},
author = {Clifford, Peter},
booktitle = {Disorder in Physical Systems: A Volume in Honour of John M. Hammersley},
editor = {Grimmett, G. R. and Welsh, D. J. A.},
isbn = {0-19-853215-6},
issn = {00952990},
pages = {19--32},
publisher = {Oxford University Press},
title = {{Markov random fields in statistics}},
year = {1990}
}
@book{evershed_teaching_0000,
annote = {OCLC: 846923096},
author = {Evershed, Jayne},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Evershed - Unknown - Teaching information technology.pdf:pdf},
isbn = {978-0-335-23799-9},
title = {{Teaching information technology.}}
}
@article{McShane2017a,
abstract = {Replication is complicated in psychological research because studies of a given psychological phenomenon can never be direct or exact replications of one another, and thus effect sizes vary from one study of the phenomenon to the next--an issue of clear importance for replication. Current large scale replication projects represent an important step forward for assessing replicability, but provide only limited information because they have thus far been designed in a manner such that heterogeneity either cannot be assessed or is intended to be eliminated. Consequently, the non-trivial degree of heterogeneity found in these projects represents a lower bound on heterogeneity. We recommend enriching large scale replication projects going forward by em- bracing heterogeneity. We argue this is key for assessing replicability: if effect sizes are sufficiently heterogeneous--even if the sign of the effect is consistent--the phenomenon in question does not seem particularly replicable and the theory underlying it seems poorly constructed and in need of enrichment. Uncovering why and revising theory in light of it will lead to improved theory that explains heterogeneity and in- creases replicability. Given this, large scale replication projects can play an important role not only in assessing replicability but also in advancing theory.},
archivePrefix = {arXiv},
arxivId = {1710.06031},
author = {McShane, Blakeley B. and Tackett, Jennifer L. and Bockenholt, Ulf and Gelman, Andrew},
doi = {10.1080/00031305.2018.1505655},
eprint = {1710.06031},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/McShane et al. - 2017 - Large Scale Replication Projects in Contemporary Psychological Research.pdf:pdf},
keywords = {between-study variation,heterogeneity,hierarchical,meta-analysis,multilevel},
title = {{Large Scale Replication Projects in Contemporary Psychological Research}},
url = {http://arxiv.org/abs/1710.06031},
volume = {1305},
year = {2017}
}
@article{Wald1939,
abstract = {*, 0k. A system of k values O (1)..., 0 (k) can be represented in the k-dimensional parameter space by the point 0 with the co-ordinates 0),..., a.(k) Denote by Q the set of all possible points 0. For any point 0 of Q we shall denote by P (E e wIO) the probability that the ...},
author = {Wald, Abraham},
doi = {10.1214/aoms/1177732144},
isbn = {978-0-940600-74-4},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {dec},
number = {4},
pages = {299--326},
publisher = {Institute of Mathematical Statistics},
title = {{Contributions to the Theory of Statistical Estimation and Testing Hypotheses}},
url = {http://projecteuclid.org/euclid.aoms/1177732144},
volume = {10},
year = {1939}
}
@article{Davis2013,
abstract = {In the past 15 years, fragment-based lead discovery (FBLD) has been adopted widely throughout academia and industry. The approach entails discovering very small molecular fragments and growing, merging, or linking them to produce drug leads. Because the affinities of the initial fragments are often low, detection methods are pushed to their limits, leading to a variety of artifacts, false positives, and false negatives that too often go unrecognized. This Digest discusses some of these problems and offers suggestions to avoid them. Although the primary focus is on FBLD, many of the lessons also apply to more established approaches such as high-throughput screening. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Davis, Ben J. and Erlanson, Daniel A.},
doi = {10.1016/j.bmcl.2013.03.028},
isbn = {0960-894X},
issn = {14643405},
journal = {Bioorganic and Medicinal Chemistry Letters},
month = {may},
number = {10},
pages = {2844--2852},
pmid = {23562240},
title = {{Learning from our mistakes: The 'unknown knowns' in fragment screening}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23562240 http://linkinghub.elsevier.com/retrieve/pii/S0960894X13003442},
volume = {23},
year = {2013}
}
@article{Jeffreys1934,
author = {Jeffreys, H.},
doi = {10.1098/rspa.1934.0135},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Jeffreys - 1934 - Probability and Scientific Method.pdf:pdf},
issn = {1364-5021},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
month = {aug},
number = {856},
pages = {9--16},
title = {{Probability and Scientific Method}},
url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1934.0135},
volume = {146},
year = {1934}
}
@article{Gronau2019a,
abstract = {Multinomial processing trees (MPTs) are a popular class of cognitive models for categorical data. Typically, researchers compare several MPTs, each equipped with many parameters, especially when the models are implemented in a hierarchical framework. A Bayesian solution is to compute posterior model probabilities and Bayes factors. Both quantities, however, rely on the marginal likelihood, a high-dimensional integral that cannot be evaluated analytically. In this case study, we show how Warp-III bridge sampling can be used to compute the marginal likelihood for hierarchical MPTs. We illustrate the procedure with two published data sets and demonstrate how Warp-III facilitates Bayesian model averaging.},
author = {Gronau, Quentin F and Wagenmakers, Eric-Jan and Heck, Daniel W and Matzke, Dora},
doi = {10.1007/s11336-018-9648-3},
issn = {1860-0980},
journal = {Psychometrika},
keywords = {Bayes factor,Bayesian model averaging,Bayesian model comparison,Warp-III,bridge sampling,multinomial processing tree,posterior model probability},
number = {1},
pages = {261--284},
pmid = {30483923},
title = {{A Simple Method for Comparing Complex Models: Bayesian Model Comparison for Hierarchical Multinomial Processing Tree Models Using Warp-III Bridge Sampling.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30483923 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6684497},
volume = {84},
year = {2019}
}
@article{Fraser2019,
author = {Fraser, D.A.S.},
journal = {The American Statistician},
number = {sup1},
pages = {135--147},
title = {{The p-value Function and Statistical Inference}},
volume = {73},
year = {2019}
}
@book{Faraway2016,
abstract = {Second edition. "A Chapman {\&} Hall book." "Since the publication of the bestselling, highly recommended first edition, R has considerably expanded both in popularity and in the number of packages available. Extending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models, Second Edition takes advantage of the greater functionality now available in R and substantially revises and adds several topics. New to the Second Edition: Expanded coverage of binary and binomial responses, including proportion responses, quasibinomial and beta regression, and applied considerations regarding these models; New sections on Poisson models with dispersion, zero inflated count models, linear discriminant analysis, and sandwich and robust estimation for generalized linear models (GLMs); Revised chapters on random effects and repeated measures that reflect changes in the lme4 package and show how to perform hypothesis testing for the models using other methods; New chapter on the Bayesian analysis of mixed effect models that illustrates the use of STAN and presents the approximation method of INLA; Revised chapter on generalized linear mixed models to reflect the much richer choice of fitting software now available; Updated coverage of splines and confidence bands in the chapter on nonparametric regression; New material on random forests for regression and classification; Revamped R code throughout, particularly the many plots using the ggplot2 package; Revised and expanded exercises with solutions now included. This textbook continues to cover a range of techniques that grow from the linear regression model. It presents three extensions to the linear framework: GLMs, mixed effect models, and nonparametric regression models. The book explains data analysis using real examples and includes all the R commands necessary to reproduce the analyses."-- Introduction -- Binary response -- Binomial and proportion responses -- Variations on logistic regression -- Count regression -- Contingency tables -- Multinomial data -- Generalized linear models -- Other GLMs -- Random effects -- Repeated measures and longitudinal data -- Bayesian mixed effect models -- Mixed effect models for nonnormal responses -- Nonparametric regression -- Additive models -- Trees -- Neural networks.},
address = {New York},
author = {Faraway, Julian James.},
doi = {https://doi.org/10.1201/9781315382722},
edition = {2nd},
isbn = {9781498720960},
pages = {399},
publisher = {Chapman and Hall/CRC},
title = {{Extending the linear model with R : Generalized Linear, Mixed Effects and Nonparametric Regression Models}},
year = {2016}
}
@incollection{Neal2011,
address = {Boca Raton},
archivePrefix = {arXiv},
arxivId = {1206.1901},
author = {Neal, Radford M},
booktitle = {Handbook of Markov Chain Monte Carlo},
chapter = {5},
doi = {doi:10.1201/b10905-6},
edition = {1st},
editor = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
eprint = {1206.1901},
isbn = {9781420079418},
issn = {{\textless}null{\textgreater}},
pages = {113--162},
pmid = {25246403},
publisher = {Chapman and Hall/CRC},
title = {{MCMC Using Hamiltonian Dynamics}},
url = {http://www.cs.utoronto.ca/{~}radford/ftp/ham-mcmc.pdf},
year = {2011}
}
@article{Kelter2019bayest,
author = {Kelter, Riko},
journal = {Comprehensive R Archive Network},
number = {https://cran.r-project.org/web/packages/bayest/index.html},
publisher = {Comprehensive R Archive Network},
title = {{bayest - Effect Size Targeted Bayesian Two-Sample t-Tests via Markov Chain Monte Carlo in Gaussian Mixture Models}},
url = {https://cran.r-project.org/web/packages/bayest/index.html},
year = {2019}
}
@article{Ioannidis2019,
author = {Ioannidis, John P. A.},
doi = {10.1080/00031305.2018.1447512},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ioannidis - 2019 - What Have We (Not) Learnt from Millions of Scientific Papers with p-Values.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Bias,P-value,Statistical significance,bias,p-value,statistical},
pages = {20--25},
publisher = {Taylor {\&} Francis},
title = {{What Have We (Not) Learnt from Millions of Scientific Papers with p-Values?}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1447512},
volume = {73},
year = {2019}
}
@inproceedings{kramer_competency_2016,
abstract = {Our project COMMOOP aims to develop a competency structure model and appropriate measurement instruments for the field of object-oriented programming (OOP). We started by reviewing existing literature on competency modelling in other subject areas regarding the development methodology as well as the model structures, identified common structural elements, verified, expanded and refined these based on an extensive literature analysis on theoretical and empirical studies on teaching and learning as well as on psychological aspects in the field of OOP. As theoretically derived candidates for potential competency dimensions we identified (1) OOP content knowledge and skills, (2) mastering representation, (3) cognitive processes and (4) metacognitive processes. This theoretically derived model framework was validated based on various competency descriptions in terms of applicability and completeness. For this purpose, we identified competency descriptions related to OOP in 44 computer science curricula and standards from several countries and compared these with our model. Further, we applied it to a list of competency definitions that was extracted by a working group at the ITiCSE 2015 from 14 case studies on K12 in 12 different countries. Finally, the structure model was aligned with the results of a survey among 59 computer science teachers and teacher students on learning difficulties. At the end, it turned out that our proposed model was quite complete already.},
annote = {Germany},
author = {Kramer, M and Hubwieser, P and Brinda, T},
booktitle = {2016 International Conference on Learning and Teaching in Computing and Engineering (LaTICE)},
doi = {10.1109/LaTiCE.2016.24},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kramer, Hubwieser, Brinda - 2016 - A Competency Structure Model of Object-Oriented Programming.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kramer, Hubwieser, Brinda - 2016 - A Competency Structure Model of Object-Oriented Programming.html:html},
keywords = {Analytical models,COMMOOP,Computational modeling,Computer science education,Context,Mathematical model,OOP content knowledge,Object oriented modeling,Problem-solving,competency modelling,competency structure model,computer science curricula,education,metacognitive processes,object-oriented programming,teaching},
month = {mar},
pages = {1--8},
title = {{A Competency Structure Model of Object-Oriented Programming}},
year = {2016}
}
@incollection{Nickel2007,
address = {Paderborn},
author = {Nickel, Gregor},
booktitle = {Autonomie durch Verantwortung},
editor = {Berendes, J.},
pages = {319--346},
publisher = {mentis Verlag},
title = {{Mathematik und Mathematisierung der Wissenschaften - Ethische Erw{\"{a}}gungen}},
year = {2007}
}
@article{Gurrin2000,
abstract = {Statistical analysis of both experimental and observational data is central to medical research. Unfortunately, the process of conventional statistical analysis is poorly understood by many medical scientists. This is due, in part, to the counter-intuitive nature of the basic tools of traditional (frequency-based) statistical inference. For example, the proper definition of a conventional 95{\%} confidence interval is quite confusing. It is based upon the imaginary results of a series of hypothetical repetitions of the data generation process and subsequent analysis. Not surprisingly, this formal definition is often ignored and a 95{\%} confidence interval is widely taken to represent a range of values that is associated with a 95{\%} probability of containing the true value of the parameter being estimated. Working within the traditional framework of frequency-based statistics, this interpretation is fundamentally incorrect. It is perfectly valid, however, if one works within the framework of Bayesian statistics and assumes a 'prior distribution' that is uniform on the scale of the main outcome variable. This reflects a limited equivalence between conventional and Bayesian statistics that can be used to facilitate a simple Bayesian interpretation based on the results of a standard analysis. Such inferences provide direct and understandable answers to many important types of question in medical research. For example, they can be used to assist decision making based upon studies with unavoidably low statistical power, where non-significant results are all too often, and wrongly, interpreted as implying 'no effect'. They can also be used to overcome the confusion that can result when statistically significant effects are too small to be clinically relevant. This paper describes the theoretical basis of the Bayesian-based approach and illustrates its application with a practical example that investigates the prevalence of major cardiac defects in a cohort of children born using the assisted reproduction technique known as ICSI (intracytoplasmic sperm injection).},
author = {Gurrin, Lyle C and Kurinczuk, Jennifer J and Burton, Paul R},
doi = {10.1046/j.1365-2753.2000.00216.x},
isbn = {1356-1294},
issn = {13561294},
journal = {Journal of Evaluation in Clinical Practice},
keywords = {Assisted reproduction technology,Bayesian statistics,Confidence intervals,ICSI,Medical statistics,P values,Statistical inference},
month = {may},
number = {2},
pages = {193--204},
pmid = {10970013},
title = {{Bayesian statistics in medical research: An intuitive alternative to conventional data analysis}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10970013},
volume = {6},
year = {2000}
}
@book{Bauer2001,
address = {Berlin},
author = {Bauer, Heinz},
isbn = {3110167190},
publisher = {De Gruyter},
title = {{Measure and integration theory}},
year = {2001}
}
@article{Merkle2019,
abstract = {Typical Bayesian methods for models with latent variables (or random effects) involve directly sampling the latent variables along with the model parameters. In high-level software code for model definitions (using, e.g., BUGS, JAGS, Stan), the likelihood is therefore specified as conditional on the latent variables. This can lead researchers to perform model comparisons via conditional likelihoods, where the latent variables are considered model parameters. In other settings, however, typical model comparisons involve marginal likelihoods where the latent variables are integrated out. This distinction is often overlooked despite the fact that it can have a large impact on the comparisons of interest. In this paper, we clarify and illustrate these issues, focusing on the comparison of conditional and marginal Deviance Information Criteria (DICs) and Watanabe–Akaike Information Criteria (WAICs) in psychometric modeling. The conditional/marginal distinction corresponds to whether the model should be predictive for the clusters that are in the data or for new clusters (where “clusters” typically correspond to higher-level units like people or schools). Correspondingly, we show that marginal WAIC corresponds to leave-one-cluster out cross-validation, whereas conditional WAIC corresponds to leave-one-unit out. These results lead to recommendations on the general application of the criteria to models with latent variables.},
author = {Merkle, Edgar C. and Furr, Daniel and Rabe-Hesketh, Sophia},
doi = {10.1007/s11336-019-09679-0},
issn = {00333123},
journal = {Psychometrika},
keywords = {Bayesian information criteria,DIC,IRT,MCMC,SEM,WAIC,conditional likelihood,cross-validation,leave-one-cluster out,marginal likelihood},
month = {sep},
number = {3},
pages = {802--829},
publisher = {Springer New York LLC},
title = {{Bayesian Comparison of Latent Variable Models: Conditional Versus Marginal Likelihoods}},
volume = {84},
year = {2019}
}
@article{Xinogalos2015c,
abstract = {The Object-Oriented Programming (OOP) technique is nowadays the most popular programming technique among tertiary education institutions. However, learning OOP is a cognitively demanding task for undergraduate students. Several difficulties and misconceptions have been recorded in the literature for both OOP concepts and languages, mainly Java. This article focuses on reviewing and advancing research on the most fundamental OOP concepts, namely, the concepts of “object” and “class” and their role during program execution. The results of a long-term investigation on the subject are presented, focusing on a study exploring undergraduate students' conceptions on “objects” and “classes.” The study advances related research on categories of conceptions on “objects” and “classes” by providing quantitative results, in addition to qualitative results, regarding the frequency of the recorded conceptions. Nearly half the students seem to comprehend the modeling and static/dynamic aspects of the concepts “object” and “class.” Implications for achieving a deep conceptual understanding of text, action, and modeling aspects of these fundamental concepts are also discussed. Information regarding the programming environments utilized in the course and key features of the applied teaching approach are presented, in order to facilitate both a better understanding of the context and a better employment of the results of the presented study. Finally, proposals for enhancing the contribution of this and similar studies are made.},
author = {Xinogalos, Stelios},
doi = {10.1145/2700519},
isbn = {1946-6226},
issn = {1946-6226},
journal = {ACM Transactions on Computing Education},
keywords = {Object-oriented programming,class,conceptions,misconceptions,object,teaching/learning programming},
month = {jul},
number = {3},
pages = {13:1--13:21},
title = {{Object-Oriented Design and Programming : An Investigation of Novices ' Conceptions on Objects and Classes}},
url = {http://dl.acm.org/citation.cfm?doid=2809889.2700519},
volume = {15},
year = {2015}
}
@article{Kelter2021ComputationalBrainAndBehavior,
abstract = {The Full Bayesian Significance Test (FBST) and the Bayesian evidence value recently have received increasing attention across a variety of sciences including psychology. Ly and Wagenmakers (2021) have provided a critical evaluation of the method and concluded that it suffers from four problems which are mostly attributed to the asymptotic relationship of the Bayesian evidence value to the frequentist p-value. While Ly and Wagenmakers (2021) tackle an important question about the best way of statistical hypothesis testing in the cognitive sciences, it is shown in this paper that their arguments are based on a specific measure-theoretic premise. The identified problems hold only under a specific class of prior distributions which are required only when adopting a Bayes factor test. However, the FBST explicitly avoids this premise, which resolves the problems in practical data analysis. In summary, the analysis leads to the more important question whether precise point null hypotheses are realistic for scientific research, and a shift towards the Hodges-Lehmann paradigm may be an appealing solution when there is doubt on the appropriateness of a precise hypothesis.},
author = {Kelter, Riko},
doi = {10.1007/s42113-021-00110-5},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kelter - 2021 - On the Measure-Theoretic Premises of Bayes Factor and Full Bayesian Significance Tests a Critical Reevaluation.pdf:pdf},
issn = {2522-0861},
journal = {Computational Brain {\&} Behavior},
keywords = {Bayes factor,Full Bayesian significance test,Hodges-Lehmann-paradigm,Mixture prior,Point null testing,Statistical evidence},
number = {online first},
pages = {1--11},
publisher = {Springer},
title = {{On the Measure-Theoretic Premises of Bayes Factor and Full Bayesian Significance Tests: a Critical Reevaluation}},
year = {2021}
}
@article{Jasp2019,
author = {{JASP Team}},
journal = {https://jasp-stats.org/},
title = {{Jeffreys Awesome Statistics Package (JASP)}},
url = {https://jasp-stats.org/},
year = {2019}
}
@book{Robert2007,
abstract = {Mathematics},
address = {Paris},
author = {Robert, Christian P.},
booktitle = {The Bayesian Choice},
doi = {10.1007/0-387-71599-1},
edition = {2nd},
publisher = {Springer New York},
title = {{The Bayesian Choice}},
year = {2007}
}
@book{baur_handbuch_2014,
address = {Wiesbaden},
annote = {DOI: 10.1007/978-3-531-18939-0},
author = {Baur, Nina and Blasius, J{\"{o}}rg},
editor = {Baur, Nina and Blasius, J{\"{o}}rg},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Baur, Blasius - 2014 - Handbuch Methoden der empirischen Sozialforschung.pdf:pdf},
isbn = {978-3-531-17809-7 978-3-531-18939-0},
publisher = {Springer Fachmedien Wiesbaden},
title = {{Handbuch Methoden der empirischen Sozialforschung}},
url = {http://link.springer.com/10.1007/978-3-531-18939-0},
year = {2014}
}
@article{Wagenmakers2018,
abstract = {Bayesian hypothesis testing presents an attractive alternative to p value hypothesis testing. Part I of this series outlined several advantages of Bayesian hypothesis testing, including the ability to quantify evidence and the ability to monitor and update this evidence as data come in, without the need to know the intention with which the data were collected. Despite these and other practical advantages, Bayesian hypothesis tests are still reported relatively rarely. An important impediment to the widespread adoption of Bayesian tests is arguably the lack of user-friendly software for the run-of-the-mill statistical problems that confront psychologists for the analysis of almost every experiment: the t-test, ANOVA, correlation, regression, and contingency tables. In Part II of this series we introduce JASP (http://www.jasp-stats.org), an open-source, cross-platform, user-friendly graphical software package that allows users to carry out Bayesian hypothesis tests for standard statistical problems. JASP is based in part on the Bayesian analyses implemented in Morey and Rouder's BayesFactor package for R. Armed with JASP, the practical advantages of Bayesian hypothesis testing are only a mouse click away.},
author = {Wagenmakers, Eric-Jan and Love, Jonathon and Marsman, Maarten and Jamil, Tahira and Ly, Alexander and Verhagen, Josine and Selker, Ravi and Gronau, Quentin F. and Dropmann, Damian and Boutin, Bruno and Meerhoff, Frans and Knight, Patrick and Raj, Akash and van Kesteren, Erik Jan and van Doorn, Johnny and {\v{S}}m{\'{i}}ra, Martin and Epskamp, Sacha and Etz, Alexander and Matzke, Dora and de Jong, Tim and van den Bergh, Don and Sarafoglou, Alexandra and Steingroever, Helen and Derks, Koen and Rouder, Jeffrey N. and Morey, Richard D.},
doi = {10.3758/s13423-017-1323-7},
issn = {15315320},
journal = {Psychonomic Bulletin and Review},
keywords = {Bayes factor,Hypothesis test,Posterior distribution,Statistical evidence},
month = {feb},
number = {1},
pages = {58--76},
pmid = {28685272},
title = {{Bayesian inference for psychology. Part II: Example applications with JASP}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28685272 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5862926 http://link.springer.com/10.3758/s13423-017-1323-7},
volume = {25},
year = {2018}
}
@article{kelleher_lowering_2005,
author = {Kelleher, Caitlin and Pausch, Randy},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kelleher, Pausch - 2005 - Lowering the barriers to programming A taxonomy of programming environments and languages for novice programme.pdf:pdf},
journal = {ACM Computing Surveys (CSUR)},
number = {2},
pages = {83--137},
shorttitle = {Lowering the barriers to programming},
title = {{Lowering the barriers to programming: A taxonomy of programming environments and languages for novice programmers}},
url = {http://dl.acm.org/citation.cfm?id=1089734},
volume = {37},
year = {2005}
}
@article{Raftery2003,
author = {Raftery, Adrian E. and Zheng, Yingye and Hjort, Nils Lid and Claeskens, Gerada},
doi = {10.1198/016214503000000891},
issn = {01621459},
journal = {Journal of the American Statistical Association},
number = {464},
pages = {931--938},
publisher = {American Statistical Association},
title = {{Discussion: Performance of Bayesian Model Averaging}},
volume = {98},
year = {2003}
}
@article{BROOKS1998,
author = {Brooks, Stephen P. and Roberts, Gareth O.},
doi = {10.1023/A:1008820505350},
issn = {09603174},
journal = {Statistics and Computing},
number = {4},
pages = {319--335},
publisher = {Kluwer Academic Publishers},
title = {{Convergence assessment techniques for Markov chain Monte Carlo}},
url = {http://link.springer.com/10.1023/A:1008820505350},
volume = {8},
year = {1998}
}
@article{Stigler1982,
author = {Stigler, Stephen M.},
journal = {Journal of the royal Statistical Society. Series A (General)},
number = {2},
pages = {250--258},
title = {{Thomas Bayes's Bayesian Inference}},
volume = {145},
year = {1982}
}
@article{Nuijten2016,
abstract = {This study documents reporting errors in a sample of over 250,000 p-values reported in eight major psychology journals from 1985 until 2013, using the new R package "statcheck." statcheck retrieved null-hypothesis significance testing (NHST) results from over half of the articles from this period. In line with earlier research, we found that half of all published psychology papers that use NHST contained at least one p-value that was inconsistent with its test statistic and degrees of freedom. One in eight papers contained a grossly inconsistent p-value that may have affected the statistical conclusion. In contrast to earlier findings, we found that the average prevalence of inconsistent p-values has been stable over the years or has declined. The prevalence of gross inconsistencies was higher in p-values reported as significant than in p-values reported as nonsignificant. This could indicate a systematic bias in favor of significant results. Possible solutions for the high prevalence of reporting inconsistencies could be to encourage sharing data, to let co-authors check results in a so-called "co-pilot model," and to use statcheck to flag possible inconsistencies in one's own manuscript or during the review process.},
author = {Nuijten, Mich{\`{e}}le B. and Hartgerink, Chris H.J. and van Assen, Marcel A.L.M. and Epskamp, Sacha and Wicherts, Jelte M.},
doi = {10.3758/s13428-015-0664-2},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Nuijten et al. - 2016 - The prevalence of statistical reporting errors in psychology (1985-2013).pdf:pdf},
issn = {15543528},
journal = {Behavior Research Methods},
keywords = {False positives,NHST,Publication bias,Questionable research practices,Reporting errors,Significance,p-values},
number = {4},
pages = {1205--1226},
publisher = {Springer US},
title = {{The prevalence of statistical reporting errors in psychology (1985-2013)}},
volume = {48},
year = {2016}
}
@misc{Kaiser2017,
abstract = {Though researchers have had general success reproducing cancer results, studies involving mice have proven difficult to replicate.},
author = {Kaiser, Jocelyn},
booktitle = {Science},
doi = {10.1126/science.aan7016},
issn = {0036-8075},
month = {jun},
title = {{Cancer studies pass reproducibility test}},
url = {http://www.sciencemag.org/news/2017/06/cancer-studies-pass-reproducibility-test},
urldate = {2018-08-07},
year = {2017}
}
@article{Muth2018,
abstract = {This tutorial provides a pragmatic introduction to specifying, estimating and interpreting single-level and hierarchical linear regression models in the Bayesian framework. We start by summarizing why one should consider the Bayesian approach to the most common forms of regression. Next we introduce the R package rstanarm for Bayesian applied regression modeling. An overview of rstanarm fundamentals accompanies step-by-step guidance for fitting a single-level regression model with the stan{\_}glm function, and fitting hierarchical regression models with the stan{\_}lmer function, illustrated with data from an experience sampling study on changes in affective states. Exploration of the results is facilitated by the intuitive and user-friendly shinystan package. Data and scripts are available on the Open Science Framework page of the project. For readers unfamiliar with R, this tutorial is self-contained to enable all researchers who apply regression techniques to try these methods with their own data. Regression modeling with the functions in the rstanarm package will be a straightforward transition for researchers familiar with their frequentist counterparts, lm (or glm) and lmer.},
author = {Muth, Chelsea and Oravecz, Zita and Gabry, Jonah},
doi = {10.20982/tqmp.14.2.p099},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Muth, Oravecz, Gabry - 2018 - User-friendly Bayesian regression modeling A tutorial with rstanarm and shinystan.pdf:pdf},
issn = {2292-1354},
journal = {The Quantitative Methods for Psychology},
keywords = {bayesian modeling,hierarchical linear model,r,regression,rstanarm,tools stan},
number = {2},
pages = {99--119},
title = {{User-friendly Bayesian regression modeling: A tutorial with rstanarm and shinystan}},
volume = {14},
year = {2018}
}
@article{white_against_2017,
author = {White, Patrick and Gorard, Stephen},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/White, Gorard - 2017 - Against inferential statistics how and why current statistics teaching gets it wrong.pdf:pdf},
journal = {Statistics Education Research Journal},
number = {1},
pages = {55--65},
shorttitle = {Against inferential statistics},
title = {{Against inferential statistics: how and why current statistics teaching gets it wrong.}},
year = {2017}
}
@article{Plummer2017,
abstract = {JAGS is Just Another Gibbs Sampler. It is a program for the analysis of Bayesian models using Markov Chain Monte Carlo (MCMC) which is not wholly unlike OpenBUGS (http: //www.openbugs.info). JAGS was written with three aims in mind: to have an engine for the BUGS language that runs on Unix; to be extensible, allowing users to write their own functions, distributions, and samplers; and to be a platform for experimentation with ideas in Bayesian modelling. JAGS is designed to work closely with the R language and environment for statistical computation and graphics (http://www.r-project.org). You will find it useful to install the coda package for R to analyze the output. You can also use the rjags package to work directly with JAGS from within R (but note that the rjags package is not described in this manual). JAGS is licensed under the GNU General Public License version 2. You may freely modify and redistribute it under certain conditions (see the file COPYING for details).},
author = {Plummer, Martyn and Northcott, B},
doi = {http://sourceforge.net/projects/mcmc-jags/files/},
title = {{JAGS Version 4.3.0 User Manual}},
year = {2017}
}
@misc{Aczel2020,
abstract = {Why is there no consensual way of conducting Bayesian analyses? We present a summary of agreements and disagreements of the authors on several discussion points regarding Bayesian inference. We also provide a thinking guideline to assist researchers in conducting Bayesian inference in the social and behavioural sciences.},
author = {Aczel, Balazs and Hoekstra, Rink and Gelman, Andrew and Wagenmakers, Eric Jan and Klugkist, Irene G. and Rouder, Jeffrey N. and Vandekerckhove, Joachim and Lee, Michael D. and Morey, Richard D. and Vanpaemel, Wolf and Dienes, Zoltan and van Ravenzwaaij, Don},
booktitle = {Nature Human Behaviour},
doi = {10.1038/s41562-019-0807-z},
issn = {23973374},
keywords = {Human behaviour,Science,technology and society},
month = {jun},
number = {6},
pages = {561--563},
publisher = {Nature Research},
title = {{Discussion points for Bayesian inference}},
url = {https://www.nature.com/articles/s41562-019-0807-z},
volume = {4},
year = {2020}
}
@article{Fisher1922a,
abstract = {Reproduced with permission of Blackwell Publishers},
author = {Fisher, Ronald Aylmer},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1922 - The Goodness of Fit of Regression Formulae and the Distribution of Regression Coefficients.pdf:pdf},
journal = {Journal of the Royal Statistical Society},
number = {85},
pages = {597--612},
title = {{The Goodness of Fit of Regression Formulae and the Distribution of Regression Coefficients.}},
url = {https://digital.library.adelaide.edu.au/dspace/handle/2440/15174},
year = {1922}
}
@article{Goodman1999,
abstract = {Bayesian inference is usually presented as a method for determining how scientific belief should be modified by data. Although Bayesian methodology has been one of the most active areas of statistical development in the past 20 years, medical researchers have been reluctant to embrace what they perceive as a subjective approach to data analysis. It is little understood that Bayesian methods have a data-based core, which can be used as a calculus of evidence. This core is the Bayes factor, which in its simplest form is also called a likelihood ratio. The minimum Bayes factor is objective and can be used in lieu of the P value as a measure of the evidential strength. Unlike P values, Bayes factors have a sound theoretical foundation and an interpretation that allows their use in both inference and decision making. Bayes factors show that P values greatly overstate the evidence against the null hypothesis. Most important, Bayes factors require the addition of background knowledge to be transformed into inferences--probabilities that a given conclusion is right or wrong. They make the distinction clear between experimental evidence and inferential conclusions while providing a framework in which to combine prior with current evidence.},
author = {Goodman, Steven N.},
doi = {10.7326/0003-4819-130-12-199906150-00019},
issn = {0003-4819},
journal = {Annals of Internal Medicine},
number = {12},
pages = {1005},
pmid = {10383350},
title = {{Toward Evidence-Based Medical Statistics. 2: The Bayes Factor}},
volume = {130},
year = {1999}
}
@inproceedings{Kohn2017,
abstract = {For novice programmers one of the most problematic concepts is variable assignment and evaluation. Several questions emerge in the mind of the beginner, such as what does x = 7 + 4 or x = x + 1 really mean? For instance, many students initially think that such statements store the entire calculation in variable x, evaluating the result lazily when actually needed. The common increment pattern x = x + 1 is even believed to be outright impossible. This paper discusses a multi-year project examining how high school students think of assignments and variables. In particular, where does the misconception of storing entire calculations come from? Can we explain the students' thinking and help them develop correct models of how programming works? It is particularly striking that a model of the computer as a machine with algebraic capabilities would indeed produce the observed misconceptions. The misconception might simply be attributed to the expectation that the computer performs computations the exact same way students are taught to in mathematics.},
author = {Kohn, Tobias},
booktitle = {Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education},
doi = {10.1145/3017680.3017724},
isbn = {978-1-4503-4698-6},
keywords = {learning,misconceptions,novices,programming,variables},
pages = {345--350},
publisher = {ACM},
shorttitle = {Variable Evaluation},
title = {{Variable Evaluation: An Exploration of Novice Programmers' Understanding and Common Misconceptions}},
url = {http://doi.acm.org/10.1145/3017680.3017724},
year = {2017}
}
@incollection{Azevedo-Filho1994,
abstract = {Laplace's method, a family of asymptotic methods used to approximate integrals, is presented as a potential candidate for the tool box of techniques used for knowledge acquisition and probabilistic inference in belief networks with continuous variables. This technique approximates posterior moments and marginal posterior distributions with reasonable accuracy [errors are O(n{\^{}}-2) for posterior means] in many interesting cases. The method also seems promising for computing approximations for Bayes factors for use in the context of model selection, model uncertainty and mixtures of pdfs. The limitations, regularity conditions and computational difficulties for the implementation of Laplace's method are comparable to those associated with the methods of maximum likelihood and posterior mode analysis.},
author = {Azevedo-Filho, Adriano and Shachter, Ross D.},
booktitle = {Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence},
doi = {10.1016/b978-1-55860-332-5.50009-2},
month = {jan},
pages = {28--36},
publisher = {Elsevier},
title = {{Laplace's Method Approximations for Probabilistic Inference in Belief Networks with Continuous Variables}},
year = {1994}
}
@techreport{HammersleyClifford1971,
author = {Hammersley, J. M. and Clifford, Peter},
institution = {University of California},
title = {{Markov Fields on Finite Graphs and Lattices}},
year = {1971}
}
@article{Semmens2009,
abstract = {Variability in resource use defines the width of a trophic niche occupied by a population. Intra-population variability in resource use may occur across hierarchical levels of population structure from individuals to subpopulations. Understanding how levels of population organization contribute to population niche width is critical to ecology and evolution. Here we describe a hierarchical stable isotope mixing model that can simultaneously estimate both the prey composition of a consumer diet and the diet variability among individuals and across levels of population organization. By explicitly estimating variance components for multiple scales, the model can deconstruct the niche width of a consumer population into relevant levels of population structure. We apply this new approach to stable isotope data from a population of gray wolves from coastal British Columbia, and show support for extensive intra-population niche variability among individuals, social groups, and geographically isolated subpopulations. The analytic method we describe improves mixing models by accounting for diet variability, and improves isotope niche width analysis by quantitatively assessing the contribution of levels of organization to the niche width of a population.},
author = {Semmens, Brice X. and Ward, Eric J. and Moore, Jonathan W. and Darimont, Chris T.},
doi = {10.1371/journal.pone.0006187},
issn = {19326203},
journal = {PLoS ONE},
number = {7},
pages = {1--9},
title = {{Quantifying inter-and intra-population niche variability using hierarchical bayesian stable isotope mixing models}},
volume = {4},
year = {2009}
}
@article{Haldane1932,
abstract = {The problem of statistical investigation is the description of a population, or Kollektiv, of which a sample has been observed. At best we can only state the probability that certain parameters of this population lie within assigned limits, i.e. specify their probability density. It has been shown, e.g. by von Mises(1), that this is only possible if we know the probability distribution of the parameter before the sample is taken. Bayes' theorem is based on the assumption that all values of the parameter in the neighbourhood of that observed are equally probable a priori. It is the purpose of this paper to examine what more reasonable assumption may be made, and how it will affect the estimate based on the observed sample.},
author = {Haldane, J. B. S.},
doi = {10.1017/S0305004100010495},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Haldane - 1932 - A note on inverse probability.pdf:pdf},
issn = {0305-0041},
journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
number = {1},
pages = {55--61},
publisher = {Cambridge University Press},
title = {{A note on inverse probability}},
url = {https://www.cambridge.org/core/product/identifier/S0305004100010495/type/journal{\_}article},
volume = {28},
year = {1932}
}
@article{Lavine1994,
author = {Lavine, Michael},
doi = {10.1214/aos/1176325623},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {Dirichlet processes,nonparametric regression,robust Bayes,tail-free processes},
month = {sep},
number = {3},
pages = {1161--1176},
publisher = {Institute of Mathematical Statistics},
title = {{More Aspects of Polya Tree Distributions for Statistical Modelling}},
url = {https://projecteuclid.org/euclid.aos/1176325623},
volume = {22},
year = {1994}
}
@article{Tierney1999,
abstract = {Monte Carlo methods, in particular Markov chain Monte Carlo methods, have become increasingly important as a tool for practical Bayesian inference in recent years. A wide range of algorithms is available, and choosing an algorithm that will work well on a specific problem is challenging. It is therefore important to explore the possibility of developing adaptive strategies that choose and adjust the algorithm to a particular context based on information obtained during sampling as well as information provided with the problem. This paper outlines some of the issues in developing adaptive methods and presents some preliminary results. Copyright {\{}$\backslash$textcopyright{\}} 1999 John Wiley {\&} Sons, Ltd.},
author = {Tierney, Luke and Mira, Antonietta},
doi = {10.1002/(SICI)1097-0258(19990915/30)18:17/18<2507::AID-SIM272>3.0.CO;2-J},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Tierney, Mira - 1999 - Some adaptive Monte Carlo methods for Bayesian inference.pdf:pdf},
isbn = {0277-6715},
issn = {02776715},
journal = {Statistics in Medicine},
month = {sep},
number = {17-18},
pages = {2507--2515},
pmid = {10474156},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Some adaptive Monte Carlo methods for Bayesian inference}},
url = {http://doi.wiley.com/10.1002/{\%}28SICI{\%}291097-0258{\%}2819990915/30{\%}2918{\%}3A17/18{\%}3C2507{\%}3A{\%}3AAID-SIM272{\%}3E3.0.CO{\%}3B2-J},
volume = {18},
year = {1999}
}
@article{Fisher1936,
author = {Fisher, R.A.},
journal = {Proceedings of the American Academy of Arts and Sciences},
number = {4},
pages = {245--258},
title = {{Uncertain Inference}},
url = {s},
volume = {71},
year = {1936}
}
@article{Collaboration2015,
abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47{\%} of original effect sizes were in the 95{\%} confidence interval of the replication effect size; 39{\%} of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68{\%} with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
author = {{Open Science Collaboration}},
doi = {10.1126/science.aac4716},
issn = {0036-8075},
journal = {Science},
month = {aug},
number = {6251},
pages = {aac4716--aac4716},
publisher = {American Association for the Advancement of Science},
title = {{Estimating the reproducibility of psychological science}},
url = {https://www.sciencemag.org/lookup/doi/10.1126/science.aac4716},
volume = {349},
year = {2015}
}
@article{anderson_asymptotic_1952,
abstract = {The statistical problem treated is that of testing the hypothesis that nnn independent, identically distributed random variables have a specified continuous distribution function F(x)F(x)F(x). If Fn(x)Fn(x)F{\_}n(x) is the empirical cumulative distribution function and $\psi$(t)$\psi$(t){\{}$\backslash$textbackslash{\}}psi(t) is some nonnegative weight function (0≦t≦1)(0≦t≦1)(0 {\{}$\backslash$textbackslash{\}}leqq t {\{}$\backslash$textbackslash{\}}leqq 1), we consider n12sup−∞{\textless}x{\textless}∞$\backslash${\{}{\{}$\backslash$textbar{\}}F(x)−Fn(x){\{}$\backslash$textbar{\}}$\psi$12[F(x)]{\}}n12sup−∞{\textless}x{\textless}∞$\backslash${\{}{\{}$\backslash$textbar{\}}F(x)−Fn(x){\{}$\backslash$textbar{\}}$\psi$12[F(x)]{\}}n{\{}$\backslash$textasciicircum{\}}$\backslash${\{}{\{}$\backslash$textbackslash{\}}frac{\{}1{\}}{\{}2{\}}{\}} {\{}$\backslash$textbackslash{\}}sup{\_}{\{}-{\{}$\backslash$textbackslash{\}}infty{\textless}x{\textless}{\{}$\backslash$textbackslash{\}}infty{\}} {\{}$\backslash$textbackslash{\}}$\backslash${\{}{\{}$\backslash$textbar{\}} F(x) - F{\_}n(x) {\{}$\backslash$textbar{\}} {\{}$\backslash$textbackslash{\}}psi{\{}$\backslash$textasciicircum{\}}{\{}$\backslash$textbackslash{\}}frac{\{}1{\}}{\{}2$\backslash${\}}{\{}$\backslash$textbackslash{\}}lbrack F(x) {\{}$\backslash$textbackslash{\}}rbrack{\{}$\backslash$textbackslash{\}}{\}} and n∫∞−∞[F(x)−Fn(x)]2$\psi$[F(x)]dF(x).n∫−∞∞[F(x)−Fn(x)]2$\psi$[F(x)]dF(x).n{\{}$\backslash$textbackslash{\}}int{\{}$\backslash$textasciicircum{\}}{\{}$\backslash$textbackslash{\}}infty{\_}{\{}-{\{}$\backslash$textbackslash{\}}infty$\backslash${\}}{\{}$\backslash$textbackslash{\}}lbrack F(x) - F{\_}n(x) {\{}$\backslash$textbackslash{\}}rbrack{\{}$\backslash$textasciicircum{\}}2 {\{}$\backslash$textbackslash{\}}psi{\{}$\backslash$textbackslash{\}}lbrack F(x){\{}$\backslash$textbackslash{\}}rbrack dF(x). A general method for calculating the limiting distributions of these criteria is developed by reducing them to corresponding problems in stochastic processes, which in turn lead to more or less classical eigenvalue and boundary value problems for special classes of differential equations. For certain weight functions including $\psi$=1$\psi$=1{\{}$\backslash$textbackslash{\}}psi = 1 and $\psi$=1/[t(1−t)]$\psi$=1/[t(1−t)]{\{}$\backslash$textbackslash{\}}psi = 1/{\{}$\backslash$textbackslash{\}}lbrack t(1 - t) {\{}$\backslash$textbackslash{\}}rbrack we give explicit limiting distributions. A table of the asymptotic distribution of the von Mises $\omega$2$\omega$2{\{}$\backslash$textbackslash{\}}omega{\{}$\backslash$textasciicircum{\}}2 criterion is given.},
author = {Anderson, T W and Darling, D A},
doi = {10.1214/aoms/1177729437},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Anderson, Darling - 1952 - Asymptotic Theory of Certain Goodness of Fit Criteria Based on Stochastic Processes.html:html},
issn = {0003-4851, 2168-8990},
journal = {The Annals of Mathematical Statistics},
month = {jun},
number = {2},
pages = {193--212},
title = {{Asymptotic Theory of Certain "Goodness of Fit" Criteria Based on Stochastic Processes}},
url = {http://projecteuclid.org/euclid.aoms/1177729437},
volume = {23},
year = {1952}
}
@incollection{Buzas2005,
address = {Berlin, Heidelberg},
author = {Buzas, Jeffrey S. and Stefanski, Leonard A. and Tosteson, Tor D.},
booktitle = {Handbook of Epidemiology},
pages = {729--765},
publisher = {Springer Berlin Heidelberg},
title = {{Measurement Error}},
year = {2005}
}
@article{thota_holistic_2010,
abstract = {This article describes a holistic approach to designing an introductory, object-oriented programming course. The design is grounded in constructivism and pedagogy of phenomenography. We use constructive alignment as the framework to align assessments, learning, and teaching with planned learning outcomes. We plan learning and teaching activities, and media with an understanding of variation theory and the ways in which students learn to program. We outline the implementation of the course, and discuss the findings from the first cycle of an action research study with a small sample of undergraduate students. An investigation of the preferred (deep/surface) learning approaches of the students led us to believe that these approaches can be influenced through course design. Personal constructs of the students, elicited through the repertory grid technique, revealed that rich inventories of learning resources are highly valued. We comment on the transformational processes of the experience of the participants, and identify areas for further refinement and investigation in the next action research cycle.},
author = {Thota, Neena and Whitfield, Richard},
doi = {10.1080/08993408.2010.486260},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Thota, Whitfield - 2010 - Holistic approach to learning and teaching introductory object-oriented programming.html:html},
issn = {0899-3408},
journal = {Computer Science Education},
keywords = {action research,constructive alignment,object-oriented programming,variation theory},
month = {jun},
number = {2},
pages = {103--127},
title = {{Holistic approach to learning and teaching introductory object-oriented programming}},
url = {http://dx.doi.org/10.1080/08993408.2010.486260},
volume = {20},
year = {2010}
}
@article{Wagenmakers2020b,
author = {Wagenmakers, Eric-Jan and Gronau, Quentin and Vandekerckhove, Joachim},
journal = {psyarxiv preprint, https://psyarxiv.com/5ntkd},
title = {{Five Bayesian Intuitions for the Stopping Rule Principle}},
year = {2020}
}
@article{Raftery1992,
author = {Raftery, Adrian E. and Lewis, Steven M.},
doi = {10.1214/ss/1177011143},
issn = {0883-4237},
journal = {Statistical Science},
month = {nov},
number = {4},
pages = {493--497},
publisher = {Institute of Mathematical Statistics},
title = {{[Practical Markov Chain Monte Carlo]: Comment: One Long Run with Diagnostics: Implementation Strategies for Markov Chain Monte Carlo}},
url = {http://projecteuclid.org/euclid.ss/1177011143},
volume = {7},
year = {1992}
}
@article{Dronamraju2012,
abstract = {This paper is a brief account of the scientific work of J.B.S. Haldane (1892-1964), with special reference to early research in Human Genetics. Brief descriptions of Haldane's background, his important contributions to the foundations of human genetics, his move to India from Great Britain and the research carried out in Human Genetics in India under his direction are outlined. Population genetic research on Y-linkage in man, inbreeding, color blindness and other aspects are described.},
author = {Dronamraju, Krishna},
doi = {10.4103/0971-6866.96634},
issn = {0971-6866},
journal = {Indian Journal of Human Genetics},
keywords = {Color blindness,Haldane,Human Genetics,Y-linkage,deaf-mutism and digital anomalies,inbreeding coefficient},
month = {jan},
number = {1},
pages = {3--8},
pmid = {22754215},
publisher = {Wolters Kluwer -- Medknow Publications},
title = {{Recollections of J.B.S. Haldane, with special reference to Human Genetics in India.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22754215 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3385175},
volume = {18},
year = {2012}
}
@article{Button2013,
abstract = {A study with low statistical power has a reduced chance of detecting a true effect, but it is less well appreciated that low power also reduces the likelihood that a statistically significant result reflects a true effect. Here, we show that the average statistical power of studies in the neurosciences is very low. The consequences of this include overestimates of effect size and low reproducibility of results. There are also ethical dimensions to this problem, as unreliable research is inefficient and wasteful. Improving reproducibility in neuroscience is a key priority and requires attention to well-established but often ignored methodological principles.},
author = {Button, Katherine S. and Ioannidis, John P.A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S.J. and Munaf{\`{o}}, Marcus R.},
doi = {10.1038/nrn3475},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Button et al. - 2013 - Power failure Why small sample size undermines the reliability of neuroscience.pdf:pdf},
isbn = {1471-0048 (Electronic)$\backslash$n1471-003X (Linking)},
issn = {1471003X},
journal = {Nature Reviews Neuroscience},
keywords = {Molecular neuroscience},
number = {5},
pages = {365--376},
pmid = {23571845},
publisher = {Nature Publishing Group},
title = {{Power failure: Why small sample size undermines the reliability of neuroscience}},
volume = {14},
year = {2013}
}
@techreport{Berger1996,
abstract = {The bioequivalence problem is of practical importance because the approval of most generic drugs in the United States and the European Community (EC) requires the establishment of bioequivalence between the brand-name drug and the proposed generic version. The problem is theoretically interesting because it has been recognized as one for which the desired inference, instead of the usual significant difference , is practical equivalence. The concept of intersection-union tests will be shown to clarify, simplify and unify bioequivalence testing. A test more powerful than the one currently specified by the FDA and EC guidelines will be derived. The claim that the bioequivalence problem defined in terms of the ratio of parameters is more difficult than the problem defined in terms of the difference of parameters will be refuted. The misconception that size-$\alpha$ bioequivalence tests generally correspond to 1001 − 2$\alpha${\%} confidence sets will be shown to lead to incorrect statistical practices, and should be abandoned. Techniques for constructing 1001−$\alpha${\%} confidence sets that correspond to size-$\alpha$ bioequivalence tests will be described. Finally, multiparameter bioequivalence problems will be discussed.},
author = {Berger, Roger L and Hsu, Jason C},
booktitle = {Statistical Science},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Berger, Hsu - 1996 - Bioequivalence Trials, Intersection-Union Tests and Equivalence Confidence Sets.pdf:pdf},
keywords = {and phrases: Bioequivalence,bioavailability,confidence interval,equivalence test,hypothesis test,intersection-union,level,pharmacokinetic,size,unbiased},
number = {4},
pages = {283--319},
title = {{Bioequivalence Trials, Intersection-Union Tests and Equivalence Confidence Sets}},
volume = {11},
year = {1996}
}
@book{Bollen1989,
abstract = {Analysis of Ordinal Categorical Data Alan Agresti Statistical Science Now has its first coordinated manual of methods for analyzing ordered categorical data. This book discusses specialized models that, unlike standard methods underlying nominal categorical data, efficiently use the information on ordering. It begins with an introduction to basic descriptive and inferential methods for categorical data, and then gives thorough coverage of the most current developments, such as loglinear and logit models for ordinal data. Special emphasis is placed on interpretation and application of methods and contains an integrated comparison of the available strategies for analyzing ordinal data. This is a case study work with illuminating examples taken from across the wide spectrum of ordinal categorical applications. 1984 (0 471-89055-3) 287 pp. Regression Diagnostics Identifying Influential Data and Sources of Collinearity David A. Belsley, Edwin Kuh and Roy E. Welsch This book provides the practicing statistician and econometrician with new tools for assessing the quality and reliability of regression estimates. Diagnostic techniques are developed that aid in the systematic location of data points that are either unusual or inordinately influential; measure the presence and intensity of collinear relations among the regression data and help to identify the variables involved in each; and pinpoint the estimated coefficients that are potentially most adversely affected. The primary emphasis of these contributions is on diagnostics, but suggestions for remedial action are given and illustrated. 1980 (0 471-05856-4) 292 pp. Applied Regression Analysis Second Edition Norman Draper and Harry Smith Featuring a significant expansion of material reflecting recent advances, here is a complete and up-to-date introduction to the fundamentals of regression analysis, focusing on understanding the latest concepts and applications of these methods. The authors thoroughly explore the fitting and checking of both linear and nonlinear regression models, using small or large data sets and pocket or high-speed computing equipment. Features added to this Second Edition include the practical implications of linear regression; the Durbin-Watson test for serial correlation; families of transformations; inverse, ridge, latent root and robust regression; and nonlinear growth models. Includes many new exercises and worked examples.},
address = {Hoboken, NJ, USA},
author = {Bollen, Kenneth A.},
booktitle = {Structural Equations with Latent Variables},
doi = {10.1002/9781118619179},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bollen - 1989 - Structural Equations with Latent Variables.pdf:pdf},
isbn = {9781118619179},
pages = {1--514},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Structural Equations with Latent Variables}},
year = {1989}
}
@article{Helfenstein2002,
abstract = {BACKGROUND: In the usual regression setting one regression line is computed for a whole data set. In a more complex situation, each person may be observed for example at several points in time and thus a regression line might be calculated for each person. Additional complexities, such as various forms of errors in covariables may make a straightforward statistical evaluation difficult or even impossible. OBJECTIVE AND METHOD: During recent years methods have been developed allowing convenient analysis of problems where the data and the corresponding models show these and many other forms of complexity. The methodology makes use of a Bayesian approach and Markov chain Monte Carlo (MCMC) simulations. The methods allow the construction of increasingly elaborate models by building them up from local sub-models. The essential structure of the models can be represented visually by directed acyclic graphs (DAG). This attractive property allows communication and discussion of the essential structure and the substantial meaning of a complex model without needing algebra. EXAMPLE: After presentation of the statistical methods an example from dentistry is presented in order to demonstrate their application and use. The dataset of the example had a complex structure; each of a set of children was followed up over several years. The number of new fillings in permanent teeth had been recorded at several ages. The dependent variables were markedly different from the normal distribution and could not be transformed to normality. In addition, explanatory variables were assumed to be measured with different forms of error. Illustration of how the corresponding models can be estimated conveniently via MCMC simulation, in particular, 'Gibbs sampling', using the freely available software BUGS is presented. In addition, how the measurement error may influence the estimates of the corresponding coefficients is explored. It is demonstrated that the effect of the independent variable on the dependent variable may be markedly underestimated if the measurement error is not taken into account ('regression dilution bias'). CONCLUSION: Markov chain Monte Carlo methods may be of great value to dentists in allowing analysis of data sets which exhibit a wide range of different forms of complexity.},
author = {Helfenstein, Ulrich and Menghini, Giorgio and Steiner, Marcel and Murati, Francesca},
isbn = {0265-539X (Print)$\backslash$r0265-539X (Linking)},
issn = {0265539X},
journal = {Community Dental Health},
keywords = {BUGS,Bayesian statistics,Complex models,Dental caries,Markov chain Monte Carlo},
month = {sep},
number = {3},
pages = {152--160},
pmid = {12269461},
title = {{An example of complex modelling in dentistry using Markov chain Monte Carlo (MCMC) simulation}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12269461},
volume = {19},
year = {2002}
}
@article{Neyman1936,
author = {Neyman, Jerzy and Pearson, E S},
journal = {Statistical Research Memoirs},
pages = {1--37},
title = {{Contributions to the theory of testing statistical hypotheses}},
url = {https://psycnet.apa.org/record/1936-05541-001},
volume = {1},
year = {1936}
}
@book{Good1950,
address = {London},
author = {Good, I.J.},
publisher = {Charles Griffin},
title = {{Probability and the Weighing of Evidence}},
year = {1950}
}
@article{Edwards1963,
author = {Edwards, Ward and Lindman, Harold and Savage, Leonard J.},
doi = {10.1037/h0044139},
issn = {1939-1471},
journal = {Psychological Review},
number = {3},
pages = {193--242},
title = {{Bayesian statistical inference for psychological research}},
volume = {70},
year = {1963}
}
@inproceedings{brinda_towards_2015,
abstract = {The German educational system is shifting towards the competences of learners since the publication of the national results of the OECD PISA studies with the goals to improve the overall learning outcomes, better individualize learning and teaching processes and also to promote the comparability of federal educational systems. Many disciplines have already developed empirically founded competency models and associated instruments to measure, compare, and interpret learner performance. Computer science education research is still in its infancy in this regard. It is therefore the aim of this project to develop such a model for the basics of object-oriented programming.},
address = {New York, NY, USA},
annote = {Germany},
author = {Brinda, Torsten and Kramer, Matthias and Hubwieser, Peter and Ruf, Alexander},
booktitle = {Proceedings of the 2015 ACM Conference on Innovation and Technology in Computer Science Education},
doi = {10.1145/2729094.2754848},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Brinda et al. - 2015 - Towards a Competency Model for Object-Oriented Programming.pdf:pdf},
isbn = {978-1-4503-3440-2},
keywords = {OOP,competency model,k-12,secondary education},
pages = {345},
publisher = {ACM},
series = {{\{}ITiCSE{\}} '15},
title = {{Towards a Competency Model for Object-Oriented Programming}},
url = {http://doi.acm.org/10.1145/2729094.2754848},
year = {2015}
}
@inproceedings{jordine_mobile-device_2014,
abstract = {Most first year computer science students would find that learning object-oriented programming is hard. Serious games have ever been used as one approach to handle this problem. But most of them cannot be played with mobile devices. This obviously does not suit the era of mobile computing that intends to allow students to learn programming skills in anytime anywhere. To enhance mobile teaching and learning, a research project started over a year ago and aims to create a mobile device based serious gaming approach along with a serious game for enhancing mobile teaching and learning for Java programming. So far the project has completed a literature review for understanding existing work and identifying problems in this area, conducted a survey for eliciting students' requirements for mobile gaming approach, and then established a mobile-device based serious gaming approach with a developed prototype of the game. This paper introduces the project, presents its details and discusses its current results. It is expected that the presented project will be helpful and useful to bring more efficient approaches with new mobile games into teaching object-oriented programming and to enhance students' learning experiences.},
annote = {R, Germany {\&} UK},
author = {Jordine, T and Liang, Y and Ihler, E},
booktitle = {2014 IEEE Frontiers in Education Conference (FIE) Proceedings},
doi = {10.1109/FIE.2014.7044206},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Jordine, Liang, Ihler - 2014 - A mobile-device based serious gaming approach for teaching and learning Java programming.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Jordine, Liang, Ihler - 2014 - A mobile-device based serious gaming approach for teaching and learning Java programming.html:html},
keywords = {Computer aided instruction,Computer science education,Games,Java programming,Land mobile radio,Programming profession,computer science student,education,educational courses,java,mobile computing,mobile device,mobile learning,mobile teaching,object-oriented programming,serious game,serious games (computing),serious gaming},
month = {oct},
pages = {1--5},
title = {{A mobile-device based serious gaming approach for teaching and learning Java programming}},
year = {2014}
}
@article{Spanos2013,
author = {Spanos, Aris},
doi = {10.1086/673730},
journal = {Philosophy of Science},
number = {1},
pages = {73--93},
title = {{Who should be afraid of the Jeffreys-Lindley paradox?}},
volume = {80},
year = {2013}
}
@incollection{Bjornstad1992,
address = {New York},
author = {Bj{\o}rnstad, Jan F.},
booktitle = {Breakthroughs in Statistics},
doi = {10.1007/978-1-4612-0919-5_31},
editor = {Kotz, Samuel and Johnson, Norman L.},
pages = {461--477},
publisher = {Springer, New York, NY},
title = {{Introduction to Birnbaum (1962) On the Foundations of Statistical Inference}},
url = {https://link.springer.com/chapter/10.1007/978-1-4612-0919-5{\_}31},
year = {1992}
}
@article{Gronau,
abstract = {AbstractAcross the empirical sciences, few statistical procedures rival the popularity of the frequentist t -test. In contrast, the Bayesian versions of the t -test have languished in obscurity. In...},
author = {Gronau, Quentin F. and Ly, Alexander and Wagenmakers, Eric-Jan},
doi = {10.1080/00031305.2018.1562983},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gronau, Ly, Wagenmakers - 2019 - Informed Bayesian t -Tests.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Bayes factor,Informed hypothesis test,Prior elicitation},
month = {may},
number = {2},
pages = {137--143},
publisher = {Taylor {\&} Francis},
title = {{Informed Bayesian t -Tests}},
volume = {74},
year = {2020}
}
@article{Box1976,
abstract = {Aspects of scientific method are discussed: In particular, its representation as a motivated iteration in which, in succession, practice confronts theory, and theory, practice. Rapid progress requires sufficient flexibility to profit from such confrontations, and the ability to devise parsimonious but effective models, to worry selectively about model inadequacies and to employ mathematics skillfully but appropriately. The development of statistical methods at Rothamsted Experimental Station by Sir Ronald Fisher is used to illustrate these themes.},
author = {Box, George E. P.},
doi = {10.2307/2286841},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Box - 1976 - Science and Statistics.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
number = {356},
pages = {791},
title = {{Science and Statistics}},
volume = {71},
year = {1976}
}
@article{Gelman1998,
author = {Gelman, Andrew and Brooks, Stephen P},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gelman, Brooks - 1998 - General Methods for Monitoring Convergence of Iterative Simulations.pdf:pdf},
journal = {Journal of Computational and Graphical Statistics},
keywords = {convergence diagnosis,inference,markov chain monte carlo},
number = {4},
pages = {434--455},
title = {{General Methods for Monitoring Convergence of Iterative Simulations}},
volume = {7},
year = {1998}
}
@misc{Altman2002,
abstract = {The aim of medical research is to advance scientific knowledge and hence--directly or indirectly--lead to improvements in the treatment and prevention of disease. Each research project should continue systematically from previous research and feed into future research. Each project should contribute beneficially to a slowly evolving body of research. A study should not mislead; otherwise it could adversely affect clinical practice and future research. In 1994 I observed that research papers commonly contain methodological errors, report results selectively, and draw unjustified conclusions. Here I revisit the topic and suggest how journal editors can help.},
author = {Altman, Douglas G.},
booktitle = {Journal of the American Medical Association},
doi = {10.1001/jama.287.21.2765},
issn = {00987484},
month = {jun},
number = {21},
pages = {2765--2767},
pmid = {12038906},
title = {{Poor-quality medical research: What can journals do?}},
volume = {287},
year = {2002}
}
@article{Colquhoun2014,
abstract = {If you use p =0.05 to suggest that you have made a discovery, you will be wrong at least 30{\%} of the time. If, as is often the case, experiments are underpowered, you will be wrong most of the time. This conclusion is demonstrated from several points of view. First, tree diagrams which show the close analogy with the screening test problem. Similar conclusions are drawn by repeated simulations of t tests. These mimic what is done in real life, which makes the results more persuasive. The simulation method is used also to evaluate the extent to which effect sizes are overestimated, especially in underpowered experiments. A script is supplied to allow the reader to do simulations themselves, with numbers appropriate for their own work. It is concluded that if you wish to keep your false discovery rate below 5{\%}, you need to use a threesigma rule, or to insist on p ≤0.001. And never use the word ‘significant'.},
archivePrefix = {arXiv},
arxivId = {1407.5296},
author = {Colquhoun, David},
doi = {10.1098/rsos.140216},
eprint = {1407.5296},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Colquhoun - 2014 - An investigation of the false discovery rate and the misinterpretation of p-values.pdf:pdf},
isbn = {2054-5703},
issn = {20545703},
journal = {Royal Society Open Science},
keywords = {False discovery rate,Reproducibility,Significance tests,Statistics},
number = {3},
pages = {140216--140216},
pmid = {16060722},
publisher = {The Royal Society},
title = {{An investigation of the false discovery rate and the misinterpretation of p-values}},
volume = {1},
year = {2014}
}
@book{Shao2003,
address = {New York},
author = {Shao, Jun},
doi = {10.1007/b97553},
isbn = {978-0-387-95382-3},
publisher = {Springer-Verlag New York},
title = {{Mathematical Statistics}},
year = {2003}
}
@article{trafimow_attenuation_2016,
abstract = {Much of the science reported in the media depends on correlation coefficients. But the size of correlation coefficients depends, in part, on the reliability with which the correlated variables are measured. Understanding this is a statistical literacy issue.},
author = {Trafimow, David},
doi = {10.1111/test.12087},
issn = {1467-9639},
journal = {Teaching Statistics},
keywords = {Attenuation,Correction for attenuation,Correlation coefficients,Regression to the mean,Reliability},
month = {mar},
number = {1},
pages = {25--28},
shorttitle = {The attenuation of correlation coefficients},
title = {{The attenuation of correlation coefficients: a statistical literacy issue}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/test.12087/abstract},
volume = {38},
year = {2016}
}
@article{Hardisty2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hardisty, Eric and Resnik, Philip},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {0198553595},
issn = {{\textless}null{\textgreater}},
journal = {UMIACS Technical Report},
keywords = {bayesian inference,gibbs sampling,markov chain monte carlo,na,ıve bayes},
number = {June},
pmid = {25246403},
title = {{Gibbs Sampling for the Uninitiated}},
year = {2010}
}
@inproceedings{or-bach_students_2003,
address = {New York, NY, USA},
author = {Or-Bach, Rachel and Lavy, Ilana},
booktitle = {Proceedings of the 8th Annual Conference on Innovation and Technology in Computer Science Education},
doi = {10.1145/961511.961610},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Or-Bach, Lavy - 2003 - Students' Understanding of Object Orientation.pdf:pdf},
isbn = {978-1-58113-672-2},
keywords = {abstraction,inheritance,object oriented design,object oriented programming,polymorphism,task analysis},
pages = {251},
publisher = {ACM},
series = {{\{}ITiCSE{\}} '03},
title = {{Students' Understanding of Object Orientation}},
url = {http://doi.acm.org/10.1145/961511.961610},
year = {2003}
}
@book{LeeWagenmakers2013,
abstract = {Subject: Bayesian inference has become a standard method of analysis in many fields of science. Students and researchers in experimental psychology and cognitive science, however, have failed to take full advantage of the new and exciting possibilities that the Bayesian approach affords. Ideal for teaching and self study, this book demonstrates how to do Bayesian modeling. Short, to-the-point chapters offer examples, exercises, and computer code (using WinBUGS or JAGS, and supported by Matlab and R), with additional support available online. No advance knowledge of statistics is required and, from the very start, readers are encouraged to apply and adjust Bayesian analyses by themselves. The book contains a series of chapters on parameter estimation and model selection, followed by detailed case studies from cognitive science. After working through this book, readers should be able to build their own Bayesian models, apply the models to their own data, and draw their own conclusions. Part I Getting started -- Part II Parameter estimation -- Part III Model selection -- Part IV Case studies.},
address = {Amsterdam},
author = {Lee, Michael David and Wagenmakers, Eric-Jan},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Wagenmakers - 2013 - Bayesian cognitive modeling a practical course.pdf:pdf},
isbn = {1107603579},
pages = {264},
publisher = {Cambridge University Press},
title = {{Bayesian cognitive modeling : a practical course}},
year = {2013}
}
@article{Carpenter2017,
abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.2.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propa- gation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line, through R using the RStan package, or through Python using the PyStan package. All three interfaces support sampling or optimization-based inference and analysis, and RStan and PyStan also provide access to log probabilities, gradients, Hessians, and data I/O.},
archivePrefix = {arXiv},
arxivId = {1210.1088},
author = {Carpenter, Bob and Guo, Jiqiang and Hoffman, Matthew D. and Brubaker, Marcus and Gelman, Andrew and Lee, Daniel and Goodrich, Ben and Li, Peter and Riddell, Allen and Betancourt, Michael},
doi = {10.18637/jss.v076.i01},
eprint = {1210.1088},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Carpenter et al. - 2017 - Stan A Probabilistic Programming Language.pdf:pdf},
isbn = {0394-6320 (Print) 0394-6320 (Linking)},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {1},
pages = {1--32},
pmid = {19505410},
title = {{Stan : A Probabilistic Programming Language}},
volume = {76},
year = {2017}
}
@techreport{ChenPearl2015,
address = {Los Angeles},
author = {Chen, Bryant and Pearl, Judea},
institution = {University of California, Los Angeles, Computer Science Department},
title = {{Graphical Tools for Linear Structural Equation Modeling}},
year = {2015}
}
@incollection{Pedersen1978,
author = {Pedersen, J. G.},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-3-642-04898-2_250},
issn = {03067734},
number = {2},
pages = {147--170},
pmid = {3605},
publisher = {Springer, New York, NY},
title = {{Fiducial Inference}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}13 http://medcontent.metapress.com/index/A65RM03P4874243N.pdf},
volume = {46},
year = {1978}
}
@article{Kmetz2019,
author = {Kmetz, John L.},
doi = {10.1080/00031305.2018.1518271},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kmetz - 2019 - Correcting Corrupt Research Recommendations for the Profession to Stop Misuse of ipi -Values.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {accepted soft-social-science,corrupt research,generally},
number = {sup1},
pages = {36--45},
title = {{Correcting Corrupt Research: Recommendations for the Profession to Stop Misuse of {\textless}i{\textgreater}p{\textless}/i{\textgreater} -Values}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1518271},
volume = {73},
year = {2019}
}
@article{bildung_standards_2005,
author = {Bildung, Informationszentrum I Z},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bildung - 2005 - Standards f{\"{u}}r die Lehrerbildung Bildungswissenschaften. Beschluss der Kultusministerkonferenz vom 16.12. 2004.pdf:pdf},
journal = {Zeitschrift f{\"{u}}r P{\"{a}}dagogik},
number = {2},
pages = {280--290},
shorttitle = {Standards f{\"{u}}r die {\{}Lehrerbildung{\}}},
title = {{Standards f{\"{u}}r die Lehrerbildung: Bildungswissenschaften. Beschluss der Kultusministerkonferenz vom 16.12. 2004}},
url = {http://www.pedocs.de/volltexte/2011/4756/},
volume = {51},
year = {2005}
}
@book{Ghosh2006,
address = {New York},
author = {Ghosh, Jayanta K. and Delampady, Mohan and Samanta, Tapas},
doi = {10.1007/978-0-387-35433-0},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2006 - An Introduction to Bayesian Analysis.pdf:pdf},
publisher = {Springer New York},
title = {{An Introduction to Bayesian Analysis}},
year = {2006}
}
@inproceedings{bergin_teaching_2005,
abstract = {Is it possible to teach dynamic polymorphism early? What techniques could facilitate teaching it in Java. This panel will bring together people who have considered this question and attempted to implement it in various ways, some more completely than others. It will also give participants an opportunity to explore the topic and to share their own ideas.},
address = {New York, NY, USA},
author = {Bergin, Joseph and Wallingford, Eugene and Caspersen, Michael and Goldweber, Michael and Kolling, Michael},
booktitle = {Proceedings of the 10th Annual SIGCSE Conference on Innovation and Technology in Computer Science Education},
doi = {10.1145/1067445.1067541},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bergin et al. - 2005 - Teaching Polymorphism Early.pdf:pdf},
isbn = {978-1-59593-024-8},
keywords = {first course,object-oriented programming,polymorphism},
pages = {342--343},
publisher = {ACM},
series = {{\{}ITiCSE{\}} '05},
title = {{Teaching Polymorphism Early}},
url = {http://doi.acm.org/10.1145/1067445.1067541},
year = {2005}
}
@article{Metropolis1987,
abstract = {The DNA story begins in 1871 with the isolation of DNA from pus cells by the Swiss scientist Friedrich Miescher, working in the laboratory of the eminent biochemist Felix Hoppe-Seyler in Germany. Since the material was isolated from nuclei, he called it “nuclein.” When nuclein was purified, it became clear that it was an acid, and it was then referred to as nucleic acid. Just prior to Miescher's publication on his findings, Charles Darwin published his book Origin of Species by Natural Selection and Gregor Mendel published the laws of inheritance. The circumstances of these three discoveries and their impacts are discussed.},
author = {Metropolis, Nicholas},
doi = {10.9783/9781512808797-001},
isbn = {978-0-12-812502-1},
issn = {1043-2256},
journal = {Los Alamos Science},
number = {Special Issue},
pages = {125--130},
pmid = {24024300},
title = {{The Beginning of the Monte Carlo Method}},
year = {1987}
}
@book{laplace1774,
author = {Laplace, Pierre-Simon de (1749-1827)},
pages = {p. 621},
title = {{Sur la Probabilit{\'{e}} des Causes par les {\'{E}}v{\'{e}}nements, M{\'{e}}moires de l'Acad{\'{e}}mie royale de Sciences de Paris, Tome IV}},
url = {http://gallica.bnf.fr/ark:/12148/bpt6k77596b/f32},
year = {1774}
}
@article{Huberty1993,
abstract = {Textbook discussion of statistical testing is the topic of interest. Some 28 books published from 1910 to 1949, 19 books published from 1990 to 1992, plus five multiple-edition books were reviewed in terms of presentations of statistical testing. It was of interest to discover textbook coverage of the P-value (i.e., Fisher) and fixed-alpha (i.e., Neyman-Pearson) approaches to statistical testing. Also of interest in the review were some issues and concerns related to the practice and teaching of statistical testing: (a) levels of significance, (b) importance of effects, (c) statistical power and sample size, and (d) multiple testing. It is concluded that it is not statistical testing itself that is at fault; rather, some of the textbook presentation, teaching practices, and journal editorial reviewing may be questioned. {\textcopyright} 1993 Taylor {\&} Francis Group, LLC.},
author = {Huberty, Carl J.},
doi = {10.1080/00220973.1993.10806593},
issn = {19400683},
journal = {Journal of Experimental Education},
number = {4},
pages = {317--333},
publisher = {Routledge},
title = {{Historical origins of statistical testing practices: The treatment of Fisher versus Neyman-Pearson views in textbooks}},
volume = {61},
year = {1993}
}
@article{Fisher1922,
abstract = {Reduced insulin/insulin-like growth factor 1 (IGF-1) signaling has been associated with longevity in various model organisms. However, the role of insulin/IGF-1 signaling in human survival remains controversial. The aim of this study was to test whether circulating IGF-1 axis parameters associate with old age survival and functional status in nonagenarians from the Leiden Longevity Study. This study examined 858 Dutch nonagenarian (males≥89 years; females≥91 years) siblings from 409 families, without selection on health or demographic characteristics. Nonagenarians were divided over sex-specific strata according to their levels of IGF-1, IGF binding protein 3 and IGF-1/IGFBP3 molar ratio. We found that lower IGF-1/IGFBP3 ratios were associated with improved survival: nonagenarians in the quartile of the lowest ratio had a lower estimated hazard ratio (95{\%} confidence interval) of 0.73 (0.59 – 0.91) compared to the quartile with the highest ratio (ptrend=0.002). Functional status was assessed by (Instrumental) Activities of Daily Living ((I)ADL) scales. Compared to those in the quartile with the highest IGF-1/IGFBP3 ratio, nonagenarians in the lowest quartile had higher scores for ADL (ptrend=0.001) and IADL (ptrend=0.003). These findings suggest that IGF-1 axis parameters are associated with increased old age survival and better functional status in nonagenarians from the Leiden Longevity Study.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fisher, R. A.},
doi = {10.1098/rsta.1922.0009},
eprint = {arXiv:1011.1669v3},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1922 - On the Mathematical Foundations of Theoretical Statistics.pdf:pdf},
isbn = {9788578110796},
issn = {1364-503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
number = {594-604},
pages = {309--368},
pmid = {25246403},
title = {{On the Mathematical Foundations of Theoretical Statistics}},
url = {http://rsta.royalsocietypublishing.org/cgi/doi/10.1098/rsta.1922.0009},
volume = {222},
year = {1922}
}
@article{Andrews2013a,
author = {Andrews, Mark and Baguley, Thom},
doi = {10.1111/bmsp.12004},
issn = {00071102},
journal = {British Journal of Mathematical and Statistical Psychology},
month = {feb},
number = {1},
pages = {1--7},
title = {{Prior approval: The growth of Bayesian methods in psychology}},
url = {http://doi.wiley.com/10.1111/bmsp.12004},
volume = {66},
year = {2013}
}
@article{Gigerenzer2015,
abstract = {The application of statistics to science is not a neutral act. Statistical tools have shaped and were also shaped by its objects. In the social sciences, statistical methods fundamentally changed research practice, making statistical inference its centerpiece. At the same time, textbook writers in the social sciences have transformed rivaling statistical systems into an apparently monolithic method that could be used mechanically. The idol of a universal method for scientific inference has been worshipped since the "inference revolution" of the 1950s. Because no such method has ever been found, surrogates have been created, most notably the quest for significant p values. This form of surrogate science fosters delusions and borderline cheating and has done much harm, creating, for one, a flood of irreproducible results. Proponents of the "Bayesian revolution" should be wary of chasing yet another chimera: an apparently universal inference procedure. A better path would be to promote both an understanding of the various devices in the "statistical toolbox" and informed judgment to select among these.},
author = {Gigerenzer, Gerd and Marewski, Julian N.},
doi = {10.1177/0149206314547522},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gigerenzer, Marewski - 2015 - Surrogate Science The Idol of a Universal Method for Scientific Inference.pdf:pdf},
issn = {15571211},
journal = {Journal of Management},
keywords = {Bayesian methods,psychometrics,regression analysis,research methods},
month = {feb},
number = {2},
pages = {421--440},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Surrogate Science: The Idol of a Universal Method for Scientific Inference}},
volume = {41},
year = {2015}
}
@article{Vrieze2012,
abstract = {This article reviews the Akaike information criterion (AIC) and the Bayesian information criterion (BIC) in model selection and the appraisal of psychological theory. The focus is on latent variable models, given their growing use in theory testing and construction. Theoretical statistical results in regression are discussed, and more important issues are illustrated with novel simulations involving latent variable models including factor analysis, latent profile analysis, and factor mixture models. Asymptotically, the BIC is consistent, in that it will select the true model if, among other assumptions, the true model is among the candidate models considered. The AIC is not consistent under these circumstances. When the true model is not in the candidate model set the AIC is efficient, in that it will asymptotically choose whichever model minimizes the mean squared error of prediction/estimation. The BIC is not efficient under these circumstances. Unlike the BIC, the AIC also has a minimax property, in that it can minimize the maximum possible risk in finite sample sizes. In sum, the AIC and BIC have quite different properties that require different assumptions, and applied researchers and methodologists alike will benefit from improved understanding of the asymptotic and finite-sample behavior of these criteria. The ultimate decision to use the AIC or BIC depends on many factors, including the loss function employed, the study's methodological design, the substantive research question, and the notion of a true model and its applicability to the study at hand. {\textcopyright} 2012 American Psychological Association.},
author = {Vrieze, Scott I.},
doi = {10.1037/a0027127},
issn = {1082989X},
journal = {Psychological Methods},
keywords = {Akaike information criterion,Bayesian information criterion,Factor analysis,Model selection,Theory testing},
month = {jun},
number = {2},
pages = {228--243},
title = {{Model selection and psychological theory: A discussion of the differences between the Akaike information criterion (AIC) and the Bayesian information criterion (BIC)}},
volume = {17},
year = {2012}
}
@article{Senn2001,
abstract = {There has been much work recently on individual bioequivalence and the topic has attracted considerable controversy. Some previous controversies regarding average bioequivalence are examined. It is argued that a contributory factor in these controversies may have been confusion over the purpose of bioequivalence trials. It is concluded that this purpose needs further clarification before guidelines for individual bioequivalence can be established and indeed that such guidelines may turn out to be unnecessary. Copyright {\textcopyright} 2001 John Wiley {\&} Sons, Ltd.},
author = {Senn, Stephen},
doi = {10.1002/sim.743},
issn = {0277-6715},
journal = {Statistics in Medicine},
month = {sep},
number = {17-18},
pages = {2785--2799},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Statistical issues in bioequivalance}},
volume = {20},
year = {2001}
}
@book{Efron2016,
abstract = {The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. ‘Big data', ‘data science', and ‘machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.},
address = {Cambridge},
author = {Efron, Bradley and Hastie, Trevor},
booktitle = {Computer Age Statistical Inference: Algorithms, Evidence, and Data Science},
doi = {10.1017/CBO9781316576533},
isbn = {9781316576533},
pages = {1--475},
publisher = {Cambridge University Press},
title = {{Computer age statistical inference: Algorithms, evidence, and data science}},
year = {2016}
}
@article{Utts2003b,
abstract = {Much has changed since the widespread introduction of statistics courses into the university curriculum, but the way introductory statistics courses are taught has not kept up with these changes. This article discusses the changes, and the way the introductory syllabus should change to reflect them. In particular, seven ideas are discussed that every student who takes elementary statistics should learn and understand in order to be an educated citizen. Misunderstanding these topics leads to cynicism among the public at best, and misuse of study results by policy-makers, physicians, and others at worst.},
author = {Utts, Jessica},
doi = {10.1198/0003130031630},
isbn = {0003-1305},
journal = {The American Statistician},
keywords = {Coincidences,Practical significance,Statistical literacy,Statistics education,Survey bias},
number = {2},
pages = {74--79},
title = {{What Educated Citizens Should Know About Statistics and Probability}},
url = {https://doi.org/10.1198/0003130031630},
volume = {57},
year = {2003}
}
@article{Watanabe2010,
abstract = {In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2$\lambda$/n, where $\lambda$ is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion. {\textcopyright} 2010 Sumio Watanabe.},
archivePrefix = {arXiv},
arxivId = {1004.2316},
author = {Watanabe, Sumio},
eprint = {1004.2316},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Watanabe - 2010 - Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theo.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Birational invariant,Cross-validation,Information criterion,Singular learning machine},
pages = {3571--3594},
title = {{Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory}},
volume = {11},
year = {2010}
}
@book{Kolling2010,
abstract = {Java lernen mit Greenfoot f{\"{u}}hrt Programmieranf{\"{a}}nger in die objektorientierte Programmierung mit Java ein. Um den Einstieg in Java zu erleichtern, wird die von Autor Michael K{\"{o}}lling mitentwickelte Lernumgebung Greenfoot verwendet, die es erlaubt, f{\"{u}}r dieses Buch einen etwas anderen didaktischen Ansatz als herk{\"{o}}mmliche Java-B{\"{u}}cher zu verwenden. Speziell f{\"{u}}r die Entwicklung von Simulationen und Spielen ausgelegt, erm{\"{o}}glicht Greenfoot den interaktiven Umgang mit Objekten und visualisiert die Klassenstruktur von Java-Projekten. Der Einsatz von Greenfoot f{\"{u}}hrt dazu, dass Objekte und Klassen von der ersten Seite an Schritt f{\"{u}}r Schritt thematisiert werden ("Objects first"). Der Autor liefert zu allen wichtigen Aspekten {\"{U}}bungen, die dem Leser helfen, das Erlernte zu festigen und den Lernerfolg zu {\"{u}}berpr{\"{u}}fen.Java lernen mit Greenfoot f{\"{u}}hrt Programmieranf{\"{a}}nger in die objektorientierte Programmierung mit Java ein. Um den Einstieg in Java zu erleichtern, wird die von Autor Michael K{\"{o}}lling mitentwickelte Lernumgebung Greenfoot verwendet, die es erlaubt, f{\"{u}}r dieses Buch einen etwas anderen didaktischen Ansatz als herk{\"{o}}mmliche Java-B{\"{u}}cher zu verwenden. Speziell f{\"{u}}r die Entwicklung von Simulationen und Spielen ausgelegt, erm{\"{o}}glicht Greenfoot den interaktiven Umgang mit Objekten und visualisiert die Klassenstruktur von Java-Projekten. Der Einsatz von Greenfoot f{\"{u}}hrt dazu, dass Objekte und Klassen von der ersten Seite an Schritt f{\"{u}}r Schritt thematisiert werden ("Objects first"). Der Autor liefert zu allen wichtigen Aspekten {\"{U}}bungen, die dem Leser helfen, das Erlernte zu festigen und den Lernerfolg zu {\"{u}}berpr{\"{u}}fen.AUS DEM INHALT:Standardprogrammiergrundlagen in JavaObjekte und KlassenObjektinteraktionen, ObjektsammlungenBenutzung von Bibliotheksklassen"Objects first"-AnsatzSchnelle Entwicklung von Animationen, Simulationen und SpielenAnregungen, Tipps und Tricks f{\"{u}}r eigene Projekte {\"{U}}BER DEN AUTOR: MICHAEL K{\"{O}}LLING ist einer der Entwickler von Greenfoot – einer preisgekr{\"{o}}nten Lernumgebung f{\"{u}}r Programmieranf{\"{a}}nger. Er lehrt zurzeit an der University of Kent in Canterbury.{\"{U}}BER DEN FACHLEKTOR:CARSTEN SCHULTE lehrt Didaktik der Informatik an der Freien Universit{\"{a}}t Berlin. Einer seiner Themenfelder ist die Didaktik und Methodik objektorientierter Zug{\"{a}}nge zur Informatik.AUF DER COMPANION WEBSITE:F{\"{u}}r LehrerAlle Abbildungen und Beispiele aus dem Buch F{\"{u}}r Sch{\"{u}}lerAlle Beispiele, Dateien und weiterf{\"{u}}hrende Links zum Thema Greenfoot},
address = {M{\"{u}}nchen},
author = {K{\"{o}}lling, Michael},
edition = {1},
isbn = {978-3-86894-902-5},
language = {Deutsch},
pages = {240},
publisher = {Pearson Studium},
shorttitle = {Einf{\"{u}}hrung in Java mit Greenfoot},
title = {{Einf{\"{u}}hrung in Java mit Greenfoot: Spielerische Programmierung mit Java}},
url = {https://www.amazon.de/Einf{\"{u}}hrung-Java-mit-Greenfoot-Programmierung/dp/386894902X/ref=sr{\_}1{\_}fkmr0{\_}3?ie=UTF8{\&}qid=1491469228{\&}sr=8-3-fkmr0{\&}keywords=michael+killing+greenfoot},
year = {2010}
}
@article{Billheimer2019,
abstract = {ABSTRACTMost statistical analyses use hypothesis tests or estimation about parameters to form inferential conclusions. I think this is noble, but misguided. The point of view expressed here is that observables are fundamental, and that the goal of statistical modeling should be to predict future observations, given the current data and other relevant information. Further, the prediction of future observables provides multiple advantages to practicing scientists, and to science in general. These include an interpretable numerical summary of a quantity of direct interest to current and future researchers, a calibrated prediction of what's likely to happen in future experiments, a prediction that can be either “corroborated” or “refuted” through experimentation, and avoidance of inference about parameters; quantities that exists only as convenient indices of hypothetical distributions. Finally, the predictive probability of a future observable can be used as a standard for communicating the reliability of th...},
author = {Billheimer, Dean},
doi = {10.1080/00031305.2018.1518270},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Billheimer - 2019 - Predictive Inference and Scientific Reproducibility.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {observables,prediction,predictive distribution,scientific inference},
number = {sup1},
pages = {291--295},
title = {{Predictive Inference and Scientific Reproducibility}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1518270},
volume = {73},
year = {2019}
}
@misc{Douven2011,
abstract = {Van Fraassen's Judy Benjamin problem has generally been taken to show that not all rational changes of belief can be modelled in a probabilistic framework if the available update rules are restricted to Bayes's rule and Jeffrey's generalization thereof. But alternative rules based on distance functions between probability assignments that allegedly can handle the problem seem to have counterintuitive consequences. Taking our cue from a recent proposal by Bradley, we argue that Jeffrey's rule can solve the Judy Benjamin problem after all. Moreover, we show that the specific instance of Jeffrey's rule that solves the Judy Benjamin problem can be underpinned by a particular distance function. Finally, we extend the set of distance functions to ones that take into account the varying degrees to which propositions may be epistemically entrenched. {\textcopyright} 2011 Cobreros.},
author = {Douven, Igor and Romeijn, Jan Willem},
booktitle = {Mind},
doi = {10.1093/mind/fzr051},
issn = {00264423},
number = {479},
pages = {637--670},
publisher = {Oxford Academic},
title = {{A new resolution of the Judy Benjamin problem}},
volume = {120},
year = {2011}
}
@article{Pratt1962,
author = {Pratt, John W.},
journal = {Journal of the American Statistical Association},
number = {298},
pages = {307--326},
title = {{On the Foundations of Statistical Inference: Discussion}},
volume = {57},
year = {1962}
}
@article{Bayarri2016,
abstract = {Much of science is (rightly or wrongly) driven by hypothesis testing. Even in situations where the hypothesis testing paradigm is correct, the common practice of basing inferences solely on p-values has been under intense criticism for over 50 years. We propose, as an alternative, the use of the odds of a correct rejection of the null hypothesis to incorrect rejection. Both pre-experimental versions (involving the power and Type I error) and post-experimental versions (depending on the actual data) are considered. Implementations are provided that range from depending only on the p-value to consideration of full Bayesian analysis. A surprise is that all implementations-even the full Bayesian analysis-have complete frequentist justification. Versions of our proposal can be implemented that require only minor modifications to existing practices yet overcome some of their most severe shortcomings.},
archivePrefix = {arXiv},
arxivId = {1512.08552},
author = {Bayarri, M. J. and Benjamin, Daniel J. and Berger, J.O. and Sellke, Thomas M.},
doi = {10.1016/j.jmp.2015.12.007},
eprint = {1512.08552},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bayarri et al. - 2016 - Rejection odds and rejection ratios A proposal for statistical practice in testing hypotheses(2).pdf:pdf},
isbn = {0022-2496},
issn = {10960880},
journal = {Journal of Mathematical Psychology},
keywords = {Bayes factors,Bayesian,Frequentist,Odds},
month = {dec},
pages = {90--103},
title = {{Rejection odds and rejection ratios: A proposal for statistical practice in testing hypotheses}},
url = {http://arxiv.org/abs/1512.08552},
volume = {72},
year = {2016}
}
@book{remmert_2016,
author = {Remmert, Volker R. and Schneider, Martina R. and S{\o}rensen, Henrik Kragh},
publisher = {Springer International Publishing Switzerland 2016},
title = {{Historiography of Mathematics in the 19th and 20th Centuries}},
year = {2016}
}
@article{Kastellec2010,
abstract = {Does public opinion influence Supreme Court confirmation politics? We present the first direct evidence that state-level public opinion on whether a particular Supreme Court nominee should be confirmed affects the roll-call votes of senators. Using national polls and applying recent advances in opinion estimation, we produce state-of-the-art estimates of public support for the confirmation of 10 recent Supreme Court nominees in all 50 states. We find that greater home-state public support does significantly and strikingly increase the probability that a senator will vote to approve a nominee, even controlling for other predictors of roll-call voting. These results establish a systematic and powerful link between constituency opinion and voting on Supreme Court nominees. We connect this finding to larger debates on the role of majoritarianism and representation. {\textcopyright} 2010 Southern Political Science Association.},
author = {Kastellec, Jonathan P. and Lax, Jeffrey R. and Phillips, Justin H.},
doi = {10.1017/S0022381610000150},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kastellec, Lax, Phillips - 2010 - Public opinion and senate confirmation of Supreme Court nominees.pdf:pdf},
issn = {00223816},
journal = {Journal of Politics},
number = {3},
pages = {767--784},
title = {{Public opinion and senate confirmation of Supreme Court nominees}},
volume = {72},
year = {2010}
}
@article{Madruga2001,
abstract = {C. Pereira and J. Stern have recently introduced a measure of evidence of a precise hypothesis consisting of the posterior probability of the set of points having smaller density than the supremum over the hypothesis. The related procedure is seen to be a Bayes test for specific loss functions. The nature of such loss functions and their relation to stylised inference problems are investigated. The dependence of the loss function on the sample is also discussed as well as the consequence of the introduction of Jeffreys's prior mass for the precise hypothesis on the separability of probability and utility.},
author = {Madruga, M. Regina and Esteves, Lu{\'{i}}s G. and Wechsler, Sergio},
doi = {10.1007/BF02595698},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Madruga, Esteves, Wechsler - 2001 - On the Bayesianity of Pereira-Stern tests.pdf:pdf},
issn = {11330686},
journal = {Test},
keywords = {Bayesian Inference,Decision Theory,Hypothesis test,Loss functions},
number = {2},
pages = {291--299},
publisher = {Sociedad de Estadistica e Investigacion Operativa},
title = {{On the Bayesianity of Pereira-Stern tests}},
volume = {10},
year = {2001}
}
@inproceedings{xinogalos_teaching_2007,
address = {New York, NY, USA},
annote = {Greece},
author = {Xinogalos, Stelios and Satratzemi, Maya and Dagdilelis, Vassilios},
booktitle = {Proceedings of the 12th Annual SIGCSE Conference on Innovation and Technology in Computer Science Education},
doi = {10.1145/1268784.1268914},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Xinogalos, Satratzemi, Dagdilelis - 2007 - Teaching Java with BlueJ A Two-year Experience.pdf:pdf},
isbn = {978-1-59593-610-3},
keywords = {java,object-oriented programming (OOP)},
pages = {345},
publisher = {ACM},
series = {{\{}ITiCSE{\}} '07},
shorttitle = {Teaching {\{}Java{\}} with {\{}BlueJ{\}}},
title = {{Teaching Java with BlueJ: A Two-year Experience}},
url = {http://doi.acm.org/10.1145/1268784.1268914},
year = {2007}
}
@article{Dass2004,
abstract = {When testing a point null hypothesis versus an alternative that is vaguely specified, a Bayesian test usually proceeds by putting a non-parametric prior on the alternative and then computing a Bayes factor based on the observations. This paper addresses the question of consistency, that is, whether the Bayes factor is correctly indicative of the null or the alternative as the sample size increases. We establish several consistency results in the affirmative under fairly general conditions. Consistency of Bayes factors for testing a point null versus a parametric alternative has long been known. The results here can also be viewed as the non-parametric extension of the parametric counterpart. {\textcopyright} 2002 Elsevier B.V. All rights reserved.},
author = {Dass, Sarat C. and Lee, Jaeyong},
doi = {10.1016/S0378-3758(02)00413-5},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Bayes factor,Consistency,Dirichlet process,Infinite dimensional exponential family,Polya tree},
month = {jan},
number = {1},
pages = {143--152},
publisher = {North-Holland},
title = {{A note on the consistency of Bayes factors for testing point null versus non-parametric alternatives}},
volume = {119},
year = {2004}
}
@unpublished{Kelter2018,
address = {NOVCONF 29th edition, University of Seville (IMUS), Seville, Spain},
author = {Kelter, Riko},
publisher = {University of Seville (IMUS)},
title = {{Markov-Chain-Monte-Carlo for Hypothesis Testing - An Alternative to p-Values with regards to the medical replication crisis}},
year = {2018}
}
@article{sharma_definitions_2017,
abstract = {Despite statistical literacy being relatively new in statistics education research, it needs special attention as attempts are being made to enhance the teaching, learning and assessing of this sub-strand. It is important that teachers and researchers are aware of the challenges of teaching this literacy. In this article, the growing importance of statistics in today's information world and conceptions and components of statistical literacy are outlined. Frameworks for developing statistical literacy from research literature are considered next. Examples of tasks used in statistics education research are provided to explain the levels of thinking. Strengths and weaknesses of the frameworks are considered. The article concludes with some implications for teaching and research.},
author = {Sharma, Sashi},
doi = {10.1080/23265507.2017.1354313},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Sharma - 2017 - Definitions and models of statistical literacy a literature review.pdf:pdf},
issn = {null},
journal = {Open Review of Educational Research},
keywords = {Statistical literacy,Statistics education research,implications for teaching and research,models of statistical literacy},
month = {jan},
number = {1},
pages = {118--133},
shorttitle = {Definitions and models of statistical literacy},
title = {{Definitions and models of statistical literacy: a literature review}},
url = {https://doi.org/10.1080/23265507.2017.1354313},
volume = {4},
year = {2017}
}
@article{Fisher1934,
abstract = {(1) Reasons are given for the use of mathematical likelihood in problems$\backslash$nof inductive inference.$\backslash$n(I1) When a statistic exists, satisfying the criterion of sufficiency, the$\backslash$nlikelihood function involves only that statistic.$\backslash$n(Ill) An example is given of a sufficient statistic, and its sampling distribution$\backslash$nis expressed in terms of the likelihood function.$\backslash$n(IV) This property is generalized for all cases of simple estimation, where$\backslash$na sufficient statistic exists.$\backslash$n(V) It is shown that these cases and only these supply tests of significance$\backslash$nof the kind termed by Neyman and Pearson "uniformly most powerful" with$\backslash$nregard to a class of alternative hypothesis.$\backslash$n(VI) Where no sufficient statistic exists the precision of estimation may in$\backslash$ngeneral be enhanced by the use of ancillary statistics. A class of cases is$\backslash$ndefined and illustrated in which the totality of the ancillary information supplied$\backslash$nby the observations may be utilized.$\backslash$n(VU) This process gives a very simple derivation of sampling distributions,$\backslash$nin which there is no loss of information, even for small samples.},
author = {Fisher, R. A.},
doi = {10.1098/rspa.1934.0050},
issn = {1364-5021},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
month = {mar},
number = {852},
pages = {285--307},
pmid = {1354382},
publisher = {The Royal Society},
title = {{Two New Properties of Mathematical Likelihood}},
url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1934.0050},
volume = {144},
year = {1934}
}
@article{Linde2020ROPE,
author = {Linde, Maximilian and Tendeiro, Jorge and Selker, Ravi and Wagenmakers, Eric-Jan and van Ravenzwaaij, Don},
journal = {psyarxiv preprint, https://psyarxiv.com/bh8vu},
title = {{Decisions About Equivalence: A Comparison of TOST, HDI-ROPE, and the Bayes Factor}},
year = {2020}
}
@article{Wells2007,
abstract = {The validity of a hypothesis test is partly determined by whether the assumptions underlying the test are satisfied. In the past, a preliminary analysis of the data has been suggested prior to the use of the statistical test. In this article, the authors describe several limitations of preliminary tests (e.g., influence on significance levels). Another strategy that uses theoretical knowledge in conjunction with prior empirical evidence and reason, prior to data collection, is described. {\textcopyright} 2007 Wiley Periodicals, Inc.},
author = {Wells, Craig S. and Hintze, John M.},
doi = {10.1002/pits.20241},
issn = {00333085},
journal = {Psychology in the Schools},
month = {may},
number = {5},
pages = {495--502},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Dealing with assumptions underlying statistical tests}},
volume = {44},
year = {2007}
}
@article{bainPackage,
author = {Gu, Xin and Hoijtink, Herbert and Mulder, Joris and van Lissa, Caspar and {Van Zundert}, Camiel and Jones, Jeff and Waller, Niels},
journal = {R package version 0.2.4},
title = {{bain: Bayes Factor for Informative Hypotheses}},
year = {2020}
}
@article{Rubin1974,
abstract = {Presents a discussion of matching, randomization, random sampling, and other methods of controlling extraneous variation. The objective was to specify the benefits of randomization in estimating causal effects of treatments. It is concluded that randomization should be employed whenever possible but that the use of carefully controlled nonrandomized data to estimate causal effects is a reasonable and necessary procedure in many cases. (15 ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1974 American Psychological Association.},
author = {Rubin, Donald B},
doi = {10.1037/h0037350},
issn = {00220663},
journal = {Journal of Educational Psychology},
keywords = {estimation of causal effects of treatments,randomization benefits},
number = {5},
pages = {688--701},
title = {{Estimating causal effects of treatments in randomized and nonrandomized studies}},
volume = {66},
year = {1974}
}
@article{Johnson2013a,
author = {Johnson, Timothy R. and Kuhn, Kristine M.},
doi = {10.3758/s13428-012-0300-3},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Johnson, Kuhn - 2013 - Bayesian Thurstonian models for ranking data using JAGS.pdf:pdf},
issn = {1554-3528},
journal = {Behavior Research Methods},
month = {sep},
number = {3},
pages = {857--872},
publisher = {Springer US},
title = {{Bayesian Thurstonian models for ranking data using JAGS}},
url = {http://link.springer.com/10.3758/s13428-012-0300-3},
volume = {45},
year = {2013}
}
@book{gaise_college_2016,
abstract = {The American Statistical Association (ASA) funded the Guidelines for Assessment and Instruction in Statistics Education (GAISE) Project, which consists of two groups, one focused on K–12 education and one focused on introductory college courses. This re- port presents the recommendations developed by the college group. The report includes a brief history of the introductory college course and summarizes the 1992 report1 by George Cobb that has been considered a generally accepted set of recommendations for teaching these courses. Results of a survey on the teaching of intro- ductory courses are summarized, along with a description of current versions of introductory statistics courses. We then offer a list of goals for students, based on what it means to be statistically literate. We present six recommendations for the teaching of introductory statistics that build on the previous recommendations from Cobb's report. Our six recommendations include the following: 1. Emphasize statistical literacy and develop statistical thinking 2. Use real data 3. Stress conceptual understanding, rather than mere knowledge of procedures 4. Foster active learning in the classroom 5. Use technology for developing conceptual understanding and analyzing data 6. Use assessments to improve and evaluate student learning The report concludes with suggestions for how to make these changes and includes numerous examples in the appendices to illus- trate details of the recommendations.},
author = {{American Statistical Association}},
booktitle = {Report},
doi = {10.3928/01484834-20140325-01},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/American Statistical Association - 2016 - Guidelines for Assessment and Instruction in Statistics Education (GAISE) Report, A Pre-K-12 C.pdf:pdf},
isbn = {9780979174711},
issn = {01484834},
title = {{Guidelines for Assessment and Instruction in Statistics Education (GAISE) Report, A Pre-K-12 Curriculum Framework}},
year = {2016}
}
@misc{Gabry2020RstanarmPriorsVignette,
author = {Gabry, J. and Goodrich, B.},
booktitle = {Comprehensive R Archive Network},
title = {{Prior Distributions for rstanarm Models}},
url = {https://cran.r-project.org/web/packages/rstanarm/vignettes/priors.html{\#}default-weakly-informative-prior-distributions},
urldate = {2020-05-11},
year = {2020}
}
@article{Neyman1957,
author = {Neyman, Jerzy},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Neyman - 1957 - Inductive Behavior as a Basic Concept of Philosophy of Science.pdf:pdf},
journal = {Review of the International Statistical Institute},
number = {1},
pages = {7--22},
title = {{"Inductive Behavior" as a Basic Concept of Philosophy of Science}},
volume = {25},
year = {1957}
}
@article{Hoijtink2016,
abstract = {The discussion following Bem's (2011) psi research highlights that applications of the Bayes factor in psychological research are not without problems. The first problem is the omission to translate subjective prior knowledge into subjective prior distributions. In the words of Savage (1961): “they make the Bayesian omelet without breaking the Bayesian egg.” The second problem occurs if the Bayesian egg is not broken: the omission to choose default prior distributions such that the ensuing inferences are well calibrated. The third problem is the adherence to inadequate rules for the interpretation of the size of the Bayes factor. The current paper will elaborate these problems and show how to avoid them using the basic hypotheses and statistical model used in the first experiment described in Bem (2011). It will be argued that a thorough investigation of these problems in the context of more encompassing hypotheses and statistical models is called for if Bayesian psychologists want to add a well-founded Bayes factor to the tool kit of psychological researchers.},
author = {Hoijtink, Herbert and van Kooten, Pascal and Hulsker, Koenraad},
doi = {10.1080/00273171.2014.969364},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hoijtink, van Kooten, Hulsker - 2016 - Why Bayesian Psychologists Should Change the Way They Use the Bayes Factor.pdf:pdf},
issn = {0027-3171},
journal = {Multivariate Behavioral Research},
keywords = {Bayes factor,calibrated Bayes,default prior distributions,frequency calculations,subjective prior distribution},
month = {jan},
number = {1},
pages = {2--10},
publisher = {Routledge},
title = {{Why Bayesian Psychologists Should Change the Way They Use the Bayes Factor}},
url = {https://www.tandfonline.com/doi/full/10.1080/00273171.2014.969364},
volume = {51},
year = {2016}
}
@book{Kleber1992,
address = {Weinheim},
annote = {Literaturverz. S. 301 - 324},
author = {Kleber, Eduard Werner},
booktitle = {Grundlagentexte P{\"{a}}dagogik},
isbn = {978-3-7799-0346-8},
keywords = {P{\"{a}}dagogische Diagnostik},
language = {ger},
pages = {335},
publisher = {Juventa-Verl},
shorttitle = {Diagnostik in p{\"{a}}dagogischen Handlungsfeldern},
title = {{Diagnostik in p{\"{a}}dagogischen Handlungsfeldern: Einf{\"{u}}hrung in Bewertung, Beurteilung, Diagnose und Evaluation}},
year = {1992}
}
@book{field_2016,
address = {London},
author = {Field, Andy},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Field - 2016 - An Adventure In Statistics - The Reality Enigma.pdf:pdf},
isbn = {9781446210444},
publisher = {SAGE Publications Ltd},
title = {{An Adventure In Statistics - The Reality Enigma}},
year = {2016}
}
@article{Andrieu2005,
abstract = {Pages 283-312,},
author = {Andrieu, Christophe and Moulines, {\'{E}}ric and Priouret, Pierre},
doi = {10.1109/CDC.2005.1583231},
isbn = {0780395689},
journal = {Proceedings of the 44th IEEE Conference on Decision and Control, and the European Control Conference, CDC-ECC '05},
number = {3},
pages = {6656--6661},
title = {{Stability of stochastic approximation under verifiable conditions}},
volume = {2005},
year = {2005}
}
@techreport{Neal1993,
address = {Toronto},
author = {Neal, Radford M.},
booktitle = {Technical Report CRG-TR-93-1},
institution = {Department of Computer Science, University of Toronto},
title = {{Probabilistic Inference Using Markov Chain Monte Carlo Methods}},
year = {1993}
}
@book{cohen_2001,
address = {New York, London},
author = {Cohen, Bernhard},
publisher = {W.W. Norton {\&} Company},
title = {{The Triumph Of Numbers - How Counting Shaped The Modern Life}}
}
@article{Fisher1925ApplicationOfStudentsT,
abstract = {Reproduced with permission of Metron},
author = {Fisher, Ronald Aylmer},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1925 - Applications of ``Student's'' Distribution.pdf:pdf},
journal = {Metron},
pages = {90--104},
title = {{Applications of ``Student's'' Distribution}},
url = {https://digital.library.adelaide.edu.au/dspace/handle/2440/15187},
volume = {5},
year = {1925}
}
@misc{Strasak2007,
abstract = {There is widespread evidence of the extensive use of statistical methods in medical research. Just the same, standards are generally low and a growing body of literature points to statistical errors in most medical journals. However, there is no comprehensive study contrasting the top medical journals of basic and clinical science for recent practice in their use of statistics. All original research articles in Volume 10, Numbers 1—6 of "Nature Medicine (Nat Med)" and Volume 350, Numbers 1—26 of "The New England Journal of Medicine (NEJM)" were screened for their statistical content. Types, frequencies, and complexity of applied statistical methods were systematically recorded. A 46-item checklist was used to evaluate statistical quality for a subgroup of papers. 94.5 percent (95{\%} CI 87.6—98.2) of NEJM articles and 82.4 percent (95{\%} CI 65.5—93.2) of Nat Med articles contained inferential statistics. NEJM papers were significantly more likely to use advanced statistical methods (p {\textless} 0.0001). Statistical errors were identified in a considerable proportion of articles, although not always serious in nature. Documentation of applied statistical methods was generally poor and insufficient, particularly in Nat Med. Compared to 1983, a vast increase in usage and complexity of statistical methods could be observed for NEJM papers. This does not necessarily hold true for Nat Med papers, as the results of the study indicate that basic science sticks with basic analysis. As statistical errors seem to remain common in medical literature, closser attention to statistical methodology should be seriously considered to raise standards.},
author = {Strasak, Alexander M and Zaman, Qamruz and Marinell, Gerhard and Pfeiffer, Karl P and Ulmer, Hanno},
booktitle = {American Statistician},
doi = {10.1198/000313007X170242},
issn = {00031305},
keywords = {Complexity,Errors and shortcomings,Statistical methods in medical journals,Techniques},
month = {feb},
number = {1},
pages = {47--55},
publisher = {Taylor {\&} Francis},
title = {{The use of statistics in medical research: A comparison of the new England journal of medicine and nature medicine}},
volume = {61},
year = {2007}
}
@article{Gronau2019,
author = {Gronau, Quentin F. and Wagenmakers, Eric-Jan and Heck, Daniel W. and Matzke, Dora},
doi = {10.1007/s11336-018-9648-3},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gronau et al. - 2019 - A Simple Method for Comparing Complex Models Bayesian Model Comparison for Hierarchical Multinomial Processing Tr.pdf:pdf},
issn = {0033-3123},
journal = {Psychometrika},
number = {1},
pages = {261--284},
publisher = {Springer US},
title = {{A Simple Method for Comparing Complex Models: Bayesian Model Comparison for Hierarchical Multinomial Processing Tree Models Using Warp-III Bridge Sampling}},
url = {http://link.springer.com/10.1007/s11336-018-9648-3},
volume = {84},
year = {2019}
}
@article{Box1980a,
abstract = {Scientific learning is an iterative process employing Criticism and Estimation. Correspondingly the formulated model factors into two complementary parts-a predictive part allowing model criticism, and a Bayes posterior part allowing estimation. Implications for significance tests, the theory of precise measurement and for ridge estimates are considered. Predictive checking functions for transformation, serial correlation, bad values, and their relation with Bayesian options are considered. Robustness is seen from a Bayesian viewpoint and examples are given. For the bad value problem a comparison with M estimators is made.},
author = {Box, George E. P.},
doi = {10.2307/2982063},
issn = {00359238},
journal = {Journal of the Royal Statistical Society. Series A (General)},
number = {4},
pages = {383},
publisher = {JSTOR},
title = {{Sampling and Bayes' Inference in Scientific Modelling and Robustness}},
volume = {143},
year = {1980}
}
@article{Wrinch1921,
author = {Wrinch, Dorothy and Jeffreys, Harold},
doi = {10.1080/14786442108633773},
issn = {1941-5982},
journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
number = {249},
pages = {369--390},
title = {{XLII. On certain fundamental principles of scientific inquiry}},
volume = {42},
year = {1921}
}
@article{Madruga2003,
abstract = {The full Bayesian significance test (FBST) for precise hypotheses is presented, with some illustrative applications. In the FBST we compute the evidence against the precise hypothesis. We discuss some of the theoretical properties of the FBST, and provide an invariant formulation for coordinate transformations, provided a reference density has been established. This evidence is the probability of the highest relative surprise set, "tangential" to the sub-manifold (of the parameter space) that defines the null hypothesis. {\textcopyright} 2002 Elsevier B.V. All rights reserved.},
author = {Madruga, M. Regina and Pereira, C. A. d. B. and Stern, J. M.},
doi = {10.1016/S0378-3758(02)00368-3},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Madruga, Pereira, Stern - 2003 - Bayesian evidence test for precise hypotheses.pdf:pdf},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Evidence,Full Bayesian significance test,Invariant formulation,Reference density,Relative surprise,Uninformative prior},
month = {dec},
number = {2},
pages = {185--198},
title = {{Bayesian evidence test for precise hypotheses}},
volume = {117},
year = {2003}
}
@book{Gelman2020,
abstract = {Most textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is about using regression to solve real problems of comparison, estimation, prediction, and causal inference. Unlike other books, it focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use immediately. Real examples, real stories from the authors' experience demonstrate what regression can do and its limitations, with practical advice for understanding assumptions and implementing methods for experiments and observational studies. They make a smooth transition to logistic regression and GLM. The emphasis is on computation in R and Stan rather than derivations, with code available online. Graphics and presentation aid understanding of the models and model fitting.},
address = {New York},
author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
booktitle = {Regression and Other Stories},
doi = {10.1017/9781139161879},
month = {jul},
publisher = {Cambridge University Press},
title = {{Regression and Other Stories}},
url = {https://www.cambridge.org/core/books/regression-and-other-stories/DD20DD6C9057118581076E54E40C372C},
year = {2020}
}
@article{Mcnicholas2016,
abstract = {Finite mixture models are being commonly used in a wide range of applications in practice concerning density estimation and clustering. An attractive feature of this approach to clustering is that it provides a sound statistical framework in which to assess the important question of how many clusters are there in the data and their validity. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1807.01987v1},
author = {Mcnicholas, Paul D.},
doi = {10.1007/s00357-016-9211-9},
eprint = {arXiv:1807.01987v1},
issn = {14321343},
journal = {Journal of Classification},
keywords = {Cluster,Cluster analysis,Mixture models},
number = {3},
pages = {331--373},
title = {{Model-Based Clustering}},
volume = {33},
year = {2016}
}
@book{Schervish1995,
address = {New York},
author = {Schervish, Mark J.},
publisher = {Springer Verlag},
title = {{Theory of Statistics}},
year = {1995}
}
@book{mc_elreath_2016,
address = {Boca Raton},
author = {McElreath, Richard},
doi = {10.3102/1076998616659752},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/McElreath - 2016 - Statistical Rethinking A Bayesian Course With Examples in R and Stan.pdf:pdf},
isbn = {1482253461},
issn = {1076-9986},
publisher = {Chapman {\&} Hall, CRC Press},
title = {{Statistical Rethinking: A Bayesian Course With Examples in R and Stan}},
url = {http://jeb.sagepub.com/cgi/doi/10.3102/1076998616659752},
year = {2016}
}
@article{Fisher1959,
abstract = {Paper delivered to the XVIII International Congress of Pharmaceutical Sciences, at Brussels, September 1958.},
author = {Fisher, Ronald A.},
doi = {10.1080/00401706.1959.10489846},
issn = {15372723},
journal = {Technometrics},
month = {feb},
number = {1},
pages = {21--29},
title = {{Mathematical Probability in the Natural Sciences}},
url = {https://www.jstor.org/stable/1266307?origin=crossref},
volume = {1},
year = {1959}
}
@article{Piironen2017,
abstract = {The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but has previously suffered from two problems. First, there has been no systematic way of specifying a prior for the global shrinkage hyperparameter based on the prior information about the degree of sparsity in the parameter vector. Second, the horseshoe prior has the undesired property that there is no possibility of specifying separately information about sparsity and the amount of regularization for the largest coefficients, which can be problematic with weakly identified parameters, such as the logistic regression coefficients in the case of data separation. This paper proposes solutions to both of these problems. We introduce a concept of effective number of nonzero parameters, show an intuitive way of formulating the prior for the global hyperparameter based on the sparsity assumptions, and argue that the previous default choices are dubious based on their tendency to favor solutions with more unshrunk parameters than we typically expect a priori. Moreover, we introduce a generalization to the horseshoe prior, called the regularized horseshoe, that allows us to specify a minimum level of regularization to the largest values. We show that the new prior can be considered as the continuous counterpart of the spike-and-slab prior with a finite slab width, whereas the original horseshoe resembles the spike-and-slab with an infinitely wide slab. Numerical experiments on synthetic and real world data illustrate the benefit of both of these theoretical advances.},
archivePrefix = {arXiv},
arxivId = {1707.01694},
author = {Piironen, Juho and Vehtari, Aki},
doi = {10.1214/17-EJS1337SI},
eprint = {1707.01694},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Piironen, Vehtari - 2017 - Sparsity information and regularization in the horseshoe and other shrinkage priors.pdf:pdf},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Bayesian inference,Horseshoe prior,Shrinkage priors,Sparse estimation},
number = {2},
pages = {5018--5051},
title = {{Sparsity information and regularization in the horseshoe and other shrinkage priors}},
volume = {11},
year = {2017}
}
@inproceedings{VermaPearl1988,
address = {Mountain View, CA},
author = {Verma, T. and Pearl, Judea},
booktitle = {Proceedings of the Fourth Workshop on Uncertainty in Artificial Intelligence},
pages = {352--359},
publisher = {Elsevier Science Publishers},
title = {{Causal networks: Semantics and expressiveness}},
year = {1988}
}
@misc{Stone1977,
abstract = {A logarithmic assessment of the performance of a predicting density is found to lead to asymptotic equivalence of choice of model by cross-validation and Akaike's criterion, when maximum likelihood estimation is used within each model.},
author = {Stone, M.},
booktitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
doi = {10.2307/2984877},
pages = {44--47},
publisher = {WileyRoyal Statistical Society},
title = {{An Asymptotic Equivalence of Choice of Model by Cross-Validation and Akaike's Criterion}},
url = {https://www.jstor.org/stable/2984877},
volume = {39},
year = {1977}
}
@article{Craiu2009,
abstract = {Starting with the seminal paper of Haario, Saksman and Tamminen (Haario et al. (2001)), a substantial amount of work has been done to validate adaptive Markov chain Monte Carlo algorithms. In this paper we focus on two practi- cal aspects of adaptive Metropolis samplers. First, we draw attention to the deficient performance of standard adaptation when the target distribution is multi-modal. We propose a parallel chain adaptation strategy that incorpo- rates multiple Markov chains which are run in parallel. Second, we note that the current adaptiveMCMC paradigmimplicitly assumes that the adaptation is uniformly efficient on all regions of the state space. However, in many practical instances, different “optimal” kernels are needed in different regions of the state space. We propose here a regional adaptation algorithm in which we account for possible errors made in defining the adaptation regions. This corresponds to the more realistic case in which one does not know exactly the optimal regions for adaptation. The methods focus on the random walk Metropolis sampling algorithm but their scope is much wider. We provide theoretical justification for the two adaptive approaches using the existent theory build for adaptive Markov chain Monte Carlo. We illustrate the performance of the methods us- ing simulations and analyze a mixture model for real data using an algorithm that combines the two approaches.},
author = {Craiu, Radu V. and Rosenthal., Jeffrey and Yang., Chao},
doi = {10.1198/jasa.2009.tm08393},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Adaptive Markov chain Monte Carlo,Metropolis sampling,Parallel chains,Random walk Metropolis sampling,Regional adaptation},
number = {488},
pages = {1454--1466},
title = {{Learn from thy neighbor: Parallel-Chain and regional adaptive MCMC}},
volume = {104},
year = {2009}
}
@article{Matthews2017,
author = {Matthews, Robert and Wasserstein, Ron and Spiegelhalter, David},
doi = {10.1111/j.1740-9713.2017.01021.x},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Matthews, Wasserstein, Spiegelhalter - 2017 - The ASA's p-value statement, one year on.pdf:pdf},
issn = {17409705},
journal = {Significance},
number = {2},
pages = {38--41},
publisher = {Wiley/Blackwell (10.1111)},
title = {{The ASA's p-value statement, one year on}},
volume = {14},
year = {2017}
}
@article{FDAAdaptiveDesignsForClinicalTrials,
author = {{U.S. Food and Drug Administration Center for Drug Evaluation and Research (CDER)}},
journal = {https://www.fda.gov/media/78495/download (accessed 01/08/2021)},
title = {{Adaptive Designs for Clinical Trials of Drugs and Biologics: Guidance for Industry}},
year = {2019}
}
@inproceedings{KelterFokos2020,
address = {Siegen},
author = {Kelter, Riko},
booktitle = {Proceedings on digitalization at the Institute for Advanced Study of the University of Siegen},
doi = {http://dx.doi.org/10.25819/ubsi/1894},
editor = {Radtke, J{\"{o}}rg and Klesel, Michael and Niehaves, Bj{\"{o}}rn},
pages = {100--108},
publisher = {Institute for Advanced Study of the University of Siegen},
title = {{New Perspectives on Statistical Data Analysis: Challenges and Possibilities of Digitalization for Hypothesis Testing in Quantitative Research}},
year = {2020}
}
@article{Metropolis1953,
abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two‐dimensional rigid‐sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four‐term virial coefficient expansion.},
archivePrefix = {arXiv},
arxivId = {5744249209},
author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N and Teller, Augusta H. and Teller, Edward},
doi = {http://dx.doi.org/10.1063/1.1699114},
eprint = {5744249209},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Metropolis et al. - 1953 - Equation of state calculations by fast computing machines.pdf:pdf},
isbn = {doi:10.1063/1.1699114},
issn = {00219606},
journal = {Journal Chemical Physics},
keywords = {and edward,arianna w,ation of state calculations,augusta h,by fast computing machines,marshall n,nicholas metropolis,rosenbluth,teller},
number = {6},
pages = {1087--1092},
pmid = {2797},
title = {{Equation of state calculations by fast computing machines}},
url = {http://dx.doi.org/10.1063/1.1699114 http://jcp.aip.org/resource/1/JCPSA6/v21/i6 http://jcp.aip.org/ http://jcp.aip.org/about/about{\_}the{\_}journal http://jcp.aip.org/features/most{\_}downloaded http://jcp.aip.org/authors http://jcp.aip.org/resource/1/jcpsa6/v21/},
volume = {21},
year = {1953}
}
@article{Rosenthal2007,
author = {Rosenthal, Jeffrey S},
number = {February},
pages = {1--6},
title = {{AMCMC : An R interface for adaptive MCMC}},
year = {2007}
}
@incollection{Hinkley1980a,
author = {Hinkley, David},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_10},
pages = {85--94},
publisher = {Springer, New York, NY},
title = {{Theory of Statistical Estimation: The 1925 Paper}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}10},
year = {1980}
}
@inproceedings{jiang_coding_2013,
abstract = {Object Oriented Programming is an important subject for computer major students, this course not only delivers the expertise on how to write program in an OOP language, but also guides students to think in object oriented paradigm. Teachers always found themselves in confusion on which language should be used as the teaching tool. In this paper we present some reform measures took based on our own teaching experiences, by introducing the thinking in object oriented paradigm. We use C{\#}, C++ and Java in parallel to deliver the course. By emphasizing the programming standard in coding, the students acquired the knowledge to distinguish the 3 languages in practice as well as the OOP itself.},
author = {Jiang, Z and Zhao, Y and Qin, Z and Lin, X},
booktitle = {2013 International Conference on Computational and Information Sciences},
doi = {10.1109/ICCIS.2013.494},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Jiang et al. - 2013 - Coding Standard Based Object Oriented Programming Course Teaching Reform.html:html;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Jiang et al. - 2013 - Coding Standard Based Object Oriented Programming Course Teaching Reform.pdf:pdf},
keywords = {C{\#},C++,C++ language,Computer science education,Computers,Educational institutions,OOP language,Programming profession,Standards,coding standard,coding standard based object oriented programming,computer major students,educational courses,java,object oriented paradigm,object oriented programming,object-oriented languages,object-oriented programming,programming standard,teaching,teaching experiences},
month = {jun},
pages = {1890--1892},
title = {{Coding Standard Based Object Oriented Programming Course Teaching Reform}},
year = {2013}
}
@article{Glantz1980,
abstract = {Approximately half the articles published in medical journals that use statistical methods use them incorrectly. These errors are so widespread that the present system of peer review has not been able to control them. This article presents a few rules of thumb to help readers identify questionable statistical analysis and estimate what the authors would have concluded had they used appropriate statistical methods. To prevent such errors from appearing, journals should secure review by someone knowledgeable in statistics before accepting a manuscript. In addition, human research committees should insist that an investigator define an appropriate strategy for data analysis before approving a protocol. Such policies should quickly and effectively increase the reliability of the clinical and scientific literature.},
author = {Glantz, S A},
doi = {10.1161/01.CIR.61.1.1},
issn = {00097322},
journal = {Circulation},
month = {jan},
number = {1},
pages = {1--7},
pmid = {7349923},
title = {{Biostatistics: how to detect, correct and prevent errors in the medical literature.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/7349923},
volume = {61},
year = {1980}
}
@book{Robert2010a,
abstract = {This is the first book to present modern Monte Carlo and Markov Chain Monte Carlo (MCMC) methods from a practical perspective through a guided implementation in the R language All concepts are carefully described with the abstract theoretical background replaced with a corresponding R program that the reader can use and modify at will The whole entire series of examples from the book is accompanied by a free R package called mcsm that allows for immediate techniques based on simulation have now become an essential part of the statistician's toolbox. It is thus crucial to provide statisticians with a practical understanding of those methods, and there is no better way to develop intuition and skills for simulation than to use simulation to solve statistical problems. Introducing Monte Carlo Methods with R covers the main tools used in statistical simulation from a programmer's point of view, explaining the R implementation of each simulation technique and providing the output for better understanding and comparison. While this book constitutes a comprehensive treatment of simulation methods, the theoretical justification of those methods has been considerably reduced, compared with Robert and Casella (2004). Similarly, the more exploratory and less stable solutions are not covered here. This book does not require a preliminary exposure to the R programming language or to Monte Carlo methods, nor an advanced mathematical background. While many examples are set within a Bayesian framework, advanced expertise in Bayesian statistics is not required. The book covers basic random generation algorithms, Monte Carlo techniques for integration and optimization, convergence diagnoses, Markov chain Monte Carlo methods, including Metropolis Hastings and Gibbs algorithms, and adaptive algorithms. All chapters include exercises and all R programs are available as an R package called mcsm. The book appeals to anyone with a practical interest in simulation methods but no previous exposure. It is meant to be useful for students and practitioners in areas such as statistics, signal processing, communications engineering, control theory, econometrics, finance and more. The programming parts are introduced progressively to be accessible to any reader.},
address = {New York},
author = {Robert, Christian and Casella, George},
booktitle = {Introducing Monte Carlo Methods with R},
doi = {10.1007/978-1-4419-1576-4},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Robert, Casella - 2010 - Introducing Monte Carlo Methods with R.pdf:pdf},
publisher = {Springer New York},
title = {{Introducing Monte Carlo Methods with R}},
year = {2010}
}
@incollection{Choi2008,
abstract = {In recent years, the literature in the area of Bayesian asymptotics has been rapidly growing. It is increasingly important to understand the concept of posterior consistency and validate specific Bayesian methods, in terms of consistency of posterior distributions. In this paper, we build up some con- ceptual issues in consistency of posterior distributions, and discuss panoramic views of them by comparing various approaches to posterior consistency that have been investigated in the literature. In addition, we provide interesting results on posterior consistency that deal with non-exponential consistency, improper priors and non i.i.d. (independent but not identically distributed) observations. We describe a few examples for illustrative purposes.},
author = {Choi, Taeryon and Ramamoorthi, R. V.},
booktitle = {Pushing the Limits of Contemporary Statistics: Contributions in Honor of Jayanta K. Ghosh},
doi = {10.1214/074921708000000138},
keywords = {Doob's theorem,affinity,exponential consistency,posterior distribution,strongly separated},
pages = {170--186},
publisher = {Institute of Mathematical Statistics},
title = {{Remarks on consistency of posterior distributions}},
url = {https://projecteuclid.org/euclid.imsc/1209398468},
year = {2008}
}
@article{Dienes2016,
author = {Dienes, Zoltan},
doi = {10.1016/j.jmp.2015.10.003},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Dienes - 2016 - How Bayes factors change scientific practice.pdf:pdf},
issn = {0022-2496},
journal = {Journal of Mathematical Psychology},
pages = {78--89},
publisher = {Elsevier Inc.},
title = {{How Bayes factors change scientific practice}},
url = {http://dx.doi.org/10.1016/j.jmp.2015.10.003},
volume = {72},
year = {2016}
}
@inproceedings{nguyen_learning_2014,
abstract = {Collaborative programming is an effective approach to software development, improving software quality, programmer's satisfaction and shortening delivery time This study examines the application of a collaborative Web-based IDE named IDEOL to execute a four-week multi-submission programming assignment in an introductory object-oriented programming class. Forty eight students forming 24 two-member groups in class used the IDE to interact and write source code required by the project. All collaborative and programming activities performed by students were recorded by IDEOL. The results of the study shows that students tend to postpone their programming work until the submission dates. This study also provides an approach to designing and executing an extended programming exercises, which receives high student satisfaction. Our results imply that IDEOL is a useful environment for students to collaborate, learn, and practice programming to improve their learning satisfaction. In addition, as students tend to procrastinate, IDEOL is a useful tool to facilitate, monitor, and report student progress in extended programming exercises.},
annote = {Vietnam},
author = {Nguyen, V and Dang, H H and Do, K N and Tran, T D},
booktitle = {2014 {\{}IEEE{\}} {\{}Frontiers{\}} in {\{}Education{\}} {\{}Conference{\}} ({\{}FIE{\}}) {\{}Proceedings{\}}},
doi = {10.1109/FIE.2014.7044141},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen et al. - 2014 - Learning and practicing object-oriented programming using a collaborative web-based {\{}IDE{\}}.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen et al. - 2014 - Learning and practicing object-oriented programming using a collaborative web-based {\{}IDE{\}}.html:html},
keywords = {Atmospheric measurements,Collaboration,Collaborative IDE,Computer science education,Debugging,IDEOL,Internet,Message systems,Particle measurements,Programming profession,Web-based IDE,collaborative Web-based IDE,collaborative programming,groupware,interaction,introductory object-oriented programming class,learning satisfaction,multisubmission programming assignment,object-oriented programming,programmer satisfaction,programming environments,programming exercise,software development,software quality,source code,source code (software),student satisfaction},
month = {oct},
pages = {1--9},
title = {{Learning and practicing object-oriented programming using a collaborative web-based {\{}IDE{\}}}},
year = {2014}
}
@incollection{Lane1980,
address = {New York},
author = {Lane, David A.},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_15},
editor = {Fienberg, Stephen E. and Hinkley, David V.},
pages = {148--160},
publisher = {Springer Verlag New York},
title = {{Fisher, Jeffreys, and the Nature of Probability}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}15},
year = {1980}
}
@article{Dienes2018,
abstract = {Inference using significance testing and Bayes factors is compared and contrasted in five case studies based on real research. The first study illustrates that the methods will often agree, both in motivating researchers to conclude that H1 is supported better than H0, and the other way round, that H0 is better supported than H1. The next four, however, show that the methods will also often disagree. In these cases, the aim of the paper will be to motivate the sensible evidential conclusion, and then see which approach matches those intuitions. Specifically, it is shown that a high-powered non-significant result is consistent with no evidence for H0 over H1 worth mentioning, which a Bayes factor can show, and, conversely, that a low-powered non-significant result is consistent with substantial evidence for H0 over H1, again indicated by Bayesian analyses. The fourth study illustrates that a high-powered significant result may not amount to any evidence for H1 over H0, matching the Bayesian conclusion. Finally, the fifth study illustrates that different theories can be evidentially supported to different degrees by the same data; a fact that P-values cannot reflect but Bayes factors can. It is argued that appropriate conclusions match the Bayesian inferences, but not those based on significance testing, where they disagree.},
author = {Dienes, Zoltan and Mclatchie, Neil},
doi = {10.3758/s13423-017-1266-z},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Dienes, Mclatchie - 2018 - Four reasons to prefer Bayesian analyses over significance testing.pdf:pdf},
issn = {15315320},
journal = {Psychonomic Bulletin and Review},
keywords = {Bayes factor,Bayesian statistics,Power,Significance testing,Statistics},
number = {1},
pages = {207--218},
pmid = {28353065},
publisher = {Springer New York LLC},
title = {{Four reasons to prefer Bayesian analyses over significance testing}},
volume = {25},
year = {2018}
}
@article{Craiu2014,
abstract = {Markov chain Monte Carlo (MCMC) algorithms are an indispensable tool for performing Bayesian inference. This review discusses widely used sampling algorithms and illustrates their implementation on a probit regression model for lupus data. The examples considered highlight the importance of tuning the simulation parameters and underscore the important contributions of modern developments such as adaptive MCMC. We then use the theory underlying MCMC to explain the validity of the algorithms considered and to assess the variance of the resulting Monte Carlo estimators.},
author = {Craiu, Radu and Rosenthal, Jeffrey S.},
doi = {10.1146/annurev-statistics-022513-115540},
journal = {Annual Review of Statistics and Its Applications},
number = {1},
pages = {179--201},
title = {{Bayesian Computation Via Markov Chain Monte Carlo}},
volume = {1},
year = {2014}
}
@article{Camerer2016,
abstract = {The replicability of some scientific findings has recently been called into question.To contribute data about replicability in economics, we replicated 18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. All of these replications followedpredefined analysis plans that weremade publicly available beforehand, and they all have a statistical power of at least 90{\%} to detect the original effect size at the5{\%}significance level.We found a significant effect in the same direction as in the original study for 11 replications (61{\%}); on average, the replicated effect size is66{\%}of the original.The replicability rate varies between 67{\%} and 78{\%} for four additional replicability indicators, including a prediction market measure of peer beliefs.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Camerer, Colin F and Dreber, Anna and Forsell, Eskil and Ho, Teck Hua and Huber, J{\"{u}}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
doi = {10.1126/science.aaf0918},
eprint = {arXiv:1011.1669v3},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Camerer et al. - 2016 - Evaluating replicability of laboratory experiments in economics.pdf:pdf},
isbn = {1068300691594},
issn = {10959203},
journal = {Science},
month = {mar},
number = {6280},
pages = {1433--1436},
pmid = {26940865},
publisher = {American Association for the Advancement of Science},
title = {{Evaluating replicability of laboratory experiments in economics}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26940865},
volume = {351},
year = {2016}
}
@article{Brownstein2019,
abstract = {This article resulted from our participation in the session on the "role of expert opinion and judgment in statistical inference" at the October 2017 ASA Symposium on Statistical Inference. We present a strong, unified statement on roles of expert judgment in statistics with processes for obtaining input, whether from a Bayesian or frequentist perspective. Topics include the role of subjectivity in the cycle of scientific inference and decisions, followed by a clinical trial and a greenhouse gas emissions case study that illustrate the role of judgments and the importance of basing them on objective information and a comprehensive uncertainty assessment. We close with a call for increased proactivity and involvement of statisticians in study conceptualization, design, conduct, analysis, and communication. ARTICLE HISTORY},
author = {Brownstein, Naomi C. and Louis, Thomas A. and O'Hagan, Anthony and Pendergast, Jane},
doi = {10.1080/00031305.2018.1529623},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Brownstein et al. - 2019 - The Role of Expert Judgment in Statistical Inference and Evidence-Based Decision-Making.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {bayesian paradigm,clinical,collaboration,elicitation,scientific method,trials},
number = {sup1},
pages = {56--68},
title = {{The Role of Expert Judgment in Statistical Inference and Evidence-Based Decision-Making}},
volume = {73},
year = {2019}
}
@article{Hurlbert2019,
abstract = {ABSTRACTMany controversies in statistics are due primarily or solely to poor quality control in journals, bad statistical textbooks, bad teaching, unclear writing, and lack of knowledge of the historical literature. One way to improve the practice of statistics and resolve these issues is to do what initiators of the 2016 ASA statement did: take one issue at a time, have extensive discussions about the issue among statisticians of diverse backgrounds and perspectives and eventually develop and publish a broadly supported consensus on that issue. Upon completion of this task, we then move on to deal with another core issue in the same way. We propose as the next project a process that might lead quickly to a strong consensus that the term “statistically significant” and all its cognates and symbolic adjuncts be disallowed in the scientific literature except where focus is on the history of statistics and its philosophies and methodologies. Calculation and presentation of accurate p-values will often remain...},
author = {Hurlbert, Stuart H. and Levine, Richard A. and Utts, Jessica},
doi = {10.1080/00031305.2018.1543616},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hurlbert, Levine, Utts - 2019 - Coup de Gr{\^{a}}ce for a Tough Old Bull “Statistically Significant” Expires.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {assessment,neofisherian significance,significance,statistical,type i error},
number = {sup1},
pages = {352--357},
title = {{Coup de Gr{\^{a}}ce for a Tough Old Bull: “Statistically Significant” Expires}},
volume = {73},
year = {2019}
}
@article{Krueger2001,
abstract = {Null hypothesis significance testing (NHST) is the researcher's workhorse for making inductive inferences. This method has often been challenged, has occasionally been defended, and has persistently been used through most of the history of scientific psychology. This article reviews both the criticisms of NHST and the arguments brought to its defense. The review shows that the criticisms address the logical validity of inferences arising from NHST, whereas the defenses stress the pragmatic value of these inferences. The author suggests that both critics and apologists implicit rely on Bayesian assumptions. When these assumptions are made explicit, the primary challenge or NHST - and any system of induction - can be confronted The challenge is to find a solution to the question of replicability.},
author = {Krueger, Joachim},
doi = {10.1037/0003-066X.56.1.16},
issn = {0003066X},
journal = {American Psychologist},
keywords = {Humans,J Krueger,MEDLINE,NCBI,NIH,NLM,National Center for Biotechnology Information,National Institutes of Health,National Library of Medicine,PubMed Abstract,Reproducibility of Results,Research Design*,Research*,doi:10.1037/0003-066x.56.1.16,pmid:11242984},
number = {1},
pages = {16--26},
pmid = {11242984},
publisher = {American Psychological Association Inc.},
title = {{Null hypothesis significance testing: On the survival of a flawed method}},
url = {https://pubmed.ncbi.nlm.nih.gov/11242984/},
volume = {56},
year = {2001}
}
@article{Fraser1963,
author = {Fraser, D.A.S.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fraser - 1963 - On the Sufficiency and Likelihood Principles.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {303},
pages = {641--647},
title = {{On the Sufficiency and Likelihood Principles}},
volume = {58},
year = {1963}
}
@article{Harlow2016,
abstract = {The introduction to this special issue on psychological research involving big data summarizes the highlights of 10 articles that address a number of important and inspiring perspectives, issues, and applications. Four common themes that emerge in the articles with respect to psychological research conducted in the area of big data are mentioned, including: (a) The benefits of collaboration across disciplines, such as those in the social sciences, applied statistics, and computer science. Doing so assists in grounding big data research in sound theory and practice, as well as in affording effective data retrieval and analysis. (b) Availability of large data sets on Facebook, Twitter, and other social media sites that provide a psychological window into the attitudes and behaviors of a broad spectrum of the population. (c) Identifying, addressing, and being sensitive to ethical considerations when analyzing large data sets gained from public or private sources. (d) The unavoidable necessity of validating predictive models in big data by applying a model developed on 1 dataset to a separate set of data or hold-out sample. Translational abstracts that summarize the articles in very clear and understandable terms are included in Appendix A, and a glossary of terms relevant to big data research discussed in the articles is presented in Appendix B.},
author = {Harlow, Lisa L. and Oswald, Frederick L.},
doi = {10.1037/met0000120},
issn = {1082989X},
journal = {Psychological Methods},
keywords = {Big data,Digital footprint,Machine learning,Social media data,Statistical learning theory},
month = {dec},
number = {4},
pages = {447--457},
pmid = {27918177},
publisher = {American Psychological Association Inc.},
title = {{Big data in psychology: Introduction to the special issue}},
volume = {21},
year = {2016}
}
@article{Diniz2012,
abstract = {The goal of this paper is to illustrate how two significance indices-the frequentist p-value and Bayesian e-value-have a straight mathematical relationship. We calculate these indices for standard statistical situations in which sharp null hypotheses are being tested. The p-value considered here is based on the likelihood ratio statistic. The existence of a functional relationship between these indices could surprise readers because they are computed in different spaces: p-values in the sample space and e-values in the parameter space.},
author = {Diniz, Marcio and Pereira, Carlos A B and Polpo, Adriano and Stern, Julio M and Wechsler, Sergio},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Diniz et al. - 2012 - Relationship between Bayesian and frequentist significance indices.pdf:pdf},
journal = {International Journal for Uncertainty Quantification},
keywords = {Bayesian test,e-value,likelihood ratio,p-values,significance test},
number = {2},
pages = {161--172},
title = {{Relationship between Bayesian and frequentist significance indices}},
volume = {2},
year = {2012}
}
@book{Savage1962FoundationsOfStatistics,
address = {London},
author = {Savage, L.J. and Bartlett, M.S. and Barnard, G.A. and Cox, D.R. and Pearson, E.S. and Smith, C.A.B.},
publisher = {Methuen {\&} Co Ltd.},
title = {{The Foundations of Statistical Inference - A Discussion}},
year = {1962}
}
@book{GossJASP2019,
author = {Goss-Sampson, M. A.},
title = {{Statistical Analysis in JASP 0.10.2: A Guide for Students}},
year = {2019}
}
@book{weinert_leistungsmessungen_2002,
abstract = {Seit Deutschland an internationalen Vergleichsstudien zur Messung der Schulleistungen teilnimmt, sind entsprechende Testverfahren ins Blickfeld der p{\"{a}}dagogischen Diskussion ger{\"{u}}ckt. Eine Forderung von P{\"{a}}dagogen ist die Beteiligung ihrer Profession an Entscheidungen {\"{u}}ber den Einsatz standardisierter und vergleichender Leistungsmessungen. Dazu empfiehlt es sich, {\"{u}}ber diese Formen auch informiert zu sein. Hier leistet vorliegende Aufsatzsammlung gute Dienste. Neben Einblicken in Ergebnisse der empirischen Erforschung der "allt{\"{a}}glichen Beurteilungspraxis" in den Schulen bietet sie einen {\"{U}}berblick {\"{u}}ber die gro{\ss}en Leistungsvergleichsstudien (die Ergebnisse von PISA sind leider noch nicht eingearbeitet, wohl aber was PISA {\"{u}}berhaupt ist und welche Kompetenzen in dieser Studie abgefragt werden). Die Beitr{\"{a}}ge behandeln den aktuellen Diskussionsstand, die Notwendigkeiten und Probleme schulischer Leistungsmessung, erl{\"{a}}utern wie Schulleistungen in den Bereichen muttersprachlicher bzw. fremdsprachlicher, mathematischer, naturwissenschaftlicher Bildung und auch im "moralisch-wertbildenden" Bereich gemessen werden k{\"{o}}nnen und stellen nationale und internatinale Studien vor. Ein Band f{\"{u}}r Fachleute. Auch in Schleswig-Holstein sind f{\"{u}}r dieses Schuljahr fl{\"{a}}chendeckend Vergleichsarbeiten f{\"{u}}r die Jahrg{\"{a}}nge der Sec. I angeordnet worden, so dass Nachfrage nach dem Thema m{\"{o}}glich ist. Deshalb gr{\"{o}}{\ss}eren B{\"{u}}chereien empfohlen. (I. M{\"{u}}ller-Boysen)},
address = {Weinheim},
annote = {Enth. 23 Beitr Literaturverz. S. [367] - 382
OCLC: 248726128},
edition = {2., unver{\"{a}}},
editor = {Weinert, Franz E},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2002 - Leistungsmessungen in Schulen.pdf:pdf},
isbn = {978-3-407-25256-2},
keywords = {6.4,Bildungsniveau,Deutschland,Educational tests and measurements,Messung,P{\"{a}}dagogische Psychologie,Schulleistungsmessung,Sch{\"{u}}ler,Special education,Welt},
publisher = {Beltz},
series = {Beltz {\{}P{\"{a}}dagogik{\}}},
title = {{Leistungsmessungen in Schulen}},
year = {2002}
}
@book{Reid1982,
address = {New York},
author = {Reid, Constance},
isbn = {9780387983578},
publisher = {Springer},
title = {{Neyman}},
volume = {91},
year = {1982}
}
@book{mayring_qualitative_2010,
abstract = {Viele Ans{\"{a}}tze der {\textgreater}{\textgreater}Qualitativen Sozialforschung{\textless}{\textless}bleiben vage, unsystematisch und schwer nachvollziehbar. Die {\textgreater}{\textgreater}Qualitative Inhaltsanalyse{\textless}{\textless}hingegen bietet als explizit theorie- und regelgeleitete Analyse die M{\"{o}}glichkeit einer systematischen, strukturierten Arbeit mit unterschiedlichstem sprachlichen Material},
address = {Weinheim},
annote = {Literaturverz. S. 136 - 144
OCLC: 663863172},
author = {Mayring, Philipp},
edition = {11., aktua},
isbn = {978-3-407-25533-4 978-3-407-29142-4},
keywords = {Bibliometrie,Content analysis (Communication),Qualitative Inhaltsanalyse,Qualitative Methode,Social sciences Methodology,Sozialpsychologie},
publisher = {Beltz},
series = {Beltz {\{}P{\"{a}}dagogik{\}}},
shorttitle = {Qualitative {\{}Inhaltsanalyse{\}}},
title = {{Qualitative Inhaltsanalyse: Grundlagen und Techniken}},
year = {2010}
}
@book{Wilcox2012,
address = {Amsterdam, Boston},
author = {Wilcox, Rand R.},
booktitle = {Introduction to Robust Estimation and Hypothesis Testing},
doi = {10.1016/C2010-0-67044-1},
isbn = {9780123869838},
publisher = {Elsevier Inc.},
title = {{Introduction to Robust Estimation and Hypothesis Testing}},
year = {2012}
}
@book{ben-zvi_challenge_2004,
address = {Dordrecht},
annote = {Literaturangaben
OCLC: 845691012},
editor = {Ben-Zvi, Dani},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2004 - The challenge of developing statistical literacy, reasoning, and thinking.pdf:pdf},
isbn = {978-1-4020-2278-4 978-1-4020-2277-7},
publisher = {Kluwer Acad. Publ},
title = {{The challenge of developing statistical literacy, reasoning, and thinking}},
year = {2004}
}
@article{Diebolt1994,
author = {Diebolt, Jean and Robert, Christian P .},
journal = {Journal of the Royal Statistical Society Series B (Methodological)},
number = {2},
pages = {363--375},
title = {{Estimation of Finite Mixture Distributions through Bayesian Sampling}},
volume = {56},
year = {1994}
}
@misc{Mahl2012,
abstract = {Markov models are mathematical models that can be used to describe disease progression and evaluate the cost-effectiveness of medical interventions. Markov models allow projecting clinical and economic outcomes into the future and are therefore frequently used to estimate long-term outcomes of medical interventions. The purpose of this paper is to demonstrate its use in dentistry, using the example of resin-bonded bridges to replace missing teeth, and to review the literature. We used literature data and a four-state Markov model to project long-term outcomes of resin-bonded bridges over a time horizon of 60 years. In addition, the literature was searched in PubMed Medline for research articles on the application of Markov models in dentistry.},
author = {Mahl, Dominik and Marinello, Carlo P and Sendi, Pedram},
booktitle = {Expert review of pharmacoeconomics {\&} outcomes research},
doi = {10.1586/erp.12.47},
issn = {17448379},
keywords = {Markov models,cost–effectiveness analysis,dentistry,prosthodontics,resin-bonded bridges},
month = {oct},
number = {5},
pages = {623--629},
pmid = {23186403},
publisher = {Taylor {\&} Francis},
title = {{Markov models in dentistry: application to resin-bonded bridges and review of the literature.}},
url = {http://www.tandfonline.com/doi/full/10.1586/erp.12.47},
volume = {12},
year = {2012}
}
@book{Klein2014,
abstract = {The objective of the series is to provide high-quality volumes covering the state-of-the-art in the theory and applications of statistical methodology. The books in the series are thoroughly edited and present comprehensive? coherent? and uni?ed summaries of speci?c methodological topics from statistics. The chapters are ?ritten by the leading researchers in the ?eld? and present a good balance of theory and application through a synthesis of the key methodological developments and examples and case studies using real data. The scope of the series is wide, covering topics of statistical methodology that are well developed and ?nd application in a range of scienti?c disciplines. The volumes are primarily of interest to researchers and graduate students from statistics and biostatistics, but also appeal to scientists from ?elds where the methodology is applied to real problems, including medical research, epidemiology and public health, engineering, biological science, environmental science, and the social sciences.},
address = {Boca Raton},
author = {Klein, John P. and van Houwelingen, Hans C. and Ibrahim, Joseph G. and Scheike, Thomas H.},
booktitle = {Handbook of Survival Analysis},
doi = {10.1201/b16248},
isbn = {1466555661},
pages = {640},
title = {{Handbook of Survival Analysis}},
year = {2014}
}
@book{McCullagh1989,
abstract = {2nd ed. Updated and expanded edition of a text for graduate and advanced undergraduate students of applied statistics. Differs most conspicuously from the 1983 edition in that exercises can now be found at the end of each of the fifteen chapters. An outline of generalized linear models -- Models for continuous data with constant variance -- Binary data -- Models for polytomous data -- Log-linear models -- Conditional likelihoods -- Models with constant coefficient of variation -- Quasi-likelihood functions -- Joint modelling of mean and dispersion -- Models with additional non-linear parameters -- Model checking -- Models for survival data -- Components of dispersion -- Further topics.},
address = {Boca Raton},
author = {McCullagh, P. (Peter) and Nelder, John A.},
isbn = {9780412317606},
pages = {511},
publisher = {Chapman and Hall},
title = {{Generalized linear models}},
year = {1989}
}
@article{Hunter1997,
abstract = {{\textless}p{\textgreater}The significance test as currently used is a disaster Whereas most researchers falsely believe that the significance test has an error rate of 5{\%}, empirical studies show the average error rate across psychology is 60{\%}–12 times higher than researchers think it to be The error rate for inference using the significance test is greater than the error rate using a coin toss to replace the empirical study The significance test has devastated the research review process Comprehensive reviews cite conflicting results on almost every issue Yet quantitatively accurate review of the same results shows that the apparent conflicts stem almost entirely from the high error rate for the significance test If 60{\%} of studies falsely interpret their primary results, then reviewers who base their reviews on the interpreted study “findings” will have a 100{\%} error rate in concluding that there is conflict between study results{\textless}/p{\textgreater}},
author = {Hunter, John E.},
doi = {10.1111/j.1467-9280.1997.tb00534.x},
issn = {0956-7976},
journal = {Psychological Science},
month = {jan},
number = {1},
pages = {3--7},
publisher = {Blackwell Publishing Ltd},
title = {{Needed: A Ban on the Significance Test}},
url = {http://journals.sagepub.com/doi/10.1111/j.1467-9280.1997.tb00534.x},
volume = {8},
year = {1997}
}
@article{Hubbard2019,
abstract = {Efforts to address a reproducibility crisis have generated several valid proposals for improving the quality of scientific research. We argue there is also need to address the separate but related issues of relevance and responsiveness. To address relevance, researchers must produce what decision makers actually need to inform investments and public policy-that is, the probability that a claim is true or the probability distribution of an effect size given the data. The term responsiveness refers to the irregularity and delay in which issues about the quality of research are brought to light. Instead of relying on the good fortune that some motivated researchers will periodically conduct efforts to reveal potential shortcomings of published research, we could establish a continuous quality-control process for scientific research itself. Quality metrics could be designed through the application of this statistical process control for the research enterprise. We argue that one quality control metric-the probability that a research hypothesis is true-is required to address at least relevance and may also be part of the solution for improving responsiveness and reproducibility. This article proposes a "straw man"solution which could be the basis of implementing these improvements. As part of this solution, we propose one way to "bootstrap" priors. The processes required for improving reproducibility and relevance can also be part of a comprehensive statistical quality control for science itself by making continuously monitored metrics about the scientific performance of a field of research. ARTICLE HISTORY},
author = {Hubbard, Douglas W. and Carriquiry, Alicia L.},
doi = {10.1080/00031305.2018.1543138},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hubbard, Carriquiry - 2019 - Quality Control for Scientific Research Addressing Reproducibility, Responsiveness, and Relevance.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {p -values,process control,relevance,replication},
number = {sup1},
pages = {46--55},
title = {{Quality Control for Scientific Research: Addressing Reproducibility, Responsiveness, and Relevance}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1543138},
volume = {73},
year = {2019}
}
@article{Gupta2012,
abstract = {Mainly, two statistical methodologies are applicable to the design and analysis of clinical trials: frequentist and Bayesian. Most traditional clinical trial designs are based on frequentist statistics. In frequentist statistics prior information is utilized formally only in the design of a clinical trial but not in the analysis of the data. On the other hand, Bayesian statistics provide a formal mathematical method for combining prior information with current information at the design stage, during the conduct of the trial, and at the analysis stage. It is easier to implement adaptive trial designs using Bayesian methods than frequentist methods. The Bayesian approach can also be applied for post-marketing surveillance purposes and in meta-analysis. The basic tenets of good trial design are same for both Bayesian and frequentist trials. It has been recommended that the type of analysis to be used (Bayesian or frequentist) should be chosen beforehand. Switching to an analysis method that produces a more favorable outcome after observing the data is not recommended.},
author = {Gupta, SandeepK},
doi = {10.4103/2229-516X.96789},
isbn = {2229-516X (Print) 2229-516X (Linking)},
issn = {2229-516X},
journal = {International Journal of Applied and Basic Medical Research},
keywords = {Adaptive trial,Bayesian statistics,drug development},
month = {jan},
number = {1},
pages = {3},
pmid = {23776799},
publisher = {Wolters Kluwer -- Medknow Publications},
title = {{Use of Bayesian statistics in drug development: Advantages and challenges}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23776799 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3657986 http://www.ijabmr.org/text.asp?2012/2/1/3/96789},
volume = {2},
year = {2012}
}
@article{Society2016,
author = {Edgeworth, F. Y.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Edgeworth - 1908 - On the Probable Errors of Frequency-Constants.pdf:pdf},
journal = {Journal of the Royal Statistical Society},
number = {2},
pages = {381},
title = {{On the Probable Errors of Frequency-Constants}},
volume = {71},
year = {1908}
}
@article{Dobruschin1968,
author = {Dobruschin, P. L.},
doi = {10.1137/1113026},
issn = {0040-585X},
journal = {Theory of Probability {\&} Its Applications},
month = {jan},
number = {2},
pages = {197--224},
publisher = {Society for Industrial and Applied Mathematics},
title = {{The Description of a Random Field by Means of Conditional Probabilities and Conditions of Its Regularity}},
url = {http://epubs.siam.org/doi/10.1137/1113026},
volume = {13},
year = {1968}
}
@article{Diaconis1982,
abstract = {Jeffrey's rule for revising a probability P to a new probability P* based on new probabilities P* (Ei) on a partition {\{}Ei{\}}i = 1n is (Equation presented). Jeffrey's rule is applicable if it is judged that P* (A ∣ Ei) = P(A ∣ Ei) for all A and i. This article discusses some of the mathematical properties of this rule, connecting it with sufficient partitions, and maximum entropy updating of contingency tables. The main results concern simultaneous revision on two partitions. {\textcopyright} 1982 Taylor {\&} Francis Group, LLC.},
author = {Diaconis, Persi and Zabell, Sandy L},
doi = {10.1080/01621459.1982.10477893},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Diaconis, Zabell - 1982 - Updating Subjective Probability.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Coefficient of dependence,Contingency tables,F divergence,I projection,Iterated proportional fitting procedure,Jeffrey conditionalization,Maximum entropy,Probability kinematics,Sufficiency},
number = {380},
pages = {822--830},
title = {{Updating subjective probability}},
volume = {77},
year = {1982}
}
@article{jones2000,
abstract = {Based on a review of research and a cognitive development model (Biggs {\&} Collis, 1991), we formulated a framework for characterizing elementary children's statistical thinking and refined it through a validation process. The 4 constructs in this framework were describing, organizing, representing, and analyzing and interpreting data. For each construct, we hypothesized 4 thinking levels, which represent a continuum from idiosyncratic to analytic reasoning. We developed statistical thinking descriptors for each level and construct and used these to design an interview protocol. We refined and validated the framework using data from protocols of 20 target students in Grades 1 through 5. Results of the study confirm that children's statistical thinking can be described according to the 4 framework levels and that the framework provides a coherent picture of children's thinking, in that 80{\%} of them exhibited thinking that was stable on at least 3 constructs. The framework contributes domain-specific theory fo...},
author = {Jones, Graham A. and Thornton, Carol A. and Langrall, Cynthia W. and Mooney, Edward S. and Perry, Bob and Putt, Ian J.},
doi = {10.1207/S15327833MTL0204_3},
issn = {1098-6065},
journal = {Mathematical Thinking and Learning},
month = {oct},
number = {4},
pages = {269--307},
publisher = {Lawrence Erlbaum Associates, Inc.},
title = {{A Framework for Characterizing Children's Statistical Thinking}},
url = {http://www.tandfonline.com/doi/abs/10.1207/S15327833MTL0204{\_}3},
volume = {2},
year = {2000}
}
@book{Lindquist1940,
address = {Boston},
author = {Lindquist, E.F.},
publisher = {Houghton Mifflin},
title = {{Statistical analysis in educational research}},
year = {1940}
}
@article{Pereira1999,
abstract = {A Bayesian measure of evidence for precise hypotheses is presented. The intention is to give a Bayesian alternative to significance tests or, equivalently, to p-values. In fact, a set is defined in the parameter space and the posterior probability, its credibility, is evaluated. This set is the "Highest Posterior Density Region" that is "tangent" to the set that defines the null hypothesis. Our measure of evidence is the complement of the credibility of the "tangent" region.},
author = {Pereira, C. A. d. B. and Stern, J. M.},
doi = {10.3390/e1040099},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Pereira, Stern - 1999 - Evidence and credibility Full Bayesian significance test for precise hypotheses.pdf:pdf},
issn = {10994300},
journal = {Entropy},
keywords = {Bayes factor,Global optimization,Numerical integration,P-value,Posterior density},
number = {4},
pages = {99--110},
title = {{Evidence and credibility: Full Bayesian significance test for precise hypotheses}},
volume = {1},
year = {1999}
}
@article{Barbieri2004,
abstract = {Often the goal of model selection is to choose a model for future prediction, and it is natural to measure the accuracy of a future prediction by squared error loss. Under the Bayesian approach, it is commonly perceived that the optimal predictive model is the model with highest posterior probability, but this is not necessarily the case. In this paper we show that, for selection among normal linear models, the optimal predictive model is often the median probability model, which is defined as the model consisting of those variables which have overall posterior probability greater than or equal to 1/2 of being in a model. The median probability model often differs from the highest probability model. {\textcopyright} Institute of Mathematical Statistics, 2004.},
author = {Barbieri, Maria Maddalena and Berger, J.O.},
doi = {10.1214/009053604000000238},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bayesian linear models,Predictive distribution,Squared error loss,Variable selection},
month = {jun},
number = {3},
pages = {870--897},
publisher = {Institute of Mathematical Statistics},
title = {{Optimal predictive model selection}},
volume = {32},
year = {2004}
}
@incollection{Fienberg1980,
author = {Fienberg, Stephen E.},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_9},
pages = {75--84},
publisher = {Springer, New York, NY},
title = {{Fisher's Contributions to the Analysis of Categorical Data}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}9},
year = {1980}
}
@article{Basu2003,
abstract = {We present a method for comparing semiparametric Bayesian models, constructed under the Dirichlet process mixture (DPM) framework, with alternative semiparameteric or parameteric Bayesian models. A distinctive feature of the method is that it can be applied to semiparametric models containing covariates and hierarchical prior structures, and is apparently the first method of its kind. Formally, the method is based on the marginal likelihood estimation approach of Chib (1995) and requires estimation of the likelihood and posterior ordinates of the DPM model at a single high-density point. An interesting computation is involved in the estimation of the likelihood ordinate, which is devised via collapsed sequential importance sampling. Extensive experiments with synthetic and real data involving semiparametric binary data regression models and hierarchical longitudinal mixed-effects models are used to illustrate the implementation, performance, and applicability of the method.},
author = {Basu, Sanjib and Chib, Siddhartha},
doi = {10.1198/01621450338861947},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Bayes factor,Bayesian model comparison,Dirichlet process mixture,Marginal likelihood,Semiparametric binary data model,Semiparametric longitudinal data model},
month = {mar},
number = {461},
pages = {224--235},
publisher = {Taylor {\&} Francis},
title = {{Marginal likelihood and bayes factors for Dirichlet process mixture models}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=uasa20},
volume = {98},
year = {2003}
}
@article{Evans2006,
author = {Evans, Michael J. and Guttman, Irwin and Swartz, Tim},
doi = {10.1002/cjs.5550340109},
issn = {03195724},
journal = {Canadian Journal of Statistics},
keywords = {Bayesian inference,computation,minimum prior content,relative surprise},
month = {mar},
number = {1},
pages = {113--129},
publisher = {Wiley},
title = {{Optimally and computations for relative surprise inferences}},
volume = {34},
year = {2006}
}
@article{Aldrich2005,
author = {Aldrich, John},
journal = {International Statistical Review},
number = {3},
pages = {289--307},
title = {{The Statistical Education of Harold Jeffreys}},
url = {https://www.jstor.org/stable/25472677?seq=1{\#}metadata{\_}info{\_}tab{\_}contents},
volume = {73},
year = {2005}
}
@incollection{savage1961,
address = {Berkeley},
author = {Savage, Leonard J.},
booktitle = {Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability},
doi = {10.1037/h0038916},
editor = {Neyman, J.},
isbn = {978-0-486-62349-8},
issn = {0095-8891},
pages = {575--586},
pmid = {8387179},
publisher = {University of California Press},
title = {{The foundations of statistics reconsidered}},
url = {http://content.apa.org/journals/ccp/19/3/237c},
volume = {1: Contrib},
year = {1961}
}
@article{Greenland2016,
abstract = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so-and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
doi = {10.1007/s10654-016-0149-3},
eprint = {1011.1669},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Greenland et al. - 2016 - Statistical tests, P values, confidence intervals, and power a guide to misinterpretations.pdf:pdf},
isbn = {1573-7284 (Electronic)$\backslash$r0393-2990 (Linking)},
issn = {15737284},
journal = {European Journal of Epidemiology},
keywords = {Confidence intervals,Hypothesis testing,Null testing,P value,Power,Significance tests,Statistical testing},
number = {4},
pages = {337--350},
pmid = {27209009},
publisher = {Springer Netherlands},
title = {{Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations}},
volume = {31},
year = {2016}
}
@article{Etz2016,
abstract = {We revisit the results of the recent Reproducibility Project: Psychology by the Open Science Collaboration. We compute Bayes factors - a quantity that can be used to express comparative evidence for an hypothesis but also for the null hypothesis - for a large subset (N = 72) of the original papers and their corresponding replication attempts. In our computation, we take into account the likely scenario that publication bias had distorted the originally published results. Overall, 75{\%} of studies gave qualitatively similar results in terms of the amount of evidence provided. However, the evidence was often weak (i.e., Bayes factor {\textless} 10). The majority of the studies (64{\%}) did not provide strong evidence for either the null or the alternative hypothesis in either the original or the replication, and no replication attempts provided strong evidence in favor of the null. In all cases where the original paper provided strong evidence but the replication did not (15{\%}), the sample size in the replication was smaller than the original. Where the replication provided strong evidence but the original did not (10{\%}), the replication sample size was larger. We conclude that the apparent failure of the Reproducibility Project to replicate many target effects can be adequately explained by overestimation of effect sizes (or overestimation of evidence against the null hypothesis) due to small sample sizes and publication bias in the psychological literature. We further conclude that traditional sample sizes are insufficient and that a more widespread adoption of Bayesian methods is desirable.},
author = {Etz, Alexander and Vandekerckhove, Joachim},
doi = {10.1371/journal.pone.0149794},
editor = {Marinazzo, Daniele},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Etz, Vandekerckhove - 2016 - A Bayesian perspective on the reproducibility project Psychology.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
month = {feb},
number = {2},
pages = {e0149794},
publisher = {Public Library of Science},
title = {{A Bayesian perspective on the reproducibility project: Psychology}},
url = {https://dx.plos.org/10.1371/journal.pone.0149794},
volume = {11},
year = {2016}
}
@inproceedings{valentine_cs_2004,
abstract = {A meta-analysis is performed on the last twenty years of SIGCSE Technical Symposium Proceedings, looking for the kind of CS Educational Research that has been done at the CS1/CS2 level. A six-point taxonomy of articles types is described. It is shown that about one in five of all CS1/CS2 presentations have used some kind of experimental model, albeit "experimental" is defined quite broadly. Over the last ten years both the number of experimental models, and the percentage of experimental models among CS1/CS2 has significantly increased. SIGCSE members are challenged to adopt a research model for their presentations to the Technical Symposium.},
address = {New York, NY, USA},
author = {Valentine, David W},
booktitle = {Proceedings of the 35th SIGCSE Technical Symposium on Computer Science Education},
doi = {10.1145/971300.971391},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Valentine - 2004 - CS Educational Research A Meta-analysis of SIGCSE Technical Symposium Proceedings.pdf:pdf},
isbn = {978-1-58113-798-9},
keywords = {CS education research,CS1/CS2},
pages = {255--259},
publisher = {ACM},
series = {{\{}SIGCSE{\}} '04},
shorttitle = {{\{}CS{\}} {\{}Educational{\}} {\{}Research{\}}},
title = {{CS Educational Research: A Meta-analysis of SIGCSE Technical Symposium Proceedings}},
url = {http://doi.acm.org/10.1145/971300.971391},
year = {2004}
}
@article{Pereira2020a,
abstract = {This article gives a survey of the e-value, a statistical significance measure a.k.a. the evidence rendered by observational data, X, in support of a statistical hypothesis, H, or, the other way around, the epistemic value of H given X. The e-value and the accompanying FBST, the Full Bayesian Significance Test, constitute the core of a research program that was started at IME-USP, is being developed by over 20 researchers worldwide, and has, so far, been referenced by over 200 publications. The e-value and the FBST comply with the best principles of Bayesian inference, including the likelihood principle, complete invariance, asymptotic consistency, etc. Furthermore, they exhibit powerful logic or algebraic properties in situations where one needs to compare or compose distinct hypotheses that can be formulated either in the same or in different statistical models. Moreover, they effortlessly accommodate the case of sharp or precise hypotheses, a situation where alternative methods often require ad hoc and convoluted procedures. Finally, the FBST has outstanding robustness and reliability characteristics, outperforming traditional tests of hypotheses in many practical applications of statistical modeling and operations research.},
author = {Pereira, C. A. d. B. and Stern, J. M.},
doi = {10.1007/s40863-020-00171-7},
issn = {1982-6907},
journal = {S{\~{a}}o Paulo Journal of Mathematical Sciences},
keywords = {Mathematics,general},
pages = {1--19},
publisher = {Springer},
title = {{The e-value: a fully Bayesian significance measure for precise statistical hypotheses and its research program}},
url = {http://link.springer.com/10.1007/s40863-020-00171-7},
year = {2020}
}
@article{Ghosal2000,
author = {Ghosal, Subhashis and Ghosh, Jayanta K. and van der Vaart, Aad W.},
doi = {10.1214/AOS/1016218228},
issn = {0090-5364},
journal = {Annals of Statistics},
keywords = {Infinite dimensional model,posterior distribution,rate of convergence,sieves,splines},
number = {2},
pages = {500--531},
publisher = {Institute of Mathematical Statistics},
title = {{Convergence rates of posterior distributions}},
volume = {28},
year = {2000}
}
@article{Ritter1992,
author = {Ritter, Christian and Tanner, Martin A.},
doi = {10.1080/01621459.1992.10475289},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Bayesian statistics,Cox regression,Monte Carlo methods,Nonlinear regression},
month = {sep},
number = {419},
pages = {861--868},
publisher = {Taylor and Francis Ltd.},
title = {{Facilitating the Gibbs Sampler: The Gibbs Stopper and the Griddy-Gibbs Sampler}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10475289},
volume = {87},
year = {1992}
}
@misc{Henriquez2016,
abstract = {The struggle with uncertainty among healthcare professionals about how to apply test results for risk stratifica-tion and diagnosis of patients was recently highlighted as a serious risk factor for diagnostic and medical decision-making errors in the 2015 Institute of Medicine (IOM) report Improving Diagnosis in Health Care (1). The Bayes theorem is well recognized by trainees and caregivers as a statistical key for updating the probability of a patient event (e.g., disease) occurring as test result(s) become available, with consideration of the clinical performance characteristics (i.e., clinical sensitivity and specificity) of a test in the clinical context (i.e., prevalence of disease/ health in a reference population). However, many trainees and caregivers struggle with the application of Bayes' theorem in clinical practice, suggesting a longstanding educational gap that can negatively impact medical decision-making and patient care. The IOM Committee on Diagnostic Error in Health Care recommends enhancing healthcare professionals' education and lifelong competency in clinical reasoning, including laboratory medicine probabilistic reasoning skills in the application of test results to subsequent clinical decision-making, to improve diagnostic performance and reduce the potential for medical errors and near misses. The Bayesian inference dilemma frequently faced by patients and their physicians was recently highlighted in Science in a perspective article by Operskalski and Barbey (2), Through the psychological sciences of clinical reasoning errors, these authors offer additional insights into how we think about probabilities and risk. Heuristics (mental shortcuts) and cognitive biases play roles in clinical reasoning, in which rapid, impression-based decision-making can lead to errors. Thoughtful assessment (e.g., calculation of Bayes' theorem statistics) may verify or challenge initial quick impressions but can be difficult to achieve in fast-paced clinical care settings. Alternatively, the "ecological rationality" view argues that our mind processes information in specific formats. Expression of probability in a single-event format (e.g., a 0.1{\%} chance) is less intuitive and may lead to clinical reasoning errors compared to interpreting event frequencies relative to a reference population (e.g., a likelihood of 1 in 1000). Optimizing the numerical format of probability can facilitate a better understanding of risk and uncertainty in medical decision-making but can also be prone to error if applied in the wrong clinical context (e.g., for the incorrect reference population). Operskalski and Barbey discuss the need for transparency in nested set relations in the statistical structure of risk, emphasizing the use of an appropriate reference population and the consideration of the prevalence and outcomes of disease within the reference population as key factors to correctly calculate and interpret the probability of a patient event occurring. Strategies to effectively reduce probabilistic reasoning errors remain unclear, but heuristics and cognitive biases are inevitable in the face of time-pressured medical decision-making in clinical reality. Enhanced education in Bayesian inference principles and increased awareness of cognitive biases are important goals for healthcare professionals , but is there a better representation of the statistical structure of risk to facilitate rapid Bayesian inference and informed decision-making? This is a question posed by Operskalski and Barbey, who recommend a complimentary strategy: depiction of event frequencies and outcomes (e.g., true positive, true negative, false positive , false negative) relative to a reference population using visual aids, such as hierarchal data flow diagrams or pictographs. Generating mental simulations using visual diagrams that illustrate frequency information can facilitate a correct Bayesian response. In the emerging era of precision medicine and empowering patients to take part in decisions about their clinical care, there is a growing need for user-friendly probabilistic reasoning tools to aid patients and physicians in the correct application of the},
author = {Henriquez, Ronald R. and Korpi-Steiner, Nichole},
booktitle = {Clinical Chemistry},
doi = {10.1373/clinchem.2016.260935},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Henriquez, Korpi-Steiner - 2016 - Bayesian inference dilemma in medical decision-making A need for user-friendly probabilistic reasoning.pdf:pdf},
issn = {15308561},
month = {sep},
number = {9},
pages = {1285--1286},
publisher = {Clinical Chemistry},
title = {{Bayesian inference dilemma in medical decision-making: A need for user-friendly probabilistic reasoning tools}},
url = {http://clinchem.aaccjnls.org/cgi/doi/10.1373/clinchem.2016.260935},
volume = {62},
year = {2016}
}
@article{Sutherland2017a,
author = {Sutherland, SINCLAIR and Ridgway, J},
journal = {Statistics education research journal.},
number = {1},
pages = {26--30},
title = {{Interactive visualisations and statistical literacy.}},
volume = {16},
year = {2017}
}
@book{Wellek2010,
abstract = {While continuing to focus on methods of testing for two-sided equivalence, Testing Statistical Hypotheses of Equivalence and Noninferiority, Second Edition gives much more attention to noninferiority testing. It covers a spectrum of equivalence testing problems of both types, ranging from a one-sample problem with normally distributed observations of fixed known variance to problems involving several dependent or independent samples and multivariate data. Along with expanding the material on noninferiority problems, this edition includes new chapters on equivalence tests for multivariate data and tests for relevant differences between treatments. A majority of the computer programs offered online are now available not only in SAS or Fortran but also as R scripts or as shared objects that can be called within the R system.This book provides readers with a rich repertoire of efficient solutions to specific equivalence and noninferiority testing problems frequently encountered in the analysis of real data sets. It first presents general approaches to problems of testing for noninferiority and two-sided equivalence. Each subsequent chapter then focuses on a specific procedure and its practical implementation. The last chapter describes basic theoretical results about tests for relevant differences as well as solutions for some specific settings often arising in practice. Drawing from real-life medical research, the author uses numerous examples throughout to illustrate the methods.FeaturesCovers equivalence and noninferiority testing problems for one sample, two samples, and multisamplesPresents a systematic account of equivalence and noninferiority testing procedures and their mathematical basisIncludes many concrete numerical examples that illustrate the individual methodsProvides computer programs in SAS, Fortran, and R for facilitating the routine application of testing procedures at www.crcpress.com},
author = {Wellek, Stefan.},
booktitle = {Testing Statistical Hypotheses of Equivalence and Noninferiority},
doi = {10.1201/ebk1439808184},
isbn = {9781439808184},
pages = {415},
publisher = {CRC Press},
title = {{Testing Statistical Hypotheses of Equivalence and Noninferiority}},
year = {2010}
}
@book{Reiss1993,
address = {New York, NY},
author = {Reiss, Rolf-Dieter},
doi = {10.1007/978-1-4613-9308-5},
isbn = {978-1-4613-9310-8},
publisher = {Springer New York},
series = {Springer Series in Statistics},
title = {{A Course on Point Processes}},
url = {http://link.springer.com/10.1007/978-1-4613-9308-5},
year = {1993}
}
@book{Bennett1990,
address = {Oxford},
author = {Bennett, J.H.},
publisher = {Clarendon Press},
title = {{Statistical Inference and Analysis: Selected Correspondance of R.A.Fisher}},
year = {1990}
}
@article{Robert2018,
abstract = {Markov chain Monte Carlo algorithms are used to simulate from complex statistical distributions by way of a local exploration of these distributions. This local feature avoids heavy requests on understanding the nature of the target, but it also potentially induces a lengthy exploration of this target, with a requirement on the number of simulations that grows with the dimension of the problem and with the complexity of the data behind it. Several techniques are available towards accelerating the convergence of these Monte Carlo algorithms, either at the exploration level (as in tempering, Hamiltonian Monte Carlo and partly deterministic methods) or at the exploitation level (with Rao-Blackwellisation and scalable methods).},
archivePrefix = {arXiv},
arxivId = {1804.02719},
author = {Robert, Christian P. and Elvira, V{\'{i}}ctor and Tawn, Nick and Wu, Changye},
doi = {10.1002/wics.1435},
eprint = {1804.02719},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Robert et al. - 2018 - Accelerating MCMC algorithms.pdf:pdf},
issn = {19390068},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
keywords = {Bayesian analysis,Hamiltonian Monte Carlo,Monte Carlo methods,Rao-Blackwellisation,computational statistics,convergence of algorithms,efficiency of algorithms,simulation,tempering},
number = {5},
title = {{Accelerating MCMC algorithms}},
volume = {10},
year = {2018}
}
@article{Fayers2000,
author = {Fayers, P.M. and Cuschieri, A. and Fielding, J. and Craven, J. and Uscinska, B. and Freedman, L.S.},
doi = {https://doi.org/10.1054/bjoc.1999.0902},
journal = {British Journal of Cancer},
number = {82},
pages = {213--219},
title = {{Sample size calculation for clinical trials: the impact of clinician beliefs}},
year = {2000}
}
@book{cohen_statistical_1988,
abstract = {Statistical Power Analysis is a nontechnical guide to power analysis in research planning that provides users of applied statistics with the tools they need for more effective analysis. The Second Edition includes: * a chapter covering power analysis in set correlation and multivariate methods; * a chapter considering effect size, psychometric reliability, and the efficacy of "qualifying" dependent variables and; * expanded power and sample size tables for multiple regression/correlation.},
address = {Hillsdale, N.J},
author = {Cohen, Jacob},
edition = {2nd},
isbn = {978-0-8058-0283-2},
publisher = {Routledge},
title = {{Statistical Power Analysis for the Behavioral Sciences}},
year = {1988}
}
@book{Evans2015,
address = {Boca Raton, FL},
author = {Evans, M.},
publisher = {CRC Press},
title = {{Measuring statistical evidence using relative belief}},
year = {2015}
}
@article{watson2003,
abstract = {The aim of this study was, first, to provide evidence to support the notion of statistical literacy as a hierarchical construct and, second, to identify levels of this hierarchy across the construct. The study used archived data collected from two large-scale research projects that studied aspects of statistical understanding of over 3000 school students in grades 3 to 9, based on 80 questionnaire items. Rasch analysis was used to explore an hypothesised underlying construct associated with statistical literacy. The analysis supported the hypothesis of a unidimensional construct and suggested six levels of understanding: Idiosyncratic, Informal, Inconsistent, Consistent non- critical, Critical, and Critical mathematical. These levels could be used by teachers and curriculum developers to incorporate appropriate aspects of statistical literacy into the existing curriculum.},
author = {Watson, Jane and Callingham, Rosemary},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Watson, Callingham - 2003 - Statistical literacy A complex hierarchical construct.pdf:pdf},
issn = {1570-1824},
journal = {Statistics Education Research Journal},
keywords = {conceptual hierarchy,rasch analysis,school students,statistical literacy},
number = {2},
pages = {3--46},
title = {{Statistical literacy: A complex hierarchical construct}},
url = {http://www.stat.auckland.ac.nz/{~}iase/serj/SERJ2(2){\_}Watson{\_}Callingham.pdf},
volume = {2},
year = {2003}
}
@book{Wasserman2004,
address = {New York, NY},
author = {Wasserman, Larry},
doi = {10.1007/978-0-387-21736-9},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Wasserman - 2004 - All of Statistics.pdf:pdf},
isbn = {978-1-4419-2322-6},
publisher = {Springer New York},
series = {Springer Texts in Statistics},
title = {{All of Statistics}},
url = {http://link.springer.com/10.1007/978-0-387-21736-9},
year = {2004}
}
@book{PearlMacKenzie2018,
address = {New York},
author = {Pearl, J. and MacKenzie, D.},
publisher = {Basic Books},
title = {{The Book of Why}},
year = {2018}
}
@book{Spiegelhalter2004,
address = {New York},
author = {Spiegelhalter, D. J. and Abrams, K. R. (Keith R.) and Myles, Jonathan P.},
isbn = {9780470092606},
pages = {391},
publisher = {Wiley},
title = {{Bayesian approaches to clinical trials and health-care evaluation}},
year = {2004}
}
@article{Evans1997,
abstract = {We consider the problem of deriving Bayesian inference procedures via the concept of relative surprise. The mathematical concept of surprise has been developed by I.J. Good in a long sequence of papers. We make a modification to this development that permits the avoidance of a serious defect; namely, the change of variable problem. We apply relative surprise to the development of estimation, hypothesis testing and model checking procedures. Important advantages of the relative surprise approach to inference include the lack of dependence on a particular loss function and complete freedom to the statistician in the choice of prior for hypothesis testing problems. Links are established with common Bayesian inference procedures such as highest posterior density regions, modal estimates and Bayes factors. From a practical perspective new inference procedures arise that possess good properties.},
author = {Evans, Michael},
doi = {10.1080/03610929708831972},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Evans - 1997 - Bayesian inference procedures derived via the concept of relative surprise(2).pdf:pdf},
issn = {03610926},
journal = {Communications in Statistics - Theory and Methods},
keywords = {Bayesian hypothesis testing,Change of variable problem,Cross-validation,Model checking,Observed relative surprise,Principle of least relative surprise},
number = {5},
pages = {1125--1143},
publisher = {Marcel Dekker Inc.},
title = {{Bayesian inference procedures derived via the concept of relative surprise}},
volume = {26},
year = {1997}
}
@article{Ioannidis2017,
abstract = {We investigate two critical dimensions of the credibility of empirical economics research: statistical power and bias. We survey 159 empirical economics literatures that draw upon 64,076 estimates of economic parameters reported in more than 6,700 empirical studies. Half of the research areas have nearly 90{\%} of their results under-powered. The median statistical power is 18{\%}, or less. A simple weighted average of those reported results that are adequately powered (power ≥ 80{\%}) reveals that nearly 80{\%} of the reported effects in these empirical economics literatures are exaggerated; typically, by a factor of two and with one-third inflated by a factor of four or more.},
author = {Ioannidis, John P.A. and Stanley, T. D. and Doucouliagos, Hristos},
doi = {10.1111/ecoj.12461},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ioannidis, Stanley, Doucouliagos - 2017 - The Power of Bias in Economics Research.pdf:pdf},
isbn = {0013-0133},
issn = {14680297},
journal = {Economic Journal},
keywords = {bias,credibility,empirical economics,publication bias,statistical power},
number = {605},
pages = {F236--F265},
title = {{The Power of Bias in Economics Research}},
volume = {127},
year = {2017}
}
@article{Pogrow2019,
abstract = {Relying on effect size as a measure of practical significance is turning out to be just as misleading as using p-values to determine the effectiveness of interventions for improving clinical practice in complex organizations such as schools. This article explains how effect sizes have misdirected practice in education and other disciplines. Even when effect size is incorporated into RCT research the recommendations of whether interventions are effective are misleading and generally useless to practitioners. As a result, a new criterion of practical benefit is recommended for evaluating research findings about the effectiveness of interventions in complex organizations where benchmarks of existing performance exist. Practical benefit exists when the unadjusted performance of an experimental group provides a noticeable advantage over an existing benchmark. Some basic principles for determining practical benefit are provided. Practical benefit is more intuitive and is expected to enable leaders to make more accurate assessments as to whether published research findings are likely to produce noticeable improvements in their organizations. In addition, practical benefit is used routinely as the research criterion for the alternative scientific methodology of improvement science that has an established track record of being a more efficient way to develop new interventions that improve practice dramatically than RCT research. Finally, the problems with practical significance suggest that the research community should seek different inferential methods for research designed to improve clinical performance in complex organizations, as compared to methods for testing theories and medicines. ARTICLE HISTORY},
author = {Pogrow, Stanley},
doi = {10.1080/00031305.2018.1549101},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Pogrow - 2019 - How Effect Size (Practical Significance) Misleads Clinical Practice The Case for Switching to Practical Benefit to Asses.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
number = {sup1},
pages = {223--234},
title = {{How Effect Size (Practical Significance) Misleads Clinical Practice: The Case for Switching to Practical Benefit to Assess Applied Research Findings}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1549101},
volume = {73},
year = {2019}
}
@misc{VanFraassen1981,
author = {{Van Fraassen}, Bas C.},
booktitle = {British Journal for the Philosophy of Science},
doi = {10.1093/bjps/32.4.375},
issn = {00070882},
number = {4},
pages = {375--379},
title = {{A problem for relative information minimizers in probability kinematics}},
volume = {32},
year = {1981}
}
@book{Edwards1950,
address = {New York},
author = {Edwards, Allen L.},
publisher = {Rinehart},
title = {{Experimental design in psychological research}},
year = {1950}
}
@article{Pratt1961,
author = {Pratt, John W.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Pratt - 1961 - Reviewed Work ( s ) Testing Statistical Hypotheses . by E . L . Lehmann.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {293},
pages = {163--167},
title = {{Reviewed Work(s): Testing Statistical Hypotheses by E. L. Lehmann}},
volume = {56},
year = {1961}
}
@article{Altman1983,
author = {Altman, D G and Gore, S M and Gardner, M J and Pocock, S J},
doi = {10.1136/bmj.286.6376.1489},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Altman et al. - 1983 - Statistical guidelines for contributors to medical journals.pdf:pdf},
journal = {British Medical Journal (Clinical research ed.)},
number = {6376},
pages = {1489--93},
publisher = {BMJ Publishing Group},
title = {{Statistical guidelines for contributors to medical journals.}},
volume = {286},
year = {1983}
}
@article{Kawak2010,
abstract = {The aim of our study was to compare the results of unrelated donor (UD) peripheral blood stem cell transplantation versus UD bone marrow transplantation and to analyze the impact of infused CD34+ and CD3+ cell doses on survival and incidence of severe graft-versus-host disease (GVHD) in 187 children who underwent UD hematopoietic cell transplantation with the use of in vivo T cell depletion (antithymocyte globulin or CAMPATH-1H). HLA typing was performed at the " high-resolution" level. Patients receiving ≥10 × 106 CD34+ cells/kg and ≥4 × 108 CD3+ cells/kg had better overall and disease-free survival. Multivariate analysis has shown that both infused CD34+ cell dose {\textless}10 × 106/kg and CD3+ cell dose {\textless}4 × 108/kg were independent risk factors for mortality (relative risk [RR] 1.8 and 1.71, P = .009 and .016, respectively). Regarding disease-free survival, multivariate analysis has revealed another independent risk factor for poor outcome apart from the 2 earlier-mentioned cell doses, which was the use of donors mismatched at 2 HLA antigens or 3 HLA allele/antigens (RR 2.5, P = .004). In age groups 0-10 years and 10-20 years, CD34+ cell doses higher than the age-adjusted median dose clearly favored survival. Higher infused doses of CD34+ and CD3+ cells did not result in an increased rate of severe GVHD. The use of mismatched donors was the only independent risk factor for the incidence of severe acute GVHD (RR 2.2, P = .046). The report demonstrates for the first time in a pediatric cohort, that higher doses of transplanted CD34+ and CD3+ cells lead to an improved survival without an increased risk of severe GVHD. The study findings may be limited to the population of patients receiving in vivo T cell depletion, which is now broadly used in unrelated donor setting in Europe. {\textcopyright} 2010 American Society for Blood and Marrow Transplantation.},
author = {Ka{\l}wak, Krzysztof and Porwolik, Julita and Mielcarek, Monika and Gorczy{\'{n}}ska, Ewa and Owoc-Lempach, Joanna and Ussowicz, Marek and Dyla, Agnieszka and Musia{\l}, Jakub and Pa{\'{z}}dzior, Dominika and Turkiewicz, Dominik and Chybicka, Alicja},
doi = {10.1016/j.bbmt.2010.04.001},
issn = {10838791},
journal = {Biology of Blood and Marrow Transplantation},
keywords = {CD3+ cell dose,CD34+ cell dose,Children,Graft content,Graft-versus-host disease,Hematopoietic stem cell transplantation,Unrelated donor},
month = {oct},
number = {10},
pages = {1388--1401},
pmid = {20382248},
publisher = {Biol Blood Marrow Transplant},
title = {{Higher CD34+ and CD3+ Cell Doses in the Graft Promote Long-Term Survival, and Have No Impact on the Incidence of Severe Acute or Chronic Graft-versus-Host Disease after In Vivo T Cell-Depleted Unrelated Donor Hematopoietic Stem Cell Transplantation in Chi}},
url = {https://pubmed.ncbi.nlm.nih.gov/20382248/},
volume = {16},
year = {2010}
}
@article{Fisher1926a,
author = {Fisher, Ronald Aylmer},
journal = {The Eugenics review},
month = {apr},
number = {1},
pages = {32--33},
pmid = {21259825},
publisher = {Oliver and Boyd},
title = {{Baye's theorem and the fourfold table}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21259825 http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2984620/},
volume = {18},
year = {1926}
}
@article{Bhattacharya2012,
abstract = {Our first focus is prediction of a categorical response variable using features that lie on a general manifold. For example, the manifold may correspond to the surface of a hypersphere. We propose a general kernel mixture model for the joint distribution of the response and predictors, with the kernel expressed in product form and dependence induced through the unknown mixing measure. We provide simple sufficient conditions for large support and weak and strong posterior consistency in estimating both the joint distribution of the response and predictors and the conditional distribution of the response. Focusing on a Dirichlet process prior for the mixing measure, these conditions hold using von Mises-Fisher kernels when the manifold is the unit hypersphere. In this case, Bayesian methods are developed for efficient posterior computation using slice sampling. Next we develop Bayesian nonparametric methods for testing whether there is a difference in distributions between groups of observations on the manifold having unknown densities. We prove consistency of the Bayes factor and develop efficient computational methods for its calculation. The proposed classification and testing methods are evaluated using simulation examples and applied to spherical data applications. {\textcopyright} 2012 Elsevier Inc.},
author = {Bhattacharya, Abhishek and Dunson, David},
doi = {10.1016/j.jmva.2012.02.020},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bhattacharya, Dunson - 2012 - Nonparametric Bayes classification and hypothesis testing on manifolds.pdf:pdf},
issn = {0047259X},
journal = {Journal of Multivariate Analysis},
keywords = {Bayes factor,Classification,Dirichlet process mixture,Flexible prior,Hypothesis testing,Non-Euclidean manifold,Nonparametric Bayes,Posterior consistency,Spherical data},
month = {oct},
pages = {1--19},
publisher = {Academic Press},
title = {{Nonparametric Bayes classification and hypothesis testing on manifolds}},
volume = {111},
year = {2012}
}
@article{Broad1918,
author = {Broad, C.D.},
journal = {Mind},
number = {108},
pages = {389--404},
title = {{On the Relation between Induction and Probability (Part I)}},
volume = {27},
year = {1918}
}
@incollection{Fraser1984,
address = {Amsterdam},
author = {Fraser, D.A.S. and Monette, G. and Ng, K.-W.},
booktitle = {Multivariate Analysis VI},
editor = {Krishnaiah, P.R.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fraser, Monette, Ng - 1984 - Marginalization, Likelihood and Structured Models.pdf:pdf},
publisher = {Elsevier Science Publishers B.V.},
title = {{Marginalization, Likelihood and Structured Models}},
year = {1984}
}
@article{Yang2005,
abstract = {A traditional approach to statistical inference is to identify the true or best model first with little or no consideration of the specific goal of inference in the model identification stage. Can the pursuit of the true model also lead to optimal regression estimation? In model selection, it is well known that BIC is consistent in selecting the true model, and AIC is minimax-rate optimal for estimating the regression function. A recent promising direction is adaptive model selection, in which, in contrast to AIC and BIC, the penalty term is data-dependent. Some theoretical and empirical results have been obtained in support of adaptive model selection, but it is still not clear if it can really share the strengths of AIC and BIC. Model combining or averaging has attracted increasing attention as a means to overcome the model selection uncertainty. Can Bayesian model averaging be optimal for estimating the regression function in a minimax sense? We show that the answers to these questions are basically in the negative: for any model selection criterion to be consistent, it must behave suboptimally for estimating the regression function in terms of minimax rate of covergence; and Bayesian model averaging cannot be minimax-rate optimal for regression estimation.},
author = {Yang, Yuhong},
doi = {10.2307/20441246},
journal = {Biometrika},
pages = {937--950},
publisher = {Oxford University PressBiometrika Trust},
title = {{Can the Strengths of AIC and BIC Be Shared? A Conflict between Model Indentification and Regression Estimation}},
url = {https://www.jstor.org/stable/20441246},
volume = {92},
year = {2005}
}
@article{LyWagenmakersFBST2021,
author = {Ly, Alexander and Wagenmakers, Eric-Jan},
doi = {10.1007/s42113-021-00109-y},
journal = {Computational Brain {\&} Behavior},
title = {{A Critical Evaluation of the FBST ev for Bayesian Hypothesis Testing}},
url = {https://link.springer.com/article/10.1007/s42113-021-00109-y},
year = {2021}
}
@article{Zhang2011Fraser,
author = {Zhang, Tong},
journal = {Statistical Science},
number = {3},
pages = {326--328},
title = {{Discussion of ``Is Bayes Posterior just Quick and Dirty Confidence?'' by D.A.S. Fraser}},
volume = {26},
year = {2011}
}
@book{Hammersley1964,
abstract = {This monograph surveys the present state of Monte Carlo methods. we have dallied with certain topics that have interested us Although personally, we hope that our coverage of the subject is reasonably complete; at least we believe that this book and the references in it come near to exhausting the present range of the subject. On the other hand, there are many loose ends; for example we mention various ideas for variance reduction that have never been seriously appli(:d in practice. This is inevitable, and typical of a subject that has remained in its infancy for twenty years or more. We are convinced Qf:ver theless that Monte Carlo methods will one day reach an impressive maturity. The main theoretical content of this book is in Chapter 5; some readers may like to begin with this chapter, referring back to Chapters 2 and 3 when necessary. Chapters 7 to 12 deal with applications of the Monte Carlo method in various fields, and can be read in any order. For the sake of completeness, we cast a very brief glance in Chapter 4 at the direct simulation used in industrial and operational research, where the very simplest Monte Carlo techniques are usually sufficient. We assume that the reader has what might roughly be described as a 'graduate' knowledge of mathematics. The actual mathematical techniques are, with few exceptions, quite elementary, but we have freely used vectors, matrices, and similar mathematical language for the sake of conciseness. 1 The general nature of Monte Carlo methods -- 2 Short resumé of statistical terms -- 3 Random, pseudorandom, and quasirandom numbers -- 4 Direct simulation -- 5 General principles of the Monte Carlo method -- 6 Conditional Monte Carlo -- 7 Solution of linear operator equations -- 8 Radiation shielding and reactor criticality -- 9 Problems in statistical mechanics -- 10 Long polymer molecules -- 11 Percolation processes -- 12 Multivariable problems -- References.},
author = {Hammersley, J. M. and Handscomb, D. C.},
isbn = {9789400958210},
publisher = {Springer Netherlands},
title = {{Monte Carlo Methods}},
year = {1964}
}
@article{Shuster2009,
abstract = {Abstract 10.1002/sim.3581.abs When the one-sample or two-sample t-test is either taught in the class room or applied in practice to small samples, there is considerable divergence of opinion as to whether or not the inferences drawn are valid. Many point to the ‘Robustness' of the t-test to violations of assumptions, while others use rank or other robust methods because they believe that the t-test is not robust against violations of such assumptions. It is quite likely, despite the apparent divergence of these two opinions, that both arguments have considerable merit. If we agree that this question cannot possibly be resolved in general, the issue becomes one of determining, before any actual data have been collected, whether the t-test will or will not be robust in a specific application. This paper describes statistical analysis system software, covering a large collection of potential input probability distributions, to investigate both the null and power properties of various one- and two-sample t-tests and their normal approximations, as well as the Wilcoxon two-sample and sign-rank one-sample tests, allowing potential practitioners to determine, at the study design stage, whether the t-test will be robust in their specific application. Sample size projections, based on these actual distributions, are also included. This paper is not intended as a tool to assess robustness after the data have been collected. Copyright {\textcopyright} 2009 John Wiley {\&} Sons, Ltd.},
author = {Shuster, Jonathan J},
doi = {10.1002/sim.3581},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Shuster - 2009 - Student t-tests for potentially abnormal data.pdf:pdf},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {Robustness,Satterthwaite approximation,Sign-rank test,T-test,Wilcoxon test},
month = {jul},
number = {16},
pages = {2170--2184},
pmid = {19326398},
publisher = {NIH Public Access},
title = {{Student t-tests for potentially abnormal data}},
volume = {28},
year = {2009}
}
@book{Walker2017,
address = {New York},
author = {Walker, Matthew},
publisher = {Allen Lane},
title = {{Why We Sleep: The New Science of Sleep and Dreams}},
year = {2017}
}
@article{Fisher1935fiducial,
author = {Fisher, R.A.},
journal = {Annals of Eugenics},
number = {4},
pages = {391--398},
title = {{The fiducial argument in statistical inference}},
volume = {6},
year = {1935}
}
@article{Maxwell2015,
abstract = {Psychology has recently been viewed as facing a replication crisis because efforts to replicate past study findings frequently do not show the same result. Often, the first study showed a statistically significant result but the replication does not. Questions then arise about whether the first study results were false positives, and whether the replication study correctly indicates that there is truly no effect after all. This article suggests these so-called failures to replicate may not be failures at all, but rather are the result of low statistical power in single replication studies, and the result of failure to appreciate the need for multiple replications in order to have enough power to identify true effects. We provide examples of these power problems and suggest some solutions using Bayesian statistics and metaanalysis. Although the need for multiple replication studies may frustrate those who would prefer quick answers to psychology's alleged crisis, the large sample sizes typically needed to provide firm evidence will almost always require concerted efforts from multiple investigators. As a result, it remains to be seen how many of the recently claimed failures to replicate will be supported or instead may turn out to be artifacts of inadequate sample sizes and single study replications.},
author = {Maxwell, Scott E. and Lau, Michael Y. and Howard, George S.},
doi = {10.1037/a0039400},
issn = {0003066X},
journal = {American Psychologist},
keywords = {Bayesian methods,Equivalence tests,False positive results,Metaanalysis,Statistical power},
month = {sep},
number = {6},
pages = {487--498},
pmid = {26348332},
publisher = {American Psychological Association Inc.},
title = {{Is psychology suffering from a replication crisis?: What does 'failure to replicate' really mean?}},
volume = {70},
year = {2015}
}
@book{Ulam1991,
abstract = {"Preface to the 1991 edition by William G. Mathews and Daniel O. Hirsch. Note on S.M. Ulam's mathematics by Jan Mycielski. Postcript by Françoise Ulam."},
address = {California},
author = {Ulam, Stanislaw M.},
isbn = {0520071549},
pages = {329},
publisher = {University of California Press},
title = {{Adventures of a mathematician}},
year = {1991}
}
@incollection{Hacking1980,
address = {Cambridge},
author = {Hacking, Ian},
booktitle = {Science, Belief and Behaviour: Essays in Honour of R. B. Brathwaite},
editor = {Mellor, D.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Hacking - 1980 - The Theory of Probable Inference Neyman, Peirce and Braithwaite.pdf:pdf},
pages = {141--160},
publisher = {Cambridge University Press},
title = {{The Theory of Probable Inference: Neyman, Peirce and Braithwaite}},
year = {1980}
}
@book{Kuchlin2005,
address = {Berlin},
annote = {Literaturverz. S. [445] - 448},
author = {K{\"{u}}chlin, Wolfgang and Weber, Andreas},
booktitle = {eXamen.press},
edition = {3., {\"{u}}berar},
isbn = {978-3-540-20958-4},
keywords = {Electronic data processing,Java (Computer program language),Java 2,Objektorientierte Programmierung,java},
language = {ger},
pages = {471},
publisher = {Springer},
shorttitle = {Einf{\"{u}}hrung in die Informatik},
title = {{Einf{\"{u}}hrung in die Informatik: objektorientiert mit Java ; mit 4 Tabellen}},
year = {2005}
}
@article{Lehmann1993,
author = {Lehmann, E.L.},
file = {:Users/riko/Downloads/lehmann{\_}1993.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {424},
pages = {1242--1249},
title = {{The Fisher, Neyman-Pearson Theories of Testign Hypotheses: One Theory or Two?}},
volume = {88},
year = {1993}
}
@article{Cortez2008,
abstract = {Although the educational level of the Portuguese population has improved in the last decades, the statistics keep Portugal at Europe's tail end due to its high student failure rates. In particular, lack of success in the core classes of Mathematics and the Portuguese language is extremely serious. On the other hand, the fields of Business Intelligence (BI)/Data Mining (DM), which aim at extracting high-level knowledge from raw data, offer interesting automated tools that can aid the education domain. The present work intends to approach student achievement in secondary education using BI/DM techniques. Recent real-world data (e.g. student grades, demographic, social and school related features) was collected by using school reports and ques-tionnaires. The two core classes (i.e. Mathematics and Portuguese) were modeled under binary/five-level classification and regression tasks. Also, four DM models (i.e. Decision 'Trees, Random Forest, Neural Networks and Support Vector Machines) and three input selections (e.g. with and without previous grades) were tested. The results show that a good predictive accuracy can be achieved, provided that the first and/or second school period grades are available. Although student achievement is highly influenced by past evaluations, an explanatory analysis has shown that there are also other relevant features (e.g. number of absences, parent's job and education, alcohol consumption). As a direct outcome of this research, more efficient student prediction tools can be be developed, improving the quality of education and enhancing school resource management.},
author = {Cortez, Paulo and Silva, Alice},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Cortez, Silva - 2008 - Using data mining to predict secondary school student performance.pdf:pdf},
isbn = {9789077381397},
journal = {15th European Concurrent Engineering Conference 2008, ECEC 2008 - 5th Future Business Technology Conference, FUBUTEC 2008},
keywords = {Business intelligence in education,Classification and regression,Decision trees,Random forest},
number = {2000},
pages = {5--12},
title = {{Using data mining to predict secondary school student performance}},
volume = {2003},
year = {2008}
}
@article{Harlow1983,
abstract = {F rom the beginning, research at Los Alamos has exploited the happy interrelationship among experiment, field observation, and theory. Each of these has its tools; for theoretical investigations the most important are the indispensable calculators and com-puters. Until the age of computers, the classical approaches to theoretical investigations centered on the mathematical techniques of perturbations and linearization. Complicated nonlinear interactions usually could be ex-amined only for special circumstances in which the behavior departed slightly from an equilibrium or otherwise solvable configura-tion. There were, and still are, many interest-ing problems of this kind, and where nature did not cooperate with this type of simplicity, there were experimental data and empirical models to help advance the technology. With increased sophistication, however, the needed answers became much more difficult and expensive to obtain. In the case of weapons calculations, for example, intrac-table nonlinearities in the mathematical ex-pressions of natural laws could no longer be avoided. Numerical techniques were required for solving the equations, an approach that was extremely time-consuming if carried out by hand. The first rudimentary computers came along just in the nick of time, especially 132 for the wartime research at Los Alamos. During World War II, the primary mission at Los Alamos was to design, build, and proof-test a fission bomb. The problems presented a variety of technical challenges, both experimental and theoretical. They were hard but eventually proved solvable. Some of the theoretical analysis was accomplished by analytical procedures, but most required tedious numerical evaluations cranked out on desk-top calculators or on electro-mechanical business machines using punch cards and sets of plug boards ap-propriately wired for different types of calcu-lations. These last devices, with a separate machine for each functional or calculational step, constituted the Laboratory's first rudi-mentary computer. Stored-program, high-speed, electronic computers were not used until after the war; they simply didn't exist. However, this experience of the war years was enough to excite the involved scientists and engineers to the power of mechanized calculations and acted as a tremendous spur to the postwar development of the modem computer.},
author = {Harlow, Francis H and Metropolis, N},
issn = {0273-7116},
journal = {Los Alamos Science},
pages = {132--141},
title = {{Computing {\&} Computers: Weapons Simulation Leads to the Computer Era}},
year = {1983}
}
@book{Hacking1965,
address = {London},
author = {Hacking, Ian},
publisher = {Cambridge University Press},
title = {{Logic of Statistical Inference}},
year = {1965}
}
@book{watson2006,
abstract = {This book reveals the development of students' understanding of statistical literacy. It provides a way to "see" student thinking and gives readers a deeper sense of how students think about important statistical topics. Intended as a complement to curriculum documents and textbook series, it is consistent with the current principles and standards of the National Council of Teachers of Mathematics.$\backslash$nThe term "statistical literacy" is used to emphasize that the purpose of the school curriculum should not be to turn out statisticians but to prepare statistically literate school graduates who are prepared to participate in social decision making. Based on ten years of research--with reference to other significant research as appropriate--the book looks at students' thinking in relation to tasks based on sampling, graphical representations, averages, chance, beginning inference, and variation, which are essential to later work in formal statistics. For those students who do not proceed to formal study, as well as those who do, these concepts provide a basis for decision making or questioning when presented with claims based on data in societal settings.$\backslash$nStatistical Literacy at School: Growth and Goals:$\backslash$n*establishes an overall framework for statistical literacy in terms of both the links to specific school curricula and the wider appreciation of contexts within which chance and data-handling ideas are applied;$\backslash$n*demonstrates, within this framework, that there are many connections among specific ideas and constructs;$\backslash$n*provides tasks, adaptable for classroom or assessment use, that are appropriate for the goals of statistical literacy;$\backslash$n*presents extensive examples of student performance on the tasks, illustrating hierarchies of achievement, to assist in monitoring gains and meeting the goals of statistical literacy; and$\backslash$n*includes a summary of analysis of survey data that suggests a developmental hierarchy for students over the years of schooling with respect to the goal of statistical literacy.$\backslash$nStatistical Literacy at School: Growth and Goals is directed to researchers, curriculum developers, professionals, and students in mathematics education as well those across the curriculum who are interested in students' cognitive development within the field; to teachers who want to focus on the concepts involved in statistical literacy without the use of formal statistical techniques; and to statisticians who are interested in the development of student understanding before students are exposed to the formal study of statistics.},
author = {Watson, Jane M.},
booktitle = {Statistical Literacy at School: Growth and Goals},
doi = {10.4324/9780203053898},
isbn = {9780203053898},
pages = {1--306},
title = {{Statistical Literacy at School}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84909390192{\&}partnerID=tZOtx3y1},
year = {2006}
}
@book{Robert2004,
abstract = {2nd ed. Subject: "Monte Carlo statistical methods, particularly those based on Markov chains, are now an essential component of the standard set of techniques used by statisticians. This new edition has been revised towards a coherent and flowing coverage of these simulation techniques, with incorporation of the most recent developments in the field. In particular, the introductory coverage of random variable generation has been totally revised, with many concepts being unified through a fundamental theorem of simulation"--Back cover. Random Variable Generation -- Monte Carlo Integration -- Controlling Monte Carlo Variance -- Monte Carlo Optimization -- Markov Chains -- Metropolis-Hastings Algorithm -- Slice Sampler -- Two-Stage Gibbs Sampler -- Multi-Stage Gibbs Sampler -- Variable Dimension Models and Reversible Jump -- Diagnosing Convergence -- Perfect Sampling -- Iterated and Sequential Importance Sampling.},
address = {New York},
author = {Robert, Christian and Casella, George},
isbn = {1441919392},
pages = {645},
publisher = {Springer},
title = {{Monte Carlo Statistical Methods}},
year = {2004}
}
@article{Robert2016,
abstract = {This note is a discussion commenting on the paper by Ly et al. on "Harold Jeffreys's Default Bayes Factor Hypothesis Tests: Explanation, Extension, and Application in Psychology" and on the perceived shortcomings of the classical Bayesian approach to testing, while reporting on an alternative approach advanced by Kamary et al. (2014) as a solution to this quintessential inference problem.},
archivePrefix = {arXiv},
arxivId = {1506.08292},
author = {Robert, Christian P.},
doi = {10.1016/j.jmp.2015.08.002},
eprint = {1506.08292},
isbn = {8573140232},
issn = {10960880},
journal = {Journal of Mathematical Psychology},
keywords = {Bayes factor,Bayesian inference,Consistency,Decision theory,Evidence,Loss function,Mixtures of distributions,Testing of hypotheses},
number = {2009},
pages = {33--37},
title = {{The expected demise of the Bayes factor}},
volume = {72},
year = {2016}
}
@article{Lakens2017,
abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
author = {Lakens, Dani{\"{e}}l},
doi = {10.1177/1948550617697177},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Lakens - 2017 - Equivalence Tests A Practical Primer for t Tests, Correlations, and Meta-Analyses.pdf:pdf},
issn = {19485514},
journal = {Social Psychological and Personality Science},
keywords = {equivalence testing,null hypothesis significance testing,power analysis,research methods},
number = {4},
pages = {355--362},
pmid = {28736600},
publisher = {SAGE Publications Inc.},
title = {{Equivalence Tests: A Practical Primer for t Tests, Correlations, and Meta-Analyses}},
volume = {8},
year = {2017}
}
@article{Pratt1977,
author = {Pratt, John W.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Pratt - 1977 - ' Decisions ' as Statistical Evidence and Birnbaum ' s ' Confidence Concept '.pdf:pdf},
journal = {Synthese},
number = {1},
pages = {59--69},
title = {{'Decisions' as Statistical Evidence and Birnbaum's 'Confidence Concept'}},
volume = {36},
year = {1977}
}
@book{Peng2018,
address = {Baltimore, Maryland},
author = {Peng, Roger D.},
publisher = {Leanpub},
title = {{Advanced Statistical Computing}},
year = {2018}
}
@article{Pearl1998,
abstract = {Structural equation models (SEMs) have dominated causal analysis in the social and behavioral sciences since the 1960s. Currently, many SEM practitioners are having difficulty articulating the causal content of SEM and are seeking foundational answers. Recent developments in the areas of graphical models and the logic of causality show potential for alleviating such difficulties and, thus, revitalizing structural equations as the primary language of causal modeling. This article summarizes several of these developments, including the prediction of vanishing partial correlations, model testing, model equivalence, parametric and nonparametric identifiability, control of confounding, and covariate selection. These developments clarify the causal and statistical components of SEMs and the role of SEM in the empirical sciences.},
author = {Pearl, Judea},
doi = {10.1177/0049124198027002004},
issn = {00491241},
journal = {Sociological Methods and Research},
month = {nov},
number = {2},
pages = {226--284},
publisher = {SAGE Publications Inc.},
title = {{Graphs, causality, and structural equation models}},
url = {http://journals.sagepub.com/doi/10.1177/0049124198027002004},
volume = {27},
year = {1998}
}
@article{Ioannidis2012,
abstract = {The ability to self-correct is considered a hallmark of science. However, self-correction does not always happen to scientific evidence by default. The trajectory of scientific credibility can fluctuate over time, both for defined scientific fields and for science at-large. History suggests that major catastrophes in scientific credibility are unfortunately possible and the argument that “it is obvious that progress is made” is weak. Careful evaluation of the current status of credibility of various scientific fields is important in order to understand any credibility deficits and how one could obtain and establish more trustworthy results. Efficient and unbiased replication mechanisms are essential for maintaining high levels of scientific credibility. Depending on the types of results obtained in the discovery and replication phases, there are different paradigms of research: optimal, self-correcting, false nonreplication, and perpetuated fallacy. In the absence of replication efforts, one is left with unconfirmed (genuine) discoveries and unchallenged fallacies. In several fields of investigation, including many areas of psychological science, perpetuated and unchallenged fallacies may comprise the majority of the circulating evidence. I catalogue a number of impediments to self-correction that have been empirically studied in psychological science. Finally, I discuss some proposed solutions to promote sound replication practices enhancing the credibility of scientific results as well as some potential disadvantages of each of them. Any deviation from the principle that seeking the truth has priority over any other goals may be seriously damaging to the self-correcting functions of science.},
author = {Ioannidis, John P.A.},
doi = {10.1177/1745691612464056},
isbn = {1745-6916$\backslash$r1745-6924},
issn = {17456916},
journal = {Perspectives on Psychological Science},
keywords = {replication,self-correction},
month = {nov},
number = {6},
pages = {645--654},
pmid = {26168125},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Why Science Is Not Necessarily Self-Correcting}},
url = {http://journals.sagepub.com/doi/10.1177/1745691612464056},
volume = {7},
year = {2012}
}
@article{Celeux2018,
abstract = {Determining the number G of components in a finite mixture distribution is an important and difficult inference issue. This is a most important question, because statistical inference about the resulting model is highly sensitive to the value of G. Selecting an erroneous value of G may produce a poor density estimate. This is also a most difficult question from a theoretical perspective as it relates to unidentifiability issues of the mixture model. This is further a most relevant question from a practical viewpoint since the meaning of the number of components G is strongly related to the modelling purpose of a mixture distribution. We distinguish in this chapter between selecting G as a density estimation problem in Section 2 and selecting G in a model-based clustering framework in Section 3. Both sections discuss frequentist as well as Bayesian approaches. We present here some of the Bayesian solutions to the different interpretations of picking the "right" number of components in a mixture, before concluding on the ill-posed nature of the question.},
archivePrefix = {arXiv},
arxivId = {1812.09885},
author = {Celeux, Gilles and Fruewirth-Schnatter, Sylvia and Robert, Christian P.},
eprint = {1812.09885},
title = {{Model Selection for Mixture Models - Perspectives and Strategies}},
url = {http://arxiv.org/abs/1812.09885},
year = {2018}
}
@article{Bartlett1937,
author = {Bartlett, M. S. and S., M.},
doi = {10.1098/rspa.1937.0109},
issn = {1364-5021},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
month = {may},
number = {901},
pages = {268--282},
title = {{Properties of Sufficiency and Statistical Tests}},
url = {http://rspa.royalsocietypublishing.org/cgi/doi/10.1098/rspa.1937.0109},
volume = {160},
year = {1937}
}
@article{Matzke2015,
author = {Matzke, Dora and Nieuwenhuis, Sander and van Rijn, Hedderik and Slagter, Heleen A. and van der Molen, Maurits W. and Wagenmakers, Eric-Jan},
doi = {10.1037/xge0000038},
issn = {1939-2222},
journal = {Journal of Experimental Psychology: General},
month = {feb},
number = {1},
pages = {e1--e15},
title = {{The effect of horizontal eye movements on free recall: A preregistered adversarial collaboration.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/xge0000038},
volume = {144},
year = {2015}
}
@book{Rathgeb,
author = {Rathgeb, Martin and Helmerich, Markus and Kr{\"{o}}mer, Ralf and Lengnink, Katja},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Rathgeb et al. - 2013 - Mathematik im Prozess.pdf:pdf},
isbn = {9783658022730},
title = {{Mathematik im Prozess}},
year = {2013}
}
@article{Fisher1920,
abstract = {Reproduced with permission of Blackwell Science},
author = {Fisher, Ronald Aylmer},
doi = {10.1093/mnras/80.8.758},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1920 - A Mathematical Examination of the Methods of Determining the Accuracy of an Observation by the Mean Error, and by the Me.pdf:pdf},
issn = {0035-8711},
journal = {Monthly Notices of the Royal Astronomical Society},
number = {80},
pages = {758--770},
title = {{A Mathematical Examination of the Methods of Determining the Accuracy of an Observation by the Mean Error, and by the Mean Square Error.}},
url = {https://digital.library.adelaide.edu.au/dspace/handle/2440/15168 http://digital.library.adelaide.edu.au/dspace/handle/2440/15168},
year = {1920}
}
@article{VanDongen2019,
abstract = {When data analysts operate within different statistical frameworks (e.g., frequentist versus Bayesian, emphasis on estimation versus emphasis on testing), how does this impact the qualitative conclusions that are drawn for real data? To study this question empirically we selected from the literature two simple scenarios-involving a comparison of two proportions and a Pearson correlation-and asked four teams of statisticians to provide a concise analysis and a qualitative interpretation of the outcome. The results showed considerable overall agreement; nevertheless, this agreement did not appear to diminish the intensity of the subsequent debate over which statistical framework is more appropriate to address the questions at hand. ARTICLE HISTORY},
author = {van Dongen, Noah N. N. and van Doorn, Johnny B. and Gronau, Quentin F. and van Ravenzwaaij, Don and Hoekstra, Rink and Haucke, Matthias N. and Lakens, Daniel and Hennig, Christian and Morey, Richard D. and Homer, Saskia and Gelman, Andrew and Sprenger, Jan and Wagenmakers, Eric-Jan},
doi = {10.1080/00031305.2019.1565553},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/van Dongen et al. - 2019 - Multiple Perspectives on Inference for Two Simple Statistical Scenarios.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {frequentist or bayesian,multilab analysis,paradigms,statistical,testing or},
number = {sup1},
pages = {328--339},
title = {{Multiple Perspectives on Inference for Two Simple Statistical Scenarios}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2019.1565553},
volume = {73},
year = {2019}
}
@book{Lee2004,
author = {Lee, P.M.},
edition = {4th},
title = {{Bayesian Statistics: An Introduction}},
year = {2004}
}
@article{Fisher1959a,
author = {Fisher, Ronald Aylmer},
doi = {10.2307/1266307},
issn = {00401706},
journal = {Technometrics},
month = {feb},
number = {1},
pages = {21},
title = {{Mathematical Probability in the Natural Sciences}},
url = {https://www.jstor.org/stable/1266307?origin=crossref},
volume = {1},
year = {1959}
}
@article{shapiro_analysis_1965,
author = {Shapiro, Samuel Sanford and Wilk, Martin B},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Shapiro, Wilk - 1965 - An analysis of variance test for normality (complete samples).pdf:pdf},
journal = {Biometrika},
number = {3/4},
pages = {591--611},
title = {{An analysis of variance test for normality (complete samples)}},
url = {http://www.jstor.org/stable/2333709},
volume = {52},
year = {1965}
}
@article{Redheffer1951,
abstract = {The object of this note is to point out and discuss a simple transformation of an absolutely continuous k-variate distribution in the uniform distribution on the k-dimensional hypercube.},
author = {Redheffer, R. M.},
doi = {10.1214/aoms/1177729704},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {1},
pages = {128--130},
publisher = {Institute of Mathematical Statistics},
title = {{A Note on the Surprise Index}},
volume = {22},
year = {1951}
}
@incollection{Mengersen1999,
address = {Oxford},
author = {Mengersen, Kerrie and Robert, Christian P. and Guihenneuc-Jouyaux, Chantal},
booktitle = {Bayesian Statistics 6},
editor = {Berger, J. and Bernado, J. and Dawid, A. and Lindley, D. and Smith, A.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Mengersen, Robert, Guihenneuc-Jouyaux - 1998 - MCMC Convergence Diagnostics A Reviewww.pdf:pdf},
pages = {415--440},
publisher = {Oxford University Press},
title = {{MCMC Convergence Diagnostics: A "Reviewww"}},
volume = {91},
year = {1998}
}
@article{Fernandes-Taylor2011,
abstract = {Background: To assist educators and researchers in improving the quality of medical research, we surveyed the editors and statistical reviewers of high-impact medical journals to ascertain the most frequent and critical statistical errors in submitted manuscripts. Findings. The Editors-in-Chief and statistical reviewers of the 38 medical journals with the highest impact factor in the 2007 Science Journal Citation Report and the 2007 Social Science Journal Citation Report were invited to complete an online survey about the statistical and design problems they most frequently found in manuscripts. Content analysis of the responses identified major issues. Editors and statistical reviewers (n = 25) from 20 journals responded. Respondents described problems that we classified into two, broad themes: A. statistical and sampling issues and B. inadequate reporting clarity or completeness. Problems included in the first theme were (1) inappropriate or incomplete analysis, including violations of model assumptions and analysis errors, (2) uninformed use of propensity scores, (3) failing to account for clustering in data analysis, (4) improperly addressing missing data, and (5) power/sample size concerns. Issues subsumed under the second theme were (1) Inadequate description of the methods and analysis and (2) Misstatement of results, including undue emphasis on p-values and incorrect inferences and interpretations. Conclusions: The scientific quality of submitted manuscripts would increase if researchers addressed these common design, analytical, and reporting issues. Improving the application and presentation of quantitative methods in scholarly manuscripts is essential to advancing medical research. {\textcopyright} 2011 Fernandes-Taylor et al; licensee BioMed Central Ltd.},
author = {Fernandes-Taylor, Sara and Hyun, Jenny K and Reeder, Rachelle N and Harris, Alex Hs},
doi = {10.1186/1756-0500-4-304},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fernandes-Taylor et al. - 2011 - Common statistical and research design problems in manuscripts submitted to high-impact medical journal.pdf:pdf},
journal = {BMC Research Notes},
month = {aug},
pages = {304},
publisher = {BioMed Central},
title = {{Common statistical and research design problems in manuscripts submitted to high-impact medical journals}},
volume = {4},
year = {2011}
}
@inproceedings{ghosal1996,
author = {Ghosal, Subhashis},
booktitle = {Proceedings of Varanashi Symposium in Bayesian Inference},
publisher = {Banaras Hindu University},
title = {{A Review of Consistency and Convergence of Posterior Distribution}},
year = {1996}
}
@misc{loo2019,
author = {Vehtari, A. and Gabry, J. and Magnusson, M. and Yao, Y. and Gelman, A.},
publisher = {Comprehensive R Archive Network},
title = {{loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models.}},
url = {https://mc-stan.org/loo},
year = {2019}
}
@article{Cribbie2009,
author = {Cribbie, Robert A: and Arpin-Cribbie, Chantal A. and Gruman, Jamie A.},
doi = {10.1080/00220970903224552},
journal = {The Journal of Experimental Education},
number = {1},
pages = {1--13},
title = {{Tests of Equivalence for One-Way Independent Groups Designs}},
volume = {78},
year = {2009}
}
@article{Nuzzo2014,
abstract = {P values, the 'gold standard' of statistical validity, are not as reliable as many scientists assume.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Nuzzo, Regina},
doi = {10.1136/bmj.1.6053.66},
eprint = {1011.1669},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Nuzzo - 2014 - Statistical errors P values, the gold standard of statistical validity, are not as reliable as many scientists assume.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0959-8138},
journal = {Nature},
number = {7487},
pages = {150--152},
pmid = {9441199},
title = {{Statistical errors: P values, the "gold standard" of statistical validity, are not as reliable as many scientists assume.}},
volume = {506},
year = {2014}
}
@article{Roberts1998,
abstract = {We review and discuss some recent progress in the theory of Markov-chain Monte Carlo applications, particularly oriented to applications in statistics. We attempt to assess the relevance of this theory for practical applications.},
author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
doi = {10.2307/3315667},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Roberts, Rosenthal - 1997 - Markov chain Monte Carlo Some practical implications of theoretical results.pdf:pdf},
issn = {03195724},
journal = {The Canadian Journal of Statistics},
keywords = {Gibbs sampler,MCMC,Markov chain,Metropolis‐Hastings algorithm,Monte Carlo},
month = {mar},
number = {1},
pages = {5--20},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Markov chain Monte Carlo: Some practical implications of theoretical results}},
url = {http://doi.wiley.com/10.2307/3315667},
volume = {26},
year = {1997}
}
@article{Schuirmann1987,
abstract = {The statistical test of hypothesis of no difference between the average bioavailabilities of two drug formulations, usually supplemented by an assessment of what the power of the statistical test would have been if the true averages had been inequivalent, continues to be used in the statistical analysis of bioavailability/bioequivalence studies. In the present article, this Power Approach (which in practice usually consists of testing the hypothesis of no difference at level 0.05 and requiring an estimated power of 0.80) is compared to another statistical approach, the Two One-Sided Tests Procedure, which leads to the same conclusion as the approach proposed by Westlake based on the usual (shortest) 1-2 alpha confidence interval for the true average difference. It is found that for the specific choice of alpha = 0.05 as the nominal level of the one-sided tests, the two one-sided tests procedure has uniformly superior properties to the power approach in most cases. The only cases where the power approach has superior properties when the true averages are equivalent correspond to cases where the chance of concluding equivalence with the power approach when the true averages are not equivalent exceeds 0.05. With appropriate choice of the nominal level of significance of the one-sided tests, the two one-sided tests procedure always has uniformly superior properties to the power approach. The two one-sided tests procedure is compared to the procedure proposed by Hauck and Anderson.},
author = {Schuirmann, D.J.},
issn = {0090-466X},
journal = {Journal of pharmacokinetics and biopharmaceutics},
month = {dec},
number = {6},
pages = {657--80},
pmid = {3450848},
title = {{A comparison of the two one-sided tests procedure and the power approach for assessing the equivalence of average bioavailability.}},
volume = {15},
year = {1987}
}
@techreport{Matthews1998,
author = {Matthews, Robert},
institution = {Department of Computer Science, Aston University, Birmingham},
title = {{Bayesian Critique of Statistics in Health: The Great Health Hoax}},
url = {http://www2.isye.gatech.edu/{~}brani/isyebayes/bank/pvalue.pdf},
year = {1998}
}
@article{Kempthorne1976,
abstract = {This paper discusses the role of significance testing, in contrast to hypothesis testing. It takes the view that the two processes must be differentiated because they are in fact different even though they possess scpie common mathematical features. A supporting view of significance testing is presented. Difficulties and obscurities are discussed. {\textcopyright} 1976, Taylor {\&} Francis Group, LLC. All rights reserved.},
author = {Kempthorne, Oscar},
doi = {10.1080/03610927608827394},
issn = {1532415X},
journal = {Communications in Statistics - Theory and Methods},
keywords = {Bayesian processes,composite null hypotheses,evidentiality of data,repeated sampling principle,significance tests,tests of hypotheses},
number = {8},
pages = {763--777},
title = {{Of What Use are Tests of Significance and Tests of Hypothesis}},
volume = {5},
year = {1976}
}
@article{Ialongo2017,
abstract = {Hypothesis testing is a methodological paradigm widely popularized outside the field of pure statistics, and nowadays more or less familiar to the largest part of biomedical researchers. Conversely, the equivalence testing is still somehow obscure and misunderstood, although it represents a conceptual mainstay for some biomedical fields like pharmacology. In order to appreciate the way it could suit laboratory medicine, it is necessary to understand the philosophy behind it, and in turn how it stemmed and differentiated along the history of classical hypothesis testing. Here we present the framework of equivalence testing, the various tests used to assess equivalence and discuss their applicability to laboratory medicine research and issues.},
author = {Ialongo, Cristiano},
doi = {10.11613/BM.2017.001},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Ialongo - 2017 - The logic of equivalence testing and its use in laboratory medicine.pdf:pdf},
issn = {13300962},
journal = {Biochemia Medica},
keywords = {Biostatistics,Methodological studies,Statistical data analysis},
month = {feb},
number = {1},
pages = {5--13},
pmid = {28392720},
publisher = {Biochemia Medica, Editorial Office},
title = {{The logic of equivalence testing and its use in laboratory medicine}},
url = {/pmc/articles/PMC5382845/?report=abstract https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5382845/},
volume = {27},
year = {2017}
}
@article{xinogalos_evaluation_2012,
abstract = {Programming microworlds are being used for introducing students to programming for many years. Although many professors and school teachers report positive results from using programming microwords, these results are usually based on anecdotal evidence rather than rigorous empirical evaluation. A question that has not been answered yet with certainty is whether the knowledge acquired in the context of a programming microworld is transferred to a conventional programming language. In an attempt to investigate this issue we used a specially designed middle-term exam and a questionnaire at the end of an undergraduate Object-Oriented Programming (OOP) course. The course uses the programming microworld objectKarel for a short introduction to the most fundamental OOP concepts, and then the environment BlueJ and Java for presenting the real thing. The analysis of students' replies in the questionnaire shows that the introduction to OOP with the microworld helps the vast majority of students to comprehend fundamental concepts of OOP and what is more important is that this knowledge is transferred to Java afterwards. These results are supported by the comparative analysis of students' performance in written exams that took place in the context of two distinct offerings of the course, prior and after the use of the microworld in it.},
author = {Xinogalos, Stelios},
doi = {10.2190/EC.47.3.b},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Xinogalos - 2012 - An Evaluation of Knowledge Transfer from Microworld Programming to Conventional Programming.html:html},
issn = {0735-6331},
journal = {Journal of Educational Computing Research},
month = {oct},
number = {3},
pages = {251--277},
title = {{An Evaluation of Knowledge Transfer from Microworld Programming to Conventional Programming}},
url = {http://journals.sagepub.com/doi/abs/10.2190/EC.47.3.b},
volume = {47},
year = {2012}
}
@book{Blunch2017,
abstract = {This comprehensive Second Edition offers readers a complete guide to carrying out research projects involving structural equation modeling (SEM). Updated to include extensive analysis of AMOS' graphical interface, a new chapter on latent curve models and detailed explanations of the structural equation modeling process, this second edition is the ideal guide for those new to the field. The book includes: Learning objectives, key concepts and questions for further discussion in each chapter. Helpful diagrams and screenshots to expand on concepts covered in the texts. Real life examples from a variety of disciplines to show how SEM is applied in real research contexts. Exercises for each chapter on an accompanying companion website. A new glossary. Assuming no previous experience of the subject, and a minimum of mathematical knowledge, this is the ideal guide for those new to SEM and an invaluable companion for students taking introductory SEM courses in any discipline. Niels J. Blunch was formerly in the Department of Marketing and Statistics at the University of Aarhus, Denmark},
address = {London},
author = {Blunch, Niels J.},
booktitle = {Introduction to Structural Equation Modeling using IBM SPSS Statistics and AMOS},
doi = {10.4135/9781526402257},
month = {jan},
publisher = {SAGE Publications, Ltd},
title = {{Introduction to Structural Equation Modeling using IBM SPSS Statistics and AMOS}},
year = {2017}
}
@article{Fisher1922RegressionFirstPValue,
author = {Fisher, R.A.},
journal = {Journal of the Royal Statistical Society},
number = {85},
pages = {597--612},
title = {{The goodness of fit of regression formulae, and the distribution of regression coefficients}},
volume = {85},
year = {1922}
}
@inproceedings{Good1985WeightOfEvidenceSurvey,
address = {Valencia, Spain},
author = {Good, I.J.},
booktitle = {Bayesian Statistics (Vol. 2)},
editor = {Bernado, J.M. and DeGroot, Morris H. and Lindley, D.V. and Smith, A.F.M.},
pages = {249--277},
publisher = {Elsevier Science Publishers B.V. (North-Holland)},
title = {{Weight of Evidence: A brief survey}},
year = {1985}
}
@misc{VanFraassen1986,
author = {{Van Fraassen}, Bas C. and Hughes, R. I.G. and Harman, Gilbert},
booktitle = {British Journal for the Philosophy of Science},
doi = {10.1093/bjps/37.4.453},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Van Fraassen, Hughes, Harman - 1986 - A problem for relative information minimizers, continued.pdf:pdf},
issn = {14643537},
number = {4},
pages = {453--475},
title = {{A problem for relative information minimizers, continued}},
volume = {37},
year = {1986}
}
@article{Hoerl1970,
author = {Hoerl, Arthur E. and Kennard, Robert W.},
doi = {10.1080/00401706.1970.10488634},
issn = {0040-1706},
journal = {Technometrics},
month = {feb},
number = {1},
pages = {55--67},
title = {{Ridge Regression: Biased Estimation for Nonorthogonal Problems}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.1970.10488634},
volume = {12},
year = {1970}
}
@article{Meehl1978,
abstract = {Theories in "soft" areas of psychology (e.g., clinical, counseling, social, personality, school, and community) lack the cumulative character of scientific knowledge because they tend neither to be refuted nor corroborated, but instead merely fade away as people lose interest. Even though intrinsic subject matter difficulties (20 are listed) contribute to this, the excessive reliance on significance testing is partly responsible (Ronald A. Fisher). Karl Popper's approach, with modifications, would be prophylactic. Since the null hypothesis is quasi-always false, tables summarizing research in terms of patterns of "significant differences" are little more than complex, causally uninterpretable outcomes of statistical power functions. Multiple paths to estimating numerical point values ("consistency tests") are better, even if approximate with rough tolerances; and lacking this, ranges, orderings, 2nd-order differences, curve peaks and valleys, and function forms should be used. Such methods are usual in developed sciences that seldom report statistical significance. Consistency tests of a conjectural taxometric model yielded 94{\%} success with no false negatives. (3 p ref) (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1978 American Psychological Association.},
author = {Meehl, Paul E.},
doi = {10.1037/0022-006X.46.4.806},
issn = {0022006X},
journal = {Journal of Consulting and Clinical Psychology},
keywords = {community psychology,counseling {\&},experimental design {\&},personality {\&},school {\&},social {\&},statistical problems, clinical {\&}},
month = {aug},
number = {4},
pages = {806--834},
title = {{Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology}},
url = {/record/1979-25042-001},
volume = {46},
year = {1978}
}
@article{Wilks1938,
abstract = {The object of this note is to point out and discuss a simple transformation of an absolutely continuous k-variate distribution in the uniform distribution on the k-dimensional hypercube.},
author = {Wilks, S. S.},
doi = {10.1214/aoms/1177732360},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Wilks - 1938 - The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses.pdf:pdf},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {mar},
number = {1},
pages = {60--62},
publisher = {Institute of Mathematical Statistics},
title = {{The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses}},
url = {http://projecteuclid.org/euclid.aoms/1177732360},
volume = {9},
year = {1938}
}
@article{Colquhoun2016,
author = {Colquhoun, David},
doi = {10.1016/S1369-7021(08)70254-2},
isbn = {0954497406 (pbk.) : ¹9.99},
issn = {1369-7021},
journal = {Aeon},
pmid = {38331},
title = {{The problem with p-values}},
url = {http://dx.doi.org/10.1016/S1369-7021(08)70254-2},
year = {2016}
}
@book{Hoffmann2016,
author = {Hoffmann, Michael},
isbn = {365814131X},
title = {{Stochastische Integration : Eine Einf{\"{u}}hrung in die Finanzmathematik}},
year = {2016}
}
@article{swett_outpatient_1975,
abstract = {Phenothiazine-induced bone marrow depression (BMD) was evaluated in three separate but complementary data bases: (1) Among 1,048 patients admitted to psychiatric hospitals, there was no evidence of subclinical depression of the white blood cell (WBC) count attributable to phenothiazines used before admission. (2) Among 18,587 medical inpatients, there were 34 patients admitted for BMD in the absence of neoplasia or prior cytotoxic drug therapy; one of the latter reported using chlorpromazine hydrochloride, but it is doubtful whether this drug was the cause of the BMD. (3) Among 24,795 medical, surgical, and gynecological patients surveyed over a ten-month period in 1972, there were four who were admitted for BMD; one of the latter had a reversible leukopenia attributed to trifluoperazine hydrochloride.},
author = {Swett, C},
issn = {0003-990X},
journal = {Archives of General Psychiatry},
keywords = {Ambulatory Care,Antipsychotic Agents,Female,Humans,Leukopenia,Male,Mental Disorders,Phenothiazines},
month = {nov},
number = {11},
pages = {1416--1418},
pmid = {978},
title = {{Outpatient phenothiazine use and bone marrow depression. {\{}A{\}} report from the drug epidemiology unit and the {\{}Boston{\}} collaborative drug surveillance program}},
volume = {32},
year = {1975}
}
@article{Hoeting1999,
author = {Hoeting, Jennifer A. and Madigan, David and Raftery, Adrian E. and Volinsky, Chris T.},
doi = {10.1214/SS/1009212519},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Bayesian graphical models,Bayesian model averaging,Markov chain Monte Carlo,learning,model uncertainty},
month = {nov},
number = {4},
pages = {382--417},
publisher = {Institute of Mathematical Statistics},
title = {{Bayesian model averaging: a tutorial (with comments by M. Clyde, David Draper and E. I. George, and a rejoinder by the authors}},
volume = {14},
year = {1999}
}
@article{Gudys2020,
abstract = {Rule-based models are often used for data analysis as they combine interpretability with predictive power. We present RuleKit, a versatile tool for rule learning. Based on a sequential covering induction algorithm, it is suitable for classification, regression, and survival problems. The presence of a user-guided induction facilitates verifying hypotheses concerning data dependencies which are expected or of interest. The powerful and flexible experimental environment allows straightforward investigation of different induction schemes. The analysis can be performed in batch mode, through RapidMiner plug-in, or R package. The software is available at GitHub (https://github.com/adaa-polsl/RuleKit) under GNU AGPL-3.0 license.},
archivePrefix = {arXiv},
arxivId = {1908.01031},
author = {Gudy{\'{s}}, Adam and Sikora, Marek and Wr{\'{o}}bel, {\L}ukasz},
doi = {10.1016/j.knosys.2020.105480},
eprint = {1908.01031},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gudy{\'{s}}, Sikora, Wr{\'{o}}bel - 2020 - RuleKit A comprehensive suite for rule-based learning.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Classification,Knowledge discovery,Regression,Rule learning,Survival analysis,User-guided induction},
month = {apr},
pages = {105480},
publisher = {Elsevier B.V.},
title = {{RuleKit: A comprehensive suite for rule-based learning}},
volume = {194},
year = {2020}
}
@article{Altman2000,
abstract = {I review some areas of medical statistics that have gained prominence over the last 5-10 years: meta-analysis, evidence-based medicine, and cluster randomized trials. I then consider several issues relating to data analysis and interpretation, many relating to the use and misuse of hypothesis testing, drawing on recent reviews of the use of statistics in medical journals. I also consider developments in the reporting of research in medical journals.},
author = {Altman, Douglas G.},
journal = {Statistics in Medicine},
number = {23},
pages = {3275--3289},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Statistics in medical journals: Some recent trends}},
volume = {19},
year = {2000}
}
@article{Durbin1970,
abstract = {If the conditioning variable is required to depend only on the value of the minimal sufficient statistic, Birnbaum's proof fails. {\textcopyright} Taylor {\&} Francis Group, LLC.},
author = {Durbin, J.},
doi = {10.1080/01621459.1970.10481088},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
number = {329},
pages = {395--398},
title = {{On birnbaum's theorem on the relation between sufficiency, conditionality and likelihood}},
volume = {65},
year = {1970}
}
@article{Aldrich1997,
abstract = {In 1922 R. A. Fisher introduced the method of maximum likelihood. He first presented the numerical procedure in 1912. This paper considers Fisher's changing justifications for the method, the concepts he developed around it (including likelihood, sufficiency, efficiency and information) and the approaches he discarded (including inverse probability).},
author = {Aldrich, John},
doi = {10.1214/ss/1030037906},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Aldrich - 1997 - R.A. Fisher and the making of maximum likelihood 1912-1922.pdf:pdf},
journal = {Statistical Science},
keywords = {and phrases,bayes,efficiency,fisher,infor-,inverse probability,maximum likelihood,pearson,s postulate,student,sufficiency},
number = {3},
pages = {162--176},
title = {{R.A. Fisher and the making of maximum likelihood 1912-1922}},
url = {http://projecteuclid.org/euclid.ss/1030037906},
volume = {12},
year = {1997}
}
@article{Stigler2008a,
author = {Stigler, Stephen},
doi = {10.1007/s00144-008-0033-3},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Stigler - 2008 - Fisher and the 5{\%} level.pdf:pdf},
isbn = {0933-2480},
issn = {0933-2480},
journal = {Chance},
number = {4},
pages = {12--12},
title = {{Fisher and the 5{\%} level}},
url = {http://link.springer.com/10.1007/s00144-008-0033-3},
volume = {21},
year = {2008}
}
@article{Flinn1974,
author = {Flinn, P. A.},
doi = {10.1007/BF01011718},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Flinn - 1974 - Monte Carlo calculation of phase separation in a two-dimensional Ising system.pdf:pdf},
issn = {0022-4715},
journal = {Journal of Statistical Physics},
month = {jan},
number = {1},
pages = {89--97},
publisher = {Kluwer Academic Publishers-Plenum Publishers},
title = {{Monte Carlo calculation of phase separation in a two-dimensional Ising system}},
url = {http://link.springer.com/10.1007/BF01011718},
volume = {10},
year = {1974}
}
@article{Gonen2005,
abstract = {This article shows how the pooled-variance two-sample t statistic arises from a Bayesian formulation of the two-sided point null testing problem, with emphasis on teaching. We identify a reasonable...},
author = {G{\"{o}}nen, Mithat and Johnson, Wesley O and Lu, Yonggang and Westfall, Peter H},
doi = {10.1198/000313005X55233},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/G{\"{o}}nen et al. - 2005 - The Bayesian Two-Sample t Test.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Bayes factor,Posterior probability,Prior elicitation,Teaching bayesian statistics},
month = {aug},
number = {3},
pages = {252--257},
publisher = {Taylor {\&} Francis},
title = {{The Bayesian Two-Sample t Test}},
volume = {59},
year = {2005}
}
@article{Rafi2020,
abstract = {Background: Researchers often misinterpret and misrepresent statistical outputs. This abuse has led to a large literature on modification or replacement of testing thresholds and P-values with confidence intervals, Bayes factors, and other devices. Because the core problems appear cognitive rather than statistical, we review some simple methods to aid researchers in interpreting statistical outputs. These methods emphasize logical and information concepts over probability, and thus may be more robust to common misinterpretations than are traditional descriptions. Methods: We use the Shannon transform of the P-value p, also known as the binary surprisal or S-value s = -log2(p), to provide a measure of the information supplied by the testing procedure, and to help calibrate intuitions against simple physical experiments like coin tossing. We also use tables or graphs of test statistics for alternative hypotheses, and interval estimates for different percentile levels, to thwart fallacies arising from arbitrary dichotomies. Finally, we reinterpret P-values and interval estimates in unconditional terms, which describe compatibility of data with the entire set of analysis assumptions. We illustrate these methods with a reanalysis of data from an existing record-based cohort study. Conclusions: In line with other recent recommendations, we advise that teaching materials and research reports discuss P-values as measures of compatibility rather than significance, compute P-values for alternative hypotheses whenever they are computed for null hypotheses, and interpret interval estimates as showing values of high compatibility with data, rather than regions of confidence. Our recommendations emphasize cognitive devices for displaying the compatibility of the observed data with various hypotheses of interest, rather than focusing on single hypothesis tests or interval estimates. We believe these simple reforms are well worth the minor effort they require.},
archivePrefix = {arXiv},
arxivId = {1909.08579},
author = {Rafi, Zad and Greenland, Sander},
doi = {10.1186/s12874-020-01105-9},
eprint = {1909.08579},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Rafi, Greenland - 2020 - Semantic and cognitive tools to aid statistical science Replace confidence and significance by compatibility an.pdf:pdf},
issn = {14712288},
journal = {BMC Medical Research Methodology},
keywords = {Bias,Cognitive science,Confidence intervals,Data interpretation,Evidence,Hypothesis tests,Information,Models,P-values,Statistical significance,statistical},
number = {1},
pages = {244},
pmid = {32998683},
publisher = {BioMed Central Ltd},
title = {{Semantic and cognitive tools to aid statistical science: Replace confidence and significance by compatibility and surprise}},
url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-020-01105-9},
volume = {20},
year = {2020}
}
@article{Lenhard2006a,
abstract = {The main thesis of the paper is that in the case of modern statistics, the differences between the various concepts of models were the key to its formative controversies. The mathematical theory of statistical inference was mainly developed by Ronald A. Fisher, Jerzy Neyman, and Egon S. Pearson. Fisher on the one side and Neyman–Pearson on the other were involved often in a polemic controversy. The common view is that Neyman and Pearson made Fisher's account more stringent mathematically. It is argued, however, that there is a profound theoretical basis for the controversy: both sides held conflicting views about the role of mathematical modelling. At the end, the influential programme of Exploratory Data Analysis is considered to be advocating another, more instrumental conception of models.},
author = {Lenhard, Johannes},
doi = {10.1093/bjps/axi152},
isbn = {0007-0882},
issn = {00070882},
journal = {British Journal for the Philosophy of Science},
number = {1},
pages = {69--91},
title = {{Models and statistical inference: The controversy between Fisher and Neyman-Pearson}},
volume = {57},
year = {2006}
}
@article{stiegler_laplace1774,
author = {Stigler, Stephen M.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Stigler - 1986 - Laplace's 1774 Memoir on Inverse Probability.pdf:pdf},
journal = {Statistics},
number = {3},
pages = {359--378},
title = {{Laplace's 1774 Memoir on Inverse Probability}},
volume = {1},
year = {1986}
}
@article{Buchanan-Wollaston1935,
abstract = {I HOPE some space will be afforded me for comment on Prof. Karl Pearson's letters1, since I fear that Prof. Pearson's expressed opinions are not calculated to engender trust in modern statistical methods. These have proved, however, to be of almost universal practical service in minimising the number of times an observer may be misled by his observations.},
author = {Buchanan-Wollaston, H. J.},
doi = {10.1038/136722a0},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Buchanan-Wollaston - 1935 - Statistical tests 8.pdf:pdf},
issn = {00280836},
journal = {Nature},
keywords = {Humanities and Social Sciences,Science,multidisciplinary},
number = {3444},
pages = {722},
publisher = {Nature Publishing Group},
title = {{Statistical tests}},
url = {https://www.nature.com/articles/136722a0},
volume = {136},
year = {1935}
}
@article{Lavine2019,
abstract = {ABSTRACTBoth philosophically and in practice, statistics is dominated by frequentist and Bayesian thinking. Under those paradigms, our courses and textbooks talk about the accuracy with which true model parameters are estimated or the posterior probability that they lie in a given set. In nonparametric problems, they talk about convergence to the true function (density, regression, etc.) or the probability that the true function lies in a given set. But the usual paradigms' focus on learning the true model and parameters can distract the analyst from another important task: discovering whether there are many sets of models and parameters that describe the data reasonably well. When we discover many good models we can see in what ways they agree. Points of agreement give us more confidence in our inferences, but points of disagreement give us less. Further, the usual paradigms' focus seduces us into judging and adopting procedures according to how well they learn the true values. An alternative is to judge...},
author = {Lavine, Michael},
doi = {10.1080/00031305.2018.1459317},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Lavine - 2019 - Frequentist, Bayes, or Other.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {likelihood,philosophy,statistical,theory of},
number = {sup1},
pages = {312--318},
title = {{Frequentist, Bayes, or Other?}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1459317},
volume = {73},
year = {2019}
}
@incollection{Zellner1980,
address = {Amsterdam},
author = {Zellner, Arnold},
booktitle = {Bayesian Analysis in Econometrics and Statistics : Essays in Honor of Harold Jeffreys},
chapter = {1},
editor = {Zellner, Arnold and Kadane, Joseph B.},
isbn = {978-0444852700},
publisher = {Elsevier North-Holland},
title = {{Introduction}},
year = {1980}
}
@article{Mirowski2011,
abstract = {This trenchant study analyzes the rise and decline in the quality and format of science in America since World War II. Science-Mart attributes this decline to a powerful neoliberal ideology in the 1980s which saw the fruits of scientific investigation as commodities that could be monetized, rather than as a public good. This trenchant study analyzes the rise and decline in the quality and format of science in America since World War II.During the Cold War, the U.S. government amply funded basic research in science and medicine. Starting in the 1980s, however, this support began to decline and for-profit corporations became the largest funders of research. Philip Mirowski argues that a powerful neoliberal ideology promoted a radically different view of knowledge and discovery: the fruits of scientific investigation are not a public good that should be freely available to all, but are commodities that could be monetized.Consequently, patent and intellectual property laws were greatly strengthened, universities demanded patents on the discoveries of their faculty, information sharing among researchers was impeded, and the line between universities and corporations began to blur. At the same time, corporations shed their in-house research laboratories, contracting with independent firms both in the States and abroad to supply new products. Among such firms were AT{\&}T and IBM, whose outstanding research laboratories during much of the twentieth century produced Nobel Prize–winning work in chemistry and physics, ranging from the transistor to superconductivity.Science-Mart offers a provocative, learned, and timely critique, of interest to anyone concerned that American science—once the envy of the world—must be more than just another way to make money. Viridiana Jones and the Temple of Mammon : or, adventures in neoliberal science studies -- [pt.] I. Why we should not depend upon the existing content of an "Economics of Science." The "Economics of Science" as repeat offender -- [pt.] II. A modern economic history of science organization. Regimes of American science organization ; Lovin' intellectual property and livin' with the MTA : retracting research tools ; Pharma's market : new horizons in outsourcing in the modern globalized regime -- [pt.] III. Where we are headed. Has science been "harmed" by the modern commercial regime? ; The new production of ignorance : the dirty secret of the new knowledge economy.},
author = {Mirowski, Philip},
doi = {10.1093/jahist/jar345},
isbn = {9780674046467},
issn = {0021-8723},
journal = {Isis},
number = {No. 1},
pages = {211--212},
pmid = {16481949},
publisher = {Harvard University Press},
title = {{Science-Mart: Privatizing American Science}},
volume = {103},
year = {2012}
}
@article{Barker2002,
abstract = {Eliminating health disparities in vaccination coverage among various groups is a cornerstone of public health policy. However, the statistical tests traditionally used cannot prove that a state of no difference between groups exists. Instead of asking, "Has a disparity - or difference - in immunization coverage among population groups been eliminated?," one can ask, "Has practical equivalence been achieved?" A method called equivalence testing can show that the difference between groups is smaller than a tolerably small amount. This paper demonstrates the method and introduces public health considerations that have an impact on defining tolerable levels of difference. Using data from the 2000 National Immunization Survey, the authors tested for statistically significant differences in rates of vaccination coverage between Whites and members of other racial/ethnic groups and for equivalencies among Whites and these same groups. For some minority groups and some vaccines, coverage was statistically significantly lower than was seen among Whites; however, for some of these groups and vaccines, equivalence testing revealed practical equivalence. To use equivalence testing to assess whether a disparity remains a threat to public health, researchers must understand when to use the method, how to establish assumptions about tolerably small differences, and how to interpret the test results.},
author = {Barker, Lawrence E. and Luman, Elizabeth T. and McCauley, Mary M. and Chu, Susan Y.},
doi = {10.1093/aje/kwf149},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Barker et al. - 2002 - Assessing equivalence An alternative to the use of difference tests for measuring disparities in vaccination cove.pdf:pdf},
issn = {00029262},
journal = {American Journal of Epidemiology},
keywords = {Epidemiologic methods,Ethnic groups,Hypothesis testing,Immunization,Statistics,Vaccination,Vaccines},
number = {11},
pages = {1056--1061},
pmid = {12446263},
title = {{Assessing equivalence: An alternative to the use of difference tests for measuring disparities in vaccination coverage}},
volume = {156},
year = {2002}
}
@article{Gronau2017,
abstract = {The marginal likelihood plays an important role in many areas of Bayesian statistics such as parameter estimation, model comparison, and model averaging. In most applications, however, the marginal likelihood is not analytically tractable and must be approximated using numerical methods. Here we provide a tutorial on bridge sampling (Bennett, 1976; Meng {\&} Wong, 1996), a reliable and relatively straightforward sampling method that allows researchers to obtain the marginal likelihood for models of varying complexity. First, we introduce bridge sampling and three related sampling methods using the beta-binomial model as a running example. We then apply bridge sampling to estimate the marginal likelihood for the Expectancy Valence (EV) model—a popular model for reinforcement learning. Our results indicate that bridge sampling provides accurate estimates for both a single participant and a hierarchical version of the EV model. We conclude that bridge sampling is an attractive method for mathematical psychologists who typically aim to approximate the marginal likelihood for a limited set of possibly high-dimensional models.},
author = {Gronau, Quentin F. and Sarafoglou, Alexandra and Matzke, Dora and Ly, Alexander and Boehm, Udo and Marsman, Maarten and Leslie, David S. and Forster, Jonathan J. and Wagenmakers, Eric-Jan and Steingroever, Helen},
doi = {10.1016/j.jmp.2017.09.005},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gronau et al. - 2017 - A tutorial on bridge sampling.pdf:pdf},
issn = {10960880},
journal = {Journal of Mathematical Psychology},
keywords = {Bayes factor,Hierarchical model,Marginal likelihood,Normalizing constant,Predictive accuracy,Reinforcement learning},
pages = {80--97},
publisher = {Academic Press Inc.},
title = {{A tutorial on bridge sampling}},
volume = {81},
year = {2017}
}
@article{Simonsohn2015,
abstract = {This article introduces a new approach for evaluating replication results. It combines effect-size estimation with hypothesis testing, assessing the extent to which the replication results are cons...},
author = {Simonsohn, Uri},
doi = {10.1177/0956797614567341},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Simonsohn - 2015 - Small Telescopes Detectability and the Evaluation of Replication Results.pdf:pdf},
journal = {Psychological Science},
keywords = {hypothesis testing,open materials,replication,statistical power},
month = {mar},
number = {5},
pages = {559--569},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Small Telescopes: Detectability and the Evaluation of Replication Results}},
volume = {26},
year = {2015}
}
@article{Fisher1918,
abstract = {Reproduced with permission of the Royal Society of Edinburgh},
author = {Fisher, Ronald Aylmer},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1918 - The Correlation Between Relatives on the Supposition of Mendelian Inheritance.pdf:pdf},
journal = {Transactions of the Royal Society of Edinburgh},
number = {52},
pages = {399--433},
title = {{The Correlation Between Relatives on the Supposition of Mendelian Inheritance.}},
url = {https://digital.library.adelaide.edu.au/dspace/handle/2440/15097},
year = {1918}
}
@book{misa_gender_2010,
abstract = {The computing profession is facing a serious gender crisis. Women are abandoning the computing field. This book explains the complex social and cultural processes at work in gender and computing today. Through engaging historical accounts, this book tells the stories of women programmers, systems analysts, managers, and IT executives who flooded this field. It then examines why the computing field has declined in female participants.--[book cover]},
address = {Hoboken, N.J. : [Piscataway, NJ]},
annote = {Gender codes : defining the problem / Thomas J. Misa -- Computer science : the incredible shrinking woman / Caroline Clarke Hayes -- Masculinity and the machine man : gender in the history of data processing / Thomas Haigh -- A gendered job carousel : employment effects of computer automation / Corinna Schlombs -- Meritocracy and feminization in conflict : computerization in the British government / Marie Hicks -- Making programming masculine / Nathan Ensmenger -- Gender and computing in the push-button library / Greg Downey -- Cultural perceptions of computers in Norway 1980-2007 : from "anybody" via "male experts" to "everybody" / Hilde G. Corneliussen -- Constructing gender and technology in advertising images : feminine and masculine computer parts / Aristotle Tympas ... [et al.] -- The pleasure paradox : bridging the gap between popular images of computing and women's historical experiences / Janet Abbate -- Programming enterprise : women entrepreneurs in software and computer services / Jeffrey R. Yost -- Gender codes : lessons from history / Thomas J. Misa -- Gender codes : prospects for change / Caroline Clarke Hayes
OCLC: ocn619124701},
editor = {Misa, Thomas J},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2010 - Gender codes why women are leaving computing.pdf:pdf},
isbn = {978-0-470-59719-4},
keywords = {Computer industry,Women in computer science},
publisher = {Wiley ; IEEE Computer Society},
shorttitle = {Gender codes},
title = {{Gender codes: why women are leaving computing}},
year = {2010}
}
@article{Blackwelder1982,
abstract = {When designing a clinical trial to show whether a new or experimental therapy is as effective as a standard therapy (but not necessarily more effective), the usual null hypothesis of equality is inappropriate and leads to logical difficulties. Since therapies cannot be shown to be literally equivalent, the appropriate null hypothesis is that the standard therapy is more effective than the experimental therapy by at least some specified amount. The problem is presented in terms of a trial in which the outcome of interest is dichotomous; test statistics, confidence intervals, and sample size calculations are discussed. The required sample size may be larger for either null hypothesis formulation than for the other, depending on the specific assumptions made. Reporting results in terms of confidence intervals is especially useful for this type of trial. {\textcopyright} 1982.},
author = {Blackwelder, William C.},
doi = {10.1016/0197-2456(82)90024-1},
issn = {01972456},
journal = {Controlled Clinical Trials},
keywords = {confidence interval,design,null hypothesis,sample size},
number = {4},
pages = {345--353},
pmid = {7160191},
publisher = {Control Clin Trials},
title = {{"Proving the null hypothesis" in clinical trials}},
url = {https://pubmed.ncbi.nlm.nih.gov/7160191/},
volume = {3},
year = {1982}
}
@article{Sellke2001,
abstract = {P-values are the most commonly used tool to measure evidence against a hypothesis or hypothesized model. Unfortunately, they are often incorrectly viewed as an error probability for rejection of the hypothesis or, even worse, as the posterior probability that the hypothesis is true. The fact that these interpretations can be completely misleading when testing precise hypotheses is first reviewed, through consideration of two revealing simulations. Then two calibrations of a p-value are developed, the first being interpretable as odds and the second as either a (conditional) frequentist error probability or as the posterior probability of the hypothesis.},
author = {Sellke, T. and Bayarri, J. J. and Berger, J.O.},
doi = {10.1198/000313001300339950},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Sellke, Bayarri, Berger - 2001 - Calibration of p-values for testing precise hypotheses.pdf:pdf},
isbn = {0003-1305},
issn = {0003-1305},
journal = {American Statistician},
keywords = {and phrases,bayes factors,bayesian robustness,conditional,frequentist error probabilities,odds,surprise},
number = {1},
pages = {62--71},
title = {{Calibration of p-values for testing precise hypotheses}},
volume = {55},
year = {2001}
}
@article{Soper1917,
author = {Soper, H.E. and Young, A.W. and Cave, B.M. and Lee, Alice and Pearson, Karl},
doi = {10.2307/2331830},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Soper et al. - 1917 - On the Distribution of the Correlation Coefficient in Small Samples. Appendix II to the Papers of Student and R. A.pdf:pdf},
isbn = {0006-3444},
issn = {00063444},
journal = {Biometrika},
number = {4},
pages = {328--413},
title = {{On the Distribution of the Correlation Coefficient in Small Samples. Appendix II to the Papers of "Student" and R. A. Fisher}},
volume = {11},
year = {1917}
}
@article{Calin-Jageman2019,
abstract = {The "New Statistics" emphasizes effect sizes, confidence intervals, meta‐analysis, and the use of Open Science practices. We present 3 specific ways in which a New Statistics approach can help improve scientific practice: by reducing over‐confidence in small samples, by reducing confirmation bias, and by fostering more cautious judgments of consistency. We illustrate these points through consideration of the literature on oxytocin and human trust, a research area that typifies some of the endemic problems that arise with poor statistical practice},
author = {Calin-Jageman, Robert J. and Cumming, Geoff},
doi = {10.1080/00031305.2018.1518266},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Calin-Jageman, Cumming - 2019 - The New Statistics for Better Science Ask How Much, How Uncertain, and What Else Is Known.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {confidence,estimation,intervals,meta-analysis,open science,the new},
number = {sup1},
pages = {271--280},
title = {{The New Statistics for Better Science: Ask How Much, How Uncertain, and What Else Is Known}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1518266},
volume = {73},
year = {2019}
}
@article{Jeffreys1933,
author = {Jeffreys, Harold},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Jeffreys - 1933 - Probability, Statistics and the Theory of Errors.pdf:pdf},
journal = {Proceedings of the Royal Society A},
pages = {523--535},
title = {{Probability, Statistics and the Theory of Errors}},
volume = {140},
year = {1933}
}
@article{Stern2003,
author = {Stern, J. M.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Stern - 2003 - Significance tests, Belief Calculi, and Burden of Proof in legal and Scientific Discourse.pdf:pdf},
journal = {Frontiers in Artificial Intelligence and its Applications},
pages = {139--147},
title = {{Significance tests, Belief Calculi, and Burden of Proof in legal and Scientific Discourse}},
volume = {101},
year = {2003}
}
@article{Ghosal2008,
abstract = {We consider nonparametric Bayesian estimation of a probability density p based on a random sample of size n from this density using a hierarchical prior. The prior consists, for instance, of prior weights on the regularity of the unknown density combined with priors that are appropriate given that the density has this regularity. More generally, the hierarchy consists of prior weights on an abstract model index and a prior on a density model for each model index. We present a general theorem on the rate of contraction of the resulting posterior distribution as n → ∞, which gives conditions under which the rate of contraction is the one attached to the model that best approximates the true density of the observations. This shows that, for instance, the posterior distribution can adapt to the smoothness of the underlying density. We also study the posterior distribution of the model index, and find that under the same conditions the posterior distribution gives negligible weight to models that are bigger than the optimal one, and thus selects the optimal model or smaller models that also approximate the true density well. We apply these result to log spline density models, where we show that the prior weights on the regularity index interact with the priors on the models, making the exact rates depend in a complicated way on the priors, but also that the rate is fairly robust to specification of the prior weights. {\textcopyright} 2008, Institute of Mathematical Statistics. All rights reserved.},
author = {Ghosal, Subhashis and Lember, J{\"{u}}ri and {Van Der Vaart}, Aad},
doi = {10.1214/07-EJS090},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Adaptation,Bayes factor,Rate of contraction,Rate of convergence},
pages = {63--89},
publisher = {The Institute of Mathematical Statistics and the Bernoulli Society},
title = {{Nonparametric bayesian model selection and averaging}},
url = {https://projecteuclid.org/euclid.ejs/1201877208},
volume = {2},
year = {2008}
}
@article{Bartlett1952,
author = {Bartlett, M.S.},
doi = {10.1093/biomet/39.3-4.228},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/BARTLETT - 1952 - THE STATISTICAL SIGNIFICANCE OF ODD BITS OF INFORMATION.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {3-4},
pages = {228--237},
publisher = {Oxford University Press (OUP)},
title = {{The Statistical Significance of Odd Bits of Information}},
volume = {39},
year = {1952}
}
@article{McElreath2015,
abstract = {Many published research results are false (Ioannidis, 2005), and controversy continues over the roles of replication and publication policy in improving the reliability of research. Addressing these problems is frustrated by the lack of a formal framework that jointly represents hypothesis formation, replication, publication bias, and variation in research quality. We develop a mathematical model of scientific discovery that combines all of these elements. This model provides both a dynamic model of research as well as a formal framework for reasoning about the normative structure of science. We show that replication may serve as a ratchet that gradually separates true hypotheses from false, but the same factors that make initial findings unreliable also make replications unreliable. The most important factors in improving the reliability of research are the rate of false positives and the base rate of true hypotheses, and we offer suggestions for addressing each. Our results also bring clarity to verbal debates about the communication of research. Surprisingly, publication bias is not always an obstacle, but instead may have positive impacts-suppression of negative novel findings is often beneficial. We also find that communication of negative replications may aid true discovery even when attempts to replicate have diminished power. The model speaks constructively to ongoing debates about the design and conduct of science, focusing analysis and discussion on precise, internally consistent models, as well as highlighting the importance of population dynamics.},
author = {McElreath, Richard and Smaldino, Paul E.},
doi = {10.1371/journal.pone.0136088},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/McElreath, Smaldino - 2015 - Replication, communication, and the population dynamics of scientific discovery.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {8},
pages = {1--16},
title = {{Replication, communication, and the population dynamics of scientific discovery}},
volume = {10},
year = {2015}
}
@book{Edwards1972,
address = {London},
author = {Edwards, A.W.F.},
publisher = {Cambridge University Press},
title = {{Likelihood}},
year = {1972}
}
@article{Zabell1992,
author = {Zabell, S. L.},
doi = {10.1214/ss/1177011233},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Behrens-Fisher problem,Fiducial inference,Jerzy Neyman,Maurice Bartlett,R. A. Fisher,recognize subsets},
month = {aug},
number = {3},
pages = {369--387},
publisher = {Institute of Mathematical Statistics},
title = {{R. A. Fisher and Fiducial Argument}},
url = {http://projecteuclid.org/euclid.ss/1177011233},
volume = {7},
year = {1992}
}
@book{Konishi2008,
abstract = {The Akaike information criterion (AIC) derived as an estimator of the Kullback-Leibler information discrepancy provides a useful tool for evaluating statistical models, and numerous successful applications of the AIC have been reported in various fields of natural sciences, social sciences and engineering. One of the main objectives of this book is to provide comprehensive explanations of the concepts and derivations of the AIC and related criteria, including Schwarz's Bayesian information criterion (BIC), together with a wide range of practical examples of model selection and evaluation crite. Front Matter; Concept of Statistical Modeling; Statistical Models; Information Criterion; Statistical Modeling by AIC; Generalized Information Criterion (GIC); Statistical Modeling by GIC; Theoretical Development and Asymptotic Properties of the GIC; Bootstrap Information Criterion; Bayesian Information Criteria; Various Model Evaluation Criteria; Back Matter.},
address = {New York},
author = {Konishi, Sadanori. and Kitagawa, G. (Genshiro)},
isbn = {9781441924568},
publisher = {Springer},
title = {{Information criteria and statistical modeling}},
year = {2008}
}
@book{Fisher1935aDesignOfExperiments1st,
address = {Edinburgh},
author = {Fisher, R.A.},
edition = {1st},
publisher = {Oliver and Boyd},
title = {{The design of experiments}},
year = {1935}
}
@article{gelfand1990,
author = {Gelfand, Alan E and Smith, Adrian F M},
doi = {10.1080/01621459.1990.10476213},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Gelfand, Smith - 1990 - Sampling-Based Approaches to Calculating Marginal Densities.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {data,gibbs sampler,hierarchical models,importance sam-,missing data,monte carlo samphng,phng,posterior distributions,probability structure,stochastic substitution,ugmentation,variance compo-},
pages = {398--409},
title = {{Sampling-Based Approaches to Calculating Marginal Densities}},
url = {http://links.jstor.org/sici?sici=0162-1459{\%}28199006{\%}2985{\%}3A410{\%}3C398{\%}3ASATCMD{\%}3E2.0.CO{\%}3B2-3},
volume = {85},
year = {1990}
}
@article{Yang2017,
author = {Yang, Jinyoung and Rosenthal, Jeffrey S},
journal = {Computational Statistics},
pages = {315--348},
title = {{Automatically Tuned General-Purpose MCMC via New Adaptive Diagnostics}},
volume = {32},
year = {2017}
}
@article{Held2016,
abstract = {ABSTRACTMinimum Bayes factors are commonly used to transform two-sided p-values to lower bounds on the posterior probability of the null hypothesis. Several proposals exist in the literature, but n...},
author = {Held, Leonhard and Ott, Manuela},
doi = {10.1080/00031305.2016.1209128},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Evidence,F-Test,Minimum Bayes factor,Sample size,p-value,t-Test},
month = {oct},
number = {4},
pages = {335--341},
publisher = {Taylor {\&} Francis},
title = {{How the Maximal Evidence of p-Values Against Point Null Hypotheses Depends on Sample Size}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1209128},
volume = {70},
year = {2016}
}
@book{Keynes1921,
address = {London},
author = {Keynes, J.M.},
publisher = {Macmillan {\&} Co},
title = {{A treatise on probability}},
year = {1921}
}
@article{Philippe2001,
author = {Philippe, Anne and Robert, Christian P.},
doi = {10.1023/A:1008926514119},
issn = {09603174},
journal = {Statistics and Computing},
number = {2},
pages = {103--115},
publisher = {Kluwer Academic Publishers},
title = {{Riemann sums for MCMC estimation and convergence monitoring}},
url = {http://link.springer.com/10.1023/A:1008926514119},
volume = {11},
year = {2001}
}
@incollection{Hinkley1980,
author = {Hinkley, David},
booktitle = {R.A. Fisher - An Appreciation},
doi = {10.1007/978-1-4612-6079-0_12},
pages = {101--108},
publisher = {Springer, New York, NY},
title = {{Fisher's Development of Conditional Inference}},
url = {http://link.springer.com/10.1007/978-1-4612-6079-0{\_}12},
year = {1980}
}
@article{Hajek1970,
abstract = {We consider a sequence of estimates in a sequence of general estimation problems with a k-dimensional parameter. Under certain very general conditions we prove that the limiting distribution of the estimates, if properly normed, is a convolution of a certain normal distribution, which depends only of the underlying distributions, and of a further distribution, which depends on the choice of the estimate. As corollaries we obtain inequalities for asymptotic variances and for asymptotic probabilities of certain sets, generalizing so some results of J. Wolfowitz (1965), S. Kaufman (1966), L. Schmetterer (1966) and G. G. Roussas (1968). {\textcopyright} 1970 Springer-Verlag.},
author = {H{\'{a}}jek, Jaroslav},
doi = {10.1007/BF00533669},
issn = {00443719},
journal = {Zeitschrift f{\"{u}}r Wahrscheinlichkeitstheorie und Verwandte Gebiete},
keywords = {Economics,Finance,Insurance,Management,Mathematical and Computational Biology,Mathematical and Computational Physics,Operations Research/Decision Theory,Probability Theory and Stochastic Processes,Quantitative Finance,Statistics for Business,Theoretical},
number = {4},
pages = {323--330},
publisher = {Springer-Verlag},
title = {{A characterization of limiting distributions of regular estimates}},
volume = {14},
year = {1970}
}
@article{Piironen2017a,
author = {Piironen, Juho and Vehtari, Aki},
doi = {10.1007/s11222-016-9649-y},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Piironen, Vehtari - 2017 - Comparison of Bayesian predictive methods for model selection.pdf:pdf},
issn = {0960-3174},
journal = {Statistics and Computing},
month = {may},
number = {3},
pages = {711--735},
publisher = {Springer US},
title = {{Comparison of Bayesian predictive methods for model selection}},
url = {http://link.springer.com/10.1007/s11222-016-9649-y},
volume = {27},
year = {2017}
}
@book{krathwohl_taxonomy_1971,
address = {New York},
author = {Krathwohl, David R and Bloom, Benjamin S and Masia, Bertram B},
number = {Handbook II - Affective Domain},
publisher = {David McKay},
series = {The {\{}Classification{\}} of {\{}Educational{\}} {\{}Goals{\}}},
title = {{Taxonomy Of Educational Objectives}},
volume = {II},
year = {1971}
}
@article{Liao2020,
abstract = {There has been strong recent interest in testing interval null hypotheses for improved scientific inference. For example, Lakens et al. and Lakens and Harms use this approach to study if there is a prespecified meaningful treatment effect in gerontology and clinical trials, instead of a point null hypothesis of any effect. Two popular Bayesian approaches are available for interval null hypothesis testing. One is the standard Bayes factor and the other is the region of practical equivalence (ROPE) procedure championed by Kruschke and others over many years. This article connects key quantities in the two approaches, which in turn allow us to contrast two major differences between the approaches with substantial practical implications. The first is that the Bayes factor depends heavily on the prior specification while a modified ROPE procedure is very robust. The second difference is concerned with the statistical property when data are generated under a neutral parameter value on the common boundary of competing hypotheses. In this case, the Bayes factors can be severely biased whereas the modified ROPE approach gives a reasonable result. Finally, the connection leads to a simple and effective algorithm for computing Bayes factors using draws from posterior distributions generated by standard Bayesian programs such as BUGS, JAGS, and Stan.},
author = {Liao, J. G. and Midya, Vishal and Berg, Arthur},
doi = {10.1080/00031305.2019.1701550},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Liao, Midya, Berg - 2020 - Connecting and Contrasting the Bayes Factor and a Modified ROPE Procedure for Testing Interval Null Hypothese.pdf:pdf},
issn = {15372731},
journal = {American Statistician},
keywords = {Bayes factor,Interval null hypothesis,Markov chain Monte Carlo,ROPE,Stan},
publisher = {American Statistical Association},
title = {{Connecting and Contrasting the Bayes Factor and a Modified ROPE Procedure for Testing Interval Null Hypotheses}},
year = {2020}
}
@misc{Seibold2021,
abstract = {Statisticians play a key role in almost all scientific research. As such, they may be key to solving the reproducibility crisis. Heidi Seibold, Alethea Charlton, Anne-Laure Boulesteix and Sabine Hoffmann urge statisticians to take an active role in promoting more credible science.},
author = {Seibold, Heidi and Charlton, Alethea and Boulesteix, Anne Laure and Hoffmann, Sabine},
booktitle = {Significance},
doi = {10.1111/1740-9713.01554},
issn = {17409713},
month = {aug},
number = {4},
pages = {42--44},
publisher = {John Wiley and Sons Inc},
title = {{Statisticians, roll up your sleeves! There's a crisis to be solved}},
volume = {18},
year = {2021}
}
@article{Callingham2017,
author = {Callingham, Rosemary and Watson, Jane M.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Callingham, Watson - 2017 - The development of statistical literacy at school.pdf:pdf},
issn = {15701824},
journal = {Statistics Education Research Journal},
keywords = {Longitudinal analysis,Middle years,Rasch modelling,Statistical literacy},
number = {1},
pages = {181--201},
title = {{The development of statistical literacy at school}},
volume = {16},
year = {2017}
}
@article{Smaldino2016,
abstract = {Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favor them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing---no deliberate cheating nor loafing---by scientists, only that publication is a principle factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modeling. We first present a 60-year meta-analysis of statistical power in the behavioral sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more "progeny", such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.},
author = {Smaldino, Paul E. and McElreath, Richard},
doi = {10.1098/rsos.160384},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Smaldino, McElreath - 2016 - The natural selection of bad science.pdf:pdf},
issn = {20545703},
journal = {Royal Society Open Science},
keywords = {Campbell's law,Cultural evolution,Incentives,Metascience,Replication,Statistical power},
number = {9},
title = {{The natural selection of bad science}},
volume = {3},
year = {2016}
}
@article{sorgo_attributes_2017,
abstract = {Digital natives are assumed to possess knowledge and skills that allow them to handle information and communication technologies (ICT) tools in a “natural” way. Accordingly, this calls for the application of different teaching/learning strategies in education. The purpose of the study was to test the predictive strength of some attributes of digital nativeness (ICT ownership, ICT experiences, internet confidence and number of ICT-rich university courses) on the information literacy (IL) of 299 Slovenian university students. Correlation and regression analysis based on survey data revealed that the attributes of digital natives are poor predictors of IL. The principal findings are: ICT experiences expressed as the sum of the use of different applications do not necessarily contribute to IL; some applications have a positive and some a negative effect; personal ownership of smartphones, portable computers and desktop computers has no direct effect on IL, while ownership of a tablet computer is actually a negative predictor; personal ownership of ICT devices has an impact on ICT experiences and Internet confidence, and, therefore, an indirect impact on IL; and ICT-rich university courses (if not designed to cultivate IL) have only a marginal impact on IL, although they may have some impact on ICT experiences and Internet confidence. The overall conclusion is that digital natives are not necessarily information literate, and that IL should be promoted with hands-on and minds-on courses based on IL standards.},
author = {{\v{S}}orgo, Andrej and Bartol, Toma{\v{z}} and Dolni{\v{c}}ar, Danica and {Boh Podgornik}, Bojana},
doi = {10.1111/bjet.12451},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/{\v{S}}orgo et al. - 2017 - Attributes of digital natives as predictors of information literacy in higher education.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/{\v{S}}orgo et al. - 2017 - Attributes of digital natives as predictors of information literacy in higher education.html:html},
issn = {1467-8535},
journal = {British Journal of Educational Technology},
month = {may},
number = {3},
pages = {749--767},
title = {{Attributes of digital natives as predictors of information literacy in higher education}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/bjet.12451/abstract},
volume = {48},
year = {2017}
}
@book{Meyn2009,
abstract = {Meyn {\&} Tweedie is back! The bible on Markov chains in general state spaces has been brought up to date to reflect developments in the field since 1996 - many of them sparked by publication of the first edition. The pursuit of more efficient simulation algorithms for complex Markovian models, or algorithms for computation of optimal policies for controlled Markov models, has opened new directions for research on Markov chains. As a result, new applications have emerged across a wide range of topics including optimisation, statistics, and economics. New commentary and an epilogue by Sean Meyn summarise recent developments and references have been fully updated. This second edition reflects the same discipline and style that marked out the original and helped it to become a classic: proofs are rigorous and concise, the range of applications is broad and knowledgeable, and key ideas are accessible to practitioners with limited mathematical background.},
address = {Cambridge},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Meyn, Sean and Tweedie, Richard L.},
booktitle = {Markov Chains and Stochastic Stability, Second Edition},
doi = {10.1017/CBO9780511626630},
eprint = {arXiv:1011.1669v3},
isbn = {9780511626630},
issn = {01621459},
pages = {1--504},
pmid = {25246403},
publisher = {Cambridge University Press},
title = {{Markov chains and stochastic stability, second edition}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511626630},
year = {2009}
}
@book{david_2001,
address = {New York, Berlin, Heidelberg},
author = {David, H.A. and Edwards, A.W.F.},
publisher = {Springer-Verlag},
title = {{Perspectives in Statistics - Springer Series in Statistics}},
year = {2001}
}
@article{Anderson2019,
abstract = {Evaluating the importance and the strength of empirical evidence requires asking three questions: First, what are the practical implications of the findings? Second, how precise are the estimates? Confidence intervals provide an intuitive way to communicate precision. Although nontechnical audiences often misinterpret confidence intervals (CIs), I argue that the result is less dangerous than the misunderstandings that arise from hypothesis tests. Third, is the model correctly specified? The validity of point estimates and CIs depends on the soundness of the underlying model. ARTICLE HISTORY},
author = {Anderson, Andrew A.},
doi = {10.1080/00031305.2018.1537889},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Anderson - 2019 - Assessing Statistical Results Magnitude, Precision, and Model Uncertainty.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {inference,robustness},
number = {sup1},
pages = {118--121},
title = {{Assessing Statistical Results: Magnitude, Precision, and Model Uncertainty}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1537889},
volume = {73},
year = {2019}
}
@article{Mcouat2017,
abstract = {In 1957, John Burdon Sanderson (JBS) Haldane (1892-1964), the world's leading population geneticist, committed political radical and one of the three 'founders' of neo-Darwinian 'Modern Synthesis' of twentieth century biology (Sarkar 1995; Haldane 1932; Cain 2009; Smocovitis 1996), ostentatiously renounced both his British citizenship and his prestigious chair at University College London. In a decisively and very public anti-imperial gesture, ostensibly played out as a reaction to the Suez crisis (although his discontent was simmering for quite some time), Haldane, and his partner, geneticistHelen Spurway (1917-1977), turned their backs on Britain and set off to India to offer their considerable scientific prestige, their inexhaustible organisational abilities, along with their leading Journal of Genetics, behind the efforts to build a 'modern', democratic India emerging out of the ashes of colonial rule. Haldane's support of independent India was a major triumph for the new state, itself in the midst of negotiating a fine balance between rapid modernization through science and technology and an postcolonial respect for traditional 'non-Western' values. Although his time in India was short, Haldane's few years in India were marked by a frenzied engagement with the new India, its science, its government and its culture (Rao 2013).},
author = {Mcouat, Gordon},
issn = {0973-7731},
journal = {Journal of Genetics},
month = {nov},
number = {5},
pages = {845--852},
pmid = {29237894},
title = {{J. B. S. Haldane's passage to India: reconfiguring science}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29237894},
volume = {96},
year = {2017}
}
@book{Bauer1996,
author = {Bauer, Heinz and Burckel, Robert B.},
booktitle = {Probability Theory},
doi = {10.1515/9783110814668},
isbn = {9783110814668},
month = {jan},
publisher = {De Gruyter},
title = {{Probability Theory}},
url = {https://www.degruyter.com/view/title/3786},
year = {1996}
}
@article{Robert2011Fraser,
author = {Robert, Christian P.},
journal = {Statistical Science},
number = {3},
pages = {317--318},
title = {{Discussion of ``Is Bayes Posterior just Quick and Dirty Confidence?'' by D.A.S. Fraser}},
volume = {26},
year = {2011}
}
@book{NavarroJASPBook2019,
author = {Navarro, Danielle J. and Foxcroft, David R. and Faulkenberry, Thomas J.},
title = {{Learning Statistics with JASP: A Tutorial for Psychology Students and Other Beginners}},
url = {https://learnstatswithjasp.com/},
year = {2019}
}
@article{Mengersen1996,
author = {{Mengersen K. L.} and Tweedie, R. L.},
journal = {the Annals of Statistics},
keywords = {1 hastings and metropolis,algorithms and markov chains},
number = {1},
pages = {101--121},
title = {{Rates of Convergence of the Hastings and Metropolis Algorithms}},
url = {http://www.jstor.org/stable/2242610},
volume = {24},
year = {2012}
}
@article{Tong2019,
abstract = {ABSTRACTScientific research of all kinds should be guided by statistical thinking: in the design and conduct of the study, in the disciplined exploration and enlightened display of the data, and to avoid statistical pitfalls in the interpretation of the results. However, formal, probability-based statistical inference should play no role in most scientific research, which is inherently exploratory, requiring flexible methods of analysis that inherently risk overfitting. The nature of exploratory work is that data are used to help guide model choice, and under these circumstances, uncertainty cannot be precisely quantified, because of the inevitable model selection bias that results. To be valid, statistical inference should be restricted to situations where the study design and analysis plan are specified prior to data collection. Exploratory data analysis provides the flexibility needed for most other situations, including statistical methods that are regularized, robust, or nonparametric. Of course, no ...},
author = {Tong, Christopher},
doi = {10.1080/00031305.2018.1518264},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Tong - 2019 - Statistical Inference Enables Bad Science Statistical Thinking Enables Good Science.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {many sets of data,model,optimism,principle,researcher degrees,uncertainty},
number = {sup1},
pages = {246--261},
title = {{Statistical Inference Enables Bad Science; Statistical Thinking Enables Good Science}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1518264},
volume = {73},
year = {2019}
}
@incollection{AzenBudesco2009,
address = {London},
author = {Azen, Razia and Budescu, David},
booktitle = {The SAGE Handbook of Quantitative Methods in Psychology},
doi = {10.4135/9780857020994.n13},
editor = {Millsap, R.E. and Maydeu-Olivares, A.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Azen, Budescu - 2009 - Applications of Multiple Regression in Psychological Research.pdf:pdf},
pages = {285--310},
publisher = {SAGE},
title = {{Applications of Multiple Regression in Psychological Research}},
year = {2009}
}
@article{Zucchini2000,
abstract = {This paper is an introduction to model selection intended for nonspecialists who have knowledge of the statistical concepts covered in a typical first (occasionally second) statistics course. The intention is to explain the ideas that generate frequentist methodology for model selection, for example the Akaike information criterion, bootstrap criteria, and cross-validation criteria. Bayesian methods, including the Bayesian information criterion, are also mentioned in the context of the framework outlined in the paper. The ideas are illustrated using an example in which observations are available for the entire population of interest. This enables us to examine and to measure effects that are usually invisible, because in practical applications only a sample from the population is observed. The problem of selection bias, a hazard of which one needs to be aware in the context of model selection, is also discussed. {\textcopyright} 2000 Academic Press.},
author = {Zucchini, Walter},
doi = {10.1006/jmps.1999.1276},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
month = {mar},
number = {1},
pages = {41--61},
publisher = {Academic Press},
title = {{An introduction to model selection}},
volume = {44},
year = {2000}
}
@article{Manski2019a,
abstract = {AbstractA convention in designing randomized clinical trials has been to choose sample sizes that yield specified statistical power when testing hypotheses about treatment response. Manski and Tetenov recently critiqued this convention and proposed enrollment of sufficiently many subjects to enable near-optimal treatment choices. This article develops a refined version of that analysis applicable to trials comparing aggressive treatment of patients with surveillance. The need for a refined analysis arises because the earlier work assumed that there is only a primary health outcome of interest, without secondary outcomes. An important aspect of choice between surveillance and aggressive treatment is that the latter may have side effects. One should then consider how the primary outcome and side effects jointly determine patient welfare. This requires new analysis of sample design. As a case study, we reconsider a trial comparing nodal observation and lymph node dissection when treating patients with cutane...},
author = {Manski, Charles F. and Tetenov, Aleksey},
doi = {10.1080/00031305.2018.1543617},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Manski, Tetenov - 2019 - Trial Size for Near-Optimal Choice Between Surveillance and Aggressive Treatment Reconsidering MSLT-II.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {analysis of treatment,clinical trials,medical decisions,minimax,response},
number = {sup1},
pages = {305--311},
title = {{Trial Size for Near-Optimal Choice Between Surveillance and Aggressive Treatment: Reconsidering MSLT-II}},
url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1543617},
volume = {73},
year = {2019}
}
@article{Coniffe1990,
abstract = {Read before the Society, 15 November 1990},
author = {Coniffe, Denis},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Coniffe - 1990 - R.A. Fisher and the Development of Statistics - a View in His Centenary Year.pdf:pdf},
isbn = {0081-4776},
issn = {00814776},
journal = {Journal of the Statistical and Social Inquiry Society of Ireland},
keywords = {314.15,Fisher,History,R. A.,Statistical analysis},
number = {3},
pages = {55--108},
publisher = {Statistical and Social Inquiry Society of Ireland},
title = {{R.A. Fisher and the Development of Statistics - a View in His Centenary Year}},
url = {http://www.tara.tcd.ie/handle/2262/2764 http://hdl.handle.net/2262/2764},
volume = {26},
year = {1990}
}
@book{VanDerVaart1998,
address = {Cambridge},
author = {van der Vaart, A.W.},
isbn = {0521496039},
publisher = {Cambridge University Press},
title = {{Asymptotic Statistics}},
year = {1998}
}
@article{Good1994a,
abstract = {(1994). C421. Turing's little theorem is not really paradoxical. Journal of Statistical Computation and Simulation: Vol. 49, No. 3-4, pp. 242-244.},
author = {Good, I. J.},
doi = {10.1080/00949659408811588},
issn = {15635163},
journal = {Journal of Statistical Computation and Simulation},
keywords = {Arc-sine law,Bayesian philosophy,Intuition training,Paradigms Paradoxes,Turing's theorem,weight of evidence},
number = {3-4},
pages = {242--244},
publisher = {Gordon and Breach Science Publishers},
title = {{C421. Turing's little theorem is not really paradoxical}},
url = {https://www.tandfonline.com/doi/abs/10.1080/00949659408811588},
volume = {49},
year = {1994}
}
@article{Newton2013,
abstract = {The classic view of metastatic cancer progression is that it is a unidirectional process initiated at the primary tumor site, progressing to variably distant metastatic sites in a fairly predictable, although not perfectly understood, fashion. A Markov chain Monte Carlo mathematical approach can determine a pathway diagram that classifies metastatic tumors as "spreaders" or "sponges" and orders the timescales of progression from site to site. In light of recent experimental evidence highlighting the potential significance of self-seeding of primary tumors, we use a Markov chain Monte Carlo (MCMC) approach, based on large autopsy data sets, to quantify the stochastic, systemic, and often multidirectional aspects of cancer progression. We quantify three types of multidirectional mechanisms of progression: (i) self-seeding of the primary tumor, (ii) reseeding of the primary tumor from a metastatic site (primary reseeding), and (iii) reseeding of metastatic tumors (metastasis reseeding). The model shows that the combined characteristics of the primary and the first metastatic site to which it spreads largely determine the future pathways and timescales of systemic disease.},
author = {Newton, Paul K and Mason, Jeremy and Bethel, Kelly and Bazhenova, Lyudmila and Nieva, Jorge and Norton, Larry and Kuhn, Peter},
doi = {10.1158/0008-5472.CAN-12-4488},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Newton et al. - 2013 - Spreaders and sponges define metastasis in lung cancer A markov chain monte carlo mathematical model.pdf:pdf},
isbn = {1538-7445 (Electronic)$\backslash$r0008-5472 (Linking)},
issn = {00085472},
journal = {Cancer Research},
month = {may},
number = {9},
pages = {2760--2769},
pmid = {23447576},
publisher = {American Association for Cancer Research},
title = {{Spreaders and sponges define metastasis in lung cancer: A markov chain monte carlo mathematical model}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23447576 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3644026},
volume = {73},
year = {2013}
}
@book{DeVaus2002,
address = {London},
annote = {Literaturverz. S. [367] - 373 Previous ed.: London: UCL, 1996},
author = {{De Vaus}, David A},
booktitle = {Social research today},
edition = {5. ed},
isbn = {978-0-415-26857-8 978-0-415-26858-5},
keywords = {Empirische Sozialforschung,Social sciences Research Methodology,Social surveys,Umfrage},
language = {eng},
pages = {379},
publisher = {Routledge},
title = {{Surveys in social research}},
year = {2002}
}
@article{Kelter2020BayesianPosteriorIndices,
author = {Kelter, Riko},
doi = {https://doi.org/10.1186/s12874-020-00968-2},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Kelter - 2020 - Simulation data for the analysis of Bayesian posterior significance and effect size indices for the two-sample t-test to.pdf:pdf},
journal = {BMC Medical Research Methodology},
number = {88},
title = {{Analysis of Bayesian posterior significance and effect size indices for the two-sample t-test to support reproducible medical research}},
volume = {20},
year = {2020}
}
@article{Good1994,
abstract = {(1994). C420. The existence of sharp null hypotheses. Journal of Statistical Computation and Simulation: Vol. 49, No. 3-4, pp. 241-242.},
author = {Good, I.J.},
doi = {10.1080/00949659408811587},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Good - 1994 - C420. The existence of sharp null hypotheses.pdf:pdf},
issn = {15635163},
journal = {Journal of Statistical Computation and Simulation},
keywords = {Inexactification,Inverse square law,Null hypothesis,Precognition,Refutation,sharp},
number = {3-4},
pages = {241--242},
publisher = {Gordon and Breach Science Publishers},
title = {{C420. The existence of sharp null hypotheses}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=gscs20},
volume = {49},
year = {1994}
}
@article{FDADemonstratingSubstantialEvidence,
author = {{U.S. Food and Drug Administration Center for Biologics Evaluation and Research (CBER)}},
journal = {https://www.fda.gov/media/133660/download (accessed 01/08/2021)},
title = {{Demonstrating Substantial Evidence of Effectiveness for Human Drugs and Biological Products: Guidance for Industry}},
year = {2019}
}
@article{Quintana2018,
abstract = {Despite its popularity as an inferential framework, classical null hypothesis significance testing (NHST) has several restrictions. Bayesian analysis can be used to complement NHST, however, this approach has been underutilized largely due to a dearth of accessible software options. JASP is a recently developed open-source statistical package that facilitates both Bayesian and NHST analysis using a graphical interface. This article provides an applied introduction to Bayesian inference with Bayes factors using JASP. We use JASP to compare and contrast Bayesian alternatives for several common classical null hypothesis significance tests: correlations, frequency distributions, t-tests, ANCOVAs, and ANOVAs. These examples are also used to illustrate the strengths and limitations of both NHST and Bayesian hypothesis testing. A comparison of NHST and Bayesian inferential frameworks demonstrates that Bayes factors can complement p-values by providing additional information for hypothesis testing. Namely, Bayes factors can quantify relative evidence for both alternative and null hypotheses. Moreover, the magnitude of this evidence can be presented as an easy-to-interpret odds ratio. While Bayesian analysis is by no means a new method, this type of statistical inference has been largely inaccessible for most psychiatry researchers. JASP provides a straightforward means of performing reproducible Bayesian hypothesis tests using a graphical “point and click” environment that will be familiar to researchers conversant with other graphical statistical packages, such as SPSS.},
author = {Quintana, Daniel S. and Williams, Donald R.},
doi = {10.1186/s12888-018-1761-4},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Quintana, Williams - 2018 - Bayesian alternatives for common null-hypothesis significance tests in psychiatry a non-technical guide usin.pdf:pdf},
issn = {1471-244X},
journal = {BMC Psychiatry},
keywords = {Psychiatry,Psychotherapy},
month = {dec},
number = {1},
pages = {178},
publisher = {BioMed Central},
title = {{Bayesian alternatives for common null-hypothesis significance tests in psychiatry: a non-technical guide using JASP}},
volume = {18},
year = {2018}
}
@incollection{Fisher1911inBenett1983FisherMendelismAndBiometry,
author = {Fisher, Ronald Aylmer},
booktitle = {Natural selection, heredity, and eugenics : Including selected correspondence of R.A. Fisher with Leonard Darwin and others, edited by J.H. Bennett},
chapter = {II},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Fisher - 1911 - Mendelism and biometry.pdf:pdf},
pages = {51--58},
publisher = {Oxford : Clarendon Press},
title = {{Mendelism and biometry}},
year = {1911}
}
@book{Edwards1992,
address = {Baltimore, Maryland},
author = {Edwards, A.W.F.},
edition = {Expanded e},
isbn = {978-0801844430},
publisher = {The Johns Hopkins University Press},
title = {{Likelihood}},
year = {1992}
}
@inproceedings{alardawi_novice_2015,
abstract = {Class structure represents one of the essential concepts of Object-Oriented paradigm and therefore, a good understanding of this concept will positively affect the effectiveness of novice programmers. Comprehension underpins many programming activities such as program design and program implementation. Program comprehension represents in this context a mental model approach that involves interesting theoretical frameworks of program comprehension. Our starting point is Burkhardt cognitive model for OO program comprehension [1]. The model considers two distinct but interacting models: program and situation. Our focus does not rely primarily in distinguishing between these models, but use both of them to assess the influence on novices of class structure on program comprehension. We report on an empirical study that aims of to investigate the effect of class structure on program comprehension for novices using controlled experimentation in which the treatments were a simple program without class structure versus the same program with classes present; they are termed respectively as: Non-Class based program and as Class based program. Data was collected from three different sets of experiments comprising of a total of 211 undergraduate first year computer science students from different institutions. Preliminary findings of this investigation are reported, in particular results indicate that Class based programs were more understandable, readable, and accessible than the corresponding Non-Class based programs. Our findings align with and support those works that claim the cognitive benefits of the OO paradigm. Directions for future research are highlighted.},
annote = {Libya},
author = {Alardawi, A S and Agil, A M},
booktitle = {2015 {\{}World{\}} {\{}Congress{\}} on {\{}Information{\}} {\{}Technology{\}} and {\{}Computer{\}} {\{}Applications{\}} ({\{}WCITCA{\}})},
doi = {10.1109/WCITCA.2015.7367057},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Alardawi, Agil - 2015 - Novice comprehension of {\{}Object{\}}-{\{}Oriented{\}} {\{}OO{\}} programs {\{}An{\}} empirical study.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Alardawi, Agil - 2015 - Novice comprehension of {\{}Object{\}}-{\{}Oriented{\}} {\{}OO{\}} programs {\{}An{\}} empirical study.html:html},
keywords = {Burkhardt cognitive model,Cognitive science,Computer science education,Encapsulation,OO program comprehension,Object oriented modeling,Object recognition,Programming profession,Software,class based programs,class structure,empirical study,mental model,mental model approach,nonclass based program,novice programmers,object-oriented program comprehension,object-oriented programming,program comprehension,program model,situation model},
month = {jun},
pages = {1--4},
shorttitle = {Novice comprehension of {\{}Object{\}}-{\{}Oriented{\}} {\{}OO{\}} p},
title = {{Novice comprehension of {\{}Object{\}}-{\{}Oriented{\}} {\{}OO{\}} programs: {\{}An{\}} empirical study}},
year = {2015}
}
@article{Barnard1947,
author = {{Barnard G.A.}},
journal = {Journal of the American Statistical Association},
pages = {658--669},
title = {{A review of 'Sequential Analysis' by Abraham Wald}},
volume = {42},
year = {1947}
}
@book{bortz_forschungsmethoden_2006,
address = {Heidelberg},
annote = {Literaturverz. S. 829 - 876
OCLC: 844977370},
author = {Bortz, J{\"{u}}rgen and D{\"{o}}ring, Nicola},
edition = {4., {\"{u}}berar},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bortz, D{\"{o}}ring - 2006 - Forschungsmethoden und Evaluation f{\"{u}}r Human- und Sozialwissenschaftler mit 87 Tabellen.pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bortz, D{\"{o}}ring - 2006 - Forschungsmethoden und Evaluation f{\"{u}}r Human- und Sozialwissenschaftler mit 87 Tabellen(2).pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bortz, D{\"{o}}ring - 2006 - Forschungsmethoden und Evaluation f{\"{u}}r Human- und Sozialwissenschaftler mit 87 Tabellen(3).pdf:pdf;:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Bortz, D{\"{o}}ring - 2006 - Forschungsmethoden und Evaluation f{\"{u}}r Human- und Sozialwissenschaftler mit 87 Tabellen(4).pdf:pdf},
isbn = {978-3-540-33305-0},
keywords = {Empirische Sozialforschung,Evaluation,Lehrbuch},
publisher = {Springer-Medizin-Verl},
series = {Springer-{\{}Lehrbuch{\}} {\{}Bachelor{\}}, {\{}Master{\}}},
shorttitle = {Forschungsmethoden und {\{}Evaluation{\}}},
title = {{Forschungsmethoden und Evaluation: f{\"{u}}r Human- und Sozialwissenschaftler ; mit 87 Tabellen}},
year = {2006}
}
@article{Stigler2008,
abstract = {Karl Pearson played an enormous role in determining the content and organization of statistical research in his day, through his research, his teaching, his establishment of laboratories, and his initiation of a vast publishing program. His technical contributions had initially and continue today to have a profound impact upon the work of both applied and theoretical statisticians, partly through their inadequately acknowledged influence upon Ronald A. Fisher. Particular attention is drawn to two of Pearson's major errors that nonetheless have left a positive and lasting impression upon the statistical world.},
archivePrefix = {arXiv},
arxivId = {0808.4032},
author = {Stigler, Stephen M.},
doi = {10.1214/08-STS256},
eprint = {0808.4032},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Stigler - 2008 - Karl Pearson's Theoretical Errors and the Advances They Inspired.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {a,a new co-worker would,and his were not,and phrases,casually constructed,chi-square test,degrees of freedom,do,fisher,history of statistics,karl pearson,parametric inference,r,shelf space,when a student or,works},
number = {2},
pages = {261--271},
title = {{Karl Pearson's Theoretical Errors and the Advances They Inspired}},
url = {http://projecteuclid.org/euclid.ss/1219339117},
volume = {23},
year = {2008}
}
@incollection{Rousseau2007,
abstract = {In this paper we study Bayes factors and special p-values in their capacity of approximating interval null hypotheses (or more generally neighbourhoods of point null) hypotheses by point null or eventually parametric families in the case of the goodness of fit test of a parametric family. We prove that when the number of observations is large Bayes factors for point null hypotheses can approximate Bayes factors for interval null hypotheses for extremely small intervals. We also interpret the significance of calibration goodness of fit tests using a p-value in terms of width of neighbourhoods of the point null hypothesis. Finally we study the consistency of Bayes factors for goodness of fit tests of parametric families, which enables us to shed light on the behaviour of the Bayes factors.},
address = {Valencia},
author = {Rousseau, Judith},
booktitle = {Bayesian Statistics (Vol. 8)},
editor = {Bernado, J.M. and Berger, J.O. and Dawid, A.P. and Smith, A.F.M.},
file = {:Users/riko/Library/Application Support/Mendeley Desktop/Downloaded/Rousseau - Unknown - Approximating Interval hypothesis p-values and Bayes factors.pdf:pdf},
keywords = {Bayes factors,Goodness of fit tests,Point null,and Phrases: Asymptotic,p-values},
pages = {417--452},
publisher = {Oxford University Press},
title = {{Approximating Interval hypothesis : p-values and Bayes factors}},
year = {2007}
}
@article{DuPrel2010,
abstract = {The interpretation of scientific articles often requires an understanding of the methods of inferential statistics. This article informs the reader about frequently used statistical tests and their correct application.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {du Prel, JB. and R{\"{o}}hrig, B and Hommel, G and Blettner, M},
doi = {10.4103/0974-7788.72494},
eprint = {arXiv:1011.1669v3},
isbn = {1866-0452 (Electronic)$\backslash$r1866-0452 (Linking)},
issn = {0974-7788},
journal = {Dtsch Arztebl Int},
month = {may},
number = {19},
pages = {343--348},
pmid = {20532129},
title = {{Choosing statistical test}},
url = {https://www.aerzteblatt.de/10.3238/arztebl.2010.0343},
volume = {107},
year = {2010}
}
@article{Lavy2009a,
abstract = {One of the subjects that undergraduate students learning Object Oriented (OO) design find hard to apply is the construction of class hierarchies in general, and the use of interface classes in particular. The design process requires decomposition and reconstruction of problems in order to model software classes. The common attributes and behaviors are usually modeled using class inheritance except when the common behaviors do not belong to the same class hierarchy; in such case, an interface class is preferred. In order to be able to properly design, the designer has to demonstrate the abstraction abilities on various levels. In this study, we aimed at examining the students' demonstration of abstraction levels during design of class hierarchy in general, and whether or not they use interface classes in particular. The results of the study reveal that a majority of students were able to build a reasonable class hierarchy; however, many of them did not use interface classes as a tool for expressing common behaviors. Nevertheless, all of them could identify and choose the best solution for a particular problem among four different solutions, which included the use of interface class.},
author = {Lavy, Ilana and Rashkovits, Rami and Kouris, Roy},
doi = {10.1080/08993400903255218},
issn = {0899-3408},
journal = {Computer Science Education},
keywords = {class hierarchy,dual-process theory,interface class,levels of abstraction,object oriented design,the SOLO taxonomy},
month = {sep},
number = {March 2012},
pages = {155--177},
publisher = {Routledge},
title = {{Coping with abstraction in object orientation with a special focus on interface classes}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08993400903255218},
volume = {19},
year = {2009}
}
@article{Lauritzen1990,
author = {Lauritzen, S.L. and Dawid, A.P. and Larsen, B.N. and Leimer, H.G.},
journal = {Networks},
pages = {491--505},
title = {{Independence properties of directed Markov fields}},
volume = {20},
year = {1990}
}
